{
  "https://ojs.aaai.org/index.php/AAAI/article/view/27749": {
    "title": "A Multi-Modal Contrastive Diffusion Model for Therapeutic Peptide Generation",
    "volume": "main",
    "abstract": "Therapeutic peptides represent a unique class of pharmaceutical agents crucial for the treatment of human diseases. Recently, deep generative models have exhibited remarkable potential for generating therapeutic peptides, but they only utilize sequence or structure information alone, which hinders the performance in generation. In this study, we propose a Multi-Modal Contrastive Diffusion model (MMCD), fusing both sequence and structure modalities in a diffusion framework to co-generate novel peptide sequences and structures. Specifically, MMCD constructs the sequence-modal and structure-modal diffusion models, respectively, and devises a multi-modal contrastive learning strategy with inter-contrastive and intra-contrastive in each diffusion timestep, aiming to capture the consistency between two modalities and boost model performance. The inter-contrastive aligns sequences and structures of peptides by maximizing the agreement of their embeddings, while the intra-contrastive differentiates therapeutic and non-therapeutic peptides by maximizing the disagreement of their sequence/structure embeddings simultaneously. The extensive experiments demonstrate that MMCD performs better than other state-of-the-art deep generative methods in generating therapeutic peptides across various metrics, including antimicrobial/anticancer score, diversity, and peptide-docking",
    "checked": true,
    "id": "7bff7fd6b87f5ff024b86a52e98bf92acc893e2e",
    "semantic_title": "a multi-modal contrastive diffusion model for therapeutic peptide generation",
    "citation_count": 4,
    "authors": [
      "Yongkang  Wang",
      "Xuan Liu",
      "Feng Huang",
      "Zhankun Xiong",
      "Wen Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27750": {
    "title": "Towards Automated RISC-V Microarchitecture Design with Reinforcement Learning",
    "volume": "main",
    "abstract": "Microarchitecture determines the implementation of a microprocessor. Designing a microarchitecture to achieve better performance, power, and area (PPA) trade-off has been increasingly difficult. Previous data-driven methodologies hold inappropriate assumptions and lack more tightly coupling with expert knowledge. This paper proposes a novel reinforcement learning-based (RL) solution that addresses these limitations. With the integration of microarchitecture scaling graph, PPA preference space embedding, and proposed lightweight environment in RL, experiments using commercial electronic design automation (EDA) tools show that our method achieves an average PPA trade-off improvement of 16.03% than previous state-of-the-art approaches with 4.07× higher efficiency. The solution qualities outperform human implementations by at most 2.03× in the PPA trade-off",
    "checked": true,
    "id": "e40663dc5ab53bf665cec97d2d916332ca1aac9f",
    "semantic_title": "towards automated risc-v microarchitecture design with reinforcement learning",
    "citation_count": 0,
    "authors": [
      "Chen Bai",
      "Jianwang Zhai",
      "Yuzhe Ma",
      "Bei Yu",
      "Martin D. F. Wong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27751": {
    "title": "Generating Novel Leads for Drug Discovery Using LLMs with Logical Feedback",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) can be used as repositories of biological and chemical information to generate pharmacological lead compounds. However, for LLMs to focus on specific drug targets typically requires experimentation with progressively more refined prompts. Results thus become dependent not just on what is known about the target, but also on what is known about the prompt- engineering. In this paper, we separate the prompt into domain-constraints that can be written in a standard logical form and a simple text-based query. We investigate whether LLMs can be guided, not by refining prompts manually, but by refining the logical component automatically, keeping the query unchanged. We describe an iterative procedure LMLF (\"Language Model with Logical Feedback\") in which the constraints are progressively refined using a logical notion of generalisation. On any iteration, newly generated instances are verified against the constraint, providing \"logical-feedback\" for the next iteration's refinement of the constraints. We evaluate LMLF using two well-known targets (inhibition of the Janus Kinase 2; and Dopamine Receptor D2); and two different LLMs (GPT-3 and PaLM). We show that LMLF, starting with the same logical constraints and query text, can be used to guide both LLMs to generate potential leads. We find: (a) Binding affinities of LMLF-generated molecules are skewed towards higher binding affinities than those from existing baselines; (b) LMLF results in generating molecules that are skewed towards higher binding affinities than without logical feedback; (c) Assessment by a computational chemist suggests that LMLF generated compounds may be novel inhibitors. These findings suggest that LLMs with logical feedback may provide a mechanism for generating new leads without requiring the domain-specialist to acquire sophisticated skills in prompt-engineering",
    "checked": true,
    "id": "3613299c54bbea66dd6db1b00573f7ade021a5a9",
    "semantic_title": "generating novel leads for drug discovery using llms with logical feedback",
    "citation_count": 1,
    "authors": [
      "Shreyas Bhat Brahmavar",
      "Ashwin Srinivasan",
      "Tirtharaj Dash",
      "Sowmya Ramaswamy Krishnan",
      "Lovekesh Vig",
      "Arijit Roy",
      "Raviprasad Aduri"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27752": {
    "title": "SeGA: Preference-Aware Self-Contrastive Learning with Prompts for Anomalous User Detection on Twitter",
    "volume": "main",
    "abstract": "In the dynamic and rapidly evolving world of social media, detecting anomalous users has become a crucial task to address malicious activities such as misinformation and cyberbullying. As the increasing number of anomalous users improves the ability to mimic normal users and evade detection, existing methods only focusing on bot detection are ineffective in terms of capturing subtle distinctions between users. To address these challenges, we proposed SeGA, preference-aware self-contrastive learning for anomalous user detection, which leverages heterogeneous entities and their relations in the Twittersphere to detect anomalous users with different malicious strategies. SeGA utilizes the knowledge of large language models to summarize user preferences via posts. In addition, integrating user preferences with prompts as pseudo-labels for preference-aware self-contrastive learning enables the model to learn multifaceted aspects for describing the behaviors of users. Extensive experiments on the proposed TwBNT benchmark demonstrate that SeGA significantly outperforms the state-of-the-art methods (+3.5% ∼ 27.6%) and empirically validate the effectiveness of the model design and pre-training strategies. Our code and data are publicly available at https://github.com/ying0409/SeGA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ying-Ying Chang",
      "Wei-Yao Wang",
      "Wen-Chih Peng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27753": {
    "title": "Neural Embeddings for kNN Search in Biological Sequence",
    "volume": "main",
    "abstract": "Biological sequence nearest neighbor search plays a fundamental role in bioinformatics. To alleviate the pain of quadratic complexity for conventional distance computation, neural distance embeddings, which project sequences into geometric space, have been recognized as a promising paradigm. To maintain the distance order between sequences, these models all deploy triplet loss and use intuitive methods to select a subset of triplets for training from a vast selection space. However, we observed that such training often enables models to distinguish only a fraction of distance orders, leaving others unrecognized. Moreover, naively selecting more triplets for training under the state-of-the-art network not only adds costs but also hampers model performance. In this paper, we introduce Bio-kNN: a kNN search framework for biological sequences. It includes a systematic triplet selection method and a multi-head network, enhancing the discernment of all distance orders without increasing training expenses. Initially, we propose a clustering-based approach to partition all triplets into several clusters with similar properties, and then select triplets from these clusters using an innovative strategy. Meanwhile, we noticed that simultaneously training different types of triplets in the same network cannot achieve the expected performance, thus we propose a multi-head network to tackle this. Our network employs a convolutional neural network(CNN) to extract local features shared by all clusters, and then learns a multi-layer perception(MLP) head for each cluster separately. Besides, we treat CNN as a special head, thereby integrating crucial local features which are neglected in previous models into our model for similarity recognition. Extensive experiments show that our Bio-kNN significantly outperforms the state-of-the-art methods on two large-scale datasets without increasing the training cost",
    "checked": true,
    "id": "085a22b48db569992b1c900fccfa3fa8e276aaf7",
    "semantic_title": "neural embeddings for knn search in biological sequence",
    "citation_count": 1,
    "authors": [
      "Zhihao Chang",
      "Linzhu Yu",
      "Yanchao Xu",
      "Wentao Hu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27754": {
    "title": "i-Rebalance: Personalized Vehicle Repositioning for Supply Demand Balance",
    "volume": "main",
    "abstract": "Ride-hailing platforms have been facing the challenge of balancing demand and supply. Existing vehicle reposition techniques often treat drivers as homogeneous agents and relocate them deterministically, assuming compliance with the reposition. In this paper, we consider a more realistic and driver-centric scenario where drivers have unique cruising preferences and can decide whether to take the recommendation or not on their own. We propose i-Rebalance, a personalized vehicle reposition technique with deep reinforcement learning (DRL). i-Rebalance estimates drivers' decisions on accepting reposition recommendations through an on-field user study involving 99 real drivers. To optimize supply-demand balance and enhance preference satisfaction simultaneously, i-Rebalance has a sequential reposition strategy with dual DRL agents: Grid Agent to determine the reposition order of idle vehicles, and Vehicle Agent to provide personalized recommendations to each vehicle in the pre-defined order. This sequential learning strategy facilitates more effective policy training within a smaller action space compared to traditional joint-action methods. Evaluation of real-world trajectory data shows that i-Rebalance improves driver acceptance rate by 38.07% and total driver income by 9.97%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyang Chen",
      "Peiyan Sun",
      "Qiyuan Song",
      "Wanyuan Wang",
      "Weiwei Wu",
      "Wencan Zhang",
      "Guanyu Gao",
      "Yan Lyu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27755": {
    "title": "GIN-SD: Source Detection in Graphs with Incomplete Nodes via Positional Encoding and Attentive Fusion",
    "volume": "main",
    "abstract": "Source detection in graphs has demonstrated robust efficacy in the domain of rumor source identification. Although recent solutions have enhanced performance by leveraging deep neural networks, they often require complete user data. In this paper, we address a more challenging task, rumor source detection with incomplete user data, and propose a novel framework, i.e., Source Detection in Graphs with Incomplete Nodes via Positional Encoding and Attentive Fusion (GIN-SD), to tackle this challenge. Specifically, our approach utilizes a positional embedding module to distinguish nodes that are incomplete and employs a self-attention mechanism to focus on nodes with greater information transmission capacity. To mitigate the prediction bias caused by the significant disparity between the numbers of source and non-source nodes, we also introduce a class-balancing mechanism. Extensive experiments validate the effectiveness of GIN-SD and its superiority to state-of-the-art methods",
    "checked": true,
    "id": "8f94d67d73e005ffd2a92fe3715c571bac8ddb6e",
    "semantic_title": "gin-sd: source detection in graphs with incomplete nodes via positional encoding and attentive fusion",
    "citation_count": 4,
    "authors": [
      "Le Cheng",
      "Peican Zhu",
      "Keke Tang",
      "Chao Gao",
      "Zhen Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27756": {
    "title": "Deep Quantum Error Correction",
    "volume": "main",
    "abstract": "Quantum error correction codes (QECC) are a key component for realizing the potential of quantum computing. QECC, as its classical counterpart (ECC), enables the reduction of error rates, by distributing quantum logical information across redundant physical qubits, such that errors can be detected and corrected. In this work, we efficiently train novel end-to-end deep quantum error decoders. We resolve the quantum measurement collapse by augmenting syndrome decoding to predict an initial estimate of the system noise, which is then refined iteratively through a deep neural network. The logical error rates calculated over finite fields are directly optimized via a differentiable objective, enabling efficient decoding under the constraints imposed by the code. Finally, our architecture is extended to support faulty syndrome measurement, by efficient decoding of repeated syndrome sampling. The proposed method demonstrates the power of neural decoders for QECC by achieving state-of-the-art accuracy, outperforming for small distance topological codes, the existing end-to-end neural and classical decoders, which are often computationally prohibitive",
    "checked": true,
    "id": "4ac64275768197f8cc4fce6df29e4b95b2fe23b1",
    "semantic_title": "deep quantum error correction",
    "citation_count": 5,
    "authors": [
      "Yoni Choukroun",
      "Lior Wolf"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27757": {
    "title": "Propagation Tree Is Not Deep: Adaptive Graph Contrastive Learning Approach for Rumor Detection",
    "volume": "main",
    "abstract": "Rumor detection on social media has become increasingly important. Most existing graph-based models presume rumor propagation trees (RPTs) have deep structures and learn sequential stance features along branches. However, through statistical analysis on real-world datasets, we find RPTs exhibit wide structures, with most nodes being shallow 1-level replies. To focus learning on intensive substructures, we propose Rumor Adaptive Graph Contrastive Learning (RAGCL) method with adaptive view augmentation guided by node centralities. We summarize three principles for RPT augmentation: 1) exempt root nodes, 2) retain deep reply nodes, 3) preserve lower-level nodes in deep sections. We employ node dropping, attribute masking and edge dropping with probabilities from centrality-based importance scores to generate views. A graph contrastive objective then learns robust rumor representations. Extensive experiments on four benchmark datasets demonstrate RAGCL outperforms state-of-the-art methods. Our work reveals the wide-structure nature of RPTs and contributes an effective graph contrastive learning approach tailored for rumor detection through principled adaptive augmentation. The proposed principles and augmentation techniques can potentially benefit other applications involving tree-structured graphs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaoqun Cui",
      "Caiyan Jia"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27758": {
    "title": "Prompt to Transfer: Sim-to-Real Transfer for Traffic Signal Control with Prompt Learning",
    "volume": "main",
    "abstract": "Numerous solutions are proposed for the Traffic Signal Control (TSC) tasks aiming to provide efficient transportation and alleviate traffic congestion. Recently, promising results have been attained by Reinforcement Learning (RL) methods through trial and error in simulators, bringing confidence in solving cities' congestion problems. However, performance gaps still exist when simulator-trained policies are deployed to the real world. This issue is mainly introduced by the system dynamic difference between the training simulators and the real-world environments. In this work, we leverage the knowledge of Large Language Models (LLMs) to understand and profile the system dynamics by a prompt-based grounded action transformation to bridge the performance gap. Specifically, this paper exploits the pre-trained LLM's inference ability to understand how traffic dynamics change with weather conditions, traffic states, and road types. Being aware of the changes, the policies' action is taken and grounded based on realistic dynamics, thus helping the agent learn a more realistic policy. We conduct experiments on four different scenarios to show the effectiveness of the proposed PromptGAT's ability to mitigate the performance gap of reinforcement learning from simulation to reality (sim-to-real)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Longchao Da",
      "Minquan Gao",
      "Hao Mei",
      "Hua Wei"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27759": {
    "title": "Multitarget Device-Free Localization via Cross-Domain Wi-Fi RSS Training Data and Attentional Prior Fusion",
    "volume": "main",
    "abstract": "Device-free localization (DFL) using easily-obtained Wi-Fi received signal strength (RSS) has wide real-world applications for not requiring people to carry trackable devices. However, accurate multitarget DFL remains challenging due to the unknown number of targets, multipath interference (MPI), especially between nearby targets, and limited real-world data. In this study, we pioneeringly propose a transformer-based learning method with Wi-Fi RSS as input, and an attentional prior fusion module, to simultaneously locate an unknown number of people at random positions. To overcome the multitarget data collection challenges, we contribute a large-scale cross-domain real-simulation-augmentation training dataset with one and two real-world nearby non-person objects at limited positions and up to five simulated and augmented randomly distributed targets. Experimental results demonstrate our method's improved accuracy, generalization ability, and robustness with fewer Wi-Fi nodes than previous methods",
    "checked": true,
    "id": "149668484ebc0523ee24ac700fca8f3fcd7c06ba",
    "semantic_title": "multitarget device-free localization via cross-domain wi-fi rss training data and attentional prior fusion",
    "citation_count": 0,
    "authors": [
      "Na Fan",
      "Zeyue Tian",
      "Amartansh Dubey",
      "Samruddhi Deshmukh",
      "Ross Murch",
      "Qifeng Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27760": {
    "title": "Heterogeneous Graph Reasoning for Fact Checking over Texts and Tables",
    "volume": "main",
    "abstract": "Fact checking aims to predict claim veracity by reasoning over multiple evidence pieces. It usually involves evidence retrieval and veracity reasoning. In this paper, we focus on the latter, reasoning over unstructured text and structured table information. Previous works have primarily relied on fine-tuning pretrained language models or training homogeneous-graph-based models. Despite their effectiveness, we argue that they fail to explore the rich semantic information underlying the evidence with different structures. To address this, we propose a novel word-level Heterogeneous-graph-based model for Fact Checking over unstructured and structured information, namely HeterFC. Our approach leverages a heterogeneous evidence graph, with words as nodes and thoughtfully designed edges representing different evidence properties. We perform information propagation via a relational graph neural network, facilitating interactions between claims and evidence. An attention-based method is utilized to integrate information, combined with a language model for generating predictions. We introduce a multitask loss function to account for potential inaccuracies in evidence retrieval. Comprehensive experiments on the large fact checking dataset FEVEROUS demonstrate the effectiveness of HeterFC. Code will be released at: https://github.com/Deno-V/HeterFC",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haisong Gong",
      "Weizhi Xu",
      "Shu Wu",
      "Qiang Liu",
      "Liang Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27761": {
    "title": "Text-Guided Molecule Generation with Diffusion Language Model",
    "volume": "main",
    "abstract": "Text-guided molecule generation is a task where molecules are generated to match specific textual descriptions. Recently, most existing SMILES-based molecule generation methods rely on an autoregressive architecture. In this work, we propose the Text-Guided Molecule Generation with Diffusion Language Model (TGM-DLM), a novel approach that leverages diffusion models to address the limitations of autoregressive methods. TGM-DLM updates token embeddings within the SMILES string collectively and iteratively, using a two-phase diffusion generation process. The first phase optimizes embeddings from random noise, guided by the text description, while the second phase corrects invalid SMILES strings to form valid molecular representations. We demonstrate that TGM-DLM outperforms MolT5-Base, an autoregressive model, without the need for additional data resources. Our findings underscore the remarkable effectiveness of TGM-DLM in generating coherent and precise molecules with specific properties, opening new avenues in drug discovery and related scientific domains. Code will be released at: https://github.com/Deno-V/tgm-dlm",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haisong Gong",
      "Qiang Liu",
      "Shu Wu",
      "Liang Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27762": {
    "title": "Adversarial Robust Safeguard for Evading Deep Facial Manipulation",
    "volume": "main",
    "abstract": "The non-consensual exploitation of facial manipulation has emerged as a pressing societal concern. In tandem with the identification of such fake content, recent research endeavors have advocated countering manipulation techniques through proactive interventions, specifically the incorporation of adversarial noise to impede the manipulation in advance. Nevertheless, with insufficient consideration of robustness, we show that current methods falter in providing protection after simple perturbations, e.g., blur. In addition, traditional optimization-based methods face limitations in scalability as they struggle to accommodate the substantial expansion of data volume, a consequence of the time-intensive iterative pipeline. To solve these challenges, we propose a learning-based model, Adversarial Robust Safeguard (ARS), to generate desirable protection noise in a single forward process, concurrently exhibiting a heightened resistance against prevalent perturbations. Specifically, our method involves a two-way protection design, characterized by a basic protection component responsible for generating efficacious noise features, coupled with robust protection for further enhancement. In robust protection, we first fuse image features with spatially duplicated noise embedding, thereby accounting for inherent information redundancy. Subsequently, a combination comprising a differentiable perturbation module and an adversarial network is devised to simulate potential information degradation during the training process. To evaluate it, we conduct experiments on four manipulation methods and compare recent works comprehensively. The results of our method exhibit good visual effects with pronounced robustness against varied perturbations at different levels",
    "checked": true,
    "id": "14d3e2fb72ccec10a802a80abef043f627d7c9f7",
    "semantic_title": "adversarial robust safeguard for evading deep facial manipulation",
    "citation_count": 0,
    "authors": [
      "Jiazhi Guan",
      "Yi Zhao",
      "Zhuoer Xu",
      "Changhua Meng",
      "Ke Xu",
      "Youjian Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27763": {
    "title": "FlightBERT++: A Non-autoregressive Multi-Horizon Flight Trajectory Prediction Framework",
    "volume": "main",
    "abstract": "Flight Trajectory Prediction (FTP) is an essential task in Air Traffic Control (ATC), which can assist air traffic controllers in managing airspace more safely and efficiently. Existing approaches generally perform multi-horizon FTP tasks in an autoregressive manner, thereby suffering from error accumulation and low-efficiency problems. In this paper, a novel framework, called FlightBERT++, is proposed to i) forecast multi-horizon flight trajectories directly in a non-autoregressive way, and ii) improve the limitation of the binary encoding (BE) representation in the FlightBERT. Specifically, the FlightBERT++ is implemented by a generalized encoder-decoder architecture, in which the encoder learns the temporal-spatial patterns from historical observations and the decoder predicts the flight status for the future horizons. Compared with conventional architecture, an innovative horizon-aware contexts generator is dedicatedly designed to consider the prior horizon information, which further enables non-autoregressive multi-horizon prediction. Moreover, a differential prompted decoder is proposed to enhance the capability of the differential predictions by leveraging the stationarity of the differential sequence. The experimental results on a real-world dataset demonstrated that the FlightBERT++ outperformed the competitive baselines in both FTP performance and computational efficiency",
    "checked": false,
    "id": "23f20241ae62e8e882d4c835b2af746fd3f0a0f8",
    "semantic_title": "a non-autoregressive multi-horizon flight trajectory prediction framework with gray code representation",
    "citation_count": 0,
    "authors": [
      "Dongyue Guo",
      "Zheng Zhang",
      "Zhen Yan",
      "Jianwei Zhang",
      "Yi Lin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27764": {
    "title": "LogFormer: A Pre-train and Tuning Pipeline for Log Anomaly Detection",
    "volume": "main",
    "abstract": "Log anomaly detection is a key component in the field of artificial intelligence for IT operations (AIOps). Considering log data of variant domains, retraining the whole network for unknown domains is inefficient in real industrial scenarios. However, previous deep models merely focused on extracting the semantics of log sequences in the same domain, leading to poor generalization on multi-domain logs. To alleviate this issue, we propose a unified Transformer-based framework for Log anomaly detection (LogFormer) to improve the generalization ability across different domains, where we establish a two-stage process including the pre-training and adapter-based tuning stage. Specifically, our model is first pre-trained on the source domain to obtain shared semantic knowledge of log data. Then, we transfer such knowledge to the target domain via shared parameters. Besides, the Log-Attention module is proposed to supplement the information ignored by the log-paring. The proposed method is evaluated on three public datasets and one real-world dataset. Experimental results on multiple benchmarks demonstrate the effectiveness of our LogFormer with fewer trainable parameters and lower training costs",
    "checked": true,
    "id": "c08a65e47b13c52744b6564e39c0e7c8f32a2074",
    "semantic_title": "logformer: a pre-train and tuning pipeline for log anomaly detection",
    "citation_count": 5,
    "authors": [
      "Hongcheng Guo",
      "Jian Yang",
      "Jiaheng Liu",
      "Jiaqi Bai",
      "Boyang Wang",
      "Zhoujun Li",
      "Tieqiao Zheng",
      "Bo Zhang",
      "Junran Peng",
      "Qi Tian"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27765": {
    "title": "ContraNovo: A Contrastive Learning Approach to Enhance De Novo Peptide Sequencing",
    "volume": "main",
    "abstract": "De novo peptide sequencing from mass spectrometry (MS) data is a critical task in proteomics research. Traditional de novo algorithms have encountered a bottleneck in accuracy due to the inherent complexity of proteomics data. While deep learning-based methods have shown progress, they reduce the problem to a translation task, potentially overlooking critical nuances between spectra and peptides. In our research, we present ContraNovo, a pioneering algorithm that leverages contrastive learning to extract the relationship between spectra and peptides and incorporates the mass information into peptide decoding, aiming to address these intricacies more efficiently. Through rigorous evaluations on two benchmark datasets, ContraNovo consistently outshines contemporary state-of-the-art solutions, underscoring its promising potential in enhancing de novo peptide sequencing",
    "checked": true,
    "id": "8c75eeee40678d237b421d0a86ac735519f12052",
    "semantic_title": "contranovo: a contrastive learning approach to enhance de novo peptide sequencing",
    "citation_count": 3,
    "authors": [
      "Zhi Jin",
      "Sheng Xu",
      "Xiang Zhang",
      "Tianze Ling",
      "Nanqing Dong",
      "Wanli Ouyang",
      "Zhiqiang Gao",
      "Cheng Chang",
      "Siqi Sun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27766": {
    "title": "Inducing Point Operator Transformer: A Flexible and Scalable Architecture for Solving PDEs",
    "volume": "main",
    "abstract": "Solving partial differential equations (PDEs) by learning the solution operators has emerged as an attractive alternative to traditional numerical methods. However, implementing such architectures presents two main challenges: flexibility in handling irregular and arbitrary input and output formats and scalability to large discretizations. Most existing architectures are limited by their desired structure or infeasible to scale large inputs and outputs. To address these issues, we introduce an attention-based model called an inducing point operator transformer (IPOT). Inspired by inducing points methods, IPOT is designed to handle any input function and output query while capturing global interactions in a computationally efficient way. By detaching the inputs/outputs discretizations from the processor with a smaller latent bottleneck, IPOT offers flexibility in processing arbitrary discretizations and scales linearly with the size of inputs/outputs. Our experimental results demonstrate that IPOT achieves strong performances with manageable computational complexity on an extensive range of PDE benchmarks and real-world weather forecasting scenarios, compared to state-of-the-art methods. Our code is publicly available at https://github.com/7tl7qns7ch/IPOT",
    "checked": true,
    "id": "746a06cd30f62841f9b5ff6d276bee5bb7171fc0",
    "semantic_title": "inducing point operator transformer: a flexible and scalable architecture for solving pdes",
    "citation_count": 1,
    "authors": [
      "Seungjun Lee",
      "TaeiL Oh"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27767": {
    "title": "MASTER: Market-Guided Stock Transformer for Stock Price Forecasting",
    "volume": "main",
    "abstract": "Stock price forecasting has remained an extremely challenging problem for many decades due to the high volatility of the stock market. Recent efforts have been devoted to modeling complex stock correlations toward joint stock price forecasting. Existing works share a common neural architecture that learns temporal patterns from individual stock series and then mixes up temporal representations to establish stock correlations. However, they only consider time-aligned stock correlations stemming from all the input stock features, which suffer from two limitations. First, stock correlations often occur momentarily and in a cross-time manner. Second, the feature effectiveness is dynamic with market variation, which affects both the stock sequential patterns and their correlations. To address the limitations, this paper introduces MASTER, a MArkert-guided Stock TransformER, which models the momentary and cross-time stock correlation and leverages market information for automatic feature selection. MASTER elegantly tackles the complex stock correlation by alternatively engaging in intra-stock and inter-stock information aggregation. Experiments show the superiority of MASTER compared with previous works and visualize the captured realistic stock correlation to provide valuable insights",
    "checked": true,
    "id": "39e25ace0b75852b33b2d34f9d99ed77f959060f",
    "semantic_title": "master: market-guided stock transformer for stock price forecasting",
    "citation_count": 0,
    "authors": [
      "Tong Li",
      "Zhaoyang Liu",
      "Yanyan Shen",
      "Xue Wang",
      "Haokun Chen",
      "Sen Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27768": {
    "title": "Learning from Polar Representation: An Extreme-Adaptive Model for Long-Term Time Series Forecasting",
    "volume": "main",
    "abstract": "In the hydrology field, time series forecasting is crucial for efficient water resource management, improving flood and drought control and increasing the safety and quality of life for the general population. However, predicting long-term streamflow is a complex task due to the presence of extreme events. It requires the capture of long-range dependencies and the modeling of rare but important extreme values. Existing approaches often struggle to tackle these dual challenges simultaneously. In this paper, we specifically delve into these issues and propose Distance-weighted Auto-regularized Neural network (DAN), a novel extreme-adaptive model for long-range forecasting of stremflow enhanced by polar representation learning. DAN utilizes a distance-weighted multi-loss mechanism and stackable blocks to dynamically refine indicator sequences from exogenous data, while also being able to handle uni-variate time-series by employing Gaussian Mixture probability modeling to improve robustness to severe events. We also introduce Kruskal-Wallis sampling and gate control vectors to handle imbalanced extreme data. On four real-life hydrologic streamflow datasets, we demonstrate that DAN significantly outperforms both state-of-the-art hydrologic time series prediction methods and general methods designed for long-term time series prediction",
    "checked": true,
    "id": "68f99f76e8d12a92a23a1fc2c489753b50aa481f",
    "semantic_title": "learning from polar representation: an extreme-adaptive model for long-term time series forecasting",
    "citation_count": 2,
    "authors": [
      "Yanhong Li",
      "Jack Xu",
      "David Anastasiu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27769": {
    "title": "The Causal Impact of Credit Lines on Spending Distributions",
    "volume": "main",
    "abstract": "Consumer credit services offered by electronic commerce platforms provide customers with convenient loan access during shopping and have the potential to stimulate sales. To understand the causal impact of credit lines on spending, previous studies have employed causal estimators, (e.g., direct regression (DR), inverse propensity weighting (IPW), and double machine learning (DML)) to estimate the treatment effect. However, these estimators do not treat the spending of each individual as a distribution that can capture the range and pattern of amounts spent across different orders. By disregarding the outcome as a distribution, valuable insights embedded within the outcome distribution might be overlooked. This paper thus develops distribution valued estimators which extend from existing real valued DR, IPW, and DML estimators within Rubin's causal framework. We establish their consistency and apply them to a real dataset from a large electronic commerce platform. Our findings reveal that credit lines generally have a positive impact on spending across all quantiles, but consumers would allocate more to luxuries (higher quantiles) than necessities (lower quantiles) as credit lines increase",
    "checked": true,
    "id": "779717183d8acf988605e7418f30a0708c455b6e",
    "semantic_title": "the causal impact of credit lines on spending distributions",
    "citation_count": 0,
    "authors": [
      "Yijun Li",
      "Cheuk Hang Leung",
      "Xiangqian Sun",
      "Chaoqun Wang",
      "Yiyan Huang",
      "Xing Yan",
      "Qi Wu",
      "Dongdong Wang",
      "Zhixiang Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27770": {
    "title": "Improving PTM Site Prediction by Coupling of Multi-Granularity Structure and Multi-Scale Sequence Representation",
    "volume": "main",
    "abstract": "Protein post-translational modification (PTM) site prediction is a fundamental task in bioinformatics. Several computational methods have been developed to predict PTM sites. However, existing methods ignore the structure information and merely utilize protein sequences. Furthermore, designing a more fine-grained structure representation learning method is urgently needed as PTM is a biological event that occurs at the atom granularity. In this paper, we propose a PTM site prediction method by Coupling of Multi-Granularity structure and Multi-Scale sequence representation, PTM-CMGMS for brevity. Specifically, multigranularity structure-aware representation learning is designed to learn neighborhood structure representations at the amino acid, atom, and whole protein granularity from AlphaFold predicted structures, followed by utilizing contrastive learning to optimize the structure representations. Additionally, multi-scale sequence representation learning is used to extract context sequence information, and motif generated by aligning all context sequences of PTM sites assists the prediction. Extensive experiments on three datasets show that PTM-CMGMS outperforms the state-of-the-art methods. Source code can be found at https://github.com/LZY-HZAU/PTM-CMGMS",
    "checked": true,
    "id": "a2a1041ddb2b46ef315dcb3658981fdf6b5ab242",
    "semantic_title": "improving ptm site prediction by coupling of multi-granularity structure and multi-scale sequence representation",
    "citation_count": 0,
    "authors": [
      "Zhengyi Li",
      "Menglu Li",
      "Lida Zhu",
      "Wen Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27771": {
    "title": "Joint Learning Neuronal Skeleton and Brain Circuit Topology with Permutation Invariant Encoders for Neuron Classification",
    "volume": "main",
    "abstract": "Determining the types of neurons within a nervous system plays a significant role in the analysis of brain connectomics and the investigation of neurological diseases. However, the efficiency of utilizing anatomical, physiological, or molecular characteristics of neurons is relatively low and costly. With the advancements in electron microscopy imaging and analysis techniques for brain tissue, we are able to obtain whole-brain connectome consisting neuronal high-resolution morphology and connectivity information. However, few models are built based on such data for automated neuron classification. In this paper, we propose NeuNet, a framework that combines morphological information of neurons obtained from skeleton and topological information between neurons obtained from neural circuit. Specifically, NeuNet consists of three components, namely Skeleton Encoder, Connectome Encoder, and Readout Layer. Skeleton Encoder integrates the local information of neurons in a bottom-up manner, with a one-dimensional convolution in neural skeleton's point data; Connectome Encoder uses a graph neural network to capture the topological information of neural circuit; finally, Readout Layer fuses the above two information and outputs classification results. We reprocess and release two new datasets for neuron classification task from volume electron microscopy(VEM) images of human brain cortex and Drosophila brain. Experiments on these two datasets demonstrated the effectiveness of our model with accuracies of 0.9169 and 0.9363, respectively. Code and data are available at: https://github.com/WHUminghui/NeuNet",
    "checked": true,
    "id": "4e514969bdf8fe28825d7497eedd562f37ba866b",
    "semantic_title": "joint learning neuronal skeleton and brain circuit topology with permutation invariant encoders for neuron classification",
    "citation_count": 0,
    "authors": [
      "Minghui Liao",
      "Guojia Wan",
      "Bo Du"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27772": {
    "title": "Root Cause Analysis in Microservice Using Neural Granger Causal Discovery",
    "volume": "main",
    "abstract": "In recent years, microservices have gained widespread adoption in IT operations due to their scalability, maintenance, and flexibility. However, it becomes challenging for site reliability engineers (SREs) to pinpoint the root cause due to the complex relationship in microservices when facing system malfunctions. Previous research employed structure learning methods (e.g., PC-algorithm) to establish causal relationships and derive root causes from causal graphs. Nevertheless, they ignored the temporal order of time series data and failed to leverage the rich information inherent in the temporal relationships. For instance, in cases where there is a sudden spike in CPU utilization, it can lead to an increase in latency for other microservices. However, in this scenario, the anomaly in CPU utilization occurs before the latency increases, rather than simultaneously. As a result, the PC-algorithm fails to capture such characteristics. To address these challenges, we propose RUN, a novel approach for root cause analysis using neural Granger causal discovery with contrastive learning. RUN enhances the backbone encoder by integrating contextual information from time series and leverages a time series forecasting model to conduct neural Granger causal discovery. In addition, RUN incorporates Pagerank with a personalization vector to efficiently recommend the top-k root causes. Extensive experiments conducted on the synthetic and real-world microservice-based datasets demonstrate that RUN noticeably outperforms the state-of-the-art root cause analysis methods. Moreover, we provide an analysis scenario for the sock-shop case to showcase the practicality and efficacy of RUN in microservice-based applications. Our code is publicly available at https://github.com/zmlin1998/RUN",
    "checked": true,
    "id": "6c21d49b193b59f84168d02f86db8b8c4c3290a7",
    "semantic_title": "root cause analysis in microservice using neural granger causal discovery",
    "citation_count": 2,
    "authors": [
      "Cheng-Ming Lin",
      "Ching Chang",
      "Wei-Yao Wang",
      "Kuang-Da Wang",
      "Wen-Chih Peng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27773": {
    "title": "Model-Driven Deep Neural Network for Enhanced AoA Estimation Using 5G gNB",
    "volume": "main",
    "abstract": "High-accuracy positioning has become a fundamental enabler for intelligent connected devices. Nevertheless, the present wireless networks still rely on model-driven approaches to achieve positioning functionality, which are susceptible to performance degradation in practical scenarios, primarily due to hardware impairments. Integrating artificial intelligence into the positioning framework presents a promising solution to revolutionize the accuracy and robustness of location-based services. In this study, we address this challenge by reformulating the problem of angle-of-arrival (AoA) estimation into image reconstruction of spatial spectrum. To this end, we design a model-driven deep neural network (MoD-DNN), which can automatically calibrate the angular-dependent phase error. The proposed MoD-DNN approach employs an iterative optimization scheme between a convolutional neural network and a sparse conjugate gradient algorithm. Simulation and experimental results are presented to demonstrate the effectiveness of the proposed method in enhancing spectrum calibration and AoA estimation",
    "checked": true,
    "id": "3ce02e3d620a5c2c3c94d20a9a344d380e4be86c",
    "semantic_title": "model-driven deep neural network for enhanced aoa estimation using 5g gnb",
    "citation_count": 1,
    "authors": [
      "Shengheng Liu",
      "Xingkang Li",
      "Zihuan Mao",
      "Peng Liu",
      "Yongming Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27774": {
    "title": "MID-FiLD: MIDI Dataset for Fine-Level Dynamics",
    "volume": "main",
    "abstract": "One of the challenges in generating human-like music is articulating musical expressions such as dynamics, phrasing, and timbre, which are difficult for computational models to mimic. Previous efforts to tackle this problem have been insufficient due to a fundamental lack of data containing information about musical expressions. In this paper, we introduce MID-FiLD, a MIDI dataset for learning fine-level dynamics control. Notable properties of MID-FiLD are as follows: (1) All 4,422 MIDI samples are constructed by professional music writers with a strong understanding of composition and musical expression. (2) Each MIDI sample contains four different musical metadata and control change \\#1 (CC\\#1) value. We verify that our metadata is a key factor in MID-FiLD, exerting a substantial influence over produced CC\\#1 values. In addition, we demonstrate the applicability of MID-FiLD to deep learning models by suggesting a token-based encoding methodology and reveal the potential for generating controllable, human-like musical expressions",
    "checked": true,
    "id": "b728b5b082a6a615001f23a993051cca7a7d39ba",
    "semantic_title": "mid-fild: midi dataset for fine-level dynamics",
    "citation_count": 0,
    "authors": [
      "Jesung Ryu",
      "Seungyeon Rhyu",
      "Hong-Gyu Yoon",
      "Eunchong Kim",
      "Ju Young Yang",
      "Taehyun Kim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27775": {
    "title": "PosDiffNet: Positional Neural Diffusion for Point Cloud Registration in a Large Field of View with Perturbations",
    "volume": "main",
    "abstract": "Point cloud registration is a crucial technique in 3D computer vision with a wide range of applications. However, this task can be challenging, particularly in large fields of view with dynamic objects, environmental noise, or other perturbations. To address this challenge, we propose a model called PosDiffNet. Our approach performs hierarchical registration based on window-level, patch-level, and point-level correspondence. We leverage a graph neural partial differential equation (PDE) based on Beltrami flow to obtain high-dimensional features and position embeddings for point clouds. We incorporate position embeddings into a Transformer module based on a neural ordinary differential equation (ODE) to efficiently represent patches within points. We employ the multi-level correspondence derived from the high feature similarity scores to facilitate alignment between point clouds. Subsequently, we use registration methods such as SVD-based algorithms to predict the transformation using corresponding point pairs. We evaluate PosDiffNet on several 3D point cloud datasets, verifying that it achieves state-of-the-art (SOTA) performance for point cloud registration in large fields of view with perturbations. The implementation code of experiments is available at https://github.com/AI-IT-AVs/PosDiffNet",
    "checked": true,
    "id": "f2b0748a078578c89e6d34c681c494960adf9200",
    "semantic_title": "posdiffnet: positional neural diffusion for point cloud registration in a large field of view with perturbations",
    "citation_count": 1,
    "authors": [
      "Rui She",
      "Sijie Wang",
      "Qiyu Kang",
      "Kai Zhao",
      "Yang Song",
      "Wee Peng Tay",
      "Tianyu Geng",
      "Xingchao Jian"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27776": {
    "title": "StegaStyleGAN: Towards Generic and Practical Generative Image Steganography",
    "volume": "main",
    "abstract": "The recent advances in generative image steganography have drawn increasing attention due to their potential for provable security and bulk embedding capacity. However, existing generative steganographic schemes are usually tailored for specific tasks and are hardly applied to applications with practical constraints. To address this issue, this paper proposes a generic generative image steganography scheme called Steganography StyleGAN (StegaStyleGAN) that meets the practical objectives of security, capacity, and robustness within the same framework. In StegaStyleGAN, a novel Distribution-Preserving Secret Data Modulator (DP-SDM) is used to achieve provably secure generative image steganography by preserving the data distribution of the model inputs. Additionally, a generic and efficient Secret Data Extractor (SDE) is invented for accurate secret data extraction. By choosing whether to incorporate the Image Attack Simulator (IAS) during the training process, one can obtain two models with different parameters but the same structure (both generator and extractor) for lossless and lossy channel covert communication, namely StegaStyleGAN-Ls and StegaStyleGAN-Ly. Furthermore, by mating with GAN inversion, conditional generative steganography can be achieved as well. Experimental results demonstrate that, whether for lossless or lossy communication channels, the proposed StegaStyleGAN can significantly outperform the corresponding state-of-the-art schemes",
    "checked": true,
    "id": "252144c3d461ee85eda8327f6d8a3bd2562e8ee4",
    "semantic_title": "stegastylegan: towards generic and practical generative image steganography",
    "citation_count": 2,
    "authors": [
      "Wenkang Su",
      "Jiangqun Ni",
      "Yiyan Sun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27777": {
    "title": "Dual-Channel Learning Framework for Drug-Drug Interaction Prediction via Relation-Aware Heterogeneous Graph Transformer",
    "volume": "main",
    "abstract": "Identifying novel drug-drug interactions (DDIs) is a crucial task in pharmacology, as the interference between pharmacological substances can pose serious medical risks. In recent years, several network-based techniques have emerged for predicting DDIs. However, they primarily focus on local structures within DDI-related networks, often overlooking the significance of indirect connections between pairwise drug nodes from a global perspective. Additionally, effectively handling heterogeneous information present in both biomedical knowledge graphs and drug molecular graphs remains a challenge for improved performance of DDI prediction. To address these limitations, we propose a Transformer-based relatIon-aware Graph rEpresentation leaRning framework (TIGER) for DDI prediction. TIGER leverages the Transformer architecture to effectively exploit the structure of heterogeneous graph, which allows it direct learning of long dependencies and high-order structures. Furthermore, TIGER incorporates a relation-aware self-attention mechanism, capturing a diverse range of semantic relations that exist between pairs of nodes in heterogeneous graph. In addition to these advancements, TIGER enhances predictive accuracy by modeling DDI prediction task using a dual-channel network, where drug molecular graph and biomedical knowledge graph are fed into two respective channels. By incorporating embeddings obtained at graph and node levels, TIGER can benefit from structural properties of drugs as well as rich contextual information provided by biomedical knowledge graph. Extensive experiments conducted on three real-world datasets demonstrate the effectiveness of TIGER in DDI prediction. Furthermore, case studies highlight its ability to provide a deeper understanding of underlying mechanisms of DDIs",
    "checked": true,
    "id": "89e5bf14cc692c44e1bb58c31d4ed7ad5f72ce5e",
    "semantic_title": "dual-channel learning framework for drug-drug interaction prediction via relation-aware heterogeneous graph transformer",
    "citation_count": 0,
    "authors": [
      "Xiaorui Su",
      "Pengwei Hu",
      "Zhu-Hong You",
      "Philip S. Yu",
      "Lun Hu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27778": {
    "title": "Molecular Optimization Model with Patentability Constraint",
    "volume": "main",
    "abstract": "In drug development, molecular optimization is a crucial challenge that involves generating novel molecules given a lead molecule as input. The task requires maintaining molecular similarity to the original molecule while simultaneously optimizing multiple chemical attributes. To aid in this process, numerous generative models have been proposed. However, in practical applications, it is crucial for these models not only to generate novel molecules with the above constraints but also to generate molecules that significantly differ from any existing patented compounds. In this work, we present a multi-optimization molecular framework to address this challenge. Our framework trains a model to prioritize both enhanced properties and substantial dissimilarity from patented compounds. By jointly learning continuous representations of optimized and patentable molecules, we ensure that the generated molecules are significantly distant from any patented compounds while improving chemical properties. Through empirical evaluation, we demonstrate the superior performance of our approach compared to state-of-the-art molecular optimization methods both in chemical property optimization and patentability",
    "checked": true,
    "id": "6be7d6083e11f5c6b105d78b349998fc4c536af9",
    "semantic_title": "molecular optimization model with patentability constraint",
    "citation_count": 0,
    "authors": [
      "Sally Turutov",
      "Kira Radinsky"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27779": {
    "title": "Generalizable Sleep Staging via Multi-Level Domain Alignment",
    "volume": "main",
    "abstract": "Automatic sleep staging is essential for sleep assessment and disorder diagnosis. Most existing methods depend on one specific dataset and are limited to be generalized to other unseen datasets, for which the training data and testing data are from the same dataset. In this paper, we introduce domain generalization into automatic sleep staging and propose the task of generalizable sleep staging which aims to improve the model generalization ability to unseen datasets. Inspired by existing domain generalization methods, we adopt the feature alignment idea and propose a framework called SleepDG to solve it. Considering both of local salient features and sequential features are important for sleep staging, we propose a Multi-level Feature Alignment combining epoch-level and sequence-level feature alignment to learn domain-invariant feature representations. Specifically, we design an Epoch-level Feature Alignment to align the feature distribution of each single sleep epoch among different domains, and a Sequence-level Feature Alignment to minimize the discrepancy of sequential features among different domains. SleepDG is validated on five public datasets, achieving the state-of-the-art performance",
    "checked": true,
    "id": "286804dedf5a8d954b1858f39f67ec9ac8a138e0",
    "semantic_title": "generalizable sleep staging via multi-level domain alignment",
    "citation_count": 3,
    "authors": [
      "Jiquan Wang",
      "Sha Zhao",
      "Haiteng Jiang",
      "Shijian Li",
      "Tao Li",
      "Gang Pan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27780": {
    "title": "Inspecting Prediction Confidence for Detecting Black-Box Backdoor Attacks",
    "volume": "main",
    "abstract": "Backdoor attacks have been shown to be a serious security threat against deep learning models, and various defenses have been proposed to detect whether a model is backdoored or not. However, as indicated by a recent black-box attack, existing defenses can be easily bypassed by implanting the backdoor in the frequency domain. To this end, we propose a new defense DTInspector against black-box backdoor attacks, based on a new observation related to the prediction confidence of learning models. That is, to achieve a high attack success rate with a small amount of poisoned data, backdoor attacks usually render a model exhibiting statistically higher prediction confidences on the poisoned samples. We provide both theoretical and empirical evidence for the generality of this observation. DTInspector then carefully examines the prediction confidences of data samples, and decides the existence of backdoor using the shortcut nature of backdoor triggers. Extensive evaluations on six backdoor attacks, four datasets, and three advanced attacking types demonstrate the effectiveness of the proposed defense",
    "checked": true,
    "id": "8675b3c6725d148a72c61c59d890b0624941cfda",
    "semantic_title": "inspecting prediction confidence for detecting black-box backdoor attacks",
    "citation_count": 2,
    "authors": [
      "Tong Wang",
      "Yuan Yao",
      "Feng Xu",
      "Miao Xu",
      "Shengwei An",
      "Ting Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27781": {
    "title": "Conformal Crystal Graph Transformer with Robust Encoding of Periodic Invariance",
    "volume": "main",
    "abstract": "Machine learning techniques, especially in the realm of materials design, hold immense promise in predicting the properties of crystal materials and aiding in the discovery of novel crystals with desirable traits. However, crystals possess unique geometric constraints—namely, E(3) invariance for primitive cell and periodic invariance—which need to be accurately reflected in crystal representations. Though past research has explored various construction techniques to preserve periodic invariance in crystal representations, their robustness remains inadequate. Furthermore, effectively capturing angular information within 3D crystal structures continues to pose a significant challenge for graph-based approaches. This study introduces novel solutions to these challenges. We first present a graph construction method that robustly encodes periodic invariance and a strategy to capture angular information in neural networks without compromising efficiency. We further introduce CrystalFormer, a pioneering graph transformer architecture that emphasizes angle preservation and enhances long-range information. Through comprehensive evaluation, we verify our model's superior performance in 5 crystal prediction tasks, reaffirming the efficiency of our proposed methods",
    "checked": true,
    "id": "602e4b2301d49f5cbe310b64381ccca81cbc8236",
    "semantic_title": "conformal crystal graph transformer with robust encoding of periodic invariance",
    "citation_count": 0,
    "authors": [
      "Yingheng Wang",
      "Shufeng Kong",
      "John M. Gregoire",
      "Carla P. Gomes"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27782": {
    "title": "SuperJunction: Learning-Based Junction Detection for Retinal Image Registration",
    "volume": "main",
    "abstract": "Keypoints-based approaches have shown to be promising for retinal image registration, which superimpose two or more images from different views based on keypoint detection and description. However, existing approaches suffer from ineffective keypoint detector and descriptor training. Meanwhile, the non-linear mapping from 3D retinal structure to 2D images is often neglected. In this paper, we propose a novel learning-based junction detection approach for retinal image registration, which enhances both the keypoint detector and descriptor training. To improve the keypoint detection, it uses a multi-task vessel detection to regularize the model training, which helps to learn more representative features and reduce the risk of over-fitting. To achieve effective training for keypoints description, a new constrained negative sampling approach is proposed to compute the descriptor loss. Moreover, we also consider the non-linearity between retinal images from different views during matching. Experimental results on FIRE dataset show that our method achieves mean area under curve of 0.850, which is 12.6% higher than 0.755 by the state-of-the-art method. All the codes are available at https://github.com/samjcheng/SuperJunction",
    "checked": true,
    "id": "751cddda211f515ea023476f462efffa71af574b",
    "semantic_title": "superjunction: learning-based junction detection for retinal image registration",
    "citation_count": 0,
    "authors": [
      "Yu Wang",
      "Xiaoye Wang",
      "Zaiwang Gu",
      "Weide Liu",
      "Wee Siong Ng",
      "Weimin Huang",
      "Jun Cheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27783": {
    "title": "Explore 3D Dance Generation via Reward Model from Automatically-Ranked Demonstrations",
    "volume": "main",
    "abstract": "This paper presents an Exploratory 3D Dance generation framework, E3D2, designed to address the exploration capability deficiency in existing music-conditioned 3D dance generation models. Current models often generate monotonous and simplistic dance sequences that misalign with human preferences because they lack exploration capabilities.The E3D2 framework involves a reward model trained from automatically-ranked dance demonstrations, which then guides the reinforcement learning process. This approach encourages the agent to explore and generate high quality and diverse dance movement sequences. The soundness of the reward model is both theoretically and experimentally validated. Empirical experiments demonstrate the effectiveness of E3D2 on the AIST++ dataset",
    "checked": true,
    "id": "fc877df6237b38ed90aa022a1b373c64c21dbd0c",
    "semantic_title": "explore 3d dance generation via reward model from automatically-ranked demonstrations",
    "citation_count": 2,
    "authors": [
      "Zilin Wang",
      "Haolin Zhuang",
      "Lu Li",
      "Yinmin Zhang",
      "Junjie Zhong",
      "Jun Chen",
      "Yu Yang",
      "Boshi Tang",
      "Zhiyong Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27784": {
    "title": "PSC-CPI: Multi-Scale Protein Sequence-Structure Contrasting for Efficient and Generalizable Compound-Protein Interaction Prediction",
    "volume": "main",
    "abstract": "Compound-Protein Interaction (CPI) prediction aims to predict the pattern and strength of compound-protein interactions for rational drug discovery. Existing deep learning-based methods utilize only the single modality of protein sequences or structures and lack the co-modeling of the joint distribution of the two modalities, which may lead to significant performance drops in complex real-world scenarios due to various factors, e.g., modality missing and domain shifting. More importantly, these methods only model protein sequences and structures at a single fixed scale, neglecting more fine-grained multi-scale information, such as those embedded in key protein fragments. In this paper, we propose a novel multi-scale Protein Sequence-structure Contrasting framework for CPI prediction (PSC-CPI), which captures the dependencies between protein sequences and structures through both intra-modality and cross-modality contrasting. We further apply length-variable protein augmentation to allow contrasting to be performed at different scales, from the amino acid level to the sequence level. Finally, in order to more fairly evaluate the model generalizability, we split the test data into four settings based on whether compounds and proteins have been observed during the training stage. Extensive experiments have shown that PSC-CPI generalizes well in all four settings, particularly in the more challenging ``Unseen-Both\" setting, where neither compounds nor proteins have been observed during training. Furthermore, even when encountering a situation of modality missing, i.e., inference with only single-modality protein data, PSC-CPI still exhibits comparable or even better performance than previous approaches",
    "checked": true,
    "id": "a97a09ed17da5aa3499cb5b72a6b7b1ecbd7a0f9",
    "semantic_title": "psc-cpi: multi-scale protein sequence-structure contrasting for efficient and generalizable compound-protein interaction prediction",
    "citation_count": 7,
    "authors": [
      "Lirong Wu",
      "Yufei Huang",
      "Cheng Tan",
      "Zhangyang Gao",
      "Bozhen Hu",
      "Haitao Lin",
      "Zicheng Liu",
      "Stan Z. Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27785": {
    "title": "Uncertainty Quantification for Forward and Inverse Problems of PDEs via Latent Global Evolution",
    "volume": "main",
    "abstract": "Deep learning-based surrogate models have demonstrated remarkable advantages over classical solvers in terms of speed, often achieving speedups of 10 to 1000 times over traditional partial differential equation (PDE) solvers. However, a significant challenge hindering their widespread adoption in both scientific and industrial domains is the lack of understanding about their prediction uncertainties, particularly in scenarios that involve critical decision making. To address this limitation, we propose a method that integrates efficient and precise uncertainty quantification into a deep learning-based surrogate model. Our method, termed Latent Evolution of PDEs with Uncertainty Quantification (LE-PDE-UQ), endows deep learning-based surrogate models with robust and efficient uncertainty quantification capabilities for both forward and inverse problems. LE-PDE-UQ leverages latent vectors within a latent space to evolve both the system's state and its corresponding uncertainty estimation. The latent vectors are decoded to provide predictions for the system's state as well as estimates of its uncertainty. In extensive experiments, we demonstrate the accurate uncertainty quantification performance of our approach, surpassing that of strong baselines including deep ensembles, Bayesian neural network layers, and dropout. Our method excels at propagating uncertainty over extended auto-regressive rollouts, making it suitable for scenarios involving long-term predictions. Our code is available at: https://github.com/AI4Science-WestlakeU/le-pde-uq",
    "checked": true,
    "id": "6f97392e270b594cbc3e8e1b28df9f5df3644017",
    "semantic_title": "uncertainty quantification for forward and inverse problems of pdes via latent global evolution",
    "citation_count": 0,
    "authors": [
      "Tailin Wu",
      "Willie Neiswanger",
      "Hongtao Zheng",
      "Stefano Ermon",
      "Jure Leskovec"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27786": {
    "title": "Multilevel Attention Network with Semi-supervised Domain Adaptation for Drug-Target Prediction",
    "volume": "main",
    "abstract": "Prediction of drug-target interactions (DTIs) is a crucial step in drug discovery, and deep learning methods have shown great promise on various DTI datasets. However, existing approaches still face several challenges, including limited labeled data, hidden bias issue, and a lack of generalization ability to out-of-domain data. These challenges hinder the model's capacity to learn truly informative interaction features, leading to shortcut learning and inferior predictive performance on novel drug-target pairs. To address these issues, we propose MlanDTI, a semi-supervised domain adaptive multilevel attention network (Mlan) for DTI prediction. We utilize two pre-trained BERT models to acquire bidirectional representations enriched with information from unlabeled data. Then, we introduce a multilevel attention mechanism, enabling the model to learn domain-invariant DTIs at different hierarchical levels. Moreover, we present a simple yet effective semi-supervised pseudo-labeling method to further enhance our model's predictive ability in cross-domain scenarios. Experiments on four datasets show that MlanDTI achieves state-of-the-art performances over other methods under intra-domain settings and outperforms all other approaches under cross-domain settings. The source code is available at https://github.com/CMACH508/MlanDTI",
    "checked": true,
    "id": "da523e004d5375958f0c7bb4b2559aa7e43dd84c",
    "semantic_title": "multilevel attention network with semi-supervised domain adaptation for drug-target prediction",
    "citation_count": 0,
    "authors": [
      "Zhousan Xie",
      "Shikui Tu",
      "Lei Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27787": {
    "title": "Geometric-Facilitated Denoising Diffusion Model for 3D Molecule Generation",
    "volume": "main",
    "abstract": "Denoising diffusion models have shown great potential in multiple research areas. Existing diffusion-based generative methods on de novo 3D molecule generation face two major challenges. Since majority heavy atoms in molecules allow connections to multiple atoms through single bonds, solely using pair-wise distance to model molecule geometries is insufficient. Therefore, the first one involves proposing an effective neural network as the denoising kernel that is capable to capture complex multi-body interatomic relationships and learn high-quality features. Due to the discrete nature of graphs, mainstream diffusion-based methods for molecules heavily rely on predefined rules and generate edges in an indirect manner. The second challenge involves accommodating molecule generation to diffusion and accurately predicting the existence of bonds. In our research, we view the iterative way of updating molecule conformations in diffusion process is consistent with molecular dynamics and introduce a novel molecule generation method named Geometric-Facilitated Molecular Diffusion (GFMDiff). For the first challenge, we introduce a Dual-track Transformer Network (DTN) to fully excevate global spatial relationships and learn high quality representations which contribute to accurate predictions of features and geometries. As for the second challenge, we design Geometric-facilitated Loss (GFLoss) which intervenes the formation of bonds during the training period, instead of directly embedding edges into the latent space. Comprehensive experiments on current benchmarks demonstrate the superiority of GFMDiff",
    "checked": true,
    "id": "cb5af772ea66ff572db4a35c0dc86b26d81acf4d",
    "semantic_title": "geometric-facilitated denoising diffusion model for 3d molecule generation",
    "citation_count": 3,
    "authors": [
      "Can Xu",
      "Haosen Wang",
      "Weigang Wang",
      "Pengfei Zheng",
      "Hongyang Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27788": {
    "title": "GAMC: An Unsupervised Method for Fake News Detection Using Graph Autoencoder with Masking",
    "volume": "main",
    "abstract": "With the rise of social media, the spread of fake news has become a significant concern, potentially misleading public perceptions and impacting social stability. Although deep learning methods like CNNs, RNNs, and Transformer-based models like BERT have enhanced fake news detection. However, they primarily focus on content and do not consider social context during news propagation. Graph-based techniques have incorporated the social context but are limited by the need for large labeled datasets. To address these challenges, this paper introduces GAMC, an unsupervised fake news detection technique using the Graph Autoencoder with Masking and Contrastive learning. By leveraging both the context and content of news propagation as self-supervised signals, our method reduces the dependency on labeled datasets. Specifically, GAMC begins by applying data augmentation to the original news propagation graphs. Subsequently, these augmented graphs are encoded using a graph encoder and subsequently reconstructed via a graph decoder. Finally, a composite loss function that encompasses both reconstruction error and contrastive loss is designed. Firstly, it ensures the model can effectively capture the latent features, based on minimizing the discrepancy between reconstructed and original graph representations. Secondly, it aligns the representations of augmented graphs that originate from the same source. Experiments on the real-world dataset validate the effectiveness of our method",
    "checked": true,
    "id": "c2c92a8e58aa8fdf5f1b76c4dfa5e15024d70b54",
    "semantic_title": "gamc: an unsupervised method for fake news detection using graph autoencoder with masking",
    "citation_count": 1,
    "authors": [
      "Shu Yin",
      "Peican Zhu",
      "Lianwei Wu",
      "Chao Gao",
      "Zhen Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27789": {
    "title": "Unsupervised Gene-Cell Collective Representation Learning with Optimal Transport",
    "volume": "main",
    "abstract": "Cell type identification plays a vital role in single-cell RNA sequencing (scRNA-seq) data analysis. Although many deep embedded methods to cluster scRNA-seq data have been proposed, they still fail in elucidating the intrinsic properties of cells and genes. Here, we present a novel end-to-end deep graph clustering model for single-cell transcriptomics data based on unsupervised Gene-Cell Collective representation learning and Optimal Transport (scGCOT) which integrates both cell and gene correlations. Specifically, scGCOT learns the latent embedding of cells and genes simultaneously and reconstructs the cell graph, the gene graph, and the gene expression count matrix. A zero-inflated negative binomial (ZINB) model is estimated via the reconstructed count matrix to capture the essential properties of scRNA-seq data. By leveraging the optimal transport-based joint representation alignment, scGCOT learns the clustering process and the latent representations through a mutually supervised self optimization strategy. Extensive experiments with 14 competing methods on 15 real scRNA-seq datasets demonstrate the competitive edges of scGCOT",
    "checked": true,
    "id": "3eff0e0d38e0be0bff0b536abc4718142f84d3df",
    "semantic_title": "unsupervised gene-cell collective representation learning with optimal transport",
    "citation_count": 0,
    "authors": [
      "Jixiang Yu",
      "Nanjun Chen",
      "Ming Gao",
      "Xiangtao Li",
      "Ka-Chun  Wong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27790": {
    "title": "MCSSME: Multi-Task Contrastive Learning for Semi-supervised Singing Melody Extraction from Polyphonic Music",
    "volume": "main",
    "abstract": "Singing melody extraction is an important task in the field of music information retrieval (MIR). The development of data-driven models for this task have achieved great successes. However, the existing models have two major limitations: firstly, most of the existing singing melody extraction models have formulated this task as a pixel-level prediction task. The lack of labeling data has limited the model for further improvements. Secondly, the generalization of the existing models are prone to be disturbed by the music genres. To address the issues mentioned above, in this paper, we propose a multi-Task contrastive learning framework for semi-supervised singing melody extraction, termed as MCSSME. Specifically, to deal with data scarcity limitation, we propose a self-consistency regularization (SCR) method to train the model on the unlabeled data. Transformations are applied to the raw signal of polyphonic music, which makes the network to improve its representation capability via recognizing the transformations. We further propose a novel multi-task learning (MTL) approach to jointly learn singing melody extraction and classification of transformed data. To deal with generalization limitation, we also propose a contrastive embedding learning, which strengthens the intra-class compactness and inter-class separability. To improve the generalization on different music genres, we also propose a domain classification method to learn task-dependent features by mapping data from different music genres to shared subspace. MCSSME evaluates on a set of well-known public melody extraction datasets with promising performances. The experimental results demonstrate the effectiveness of the MCSSME framework for singing melody extraction from polyphonic music using very limited labeled data scenarios",
    "checked": true,
    "id": "51dfe652c6fc8120c38e01c3ac62ec5fad6be60a",
    "semantic_title": "mcssme: multi-task contrastive learning for semi-supervised singing melody extraction from polyphonic music",
    "citation_count": 0,
    "authors": [
      "Shuai Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27791": {
    "title": "RetroOOD: Understanding Out-of-Distribution Generalization in Retrosynthesis Prediction",
    "volume": "main",
    "abstract": "Machine learning-assisted retrosynthesis prediction models have been gaining widespread adoption, though their performances oftentimes degrade significantly when deployed in real-world applications embracing out-of-distribution (OOD) molecules or reactions. Despite steady progress on standard benchmarks, our understanding of existing retrosynthesis prediction models under the premise of distribution shifts remains stagnant. To this end, we first formally sort out two types of distribution shifts in retrosynthesis prediction and construct two groups of benchmark datasets. Next, through comprehensive experiments, we systematically compare state-of-the-art retrosynthesis prediction models on the two groups of benchmarks, revealing the limitations of previous in-distribution evaluation and re-examining the advantages of each model. More remarkably, we are motivated by the above empirical insights to propose two model-agnostic techniques that can improve the OOD generalization of arbitrary off-the-shelf retrosynthesis prediction algorithms. Our preliminary experiments show their high potential with an average performance improvement of 4.6%, and the established benchmarks serve as a foothold for further retrosynthesis prediction research towards OOD generalization",
    "checked": true,
    "id": "9947402dae7238243f29496fc00451606eb151c7",
    "semantic_title": "retroood: understanding out-of-distribution generalization in retrosynthesis prediction",
    "citation_count": 1,
    "authors": [
      "Yemin Yu",
      "Luotian Yuan",
      "Ying Wei",
      "Hanyu Gao",
      "Fei Wu",
      "Zhihua Wang",
      "Xinhai Ye"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27792": {
    "title": "Designing Biological Sequences without Prior Knowledge Using Evolutionary Reinforcement Learning",
    "volume": "main",
    "abstract": "Designing novel biological sequences with desired properties is a significant challenge in biological science because of the extra large search space. The traditional design process usually involves multiple rounds of costly wet lab evaluations. To reduce the need for expensive wet lab experiments, machine learning methods are used to aid in designing biological sequences. However, the limited availability of biological sequences with known properties hinders the training of machine learning models, significantly restricting their applicability and performance. To fill this gap, we present ERLBioSeq, an Evolutionary Reinforcement Learning algorithm for BIOlogical SEQuence design. ERLBioSeq leverages the capability of reinforcement learning to learn without prior knowledge and the potential of evolutionary algorithms to enhance the exploration of reinforcement learning in the large search space of biological sequences. Additionally, to enhance the efficiency of biological sequence design, we developed a predictor for sequence screening in the biological sequence design process, which incorporates both the local and global sequence information. We evaluated the proposed method on three main types of biological sequence design tasks, including the design of DNA, RNA, and protein. The results demonstrate that the proposed method achieves significant improvement compared to the existing state-of-the-art methods",
    "checked": true,
    "id": "a13fbf12c13cf217db50252c4a8b959e734ff377",
    "semantic_title": "designing biological sequences without prior knowledge using evolutionary reinforcement learning",
    "citation_count": 0,
    "authors": [
      "Xi Zeng",
      "Xiaotian Hao",
      "Hongyao Tang",
      "Zhentao Tang",
      "Shaoqing Jiao",
      "Dazhi Lu",
      "Jiajie Peng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27793": {
    "title": "Adversarial Socialbots Modeling Based on Structural Information Principles",
    "volume": "main",
    "abstract": "The importance of effective detection is underscored by the fact that socialbots imitate human behavior to propagate misinformation, leading to an ongoing competition between socialbots and detectors. Despite the rapid advancement of reactive detectors, the exploration of adversarial socialbot modeling remains incomplete, significantly hindering the development of proactive detectors. To address this issue, we propose a mathematical Structural Information principles-based Adversarial Socialbots Modeling framework, namely SIASM, to enable more accurate and effective modeling of adversarial behaviors. First, a heterogeneous graph is presented to integrate various users and rich activities in the original social network and measure its dynamic uncertainty as structural entropy. By minimizing the high-dimensional structural entropy, a hierarchical community structure of the social network is generated and referred to as the optimal encoding tree. Secondly, a novel method is designed to quantify influence by utilizing the assigned structural entropy, which helps reduce the computational cost of SIASM by filtering out uninfluential users. Besides, a new conditional structural entropy is defined between the socialbot and other users to guide the follower selection for network influence maximization. Extensive and comparative experiments on both homogeneous and heterogeneous social networks demonstrate that, compared with state-of-the-art baselines, the proposed SIASM framework yields substantial performance improvements in terms of network influence (up to 16.32%) and sustainable stealthiness (up to 16.29%) when evaluated against a robust detector with 90% accuracy",
    "checked": true,
    "id": "3c05e98a77a29fd880b7ff998c34cfe7cc34a721",
    "semantic_title": "adversarial socialbots modeling based on structural information principles",
    "citation_count": 4,
    "authors": [
      "Xianghua Zeng",
      "Hao Peng",
      "Angsheng Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27794": {
    "title": "NondBREM: Nondeterministic Offline Reinforcement Learning for Large-Scale Order Dispatching",
    "volume": "main",
    "abstract": "One of the most important tasks in ride-hailing is order dispatching, i.e., assigning unserved orders to available drivers. Recent order dispatching has achieved a significant improvement due to the advance of reinforcement learning, which has been approved to be able to effectively address sequential decision-making problems like order dispatching. However, most existing reinforcement learning methods require agents to learn the optimal policy by interacting with environments online, which is challenging or impractical for real-world deployment due to high costs or safety concerns. For example, due to the spatiotemporally unbalanced supply and demand, online reinforcement learning-based order dispatching may significantly impact the revenue of the ride-hailing platform and passenger experience during the policy learning period. Hence, in this work, we develop an offline deep reinforcement learning framework called NondBREM for large-scale order dispatching, which learns policy from only the accumulated logged data to avoid costly and unsafe interactions with the environment. In NondBREM, a Nondeterministic Batch-Constrained Q-learning (NondBCQ) module is developed to reduce the algorithm extrapolation error and a Random Ensemble Mixture (REM) module that integrates multiple value networks with multi-head networks is utilized to improve the model generalization and robustness. Extensive experiments on large-scale real-world ride-hailing datasets show the superiority of our design",
    "checked": true,
    "id": "90895a914b4df1aabd7a5dacbd7bc72fbdc25cd9",
    "semantic_title": "nondbrem: nondeterministic offline reinforcement learning for large-scale order dispatching",
    "citation_count": 1,
    "authors": [
      "Hongbo Zhang",
      "Guang Wang",
      "Xu Wang",
      "Zhengyang Zhou",
      "Chen Zhang",
      "Zheng Dong",
      "Yang Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27795": {
    "title": "Scale Optimization Using Evolutionary Reinforcement Learning for Object Detection on Drone Imagery",
    "volume": "main",
    "abstract": "Object detection in aerial imagery presents a significant challenge due to large scale variations among objects. This paper proposes an evolutionary reinforcement learning agent, integrated within a coarse-to-fine object detection framework, to optimize the scale for more effective detection of objects in such images. Specifically, a set of patches potentially containing objects are first generated. A set of rewards measuring the localization accuracy, the accuracy of predicted labels, and the scale consistency among nearby patches are designed in the agent to guide the scale optimization. The proposed scale-consistency reward ensures similar scales for neighboring objects of the same category. Furthermore, a spatial-semantic attention mechanism is designed to exploit the spatial semantic relations between patches. The agent employs the proximal policy optimization strategy in conjunction with the evolutionary strategy, effectively utilizing both the current patch status and historical experience embedded in the agent. The proposed model is compared with state-of-the-art methods on two benchmark datasets for object detection on drone imagery. It significantly outperforms all the compared methods. Code is available at https://github.com/UNNC-CV/EvOD/",
    "checked": true,
    "id": "ea85f8d412c6d6cfae90dd54623fcb5ebe80cf42",
    "semantic_title": "scale optimization using evolutionary reinforcement learning for object detection on drone imagery",
    "citation_count": 1,
    "authors": [
      "Jialu Zhang",
      "Xiaoying Yang",
      "Wentao He",
      "Jianfeng Ren",
      "Qian Zhang",
      "Yitian Zhao",
      "Ruibin Bai",
      "Xiangjian He",
      "Jiang Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27796": {
    "title": "Adversarial Attacks on Federated-Learned Adaptive Bitrate Algorithms",
    "volume": "main",
    "abstract": "Learning-based adaptive bitrate (ABR) algorithms have revolutionized video streaming solutions. With the growing demand for data privacy and the rapid development of mobile devices, federated learning (FL) has emerged as a popular training method for neural ABR algorithms in both academia and industry. However, we have discovered that FL-based ABR models are vulnerable to model-poisoning attacks as local updates remain unseen during global aggregation. In response, we propose MAFL (Malicious ABR model based on Federated Learning) to prove that backdooring the learning-based ABR model via FL is practical. Instead of attacking the global policy, MAFL only targets a single ``target client''. Moreover, the unique challenges brought by deep reinforcement learning (DRL) make the attack even more challenging. To address these challenges, MAFL is designed with a two-stage attacking mechanism. Using two representative attack cases with real-world traces, we show that MAFL significantly degrades the model performance on the target client (i.e., increasing rebuffering penalty by 2x and 5x) with a minimal negative impact on benign clients",
    "checked": true,
    "id": "9858cc69abd8b042e8891e68c12bbc08aebbbea0",
    "semantic_title": "adversarial attacks on federated-learned adaptive bitrate algorithms",
    "citation_count": 0,
    "authors": [
      "Rui-Xiao Zhang",
      "Tianchi Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27797": {
    "title": "Generalize for Future: Slow and Fast Trajectory Learning for CTR Prediction",
    "volume": "main",
    "abstract": "Deep neural networks (DNNs) have achieved significant advancements in click-through rate (CTR) prediction by demonstrating strong generalization on training data. However, in real-world scenarios, the assumption of independent and identically distributed (i.i.d.) conditions, which is fundamental to this problem, is often violated due to temporal distribution shifts. This violation can lead to suboptimal model performance when optimizing empirical risk without access to future data, resulting in overfitting on the training data and convergence to a single sharp minimum. To address this challenge, we propose a novel model updating framework called Slow and Fast Trajectory Learning (SFTL) network. SFTL aims to mitigate the discrepancy between past and future domains while quickly adapting to recent changes in small temporal drifts. This mechanism entails two interactions among three complementary learners: (i) the Working Learner, which updates model parameters using modern optimizers (e.g., Adam, Adagrad) and serves as the primary learner in the recommendation system, (ii) the Slow Learner, which is updated in each temporal domain by directly assigning the model weights of the working learner, and (iii) the Fast Learner, which is updated in each iteration by assigning exponentially moving average weights of the working learner. Additionally, we propose a novel rank-based trajectory loss to facilitate interaction between the working learner and trajectory learner, aiming to adapt to temporal drift and enhance performance in the current domain compared to the past. We provide theoretical understanding and conduct extensive experiments on real-world CTR prediction datasets to validate the effectiveness and efficiency of SFTL in terms of both convergence speed and model performance. The results demonstrate the superiority of SFTL over existing approaches",
    "checked": true,
    "id": "30170172a98512badd6b1e6ad2ccb3e7d97e116e",
    "semantic_title": "generalize for future: slow and fast trajectory learning for ctr prediction",
    "citation_count": 0,
    "authors": [
      "Jian Zhu",
      "Congcong Liu",
      "Xue Jiang",
      "Changping Peng",
      "Zhangang Lin",
      "Jingping Shao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27798": {
    "title": "Hot or Cold? Adaptive Temperature Sampling for Code Generation with Large Language Models",
    "volume": "main",
    "abstract": "Recently, Large Language Models (LLMs) have shown impressive abilities in code generation. However, existing LLMs' decoding strategies are designed for Natural Language (NL) generation, overlooking the differences between NL and programming languages (PL). Due to this oversight, a better decoding strategy for code generation remains an open question. In this paper, we conduct the first systematic study to explore a decoding strategy specialized in code generation. With an analysis of loss distributions of code tokens, we find that code tokens can be divided into two categories: challenging tokens that are difficult to predict and confident tokens that can be easily inferred. Among them, the challenging tokens mainly appear at the beginning of a code block. Inspired by the above findings, we propose a simple yet effective method: Adaptive Temperature (AdapT) sampling, which dynamically adjusts the temperature coefficient when decoding different tokens. We apply a larger temperature when sampling for challenging tokens, allowing LLMs to explore diverse choices. We employ a smaller temperature for confident tokens avoiding the influence of tail randomness noises. We apply AdapT sampling to LLMs with different sizes and conduct evaluations on two popular datasets. Results show that AdapT sampling significantly outperforms state-of-the-art decoding strategy",
    "checked": true,
    "id": "4a12c3e9dc9dfab3173f357615e0a5320ce2bf48",
    "semantic_title": "hot or cold? adaptive temperature sampling for code generation with large language models",
    "citation_count": 5,
    "authors": [
      "Yuqi Zhu",
      "Jia Li",
      "Ge Li",
      "YunFei Zhao",
      "Jia Li",
      "Zhi Jin",
      "Hong Mei"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27799": {
    "title": "Operationalizing Essential Characteristics of Creativity in a Computational System for Music Composition",
    "volume": "main",
    "abstract": "We address the problem of building and evaluating a computational system whose primary objective is creativity. We illustrate seven characteristics for computational creativity in the context of a system that autonomously composes Western lyrical music. We conduct an external evaluation of the system in which respondents rated the system with regard to each characteristic as well as with regard to overall creativity. Average scores for overall creativity exceeded the ratings for any single characteristic, suggesting that creativity may be an emergent property and that unique research opportunities exist for building CC systems whose design attempts to comprehend all known characteristics of creativity",
    "checked": true,
    "id": "6a013129353f34bd74d02844e155e66ce20b8d1c",
    "semantic_title": "operationalizing essential characteristics of creativity in a computational system for music composition",
    "citation_count": 0,
    "authors": [
      "Paul M. Bodily",
      "Dan Ventura"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27800": {
    "title": "Neural Reasoning about Agents' Goals, Preferences, and Actions",
    "volume": "main",
    "abstract": "We propose the Intuitive Reasoning Network (IRENE) - a novel neural model for intuitive psychological reasoning about agents' goals, preferences, and actions that can generalise previous experiences to new situations. IRENE combines a graph neural network for learning agent and world state representations with a transformer to encode the task context. When evaluated on the challenging Baby Intuitions Benchmark, IRENE achieves new state-of-the-art performance on three out of its five tasks - with up to 48.9% improvement. In contrast to existing methods, IRENE is able to bind preferences to specific agents, to better distinguish between rational and irrational agents, and to better understand the role of blocking obstacles. We also investigate, for the first time, the influence of the training tasks on test performance. Our analyses demonstrate the effectiveness of IRENE in combining prior knowledge gained during training for unseen evaluation tasks",
    "checked": true,
    "id": "b0abbbb104d74cd4bf0e3e3e9753265b26ce3154",
    "semantic_title": "neural reasoning about agents' goals, preferences, and actions",
    "citation_count": 4,
    "authors": [
      "Matteo Bortoletto",
      "Lei Shi",
      "Andreas Bulling"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27801": {
    "title": "An Empirical Study of CLIP for Text-Based Person Search",
    "volume": "main",
    "abstract": "Text-based Person Search (TBPS) aims to retrieve the person images using natural language descriptions. Recently, Contrastive Language Image Pretraining (CLIP), a universal large cross-modal vision-language pre-training model, has remarkably performed over various cross-modal downstream tasks due to its powerful cross-modal semantic learning capacity. TPBS, as a fine-grained cross-modal retrieval task, is also facing the rise of research on the CLIP-based TBPS. In order to explore the potential of the visual-language pre-training model for downstream TBPS tasks, this paper makes the first attempt to conduct a comprehensive empirical study of CLIP for TBPS and thus contribute a straightforward, incremental, yet strong TBPS-CLIP baseline to the TBPS community. We revisit critical design considerations under CLIP, including data augmentation and loss function. The model, with the aforementioned designs and practical training tricks, can attain satisfactory performance without any sophisticated modules. Also, we conduct the probing experiments of TBPS-CLIP in model generalization and model compression, demonstrating the effectiveness of TBPS-CLIP from various aspects. This work is expected to provide empirical insights and highlight future CLIP-based TBPS research",
    "checked": true,
    "id": "8fa4800425121b885ef9e03ec5332b6b5bfcc8bc",
    "semantic_title": "an empirical study of clip for text-based person search",
    "citation_count": 6,
    "authors": [
      "Min Cao",
      "Yang Bai",
      "Ziyin Zeng",
      "Mang Ye",
      "Min Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27802": {
    "title": "Social Physics Informed Diffusion Model for Crowd Simulation",
    "volume": "main",
    "abstract": "Crowd simulation holds crucial applications in various domains, such as urban planning, architectural design, and traffic arrangement. In recent years, physics-informed machine learning methods have achieved state-of-the-art performance in crowd simulation but fail to model the heterogeneity and multi-modality of human movement comprehensively. In this paper, we propose a social physics-informed diffusion model named SPDiff to mitigate the above gap. SPDiff takes both the interactive and historical information of crowds in the current timeframe to reverse the diffusion process, thereby generating the distribution of pedestrian movement in the subsequent timeframe. Inspired by the well-known social physics model, i.e., Social Force, regarding crowd dynamics, we design a crowd interaction encoder to guide the denoising process and further enhance this module with the equivariant properties of crowd interactions. To mitigate error accumulation in long-term simulations, we propose a multi-frame rollout training algorithm for diffusion modeling. Experiments conducted on two real-world datasets demonstrate the superior performance of SPDiff in terms of both macroscopic and microscopic evaluation metrics. Code and appendix are available at https://github.com/tsinghua-fib-lab/SPDiff",
    "checked": true,
    "id": "ad8d9ce3e2ee35e765827a11bc15918362459a35",
    "semantic_title": "social physics informed diffusion model for crowd simulation",
    "citation_count": 1,
    "authors": [
      "Hongyi Chen",
      "Jingtao Ding",
      "Yong Li",
      "Yue Wang",
      "Xiao-Ping Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27803": {
    "title": "Trend-Aware Supervision: On Learning Invariance for Semi-supervised Facial Action Unit Intensity Estimation",
    "volume": "main",
    "abstract": "With the increasing need for facial behavior analysis, semi-supervised AU intensity estimation using only keyframe annotations has emerged as a practical and effective solution to relieve the burden of annotation. However, the lack of annotations makes the spurious correlation problem caused by AU co-occurrences and subject variation much more prominent, leading to non-robust intensity estimation that is entangled among AUs and biased among subjects. We observe that trend information inherent in keyframe annotations could act as extra supervision and raising the awareness of AU-specific facial appearance changing trends during training is the key to learning invariant AU-specific features. To this end, we propose Trend-AwareSupervision (TAS), which pursues three kinds of trend awareness, including intra-trend ranking awareness, intra-trend speed awareness, and inter-trend subject awareness. TAS alleviates the spurious correlation problem by raising trend awareness during training to learn AU-specific features that represent the corresponding facial appearance changes, to achieve intensity estimation invariance. Experiments conducted on two commonly used AU benchmark datasets, BP4D and DISFA, show the effectiveness of each kind of awareness. And under trend-aware supervision, the performance can be improved without extra computational or storage costs during inference",
    "checked": true,
    "id": "be07fe74e59996ba4e44bed1cb56c3dc4cdbd7bb",
    "semantic_title": "trend-aware supervision: on learning invariance for semi-supervised facial action unit intensity estimation",
    "citation_count": 0,
    "authors": [
      "Yingjie Chen",
      "Jiarui Zhang",
      "Tao Wang",
      "Yun Liang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27804": {
    "title": "Enhancing the Robustness of Spiking Neural Networks with Stochastic Gating Mechanisms",
    "volume": "main",
    "abstract": "Spiking neural networks (SNNs) exploit neural spikes to provide solutions for low-power intelligent applications on neuromorphic hardware. Although SNNs have high computational efficiency due to spiking communication, they still lack resistance to adversarial attacks and noise perturbations. In the brain, neuronal responses generally possess stochasticity induced by ion channels and synapses, while the role of stochasticity in computing tasks is poorly understood. Inspired by this, we elaborate a stochastic gating spiking neural model for layer-by-layer spike communication, introducing stochasticity to SNNs. Through theoretical analysis, our gating model can be viewed as a regularizer that prevents error amplification under attacks. Meanwhile, our work can explain the robustness of Poisson coding. Experimental results prove that our method can be used alone or with existing robust enhancement algorithms to improve SNN robustness and reduce SNN energy consumption. We hope our work will shed new light on the role of stochasticity in the computation of SNNs. Our code is available at https://github.com/DingJianhao/StoG-meets-SNN/",
    "checked": true,
    "id": "3302584d6f892d00dc9e013d67714abde34fc931",
    "semantic_title": "enhancing the robustness of spiking neural networks with stochastic gating mechanisms",
    "citation_count": 2,
    "authors": [
      "Jianhao Ding",
      "Zhaofei Yu",
      "Tiejun Huang",
      "Jian K. Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27805": {
    "title": "Imitation of Life: A Search Engine for Biologically Inspired Design",
    "volume": "main",
    "abstract": "Biologically Inspired Design (BID), or Biomimicry, is a problem-solving methodology that applies analogies from nature to solve engineering challenges. For example, Speedo engineers designed swimsuits based on shark skin. Finding relevant biological solutions for real-world problems poses significant challenges, both due to the limited biological knowledge engineers and designers typically possess and to the limited BID resources. Existing BID datasets are hand-curated and small, and scaling them up requires costly human annotations. In this paper, we introduce BARcode (Biological Analogy Retriever), a search engine for automatically mining bio-inspirations from the web at scale. Using advances in natural language understanding and data programming, BARcode identifies potential inspirations for engineering challenges. Our experiments demonstrate that BARcode can retrieve inspirations that are valuable to engineers and designers tackling real-world problems, as well as recover famous historical BID examples. We release data and code; we view BARcode as a step towards addressing the challenges that have historically hindered the practical application of BID to engineering innovation",
    "checked": true,
    "id": "3f0736c8d2e5418b5ef077c4218c2790a4823765",
    "semantic_title": "imitation of life: a search engine for biologically inspired design",
    "citation_count": 0,
    "authors": [
      "Hen Emuna",
      "Nadav Borenstein",
      "Xin Qian",
      "Hyeonsu Kang",
      "Joel Chan",
      "Aniket  Kittur",
      "Dafna Shahaf"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27806": {
    "title": "An Efficient Knowledge Transfer Strategy for Spiking Neural Networks from Static to Event Domain",
    "volume": "main",
    "abstract": "Spiking neural networks (SNNs) are rich in spatio-temporal dynamics and are suitable for processing event-based neuromorphic data. However, event-based datasets are usually less annotated than static datasets. This small data scale makes SNNs prone to overfitting and limits their performance. In order to improve the generalization ability of SNNs on event-based datasets, we use static images to assist SNN training on event data. In this paper, we first discuss the domain mismatch problem encountered when directly transferring networks trained on static datasets to event data. We argue that the inconsistency of feature distributions becomes a major factor hindering the effective transfer of knowledge from static images to event data. To address this problem, we propose solutions in terms of two aspects: feature distribution and training strategy. Firstly, we propose a knowledge transfer loss, which consists of domain alignment loss and spatio-temporal regularization. The domain alignment loss learns domain-invariant spatial features by reducing the marginal distribution distance between the static image and the event data. Spatio-temporal regularization provides dynamically learnable coefficients for domain alignment loss by using the output features of the event data at each time step as a regularization term. In addition, we propose a sliding training strategy, which gradually replaces static image inputs probabilistically with event data, resulting in a smoother and more stable training for the network. We validate our method on neuromorphic datasets, including N-Caltech101, CEP-DVS, and N-Omniglot. The experimental results show that our proposed method achieves better performance on all datasets compared to the current state-of-the-art methods. Code is available at https://github.com/Brain-Cog-Lab/Transfer-for-DVS",
    "checked": true,
    "id": "4bd7de4aa8c6c3d5b6e3f37fc73cfb6abc6a2623",
    "semantic_title": "an efficient knowledge transfer strategy for spiking neural networks from static to event domain",
    "citation_count": 0,
    "authors": [
      "Xiang He",
      "Dongcheng Zhao",
      "Yang Li",
      "Guobin Shen",
      "Qingqun Kong",
      "Yi Zeng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27807": {
    "title": "Responding to the Call: Exploring Automatic Music Composition Using a Knowledge-Enhanced Model",
    "volume": "main",
    "abstract": "Call-and-response is a musical technique that enriches the creativity of music, crafting coherent musical ideas that mirror the back-and-forth nature of human dialogue with distinct musical characteristics. Although this technique is integral to numerous musical compositions, it remains largely uncharted in automatic music composition. To enhance the creativity of machine-composed music, we first introduce the Call-Response Dataset (CRD) containing 19,155 annotated musical pairs and crafted comprehensive objective evaluation metrics for musical assessment. Then, we design a knowledge-enhanced learning-based method to bridge the gap between human and machine creativity. Specifically, we train the composition module using the call-response pairs, supplementing it with musical knowledge in terms of rhythm, melody, and harmony. Our experimental results underscore that our proposed model adeptly produces a wide variety of creative responses for various musical calls",
    "checked": true,
    "id": "01781d9669355576968e0f7c148bdd98fd7be1ba",
    "semantic_title": "responding to the call: exploring automatic music composition using a knowledge-enhanced model",
    "citation_count": 0,
    "authors": [
      "Zhejing Hu",
      "Yan Liu",
      "Gong Chen",
      "Xiao Ma",
      "Shenghua Zhong",
      "Qianwen Luo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27808": {
    "title": "Neural Amortized Inference for Nested Multi-Agent Reasoning",
    "volume": "main",
    "abstract": "Multi-agent interactions, such as communication, teaching, and bluffing, often rely on higher-order social inference, i.e., understanding how others infer oneself. Such intricate reasoning can be effectively modeled through nested multi-agent reasoning. Nonetheless, the computational complexity escalates exponentially with each level of reasoning, posing a significant challenge. However, humans effortlessly perform complex social inferences as part of their daily lives. To bridge the gap between human-like inference capabilities and computational limitations, we propose a novel approach: leveraging neural networks to amortize high-order social inference, thereby expediting nested multi-agent reasoning. We evaluate our method in two challenging multi-agent interaction domains. The experimental results demonstrate that our method is computationally efficient while exhibiting minimal degradation in accuracy",
    "checked": true,
    "id": "15f18b7bfd822e9c924477bb951d45a0eeb2486b",
    "semantic_title": "neural amortized inference for nested multi-agent reasoning",
    "citation_count": 1,
    "authors": [
      "Kunal Jha",
      "Tuan Anh Le",
      "Chuanyang Jin",
      "Yen-Ling Kuo",
      "Joshua B. Tenenbaum",
      "Tianmin Shu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27809": {
    "title": "Hidden Follower Detection: How Is the Gaze-Spacing Pattern Embodied in Frequency Domain?",
    "volume": "main",
    "abstract": "Spatiotemporal social behavior analysis is a technique that studies the social behavior patterns of objects and estimates their risks based on their trajectories. In social public scenarios such as train stations, hidden following behavior has become one of the most challenging issues due to its probability of evolving into violent events, which is more than 25%. In recent years, research on hidden following detection (HFD) has focused on differences in time series between hidden followers and normal pedestrians under two temporal characteristics: gaze and spatial distance. However, the time-domain representation for time series is irreversible and usually causes the loss of critical information. In this paper, we deeply study the expression efficiency of time/frequency domain features of time series, by exploring the recovery mechanism of features to source time series, we establish a fidelity estimation method for feature expression and a selection model for frequency-domain features based on the signal-to-distortion ratio (SDR). Experimental results demonstrate the feature fidelity of time series and HFD performance are positively correlated, and the fidelity of frequency-domain features and HFD performance are significantly better than the time-domain features. On both real and simulated datasets, the accuracy of the proposed method is increased by 3%, and the gaze-only module is improved by 10%. Related research has explored new methods for optimal feature selection based on fidelity, new patterns for efficient feature expression of hidden following behavior, and the mechanism of multimodal collaborative identification",
    "checked": true,
    "id": "e1b28ba6a6a3ffd5ae3ab5fd5f53eeb85399f301",
    "semantic_title": "hidden follower detection: how is the gaze-spacing pattern embodied in frequency domain?",
    "citation_count": 1,
    "authors": [
      "Shu Li",
      "Ruimin Hu",
      "Suhui Li",
      "Liang Liao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27810": {
    "title": "Music Style Transfer with Time-Varying Inversion of Diffusion Models",
    "volume": "main",
    "abstract": "With the development of diffusion models, text-guided image style transfer has demonstrated great controllable and high-quality results. However, the utilization of text for diverse music style transfer poses significant challenges, primarily due to the limited availability of matched audio-text datasets. Music, being an abstract and complex art form, exhibits variations and intricacies even within the same genre, thereby making accurate textual descriptions challenging. This paper presents a music style transfer approach that effectively captures musical attributes using minimal data. We introduce a novel time-varying textual inversion module to precisely capture mel-spectrogram features at different levels. During inference, we utilize a bias-reduced stylization technique to get stable results. Experimental results demonstrate that our method can transfer the style of specific instruments, as well as incorporate natural sounds to compose melodies. Samples and code are available at https://lsfhuihuiff.github.io/MusicTI/",
    "checked": true,
    "id": "66c72f9d18b3a996c491f62937543072ef382d6b",
    "semantic_title": "music style transfer with time-varying inversion of diffusion models",
    "citation_count": 2,
    "authors": [
      "Sifei Li",
      "Yuxin Zhang",
      "Fan Tang",
      "Chongyang Ma",
      "Weiming Dong",
      "Changsheng Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27811": {
    "title": "A Brain-Inspired Way of Reducing the Network Complexity via Concept-Regularized Coding for Emotion Recognition",
    "volume": "main",
    "abstract": "The human brain can effortlessly and reliably perceive emotions, whereas existing facial emotion recognition (FER) methods suffer from drawbacks such as complex model structures, high storage requirements, and poor interpretability. Inspired by the role of emotion concepts in visual perception coding within the human brain, we propose a dual-pathway framework emulating the neural computation of emotion recognition. Specifically, these two pathways are designed to model the representation of emotion concepts in the brain and the visual perception process, respectively. For the former, we adopt a disentangled approach to extract emotion concepts from complex facial geometric attributes; for the latter, we employ an emotional confidence evaluation strategy to determine which concept is optimal for regularizing the perceptual coding. The proposed concept-regularized coding strategy endows the framework with flexibility and interpretability as well as good performances on several benchmarking FER datasets",
    "checked": true,
    "id": "02029178c63a96d9165ee33062ef8d179d5a9ad4",
    "semantic_title": "a brain-inspired way of reducing the network complexity via concept-regularized coding for emotion recognition",
    "citation_count": 0,
    "authors": [
      "Han Lu",
      "Xiahai Zhuang",
      "Qiang Luo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27812": {
    "title": "Multi-Energy Guided Image Translation with Stochastic Differential Equations for Near-Infrared Facial Expression Recognition",
    "volume": "main",
    "abstract": "Illumination variation has been a long-term challenge in real-world facial expression recognition (FER). Under uncontrolled or non-visible light conditions, near-infrared (NIR) can provide a simple and alternative solution to obtain high-quality images and supplement the geometric and texture details that are missing in the visible (VIS) domain. Due to the lack of large-scale NIR facial expression datasets, directly extending VIS FER methods to the NIR spectrum may be ineffective. Additionally, previous heterogeneous image synthesis methods are restricted by low controllability without prior task knowledge. To tackle these issues, we present the first approach, called for NIR-FER Stochastic Differential Equations (NFER-SDE), that transforms face expression appearance between heterogeneous modalities to the overfitting problem on small-scale NIR data. NFER-SDE can take the whole VIS source image as input and, together with domain-specific knowledge, guide the preservation of modality-invariant information in the high-frequency content of the image. Extensive experiments and ablation studies show that NFER-SDE significantly improves the performance of NIR FER and achieves state-of-the-art results on the only two available NIR FER datasets, Oulu-CASIA and Large-HFE",
    "checked": true,
    "id": "21eef3fadc380934f19d1283f36e0dbcec8dc90b",
    "semantic_title": "multi-energy guided image translation with stochastic differential equations for near-infrared facial expression recognition",
    "citation_count": 0,
    "authors": [
      "Bingjun Luo",
      "Zewen Wang",
      "Jinpeng Wang",
      "Junjie Zhu",
      "Xibin Zhao",
      "Yue Gao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27813": {
    "title": "Successive POI Recommendation via Brain-Inspired Spatiotemporal Aware Representation",
    "volume": "main",
    "abstract": "Existing approaches usually perform spatiotemporal representation in the spatial and temporal dimensions, respectively, which isolates the spatial and temporal natures of the target and leads to sub-optimal embeddings. Neuroscience research has shown that the mammalian brain entorhinal-hippocampal system provides efficient graph representations for general knowledge. Moreover, entorhinal grid cells present concise spatial representations, while hippocampal place cells represent perception conjunctions effectively. Thus, the entorhinal-hippocampal system provides a novel angle for spatiotemporal representation, which inspires us to propose the SpatioTemporal aware Embedding framework (STE) and apply it to POIs (STEP). STEP considers two types of POI-specific representations: sequential representation and spatiotemporal conjunctive representation, learned using sparse unlabeled data based on the proposed graph-building policies. Notably, STEP jointly represents the spatiotemporal natures of POIs using both observations and contextual information from integrated spatiotemporal dimensions by constructing a spatiotemporal context graph. Furthermore, we introduce a successive POI recommendation method using STEP, which achieves state-of-the-art performance on two benchmarks. In addition, we demonstrate the excellent performance of the STE representation approach in other spatiotemporal representation-centered tasks through a case study of the traffic flow prediction problem. Therefore, this work provides a novel solution to spatiotemporal representation and paves a new way for spatiotemporal modeling-related tasks",
    "checked": true,
    "id": "3e79febc340e80eaf61e00f36bb7ee00c0449372",
    "semantic_title": "successive poi recommendation via brain-inspired spatiotemporal aware representation",
    "citation_count": 1,
    "authors": [
      "Gehua Ma",
      "He Wang",
      "Jingyuan Zhao",
      "Rui Yan",
      "Huajin Tang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27814": {
    "title": "BDIQA: A New Dataset for Video Question Answering to Explore Cognitive Reasoning through Theory of Mind",
    "volume": "main",
    "abstract": "As a foundational component of cognitive intelligence, theory of mind (ToM) can make AI more closely resemble human thought processes, thereby enhancing their interaction and collaboration with human. In particular, it can significantly improve a model's comprehension of videos in complex scenes. However, current video question answer (VideoQA) datasets focus on studying causal reasoning within events, few of them genuinely incorporating human ToM. Consequently, there is a lack of development in ToM reasoning tasks within the area of VideoQA. This paper presents BDIQA, the first benchmark to explore the cognitive reasoning capabilities of VideoQA models in the context of ToM. BDIQA is inspired by the cognitive development of children's ToM and addresses the current deficiencies in machine ToM within datasets and tasks. Specifically, it offers tasks at two difficulty levels, assessing Belief, Desire and Intention (BDI) reasoning in both simple and complex scenarios. We conduct evaluations on several mainstream methods of VideoQA and diagnose their capabilities with zero-shot, few-shot and supervised learning. We find that the performance of pre-trained models on cognitive reasoning tasks remains unsatisfactory. To counter this challenge, we undertake thorough analysis and experimentation, ultimately presenting two guidelines to enhance cognitive reasoning derived from ablation analysis",
    "checked": true,
    "id": "825663ac1457fd9f5bbd23568f955af579ff0a4d",
    "semantic_title": "bdiqa: a new dataset for video question answering to explore cognitive reasoning through theory of mind",
    "citation_count": 0,
    "authors": [
      "Yuanyuan Mao",
      "Xin Lin",
      "Qin Ni",
      "Liang He"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27815": {
    "title": "Unveiling the Significance of Toddler-Inspired Reward Transition in Goal-Oriented Reinforcement Learning",
    "volume": "main",
    "abstract": "Toddlers evolve from free exploration with sparse feedback to exploiting prior experiences for goal-directed learning with denser rewards. Drawing inspiration from this Toddler-Inspired Reward Transition, we set out to explore the implications of varying reward transitions when incorporated into Reinforcement Learning (RL) tasks. Central to our inquiry is the transition from sparse to potential-based dense rewards, which share optimal strategies regardless of reward changes. Through various experiments, including those in egocentric navigation and robotic arm manipulation tasks, we found that proper reward transitions significantly influence sample efficiency and success rates. Of particular note is the efficacy of the toddler-inspired Sparse-to-Dense (S2D) transition. Beyond these performance metrics, using Cross-Density Visualizer technique, we observed that transitions, especially the S2D, smooth the policy loss landscape, promoting wide minima that enhance generalization in RL models",
    "checked": true,
    "id": "740e69d93e9dbcb78b987c2955f709c07b578266",
    "semantic_title": "unveiling the significance of toddler-inspired reward transition in goal-oriented reinforcement learning",
    "citation_count": 1,
    "authors": [
      "Junseok Park",
      "Yoonsung Kim",
      "Hee bin Yoo",
      "Min Whoo Lee",
      "Kibeom Kim",
      "Won-Seok Choi",
      "Minsu Lee",
      "Byoung-Tak Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27816": {
    "title": "Gated Attention Coding for Training High-Performance and Efficient Spiking Neural Networks",
    "volume": "main",
    "abstract": "Spiking neural networks (SNNs) are emerging as an energy-efficient alternative to traditional artificial neural networks (ANNs) due to their unique spike-based event-driven nature. Coding is crucial in SNNs as it converts external input stimuli into spatio-temporal feature sequences. However, most existing deep SNNs rely on direct coding that generates powerless spike representation and lacks the temporal dynamics inherent in human vision. Hence, we introduce Gated Attention Coding (GAC), a plug-and-play module that leverages the multi-dimensional gated attention unit to efficiently encode inputs into powerful representations before feeding them into the SNN architecture. GAC functions as a preprocessing layer that does not disrupt the spike-driven nature of the SNN, making it amenable to efficient neuromorphic hardware implementation with minimal modifications. Through an observer model theoretical analysis, we demonstrate GAC's attention mechanism improves temporal dynamics and coding efficiency. Experiments on CIFAR10/100 and ImageNet datasets demonstrate that GAC achieves state-of-the-art accuracy with remarkable efficiency. Notably, we improve top-1 accuracy by 3.10% on CIFAR100 with only 6-time steps and 1.07% on ImageNet while reducing energy usage to 66.9% of the previous works. To our best knowledge, it is the first time to explore the attention-based dynamic coding scheme in deep SNNs, with exceptional effectiveness and efficiency on large-scale datasets. Code is available at https://github.com/bollossom/GAC",
    "checked": true,
    "id": "b9a6a6dee5803d9d4c6a6d4bb3da59fe512459ca",
    "semantic_title": "gated attention coding for training high-performance and efficient spiking neural networks",
    "citation_count": 8,
    "authors": [
      "Xuerui Qiu",
      "Rui-Jie Zhu",
      "Yuhong Chou",
      "Zhaorui Wang",
      "Liang-Jian Deng",
      "Guoqi Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27817": {
    "title": "Efficient Spiking Neural Networks with Sparse Selective Activation for Continual Learning",
    "volume": "main",
    "abstract": "The next generation of machine intelligence requires the capability of continual learning to acquire new knowledge without forgetting the old one while conserving limited computing resources. Spiking neural networks (SNNs), compared to artificial neural networks (ANNs), have more characteristics that align with biological neurons, which may be helpful as a potential gating function for knowledge maintenance in neural networks. Inspired by the selective sparse activation principle of context gating in biological systems, we present a novel SNN model with selective activation to achieve continual learning. The trace-based K-Winner-Take-All (K-WTA) and variable threshold components are designed to form the sparsity in selective activation in spatial and temporal dimensions of spiking neurons, which promotes the subpopulation of neuron activation to perform specific tasks. As a result, continual learning can be maintained by routing different tasks via different populations of neurons in the network. The experiments are conducted on MNIST and CIFAR10 datasets under the class incremental setting. The results show that the proposed SNN model achieves competitive performance similar to and even surpasses the other regularization-based methods deployed under traditional ANNs",
    "checked": true,
    "id": "bf6aa1317cdebcd7a2e60f12c5071ab08aa84335",
    "semantic_title": "efficient spiking neural networks with sparse selective activation for continual learning",
    "citation_count": 3,
    "authors": [
      "Jiangrong Shen",
      "Wenyao Ni",
      "Qi Xu",
      "Huajin Tang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27818": {
    "title": "Boosting Neural Cognitive Diagnosis with Student's Affective State Modeling",
    "volume": "main",
    "abstract": "Cognitive Diagnosis Modeling aims to infer students' proficiency level on knowledge concepts from their response logs. Existing methods typically model students' response processes as the interaction between students and exercises or concepts based on hand-crafted or deeply-learned interaction functions. Despite their promising achievements, they fail to consider the relationship between students' cognitive states and affective states in learning, e.g., the feelings of frustration, boredom, or confusion with the learning content, which is insufficient for comprehensive cognitive diagnosis in intelligent education. To fill the research gap, we propose a novel Affect-aware Cognitive Diagnosis (ACD) model which can effectively diagnose the knowledge proficiency levels of students by taking into consideration the affective factors. Specifically, we first design a student affect perception module under the assumption that the affective state is jointly influenced by the student's affect trait and the difficulty of the exercise. Then, our inferred affective distribution is further used to estimate the student's subjective factors, i.e., guessing and slipping, respectively. Finally, we integrate the estimated guessing and slipping parameters with the basic neural cognitive diagnosis framework based on the DINA model, which facilitates the modeling of complex exercising interactions in a more accurate and interpretable fashion. Besides, we also extend our affect perception module in an unsupervised learning setting based on contrastive learning, thus significantly improving the compatibility of our ACD. To the best of our knowledge, we are the first to unify the cognition modeling and affect modeling into the same framework for student cognitive diagnosis. Extensive experiments on real-world datasets clearly demonstrate the effectiveness of our ACD. Our code is available at https://github.com/zeng-zhen/ACD",
    "checked": true,
    "id": "275c962a5d5f9e1b36e5a20fea01edec14a3e085",
    "semantic_title": "boosting neural cognitive diagnosis with student's affective state modeling",
    "citation_count": 2,
    "authors": [
      "Shanshan Wang",
      "Zhen Zeng",
      "Xun Yang",
      "Ke Xu",
      "Xingyi Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27819": {
    "title": "DMMR: Cross-Subject Domain Generalization for EEG-Based Emotion Recognition via Denoising Mixed Mutual Reconstruction",
    "volume": "main",
    "abstract": "Electroencephalography (EEG) has proven to be effective in emotion analysis. However, current methods struggle with individual variations, complicating the generalization of models trained on data from source subjects to unseen target subjects. To tackle this issue, we propose the Denoising Mixed Mutual Reconstruction (DMMR) model, employing a two-stage pre-training followed by fine-tuning approach. During the pre-training phase, DMMR leverages self-supervised learning through a multi-decoder autoencoder, which encodes and reconstructs features of one subject, aiming to generate features resembling those from other subjects within the same category, thereby encouraging the encoder to learn subject-invariant features. We introduce a hidden-layer mixed data augmentation approach to mitigate the limitations posed by the scarcity of source data, thereby extending the method to a two-stage process. To bolster stability against noise, we incorporate a noise injection method, named \"Time Steps Shuffling\", into the input data. During the fine-tuning phase, an emotion classifier is integrated to extract emotion-related features. Experimental accuracy on the SEED and SEED-IV datasets reached 88.27% (±5.62) and 72.70% (±8.01), respectively, demonstrating state-of-the-art and comparable performance, thereby showcasing the superiority of DMMR. The proposed data augmentation and noise injection methods were observed to complementarily enhance accuracy and stability, thus alleviating the aforementioned issues",
    "checked": true,
    "id": "227e7709ea488d205fe8f87c321afc7a8570bf4e",
    "semantic_title": "dmmr: cross-subject domain generalization for eeg-based emotion recognition via denoising mixed mutual reconstruction",
    "citation_count": 0,
    "authors": [
      "Yiming Wang",
      "Bin Zhang",
      "Yujiao Tang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27820": {
    "title": "Transient Glimpses: Unveiling Occluded Backgrounds through the Spike Camera",
    "volume": "main",
    "abstract": "The de-occlusion problem, involving extracting clear background images by removing foreground occlusions, holds significant practical importance but poses considerable challenges. Most current research predominantly focuses on generating discrete images from calibrated camera arrays, but this approach often struggles with dense occlusions and fast motions due to limited perspectives and motion blur. To overcome these limitations, an effective solution requires the integration of multi-view visual information. The spike camera, as an innovative neuromorphic sensor, shows promise with its ultra-high temporal resolution and dynamic range. In this study, we propose a novel approach that utilizes a single spike camera for continuous multi-view imaging to address occlusion removal. By rapidly moving the spike camera, we capture a dense stream of spikes from occluded scenes. Our model, SpkOccNet, processes these spikes by integrating multi-view spatial-temporal information via long-short-window feature extractor (LSW) and employs a novel cross-view mutual attention-based module (CVA) for effective fusion and refinement. Additionally, to facilitate research in occlusion removal, we introduce the S-OCC dataset, which consists of real-world spike-based data. Experimental results demonstrate the efficiency and generalization capabilities of our model in effectively removing dense occlusions across diverse scenes. Public project page: https://github.com/Leozhangjiyuan/SpikeDeOcclusion",
    "checked": true,
    "id": "d60ba069365512f4203c4e72e5741a19d0a55f4c",
    "semantic_title": "transient glimpses: unveiling occluded backgrounds through the spike camera",
    "citation_count": 0,
    "authors": [
      "Jiyuan Zhang",
      "Shiyan Chen",
      "Yajing Zheng",
      "Zhaofei Yu",
      "Tiejun Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27821": {
    "title": "Open-Set Facial Expression Recognition",
    "volume": "main",
    "abstract": "Facial expression recognition (FER) models are typically trained on datasets with a fixed number of seven basic classes. However, recent research works (Cowen et al. 2021; Bryant et al. 2022; Kollias 2023) point out that there are far more expressions than the basic ones. Thus, when these models are deployed in the real world, they may encounter unknown classes, such as compound expressions that cannot be classified into existing basic classes. To address this issue, we propose the open-set FER task for the first time. Though there are many existing open-set recognition methods, we argue that they do not work well for open-set FER because FER data are all human faces with very small inter-class distances, which makes the open-set samples very similar to close-set samples. In this paper, we are the first to transform the disadvantage of small inter-class distance into an advantage by proposing a new way for open-set FER. Specifically, we find that small inter-class distance allows for sparsely distributed pseudo labels of open-set samples, which can be viewed as symmetric noisy labels. Based on this novel observation, we convert the open-set FER to a noisy label detection problem. We further propose a novel method that incorporates attention map consistency and cycle training to detect the open-set samples. Extensive experiments on various FER datasets demonstrate that our method clearly outperforms state-of-the-art open-set recognition methods by large margins. Code is available at https://github.com/zyh-uaiaaaa",
    "checked": true,
    "id": "43f02c665e1f1b8638dc567f185c27588e39889b",
    "semantic_title": "open-set facial expression recognition",
    "citation_count": 1,
    "authors": [
      "Yuhang Zhang",
      "Yue Yao",
      "Xuannan Liu",
      "Lixiong Qin",
      "Wenjing Wang",
      "Weihong Deng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27822": {
    "title": "Bootstrapping Cognitive Agents with a Large Language Model",
    "volume": "main",
    "abstract": "Large language models contain noisy general knowledge of the world, yet are hard to train or fine-tune. In contrast cognitive architectures have excellent interpretability and are flexible to update but require a lot of manual work to instantiate. In this work, we combine the best of both worlds: bootstrapping a cognitive-based model with the noisy knowledge encoded in large language models. Through an embodied agent doing kitchen tasks, we show that our proposed framework yields better efficiency compared to an agent entirely based on large language models. Our experiments also indicate that the cognitive agent bootstrapped using this framework can generalize to novel environments and be scaled to complex tasks",
    "checked": true,
    "id": "99988640d3926f85c589a0bf96fa5487c218af3b",
    "semantic_title": "bootstrapping cognitive agents with a large language model",
    "citation_count": 1,
    "authors": [
      "Feiyu Zhu",
      "Reid Simmons"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27823": {
    "title": "Data Augmented Graph Neural Networks for Personality Detection",
    "volume": "main",
    "abstract": "Personality detection is a fundamental task for user psychology research. One of the biggest challenges in personality detection lies in the quantitative limitation of labeled data collected by completing the personality questionnaire, which is very time-consuming and labor-intensive. Most of the existing works are mainly devoted to learning the rich representations of posts based on labeled data. However, they still suffer from the inherent weakness of the amount limitation of labels, which potentially restricts the capability of the model to deal with unseen data. In this paper, we construct a heterogeneous personality graph for each labeled and unlabeled user and develop a novel psycholinguistic augmented graph neural network to detect personality in a semi-supervised manner, namely Semi-PerGCN. Specifically, our model first explores a supervised Personality Graph Neural Network (PGNN) to refine labeled user representation on the heterogeneous graph. For the remaining massive unlabeled users, we utilize the empirical psychological knowledge of the Linguistic Inquiry and Word Count (LIWC) lexicon for multi-view graph augmentation and perform unsupervised graph consistent constraints on the parameters shared PGNN. During the learning process of finite labeled users, noise-invariant learning on a large scale of unlabeled users is combined to enhance the generalization ability. Extensive experiments on three real-world datasets, Youtube, PAN2015, and MyPersonality demonstrate the effectiveness of our Semi-PerGCN in personality detection, especially in scenarios with limited labeled users",
    "checked": true,
    "id": "7189401cb3a6a6f8e0fe7b16fb05f2abe2ff9fc7",
    "semantic_title": "data augmented graph neural networks for personality detection",
    "citation_count": 1,
    "authors": [
      "Yangfu Zhu",
      "Yue Xia",
      "Meiling Li",
      "Tingting Zhang",
      "Bin Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27824": {
    "title": "DreamStyler: Paint by Style Inversion with Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "Recent progresses in large-scale text-to-image models have yielded remarkable accomplishments, finding various applications in art domain. However, expressing unique characteristics of an artwork (e.g. brushwork, colortone, or composition) with text prompts alone may encounter limitations due to the inherent constraints of verbal description. To this end, we introduce DreamStyle, a novel framework designed for artistic image synthesis, proficient in both text-to-image synthesis and style transfer. DreamStyle optimizes a multi-stage textual embedding with a context-aware text prompt, resulting in prominent image quality. In addition, with content and style guidance, DreamStyle exhibits flexibility to accommodate a range of style references. Experimental results demonstrate its superior performance across multiple scenarios, suggesting its promising potential in artistic product creation. Project page: https://nmhkahn.github.io/dreamstyler/",
    "checked": true,
    "id": "68708ea853006387537df85f4b811f7aeab6c4f5",
    "semantic_title": "dreamstyler: paint by style inversion with text-to-image diffusion models",
    "citation_count": 11,
    "authors": [
      "Namhyuk Ahn",
      "Junsoo Lee",
      "Chunggi Lee",
      "Kunhee Kim",
      "Daesik Kim",
      "Seung-Hun Nam",
      "Kibeom Hong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27825": {
    "title": "Context Enhanced Transformer for Single Image Object Detection in Video Data",
    "volume": "main",
    "abstract": "With the increasing importance of video data in real-world applications, there is a rising need for efficient object detection methods that utilize temporal information. While existing video object detection (VOD) techniques employ various strategies to address this challenge, they typically depend on locally adjacent frames or randomly sampled images within a clip. Although recent Transformer-based VOD methods have shown promising results, their reliance on multiple inputs and additional network complexity to incorporate temporal information limits their practical applicability. In this paper, we propose a novel approach to single image object detection, called Context Enhanced TRansformer (CETR), by incorporating temporal context into DETR using a newly designed memory module. To efficiently store temporal information, we construct a class-wise memory that collects contextual information across data. Additionally, we present a classification-based sampling technique to selectively utilize the relevant memory for the current image. In the testing, We introduce a test-time memory adaptation method that updates individual memory functions by considering the test distribution. Experiments with CityCam and ImageNet VID datasets exhibit the efficiency of the framework on various video systems. The project page and code will be made available at: https://ku-cvlab.github.io/CETR",
    "checked": false,
    "id": "914e54c4c7a54d593f5f88332ca4b3ab4c71abe9",
    "semantic_title": "context enhanced transformer for single image object detection",
    "citation_count": 0,
    "authors": [
      "Seungjun An",
      "Seonghoon Park",
      "Gyeongnyeon Kim",
      "Jeongyeol Baek",
      "Byeongwon Lee",
      "Seungryong Kim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27826": {
    "title": "SHaRPose: Sparse High-Resolution Representation for Human Pose Estimation",
    "volume": "main",
    "abstract": "High-resolution representation is essential for achieving good performance in human pose estimation models. To obtain such features, existing works utilize high-resolution input images or fine-grained image tokens. However, this dense high-resolution representation brings a significant computational burden. In this paper, we address the following question: \"Only sparse human keypoint locations are detected for human pose estimation, is it really necessary to describe the whole image in a dense, high-resolution manner?\" Based on dynamic transformer models, we propose a framework that only uses Sparse High-resolution Representations for human Pose estimation (SHaRPose). In detail, SHaRPose consists of two stages. At the coarse stage, the relations between image regions and keypoints are dynamically mined while a coarse estimation is generated. Then, a quality predictor is applied to decide whether the coarse estimation results should be refined. At the fine stage, SHaRPose builds sparse high-resolution representations only on the regions related to the keypoints and provides refined high-precision human pose estimations. Extensive experiments demonstrate the outstanding performance of the proposed method. Specifically, compared to the state-of-the-art method ViTPose, our model SHaRPose-Base achieves 77.4 AP (+0.5 AP) on the COCO validation set and 76.7 AP (+0.5 AP) on the COCO test-dev set, and infers at a speed of 1.4x faster than ViTPose-Base. Code is available at https://github.com/AnxQ/sharpose",
    "checked": true,
    "id": "3aa878e0b792f3c220ec58f26cebca4d32b27b3d",
    "semantic_title": "sharpose: sparse high-resolution representation for human pose estimation",
    "citation_count": 2,
    "authors": [
      "Xiaoqi An",
      "Lin Zhao",
      "Chen Gong",
      "Nannan Wang",
      "Di Wang",
      "Jian Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27827": {
    "title": "Comparing the Robustness of Modern No-Reference Image- and Video-Quality Metrics to Adversarial Attacks",
    "volume": "main",
    "abstract": "Nowadays, neural-network-based image- and video-quality metrics perform better than traditional methods. However, they also became more vulnerable to adversarial attacks that increase metrics' scores without improving visual quality. The existing benchmarks of quality metrics compare their performance in terms of correlation with subjective quality and calculation time. Nonetheless, the adversarial robustness of image-quality metrics is also an area worth researching. This paper analyses modern metrics' robustness to different adversarial attacks. We adapted adversarial attacks from computer vision tasks and compared attacks' efficiency against 15 no-reference image- and video-quality metrics. Some metrics showed high resistance to adversarial attacks, which makes their usage in benchmarks safer than vulnerable metrics. The benchmark accepts submissions of new metrics for researchers who want to make their metrics more robust to attacks or to find such metrics for their needs. The latest results can be found online: https://videoprocessing.ai/benchmarks/metrics-robustness.html",
    "checked": true,
    "id": "228eeb5e74cfa52761434a125998bc5c2a674859",
    "semantic_title": "comparing the robustness of modern no-reference image- and video-quality metrics to adversarial attacks",
    "citation_count": 4,
    "authors": [
      "Anastasia Antsiferova",
      "Khaled Abud",
      "Aleksandr Gushchin",
      "Ekaterina Shumitskaya",
      "Sergey Lavrushkin",
      "Dmitriy Vatolin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27828": {
    "title": "DocFormerv2: Local Features for Document Understanding",
    "volume": "main",
    "abstract": "We propose DocFormerv2, a multi-modal transformer for Visual Document Understanding (VDU). The VDU domain entails understanding documents (beyond mere OCR predictions) e.g., extracting information from a form, VQA for documents and other tasks. VDU is challenging as it needs a model to make sense of multiple modalities (visual, language and spatial) to make a prediction. Our approach, termed DocFormerv2 is an encoder-decoder transformer which takes as input - vision, language and spatial features. DocFormerv2 is pre-trained with unsupervised tasks employed asymmetrically i.e., two novel document tasks on encoder and one on the auto-regressive decoder. The unsupervised tasks have been carefully designed to ensure that the pre-training encourages local-feature alignment between multiple modalities. DocFormerv2 when evaluated on nine challenging datasets shows state-of-the-art performance on all over strong baselines - On TabFact (+4.3%), InfoVQA (+1.4%), FUNSD (+1.0%). Furthermore, to show generalization capabilities, on three VQA tasks involving scene-text, DocFormerv2 outperforms previous comparably-sized models and even does better than much larger models (such as GIT2, PaLI and Flamingo) on these tasks. Extensive ablations show that due to its novel pre-training tasks, DocFormerv2 understands multiple modalities better than prior-art in VDU",
    "checked": true,
    "id": "c4b39dd45e64324198d2f47bc191b12ab7eeafae",
    "semantic_title": "docformerv2: local features for document understanding",
    "citation_count": 17,
    "authors": [
      "Srikar Appalaraju",
      "Peng Tang",
      "Qi Dong",
      "Nishant Sankaran",
      "Yichu Zhou",
      "R. Manmatha"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27829": {
    "title": "Exposing the Deception: Uncovering More Forgery Clues for Deepfake Detection",
    "volume": "main",
    "abstract": "Deepfake technology has given rise to a spectrum of novel and compelling applications. Unfortunately, the widespread proliferation of high-fidelity fake videos has led to pervasive confusion and deception, shattering our faith that seeing is believing. One aspect that has been overlooked so far is that current deepfake detection approaches may easily fall into the trap of overfitting, focusing only on forgery clues within one or a few local regions. Moreover, existing works heavily rely on neural networks to extract forgery features, lacking theoretical constraints guaranteeing that sufficient forgery clues are extracted and superfluous features are eliminated. These deficiencies culminate in unsatisfactory accuracy and limited generalizability in real-life scenarios. In this paper, we try to tackle these challenges through three designs: (1) We present a novel framework to capture broader forgery clues by extracting multiple non-overlapping local representations and fusing them into a global semantic-rich feature. (2) Based on the information bottleneck theory, we derive Local Information Loss to guarantee the orthogonality of local representations while preserving comprehensive task-relevant information. (3) Further, to fuse the local representations and remove task-irrelevant information, we arrive at a Global Information Loss through the theoretical analysis of mutual information. Empirically, our method achieves state-of-the-art performance on five benchmark datasets. Our code is available at https://github.com/QingyuLiu/Exposing-the-Deception, hoping to inspire researchers",
    "checked": true,
    "id": "a963409107c15b9148e1fedbce20c136f34e2ecc",
    "semantic_title": "exposing the deception: uncovering more forgery clues for deepfake detection",
    "citation_count": 4,
    "authors": [
      "Zhongjie Ba",
      "Qingyu Liu",
      "Zhenguang Liu",
      "Shuang Wu",
      "Feng Lin",
      "Li Lu",
      "Kui Ren"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27830": {
    "title": "Prompt-Based Distribution Alignment for Unsupervised Domain Adaptation",
    "volume": "main",
    "abstract": "Recently, despite the unprecedented success of large pre-trained visual-language models (VLMs) on a wide range of downstream tasks, the real-world unsupervised domain adaptation (UDA) problem is still not well explored. Therefore, in this paper, we first experimentally demonstrate that the unsupervised-trained VLMs can significantly reduce the distribution discrepancy between source and target domains, thereby improving the performance of UDA. However, a major challenge for directly deploying such models on downstream UDA tasks is prompt engineering, which requires aligning the domain knowledge of source and target domains, since the performance of UDA is severely influenced by a good domain-invariant representation. We further propose a Prompt-based Distribution Alignment (PDA) method to incorporate the domain knowledge into prompt learning. Specifically, PDA employs a two-branch prompt-tuning paradigm, namely base branch and alignment branch. The base branch focuses on integrating class-related representation into prompts, ensuring discrimination among different classes. To further minimize domain discrepancy, for the alignment branch, we construct feature banks for both the source and target domains and propose image-guided feature tuning (IFT) to make the input attend to feature banks, which effectively integrates self-enhanced and cross-domain features into the model. In this way, these two branches can be mutually promoted to enhance the adaptation of VLMs for UDA. We conduct extensive experiments on three benchmarks to demonstrate that our proposed PDA achieves state-of-the-art performance. The code is available at https://github.com/BaiShuanghao/Prompt-based-Distribution-Alignment",
    "checked": true,
    "id": "bd46cb09425c1eb8e2d2e7dd612d839cdf4d0f39",
    "semantic_title": "prompt-based distribution alignment for unsupervised domain adaptation",
    "citation_count": 9,
    "authors": [
      "Shuanghao Bai",
      "Min Zhang",
      "Wanqi Zhou",
      "Siteng Huang",
      "Zhirong Luan",
      "Donglin Wang",
      "Badong Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27831": {
    "title": "Local-Global Multi-Modal Distillation for Weakly-Supervised Temporal Video Grounding",
    "volume": "main",
    "abstract": "This paper for the first time leverages multi-modal videos for weakly-supervised temporal video grounding. As labeling the video moment is labor-intensive and subjective, the weakly-supervised approaches have gained increasing attention in recent years. However, these approaches could inherently compromise performance due to inadequate supervision. Therefore, to tackle this challenge, we for the first time pay attention to exploiting complementary information extracted from multi-modal videos (e.g., RGB frames, optical flows), where richer supervision is naturally introduced in the weaklysupervised context. Our motivation is that by integrating different modalities of the videos, the model is learned from synergic supervision and thereby can attain superior generalization capability. However, addressing multiple modalities† would also inevitably introduce additional computational overhead, and might become inapplicable if a particular modality is inaccessible. To solve this issue, we adopt a novel route: building a multi-modal distillation algorithm to capitalize on the multi-modal knowledge as supervision for model training, while still being able to work with only the single modal input during inference. As such, we can utilize the benefits brought by the supplementary nature of multiple modalities, without compromising the applicability in practical scenarios. Specifically, we first propose a cross-modal mutual learning framework and train a sophisticated teacher model to learn collaboratively from the multi-modal videos. Then we identify two sorts of knowledge from the teacher model, i.e., temporal boundaries and semantic activation map. And we devise a local-global distillation algorithm to transfer this knowledge to a student model of single-modal input at both local and global levels. Extensive experiments on large-scale datasets demonstrate that our method achieves state-of-the-art performance with/without multi-modal inputs",
    "checked": true,
    "id": "d00324945509fc9fe109153457512b66044e43dc",
    "semantic_title": "local-global multi-modal distillation for weakly-supervised temporal video grounding",
    "citation_count": 0,
    "authors": [
      "Peijun Bao",
      "Yong Xia",
      "Wenhan Yang",
      "Boon Poh Ng",
      "Meng Hwa Er",
      "Alex C. Kot"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27832": {
    "title": "Omnipotent Distillation with LLMs for Weakly-Supervised Natural Language Video Localization: When Divergence Meets Consistency",
    "volume": "main",
    "abstract": "Natural language video localization plays a pivotal role in video understanding, and leveraging weakly-labeled data is considered a promising approach to circumvent the laborintensive process of manual annotations. However, this approach encounters two significant challenges: 1) limited input distribution, namely that the limited writing styles of the language query, annotated by human annotators, hinder the model's generalization to real-world scenarios with diverse vocabularies and sentence structures; 2) the incomplete ground truth, whose supervision guidance is insufficient. To overcome these challenges, we propose an omnipotent distillation algorithm with large language models (LLM). The distribution of the input sample is enriched to obtain diverse multi-view versions while a consistency then comes to regularize the consistency of their results for distillation. Specifically, we first train our teacher model with the proposed intra-model agreement, where multiple sub-models are supervised by each other. Then, we leverage the LLM to paraphrase the language query and distill the teacher model to a lightweight student model by enforcing the consistency between the localization results of the paraphrased sentence and the original one. In addition, to assess the generalization of the model across different dimensions of language variation, we create extensive datasets by building upon existing datasets. Our experiments demonstrate substantial performance improvements adaptively to diverse kinds of language queries",
    "checked": true,
    "id": "ea0da979d3232ddaf922c11a04a1d0dfda04661f",
    "semantic_title": "omnipotent distillation with llms for weakly-supervised natural language video localization: when divergence meets consistency",
    "citation_count": 0,
    "authors": [
      "Peijun Bao",
      "Zihao Shao",
      "Wenhan Yang",
      "Boon Poh Ng",
      "Meng Hwa Er",
      "Alex C. Kot"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27833": {
    "title": "Improving Diffusion-Based Image Restoration with Error Contraction and Error Correction",
    "volume": "main",
    "abstract": "Generative diffusion prior captured from the off-the-shelf denoising diffusion generative model has recently attained significant interest. However, several attempts have been made to adopt diffusion models to noisy inverse problems either fail to achieve satisfactory results or require a few thousand iterations to achieve high-quality reconstructions. In this work, we propose a diffusion-based image restoration with error contraction and error correction (DiffECC) method. Two strategies are introduced to contract the restoration error in the posterior sampling process. First, we combine existing CNN-based approaches with diffusion models to ensure data consistency from the beginning. Second, to amplify the error contraction effects of the noise, a restart sampling algorithm is designed. In the error correction strategy, the estimation-correction idea is proposed on both the data term and the prior term. Solving them iteratively within the diffusion sampling framework leads to superior image generation results. Experimental results for image restoration tasks such as super-resolution (SR), Gaussian deblurring, and motion deblurring demonstrate that our approach can reconstruct high-quality images compared with state-of-the-art sampling-based diffusion models",
    "checked": true,
    "id": "5f417528149d7a5e1a00acb24b9d63cd7f7e123b",
    "semantic_title": "improving diffusion-based image restoration with error contraction and error correction",
    "citation_count": 1,
    "authors": [
      "Qiqi Bao",
      "Zheng Hui",
      "Rui Zhu",
      "Peiran Ren",
      "Xuansong Xie",
      "Wenming Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27834": {
    "title": "Relevant Intrinsic Feature Enhancement Network for Few-Shot Semantic Segmentation",
    "volume": "main",
    "abstract": "For few-shot semantic segmentation, the primary task is to extract class-specific intrinsic information from limited labeled data. However, the semantic ambiguity and inter-class similarity of previous methods limit the accuracy of pixel-level foreground-background classification. To alleviate these issues, we propose the Relevant Intrinsic Feature Enhancement Network (RiFeNet). To improve the semantic consistency of foreground instances, we propose an unlabeled branch as an efficient data utilization method, which teaches the model how to extract intrinsic features robust to intra-class differences. Notably, during testing, the proposed unlabeled branch is excluded without extra unlabeled data and computation. Furthermore, we extend the inter-class variability between foreground and background by proposing a novel multi-level prototype generation and interaction module. The different-grained complementarity between global and local prototypes allows for better distinction between similar categories. The qualitative and quantitative performance of RiFeNet surpasses the state-of-the-art methods on PASCAL-5i and COCO benchmarks",
    "checked": true,
    "id": "b4103ca231ce058122304b64d62aa67de8e32e70",
    "semantic_title": "relevant intrinsic feature enhancement network for few-shot semantic segmentation",
    "citation_count": 1,
    "authors": [
      "Xiaoyi Bao",
      "Jie Qin",
      "Siyang Sun",
      "Xingang Wang",
      "Yun Zheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27835": {
    "title": "Image Safeguarding: Reasoning with Conditional Vision Language Model and Obfuscating Unsafe Content Counterfactually",
    "volume": "main",
    "abstract": "Social media platforms are being increasingly used by malicious actors to share unsafe content, such as images depicting sexual activity, cyberbullying, and self-harm. Consequently, major platforms use artificial intelligence (AI) and human moderation to obfuscate such images to make them safer. Two critical needs for obfuscating unsafe images is that an accurate rationale for obfuscating image regions must be provided, and the sensitive regions should be obfuscated (e.g. blurring) for users' safety. This process involves addressing two key problems: (1) the reason for obfuscating unsafe images demands the platform to provide an accurate rationale that must be grounded in unsafe image-specific attributes, and (2) the unsafe regions in the image must be minimally obfuscated while still depicting the safe regions. In this work, we address these key issues by first performing visual reasoning by designing a visual reasoning model (VLM) conditioned on pre-trained unsafe image classifiers to provide an accurate rationale grounded in unsafe image attributes, and then proposing a counterfactual explanation algorithm that minimally identifies and obfuscates unsafe regions for safe viewing, by first utilizing an unsafe image classifier attribution matrix to guide segmentation for a more optimal subregion segmentation followed by an informed greedy search to determine the minimum number of subregions required to modify the classifier's output based on attribution score. Extensive experiments on uncurated data from social networks emphasize the efficacy of our proposed method. We make our code available at: https://github.com/SecureAIAutonomyLab/ConditionalVLM",
    "checked": true,
    "id": "dc8a42d67258d6811506406056406fe31800bbed",
    "semantic_title": "image safeguarding: reasoning with conditional vision language model and obfuscating unsafe content counterfactually",
    "citation_count": 1,
    "authors": [
      "Mazal Bethany",
      "Brandon Wherry",
      "Nishant Vishwamitra",
      "Peyman  Najafirad"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27836": {
    "title": "DanceAnyWay: Synthesizing Beat-Guided 3D Dances with Randomized Temporal Contrastive Learning",
    "volume": "main",
    "abstract": "We present DanceAnyWay, a generative learning method to synthesize beat-guided dances of 3D human characters synchronized with music. Our method learns to disentangle the dance movements at the beat frames from the dance movements at all the remaining frames by operating at two hierarchical levels. At the coarser \"beat\" level, it encodes the rhythm, pitch, and melody information of the input music via dedicated feature representations only at the beat frames. It leverages them to synthesize the beat poses of the target dances using a sequence-to-sequence learning framework. At the finer \"repletion\" level, our method encodes similar rhythm, pitch, and melody information from all the frames of the input music via dedicated feature representations. It generates the full dance sequences by combining the synthesized beat and repletion poses and enforcing plausibility through an adversarial learning framework. Our training paradigm also enforces fine-grained diversity in the synthesized dances through a randomized temporal contrastive loss, which ensures different segments of the dance sequences have different movements and avoids motion freezing or collapsing to repetitive movements. We evaluate the performance of our approach through extensive experiments on the benchmark AIST++ dataset and observe improvements of about 7%-12% in motion quality metrics and 1.5%-4% in motion diversity metrics over the current baselines, respectively. We also conducted a user study to evaluate the visual quality of our synthesized dances. We noted that, on average, the samples generated by our method were about 9-48% more preferred by the participants and had a 4-27% better five-point Likert-scale score over the best available current baseline in terms of motion quality and synchronization. Our source code and project page are available at https://github.com/aneeshbhattacharya/DanceAnyWay",
    "checked": true,
    "id": "c5381cc8c5eb95fa67afcae1e3849bda186e296d",
    "semantic_title": "danceanyway: synthesizing beat-guided 3d dances with randomized temporal contrastive learning",
    "citation_count": 1,
    "authors": [
      "Aneesh Bhattacharya",
      "Manas Paranjape",
      "Uttaran Bhattacharya",
      "Aniket Bera"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27837": {
    "title": "DiffSED: Sound Event Detection with Denoising Diffusion",
    "volume": "main",
    "abstract": "Sound Event Detection (SED) aims to predict the temporal boundaries of all the events of interest and their class labels, given an unconstrained audio sample. Taking either the split-and-classify (i.e., frame-level) strategy or the more principled event-level modeling approach, all existing methods consider the SED problem from the discriminative learning perspective. In this work, we reformulate the SED problem by taking a generative learning perspective. Specifically, we aim to generate sound temporal boundaries from noisy proposals in a denoising diffusion process, conditioned on a target audio sample. During training, our model learns to reverse the noising process by converting noisy latent queries to the ground-truth versions in the elegant Transformer decoder framework. Doing so enables the model generate accurate event boundaries from even noisy queries during inference. Extensive experiments on the Urban-SED and EPIC-Sounds datasets demonstrate that our model significantly outperforms existing alternatives, with 40+% faster convergence in training. Code: https://github.com/Surrey-UPLab/DiffSED",
    "checked": true,
    "id": "33cd70c6e1cd4c9b504d40c9f170ca95af479272",
    "semantic_title": "diffsed: sound event detection with denoising diffusion",
    "citation_count": 3,
    "authors": [
      "Swapnil Bhosale",
      "Sauradip Nag",
      "Diptesh Kanojia",
      "Jiankang Deng",
      "Xiatian Zhu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27838": {
    "title": "Learning Generalized Segmentation for Foggy-Scenes by Bi-directional Wavelet Guidance",
    "volume": "main",
    "abstract": "Learning scene semantics that can be well generalized to foggy conditions is important for safety-crucial applications such as autonomous driving. Existing methods need both annotated clear images and foggy images to train a curriculum domain adaptation model. Unfortunately, these methods can only generalize to the target foggy domain that has seen in the training stage, but the foggy domains vary a lot in both urban-scene styles and fog styles. In this paper, we propose to learn scene segmentation well generalized to foggy-scenes under the domain generalization setting, which does not involve any foggy images in the training stage and can generalize to any arbitrary unseen foggy scenes. We argue that an ideal segmentation model that can be well generalized to foggy-scenes need to simultaneously enhance the content, de-correlate the urban-scene style and de-correlate the fog style. As the content (e.g., scene semantic) rests more in low-frequency features while the style of urban-scene and fog rests more in high-frequency features, we propose a novel bi-directional wavelet guidance (BWG) mechanism to realize the above three objectives in a divide-and-conquer manner. With the aid of Haar wavelet transformation, the low frequency component is concentrated on the content enhancement self-attention, while the high frequency component is shifted to the style and fog self-attention for de-correlation purpose. It is integrated into existing mask-level Transformer segmentation pipelines in a learnable fashion. Large-scale experiments are conducted on four foggy-scene segmentation datasets under a variety of interesting settings. The proposed method significantly outperforms existing directly-supervised, curriculum domain adaptation and domain generalization segmentation methods. Source code is available at https://github.com/BiQiWHU/BWG",
    "checked": true,
    "id": "337c08ff7f17f03fffb6acb82be1a12982fe003a",
    "semantic_title": "learning generalized segmentation for foggy-scenes by bi-directional wavelet guidance",
    "citation_count": 1,
    "authors": [
      "Qi Bi",
      "Shaodi You",
      "Theo Gevers"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27839": {
    "title": "Learning Generalized Medical Image Segmentation from Decoupled Feature Queries",
    "volume": "main",
    "abstract": "Domain generalized medical image segmentation requires models to learn from multiple source domains and generalize well to arbitrary unseen target domain. Such a task is both technically challenging and clinically practical, due to the domain shift problem (i.e., images are collected from different hospitals and scanners). Existing methods focused on either learning shape-invariant representation or reaching consensus among the source domains. An ideal generalized representation is supposed to show similar pattern responses within the same channel for cross-domain images. However, to deal with the significant distribution discrepancy, the network tends to capture similar patterns by multiple channels, while different cross-domain patterns are also allowed to rest in the same channel. To address this issue, we propose to leverage channel-wise decoupled deep features as queries. With the aid of cross-attention mechanism, the long-range dependency between deep and shallow features can be fully mined via self-attention and then guides the learning of generalized representation. Besides, a relaxed deep whitening transformation is proposed to learn channel-wise decoupled features in a feasible way. The proposed decoupled fea- ture query (DFQ) scheme can be seamlessly integrate into the Transformer segmentation model in an end-to-end manner. Extensive experiments show its state-of-the-art performance, notably outperforming the runner-up by 1.31% and 1.98% with DSC metric on generalized fundus and prostate benchmarks, respectively. Source code is available at https://github.com/BiQiWHU/DFQ",
    "checked": true,
    "id": "daea96370b8cb1ec555f2610307de83c8a9ee1bf",
    "semantic_title": "learning generalized medical image segmentation from decoupled feature queries",
    "citation_count": 0,
    "authors": [
      "Qi Bi",
      "Jingjun Yi",
      "Hao Zheng",
      "Wei Ji",
      "Yawen Huang",
      "Yuexiang Li",
      "Yefeng Zheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27840": {
    "title": "Learning Content-Enhanced Mask Transformer for Domain Generalized Urban-Scene Segmentation",
    "volume": "main",
    "abstract": "Domain-generalized urban-scene semantic segmentation (USSS) aims to learn generalized semantic predictions across diverse urban-scene styles. Unlike generic domain gap challenges, USSS is unique in that the semantic categories are often similar in different urban scenes, while the styles can vary significantly due to changes in urban landscapes, weather conditions, lighting, and other factors. Existing approaches typically rely on convolutional neural networks (CNNs) to learn the content of urban scenes. In this paper, we propose a Content-enhanced Mask TransFormer (CMFormer) for domain-generalized USSS. The main idea is to enhance the focus of the fundamental component, the mask attention mechanism, in Transformer segmentation models on content information. We have observed through empirical analysis that a mask representation effectively captures pixel segments, albeit with reduced robustness to style variations. Conversely, its lower-resolution counterpart exhibits greater ability to accommodate style variations, while being less proficient in representing pixel segments. To harness the synergistic attributes of these two approaches, we introduce a novel content-enhanced mask attention mechanism. It learns mask queries from both the image feature and its down-sampled counterpart, aiming to simultaneously encapsulate the content and address stylistic variations. These features are fused into a Transformer decoder and integrated into a multi-resolution content-enhanced mask attention learning scheme. Extensive experiments conducted on various domain-generalized urban-scene segmentation datasets demonstrate that the proposed CMFormer significantly outperforms existing CNN-based methods by up to 14.0% mIoU and the contemporary HGFormer by up to 1.7% mIoU. The source code is publicly available at https://github.com/BiQiWHU/CMFormer",
    "checked": true,
    "id": "efc16345ea3f7fef1a823486ce0c26a3d2c9ccaf",
    "semantic_title": "learning content-enhanced mask transformer for domain generalized urban-scene segmentation",
    "citation_count": 7,
    "authors": [
      "Qi Bi",
      "Shaodi You",
      "Theo Gevers"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27841": {
    "title": "ShapeBoost: Boosting Human Shape Estimation with Part-Based Parameterization and Clothing-Preserving Augmentation",
    "volume": "main",
    "abstract": "Accurate human shape recovery from a monocular RGB image is a challenging task because humans come in different shapes and sizes and wear different clothes. In this paper, we propose ShapeBoost, a new human shape recovery framework that achieves pixel-level alignment even for rare body shapes and high accuracy for people wearing different types of clothes. Unlike previous approaches that rely on the use of PCA-based shape coefficients, we adopt a new human shape parameterization that decomposes the human shape into bone lengths and the mean width of each part slice. This part-based parameterization technique achieves a balance between flexibility and validity using a semi-analytical shape reconstruction algorithm. Based on this new parameterization, a clothing-preserving data augmentation module is proposed to generate realistic images with diverse body shapes and accurate annotations. Experimental results show that our method outperforms other state-of-the-art methods in diverse body shape situations as well as in varied clothing situations",
    "checked": true,
    "id": "1890ed951ffdcf46e93a31c79fe6e882f3c4c5ce",
    "semantic_title": "shapeboost: boosting human shape estimation with part-based parameterization and clothing-preserving augmentation",
    "citation_count": 0,
    "authors": [
      "Siyuan Bian",
      "Jiefeng Li",
      "Jiasheng Tang",
      "Cewu Lu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27842": {
    "title": "MICA: Towards Explainable Skin Lesion Diagnosis via Multi-Level Image-Concept Alignment",
    "volume": "main",
    "abstract": "Black-box deep learning approaches have showcased significant potential in the realm of medical image analysis. However, the stringent trustworthiness requirements intrinsic to the medical field have catalyzed research into the utilization of Explainable Artificial Intelligence (XAI), with a particular focus on concept-based methods. Existing concept-based methods predominantly apply concept annotations from a single perspective (e.g., global level), neglecting the nuanced semantic relationships between sub-regions and concepts embedded within medical images. This leads to underutilization of the valuable medical information and may cause models to fall short in harmoniously balancing interpretability and performance when employing inherently interpretable architectures such as Concept Bottlenecks. To mitigate these shortcomings, we propose a multi-modal explainable disease diagnosis framework that meticulously aligns medical images and clinical-related concepts semantically at multiple strata, encompassing the image level, token level, and concept level. Moreover, our method allows for model intervention and offers both textual and visual explanations in terms of human-interpretable concepts. Experimental results on three skin image datasets demonstrate that our method, while preserving model interpretability, attains high performance and label efficiency for concept detection and disease diagnosis. The code is available at https://github.com/Tommy-Bie/MICA",
    "checked": true,
    "id": "445a5d47b0cc0958ab4d539c829fd8d4400ce8f3",
    "semantic_title": "mica: towards explainable skin lesion diagnosis via multi-level image-concept alignment",
    "citation_count": 3,
    "authors": [
      "Yequan Bie",
      "Luyang Luo",
      "Hao Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27843": {
    "title": "VIXEN: Visual Text Comparison Network for Image Difference Captioning",
    "volume": "main",
    "abstract": "We present VIXEN - a technique that succinctly summarizes in text the visual differences between a pair of images in order to highlight any content manipulation present. Our proposed network linearly maps image features in a pairwise manner, constructing a soft prompt for a pretrained large language model. We address the challenge of low volume of training data and lack of manipulation variety in existing image difference captioning (IDC) datasets by training on synthetically manipulated images from the recent InstructPix2Pix dataset generated via prompt-to-prompt editing framework. We augment this dataset with change summaries produced via GPT-3. We show that VIXEN produces state-of-the-art, comprehensible difference captions for diverse image contents and edit types, offering a potential mitigation against misinformation disseminated via manipulated image content. Code and data are available at http://github.com/alexblck/vixen",
    "checked": true,
    "id": "f478e096d7802179765f5bc0d4f46da2f82b816d",
    "semantic_title": "vixen: visual text comparison network for image difference captioning",
    "citation_count": 2,
    "authors": [
      "Alexander Black",
      "Jing Shi",
      "Yifei Fan",
      "Tu Bui",
      "John Collomosse"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27844": {
    "title": "SRFormer: Text Detection Transformer with Incorporated Segmentation and Regression",
    "volume": "main",
    "abstract": "Existing techniques for text detection can be broadly classified into two primary groups: segmentation-based and regression-based methods. Segmentation models offer enhanced robustness to font variations but require intricate post-processing, leading to high computational overhead. Regression-based methods undertake instance-aware prediction but face limitations in robustness and data efficiency due to their reliance on high-level representations. In our academic pursuit, we propose SRFormer, a unified DETR-based model with amalgamated Segmentation and Regression, aiming at the synergistic harnessing of the inherent robustness in segmentation representations, along with the straightforward post-processing of instance-level regression. Our empirical analysis indicates that favorable segmentation predictions can be obtained at the initial decoder layers. In light of this, we constrain the incorporation of segmentation branches to the first few decoder layers and employ progressive regression refinement in subsequent layers, achieving performance gains while minimizing computational load from the mask. Furthermore, we propose a Mask-informed Query Enhancement module. We take the segmentation result as a natural soft-ROI to pool and extract robust pixel representations, which are then employed to enhance and diversify instance queries. Extensive experimentation across multiple benchmarks has yielded compelling findings, highlighting our method's exceptional robustness, superior training and data efficiency, as well as its state-of-the-art performance. Our code is available at https://github.com/retsuh-bqw/SRFormer-Text-Det",
    "checked": true,
    "id": "6ecf9ea86f4187c3438c87e5e9774abbaf9c83b3",
    "semantic_title": "srformer: text detection transformer with incorporated segmentation and regression",
    "citation_count": 0,
    "authors": [
      "Qingwen Bu",
      "Sungrae Park",
      "Minsoo Khang",
      "Yichuan Cheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27845": {
    "title": "Orthogonal Dictionary Guided Shape Completion Network for Point Cloud",
    "volume": "main",
    "abstract": "Point cloud shape completion, which aims to reconstruct the missing regions of the incomplete point clouds with plausible shapes, is an ill-posed and challenging task that benefits many downstream 3D applications. Prior approaches achieve this goal by employing a two-stage completion framework, generating a coarse yet complete seed point cloud through an encoder-decoder network, followed by refinement and upsampling. However, the encoded features suffer from information loss of the missing portion, leading to an inability of the decoder to reconstruct seed points with detailed geometric clues. To tackle this issue, we propose a novel Orthogonal Dictionary Guided Shape Completion Network (ODGNet). The proposed ODGNet consists of a Seed Generation U-Net, which leverages multi-level feature extraction and concatenation to significantly enhance the representation capability of seed points, and Orthogonal Dictionaries that can learn shape priors from training samples and thus compensate for the information loss of the missing portions during inference. Our design is simple but to the point, extensive experiment results indicate that the proposed method can reconstruct point clouds with more details and outperform previous state-of-the-art counterparts. The implementation code is available at https://github.com/corecai163/ODGNet",
    "checked": true,
    "id": "383fc37cce012859d7a1f7d7e42bf0275e4299ea",
    "semantic_title": "orthogonal dictionary guided shape completion network for point cloud",
    "citation_count": 1,
    "authors": [
      "Pingping Cai",
      "Deja Scott",
      "Xiaoguang Li",
      "Song Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27846": {
    "title": "Spherical Pseudo-Cylindrical Representation for Omnidirectional Image Super-resolution",
    "volume": "main",
    "abstract": "Omnidirectional images have attracted significant attention in recent years due to the rapid development of virtual reality technologies. Equirectangular projection (ERP), a naive form to store and transfer omnidirectional images, however, is challenging for existing two-dimensional (2D) image super-resolution (SR) methods due to its inhomogeneous distributed sampling density and distortion across latitude. In this paper, we make one of the first attempts to design a spherical pseudo-cylindrical representation, which not only allows pixels at different latitudes to adaptively adopt the best distinct sampling density but also is model-agnostic to most off-the-shelf SR methods, enhancing their performances. Specifically, we start by upsampling each latitude of the input ERP image and design a computationally tractable optimization algorithm to adaptively obtain a (sub)-optimal sampling density for each latitude of the ERP image. Addressing the distortion of ERP, we introduce a new viewport-based training loss based on the original 3D sphere format of the omnidirectional image, which inherently lacks distortion. Finally, we present a simple yet effective recursive progressive omnidirectional SR network to showcase the feasibility of our idea. The experimental results on public datasets demonstrate the effectiveness of the proposed method as well as the consistently superior performance of our method over most state-of-the-art methods both quantitatively and qualitatively",
    "checked": true,
    "id": "64c399890dabacc78a556fe19616732864e812f8",
    "semantic_title": "spherical pseudo-cylindrical representation for omnidirectional image super-resolution",
    "citation_count": 2,
    "authors": [
      "Qing Cai",
      "Mu Li",
      "Dongwei Ren",
      "Jun Lyu",
      "Haiyong Zheng",
      "Junyu Dong",
      "Yee-Hong Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27847": {
    "title": "Disentangled Diffusion-Based 3D Human Pose Estimation with Hierarchical Spatial and Temporal Denoiser",
    "volume": "main",
    "abstract": "Recently, diffusion-based methods for monocular 3D human pose estimation have achieved state-of-the-art (SOTA) performance by directly regressing the 3D joint coordinates from the 2D pose sequence. Although some methods decompose the task into bone length and bone direction prediction based on the human anatomical skeleton to explicitly incorporate more human body prior constraints, the performance of these methods is significantly lower than that of the SOTA diffusion-based methods. This can be attributed to the tree structure of the human skeleton. Direct application of the disentangled method could amplify the accumulation of hierarchical errors, propagating through each hierarchy. Meanwhile, the hierarchical information has not been fully explored by the previous methods. To address these problems, a Disentangled Diffusion-based 3D human Pose Estimation method with Hierarchical Spatial and Temporal Denoiser is proposed, termed DDHPose. In our approach: (1) We disentangle the 3d pose and diffuse the bone length and bone direction during the forward process of the diffusion model to effectively model the human pose prior. A disentanglement loss is proposed to supervise diffusion model learning. (2) For the reverse process, we propose Hierarchical Spatial and Temporal Denoiser (HSTDenoiser) to improve the hierarchical modelling of each joint. Our HSTDenoiser comprises two components: the Hierarchical-Related Spatial Transformer (HRST) and the Hierarchical-Related Temporal Transformer (HRTT). HRST exploits joint spatial information and the influence of the parent joint on each joint for spatial modeling, while HRTT utilizes information from both the joint and its hierarchical adjacent joints to explore the hierarchical temporal correlations among joints. Extensive experiments on the Human3.6M and MPI-INF-3DHP datasets show that our method outperforms the SOTA disentangled-based, non-disentangled based, and probabilistic approaches by 10.0%, 2.0%, and 1.3%, respectively",
    "checked": true,
    "id": "4ca1c9bdbd0b4170c6e1d0799b105145e34ab3bf",
    "semantic_title": "disentangled diffusion-based 3d human pose estimation with hierarchical spatial and temporal denoiser",
    "citation_count": 0,
    "authors": [
      "Qingyuan Cai",
      "Xuecai  Hu",
      "Saihui Hou",
      "Li Yao",
      "Yongzhen Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27848": {
    "title": "Rethinking the Paradigm of Content Constraints in Unpaired Image-to-Image Translation",
    "volume": "main",
    "abstract": "In an unpaired setting, lacking sufficient content constraints for image-to-image translation (I2I) tasks, GAN-based approaches are usually prone to model collapse. Current solutions can be divided into two categories, reconstruction-based and Siamese network-based. The former requires that the transformed or transforming image can be perfectly converted back to the original image, which is sometimes too strict and limits the generative performance. The latter involves feeding the original and generated images into a feature extractor and then matching their outputs. This is not efficient enough, and a universal feature extractor is not easily available. In this paper, we propose EnCo, a simple but efficient way to maintain the content by constraining the representational similarity in the latent space of patch-level features from the same stage of the encoder and decoder of the generator. For the similarity function, we use a simple MSE loss instead of contrastive loss, which is currently widely used in I2I tasks. Benefits from the design, EnCo training is extremely efficient, while the features from the encoder produce a more positive effect on the decoding, leading to more satisfying generations. In addition, we rethink the role played by discriminators in sampling patches and propose a discriminative attention-guided (DAG) patch sampling strategy to replace random sampling. DAG is parameter-free and only requires negligible computational overhead, while significantly improving the performance of the model. Extensive experiments on multiple datasets demonstrate the effectiveness and advantages of EnCo, and we achieve multiple state-of-the-art compared to previous methods",
    "checked": true,
    "id": "44a7061092a40b93996f0f3d8a1f0d91d1949549",
    "semantic_title": "rethinking the paradigm of content constraints in unpaired image-to-image translation",
    "citation_count": 0,
    "authors": [
      "Xiuding Cai",
      "Yaoyao Zhu",
      "Dong Miao",
      "Linjie Fu",
      "Yu Yao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27849": {
    "title": "FusionFormer: A Concise Unified Feature Fusion Transformer for 3D Pose Estimation",
    "volume": "main",
    "abstract": "Depth uncertainty is a core challenge in 3D human pose estimation, especially when the camera parameters are unknown. Previous methods try to reduce the impact of depth uncertainty by multi-view and/or multi-frame feature fusion to utilize more spatial and temporal information. However, they generally lead to marginal improvements and their performance still cannot match the camera-parameter-required methods. The reason is that their handcrafted fusion schemes cannot fuse the features flexibly, e.g., the multi-view and/or multi-frame features are fused separately. Moreover, the diverse and complicated fusion schemes make the principle for developing effective fusion schemes unclear and also raises an open problem that whether there exist more simple and elegant fusion schemes. To address these issues, this paper proposes an extremely concise unified feature fusion transformer (FusionFormer) with minimized handcrafted design for 3D pose estimation. FusionFormer fuses both the multi-view and multi-frame features in a unified fusion scheme, in which all the features are accessible to each other and thus can be fused flexibly. Experimental results on several mainstream datasets demonstrate that FusionFormer achieves state-of-the-art performance. To our best knowledge, this is the first camera-parameter-free method to outperform the existing camera-parameter-required methods, revealing the tremendous potential of camera-parameter-free models. These impressive experimental results together with our concise feature fusion scheme resolve the above open problem. Another appealing feature of FusionFormer we observe is that benefiting from its effective fusion scheme, we can achieve impressive performance with smaller model size and less FLOPs",
    "checked": true,
    "id": "f618e5599d5ed5e43cf76b418abe764cb341229c",
    "semantic_title": "fusionformer: a concise unified feature fusion transformer for 3d pose estimation",
    "citation_count": 1,
    "authors": [
      "Yanlu Cai",
      "Weizhong Zhang",
      "Yuan Wu",
      "Cheng Jin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27850": {
    "title": "Decoupled Textual Embeddings for Customized Image Generation",
    "volume": "main",
    "abstract": "Customized text-to-image generation, which aims to learn user-specified concepts with a few images, has drawn significant attention recently. However, existing methods usually suffer from overfitting issues and entangle the subject-unrelated information (e.g., background and pose) with the learned concept, limiting the potential to compose concept into new scenes. To address these issues, we propose the DETEX, a novel approach that learns the disentangled concept embedding for flexible customized text-to-image generation. Unlike conventional methods that learn a single concept embedding from the given images, our DETEX represents each image using multiple word embeddings during training, i.e., a learnable image-shared subject embedding and several image-specific subject-unrelated embeddings. To decouple irrelevant attributes (i.e., background and pose) from the subject embedding, we further present several attribute mappers that encode each image as several image-specific subject-unrelated embeddings. To encourage these unrelated embeddings to capture the irrelevant information, we incorporate them with corresponding attribute words and propose a joint training strategy to facilitate the disentanglement. During inference, we only use the subject embedding for image generation, while selectively using image-specific embeddings to retain image-specified attributes. Extensive experiments demonstrate that the subject embedding obtained by our method can faithfully represent the target concept, while showing superior editability compared to the state-of-the-art methods. Our code will be available at https://github.com/PrototypeNx/DETEX",
    "checked": true,
    "id": "f2832c8404f9dd823667e2cc6ffd39076c473369",
    "semantic_title": "decoupled textual embeddings for customized image generation",
    "citation_count": 7,
    "authors": [
      "Yufei Cai",
      "Yuxiang Wei",
      "Zhilong Ji",
      "Jinfeng Bai",
      "Hu Han",
      "Wangmeng Zuo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27851": {
    "title": "Disguise without Disruption: Utility-Preserving Face De-identification",
    "volume": "main",
    "abstract": "With the rise of cameras and smart sensors, humanity generates an exponential amount of data. This valuable information, including underrepresented cases like AI in medical settings, can fuel new deep-learning tools. However, data scientists must prioritize ensuring privacy for individuals in these untapped datasets, especially for images or videos with faces, which are prime targets for identification methods. Proposed solutions to de-identify such images often compromise non-identifying facial attributes relevant to downstream tasks. In this paper, we introduce Disguise, a novel algorithm that seamlessly de-identifies facial images while ensuring the usability of the modified data. Unlike previous approaches, our solution is firmly grounded in the domains of differential privacy and ensemble-learning research. Our method involves extracting and substituting depicted identities with synthetic ones, generated using variational mechanisms to maximize obfuscation and non-invertibility. Additionally, we leverage supervision from a mixture-of-experts to disentangle and preserve other utility attributes. We extensively evaluate our method using multiple datasets, demonstrating a higher de-identification rate and superior consistency compared to prior approaches in various downstream tasks",
    "checked": true,
    "id": "f866c0e457496fe5855b229ff9ba96972a5efa0c",
    "semantic_title": "disguise without disruption: utility-preserving face de-identification",
    "citation_count": 2,
    "authors": [
      "Zikui Cai",
      "Zhongpai Gao",
      "Benjamin Planche",
      "Meng Zheng",
      "Terrence Chen",
      "M. Salman Asif",
      "Ziyan Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27852": {
    "title": "Bi-directional Adapter for Multimodal Tracking",
    "volume": "main",
    "abstract": "Due to the rapid development of computer vision, single-modal (RGB) object tracking has made significant progress in recent years. Considering the limitation of single imaging sensor, multi-modal images (RGB, infrared, etc.) are introduced to compensate for this deficiency for all-weather object tracking in complex environments. However, as acquiring sufficient multi-modal tracking data is hard while the dominant modality changes with the open environment, most existing techniques fail to extract multi-modal complementary information dynamically, yielding unsatisfactory tracking performance. To handle this problem, we propose a novel multi-modal visual prompt tracking model based on a universal bi-directional adapter, cross-prompting multiple modalities mutually. Our model consists of a universal bi-directional adapter and multiple modality-specific transformer encoder branches with sharing parameters. The encoders extract features of each modality separately by using a frozen, pre-trained foundation model. We develop a simple but effective light feature adapter to transfer modality-specific information from one modality to another, performing visual feature prompt fusion in an adaptive manner. With adding fewer (0.32M) trainable parameters, our model achieves superior tracking performance in comparison with both the full fine-tuning methods and the prompt learning-based methods. Our code is available: https://github.com/SparkTempest/BAT",
    "checked": true,
    "id": "2e80336f5a72a47ca2c3c8fdd7a9dc3ab5253140",
    "semantic_title": "bi-directional adapter for multimodal tracking",
    "citation_count": 3,
    "authors": [
      "Bing Cao",
      "Junliang Guo",
      "Pengfei Zhu",
      "Qinghua Hu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27853": {
    "title": "Domain-Controlled Prompt Learning",
    "volume": "main",
    "abstract": "Large pre-trained vision-language models, such as CLIP, have shown remarkable generalization capabilities across various tasks when appropriate text prompts are provided. However, adapting these models to specific domains, like remote sensing images (RSIs), medical images, etc, remains unexplored and challenging. Existing prompt learning methods often lack domain-awareness or domain-transfer mechanisms, leading to suboptimal performance due to the misinterpretation of specific images in natural image patterns. To tackle this dilemma, we proposed a Domain-Controlled Prompt Learning for the specific domains. Specifically, the large-scale specific domain foundation model (LSDM) is first introduced to provide essential specific domain knowledge. Using lightweight neural networks, we transfer this knowledge into domain biases, which control both the visual and language branches to obtain domain-adaptive prompts in a directly incorporating manner. Simultaneously, to overcome the existing overfitting challenge, we propose a novel noisy-adding strategy, without extra trainable parameters, to help the model escape the suboptimal solution in a global domain oscillation manner. Experimental results show our method achieves state-of-the-art performance in specific domain image recognition datasets. Our code is available at https://github.com/caoql98/DCPL",
    "checked": true,
    "id": "b8874ac74a5de32522e7e93c7541ca229ad89f0d",
    "semantic_title": "domain-controlled prompt learning",
    "citation_count": 3,
    "authors": [
      "Qinglong Cao",
      "Zhengqin Xu",
      "Yuntian Chen",
      "Chao Ma",
      "Xiaokang Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27854": {
    "title": "LogoStyleFool: Vitiating Video Recognition Systems via Logo Style Transfer",
    "volume": "main",
    "abstract": "Video recognition systems are vulnerable to adversarial examples. Recent studies show that style transfer-based and patch-based unrestricted perturbations can effectively improve attack efficiency. These attacks, however, face two main challenges: 1) Adding large stylized perturbations to all pixels reduces the naturalness of the video and such perturbations can be easily detected. 2) Patch-based video attacks are not extensible to targeted attacks due to the limited search space of reinforcement learning that has been widely used in video attacks recently. In this paper, we focus on the video black-box setting and propose a novel attack framework named LogoStyleFool by adding a stylized logo to the clean video. We separate the attack into three stages: style reference selection, reinforcement-learning-based logo style transfer, and perturbation optimization. We solve the first challenge by scaling down the perturbation range to a regional logo, while the second challenge is addressed by complementing an optimization stage after reinforcement learning. Experimental results substantiate the overall superiority of LogoStyleFool over three state-of-the-art patch-based attacks in terms of attack performance and semantic preservation. Meanwhile, LogoStyleFool still maintains its performance against two existing patch-based defense methods. We believe that our research is beneficial in increasing the attention of the security community to such subregional style transfer attacks",
    "checked": true,
    "id": "217c6a8d5e872964b19f25a4247fe936103c027a",
    "semantic_title": "logostylefool: vitiating video recognition systems via logo style transfer",
    "citation_count": 2,
    "authors": [
      "Yuxin Cao",
      "Ziyu Zhao",
      "Xi Xiao",
      "Derui Wang",
      "Minhui Xue",
      "Jin Lu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27855": {
    "title": "Descanning: From Scanned to the Original Images with a Color Correction Diffusion Model",
    "volume": "main",
    "abstract": "A significant volume of analog information, i.e., documents and images, have been digitized in the form of scanned copies for storing, sharing, and/or analyzing in the digital world. However, the quality of such contents is severely degraded by various distortions caused by printing, storing, and scanning processes in the physical world. Although restoring high-quality content from scanned copies has become an indispensable task for many products, it has not been systematically explored, and to the best of our knowledge, no public datasets are available. In this paper, we define this problem as Descanning and introduce a new high-quality and large-scale dataset named DESCAN-18K. It contains 18K pairs of original and scanned images collected in the wild containing multiple complex degradations. In order to eliminate such complex degradations, we propose a new image restoration model called DescanDiffusion consisting of a color encoder that corrects the global color degradation and a conditional denoising diffusion probabilistic model (DDPM) that removes local degradations. To further improve the generalization ability of DescanDiffusion, we also design a synthetic data generation scheme by reproducing prominent degradations in scanned images. We demonstrate that our DescanDiffusion outperforms other baselines including commercial restoration products, objectively and subjectively, via comprehensive experiments and analyses",
    "checked": true,
    "id": "e2e493678e6bda55c1c44114bbd58c3b863a8dc7",
    "semantic_title": "descanning: from scanned to the original images with a color correction diffusion model",
    "citation_count": 0,
    "authors": [
      "Junghun Cha",
      "Ali Haider",
      "Seoyun Yang",
      "Hoeyeong Jin",
      "Subin Yang",
      "A. F. M. Shahab Uddin",
      "Jaehyoung Kim",
      "Soo Ye Kim",
      "Sung-Ho Bae"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27856": {
    "title": "Fine Structure-Aware Sampling: A New Sampling Training Scheme for Pixel-Aligned Implicit Models in Single-View Human Reconstruction",
    "volume": "main",
    "abstract": "Pixel-aligned implicit models, such as PIFu, PIFuHD, and ICON, are used for single-view clothed human reconstruction. These models need to be trained using a sampling training scheme. Existing sampling training schemes either fail to capture thin surfaces (e.g. ears, fingers) or cause noisy artefacts in reconstructed meshes. To address these problems, we introduce Fine Structured-Aware Sampling (FSS), a new sampling training scheme to train pixel-aligned implicit models for single-view human reconstruction. FSS resolves the aforementioned problems by proactively adapting to the thickness and complexity of surfaces. In addition, unlike existing sampling training schemes, FSS shows how normals of sample points can be capitalized in the training process to improve results. Lastly, to further improve the training process, FSS proposes a mesh thickness loss signal for pixel-aligned implicit models. It becomes computationally feasible to introduce this loss once a slight reworking of the pixel-aligned implicit function framework is carried out. Our results show that our methods significantly outperform SOTA methods qualitatively and quantitatively. Our code is publicly available at https://github.com/kcyt/FSS",
    "checked": true,
    "id": "f75695db8893b38d9bd61f6901f8524ee9ec9645",
    "semantic_title": "fine structure-aware sampling: a new sampling training scheme for pixel-aligned implicit models in single-view human reconstruction",
    "citation_count": 1,
    "authors": [
      "Kennard Yanting Chan",
      "Fayao Liu",
      "Guosheng Lin",
      "Chuan Sheng Foo",
      "Weisi Lin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27857": {
    "title": "CMDA: Cross-Modal and Domain Adversarial Adaptation for LiDAR-Based 3D Object Detection",
    "volume": "main",
    "abstract": "Recent LiDAR-based 3D Object Detection (3DOD) methods show promising results, but they often do not generalize well to target domains outside the source (or training) data distribution. To reduce such domain gaps and thus to make 3DOD models more generalizable, we introduce a novel unsupervised domain adaptation (UDA) method, called CMDA, which (i) leverages visual semantic cues from an image modality (i.e., camera images) as an effective semantic bridge to close the domain gap in the cross-modal Bird's Eye View (BEV) representations. Further, (ii) we also introduce a self-training-based learning strategy, wherein a model is adversarially trained to generate domain-invariant features, which disrupt the discrimination of whether a feature instance comes from a source or an unseen target domain. Overall, our CMDA framework guides the 3DOD model to generate highly informative and domain-adaptive features for novel data distributions. In our extensive experiments with large-scale benchmarks, such as nuScenes, Waymo, and KITTI, those mentioned above provide significant performance gains for UDA tasks, achieving state-of-the-art performance",
    "checked": true,
    "id": "6fb888c542dd50d28b78c71df0d13dcb2148d8e6",
    "semantic_title": "cmda: cross-modal and domain adversarial adaptation for lidar-based 3d object detection",
    "citation_count": 0,
    "authors": [
      "Gyusam Chang",
      "Wonseok Roh",
      "Sujin Jang",
      "Dongwook Lee",
      "Daehyun Ji",
      "Gyeongrok Oh",
      "Jinsun Park",
      "Jinkyu Kim",
      "Sangpil Kim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27858": {
    "title": "A Hybrid Global-Local Perception Network for Lane Detection",
    "volume": "main",
    "abstract": "Lane detection is a critical task in autonomous driving, which requires accurately predicting the complex topology of lanes in various scenarios. While previous methods of lane detection have shown success, challenges still exist, especially in scenarios where lane markings are absent. In this paper, we analyze the role of global and local features in accurately detecting lanes and propose a Hybrid Global-Local Perception Network (HGLNet) to leverage them. Global and local features play distinct roles in lane detection by respectively aiding in the detection of lane instances and the localization of corresponding lanes. HGLNet extracts global semantic context by utilizing a global extraction head that aggregates information about adaptive sampling points around lanes, achieving an optimal trade-off between performance and efficiency. Moreover, we introduce a Multi-hierarchy feature aggregator (MFA) to capture feature hierarchies in both regional and local ranges, elevating the representation of local features. The proposed Hybrid architecture can simultaneously focus on global and local features at different depth levels and efficiently integrate them to sense the global presence of lanes and accurately regress their locations. Experimental results demonstrate that our proposed method improves detection accuracy in various challenging scenarios, outperforming the state-of-the-art lane detection methods",
    "checked": true,
    "id": "248aec86f74aac91cb056fd803ede9f6e0beb3e8",
    "semantic_title": "a hybrid global-local perception network for lane detection",
    "citation_count": 0,
    "authors": [
      "Qing Chang",
      "Yifei Tong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27859": {
    "title": "Improving Robustness for Joint Optimization of Camera Pose and Decomposed Low-Rank Tensorial Radiance Fields",
    "volume": "main",
    "abstract": "In this paper, we propose an algorithm that allows joint refinement of camera pose and scene geometry represented by decomposed low-rank tensor, using only 2D images as supervision. First, we conduct a pilot study based on a 1D signal and relate our findings to 3D scenarios, where the naive joint pose optimization on voxel-based NeRFs can easily lead to sub-optimal solutions. Moreover, based on the analysis of the frequency spectrum, we propose to apply convolutional Gaussian filters on 2D and 3D radiance fields for a coarse-to-fine training schedule that enables joint camera pose optimization. Leveraging the decomposition property in decomposed low-rank tensor, our method achieves an equivalent effect to brute-force 3D convolution with only incurring little computational overhead. To further improve the robustness and stability of joint optimization, we also propose techniques of smoothed 2D supervision, randomly scaled kernel parameters, and edge-guided loss mask. Extensive quantitative and qualitative evaluations demonstrate that our proposed framework achieves superior performance in novel view synthesis as well as rapid convergence for optimization. The source code is available at https://github.com/Nemo1999/Joint-TensoRF",
    "checked": false,
    "id": "874841f6f3f52c04c11ae27181b9b4e05273d8a5",
    "semantic_title": "improving robustness for joint optimization of camera poses and decomposed low-rank tensorial radiance fields",
    "citation_count": 1,
    "authors": [
      "Bo-Yu Chen",
      "Wei-Chen Chiu",
      "Yu-Lun Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27860": {
    "title": "Sketch and Refine: Towards Fast and Accurate Lane Detection",
    "volume": "main",
    "abstract": "Lane detection is to determine the precise location and shape of lanes on the road. Despite efforts made by current methods, it remains a challenging task due to the complexity of real-world scenarios. Existing approaches, whether proposal-based or keypoint-based, suffer from depicting lanes effectively and efficiently. Proposal-based methods detect lanes by distinguishing and regressing a collection of proposals in a streamlined top-down way, yet lack sufficient flexibility in lane representation. Keypoint-based methods, on the other hand, construct lanes flexibly from local descriptors, which typically entail complicated post-processing. In this paper, we present a \"Sketch-and-Refine\" paradigm that utilizes the merits of both keypoint-based and proposal-based methods. The motivation is that local directions of lanes are semantically simple and clear. At the \"Sketch\" stage, local directions of keypoints can be easily estimated by fast convolutional layers. Then we can build a set of lane proposals accordingly with moderate accuracy. At the \"Refine\" stage, we further optimize these proposals via a novel Lane Segment Association Module (LSAM), which allows adaptive lane segment adjustment. Last but not least, we propose multi-level feature integration to enrich lane feature representations more efficiently. Based on the proposed \"Sketch-and-Refine\" paradigm, we propose a fast yet effective lane detector dubbed \"SRLane\". Experiments show that our SRLane can run at a fast speed (i.e., 278 FPS) while yielding an F1 score of 78.9%. The source code is available at: https://github.com/passerer/SRLane",
    "checked": true,
    "id": "e5c54730fb852352565590b8ac1a27b37402d2d7",
    "semantic_title": "sketch and refine: towards fast and accurate lane detection",
    "citation_count": 0,
    "authors": [
      "Chao Chen",
      "Jie Liu",
      "Chang Zhou",
      "Jie Tang",
      "Gangshan Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27861": {
    "title": "Iterative Token Evaluation and Refinement for Real-World Super-resolution",
    "volume": "main",
    "abstract": "Real-world image super-resolution (RWSR) is a long-standing problem as low-quality (LQ) images often have complex and unidentified degradations. Existing methods such as Generative Adversarial Networks (GANs) or continuous diffusion models present their own issues including GANs being difficult to train while continuous diffusion models requiring numerous inference steps. In this paper, we propose an Iterative Token Evaluation and Refinement (ITER) framework for RWSR, which utilizes a discrete diffusion model operating in the discrete token representation space, i.e., indexes of features extracted from a VQGAN codebook pre-trained with high-quality (HQ) images. We show that ITER is easier to train than GANs and more efficient than continuous diffusion models. Specifically, we divide RWSR into two sub-tasks, i.e., distortion removal and texture generation. Distortion removal involves simple HQ token prediction with LQ images, while texture generation uses a discrete diffusion model to iteratively refine the distortion removal output with a token refinement network. In particular, we propose to include a token evaluation network in the discrete diffusion process. It learns to evaluate which tokens are good restorations and helps to improve the iterative refinement results. Moreover, the evaluation network can first check status of the distortion removal output and then adaptively select total refinement steps needed, thereby maintaining a good balance between distortion removal and texture generation. Extensive experimental results show that ITER is easy to train and performs well within just 8 iterative steps",
    "checked": true,
    "id": "1282229a4dcbced5b14376e141d1895168b2534d",
    "semantic_title": "iterative token evaluation and refinement for real-world super-resolution",
    "citation_count": 2,
    "authors": [
      "Chaofeng Chen",
      "Shangchen Zhou",
      "Liang Liao",
      "Haoning Wu",
      "Wenxiu Sun",
      "Qiong Yan",
      "Weisi Lin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27862": {
    "title": "FeatWalk: Enhancing Few-Shot Classification through Local View Leveraging",
    "volume": "main",
    "abstract": "Few-shot learning is a challenging task due to the limited availability of training samples. Recent few-shot learning studies with meta-learning and simple transfer learning methods have achieved promising performance. However, the feature extractor pre-trained with the upstream dataset may neglect the extraction of certain features which could be crucial for downstream tasks. In this study, inspired by the process of human learning in few-shot tasks, where humans not only observe the whole image (`global view') but also attend to various local image regions (`local view') for comprehensive understanding of detailed features, we propose a simple yet effective few-shot learning method called FeatWalk which can utilize the complementary nature of global and local views, therefore providing an intuitive and effective solution to the problem of insufficient local information extraction from the pre-trained feature extractor. Our method can be easily and flexibly combined with various existing methods, further enhancing few-shot learning performance. Extensive experiments on multiple benchmark datasets consistently demonstrate the effectiveness and versatility of our method.The source code is available at https://github.com/exceefind/FeatWalk",
    "checked": true,
    "id": "f229b04d7583421ce58cf6f232c370cb203f8bcc",
    "semantic_title": "featwalk: enhancing few-shot classification through local view leveraging",
    "citation_count": 0,
    "authors": [
      "Dalong Chen",
      "Jianjia Zhang",
      "Wei-Shi Zheng",
      "Ruixuan Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27863": {
    "title": "Real3D: The Curious Case of Neural Scene Degeneration",
    "volume": "main",
    "abstract": "Despite significant progress in utilizing pre-trained text-to-image diffusion models to guide the creation of 3D scenes, these methods often struggle to generate scenes that are sufficiently realistic, leading to \"neural scene degeneration\". In this work, we propose a new 3D scene generation model called Real3D. Specifically, Real3D designs a pipeline from a NeRF-like implicit renderer to a tetrahedrons-based explicit renderer, greatly improving the neural network's ability to generate various neural scenes. Moreover, Real3D introduces an additional discriminator to prevent neural scenes from falling into undesirable local optima, thus avoiding the degeneration phenomenon. Our experimental results demonstrate that Real3D outperforms all existing state-of-the-art text-to-3D generation methods, providing valuable insights to facilitate the development of learning-based 3D scene generation approaches",
    "checked": true,
    "id": "c4d1c5f5374d2e4d3cc7f924de59a2688622b701",
    "semantic_title": "real3d: the curious case of neural scene degeneration",
    "citation_count": 0,
    "authors": [
      "Dengsheng Chen",
      "Jie Hu",
      "Xiaoming Wei",
      "Enhua Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27864": {
    "title": "DDAE: Towards Deep Dynamic Vision BERT Pretraining",
    "volume": "main",
    "abstract": "Recently, masked image modeling (MIM) has demonstrated promising prospects in self-supervised representation learning. However, existing MIM frameworks recover all masked patches equivalently, ignoring that the reconstruction difficulty of different patches can vary sharply due to their diverse distance from visible patches. In this paper, we propose a novel deep dynamic supervision to enable MIM methods to dynamically reconstruct patches with different degrees of difficulty at different pretraining phases and depths of the model. Our deep dynamic supervision helps to provide more locality inductive bias for ViTs especially in deep layers, which inherently makes up for the absence of local prior for self-attention mechanism. Built upon the deep dynamic supervision, we propose Deep Dynamic AutoEncoder (DDAE), a simple yet effective MIM framework that utilizes dynamic mechanisms for pixel regression and feature self-distillation simultaneously. Extensive experiments across a variety of vision tasks including ImageNet classification, semantic segmentation on ADE20K and object detection on COCO demonstrate the effectiveness of our approach",
    "checked": true,
    "id": "74a3874254997657e0697f4abee0d69ece688e42",
    "semantic_title": "ddae: towards deep dynamic vision bert pretraining",
    "citation_count": 1,
    "authors": [
      "Honghao Chen",
      "Xiangwen Kong",
      "Xiangyu Zhang",
      "Xin Zhao",
      "Kaiqi Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27865": {
    "title": "Rethinking Multi-Scale Representations in Deep Deraining Transformer",
    "volume": "main",
    "abstract": "Existing Transformer-based image deraining methods depend mostly on fixed single-input single-output U-Net architecture. In fact, this not only neglects the potentially explicit information from multiple image scales, but also lacks the capability of exploring the complementary implicit information across different scales. In this work, we rethink the multi-scale representations and design an effective multi-input multi-output framework that constructs intra- and inter-scale hierarchical modulation to better facilitate rain removal and help image restoration. We observe that rain levels reduce dramatically in coarser image scales, thus proposing to restore rain-free results from the coarsest scale to the finest scale in image pyramid inputs, which also alleviates the difficulty of model learning. Specifically, we integrate a sparsity-compensated Transformer block and a frequency-enhanced convolutional block into a coupled representation module, in order to jointly learn the intra-scale content-aware features. To facilitate representations learned at different scales to communicate with each other, we leverage a gated fusion module to adaptively aggregate the inter-scale spatial-aware features, which are rich in correlated information of rain appearances, leading to high-quality results. Extensive experiments demonstrate that our model achieves consistent gains on five benchmarks",
    "checked": true,
    "id": "1bc2390c2e32c434043936734c2fb4af5ad800c0",
    "semantic_title": "rethinking multi-scale representations in deep deraining transformer",
    "citation_count": 1,
    "authors": [
      "Hongming Chen",
      "Xiang Chen",
      "Jiyang Lu",
      "Yufeng Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27866": {
    "title": "Unsupervised Group Re-identification via Adaptive Clustering-Driven Progressive Learning",
    "volume": "main",
    "abstract": "Group re-identification (G-ReID) aims to correctly associate groups with the same members captured by different cameras. However, supervised approaches for this task often suffer from the high cost of cross-camera sample labeling. Unsupervised methods based on clustering can avoid sample labeling, but the problem of member variations often makes clustering unstable, leading to incorrect pseudo-labels. To address these challenges, we propose an adaptive clustering-driven progressive learning approach (ACPL), which consists of a group adaptive clustering (GAC) module and a global dynamic prototype update (GDPU) module. Specifically, GAC designs the quasi-distance between groups, thus fully capitalizing on both individual-level and holistic information within groups. In the case of great uncertainty in intra-group members, GAC effectively minimizes the impact of non-discriminative features and reduces the noise in the model's pseudo-labels. Additionally, our GDPU devises a dynamic weight to update the prototypes and effectively mine the hard samples with complex member variations, which improves the model's robustness. Extensive experiments conducted on four popular G-ReID datasets demonstrate that our method not only achieves state-of-the-art performance on unsupervised G-ReID but also performs comparably to several fully supervised approaches",
    "checked": true,
    "id": "168bd2caa336413680abf84864a6390fabc32208",
    "semantic_title": "unsupervised group re-identification via adaptive clustering-driven progressive learning",
    "citation_count": 0,
    "authors": [
      "Hongxu Chen",
      "Quan Zhang",
      "Jian-Huang Lai",
      "Xiaohua Xie"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27867": {
    "title": "Guiding a Harsh-Environments Robust Detector via RAW Data Characteristic Mining",
    "volume": "main",
    "abstract": "Consumer-grade cameras capture the RAW physical description of a scene and then process the image signals to obtain high-quality RGB images that are faithful to human visual perception. Conventionally, dense prediction scenes require high-precision recognition of objects in RGB images. However, predicting RGB data to exhibit the expected adaptability and robustness in harsh environments can be challenging. By capitalizing on the broader color gamut and higher bit depth offered by RAW data, in this paper, we demonstrate that RAW data can significantly improve the accuracy and robustness of object detectors in harsh environments. Firstly, we propose a general Pipeline for RAW Detection (PRD), along with a preprocessing strategy tailored to RAW data. Secondly, we design the RAW Corruption Benchmark (RCB) to address the dearth of benchmarks that reflect realistic scenarios in harsh environments. Thirdly, we demonstrate the significant improvement of RAW images in object detection for low-light and corrupt scenes. Specifically, our experiments indicate that PRD (using FCOS) outperforms RGB detection by 13.9mAP on LOD-Snow without generating restored images. Finally, we introduce a new nonlinear method called Functional Regularization (FR), which can effectively mine the unique characteristics of RAW data. The code is available at https://github.com/DreamerCCC/RawMining",
    "checked": true,
    "id": "316f9c8df62d228117e6067b3a3fee391631004a",
    "semantic_title": "guiding a harsh-environments robust detector via raw data characteristic mining",
    "citation_count": 0,
    "authors": [
      "Hongyang Chen",
      "Hung-Shuo Tai",
      "Kaisheng Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27868": {
    "title": "CutFreq: Cut-and-Swap Frequency Components for Low-Level Vision Augmentation",
    "volume": "main",
    "abstract": "Low-level vision plays a crucial role in a wide range of imaging quality and image recognition applications. However, the limited size, quality, and diversity of datasets often pose significant challenges for low-level tasks. Data augmentation is the most effective and practical way of sample expansion, but the commonly used augmentation methods in high-level tasks have limited improvement in the low-level due to the boundary effects or the non-realistic context information. In this paper, we propose the Cut-and-Swap Frequency Components (CutFreq) method for low-level vision, which aims to preserve high-level representations with directionality and improve image synthesis quality. Observing the significant frequency domain differences between reconstructed images and real ones, in CutFreq, we propose to transform the input and real images separately in the frequency domain, then define two stages for the model training process, and finally swap the specified frequency bands respectively and inversely transform to generate augmented samples. The experimental results show the superior performance of CutFreq on five low-level vision tasks. Moreover, we demonstrate the effectiveness of CutFreq in the low-data regime. Code is available at https://github.com/DreamerCCC/CutFreq",
    "checked": true,
    "id": "b80eeb2ed8c107490bb3a2bbf13a1fc717423f7d",
    "semantic_title": "cutfreq: cut-and-swap frequency components for low-level vision augmentation",
    "citation_count": 0,
    "authors": [
      "Hongyang Chen",
      "Kaisheng Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27869": {
    "title": "Null Space Matters: Range-Null Decomposition for Consistent Multi-Contrast MRI Reconstruction",
    "volume": "main",
    "abstract": "Consistency and interpretability have long been the critical issues in MRI reconstruction. While interpretability has been dramatically improved with the employment of deep unfolding networks (DUNs), current methods still suffer from inconsistencies and generate inferior anatomical structure. Especially in multi-contrast scenes, different imaging protocols often exacerbate the concerned issue. In this paper, we propose a range-null decomposition-assisted DUN architecture to ensure consistency while still providing desirable interpretability. Given the input decomposed, we argue that the inconsistency could be analytically relieved by feeding solely the null-space component into proximal mapping, while leaving the range-space counterpart fixed. More importantly, a correlation decoupling scheme is further proposed to narrow the information gap for multi-contrast fusion, which dynamically borrows isotropic features from the opponent while maintaining the modality-specific ones. Specifically, the two features are attached to different frequencies and learned individually by the newly designed isotropy encoder and anisotropy encoder. The former strives for the contrast-shared information, while the latter serves to capture the contrast-specific features. The quantitative and qualitative results show that our proposal outperforms most cutting-edge methods by a large margin. Codes will be released on https://github.com/chenjiachengzzz/RNU",
    "checked": true,
    "id": "14f7cbde73144336bbeb8b0b04247d30d66f1454",
    "semantic_title": "null space matters: range-null decomposition for consistent multi-contrast mri reconstruction",
    "citation_count": 0,
    "authors": [
      "Jiacheng Chen",
      "Jiawei Jiang",
      "Fei Wu",
      "Jianwei Zheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27870": {
    "title": "PNeSM: Arbitrary 3D Scene Stylization via Prompt-Based Neural Style Mapping",
    "volume": "main",
    "abstract": "3D scene stylization refers to transform the appearance of a 3D scene to match a given style image, ensuring that images rendered from different viewpoints exhibit the same style as the given style image, while maintaining the 3D consistency of the stylized scene. Several existing methods have obtained impressive results in stylizing 3D scenes. However, the mod- els proposed by these methods need to be re-trained when applied to a new scene. In other words, their models are cou- pled with a specific scene and cannot adapt to arbitrary other scenes. To address this issue, we propose a novel 3D scene stylization framework to transfer an arbitrary style to an ar- bitrary scene, without any style-related or scene-related re- training. Concretely, we first map the appearance of the 3D scene into a 2D style pattern space, which realizes complete disentanglement of the geometry and appearance of the 3D scene and makes our model be generalized to arbitrary 3D scenes. Then we stylize the appearance of the 3D scene in the 2D style pattern space via a prompt-based 2D stylization al- gorithm. Experimental results demonstrate that our proposed framework is superior to SOTA methods in both visual qual- ity and generalization",
    "checked": true,
    "id": "6ce6cf6ad2f1d09652b4b5829436199a4c06cc1b",
    "semantic_title": "pnesm: arbitrary 3d scene stylization via prompt-based neural style mapping",
    "citation_count": 0,
    "authors": [
      "Jiafu Chen",
      "Wei Xing",
      "Jiakai Sun",
      "Tianyi Chu",
      "Yiling Huang",
      "Boyan Ji",
      "Lei Zhao",
      "Huaizhong Lin",
      "Haibo Chen",
      "Zhizhong Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27871": {
    "title": "TagFog: Textual Anchor Guidance and Fake Outlier Generation for Visual Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "Out-of-distribution (OOD) detection is crucial in many real-world applications. However, intelligent models are often trained solely on in-distribution (ID) data, leading to overconfidence when misclassifying OOD data as ID classes. In this study, we propose a new learning framework which leverage simple Jigsaw-based fake OOD data and rich semantic embeddings (`anchors') from the ChatGPT description of ID knowledge to help guide the training of the image encoder. The learning framework can be flexibly combined with existing post-hoc approaches to OOD detection, and extensive empirical evaluations on multiple OOD detection benchmarks demonstrate that rich textual representation of ID knowledge and fake OOD knowledge can well help train a visual encoder for OOD detection. With the learning framework, new state-of-the-art performance was achieved on all the benchmarks. The code is available at https://github.com/Cverchen/TagFog",
    "checked": true,
    "id": "835d157e2c23d3577f23778ce051ab8d706babf6",
    "semantic_title": "tagfog: textual anchor guidance and fake outlier generation for visual out-of-distribution detection",
    "citation_count": 0,
    "authors": [
      "Jiankang Chen",
      "Tong Zhang",
      "Wei-Shi Zheng",
      "Ruixuan Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27872": {
    "title": "EVE: Efficient Vision-Language Pre-training with Masked Prediction and Modality-Aware MoE",
    "volume": "main",
    "abstract": "Building scalable vision-language models to learn from diverse, multimodal data remains an open challenge. In this paper, we introduce an Efficient Vision-languagE foundation model, namely EVE, which is one unified multimodal Transformer pre-trained solely by one unified pre-training task. Specifically, EVE encodes both vision and language within a shared Transformer network integrated with modality-aware sparse Mixture-of-Experts (MoE) modules, which capture modality-specific information by selectively switching to different experts. To unify pre-training tasks of vision and language, EVE performs masked signal modeling on image-text pairs to reconstruct masked signals, i.e., image pixels and text tokens, given visible signals. This simple yet effective pre-training objective accelerates training by 4x compared to the model pre-trained with Image-Text Contrastive and Image-Text Matching losses. Owing to the combination of the unified architecture and pre-training task, EVE is easy to scale up, enabling better downstream performance with fewer resources and faster training speed. Despite its simplicity, EVE achieves state-of-the-art performance on various vision-language downstream tasks, including visual question answering, visual reasoning, and image-text retrieval",
    "checked": true,
    "id": "0ff86630bf775f0510ef20b76352a0757a4ed70b",
    "semantic_title": "eve: efficient vision-language pre-training with masked prediction and modality-aware moe",
    "citation_count": 1,
    "authors": [
      "Junyi Chen",
      "Longteng Guo",
      "Jia Sun",
      "Shuai Shao",
      "Zehuan Yuan",
      "Liang Lin",
      "Dongyu Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27873": {
    "title": "CaMIL: Causal Multiple Instance Learning for Whole Slide Image Classification",
    "volume": "main",
    "abstract": "Whole slide image (WSI) classification is a crucial component in automated pathology analysis. Due to the inherent challenges of high-resolution WSIs and the absence of patch-level labels, most of the proposed methods follow the multiple instance learning (MIL) formulation. While MIL has been equipped with excellent instance feature extractors and aggregators, it is prone to learn spurious associations that undermine the performance of the model. For example, relying solely on color features may lead to erroneous diagnoses due to spurious associations between the disease and the color of patches. To address this issue, we develop a causal MIL framework for WSI classification, effectively distinguishing between causal and spurious associations. Specifically, we use the expectation of the intervention P(Y | do(X)) for bag prediction rather than the traditional likelihood P(Y | X). By applying the front-door adjustment, the spurious association is effectively blocked, where the intervened mediator is aggregated from patch-level features. We evaluate our proposed method on two publicly available WSI datasets, Camelyon16 and TCGA-NSCLC. Our causal MIL framework shows outstanding performance and is plug-and-play, seamlessly integrating with various feature extractors and aggregators",
    "checked": true,
    "id": "bd185f06cc119ed7b1d2dc8e9cc16f2cba067901",
    "semantic_title": "camil: causal multiple instance learning for whole slide image classification",
    "citation_count": 2,
    "authors": [
      "Kaitao Chen",
      "Shiliang Sun",
      "Jing Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27874": {
    "title": "Multi-Prototype Space Learning for Commonsense-Based Scene Graph Generation",
    "volume": "main",
    "abstract": "In the domain of scene graph generation, modeling commonsense as a single-prototype representation has been typically employed to facilitate the recognition of infrequent predicates. However, a fundamental challenge lies in the large intra-class variations of the visual appearance of predicates, resulting in subclasses within a predicate class. Such a challenge typically leads to the problem of misclassifying diverse predicates due to the rough predicate space clustering. In this paper, inspired by cognitive science, we maintain multi-prototype representations for each predicate class, which can accurately find the multiple class centers of the predicate space. Technically, we propose a novel multi-prototype learning framework consisting of three main steps: prototype-predicate matching, prototype updating, and prototype space optimization. We first design a triple-level optimal transport to match each predicate feature within the same class to a specific prototype. In addition, the prototypes are updated using momentum updating to find the class centers according to the matching results. Finally, we enhance the inter-class separability of the prototype space through iterations of the inter-class separability loss and intra-class compactness loss. Extensive evaluations demonstrate that our approach significantly outperforms state-of-the-art methods on the Visual Genome dataset",
    "checked": true,
    "id": "b2cfd63fc1a6a4992a81840208e6d1d462b36045",
    "semantic_title": "multi-prototype space learning for commonsense-based scene graph generation",
    "citation_count": 1,
    "authors": [
      "Lianggangxu Chen",
      "Youqi Song",
      "Yiqing  Cai",
      "Jiale Lu",
      "Yang Li",
      "Yuan Xie",
      "Changbo Wang",
      "Gaoqi He"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27875": {
    "title": "Kumaraswamy Wavelet for Heterophilic Scene Graph Generation",
    "volume": "main",
    "abstract": "Graph neural networks (GNNs) has demonstrated its capabilities in the field of scene graph generation (SGG) by updating node representations from neighboring nodes. Actually it can be viewed as a form of low-pass filter in the spatial domain, which smooths node feature representation and retains commonalities among nodes. However, spatial GNNs does not work well in the case of heterophilic SGG in which fine-grained predicates are always connected to a large number of coarse-grained predicates. Blind smoothing undermines the discriminative information of the fine-grained predicates, resulting in failure to predict them accurately. To address the heterophily, our key idea is to design tailored filters by wavelet transform from the spectral domain. First, we prove rigorously that when the heterophily on the scene graph increases, the spectral energy gradually shifts towards the high-frequency part. Inspired by this observation, we subsequently propose the Kumaraswamy Wavelet Graph Neural Network (KWGNN). KWGNN leverages complementary multi-group Kumaraswamy wavelets to cover all frequency bands. Finally, KWGNN adaptively generates band-pass filters and then integrates the filtering results to better accommodate varying levels of smoothness on the graph. Comprehensive experiments on the Visual Genome and Open Images datasets show that our method achieves state-of-the-art performance",
    "checked": true,
    "id": "c34d39dce5b8611900f747cc27990ea8ac36ef4c",
    "semantic_title": "kumaraswamy wavelet for heterophilic scene graph generation",
    "citation_count": 0,
    "authors": [
      "Lianggangxu Chen",
      "Youqi Song",
      "Shaohui Lin",
      "Changbo Wang",
      "Gaoqi He"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27876": {
    "title": "ViT-Calibrator: Decision Stream Calibration for Vision Transformer",
    "volume": "main",
    "abstract": "A surge of interest has emerged in utilizing Transformers in diverse vision tasks owing to its formidable performance. However, existing approaches primarily focus on optimizing internal model architecture designs that often entail significant trial and error with high burdens. In this work, we propose a new paradigm dubbed Decision Stream Calibration that boosts the performance of general Vision Transformers. To achieve this, we shed light on the information propagation mechanism in the learning procedure by exploring the correlation between different tokens and the relevance coefficient of multiple dimensions. Upon further analysis, it was discovered that 1) the final decision is associated with tokens of foreground targets, while token features of foreground target will be transmitted into the next layer as much as possible, and the useless token features of background area will be eliminated gradually in the forward propagation. 2) Each category is solely associated with specific sparse dimensions in the tokens. Based on the discoveries mentioned above, we designed a two-stage calibration scheme, namely ViT-Calibrator, including token propagation calibration stage and dimension propagation calibration stage. Extensive experiments on commonly used datasets show that the proposed approach can achieve promising results",
    "checked": true,
    "id": "2e46788980581212aba95915a287aa38acd635f3",
    "semantic_title": "vit-calibrator: decision stream calibration for vision transformer",
    "citation_count": 0,
    "authors": [
      "Lin Chen",
      "Zhijie Jia",
      "Lechao Cheng",
      "Yang Gao",
      "Jie Lei",
      "Yijun Bei",
      "Zunlei Feng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27877": {
    "title": "NeRF-VPT: Learning Novel View Representations with Neural Radiance Fields via View Prompt Tuning",
    "volume": "main",
    "abstract": "Neural Radiance Fields (NeRF) have garnered remarkable success in novel view synthesis. Nonetheless, the task of generating high-quality images for novel views persists as a critical challenge. While the existing efforts have exhibited commendable progress, capturing intricate details, enhancing textures, and achieving superior Peak Signal-to-Noise Ratio (PSNR) metrics warrant further focused attention and advancement. In this work, we propose NeRF-VPT, an innovative method for novel view synthesis to address these challenges. Our proposed NeRF-VPT employs a cascading view prompt tuning paradigm, wherein RGB information gained from preceding rendering outcomes serves as instructive visual prompts for subsequent rendering stages, with the aspiration that the prior knowledge embedded in the prompts can facilitate the gradual enhancement of rendered image quality. NeRF-VPT only requires sampling RGB data from previous stage renderings as priors at each training stage, without relying on extra guidance or complex techniques. Thus, our NeRF-VPT is plug-and-play and can be readily integrated into existing methods. By conducting comparative analyses of our NeRF-VPT against several NeRF-based approaches on demanding real-scene benchmarks, such as Realistic Synthetic 360, Real Forward-Facing, Replica dataset, and a user-captured dataset, we substantiate that our NeRF-VPT significantly elevates baseline performance and proficiently generates more high-quality novel view images than all the compared state-of-the-art methods. Furthermore, the cascading learning of NeRF-VPT introduces adaptability to scenarios with sparse inputs, resulting in a significant enhancement of accuracy for sparse-view novel view synthesis. The source code and dataset are available at https://github.com/Freedomcls/NeRF-VPT",
    "checked": true,
    "id": "9aa058392c30f58f06ad889335d341c0baa5b095",
    "semantic_title": "nerf-vpt: learning novel view representations with neural radiance fields via view prompt tuning",
    "citation_count": 0,
    "authors": [
      "Linsheng Chen",
      "Guangrun Wang",
      "Liuchun Yuan",
      "Keze Wang",
      "Ken Deng",
      "Philip H.S. Torr"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27878": {
    "title": "WebVLN: Vision-and-Language Navigation on Websites",
    "volume": "main",
    "abstract": "Vision-and-Language Navigation (VLN) task aims to enable AI agents to accurately understand and follow natural language instructions to navigate through real-world environments, ultimately reaching specific target locations. We recognise a promising opportunity to extend VLN to a comparable navigation task that holds substantial significance in our daily lives, albeit within the virtual realm: navigating websites on the Internet. This paper proposes a new task named Vision-and-Language Navigation on Websites (WebVLN), where we use question-based instructions to train an agent, emulating how users naturally browse websites. Unlike the existing VLN task that only pays attention to vision and instruction (language), the WebVLN agent further considers underlying web-specific content like HTML, which could not be seen on the rendered web pages yet contain rich visual and textual information. Toward this goal, we contribute a dataset, WebVLN-v1, and introduce a novel approach called Website-aware VLN Network (WebVLN-Net), which is built upon the foundation of state-of-the-art VLN techniques. Experimental results show that WebVLN-Net outperforms current VLN and web-related navigation methods. We believe that the introduction of the newWebVLN task and its dataset will establish a new dimension within the VLN domain and contribute to the broader vision-and-language research community. Code is available at: https://github.com/WebVLN/WebVLN",
    "checked": true,
    "id": "21d7ef3fd958a96efae2a086513e31e9e911853c",
    "semantic_title": "webvln: vision-and-language navigation on websites",
    "citation_count": 1,
    "authors": [
      "Qi Chen",
      "Dileepa Pitawela",
      "Chongyang Zhao",
      "Gengze Zhou",
      "Hsiang-Ting Chen",
      "Qi Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27879": {
    "title": "Learning Multimodal Volumetric Features for Large-Scale Neuron Tracing",
    "volume": "main",
    "abstract": "The current neuron reconstruction pipeline for electron microscopy (EM) data usually includes automatic image segmentation followed by extensive human expert proofreading. In this work, we aim to reduce human workload by predicting connectivity between over-segmented neuron pieces, taking both microscopy image and 3D morphology features into account, similar to human proofreading workflow. To this end, we first construct a dataset, named FlyTracing, that contains millions of pairwise connections of segments expanding the whole fly brain, which is three orders of magnitude larger than existing datasets for neuron segment connection. To learn sophisticated biological imaging features from the connectivity annotations, we propose a novel connectivity-aware contrastive learning method to generate dense volumetric EM image embedding. The learned embeddings can be easily incorporated with any point or voxel-based morphological representations for automatic neuron tracing. Extensive comparisons of different combination schemes of image and morphological representation in identifying split errors across the whole fly brain demonstrate the superiority of the proposed approach, especially for the locations that contain severe imaging artifacts, such as section missing and misalignment. The dataset and code are available at https://github.com/Levishery/Flywire-Neuron-Tracing",
    "checked": true,
    "id": "57e936bc0d2758234bb80b7696b574ff57a928b4",
    "semantic_title": "learning multimodal volumetric features for large-scale neuron tracing",
    "citation_count": 0,
    "authors": [
      "Qihua Chen",
      "Xuejin Chen",
      "Chenxuan Wang",
      "Yixiong Liu",
      "Zhiwei Xiong",
      "Feng Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27880": {
    "title": "M-BEV: Masked BEV Perception for Robust Autonomous Driving",
    "volume": "main",
    "abstract": "3D perception is a critical problem in autonomous driving. Recently, the Bird's-Eye-View (BEV) approach has attracted extensive attention, due to low-cost deployment and desirable vision detection capacity. However, the existing models ignore a realistic scenario during the driving procedure, i.e., one or more view cameras may be failed, which largely deteriorates their performance. To tackle this problem, we propose a generic Masked BEV (M-BEV) perception framework, which can effectively improve robustness to this challenging scenario, by random masking and reconstructing camera views in the end-to-end training. More specifically, we develop a novel Masked View Reconstruction (MVR) module in our M-BEV. It mimics various missing cases by randomly masking features of different camera views, then leverages the original features of these views as self-supervision and reconstructs the masked ones with the distinct spatio-temporal context across camera views. Via such a plug-and-play MVR, our M-BEV is capable of learning the missing views from the resting ones, and thus well generalized for robust view recovery and accurate perception in the testing. We perform extensive experiments on the popular NuScenes benchmark, where our framework can significantly boost 3D perception performance of the state-of-the-art models on various missing view cases, e.g., for the absence of back view, our M-BEV promotes the PETRv2 model with 10.3% mAP gain",
    "checked": true,
    "id": "54d9039e6a5ccc46292ad0ecfba1a5f08ba6fa5e",
    "semantic_title": "m-bev: masked bev perception for robust autonomous driving",
    "citation_count": 4,
    "authors": [
      "Siran Chen",
      "Yue Ma",
      "Yu Qiao",
      "Yali Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27881": {
    "title": "VPDETR: End-to-End Vanishing Point DEtection TRansformers",
    "volume": "main",
    "abstract": "In the field of vanishing point detection, previous works commonly relied on extracting and clustering straight lines or classifying candidate points as vanishing points. This paper proposes a novel end-to-end framework, called VPDETR (Vanishing Point DEtection TRansformer), that views vanishing point detection as a set prediction problem, applicable to both Manhattan and non-Manhattan world datasets. By using the positional embedding of anchor points as queries in Transformer decoders and dynamically updating them layer by layer, our method is able to directly input images and output their vanishing points without the need for explicit straight line extraction and candidate points sampling. Additionally, we introduce an orthogonal loss and a cross-prediction loss to improve accuracy on the Manhattan world datasets. Experimental results demonstrate that VPDETR achieves competitive performance compared to state-of-the-art methods, without requiring post-processing",
    "checked": true,
    "id": "8a11e0d0d1acdffc37d8eb1b33ca2b64bc9e851f",
    "semantic_title": "vpdetr: end-to-end vanishing point detection transformers",
    "citation_count": 0,
    "authors": [
      "Taiyan Chen",
      "Xianghua Ying",
      "Jinfa Yang",
      "Ruibin Wang",
      "Ruohao Guo",
      "Bowei Xing",
      "Ji Shi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27882": {
    "title": "TCI-Former: Thermal Conduction-Inspired Transformer for Infrared Small Target Detection",
    "volume": "main",
    "abstract": "Infrared small target detection (ISTD) is critical to national security and has been extensively applied in military areas. ISTD aims to segment small target pixels from background. Most ISTD networks focus on designing feature extraction blocks or feature fusion modules, but rarely describe the ISTD process from the feature map evolution perspective. In the ISTD process, the network attention gradually shifts towards target areas. We abstract this process as the directional movement of feature map pixels to target areas through convolution, pooling and interactions with surrounding pixels, which can be analogous to the movement of thermal particles constrained by surrounding variables and particles. In light of this analogy, we propose Thermal Conduction-Inspired Transformer (TCI-Former) based on the theoretical principles of thermal conduction. According to thermal conduction differential equation in heat dynamics, we derive the pixel movement differential equation (PMDE) in the image domain and further develop two modules: Thermal Conduction-Inspired Attention (TCIA) and Thermal Conduction Boundary Module (TCBM). TCIA incorporates finite difference method with PMDE to reach a numerical approximation so that target body features can be extracted. To further remove errors in boundary areas, TCBM is designed and supervised by boundary masks to refine target body features with fine boundary details. Experiments on IRSTD-1k and NUAA-SIRST demonstrate the superiority of our method",
    "checked": true,
    "id": "d242afe48d20c9890e9e9d25bc4d66228f64cf97",
    "semantic_title": "tci-former: thermal conduction-inspired transformer for infrared small target detection",
    "citation_count": 2,
    "authors": [
      "Tianxiang Chen",
      "Zhentao Tan",
      "Qi Chu",
      "Yue Wu",
      "Bin Liu",
      "Nenghai Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27883": {
    "title": "Intrinsic Phase-Preserving Networks for Depth Super Resolution",
    "volume": "main",
    "abstract": "Depth map super-resolution (DSR) plays an indispensable role in 3D vision. We discover an non-trivial spectral phenomenon: the components of high-resolution (HR) and low-resolution (LR) depth maps manifest the same intrinsic phase, and the spectral phase of RGB is a superset of them, which suggests that a phase-aware filter can assist in the precise use of RGB cues. Motivated by this, we propose an intrinsic phase-preserving DSR paradigm, named IPPNet, to fully exploit inter-modality collaboration in a mutually guided way. In a nutshell, a novel Phase-Preserving Filtering Module (PPFM) is developed to generate dynamic phase-aware filters according to the LR depth flow to filter out erroneous noisy components contained in RGB and then conduct depth enhancement via the modulation of the phase-preserved RGB signal. By stacking multiple PPFM blocks, the proposed IPPNet is capable of reaching a highly competitive restoration performance. Extensive experiments on various benchmark datasets, e.g., NYU v2, RGB-D-D, reach SOTA performance and also well demonstrate the validity of the proposed phase-preserving scheme. Code: https://github.com/neuralchen/IPPNet/",
    "checked": true,
    "id": "79d3b4119f68b26a216bb9d5e1a0b05ec0e499ef",
    "semantic_title": "intrinsic phase-preserving networks for depth super resolution",
    "citation_count": 0,
    "authors": [
      "Xuanhong Chen",
      "Hang Wang",
      "Jialiang Chen",
      "Kairui Feng",
      "Jinfan Liu",
      "Xiaohang Wang",
      "Weimin Zhang",
      "Bingbing Ni"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27884": {
    "title": "Box2Poly: Memory-Efficient Polygon Prediction of Arbitrarily Shaped and Rotated Text",
    "volume": "main",
    "abstract": "Recently, Transformer-based text detection techniques have sought to predict polygons by encoding the coordinates of individual boundary vertices using distinct query features. However, this approach incurs a significant memory overhead and struggles to effectively capture the intricate relationships between vertices belonging to the same instance. Consequently, irregular text layouts often lead to the prediction of outlined vertices, diminishing the quality of results. To address these challenges, we present an innovative approach rooted in Sparse R-CNN: a cascade decoding pipeline for polygon prediction. Our method ensures precision by iteratively refining polygon predictions, considering both the scale and location of preceding results. Leveraging this stabilized regression pipeline, even employing just a single feature vector to guide polygon instance regression yields promising detection results. Simultaneously, the leverage of instance-level feature proposal substantially enhances memory efficiency ( > 50% less vs. the SOTA method DPText-DETR) and reduces inference speed (> 40% less vs. DPText-DETR) with comparable performance on benchmarks. The code is available at https://github.com/Albertchen98/Box2Poly.git",
    "checked": true,
    "id": "b213fd13ea9188837f95e6cef6583dd7ebe43439",
    "semantic_title": "box2poly: memory-efficient polygon prediction of arbitrarily shaped and rotated text",
    "citation_count": 0,
    "authors": [
      "Xuyang Chen",
      "Dong Wang",
      "Konrad Schindler",
      "Mingwei Sun",
      "Yongliang Wang",
      "Nicolo Savioli",
      "Liqiu Meng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27885": {
    "title": "FashionERN: Enhance-and-Refine Network for Composed Fashion Image Retrieval",
    "volume": "main",
    "abstract": "The goal of composed fashion image retrieval is to locate a target image based on a reference image and modified text. Recent methods utilize symmetric encoders (e.g., CLIP) pre-trained on large-scale non-fashion datasets. However, the input for this task exhibits an asymmetric nature, where the reference image contains rich content while the modified text is often brief. Therefore, methods employing symmetric encoders encounter a severe phenomenon: retrieval results dominated by reference images, leading to the oversight of modified text. We propose a Fashion Enhance-and-Refine Network (FashionERN) centered around two aspects: enhancing the text encoder and refining visual semantics. We introduce a Triple-branch Modifier Enhancement model, which injects relevant information from the reference image and aligns the modified text modality with the target image modality. Furthermore, we propose a Dual-guided Vision Refinement model that retains critical visual information through text-guided refinement and self-guided refinement processes. The combination of these two models significantly mitigates the reference dominance phenomenon, ensuring accurate fulfillment of modifier requirements. Comprehensive experiments demonstrate our approach's state-of-the-art performance on four commonly used datasets",
    "checked": true,
    "id": "fd5fdfb299b4f6f21f4a660ef100bc96ead9df12",
    "semantic_title": "fashionern: enhance-and-refine network for composed fashion image retrieval",
    "citation_count": 0,
    "authors": [
      "Yanzhe Chen",
      "Huasong Zhong",
      "Xiangteng He",
      "Yuxin Peng",
      "Jiahuan Zhou",
      "Lele Cheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27886": {
    "title": "IT3D: Improved Text-to-3D Generation with Explicit View Synthesis",
    "volume": "main",
    "abstract": "Recent strides in Text-to-3D techniques have been propelled by distilling knowledge from powerful large text-to-image diffusion models (LDMs). Nonetheless, existing Text-to-3D approaches often grapple with challenges such as over-saturation, inadequate detailing, and unrealistic outputs. This study presents a novel strategy that leverages explicitly synthesized multi-view images to address these issues. Our approach involves the utilization of image-to-image pipelines, empowered by LDMs, to generate posed high-quality images based on the renderings of coarse 3D models. Although the generated images mostly alleviate the aforementioned issues, challenges such as view inconsistency and significant content variance persist due to the inherent generative nature of large diffusion models, posing extensive difficulties in leveraging these images effectively. To overcome this hurdle, we advocate integrating a discriminator alongside a novel Diffusion-GAN dual training strategy to guide the training of 3D models. For the incorporated discriminator, the synthesized multi-view images are considered real data, while the renderings of the optimized 3D models function as fake data. We conduct a comprehensive set of experiments that demonstrate the effectiveness of our method over baseline approaches",
    "checked": true,
    "id": "2b94785cbfd865a01cc68d7d4c7500b710e5e2fb",
    "semantic_title": "it3d: improved text-to-3d generation with explicit view synthesis",
    "citation_count": 40,
    "authors": [
      "Yiwen Chen",
      "Chi Zhang",
      "Xiaofeng Yang",
      "Zhongang Cai",
      "Gang Yu",
      "Lei Yang",
      "Guosheng Lin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27887": {
    "title": "Beyond the Label Itself: Latent Labels Enhance Semi-supervised Point Cloud Panoptic Segmentation",
    "volume": "main",
    "abstract": "As the exorbitant expense of labeling autopilot datasets and the growing trend of utilizing unlabeled data, semi-supervised segmentation on point clouds becomes increasingly imperative. Intuitively, finding out more ``unspoken words'' (i.e., latent instance information) beyond the label itself should be helpful to improve performance. In this paper, we discover two types of latent labels behind the displayed label embedded in LiDAR and image data. First, in the LiDAR Branch, we propose a novel augmentation, Cylinder-Mix, which is able to augment more yet reliable samples for training. Second, in the Image Branch, we propose the Instance Position-scale Learning (IPSL) Module to learn and fuse the information of instance position and scale, which is from a 2D pre-trained detector and a type of latent label obtained from 3D to 2D projection. Finally, the two latent labels are embedded into the multi-modal panoptic segmentation network. The ablation of the IPSL module demonstrates its robust adaptability, and the experiments evaluated on SemanticKITTI and nuScenes demonstrate that our model outperforms the state-of-the-art method, LaserMix",
    "checked": true,
    "id": "20d2a235544c712c9529e080973d940329be8042",
    "semantic_title": "beyond the label itself: latent labels enhance semi-supervised point cloud panoptic segmentation",
    "citation_count": 0,
    "authors": [
      "Yujun Chen",
      "Xin Tan",
      "Zhizhong Zhang",
      "Yanyun Qu",
      "Yuan Xie"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27888": {
    "title": "Visual Chain-of-Thought Prompting for Knowledge-Based Visual Reasoning",
    "volume": "main",
    "abstract": "Knowledge-based visual reasoning remains a daunting task since it not only requires machines to interpret the concepts and relationships from visual scenes but also associate them with external world knowledge to conduct a chain of reasoning on open-world questions. Previous works, however, treat visual perception and language-based reasoning as two independent modules, failing to attend to both modules throughout all stages of reasoning. To this end, we propose Visual Chain-of-thought Prompting (VCTP) for knowledge-based reasoning, which involves the interaction between visual content and natural language in an iterative step-by-step reasoning manner. VCTP contains three stages, see, think, and confirm. The see stage scans the image and grounds the visual concept candidates with a visual perception model. The think stage adopts a pre-trained large language model (LLM) to attend to key visual concepts from natural language questions adaptively. It then transforms key visual context into text context for prompting with a visual captioning model, and adopts the LLM to generate the answer. The confirm stage further uses the LLM to generate the supporting rationale to the answer, which is then passed through a cross-modality classifier to verify that it's consistent with the visual context. We iterate through the think-confirm stages to ensure the verified rationale is consistent with the answer. We conduct experiments on a range of knowledge-based visual reasoning datasets. We found our VCTP enjoys several benefits, 1). it achieves better performance than the previous few-shot learning baselines; 2). it enjoys the total transparency and trustworthiness of the whole reasoning process by providing rationales for each reasoning step; 3). it is computation-efficient compared with other fine-tuning baselines. Our code is available at https://github.com/UMass-Foundation-Model/VisualCoT.git",
    "checked": true,
    "id": "edf3ad2b10d8084c5185072b07f0318f8ed110c9",
    "semantic_title": "visual chain-of-thought prompting for knowledge-based visual reasoning",
    "citation_count": 1,
    "authors": [
      "Zhenfang Chen",
      "Qinhong Zhou",
      "Yikang Shen",
      "Yining Hong",
      "Zhiqing Sun",
      "Dan Gutfreund",
      "Chuang Gan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27889": {
    "title": "Blind Face Restoration under Extreme Conditions: Leveraging 3D-2D Prior Fusion for Superior Structural and Texture Recovery",
    "volume": "main",
    "abstract": "Blind face restoration under extreme conditions involves reconstructing high-quality face images from severely degraded inputs. These input images are often in poor quality and have extreme facial poses, leading to errors in facial structure and unnatural artifacts within the restored images. In this paper, we show that utilizing 3D priors effectively compensates for structure knowledge deficiencies in 2D priors while preserving the texture details. Based on this, we introduce FREx (Face Restoration under Extreme conditions) that combines structure-accurate 3D priors and texture-rich 2D priors in pretrained generative networks for blind face restoration under extreme conditions. To fuse the different information in 3D and 2D priors, we introduce an adaptive weight module that adjusts the importance of features based on the input image's condition. With this approach, our model can restore structure-accurate and natural-looking faces even when the images have lost a lot of information due to degradation and extreme pose. Extensive experimental results on synthetic and real-world datasets validate the effectiveness of our methods",
    "checked": true,
    "id": "2d1159cb9821994a485c5e028adb42a8d03662b4",
    "semantic_title": "blind face restoration under extreme conditions: leveraging 3d-2d prior fusion for superior structural and texture recovery",
    "citation_count": 0,
    "authors": [
      "Zhengrui Chen",
      "Liying Lu",
      "Ziyang Yuan",
      "Yiming Zhu",
      "Yu Li",
      "Chun Yuan",
      "Weihong Deng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27890": {
    "title": "CamoDiffusion: Camouflaged Object Detection via Conditional Diffusion Models",
    "volume": "main",
    "abstract": "Camouflaged Object Detection (COD) is a challenging task in computer vision due to the high similarity between camouflaged objects and their surroundings. Existing COD methods struggle with nuanced object boundaries and overconfident incorrect predictions. In response, we propose a new paradigm that treats COD as a conditional mask-generation task leveraging diffusion models. Our method, dubbed CamoDiffusion, employs the denoising process to progressively refine predictions while incorporating image conditions. Due to the stochastic sampling process of diffusion, our model is capable of sampling multiple possible predictions, avoiding the problem of overconfident point estimation. Moreover, we develop specialized network architecture, training, and sampling strategies, to enhance the model's expressive power, refinement capabilities and suppress overconfident mis-segmentations, thus aptly tailoring the diffusion model to the demands of COD. Extensive experiments on three COD datasets attest to the superior performance of our model compared to existing state-of-the-art methods, particularly on the most challenging COD10K dataset, where our approach achieves 0.019 in terms of MAE. Codes and models are available at https://github.com/Rapisurazurite/CamoDiffusion",
    "checked": true,
    "id": "ba4185ed30b781d8429193e53401b9eda5c5284a",
    "semantic_title": "camodiffusion: camouflaged object detection via conditional diffusion models",
    "citation_count": 4,
    "authors": [
      "Zhongxi Chen",
      "Ke Sun",
      "Xianming Lin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27891": {
    "title": "DreamIdentity: Enhanced Editability for Efficient Face-Identity Preserved Image Generation",
    "volume": "main",
    "abstract": "While large-scale pre-trained text-to-image models can synthesize diverse and high-quality human-centric images, an intractable problem is how to preserve the face identity and follow the text prompts simultaneously for conditioned input face images and texts. Despite existing encoder-based methods achieving high efficiency and decent face similarity, the generated image often fails to follow the textual prompts. To ease this editability issue, we present DreamIdentity, to learn edit-friendly and accurate face-identity representations in the word embedding space. Specifically, we propose self-augmented editability learning to enhance the editability for projected embedding, which is achieved by constructing paired generated celebrity's face and edited celebrity images for training, aiming at transferring mature editability of off-the-shelf text-to-image models in celebrity to unseen identities. Furthermore, we design a novel dedicated face-identity encoder to learn an accurate representation of human faces, which applies multi-scale ID-aware features followed by a multi-embedding projector to generate the pseudo words in the text embedding space directly. Extensive experiments show that our method can generate more text-coherent and ID-preserved images with negligible time overhead compared to the standard text-to-image generation process",
    "checked": true,
    "id": "657099b643567828b2fa6b867d723b4b53d666f5",
    "semantic_title": "dreamidentity: enhanced editability for efficient face-identity preserved image generation",
    "citation_count": 2,
    "authors": [
      "Zhuowei Chen",
      "Shancheng Fang",
      "Wei Liu",
      "Qian He",
      "Mengqi Huang",
      "Zhendong Mao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27892": {
    "title": "Deep Linear Array Pushbroom Image Restoration: A Degradation Pipeline and Jitter-Aware Restoration Network",
    "volume": "main",
    "abstract": "Linear Array Pushbroom (LAP) imaging technology is widely used in the realm of remote sensing. However, images acquired through LAP always suffer from distortion and blur because of camera jitter. Traditional methods for restoring LAP images, such as algorithms estimating the point spread function (PSF), exhibit limited performance. To tackle this issue, we propose a Jitter-Aware Restoration Network (JARNet), to remove the distortion and blur in two stages. In the first stage, we formulate an Optical Flow Correction (OFC) block to refine the optical flow of the degraded LAP images, resulting in pre-corrected images where most of the distortions are alleviated. In the second stage, for further enhancement of the pre-corrected images, we integrate two jitter-aware techniques within the Spatial and Frequency Residual (SFRes) block: 1) introducing Coordinate Attention (CoA) to the SFRes block in order to capture the jitter state in orthogonal direction; 2) manipulating image features in both spatial and frequency domains to leverage local and global priors. Additionally, we develop a data synthesis pipeline, which applies Continue Dynamic Shooting Model (CDSM) to simulate realistic degradation in LAP images. Both the proposed JARNet and LAP image synthesis pipeline establish a foundation for addressing this intricate challenge. Extensive experiments demonstrate that the proposed two-stage method outperforms state-of-the-art image restoration models. Code is available at https://github.com/JHW2000/JARNet",
    "checked": true,
    "id": "eeb4f4fdeb2a69c93e5b2f2b041a261e8e8bd080",
    "semantic_title": "deep linear array pushbroom image restoration: a degradation pipeline and jitter-aware restoration network",
    "citation_count": 1,
    "authors": [
      "Zida Chen",
      "Ziran Zhang",
      "Haoying Li",
      "Menghao Li",
      "Yueting Chen",
      "Qi Li",
      "Huajun Feng",
      "Zhihai Xu",
      "Shiqi Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27893": {
    "title": "Context-Aware Iteration Policy Network for Efficient Optical Flow Estimation",
    "volume": "main",
    "abstract": "Existing recurrent optical flow estimation networks are computationally expensive since they use a fixed large number of iterations to update the flow field for each sample. An efficient network should skip iterations when the flow improvement is limited. In this paper, we develop a Context-Aware Iteration Policy Network for efficient optical flow estimation, which determines the optimal number of iterations per sample. The policy network achieves this by learning contextual information to realize whether flow improvement is bottlenecked or minimal. On the one hand, we use iteration embedding and historical hidden cell, which include previous iterations information, to convey how flow has changed from previous iterations. On the other hand, we use the incremental loss to make the policy network implicitly perceive the magnitude of optical flow improvement in the subsequent iteration. Furthermore, the computational complexity in our dynamic network is controllable, allowing us to satisfy various resource preferences with a single trained model. Our policy network can be easily integrated into state-of-the-art optical flow networks. Extensive experiments show that our method maintains performance while reducing FLOPs by about 40%/20% for the Sintel/KITTI datasets",
    "checked": true,
    "id": "1096340c3a5fb667a8fb59749a841ac5bc86b42b",
    "semantic_title": "context-aware iteration policy network for efficient optical flow estimation",
    "citation_count": 0,
    "authors": [
      "Ri Cheng",
      "Ruian He",
      "Xuhao Jiang",
      "Shili Zhou",
      "Weimin Tan",
      "Bo Yan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27894": {
    "title": "SparseGNV: Generating Novel Views of Indoor Scenes with Sparse RGB-D Images",
    "volume": "main",
    "abstract": "We study to generate novel views of indoor scenes given sparse input views. The challenge is to achieve both photorealism and view consistency. We present SparseGNV: a learning framework that incorporates 3D structures and image generative models to generate novel views with three modules. The first module builds a neural point cloud as underlying geometry, providing scene context and guidance for the target novel view. The second module utilizes a transformer-based network to map the scene context and the guidance into a shared latent space and autoregressively decodes the target view in the form of discrete image tokens. The third module reconstructs the tokens back to the image of the target view. SparseGNV is trained across a large-scale indoor scene dataset to learn generalizable priors. Once trained, it can efficiently generate novel views of an unseen indoor scene in a feed-forward manner. We evaluate SparseGNV on real-world indoor scenes and demonstrate that it outperforms state-of-the-art methods based on either neural radiance fields or conditional image generation",
    "checked": true,
    "id": "681424f0f6e04d70d2faface501e9e181691e681",
    "semantic_title": "sparsegnv: generating novel views of indoor scenes with sparse rgb-d images",
    "citation_count": 0,
    "authors": [
      "Weihao Cheng",
      "Yan-Pei Cao",
      "Ying Shan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27895": {
    "title": "Colorizing Monochromatic Radiance Fields",
    "volume": "main",
    "abstract": "Though Neural Radiance Fields (NeRF) can produce colorful 3D representations of the world by using a set of 2D images, such ability becomes non-existent when only monochromatic images are provided. Since color is necessary in representing the world, reproducing color from monochromatic radiance fields becomes crucial. To achieve this goal, instead of manipulating the monochromatic radiance fields directly, we consider it as a representation-prediction task in the Lab color space. By first constructing the luminance and density representation using monochromatic images, our prediction stage can recreate color representation on the basis of an image colorization module. We then reproduce a colorful implicit model through the representation of luminance, density, and color. Extensive experiments have been conducted to validate the effectiveness of our approaches. Our project page: https://liquidammonia.github.io/color-nerf",
    "checked": true,
    "id": "7d8c8e1c0db73e92e9eb9c6ffdf93acb4fa45156",
    "semantic_title": "colorizing monochromatic radiance fields",
    "citation_count": 2,
    "authors": [
      "Yean Cheng",
      "Renjie Wan",
      "Shuchen Weng",
      "Chengxuan Zhu",
      "Yakun Chang",
      "Boxin Shi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27896": {
    "title": "Parallel Vertex Diffusion for Unified Visual Grounding",
    "volume": "main",
    "abstract": "Unified visual grounding (UVG) capitalizes on a wealth of task-related knowledge across various grounding tasks via one-shot training, which curtails retraining costs and task-specific architecture design efforts. Vertex generation-based UVG methods achieve this versatility by unified modeling object box and contour prediction and provide a text-powered interface to vast related multi-modal tasks, e.g., visual question answering and captioning. However, these methods typically generate vertexes sequentially through autoregression, which is prone to be trapped in error accumulation and heavy computation, especially for high-dimension sequence generation in complex scenarios. In this paper, we develop Parallel Vertex Diffusion (PVD) based on the parallelizability of diffusion models to accurately and efficiently generate vertexes in a parallel and scalable manner. Since the coordinates fluctuate greatly, it typically encounters slow convergence when training diffusion models without geometry constraints. Therefore, we consummate our PVD by two critical components, i.e., center anchor mechanism and angle summation loss, which serve to normalize coordinates and adopt a differentiable geometry descriptor from the point-in-polygon problem of computational geometry to constrain the overall difference of prediction and label vertexes. These innovative designs empower our PVD to demonstrate its superiority with state-of-the-art performance across various grounding tasks",
    "checked": true,
    "id": "d38edd256b44131d752bca76907aeb1d28bf9111",
    "semantic_title": "parallel vertex diffusion for unified visual grounding",
    "citation_count": 13,
    "authors": [
      "Zesen Cheng",
      "Kehan Li",
      "Peng Jin",
      "Siheng Li",
      "Xiangyang Ji",
      "Li Yuan",
      "Chang Liu",
      "Jie Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27897": {
    "title": "iDet3D: Towards Efficient Interactive Object Detection for LiDAR Point Clouds",
    "volume": "main",
    "abstract": "Accurately annotating multiple 3D objects in LiDAR scenes is laborious and challenging. While a few previous studies have attempted to leverage semi-automatic methods for cost-effective bounding box annotation, such methods have limitations in efficiently handling numerous multi-class objects. To effectively accelerate 3D annotation pipelines, we propose iDet3D, an efficient interactive 3D object detector. Supporting a user-friendly 2D interface, which can ease the cognitive burden of exploring 3D space to provide click interactions, iDet3D enables users to annotate the entire objects in each scene with minimal interactions. Taking the sparse nature of 3D point clouds into account, we design a negative click simulation (NCS) to improve accuracy by reducing false-positive predictions. In addition, iDet3D incorporates two click propagation techniques to take full advantage of user interactions: (1) dense click guidance (DCG) for keeping user-provided information throughout the network and (2) spatial click propagation (SCP) for detecting other instances of the same class based on the user-specified objects. Through our extensive experiments, we present that our method can construct precise annotations in a few clicks, which shows the practicality as an efficient annotation tool for 3D object detection",
    "checked": true,
    "id": "d0e6b8a7f419588f9c16faba245999ea3daeb8bf",
    "semantic_title": "idet3d: towards efficient interactive object detection for lidar point clouds",
    "citation_count": 1,
    "authors": [
      "Dongmin Choi",
      "Wonwoo Cho",
      "Kangyeol Kim",
      "Jaegul Choo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27898": {
    "title": "Fusion-Vital: Video-RF Fusion Transformer for Advanced Remote Physiological Measurement",
    "volume": "main",
    "abstract": "Remote physiology, which involves monitoring vital signs without the need for physical contact, has great potential for various applications. Current remote physiology methods rely only on a single camera or radio frequency (RF) sensor to capture the microscopic signatures from vital movements. However, our study shows that fusing deep RGB and RF features from both sensor streams can further improve performance. Because these multimodal features are defined in distinct dimensions and have varying contextual importance, the main challenge in the fusion process lies in the effective alignment of them and adaptive integration of features under dynamic scenarios. To address this challenge, we propose a novel vital sensing model, named Fusion-Vital, that combines the RGB and RF modalities through the new introduction of pairwise input formats and transformer-based fusion strategies. We also perform comprehensive experiments based on a newly collected and released remote vital dataset comprising synchronized video-RF sensors, showing the superiority of the fusion approach over the previous single-sensor baselines in various aspects",
    "checked": true,
    "id": "ec9b8d518e52fe728cc4898a31d0bf85fb6a4325",
    "semantic_title": "fusion-vital: video-rf fusion transformer for advanced remote physiological measurement",
    "citation_count": 2,
    "authors": [
      "Jae-Ho Choi",
      "Ki-Bong Kang",
      "Kyung-Tae Kim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27899": {
    "title": "MeDM: Mediating Image Diffusion Models for Video-to-Video Translation with Temporal Correspondence Guidance",
    "volume": "main",
    "abstract": "This study introduces an efficient and effective method, MeDM, that utilizes pre-trained image Diffusion Models for video-to-video translation with consistent temporal flow. The proposed framework can render videos from scene position information, such as a normal G-buffer, or perform text-guided editing on videos captured in real-world scenarios. We employ explicit optical flows to construct a practical coding that enforces physical constraints on generated frames and mediates independent frame-wise scores. By leveraging this coding, maintaining temporal consistency in the generated videos can be framed as an optimization problem with a closed-form solution. To ensure compatibility with Stable Diffusion, we also suggest a workaround for modifying observation-space scores in latent Diffusion Models. Notably, MeDM does not require fine-tuning or test-time optimization of the Diffusion Models. Through extensive qualitative, quantitative, and subjective experiments on various benchmarks, the study demonstrates the effectiveness and superiority of the proposed approach. Our project page can be found at https://medm2023.github.io",
    "checked": true,
    "id": "2ef3c9b0830f0a599d900d267c840667295a0811",
    "semantic_title": "medm: mediating image diffusion models for video-to-video translation with temporal correspondence guidance",
    "citation_count": 5,
    "authors": [
      "Ernie Chu",
      "Tzuhsuan Huang",
      "Shuo-Yen Lin",
      "Jun-Cheng Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27900": {
    "title": "Attack Deterministic Conditional Image Generative Models for Diverse and Controllable Generation",
    "volume": "main",
    "abstract": "Existing generative adversarial network (GAN) based conditional image generative models typically produce fixed output for the same conditional input, which is unreasonable for highly subjective tasks, such as large-mask image inpainting or style transfer. On the other hand, GAN-based diverse image generative methods require retraining/fine-tuning the network or designing complex noise injection functions, which is computationally expensive, task-specific, or struggle to generate high-quality results. Given that many deterministic conditional image generative models have been able to produce high-quality yet fixed results, we raise an intriguing question: is it possible for pre-trained deterministic conditional image generative models to generate diverse results without changing network structures or parameters? To answer this question, we re-examine the conditional image generation tasks from the perspective of adversarial attack and propose a simple and efficient plug-in projected gradient descent (PGD) like method for diverse and controllable image generation. The key idea is attacking the pre-trained deterministic generative models by adding a micro perturbation to the input condition. In this way, diverse results can be generated without any adjustment of network structures or fine-tuning of the pre-trained models. In addition, we can also control the diverse results to be generated by specifying the attack direction according to a reference text or image. Our work opens the door to applying adversarial attack to low-level vision tasks, and experiments on various conditional image generation tasks demonstrate the effectiveness and superiority of the proposed method",
    "checked": true,
    "id": "db8d6d9d742dca2056d26cc89d9fb3f07cf95257",
    "semantic_title": "attack deterministic conditional image generative models for diverse and controllable generation",
    "citation_count": 0,
    "authors": [
      "Tianyi Chu",
      "Wei Xing",
      "Jiafu Chen",
      "Zhizhong Wang",
      "Jiakai Sun",
      "Lei Zhao",
      "Haibo Chen",
      "Huaizhong Lin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27901": {
    "title": "NILUT: Conditional Neural Implicit 3D Lookup Tables for Image Enhancement",
    "volume": "main",
    "abstract": "3D lookup tables (3D LUTs) are a key component for image enhancement. Modern image signal processors (ISPs) have dedicated support for these as part of the camera rendering pipeline. Cameras typically provide multiple options for picture styles, where each style is usually obtained by applying a unique handcrafted 3D LUT. Current approaches for learning and applying 3D LUTs are notably fast, yet not so memory-efficient, as storing multiple 3D LUTs is required. For this reason and other implementation limitations, their use on mobile devices is less popular. In this work, we propose a Neural Implicit LUT (NILUT), an implicitly defined continuous 3D color transformation parameterized by a neural network. We show that NILUTs are capable of accurately emulating real 3D LUTs. Moreover, a NILUT can be extended to incorporate multiple styles into a single network with the ability to blend styles implicitly. Our novel approach is memory-efficient, controllable and can complement previous methods, including learned ISPs. Code at https://github.com/mv-lab/nilut",
    "checked": true,
    "id": "fd74c54504dcd17fcd0f1bc6c28e0ed7ee44d270",
    "semantic_title": "nilut: conditional neural implicit 3d lookup tables for image enhancement",
    "citation_count": 3,
    "authors": [
      "Marcos V. Conde",
      "Javier Vazquez-Corral",
      "Michael S. Brown",
      "Radu Timofte"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27902": {
    "title": "Decoupled Optimisation for Long-Tailed Visual Recognition",
    "volume": "main",
    "abstract": "When training on a long-tailed dataset, conventional learning algorithms tend to exhibit a bias towards classes with a larger sample size. Our investigation has revealed that this biased learning tendency originates from the model parameters, which are trained to disproportionately contribute to the classes characterised by their sample size (e.g., many, medium, and few classes). To balance the overall parameter contribution across all classes, we investigate the importance of each model parameter to the learning of different class groups, and propose a multistage parameter Decouple and Optimisation (DO) framework that decouples parameters into different groups with each group learning a specific portion of classes. To optimise the parameter learning, we apply different training objectives with a collaborative optimisation step to learn complementary information about each class group. Extensive experiments on long-tailed datasets, including CIFAR100, Places-LT, ImageNet-LT, and iNaturaList 2018, show that our framework achieves competitive performance compared to the state-of-the-art",
    "checked": true,
    "id": "9578e815851bad56c71bc45c256c45307e0895e8",
    "semantic_title": "decoupled optimisation for long-tailed visual recognition",
    "citation_count": 1,
    "authors": [
      "Cong Cong",
      "Shiyu Xuan",
      "Sidong Liu",
      "Shiliang Zhang",
      "Maurice Pagnucco",
      "Yang Song"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27903": {
    "title": "Underwater Organism Color Fine-Tuning via Decomposition and Guidance",
    "volume": "main",
    "abstract": "Due to the wavelength dependent light attenuation and scattering, the color of the underwater organism usually appears distorted. The existing underwater image enhancement methods mainly focus on designing networks capable of generating enhanced underwater organisms with fixed color. Due to the complexity of the underwater environment, ground truth labels are difficult to obtain, which results in the non-existence of perfect enhancement effects. Different from the existing methods, this paper proposes an algorithm with color enhancement and color fine-tuning (CECF) capabilities. The color enhancement behavior of CECF is the same as that of existing methods, aiming to restore the color of the distorted underwater organism. Beyond this general purpose, the color fine-tuning behavior of CECF can adjust the color of organisms in a controlled manner, which can generate enhanced organisms with diverse colors. To achieve this purpose, four processes are used in CECF. A supervised enhancement process learns the mapping from a distorted image to an enhanced image by the decomposition of color code. A self reconstruction process and a cross-reconstruction process are used for content-invariant learning. A color fine-tuning process is designed based on the guidance for obtaining various enhanced results with different colors. Experimental results have proven the enhancement ability and color fine-tuning ability of the proposed CECF. The source code is provided in https://github.com/Xiaofeng-life/CECF",
    "checked": true,
    "id": "5bc88f8f3b0b01861e2c257c175d2d78771f9c69",
    "semantic_title": "underwater organism color fine-tuning via decomposition and guidance",
    "citation_count": 1,
    "authors": [
      "Xiaofeng Cong",
      "Jie Gui",
      "Junming Hou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27904": {
    "title": "Color Event Enhanced Single-Exposure HDR Imaging",
    "volume": "main",
    "abstract": "Single-exposure high dynamic range (HDR) imaging aims to reconstruct the wide-range intensities of a scene by using its single low dynamic range (LDR) image, thus providing significant efficiency. Existing methods pay high attention to restoring the luminance by inversing the tone-mapping process, while the color in the over-/under-exposed area cannot be well restored due to the information loss of the single LDR image. To address this issue, we introduce color events into the imaging pipeline, which record asynchronous pixel-wise color changes in a high dynamic range, enabling edge-like scene perception under challenging lighting conditions. Specifically, we propose a joint framework that incorporates color events and a single LDR image to restore both content and color of an HDR image, where an exposureaware transformer (EaT) module is designed to propagate the informative hints, provided by the normal-exposed LDR regions and the event streams, to the missing areas. In this module, an exposure-aware mask is estimated to suppress distractive information and strengthen the restoration of the over-/under-exposed regions. To our knowledge, we are the first to use color events to enhance single-exposure HDR imaging. We also contribute corresponding datasets, consisting of synthesized datasets and a real-world dataset collected by a DAVIS346-color camera. The datasets can be found at https://www.kaggle.com/datasets/mengyaocui/ce-hdr. Extensive experiments demonstrate the effectiveness of the proposed method",
    "checked": true,
    "id": "acbc0fd34ff5049f5e1b91d521d515451852929d",
    "semantic_title": "color event enhanced single-exposure hdr imaging",
    "citation_count": 0,
    "authors": [
      "Mengyao Cui",
      "Zhigang Wang",
      "Dong Wang",
      "Bin Zhao",
      "Xuelong Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27905": {
    "title": "PHFormer: Multi-Fragment Assembly Using Proxy-Level Hybrid Transformer",
    "volume": "main",
    "abstract": "Fragment assembly involves restoring broken objects to their original geometries, and has many applications, such as archaeological restoration. Existing learning based frameworks have shown potential for solving part assembly problems with semantic decomposition, but cannot handle such geometrical decomposition problems. In this work, we propose a novel assembly framework, proxy level hybrid Transformer, with the core idea of using a hybrid graph to model and reason complex structural relationships between patches of fragments, dubbed as proxies. To this end, we propose a hybrid attention module, composed of intra and inter attention layers, enabling capturing of crucial contextual information within fragments and relative structural knowledge across fragments. Furthermore, we propose an adjacency aware hierarchical pose estimator, exploiting a decompose and integrate strategy. It progressively predicts adjacent probability and relative poses between fragments, and then implicitly infers their absolute poses by dynamic information integration. Extensive experimental results demonstrate that our method effectively reduces assembly errors while maintaining fast inference speed. The code is available at https://github.com/521piglet/PHFormer",
    "checked": true,
    "id": "adbfbef540dea9e93eec5f01898766f2931e4e64",
    "semantic_title": "phformer: multi-fragment assembly using proxy-level hybrid transformer",
    "citation_count": 0,
    "authors": [
      "Wenting Cui",
      "Runzhao Yao",
      "Shaoyi Du"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27906": {
    "title": "Trash to Treasure: Low-Light Object Detection via Decomposition-and-Aggregation",
    "volume": "main",
    "abstract": "Object detection in low-light scenarios has attracted much attention in the past few years. A mainstream and representative scheme introduces enhancers as the pre-processing for regular detectors. However, because of the disparity in task objectives between the enhancer and detector, this paradigm cannot shine at its best ability. In this work, we try to arouse the potential of enhancer + detector. Different from existing works, we extend the illumination-based enhancers (our newly designed or existing) as a scene decomposition module, whose removed illumination is exploited as the auxiliary in the detector for extracting detection-friendly features. A semantic aggregation module is further established for integrating multi-scale scene-related semantic information in the context space. Actually, our built scheme successfully transforms the \"trash\" (i.e., the ignored illumination in the detector) into the \"treasure\" for the detector. Plenty of experiments are conducted to reveal our superiority against other state-of-the-art methods. The code will be public if it is accepted",
    "checked": true,
    "id": "0eb0f6f72f36b55594a6ffdb4ae64018e3d22983",
    "semantic_title": "trash to treasure: low-light object detection via decomposition-and-aggregation",
    "citation_count": 1,
    "authors": [
      "Xiaohan Cui",
      "Long Ma",
      "Tengyu Ma",
      "Jinyuan Liu",
      "Xin Fan",
      "Risheng Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27907": {
    "title": "Omni-Kernel Network for Image Restoration",
    "volume": "main",
    "abstract": "Image restoration aims to reconstruct a high-quality image from a degraded low-quality observation. Recently, Transformer models have achieved promising performance on image restoration tasks due to their powerful ability to model long-range dependencies. However, the quadratically growing complexity with respect to the input size makes them inapplicable to practical applications. In this paper, we develop an efficient convolutional network for image restoration by enhancing multi-scale representation learning. To this end, we propose an omni-kernel module that consists of three branches, i.e., global, large, and local branches, to learn global-to-local feature representations efficiently. Specifically, the global branch achieves a global perceptive field via the dual-domain channel attention and frequency-gated mechanism. Furthermore, to provide multi-grained receptive fields, the large branch is formulated via different shapes of depth-wise convolutions with unusually large kernel sizes. Moreover, we complement local information using a point-wise depth-wise convolution. Finally, the proposed network, dubbed OKNet, is established by inserting the omni-kernel module into the bottleneck position for efficiency. Extensive experiments demonstrate that our network achieves state-of-the-art performance on 11 benchmark datasets for three representative image restoration tasks, including image dehazing, image desnowing, and image defocus deblurring. The code is available at https://github.com/c-yn/OKNet",
    "checked": true,
    "id": "d5a17832f115c16d4097ea39f204917734f9fba0",
    "semantic_title": "omni-kernel network for image restoration",
    "citation_count": 5,
    "authors": [
      "Yuning Cui",
      "Wenqi Ren",
      "Alois Knoll"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27908": {
    "title": "Aleth-NeRF: Illumination Adaptive NeRF with Concealing Field Assumption",
    "volume": "main",
    "abstract": "The standard Neural Radiance Fields (NeRF) paradigm employs a viewer-centered methodology, entangling the aspects of illumination and material reflectance into emission solely from 3D points. This simplified rendering approach presents challenges in accurately modeling images captured under adverse lighting conditions, such as low light or over-exposure. Motivated by the ancient Greek emission theory that posits visual perception as a result of rays emanating from the eyes, we slightly refine the conventional NeRF framework to train NeRF under challenging light conditions and generate normal-light condition novel views unsupervisedly. We introduce the concept of a ``Concealing Field,\" which assigns transmittance values to the surrounding air to account for illumination effects. In dark scenarios, we assume that object emissions maintain a standard lighting level but are attenuated as they traverse the air during the rendering process. Concealing Field thus compel NeRF to learn reasonable density and colour estimations for objects even in dimly lit situations. Similarly, the Concealing Field can mitigate over-exposed emissions during rendering stage. Furthermore, we present a comprehensive multi-view dataset captured under challenging illumination conditions for evaluation. Our code and proposed dataset are available at https://github.com/cuiziteng/Aleth-NeRF",
    "checked": true,
    "id": "c799930bc727d1e891a578dd7001a3d7e1380957",
    "semantic_title": "aleth-nerf: illumination adaptive nerf with concealing field assumption",
    "citation_count": 2,
    "authors": [
      "Ziteng Cui",
      "Lin Gu",
      "Xiao Sun",
      "Xianzheng Ma",
      "Yu Qiao",
      "Tatsuya Harada"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27909": {
    "title": "Federated Modality-Specific Encoders and Multimodal Anchors for Personalized Brain Tumor Segmentation",
    "volume": "main",
    "abstract": "Most existing federated learning (FL) methods for medical image analysis only considered intramodal heterogeneity, limiting their applicability to multimodal imaging applications. In practice, it is not uncommon that some FL participants only possess a subset of the complete imaging modalities, posing inter-modal heterogeneity as a challenge to effectively training a global model on all participants' data. In addition, each participant would expect to obtain a personalized model tailored for its local data characteristics from the FL in such a scenario. In this work, we propose a new FL framework with federated modality-specific encoders and multimodal anchors (FedMEMA) to simultaneously address the two concurrent issues. Above all, FedMEMA employs an exclusive encoder for each modality to account for the inter-modal heterogeneity in the first place. In the meantime, while the encoders are shared by the participants, the decoders are personalized to meet individual needs. Specifically, a server with full-modal data employs a fusion decoder to aggregate and fuse representations from all modality-specific encoders, thus bridging the modalities to optimize the encoders via backpropagation reversely. Meanwhile, multiple anchors are extracted from the fused multimodal representations and distributed to the clients in addition to the encoder parameters. On the other end, the clients with incomplete modalities calibrate their missing-modal representations toward the global full-modal anchors via scaled dot-product cross-attention, making up the information loss due to absent modalities while adapting the representations of present ones. FedMEMA is validated on the BraTS 2020 benchmark for multimodal brain tumor segmentation. Results show that it outperforms various up-to-date methods for multimodal and personalized FL and that its novel designs are effective. Our code is available",
    "checked": true,
    "id": "d1e98ba0e7d6730ddf4be207ed95c0890d4e4045",
    "semantic_title": "federated modality-specific encoders and multimodal anchors for personalized brain tumor segmentation",
    "citation_count": 0,
    "authors": [
      "Qian Dai",
      "Dong Wei",
      "Hong Liu",
      "Jinghan Sun",
      "Liansheng Wang",
      "Yefeng Zheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27910": {
    "title": "Generating and Reweighting Dense Contrastive Patterns for Unsupervised Anomaly Detection",
    "volume": "main",
    "abstract": "Recent unsupervised anomaly detection methods often rely on feature extractors pretrained with auxiliary datasets or on well-crafted anomaly-simulated samples. However, this might limit their adaptability to an increasing set of anomaly detection tasks due to the priors in the selection of auxiliary datasets or the strategy of anomaly simulation. To tackle this challenge, we first introduce a prior-less anomaly generation paradigm and subsequently develop an innovative unsupervised anomaly detection framework named GRAD, grounded in this paradigm. GRAD comprises three essential components: (1) a diffusion model (PatchDiff) to generate contrastive patterns by preserving the local structures while disregarding the global structures present in normal images, (2) a self-supervised reweighting mechanism to handle the challenge of long-tailed and unlabeled contrastive patterns generated by PatchDiff, and (3) a lightweight patch-level detector to efficiently distinguish the normal patterns and reweighted contrastive patterns. The generation results of PatchDiff effectively expose various types of anomaly patterns, e.g. structural and logical anomaly patterns. In addition, extensive experiments on both MVTec AD and MVTec LOCO datasets also support the aforementioned observation and demonstrate that GRAD achieves competitive anomaly detection accuracy and superior inference speed",
    "checked": true,
    "id": "814e8989af511bdfea1e34afee27aaabb649fef7",
    "semantic_title": "generating and reweighting dense contrastive patterns for unsupervised anomaly detection",
    "citation_count": 1,
    "authors": [
      "Songmin Dai",
      "Yifan Wu",
      "Xiaoqiang Li",
      "Xiangyang Xue"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27911": {
    "title": "Noisy Correspondence Learning with Self-Reinforcing Errors Mitigation",
    "volume": "main",
    "abstract": "Cross-modal retrieval relies on well-matched large-scale datasets that are laborious in practice. Recently, to alleviate expensive data collection, co-occurring pairs from the Internet are automatically harvested for training. However, it inevitably includes mismatched pairs, i.e., noisy correspondences, undermining supervision reliability and degrading performance. Current methods leverage deep neural networks' memorization effect to address noisy correspondences, which overconfidently focus on similarity-guided training with hard negatives and suffer from self-reinforcing errors. In light of above, we introduce a novel noisy correspondence learning framework, namely Self-Reinforcing Errors Mitigation (SREM). Specifically, by viewing sample matching as classification tasks within the batch, we generate classification logits for the given sample. Instead of a single similarity score, we refine sample filtration through energy uncertainty and estimate model's sensitivity of selected clean samples using swapped classification entropy, in view of the overall prediction distribution. Additionally, we propose cross-modal biased complementary learning to leverage negative matches overlooked in hard-negative training, further improving model optimization stability and curbing self-reinforcing errors. Extensive experiments on challenging benchmarks affirm the efficacy and efficiency of SREM",
    "checked": true,
    "id": "1c18b9be598b5c95f9b3d7e56e9c8ffa363e6828",
    "semantic_title": "noisy correspondence learning with self-reinforcing errors mitigation",
    "citation_count": 0,
    "authors": [
      "Zhuohang Dang",
      "Minnan Luo",
      "Chengyou Jia",
      "Guang Dai",
      "Xiaojun Chang",
      "Jingdong Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27912": {
    "title": "LDMVFI: Video Frame Interpolation with Latent Diffusion Models",
    "volume": "main",
    "abstract": "Existing works on video frame interpolation (VFI) mostly employ deep neural networks that are trained by minimizing the L1, L2, or deep feature space distance (e.g. VGG loss) between their outputs and ground-truth frames. However, recent works have shown that these metrics are poor indicators of perceptual VFI quality. Towards developing perceptually-oriented VFI methods, in this work we propose latent diffusion model-based VFI, LDMVFI. This approaches the VFI problem from a generative perspective by formulating it as a conditional generation problem. As the first effort to address VFI using latent diffusion models, we rigorously benchmark our method on common test sets used in the existing VFI literature. Our quantitative experiments and user study indicate that LDMVFI is able to interpolate video content with favorable perceptual quality compared to the state of the art, even in the high-resolution regime. Our code is available at https://github.com/danier97/LDMVFI",
    "checked": true,
    "id": "3bc8bbf3f3909d83b96759af4831cf175a1a0e6e",
    "semantic_title": "ldmvfi: video frame interpolation with latent diffusion models",
    "citation_count": 18,
    "authors": [
      "Duolikun Danier",
      "Fan Zhang",
      "David Bull"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27913": {
    "title": "No More Shortcuts: Realizing the Potential of Temporal Self-Supervision",
    "volume": "main",
    "abstract": "Self-supervised approaches for video have shown impressive results in video understanding tasks. However, unlike early works that leverage temporal self-supervision, current state-of-the-art methods primarily rely on tasks from the image domain (e.g., contrastive learning) that do not explicitly promote the learning of temporal features. We identify two factors that limit existing temporal self-supervision: 1) tasks are too simple, resulting in saturated training performance, and 2) we uncover shortcuts based on local appearance statistics that hinder the learning of high-level features. To address these issues, we propose 1) a more challenging reformulation of temporal self-supervision as frame-level (rather than clip-level) recognition tasks and 2) an effective augmentation strategy to mitigate shortcuts. Our model extends a representation of single video frames, pre-trained through contrastive learning, with a transformer that we train through temporal self-supervision. We demonstrate experimentally that our more challenging frame-level task formulations and the removal of shortcuts drastically improve the quality of features learned through temporal self-supervision. Our extensive experiments show state-of-the-art performance across 10 video understanding datasets, illustrating the generalization ability and robustness of our learned video representations. Project Page: https://daveishan.github.io/nms-webpage",
    "checked": true,
    "id": "4294b80325c89b291b29418f60163cd736da7de6",
    "semantic_title": "no more shortcuts: realizing the potential of temporal self-supervision",
    "citation_count": 2,
    "authors": [
      "Ishan Rajendrakumar Dave",
      "Simon Jenni",
      "Mubarak Shah"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27914": {
    "title": "A Dynamic GCN with Cross-Representation Distillation for Event-Based Learning",
    "volume": "main",
    "abstract": "Recent advances in event-based research prioritize sparsity and temporal precision. Approaches learning sparse point-based representations through graph CNNs (GCN) become more popular. Yet, these graph techniques hold lower performance than their frame-based counterpart due to two issues: (i) Biased graph structures that don't properly incorporate varied attributes (such as semantics, and spatial and temporal signals) for each vertex, resulting in inaccurate graph representations. (ii) A shortage of robust pretrained models. Here we solve the first problem by proposing a new event-based GCN (EDGCN), with a dynamic aggregation module to integrate all attributes of vertices adaptively. To address the second problem, we introduce a novel learning framework called cross-representation distillation (CRD), which leverages the dense representation of events as a cross-representation auxiliary to provide additional supervision and prior knowledge for the event graph. This frame-to-graph distillation allows us to benefit from the large-scale priors provided by CNNs while still retaining the advantages of graph-based models. Extensive experiments show our model and learning framework are effective and generalize well across multiple vision tasks",
    "checked": true,
    "id": "f3427a07ffb1e38d86d2d284be7786d7f0cfd3b7",
    "semantic_title": "a dynamic gcn with cross-representation distillation for event-based learning",
    "citation_count": 1,
    "authors": [
      "Yongjian Deng",
      "Hao Chen",
      "Youfu Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27915": {
    "title": "ResMatch: Residual Attention Learning for Feature Matching",
    "volume": "main",
    "abstract": "Attention-based graph neural networks have made great progress in feature matching. However, the literature lacks a comprehensive understanding of how the attention mechanism operates for feature matching. In this paper, we rethink cross- and self-attention from the viewpoint of traditional feature matching and filtering. To facilitate the learning of matching and filtering, we incorporate the similarity of descriptors into cross-attention and relative positions into self-attention. In this way, the attention can concentrate on learning residual matching and filtering functions with reference to the basic functions of measuring visual and spatial correlation. Moreover, we leverage descriptor similarity and relative positions to extract inter- and intra-neighbors. Then sparse attention for each point can be performed only within its neighborhoods to acquire higher computation efficiency. Extensive experiments, including feature matching, pose estimation and visual localization, confirm the superiority of the proposed method. Our codes are available at https://github.com/ACuOoOoO/ResMatch",
    "checked": true,
    "id": "96e0a6f9d2c2c0124f390c73263511c580cf812a",
    "semantic_title": "resmatch: residual attention learning for feature matching",
    "citation_count": 0,
    "authors": [
      "Yuxin Deng",
      "Kaining Zhang",
      "Shihua Zhang",
      "Yansheng Li",
      "Jiayi Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27916": {
    "title": "SDGMNet: Statistic-Based Dynamic Gradient Modulation for Local Descriptor Learning",
    "volume": "main",
    "abstract": "Rescaling the backpropagated gradient of contrastive loss has made significant progress in descriptor learning. However, current gradient modulation strategies have no regard for the varying distribution of global gradients, so they would suffer from changes in training phases or datasets. In this paper, we propose a dynamic gradient modulation, named SDGMNet, for contrastive local descriptor learning. The core of our method is formulating modulation functions with dynamically estimated statistical characteristics. Firstly, we introduce angle for distance measure after deep analysis on backpropagation of pair-wise loss. On this basis, auto-focus modulation is employed to moderate the impact of statistically uncommon individual pairs in stochastic gradient descent optimization; probabilistic margin cuts off the gradients of proportional triplets that have achieved enough optimization; power adjustment balances the total weights of negative pairs and positive pairs. Extensive experiments demonstrate that our novel descriptor surpasses previous state-of-the-art methods in several tasks including patch verification, retrieval, pose estimation, and 3D reconstruction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxin Deng",
      "Jiayi Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27917": {
    "title": "Stereo Vision Conversion from Planar Videos Based on Temporal Multiplane Images",
    "volume": "main",
    "abstract": "With the rapid development of 3D movie and light-field displays, there is a growing demand for stereo videos. However, generating high-quality stereo videos from planar videos remains a challenging task. Traditional depth-image-based rendering techniques struggle to effectively handle the problem of occlusion exposure, which occurs when the occluded contents become visible in other views. Recently, the single-view multiplane images (MPI) representation has shown promising performance for planar video stereoscopy. However, the MPI still lacks real details that are occluded in the current frame, resulting in blurry artifacts in occlusion exposure regions. In fact, planar videos can leverage complementary information from adjacent frames to predict a more complete scene representation for the current frame. Therefore, this paper extends the MPI from still frames to the temporal domain, introducing the temporal MPI (TMPI). By extracting complementary information from adjacent frames based on optical flow guidance, obscured regions in the current frame can be effectively repaired. Additionally, a new module called masked optical flow warping (MOFW) is introduced to improve the propagation of pixels along optical flow trajectories. Experimental results demonstrate that the proposed method can generate high-quality stereoscopic or light-field videos from a single view and reproduce better occluded details than other state-of-the-art (SOTA) methods. https://github.com/Dio3ding/TMPI",
    "checked": true,
    "id": "e923586185f3554bca7543c1db293c2df93c71ba",
    "semantic_title": "stereo vision conversion from planar videos based on temporal multiplane images",
    "citation_count": 0,
    "authors": [
      "Shanding Diao",
      "Yuan Chen",
      "Yang Zhao",
      "Wei Jia",
      "Zhao Zhang",
      "Ronggang Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27918": {
    "title": "Weak Distribution Detectors Lead to Stronger Generalizability of Vision-Language Prompt Tuning",
    "volume": "main",
    "abstract": "We propose a generalized method for boosting the generalization ability of pre-trained vision-language models (VLMs) while fine-tuning on downstream few-shot tasks. The idea is realized by exploiting out-of-distribution (OOD) detection to predict whether a sample belongs to a base distribution or a novel distribution and then using the score generated by a dedicated competition based scoring function to fuse the zero-shot and few-shot classifier. The fused classifier is dynamic, which will bias towards the zero-shot classifier if a sample is more likely from the distribution pre-trained on, leading to improved base-to-novel generalization ability. Our method is performed only in test stage, which is applicable to boost existing methods without time-consuming re-training. Extensive experiments show that even weak distribution detectors can still improve VLMs' generalization ability. Specifically, with the help of OOD detectors, the harmonic mean of CoOp and ProGrad increase by 2.6 and 1.5 percentage points over 11 recognition datasets in the base-to-novel setting",
    "checked": true,
    "id": "d6edcea64948ca6f61a42797957c86a0c2a41fef",
    "semantic_title": "weak distribution detectors lead to stronger generalizability of vision-language prompt tuning",
    "citation_count": 1,
    "authors": [
      "Kun Ding",
      "Haojian Zhang",
      "Qiang Yu",
      "Ying Wang",
      "Shiming Xiang",
      "Chunhong Pan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27919": {
    "title": "Expressive Forecasting of 3D Whole-Body Human Motions",
    "volume": "main",
    "abstract": "Human motion forecasting, with the goal of estimating future human behavior over a period of time, is a fundamental task in many real-world applications. However, existing works typically concentrate on foretelling the major joints of the human body without considering the delicate movements of the human hands. In practical applications, hand gesture plays an important role in human communication with the real world, and expresses the primary intention of human beings. In this work, we are the first to formulate whole-body human pose forecasting task, which jointly predicts future both body and gesture activities. Correspondingly, we propose a novel Encoding-Alignment-Interaction (EAI) framework that aims to predict both coarse (body joints) and fine-grained (gestures) activities collaboratively, enabling expressive and cross-facilitated forecasting of 3D whole-body human motions. Specifically, our model involves two key constituents: cross-context alignment (XCA) and cross-context interaction (XCI). Considering the heterogeneous information within the whole-body, XCA aims to align the latent features of various human components, while XCI focuses on effectively capturing the context interaction among the human components. We conduct extensive experiments on a newly-introduced large-scale benchmark and achieve state-of-the-art performance. The code is public for research purposes at https://github.com/Dingpx/EAI",
    "checked": true,
    "id": "661e99f046cd40e4506d4ebe48befaecd9c220ec",
    "semantic_title": "expressive forecasting of 3d whole-body human motions",
    "citation_count": 1,
    "authors": [
      "Pengxiang Ding",
      "Qiongjie Cui",
      "Haofan Wang",
      "Min Zhang",
      "Mengyuan Liu",
      "Donglin Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27920": {
    "title": "Transferable Adversarial Attacks for Object Detection Using Object-Aware Significant Feature Distortion",
    "volume": "main",
    "abstract": "Transferable black-box adversarial attacks against classifiers by disturbing the intermediate-layer features have been extensively studied in recent years. However, these methods have not yet achieved satisfactory performances when directly applied to object detectors. This is largely because the features of detectors are fundamentally different from that of the classifiers. In this study, we propose a simple but effective method to improve the transferability of adversarial examples for object detectors by leveraging the properties of spatial consistency and limited equivariance of object detectors' features. Specifically, we combine a novel loss function and deliberately designed data augmentation to distort the backbone features of object detectors by suppressing significant features corresponding to objects and amplifying the surrounding vicinal features corresponding to object boundaries. As such the target object and background area on the generated adversarial samples are more likely to be confused by other detectors. Extensive experimental results show that our proposed method achieves state-of-the-art black-box transferability for untargeted attacks on various models, including one/two-stage, CNN/Transformer-based, and anchor-free/anchor-based detectors",
    "checked": true,
    "id": "992922464cae18df7a6c22e53f2ccf27b9b7bc45",
    "semantic_title": "transferable adversarial attacks for object detection using object-aware significant feature distortion",
    "citation_count": 0,
    "authors": [
      "Xinlong Ding",
      "Jiansheng Chen",
      "Hongwei Yu",
      "Yu Shang",
      "Yining Qin",
      "Huimin Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27921": {
    "title": "Hyp-OW: Exploiting Hierarchical Structure Learning with Hyperbolic Distance Enhances Open World Object Detection",
    "volume": "main",
    "abstract": "Open World Object Detection (OWOD) is a challenging and realistic task that extends beyond the scope of standard Object Detection task. It involves detecting both known and unknown objects while integrating learned knowledge for future tasks. However, the level of \"unknownness\" varies significantly depending on the context. For example, a tree is typically considered part of the background in a self-driving scene, but it may be significant in a household context. We argue that this contextual information should already be embedded within the known classes. In other words, there should be a semantic or latent structure relationship between the known and unknown items to be discovered. Motivated by this observation, we propose Hyp-OW, a method that learns and models hierarchical representation of known items through a SuperClass Regularizer. Leveraging this representation allows us to effectively detect unknown objects using a similarity distance-based relabeling module. Extensive experiments on benchmark datasets demonstrate the effectiveness of Hyp-OW, achieving improvement in both known and unknown detection (up to 6 percent). These findings are particularly pronounced in our newly designed benchmark, where a strong hierarchical structure exists between known and unknown objects",
    "checked": true,
    "id": "41467217a3d273fa73ca713f14ba3f025ae3642a",
    "semantic_title": "hyp-ow: exploiting hierarchical structure learning with hyperbolic distance enhances open world object detection",
    "citation_count": 4,
    "authors": [
      "Thang Doan",
      "Xin Li",
      "Sima Behpour",
      "Wenbin He",
      "Liang Gou",
      "Liu Ren"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27922": {
    "title": "Exploiting Polarized Material Cues for Robust Car Detection",
    "volume": "main",
    "abstract": "Car detection is an important task that serves as a crucial prerequisite for many automated driving functions. The large variations in lighting/weather conditions and vehicle densities of the scenes pose significant challenges to existing car detection algorithms to meet the highly accurate perception demand for safety, due to the unstable/limited color information, which impedes the extraction of meaningful/discriminative features of cars. In this work, we present a novel learning-based car detection method that leverages trichromatic linear polarization as an additional cue to disambiguate such challenging cases. A key observation is that polarization, characteristic of the light wave, can robustly describe intrinsic physical properties of the scene objects in various imaging conditions and is strongly linked to the nature of materials for cars (e.g., metal and glass) and their surrounding environment (e.g., soil and trees), thereby providing reliable and discriminative features for robust car detection in challenging scenes. To exploit polarization cues, we first construct a pixel-aligned RGB-Polarization car detection dataset, which we subsequently employ to train a novel multimodal fusion network. Our car detection network dynamically integrates RGB and polarization features in a request-and-complement manner and can explore the intrinsic material properties of cars across all learning samples. We extensively validate our method and demonstrate that it outperforms state-of-the-art detection methods. Experimental results show that polarization is a powerful cue for car detection. Our code is available at https://github.com/wind1117/AAAI24-PCDNet",
    "checked": true,
    "id": "6f89f5cb2cbafcaf123d672335d1d21c1a62849a",
    "semantic_title": "exploiting polarized material cues for robust car detection",
    "citation_count": 0,
    "authors": [
      "Wen Dong",
      "Haiyang Mei",
      "Ziqi Wei",
      "Ao Jin",
      "Sen Qiu",
      "Qiang Zhang",
      "Xin Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27923": {
    "title": "Learning Multi-Modal Cross-Scale Deformable Transformer Network for Unregistered Hyperspectral Image Super-resolution",
    "volume": "main",
    "abstract": "Hyperspectral image super-resolution (HSI-SR) is a technology to improve the spatial resolution of HSI. Existing fusion-based SR methods have shown great performance, but still have some problems as follows: 1) existing methods assume that the auxiliary image providing spatial information is strictly registered with the HSI, but images are difficult to be registered finely due to the shooting platforms, shooting viewpoints and the influence of atmospheric turbulence; 2) most of the methods are based on convolutional neural networks (CNNs), which is effective for local features but cannot utilize the global features. To this end, we propose a multi-modal cross-scale deformable transformer network (M2DTN) to achieve unregistered HSI-SR. Specifically, we formulate a spectrum-preserving based spatial-guided registration-SR unified model (SSRU) from the view of the realistic degradation scenarios. According to SSRU, we propose multi-modal registration deformable module (MMRD) to align features between different modalities by deformation field. In order to efficiently utilize the unique information between different modals, we design multi-scale feature transformer (MSFT) to emphasize the spatial-spectral features at different scales. In addition, we propose the cross-scale feature aggregation module (CSFA) to accurately reconstruct the HSI by aggregating feature information at different scales. Experiments show that M2DTN outperforms the-state-of-the-art HSI-SR methods. Code is obtainable at https://github.com/Jiahuiqu/M2DTN",
    "checked": true,
    "id": "d634ca4341ba3f6e454945c7fd0973b53174cf83",
    "semantic_title": "learning multi-modal cross-scale deformable transformer network for unregistered hyperspectral image super-resolution",
    "citation_count": 0,
    "authors": [
      "Wenqian Dong",
      "Yang Xu",
      "Jiahui Qu",
      "Shaoxiong Hou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27924": {
    "title": "Joint Demosaicing and Denoising for Spike Camera",
    "volume": "main",
    "abstract": "As a neuromorphic camera with high temporal resolution, spike camera can capture dynamic scenes with high-speed motion. Recently, spike camera with a color filter array (CFA) has been developed for color imaging. There are some methods for spike camera demosaicing to reconstruct color images from Bayer-pattern spike streams. However, the demosaicing results are bothered by severe noise in spike streams, to which previous works pay less attention. In this paper, we propose an iterative joint demosaicing and denoising network (SJDD-Net) for spike cameras based on the observation model. Firstly, we design a color spike representation (CSR) to learn latent representation from Bayer-pattern spike streams. In CSR, we propose an offset-sharing deformable convolution module to align temporal features of color channels. Then we develop a spike noise estimator (SNE) to obtain features of the noise distribution. Finally, a color correlation prior (CCP) module is proposed to utilize the color correlation for better details. For training and evaluation, we designed a spike camera simulator to generate Bayer-pattern spike streams with synthesized noise. Besides, we captured some Bayer-pattern spike streams, building the first real-world captured dataset to our knowledge. Experimental results show that our method can restore clean images from Bayer-pattern spike streams. The source codes and dataset are available at https://github.com/csycdong/SJDD-Net",
    "checked": true,
    "id": "8ef9169f4362b6b5686756d707d735c0d8644e4f",
    "semantic_title": "joint demosaicing and denoising for spike camera",
    "citation_count": 1,
    "authors": [
      "Yanchen Dong",
      "Ruiqin Xiong",
      "Jing Zhao",
      "Jian Zhang",
      "Xiaopeng Fan",
      "Shuyuan Zhu",
      "Tiejun Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27925": {
    "title": "ChromaFusionNet (CFNet): Natural Fusion of Fine-Grained Color Editing",
    "volume": "main",
    "abstract": "Digital image enhancement aims to deliver visually striking, pleasing images that align with human perception. While global techniques can elevate the image's overall aesthetics, fine-grained color enhancement can further boost visual appeal and expressiveness. However, colorists frequently face challenges in achieving accurate, localized color adjustments. Direct composition of these local edits can result in spatial color inconsistencies. Existing methods, including color style transfer and image harmonization, exhibit inconsistencies, especially at boundary regions. Addressing this, we present ChromaFusionNet (CFNet), a novel approach that views the color fusion problem through the lens of image color inpainting. Built on the Vision Transformer architecture, CFNet captures global context and delivers high-fidelity outputs, seamlessly blending colors while preserving boundary integrity. Empirical studies on ImageNet and COCO datasets demonstrate CFNet's superiority over existing methods in maintaining color harmony and color fidelity. Robustness evaluations and user studies have further validated the effectiveness of CFNet. In conclusion, CFNet introduces an innovative approach to seamless, fine-grained color fusion, paving the way for advancements in the domain of fine-grained color editing. Code and pretrained models are available at our project page: https://yidong.pro/projects/cfnet",
    "checked": true,
    "id": "a5d4f8c467381d4aafe703f6f1dde1dd0f21119a",
    "semantic_title": "chromafusionnet (cfnet): natural fusion of fine-grained color editing",
    "citation_count": 0,
    "authors": [
      "Yi Dong",
      "Yuxi Wang",
      "Ruoxi Fan",
      "Wenqi Ouyang",
      "Zhiqi Shen",
      "Peiran Ren",
      "Xuansong Xie"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27926": {
    "title": "HybridGait: A Benchmark for Spatial-Temporal Cloth-Changing Gait Recognition with Hybrid Explorations",
    "volume": "main",
    "abstract": "Existing gait recognition benchmarks mostly include minor clothing variations in the laboratory environments, but lack persistent changes in appearance over time and space. In this paper, we propose the first in-the-wild benchmark CCGait for cloth-changing gait recognition, which incorporates diverse clothing changes, indoor and outdoor scenes, and multi-modal statistics over 92 days. To further address the coupling effect of clothing and viewpoint variations, we propose a hybrid approach HybridGait that exploits both temporal dynamics and the projected 2D information of 3D human meshes. Specifically, we introduce a Canonical Alignment Spatial-Temporal Transformer (CA-STT) module to encode human joint position-aware features, and fully exploit 3D dense priors via a Silhouette-guided Deformation with 3D-2D Appearance Projection (SilD) strategy. Our contributions are twofold: we provide a challenging benchmark CCGait that captures realistic appearance changes over expanded time and space, and we propose a hybrid framework HybridGait that outperforms prior works on CCGait and Gait3D benchmarks. Our project page is available at https://github.com/HCVLab/HybridGait",
    "checked": true,
    "id": "e43e1d43f99b503ddb3450db87ee604cd893b1c1",
    "semantic_title": "hybridgait: a benchmark for spatial-temporal cloth-changing gait recognition with hybrid explorations",
    "citation_count": 2,
    "authors": [
      "Yilan Dong",
      "Chunlin Yu",
      "Ruiyang Ha",
      "Ye Shi",
      "Yuexin Ma",
      "Lan Xu",
      "Yanwei Fu",
      "Jingya Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27927": {
    "title": "PPEA-Depth: Progressive Parameter-Efficient Adaptation for Self-Supervised Monocular Depth Estimation",
    "volume": "main",
    "abstract": "Self-supervised monocular depth estimation is of significant importance with applications spanning across autonomous driving and robotics. However, the reliance on self-supervision introduces a strong static-scene assumption, thereby posing challenges in achieving optimal performance in dynamic scenes, which are prevalent in most real-world situations. To address these issues, we propose PPEA-Depth, a Progressive Parameter-Efficient Adaptation approach to transfer a pre-trained image model for self-supervised depth estimation. The training comprises two sequential stages: an initial phase trained on a dataset primarily composed of static scenes, succeeded by an expansion to more intricate datasets involving dynamic scenes. To facilitate this process, we design compact encoder and decoder adapters to enable parameter-efficient tuning, allowing the network to adapt effectively. They not only uphold generalized patterns from pre-trained image models but also retain knowledge gained from the preceding phase into the subsequent one. Extensive experiments demonstrate that PPEA-Depth achieves state-of-the-art performance on KITTI, CityScapes and DDAD datasets",
    "checked": true,
    "id": "413ae5b85284c9c0ef7fd8c7e3eb95408b49257c",
    "semantic_title": "ppea-depth: progressive parameter-efficient adaptation for self-supervised monocular depth estimation",
    "citation_count": 1,
    "authors": [
      "Yue-Jiang Dong",
      "Yuan-Chen Guo",
      "Ying-Tian Liu",
      "Fang-Lue Zhang",
      "Song-Hai  Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27928": {
    "title": "CycleVTON: A Cycle Mapping Framework for Parser-Free Virtual Try-On",
    "volume": "main",
    "abstract": "Image-based virtual try-on aims to transfer a target clothing onto a specific person. A significant challenge is arbitrarily matched clothing and person lack corresponding ground truth to supervised learning. A recent pioneering work leveraged an improved cycleGAN to enable one network to generate the desired image for another network during training. However, there is no difference in the result distribution before and after the clothing changes. Therefore, using two different networks is unnecessary and may even increase the difficulty of convergence. Furthermore, the introduced human parsing used to provide body structure information in the input also have a negative impact on the try-on result. How to employ a single network for supervised learning while eliminating human parsing? To tackle these issues, we present a Cycle mapping Virtual Try-On Network (CycleVTON), which can produce photo-realistic try-on results by using a cycle mapping framework without the parser. In particular, we introduce a flow constraint loss to achieve supervised learning of arbitrarily matched clothing and person as inputs to the deformer, thus naturally mimicking the interaction between clothing and the human body. Additionally, we design a skin generation strategy that can adapt to the shape of the target clothing by dynamically adjusting the skin region, i.e., by first removing and then filling skin areas. Extensive experiments conducted on challenging benchmarks demonstrate that our proposed method exhibits superior performance compared to state-of-the-art methods",
    "checked": true,
    "id": "edba8d4c8ba6235fff028110fd46e491c65f2943",
    "semantic_title": "cyclevton: a cycle mapping framework for parser-free virtual try-on",
    "citation_count": 0,
    "authors": [
      "Chenghu Du",
      "Junyin Wang",
      "Yi Rong",
      "Shuqing Liu",
      "Kai Liu",
      "Shengwu Xiong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27929": {
    "title": "Arbitrary-Scale Point Cloud Upsampling by Voxel-Based Network with Latent Geometric-Consistent Learning",
    "volume": "main",
    "abstract": "Recently, arbitrary-scale point cloud upsampling mechanism became increasingly popular due to its efficiency and convenience for practical applications. To achieve this, most previous approaches formulate it as a problem of surface approximation and employ point-based networks to learn surface representations. However, learning surfaces from sparse point clouds is more challenging, and thus they often suffer from the low-fidelity geometry approximation. To address it, we propose an arbitrary-scale Point cloud Upsampling framework using Voxel-based Network (PU-VoxelNet). Thanks to the completeness and regularity inherited from the voxel representation, voxel-based networks are capable of providing predefined grid space to approximate 3D surface, and an arbitrary number of points can be reconstructed according to the predicted density distribution within each grid cell. However, we investigate the inaccurate grid sampling caused by imprecise density predictions. To address this issue, a density-guided grid resampling method is developed to generate high-fidelity points while effectively avoiding sampling outliers. Further, to improve the fine-grained details, we present an auxiliary training supervision to enforce the latent geometric consistency among local surface patches. Extensive experiments indicate the proposed approach outperforms the state-of-the-art approaches not only in terms of fixed upsampling rates but also for arbitrary-scale upsampling. The code is available at https://github.com/hikvision-research/3DVision",
    "checked": true,
    "id": "1309ec6ba3243296cd0e1d2a268c02c9a04a30a7",
    "semantic_title": "arbitrary-scale point cloud upsampling by voxel-based network with latent geometric-consistent learning",
    "citation_count": 0,
    "authors": [
      "Hang Du",
      "Xuejun Yan",
      "Jingjing Wang",
      "Di Xie",
      "Shiliang Pu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27930": {
    "title": "CDPNet: Cross-Modal Dual Phases Network for Point Cloud Completion",
    "volume": "main",
    "abstract": "Point cloud completion aims at completing shapes from their partial. Most existing methods utilized shape's priors information for point cloud completion, such as inputting the partial and getting the complete one through an encoder-decoder deep learning structure. However, it is very often to easily cause the loss of information in the generation process because of the invisibility of missing areas. Unlike most existing methods directly inferring the missing points using shape priors, we address it as a cross-modality task. We propose a new Cross-modal Dual Phases Network (CDPNet) for shape completion. Our key idea is that the global information of the shape is obtained from the extra single-view image, and the partial point clouds provide the geometric information. After that, the multi-modal features jointly guide the specific structural information. To learn the geometric details of the shape, we chose to use patches to preserve the local geometric feature. In this way, we can generate shapes with enough geometric details. Experimental results show that our method achieves state-of-the-art performance on point cloud completion",
    "checked": true,
    "id": "65e662b0fd2c35aad7f4945e02b0260f2e3fea0e",
    "semantic_title": "cdpnet: cross-modal dual phases network for point cloud completion",
    "citation_count": 2,
    "authors": [
      "Zhenjiang Du",
      "Jiale Dou",
      "Zhitao Liu ",
      "Jiwei Wei",
      "Guan Wang",
      "Ning Xie",
      "Yang Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27931": {
    "title": "Tuning-Free Inversion-Enhanced Control for Consistent Image Editing",
    "volume": "main",
    "abstract": "Consistent editing of real images is a challenging task, as it requires performing non-rigid edits (e.g., changing postures) to the main objects in the input image without changing their identity or attributes. To guarantee consistent attributes, some existing methods fine-tune the entire model or the textual embedding for structural consistency, but they are time-consuming and fail to perform non-rigid edits. Other works are tuning-free, but their performances are weakened by the quality of Denoising Diffusion Implicit Model (DDIM) reconstruction, which often fails in real-world scenarios. In this paper, we present a novel approach called Tuning-free Inversion-enhanced Control (TIC), which directly correlates features from the inversion process with those from the sampling process to mitigate the inconsistency in DDIM reconstruction. Specifically, our method effectively obtains inversion features from the key and value features in the self-attention layers, and enhances the sampling process by these inversion features, thus achieving accurate reconstruction and content-consistent editing. To extend the applicability of our method to general editing scenarios, we also propose a mask-guided attention concatenation strategy that combines contents from both the inversion and the naive DDIM editing processes. Experiments show that the proposed method outperforms previous works in reconstruction and consistent editing, and produces impressive results in various settings",
    "checked": true,
    "id": "ed526b03334bfaa372f153e69f652eb736186895",
    "semantic_title": "tuning-free inversion-enhanced control for consistent image editing",
    "citation_count": 3,
    "authors": [
      "Xiaoyue Duan",
      "Shuhao Cui",
      "Guoliang Kang",
      "Baochang Zhang",
      "Zhengcong Fei",
      "Mingyuan Fan",
      "Junshi Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27932": {
    "title": "WeditGAN: Few-Shot Image Generation via Latent Space Relocation",
    "volume": "main",
    "abstract": "In few-shot image generation, directly training GAN models on just a handful of images faces the risk of overfitting. A popular solution is to transfer the models pretrained on large source domains to small target ones. In this work, we introduce WeditGAN, which realizes model transfer by editing the intermediate latent codes w in StyleGANs with learned constant offsets (delta w), discovering and constructing target latent spaces via simply relocating the distribution of source latent spaces. The established one-to-one mapping between latent spaces can naturally prevents mode collapse and overfitting. Besides, we also propose variants of WeditGAN to further enhance the relocation process by regularizing the direction or finetuning the intensity of delta w. Experiments on a collection of widely used source/target datasets manifest the capability of WeditGAN in generating realistic and diverse images, which is simple yet highly effective in the research area of few-shot image generation. Codes are available at https://github.com/Ldhlwh/WeditGAN",
    "checked": true,
    "id": "12d37b36d7504a7275800f86eeea0f2eab6aa1ca",
    "semantic_title": "weditgan: few-shot image generation via latent space relocation",
    "citation_count": 5,
    "authors": [
      "Yuxuan Duan",
      "Li Niu",
      "Yan Hong",
      "Liqing Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27933": {
    "title": "SkeletonGait: Gait Recognition Using Skeleton Maps",
    "volume": "main",
    "abstract": "The choice of the representations is essential for deep gait recognition methods. The binary silhouettes and skeletal coordinates are two dominant representations in recent literature, achieving remarkable advances in many scenarios. However, inherent challenges remain, in which silhouettes are not always guaranteed in unconstrained scenes, and structural cues have not been fully utilized from skeletons. In this paper, we introduce a novel skeletal gait representation named skeleton map, together with SkeletonGait, a skeleton-based method to exploit structural information from human skeleton maps. Specifically, the skeleton map represents the coordinates of human joints as a heatmap with Gaussian approximation, exhibiting a silhouette-like image devoid of exact body structure. Beyond achieving state-of-the-art performances over five popular gait datasets, more importantly, SkeletonGait uncovers novel insights about how important structural features are in describing gait and when they play a role. Furthermore, we propose a multi-branch architecture, named SkeletonGait++, to make use of complementary features from both skeletons and silhouettes. Experiments indicate that SkeletonGait++ outperforms existing state-of-the-art methods by a significant margin in various scenarios. For instance, it achieves an impressive rank-1 accuracy of over 85% on the challenging GREW dataset. The source code is available at https://github.com/ShiqiYu/OpenGait",
    "checked": true,
    "id": "b460fe50ab5f44f6c5dfca5c1a27415bce06b61d",
    "semantic_title": "skeletongait: gait recognition using skeleton maps",
    "citation_count": 2,
    "authors": [
      "Chao Fan",
      "Jingzhe Ma",
      "Dongyang Jin",
      "Chuanfu Shen",
      "Shiqi Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27934": {
    "title": "TDeLTA: A Light-Weight and Robust Table Detection Method Based on Learning Text Arrangement",
    "volume": "main",
    "abstract": "The diversity of tables makes table detection a great challenge, leading to existing models becoming more tedious and complex. Despite achieving high performance, they often overfit to the table style in training set, and suffer from significant performance degradation when encountering out-of-distribution tables in other domains. To tackle this problem, we start from the essence of the table, which is a set of text arranged in rows and columns. Based on this, we propose a novel, light-weighted and robust Table Detection method based on Learning Text Arrangement, namely TDeLTA. TDeLTA takes the text blocks as input, and then models the arrangement of them with a sequential encoder and an attention module. To locate the tables precisely, we design a text-classification task, classifying the text blocks into 4 categories according to their semantic roles in the tables. Experiments are conducted on both the text blocks parsed from PDF and extracted by open-source OCR tools, respectively. Compared to several state-of-the-art methods, TDeLTA achieves competitive results with only 3.1M model parameters on the large-scale public datasets. Moreover, when faced with the cross-domain data under the 0-shot setting, TDeLTA outperforms baselines by a large margin of nearly 7%, which shows the strong robustness and transferability of the proposed model",
    "checked": true,
    "id": "3a8c4f5b09788d537b0b7bb09037c5fa055317d2",
    "semantic_title": "tdelta: a light-weight and robust table detection method based on learning text arrangement",
    "citation_count": 0,
    "authors": [
      "Yang Fan",
      "Xiangping Wu",
      "Qingcai Chen",
      "Heng Li",
      "Yan Huang",
      "Zhixiang Cai",
      "Qitian Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27935": {
    "title": "Collaborative Tooth Motion Diffusion Model in Digital Orthodontics",
    "volume": "main",
    "abstract": "Tooth motion generation is an essential task in digital orthodontic treatment for precise and quick dental healthcare, which aims to generate the whole intermediate tooth motion process given the initial pathological and target ideal tooth alignments. Most prior works for multi-agent motion planning problems usually result in complex solutions. Moreover, the occlusal relationship between upper and lower teeth is often overlooked. In this paper, we propose a collaborative tooth motion diffusion model. The critical insight is to remodel the problem as a diffusion process. In this sense, we model the whole tooth motion distribution with a diffusion model and transform the planning problem into a sampling process from this distribution. We design a tooth latent representation to provide accurate conditional guides consisting of two key components: the tooth frame represents the position and posture, and the tooth latent shape code represents the geometric morphology. Subsequently, we present a collaborative diffusion model to learn the multi-tooth motion distribution based on inter-tooth and occlusal constraints, which are implemented by graph structure and new loss functions, respectively. Extensive qualitative and quantitative experiments demonstrate the superiority of our framework in the application of orthodontics compared with state-of-the-art methods",
    "checked": true,
    "id": "b425c1277355136c14af0954de5aeb5fbe0fc5dd",
    "semantic_title": "collaborative tooth motion diffusion model in digital orthodontics",
    "citation_count": 0,
    "authors": [
      "Yeying Fan",
      "Guangshun Wei",
      "Chen Wang",
      "Shaojie Zhuang",
      "Wenping Wang",
      "Yuanfeng Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27936": {
    "title": "Everything2Motion: Synchronizing Diverse Inputs via a Unified Framework for Human Motion Synthesis",
    "volume": "main",
    "abstract": "In the dynamic field of film and game development, the emergence of human motion synthesis methods has revolutionized avatar animation. Traditional methodologies, typically reliant on single modality inputs like text or audio, employ modality-specific model frameworks, posing challenges for unified model deployment and application. To address this, we propose Everything2Motion, a unified model framework. Everything2Motion consists of three key modules. The Input-Output Modality Modulation module tailors structures for specific multimodal inputs, eliminating the need for modality-specific frameworks. The Query-aware Autoencoder, based on the transformer encoder-decoder architecture, enables efficient latent motion generation. Lastly, the Prior Motion Distillation Decoder, a pretrained module, enhances the final skeleton sequence's naturalness and fluidity. Comprehensive experiments on several public datasets demonstrate the effectiveness of Everything2Motion, highlighting its potential for practical applications and setting a new benchmark in human motion synthesis",
    "checked": true,
    "id": "4680c093e354690536e841d302073c400a97184f",
    "semantic_title": "everything2motion: synchronizing diverse inputs via a unified framework for human motion synthesis",
    "citation_count": 0,
    "authors": [
      "Zhaoxin Fan",
      "Longbin Ji",
      "Pengxin Xu",
      "Fan Shen",
      "Kai Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27937": {
    "title": "Variance-Insensitive and Target-Preserving Mask Refinement for Interactive Image Segmentation",
    "volume": "main",
    "abstract": "Point-based interactive image segmentation can ease the burden of mask annotation in applications such as semantic segmentation and image editing. However, fully extracting the target mask with limited user inputs remains challenging. We introduce a novel method, Variance-Insensitive and Target-Preserving Mask Refinement to enhance segmentation quality with fewer user inputs. Regarding the last segmentation result as the initial mask, an iterative refinement process is commonly employed to continually enhance the initial mask. Nevertheless, conventional techniques suffer from sensitivity to the variance in the initial mask. To circumvent this problem, our proposed method incorporates a mask matching algorithm for ensuring consistent inferences from different types of initial masks. We also introduce a target-aware zooming algorithm to preserve object information during downsampling, balancing efficiency and accuracy. Experiments on GrabCut, Berkeley, SBD, and DAVIS datasets demonstrate our method's state-of-the-art performance in interactive image segmentation",
    "checked": true,
    "id": "3cb7f5d89f32ec0e9aaba2c986c8aa4cfda40f1a",
    "semantic_title": "variance-insensitive and target-preserving mask refinement for interactive image segmentation",
    "citation_count": 1,
    "authors": [
      "Chaowei Fang",
      "Ziyin Zhou",
      "Junye Chen",
      "Hanjing Su",
      "Qingyao Wu",
      "Guanbin Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27938": {
    "title": "Evaluate Geometry of Radiance Fields with Low-Frequency Color Prior",
    "volume": "main",
    "abstract": "A radiance field is an effective representation of 3D scenes, which has been widely adopted in novel-view synthesis and 3D reconstruction. It is still an open and challenging problem to evaluate the geometry, i.e., the density field, as the ground-truth is almost impossible to obtain. One alternative indirect solution is to transform the density field into a point-cloud and compute its Chamfer Distance with the scanned ground-truth. However, many widely-used datasets have no point-cloud ground-truth since the scanning process along with the equipment is expensive and complicated. To this end, we propose a novel metric, named Inverse Mean Residual Color (IMRC), which can evaluate the geometry only with the observation images. Our key insight is that the better the geometry, the lower-frequency the computed color field. From this insight, given a reconstructed density field and observation images, we design a closed-form method to approximate the color field with low-frequency spherical harmonics, and compute the inverse mean residual color. Then the higher the IMRC, the better the geometry. Qualitative and quantitative experimental results verify the effectiveness of our proposed IMRC metric. We also benchmark several state-of-the-art methods using IMRC to promote future related research. Our code is available at https://github.com/qihangGH/IMRC",
    "checked": true,
    "id": "564a21fc1c77dc8b5b401ce182ba1699ed8d9179",
    "semantic_title": "evaluate geometry of radiance fields with low-frequency color prior",
    "citation_count": 2,
    "authors": [
      "Qihang Fang",
      "Yafei Song",
      "Keqiang Li",
      "Li Shen",
      "Huaiyu Wu",
      "Gang Xiong",
      "Liefeng Bo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27939": {
    "title": "Simple Image-Level Classification Improves Open-Vocabulary Object Detection",
    "volume": "main",
    "abstract": "Open-Vocabulary Object Detection (OVOD) aims to detect novel objects beyond a given set of base categories on which the detection model is trained. Recent OVOD methods focus on adapting the image-level pre-trained vision-language models (VLMs), such as CLIP, to a region-level object detection task via, eg., region-level knowledge distillation, regional prompt learning, or region-text pre-training, to expand the detection vocabulary. These methods have demonstrated remarkable performance in recognizing regional visual concepts, but they are weak in exploiting the VLMs' powerful global scene understanding ability learned from the billion-scale image-level text descriptions. This limits their capability in detecting hard objects of small, blurred, or occluded appearance from novel/base categories, whose detection heavily relies on contextual information. To address this, we propose a novel approach, namely Simple Image-level Classification for Context-Aware Detection Scoring (SIC-CADS), to leverage the superior global knowledge yielded from CLIP for complementing the current OVOD models from a global perspective. The core of SIC-CADS is a multi-modal multi-label recognition (MLR) module that learns the object co-occurrence-based contextual information from CLIP to recognize all possible object categories in the scene. These image-level MLR scores can then be utilized to refine the instance-level detection scores of the current OVOD models in detecting those hard objects. This is verified by extensive empirical results on two popular benchmarks, OV-LVIS and OV-COCO, which show that SIC-CADS achieves significant and consistent improvement when combined with different types of OVOD models. Further, SIC-CADS also improves the cross-dataset generalization ability on Objects365 and OpenImages. Code is available at https://github.com/mala-lab/SIC-CADS",
    "checked": true,
    "id": "ad08b280acbdadfe7ea695479d6b75809aa1971f",
    "semantic_title": "simple image-level classification improves open-vocabulary object detection",
    "citation_count": 4,
    "authors": [
      "Ruohuan Fang",
      "Guansong Pang",
      "Xiao Bai"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27940": {
    "title": "Self-Supervised Bird's Eye View Motion Prediction with Cross-Modality Signals",
    "volume": "main",
    "abstract": "Learning the dense bird's eye view (BEV) motion flow in a self-supervised manner is an emerging research for robotics and autonomous driving. Current self-supervised methods mainly rely on point correspondences between point clouds, which may introduce the problems of fake flow and inconsistency, hindering the model's ability to learn accurate and realistic motion. In this paper, we introduce a novel cross-modality self-supervised training framework that effectively addresses these issues by leveraging multi-modality data to obtain supervision signals. We design three innovative supervision signals to preserve the inherent properties of scene motion, including the masked Chamfer distance loss, the piecewise rigidity loss, and the temporal consistency loss. Through extensive experiments, we demonstrate that our proposed self-supervised framework outperforms all previous self-supervision methods for the motion prediction task",
    "checked": true,
    "id": "216507b805664f1678be185830c9add9d824ac04",
    "semantic_title": "self-supervised bird's eye view motion prediction with cross-modality signals",
    "citation_count": 1,
    "authors": [
      "Shaoheng Fang",
      "Zuhong Liu",
      "Mingyu Wang",
      "Chenxin Xu",
      "Yiqi Zhong",
      "Siheng Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27941": {
    "title": "Fewer Steps, Better Performance: Efficient Cross-Modal Clip Trimming for Video Moment Retrieval Using Language",
    "volume": "main",
    "abstract": "Given an untrimmed video and a sentence query, video moment retrieval using language (VMR) aims to locate a target query-relevant moment. Since the untrimmed video is overlong, almost all existing VMR methods first sparsely down-sample each untrimmed video into multiple fixed-length video clips and then conduct multi-modal interactions with the query feature and expensive clip features for reasoning, which is infeasible for long real-world videos that span hours. Since the video is downsampled into fixed-length clips, some query-related frames may be filtered out, which will blur the specific boundary of the target moment, take the adjacent irrelevant frames as new boundaries, easily leading to cross-modal misalignment and introducing both boundary-bias and reasoning-bias. To this end, in this paper, we propose an efficient approach, SpotVMR, to trim the query-relevant clip. Besides, our proposed SpotVMR can serve as plug-and-play module, which achieves efficiency for state-of-the-art VMR methods while maintaining good retrieval performance. Especially, we first design a novel clip search model that learns to identify promising video regions to search conditioned on the language query. Then, we introduce a set of low-cost semantic indexing features to capture the context of objects and interactions that suggest where to search the query-relevant moment. Also, the distillation loss is utilized to address the optimization issues arising from end-to-end joint training of the clip selector and VMR model. Extensive experiments on three challenging datasets demonstrate its effectiveness",
    "checked": true,
    "id": "268533ed51433203062c44347a5d09502a8ac4bd",
    "semantic_title": "fewer steps, better performance: efficient cross-modal clip trimming for video moment retrieval using language",
    "citation_count": 1,
    "authors": [
      "Xiang Fang",
      "Daizong Liu",
      "Wanlong Fang",
      "Pan Zhou",
      "Zichuan Xu",
      "Wenzheng Xu",
      "Junyang Chen",
      "Renfu Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27942": {
    "title": "An Embedding-Unleashing Video Polyp Segmentation Framework via Region Linking and Scale Alignment",
    "volume": "main",
    "abstract": "Automatic polyp segmentation from colonoscopy videos is a critical task for the development of computer-aided screening and diagnosis systems. However, accurate and real-time video polyp segmentation (VPS) is a very challenging task due to low contrast between background and polyps and frame-to-frame dramatic variations in colonoscopy videos. We propose a novel embedding-unleashing framework consisting of a proposal-generative network (PGN) and an appearance-embedding network (AEN) to comprehensively address these challenges. Our framework, for the first time, models VPS as an appearance-level semantic embedding process to facilitate generate more global information to counteract background disturbances and dramatic variations. Specifically, PGN is a video segmentation network to obtain segmentation mask proposals, while AEN is a network we specially designed to produce appearance-level embedding semantics for PGN, thereby unleashing the capability of PGN in VPS. Our AEN consists of a cross-scale region linking (CRL) module and a cross-wise scale alignment (CSA) module. The former screens reliable background information against background disturbances by constructing linking of region semantics, while the latter performs the scale alignment to resist dramatic variations by modeling the center-perceived motion dependence with a cross-wise manner. We further introduce a parameter-free semantic interaction to embed the semantics of AEN into PGN to obtain the segmentation results. Extensive experiments on CVC-612 and SUN-SEG demonstrate that our approach achieves better performance than other state-of-the-art methods. Codes are available at https://github.com/zhixue-fang/EUVPS",
    "checked": true,
    "id": "a7384462247ae51af2d4911b2958585d6891022e",
    "semantic_title": "an embedding-unleashing video polyp segmentation framework via region linking and scale alignment",
    "citation_count": 1,
    "authors": [
      "Zhixue Fang",
      "Xinrong Guo",
      "Jingyin Lin",
      "Huisi Wu",
      "Jing Qin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27943": {
    "title": "Debiased Novel Category Discovering and Localization",
    "volume": "main",
    "abstract": "In recent years, object detection in deep learning has experienced rapid development. However, most existing object detection models perform well only on closed-set datasets, ignoring a large number of potential objects whose categories are not defined in the training set. These objects are often identified as background or incorrectly classified as pre-defined categories by the detectors. In this paper, we focus on the challenging problem of Novel Class Discovery and Localization (NCDL), aiming to train detectors that can detect the categories present in the training data, while also actively discover, localize, and cluster new categories. We analyze existing NCDL methods and identify the core issue: object detectors tend to be biased towards seen objects, and this leads to the neglect of unseen targets. To address this issue, we first propose an Debiased Region Mining (DRM) approach that combines class-agnostic Region Proposal Network (RPN) and class-aware RPN in a complementary manner. Additionally, we suggest to improve the representation network through semi-supervised contrastive learning by leveraging unlabeled data. Finally, we adopt a simple and efficient mini-batch K-means clustering method for novel class discovery. We conduct extensive experiments on the NCDL benchmark, and the results demonstrate that the proposed DRM approach significantly outperforms previous methods, establishing a new state-of-the-art",
    "checked": true,
    "id": "54d3cf13a1dab8e8386fce96b89287edc574bbe2",
    "semantic_title": "debiased novel category discovering and localization",
    "citation_count": 1,
    "authors": [
      "Juexiao Feng",
      "Yuhong Yang",
      "Yanchun Xie",
      "Yaqian Li",
      "Yandong Guo",
      "Yuchen Guo",
      "Yuwei He",
      "Liuyu Xiang",
      "Guiguang Ding"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27944": {
    "title": "Interpretable3D: An Ad-Hoc Interpretable Classifier for 3D Point Clouds",
    "volume": "main",
    "abstract": "3D decision-critical tasks urgently require research on explanations to ensure system reliability and transparency. Extensive explanatory research has been conducted on 2D images, but there is a lack in the 3D field. Furthermore, the existing explanations for 3D models are post-hoc and can be misleading, as they separate explanations from the original model. To address these issues, we propose an ad-hoc interpretable classifier for 3D point clouds (i.e., Interpretable3D). As an intuitive case-based classifier, Interpretable3D can provide reliable ad-hoc explanations without any embarrassing nuances. It allows users to understand how queries are embedded within past observations in prototype sets. Interpretable3D has two iterative training steps: 1) updating one prototype with the mean of the embeddings within the same sub-class in Prototype Estimation, and 2) penalizing or rewarding the estimated prototypes in Prototype Optimization. The mean of embeddings has a clear statistical meaning, i.e., class sub-centers. Moreover, we update prototypes with their most similar observations in the last few epochs. Finally, Interpretable3D classifies new samples according to prototypes. We evaluate the performance of Interpretable3D on four popular point cloud models: DGCNN, PointNet2, PointMLP, and PointNeXt. Our Interpretable3D demonstrates comparable or superior performance compared to softmax-based black-box models in the tasks of 3D shape classification and part segmentation. Our code is released at: github.com/FengZicai/Interpretable3D",
    "checked": true,
    "id": "074a9cdd78cd9bac459ee4b3a9c4939db2857a09",
    "semantic_title": "interpretable3d: an ad-hoc interpretable classifier for 3d point clouds",
    "citation_count": 4,
    "authors": [
      "Tuo Feng",
      "Ruijie Quan",
      "Xiaohan Wang",
      "Wenguan Wang",
      "Yi Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27945": {
    "title": "Mimic: Speaking Style Disentanglement for Speech-Driven 3D Facial Animation",
    "volume": "main",
    "abstract": "Speech-driven 3D facial animation aims to synthesize vivid facial animations that accurately synchronize with speech and match the unique speaking style. However, existing works primarily focus on achieving precise lip synchronization while neglecting to model the subject-specific speaking style, often resulting in unrealistic facial animations. To the best of our knowledge, this work makes the first attempt to explore the coupled information between the speaking style and the semantic content in facial motions. Specifically, we introduce an innovative speaking style disentanglement method, which enables arbitrary-subject speaking style encoding and leads to a more realistic synthesis of speech-driven facial animations. Subsequently, we propose a novel framework called Mimic to learn disentangled representations of the speaking style and content from facial motions by building two latent spaces for style and content, respectively. Moreover, to facilitate disentangled representation learning, we introduce four well-designed constraints: an auxiliary style classifier, an auxiliary inverse classifier, a content contrastive loss, and a pair of latent cycle losses, which can effectively contribute to the construction of the identity-related style space and semantic-related content space. Extensive qualitative and quantitative experiments conducted on three publicly available datasets demonstrate that our approach outperforms state-of-the-art methods and is capable of capturing diverse speaking styles for speech-driven 3D facial animation. The source code and supplementary video are publicly available at: https://zeqing-wang.github.io/Mimic/",
    "checked": true,
    "id": "8956944d389a03650e3181ce607b5ba9ab209e89",
    "semantic_title": "mimic: speaking style disentanglement for speech-driven 3d facial animation",
    "citation_count": 2,
    "authors": [
      "Hui Fu",
      "Zeqing Wang",
      "Ke Gong",
      "Keze Wang",
      "Tianshui Chen",
      "Haojie Li",
      "Haifeng Zeng",
      "Wenxiong Kang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27946": {
    "title": "Fine-Grained Multi-View Hand Reconstruction Using Inverse Rendering",
    "volume": "main",
    "abstract": "Reconstructing high-fidelity hand models with intricate textures plays a crucial role in enhancing human-object interaction and advancing real-world applications. Despite the state-of-the-art methods excelling in texture generation and image rendering, they often face challenges in accurately capturing geometric details. Learning-based approaches usually offer better robustness and faster inference, which tend to produce smoother results and require substantial amounts of training data. To address these issues, we present a novel fine-grained multi-view hand mesh reconstruction method that leverages inverse rendering to restore hand poses and intricate details. Firstly, our approach predicts a parametric hand mesh model through Graph Convolutional Networks (GCN) based method from multi-view images. We further introduce a novel Hand Albedo and Mesh (HAM) optimization module to refine both the hand mesh and textures, which is capable of preserving the mesh topology. In addition, we suggest an effective mesh-based neural rendering scheme to simultaneously generate photo-realistic image and optimize mesh geometry by fusing the pre-trained rendering network with vertex features. We conduct the comprehensive experiments on InterHand2.6M, DeepHandMesh and dataset collected by ourself, whose promising results show that our proposed approach outperforms the state-of-the-art methods on both reconstruction accuracy and rendering quality. Code and dataset are publicly available at https://github.com/agnJason/FMHR",
    "checked": true,
    "id": "ffa25e8ac2981cccf8396c417c1d83cf8252baeb",
    "semantic_title": "fine-grained multi-view hand reconstruction using inverse rendering",
    "citation_count": 1,
    "authors": [
      "Qijun Gan",
      "Wentong Li",
      "Jinwei Ren",
      "Jianke Zhu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27947": {
    "title": "Attacking Transformers with Feature Diversity Adversarial Perturbation",
    "volume": "main",
    "abstract": "Understanding the mechanisms behind Vision Transformer (ViT), particularly its vulnerability to adversarial perturbations, is crucial for addressing challenges in its real-world applications. Existing ViT adversarial attackers rely on labels to calculate the gradient for perturbation, and exhibit low transferability to other structures and tasks. In this paper, we present a label-free white-box attack approach for ViT-based models that exhibits strong transferability to various black-box models, including most ViT variants, CNNs, and MLPs, even for models developed for other modalities. Our inspiration comes from the feature collapse phenomenon in ViTs, where the critical attention mechanism overly depends on the low-frequency component of features, causing the features in middle-to-end layers to become increasingly similar and eventually collapse. We propose the feature diversity attacker to naturally accelerate this process and achieve remarkable performance and transferability",
    "checked": true,
    "id": "b7459d137cf43bfb06e9a74667a21ba285b94ce6",
    "semantic_title": "attacking transformers with feature diversity adversarial perturbation",
    "citation_count": 0,
    "authors": [
      "Chenxing Gao",
      "Hang Zhou",
      "Junqing Yu",
      "YuTeng Ye",
      "Jiale Cai",
      "Junle Wang",
      "Wei Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27948": {
    "title": "Leveraging Imagery Data with Spatial Point Prior for Weakly Semi-supervised 3D Object Detection",
    "volume": "main",
    "abstract": "Training high-accuracy 3D detectors necessitates massive labeled 3D annotations with 7 degree-of-freedom, which is laborious and time-consuming. Therefore, the form of point annotations is proposed to offer significant prospects for practical applications in 3D detection, which is not only more accessible and less expensive but also provides strong spatial information for object localization. In this paper, we empirically discover that it is non-trivial to merely adapt Point-DETR to its 3D form, encountering two main bottlenecks: 1) it fails to encode strong 3D prior into the model, and 2) it generates low-quality pseudo labels in distant regions due to the extreme sparsity of LiDAR points. To overcome these challenges, we introduce Point-DETR3D, a teacher-student framework for weakly semi-supervised 3D detection, designed to fully capitalize on point-wise supervision within a constrained instance-wise annotation budget. Different from Point-DETR which encodes 3D positional information solely through a point encoder, we propose an explicit positional query initialization strategy to enhance the positional prior. Considering the low quality of pseudo labels at distant regions produced by the teacher model, we enhance the detector's perception by incorporating dense imagery data through a novel Cross-Modal Deformable RoI Fusion (D-RoI). Moreover, an innovative point-guided self-supervised learning technique is proposed to allow for fully exploiting point priors, even in student models. Extensive experiments on representative nuScenes dataset demonstrate our Point-DETR3D obtains significant improvements compared to previous works. Notably, with only 5% of labeled data, Point-DETR3D achieves over 90% performance of its fully supervised counterpart",
    "checked": true,
    "id": "6f6721f408dee5f75939b0b9747c4d0def4d3223",
    "semantic_title": "leveraging imagery data with spatial point prior for weakly semi-supervised 3d object detection",
    "citation_count": 0,
    "authors": [
      "Hongzhi Gao",
      "Zheng Chen",
      "Zehui Chen",
      "Lin Chen",
      "Jiaming Liu",
      "Shanghang Zhang",
      "Feng Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27949": {
    "title": "Dual-Prior Augmented Decoding Network for Long Tail Distribution in HOI Detection",
    "volume": "main",
    "abstract": "Human object interaction detection aims at localizing human-object pairs and recognizing their interactions. Trapped by the long-tailed distribution of the data, existing HOI detection methods often have difficulty recognizing the tail categories. Many approaches try to improve the recognition of HOI tasks by utilizing external knowledge (e.g. pre-trained visual-language models). However, these approaches mainly utilize external knowledge at the HOI combination level and achieve limited improvement in the tail categories. In this paper, we propose a dual-prior augmented decoding network by decomposing the HOI task into two sub-tasks: human-object pair detection and interaction recognition. For each subtask, we leverage external knowledge to enhance the model's ability at a finer granularity. Specifically, we acquire the prior candidates from an external classifier and embed them to assist the subsequent decoding process. Thus, the long-tail problem is mitigated from a coarse-to-fine level with the corresponding external knowledge. Our approach outperforms existing state-of-the-art models in various settings and significantly boosts the performance on the tail HOI categories. The source code is available at https://github.com/PRIS-CV/DP-ADN",
    "checked": true,
    "id": "cbc05849bd04058e4aee09738dd73d9ad6621964",
    "semantic_title": "dual-prior augmented decoding network for long tail distribution in hoi detection",
    "citation_count": 0,
    "authors": [
      "Jiayi Gao",
      "Kongming Liang",
      "Tao Wei",
      "Wei Chen",
      "Zhanyu Ma",
      "Jun Guo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27950": {
    "title": "LAMM: Label Alignment for Multi-Modal Prompt Learning",
    "volume": "main",
    "abstract": "With the success of pre-trained visual-language (VL) models such as CLIP in visual representation tasks, transferring pre-trained models to downstream tasks has become a crucial paradigm. Recently, the prompt tuning paradigm, which draws inspiration from natural language processing (NLP), has made significant progress in VL field. However, preceding methods mainly focus on constructing prompt templates for text and visual inputs, neglecting the gap in class label representations between the VL models and downstream tasks. To address this challenge, we introduce an innovative label alignment method named \\textbf{LAMM}, which can dynamically adjust the category embeddings of downstream datasets through end-to-end training. Moreover, to achieve a more appropriate label distribution, we propose a hierarchical loss, encompassing the alignment of the parameter space, feature space, and logits space. We conduct experiments on 11 downstream vision datasets and demonstrate that our method significantly improves the performance of existing multi-modal prompt learning models in few-shot scenarios, exhibiting an average accuracy improvement of 2.31(\\%) compared to the state-of-the-art methods on 16 shots. Moreover, our methodology exhibits the preeminence in continual learning compared to other prompt tuning methods. Importantly, our method is synergistic with existing prompt tuning methods and can boost the performance on top of them. Our code and dataset will be publicly available at https://github.com/gaojingsheng/LAMM",
    "checked": true,
    "id": "166802fb539fafbf4be0d6956defd26d70ab2cfe",
    "semantic_title": "lamm: label alignment for multi-modal prompt learning",
    "citation_count": 2,
    "authors": [
      "Jingsheng Gao",
      "Jiacheng Ruan",
      "Suncheng Xiang",
      "Zefang Yu",
      "Ke Ji",
      "Mingye Xie",
      "Ting Liu",
      "Yuzhuo Fu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27951": {
    "title": "Frequency-Controlled Diffusion Model for Versatile Text-Guided Image-to-Image Translation",
    "volume": "main",
    "abstract": "Recently, text-to-image diffusion models have emerged as a powerful tool for image-to-image translation (I2I), allowing flexible image translation via user-provided text prompts. This paper proposes frequency-controlled diffusion model (FCDiffusion), an end-to-end diffusion-based framework contributing a novel solution to text-guided I2I from a frequency-domain perspective. At the heart of our framework is a feature-space frequency-domain filtering module based on Discrete Cosine Transform, which extracts image features carrying different DCT spectral bands to control the text-to-image generation process of the Latent Diffusion Model, realizing versatile I2I applications including style-guided content creation, image semantic manipulation, image scene translation, and image style translation. Different from related methods, FCDiffusion establishes a unified text-driven I2I framework suiting diverse I2I application scenarios simply by switching among different frequency control branches. The effectiveness and superiority of our method for text-guided I2I are demonstrated with extensive experiments both qualitatively and quantitatively. Our project is publicly available at: https://xianggao1102.github.io/FCDiffusion/",
    "checked": true,
    "id": "889ccbd40ee023aab59f9f376fbec79e2823d93d",
    "semantic_title": "frequency-controlled diffusion model for versatile text-guided image-to-image translation",
    "citation_count": 0,
    "authors": [
      "Xiang Gao",
      "Zhengbo Xu",
      "Junhan Zhao",
      "Jiaying Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27952": {
    "title": "A General Implicit Framework for Fast NeRF Composition and Rendering",
    "volume": "main",
    "abstract": "A variety of Neural Radiance Fields (NeRF) methods have recently achieved remarkable success in high render speed. However, current accelerating methods are specialized and incompatible with various implicit methods, preventing real-time composition over various types of NeRF works. Because NeRF relies on sampling along rays, it is possible to provide general guidance for acceleration. To that end, we propose a general implicit pipeline for composing NeRF objects quickly. Our method enables the casting of dynamic shadows within or between objects using analytical light sources while allowing multiple NeRF objects to be seamlessly placed and rendered together with any arbitrary rigid transformations. Mainly, our work introduces a new surface representation known as Neural Depth Fields (NeDF) that quickly determines the spatial relationship between objects by allowing direct intersection computation between rays and implicit surfaces. It leverages an intersection neural network to query NeRF for acceleration instead of depending on an explicit spatial structure.Our proposed method is the first to enable both the progressive and interactive composition of NeRF objects. Additionally, it also serves as a previewing plugin for a range of existing NeRF works",
    "checked": true,
    "id": "d0ab642f46a81b88bf7bad8af3c089e2a21b5241",
    "semantic_title": "a general implicit framework for fast nerf composition and rendering",
    "citation_count": 0,
    "authors": [
      "Xinyu Gao",
      "Ziyi Yang",
      "Yunlu Zhao",
      "Yuxiang Sun",
      "Xiaogang Jin",
      "Changqing Zou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27953": {
    "title": "Multi-Scene Generalized Trajectory Global Graph Solver with Composite Nodes for Multiple Object Tracking",
    "volume": "main",
    "abstract": "The global multi-object tracking (MOT) system can consider interaction, occlusion, and other ``visual blur'' scenarios to ensure effective object tracking in long videos. Among them, graph-based tracking-by-detection paradigms achieve surprising performance. However, their fully-connected nature poses storage space requirements that challenge algorithm handling long videos. Currently, commonly used methods are still generated trajectories by building one-forward associations across frames. Such matches produced under the guidance of first-order similarity information may not be optimal from a longer-time perspective. Moreover, they often lack an end-to-end scheme for correcting mismatches. This paper proposes the Composite Node Message Passing Network (CoNo-Link), a multi-scene generalized framework for modeling ultra-long frames information for association. CoNo-Link's solution is a low-storage overhead method for building constrained connected graphs. In addition to the previous method of treating objects as nodes, the network innovatively treats object trajectories as nodes for information interaction, improving the graph neural network's feature representation capability. Specifically, we formulate the graph-building problem as a top-k selection task for some reliable objects or trajectories. Our model can learn better predictions on longer-time scales by adding composite nodes. As a result, our method outperforms the state-of-the-art in several commonly used datasets",
    "checked": true,
    "id": "033c3db5906468afe360bd3897bfd72a989e5c34",
    "semantic_title": "multi-scene generalized trajectory global graph solver with composite nodes for multiple object tracking",
    "citation_count": 0,
    "authors": [
      "Yan Gao",
      "Haojun Xu",
      "Jie Li",
      "Nannan Wang",
      "Xinbo Gao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27954": {
    "title": "A Dual Stealthy Backdoor: From Both Spatial and Frequency Perspectives",
    "volume": "main",
    "abstract": "Backdoor attacks pose serious security threats to deep neural networks (DNNs). Backdoored models make arbitrarily (targeted) incorrect predictions on inputs containing well-designed triggers, while behaving normally on clean inputs. Prior researches have explored the invisibility of backdoor triggers to enhance attack stealthiness. However, most of them only focus on the invisibility in the spatial domain, neglecting the generation of invisible triggers in the frequency domain. This limitation renders the generated poisoned images easily detectable by recent defense methods. To address this issue, we propose a DUal stealthy BAckdoor attack method named DUBA, which simultaneously considers the invisibility of triggers in both the spatial and frequency domains, to achieve desirable attack performance, while ensuring strong stealthiness. Specifically, we first use Wavelet Transform to embed the high-frequency information of the trigger image into the clean image to ensure attack effectiveness. Then, to attain strong stealthiness, we incorporate Fourier Transform and Cosine Transform to mix the poisoned image and clean image in the frequency domain. Moreover, DUBA adopts a novel attack strategy, training the model with weak triggers and attacking with strong triggers to further enhance attack performance and stealthiness. DUBA is evaluated extensively on four datasets against popular image classifiers, showing significant superiority over state-of-the-art backdoor attacks in attack success rate and stealthiness",
    "checked": true,
    "id": "0982a5b5e88e8ee4eee3758fbaf7f8a99a4ee4fe",
    "semantic_title": "a dual stealthy backdoor: from both spatial and frequency perspectives",
    "citation_count": 1,
    "authors": [
      "Yudong Gao",
      "Honglong Chen",
      "Peng Sun",
      "Junjian Li",
      "Anqing Zhang",
      "Zhibo Wang",
      "Weifeng Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27955": {
    "title": "SoftCLIP: Softer Cross-Modal Alignment Makes CLIP Stronger",
    "volume": "main",
    "abstract": "During the preceding biennium, vision-language pre-training has achieved noteworthy success on several downstream tasks. Nevertheless, acquiring high-quality image-text pairs, where the pairs are entirely exclusive of each other, remains a challenging task, and noise exists in the commonly used datasets. To address this issue, we propose SoftCLIP, a novel approach that relaxes the strict one-to-one constraint and achieves a soft cross-modal alignment by introducing a softened target, which is generated from the fine-grained intra-modal self-similarity. The intra-modal guidance is indicative to enable two pairs have some local similarities and model many-to-many relationships between the two modalities. Besides, since the positive still dominates in the softened target distribution, we disentangle the negatives in the distribution to further boost the relation alignment with the negatives in the cross-modal learning. Extensive experiments demonstrate the effectiveness of SoftCLIP. In particular, on ImageNet zero-shot classification task, using CC3M/CC12M as pre-training dataset, SoftCLIP brings a top-1 accuracy improvement of 6.8%/7.2% over the CLIP baseline",
    "checked": true,
    "id": "9df8e151aadbc3c4df81524f7bb6dee3256ea8dc",
    "semantic_title": "softclip: softer cross-modal alignment makes clip stronger",
    "citation_count": 19,
    "authors": [
      "Yuting Gao",
      "Jinfeng Liu",
      "Zihan Xu",
      "Tong Wu",
      "Enwei Zhang",
      "Ke Li",
      "Jie Yang",
      "Wei  Liu",
      "Xing Sun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27956": {
    "title": "Composite Sketch+Text Queries for Retrieving Objects with Elusive Names and Complex Interactions",
    "volume": "main",
    "abstract": "Non-native speakers with limited vocabulary often struggle to name specific objects despite being able to visualize them, e.g., people outside Australia searching for ‘numbats.' Further, users may want to search for such elusive objects with difficult-to-sketch interactions, e.g., \"numbat digging in the ground.\" In such common but complex situations, users desire a search interface that accepts composite multimodal queries comprising hand-drawn sketches of \"difficult-to-name but easy-to-draw\" objects and text describing \"difficult-to-sketch but easy-to-verbalize\" object's attributes or interaction with the scene. This novel problem statement distinctly differs from the previously well-researched TBIR (text-based image retrieval) and SBIR (sketch-based image retrieval) problems. To study this under-explored task, we curate a dataset, CSTBIR (Composite Sketch+Text Based Image Retrieval), consisting of ~2M queries and 108K natural scene images. Further, as a solution to this problem, we propose a pretrained multimodal transformer-based baseline, STNet (Sketch+Text Network), that uses a hand-drawn sketch to localize relevant objects in the natural scene image, and encodes the text and image to perform image retrieval. In addition to contrastive learning, we propose multiple training objectives that improve the performance of our model. Extensive experiments show that our proposed method outperforms several state-of-the-art retrieval methods for text-only, sketch-only, and composite query modalities. We make the dataset and code available at: https://vl2g.github.io/projects/cstbir",
    "checked": true,
    "id": "b2ebbd6ccec4e5de1791f0e1c3cd13332a2e668a",
    "semantic_title": "composite sketch+text queries for retrieving objects with elusive names and complex interactions",
    "citation_count": 0,
    "authors": [
      "Prajwal Gatti",
      "Kshitij Parikh",
      "Dhriti Prasanna Paul",
      "Manish Gupta",
      "Anand Mishra"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27957": {
    "title": "Neuromorphic Event Signal-Driven Network for Video De-raining",
    "volume": "main",
    "abstract": "Convolutional neural networks-based video de-raining methods commonly rely on dense intensity frames captured by CMOS sensors. However, the limited temporal resolution of these sensors hinders the capture of dynamic rainfall information, limiting further improvement in de-raining performance. This study aims to overcome this issue by incorporating the neuromorphic event signal into the video de-raining to enhance the dynamic information perception. Specifically, we first utilize the dynamic information from the event signal as prior knowledge, and integrate it into existing de-raining objectives to better constrain the solution space. We then design an optimization algorithm to solve the objective, and construct a de-raining network with CNNs as the backbone architecture using a modular strategy to mimic the optimization process. To further explore the temporal correlation of the event signal, we incorporate a spiking self-attention module into our network. By leveraging the low latency and high temporal resolution of the event signal, along with the spatial and temporal representation capabilities of convolutional and spiking neural networks, our model captures more accurate dynamic information and significantly improves de-raining performance. For example, our network achieves a 1.24dB improvement on the SynHeavy25 dataset compared to the previous state-of-the-art method, while utilizing only 39% of the parameters",
    "checked": true,
    "id": "81413d393a3d5dc2f1300b90b9b365b8710cb69d",
    "semantic_title": "neuromorphic event signal-driven network for video de-raining",
    "citation_count": 0,
    "authors": [
      "Chengjie Ge",
      "Xueyang Fu",
      "Peng He",
      "Kunyu Wang",
      "Chengzhi Cao",
      "Zheng-Jun Zha"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27958": {
    "title": "Beyond Prototypes: Semantic Anchor Regularization for Better Representation Learning",
    "volume": "main",
    "abstract": "One of the ultimate goals of representation learning is to achieve compactness within a class and well-separability between classes. Many outstanding metric-based and prototype-based methods following the Expectation-Maximization paradigm, have been proposed for this objective. However, they inevitably introduce biases into the learning process, particularly with long-tail distributed training data. In this paper, we reveal that the class prototype is not necessarily to be derived from training features and propose a novel perspective to use pre-defined class anchors serving as feature centroid to unidirectionally guide feature learning. However, the pre-defined anchors may have a large semantic distance from the pixel features, which prevents them from being directly applied. To address this issue and generate feature centroid independent from feature learning, a simple yet effective Semantic Anchor Regularization (SAR) is proposed. SAR ensures the inter-class separability of semantic anchors in the semantic space by employing a classifier-aware auxiliary cross-entropy loss during training via disentanglement learning. By pulling the learned features to these semantic anchors, several advantages can be attained: 1) the intra-class compactness and naturally inter-class separability, 2) induced bias or errors from feature learning can be avoided, and 3) robustness to the long-tailed problem. The proposed SAR can be used in a plug-and-play manner in the existing models. Extensive experiments demonstrate that the SAR performs better than previous sophisticated prototype-based methods. The implementation is available at https://github.com/geyanqi/SAR",
    "checked": true,
    "id": "b30db6576795858f9618dd343677615f9e223d42",
    "semantic_title": "beyond prototypes: semantic anchor regularization for better representation learning",
    "citation_count": 3,
    "authors": [
      "Yanqi Ge",
      "Qiang Nie",
      "Ye Huang",
      "Yong Liu",
      "Chengjie Wang",
      "Feng Zheng",
      "Wen Li",
      "Lixin Duan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27959": {
    "title": "Learning Multi-Scale Video-Text Correspondence for Weakly Supervised Temporal Article Gronding",
    "volume": "main",
    "abstract": "Weakly Supervised temporal Article Grounding (WSAG) is a challenging and practical task in video understanding. Specifically, given a video and a relevant article, whose sentences are at different semantic scales, WSAG aims to localize corresponding video segments for all \"groundable\" sentences. Compared to other grounding tasks, e.g., localizing one target segment with respect to a given sentence query, WSAG confronts an essential obstacle rooted in the intricate multi-scale information inherent within both textual and visual modalities. Existing methods overlook the modeling and alignment of such structured information present in multi-scale video segments and hierarchical textual content. To this end, we propose a Multi-Scale Video-Text Correspondence Learning (MVTCL) framework, which enhances the grounding performance in complex scenes by modeling multi-scale semantic correspondence both within and between modalities. Specifically, MVTCL initially aggregates video content spanning distinct temporal scales and leverages hierarchical textual relationships in both temporal and semantic dimensions via a semantic calibration module. Then multi-scale contrastive learning module is introduced to generate more discriminative representations by selecting typical contexts and performing inter-video contrastive learning. Through the multi-scale semantic calibration architecture and supervision design, our method achieves new state-of-the-art performance on existing WSAG benchmarks",
    "checked": true,
    "id": "7d7c9e92ae6c234a5802e560c61bb461e26d0b99",
    "semantic_title": "learning multi-scale video-text correspondence for weakly supervised temporal article gronding",
    "citation_count": 0,
    "authors": [
      "Wenjia Geng",
      "Yong Liu",
      "Lei Chen",
      "Sujia Wang",
      "Jie Zhou",
      "Yansong Tang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27960": {
    "title": "PoseGen: Learning to Generate 3D Human Pose Dataset with NeRF",
    "volume": "main",
    "abstract": "This paper proposes an end-to-end framework for generating 3D human pose datasets using Neural Radiance Fields (NeRF). Public datasets generally have limited diversity in terms of human poses and camera viewpoints, largely due to the resource-intensive nature of collecting 3D human pose data. As a result, pose estimators trained on public datasets significantly underperform when applied to unseen out-of-distribution samples. Previous works proposed augmenting public datasets by generating 2D-3D pose pairs or rendering a large amount of random data. Such approaches either overlook image rendering or result in suboptimal datasets for pre-trained models. Here we propose PoseGen, which learns to generate a dataset (human 3D poses and images) with a feedback loss from a given pre-trained pose estimator. In contrast to prior art, our generated data is optimized to improve the robustness of the pre-trained model. The objective of PoseGen is to learn a distribution of data that maximizes the prediction error of a given pre-trained model. As the learned data distribution contains OOD samples of the pre-trained model, sampling data from such a distribution for further fine-tuning a pre-trained model improves the generalizability of the model. This is the first work that proposes NeRFs for 3D human data generation. NeRFs are data-driven and do not require 3D scans of humans. Therefore, using NeRF for data generation is a new direction for convenient user-specific data generation. Our extensive experiments show that the proposed PoseGen improves two baseline models (SPIN and HybrIK) on four datasets with an average 6% relative improvement",
    "checked": true,
    "id": "76c3a0c313615d8f157af42fe7ca2488a9fed85b",
    "semantic_title": "posegen: learning to generate 3d human pose dataset with nerf",
    "citation_count": 0,
    "authors": [
      "Mohsen Gholami",
      "Rabab Ward",
      "Z. Jane Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27961": {
    "title": "SDAC: A Multimodal Synthetic Dataset for Anomaly and Corner Case Detection in Autonomous Driving",
    "volume": "main",
    "abstract": "Nowadays, closed-set perception methods for autonomous driving perform well on datasets containing normal scenes. However, they still struggle to handle anomalies in the real world, such as unknown objects that have never been seen while training. The lack of public datasets to evaluate the model performance on anomaly and corner cases has hindered the development of reliable autonomous driving systems. Therefore, we propose a multimodal Synthetic Dataset for Anomaly and Corner case detection, called SDAC, which encompasses anomalies captured from multi-view cameras and the LiDAR sensor, providing a rich set of annotations for multiple mainstream perception tasks. SDAC is the first public dataset for autonomous driving that categorizes anomalies into object, scene, and scenario levels, allowing the evaluation under different anomalous conditions. Experiments show that closed-set models suffer significant performance drops on anomaly subsets in SDAC. Existing anomaly detection methods fail to achieve satisfactory performance, suggesting that anomaly detection remains a challenging problem. We anticipate that our SDAC dataset could foster the development of safe and reliable systems for autonomous driving",
    "checked": true,
    "id": "66adf0186306940efa4896b3f35fdeab205667e5",
    "semantic_title": "sdac: a multimodal synthetic dataset for anomaly and corner case detection in autonomous driving",
    "citation_count": 2,
    "authors": [
      "Lei Gong",
      "Yu Zhang",
      "Yingqing Xia",
      "Yanyong Zhang",
      "Jianmin Ji"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27962": {
    "title": "ContactGen: Contact-Guided Interactive 3D Human Generation for Partners",
    "volume": "main",
    "abstract": "Among various interactions between humans, such as eye contact and gestures, physical interactions by contact can act as an essential moment in understanding human behaviors. Inspired by this fact, given a 3D partner human with the desired interaction label, we introduce a new task of 3D human generation in terms of physical contact. Unlike previous works of interacting with static objects or scenes, a given partner human can have diverse poses and different contact regions according to the type of interaction. To handle this challenge, we propose a novel method of generating interactive 3D humans for a given partner human based on a guided diffusion framework (ContactGen in short). Specifically, we newly present a contact prediction module that adaptively estimates potential contact regions between two input humans according to the interaction label. Using the estimated potential contact regions as complementary guidances, we dynamically enforce ContactGen to generate interactive 3D humans for a given partner human within a guided diffusion model. We demonstrate ContactGen on the CHI3D dataset, where our method generates physically plausible and diverse poses compared to comparison methods",
    "checked": true,
    "id": "8e3069a48c28f30a4dfd15c0f4798d425703e4e9",
    "semantic_title": "contactgen: contact-guided interactive 3d human generation for partners",
    "citation_count": 1,
    "authors": [
      "Dongjun Gu",
      "Jaehyeok Shim",
      "Jaehoon Jang",
      "Changwoo Kang",
      "Kyungdon Joo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27963": {
    "title": "AnomalyGPT: Detecting Industrial Anomalies Using Large Vision-Language Models",
    "volume": "main",
    "abstract": "Large Vision-Language Models (LVLMs) such as MiniGPT-4 and LLaVA have demonstrated the capability of understanding images and achieved remarkable performance in various visual tasks. Despite their strong abilities in recognizing common objects due to extensive training datasets, they lack specific domain knowledge and have a weaker understanding of localized details within objects, which hinders their effectiveness in the Industrial Anomaly Detection (IAD) task. On the other hand, most existing IAD methods only provide anomaly scores and necessitate the manual setting of thresholds to distinguish between normal and abnormal samples, which restricts their practical implementation. In this paper, we explore the utilization of LVLM to address the IAD problem and propose AnomalyGPT, a novel IAD approach based on LVLM. We generate training data by simulating anomalous images and producing corresponding textual descriptions for each image. We also employ an image decoder to provide fine-grained semantic and design a prompt learner to fine-tune the LVLM using prompt embeddings. Our AnomalyGPT eliminates the need for manual threshold adjustments, thus directly assesses the presence and locations of anomalies. Additionally, AnomalyGPT supports multi-turn dialogues and exhibits impressive few-shot in-context learning capabilities. With only one normal shot, AnomalyGPT achieves the state-of-the-art performance with an accuracy of 86.1%, an image-level AUC of 94.1%, and a pixel-level AUC of 95.3% on the MVTec-AD dataset",
    "checked": true,
    "id": "f2ec0182c6646d3128afa5100f37d9de7b533463",
    "semantic_title": "anomalygpt: detecting industrial anomalies using large vision-language models",
    "citation_count": 24,
    "authors": [
      "Zhaopeng Gu",
      "Bingke Zhu",
      "Guibo Zhu",
      "Yingying Chen",
      "Ming Tang",
      "Jinqiao Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27964": {
    "title": "SeqRank: Sequential Ranking of Salient Objects",
    "volume": "main",
    "abstract": "Salient Object Ranking (SOR) is the process of predicting the order of an observer's attention to objects when viewing a complex scene. Existing SOR methods primarily focus on ranking various scene objects simultaneously by exploring their spatial and semantic properties. However, their solutions of simultaneously ranking all salient objects do not align with human viewing behavior, and may result in incorrect attention shift predictions. We observe that humans view a scene through a sequential and continuous process involving a cycle of foveating to objects of interest with our foveal vision while using peripheral vision to prepare for the next fixation location. For instance, when we see a flying kite, our foveal vision captures the kite itself, while our peripheral vision can help us locate the person controlling it such that we can smoothly divert our attention to it next. By repeatedly carrying out this cycle, we can gain a thorough understanding of the entire scene. Based on this observation, we propose to model the dynamic interplay between foveal and peripheral vision to predict human attention shifts sequentially. To this end, we propose a novel SOR model, SeqRank, which reproduces foveal vision to extract high-acuity visual features for accurate salient instance segmentation while also modeling peripheral vision to select the object that is likely to grab the viewer's attention next. By incorporating both types of vision, our model can mimic human viewing behavior better and provide a more faithful ranking among various scene objects. Most notably, our model improves the SA-SOR/MAE scores by +6.1%/-13.0% on IRSR, compared with the state-of-the-art. Extensive experiments show the superior performance of our model on the SOR benchmarks. Code is available at https://github.com/guanhuankang/SeqRank",
    "checked": true,
    "id": "a9cd7d9e17b14697733e55959c7e56b28a67b831",
    "semantic_title": "seqrank: sequential ranking of salient objects",
    "citation_count": 0,
    "authors": [
      "Huankang Guan",
      "Rynson W.H. Lau"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27965": {
    "title": "Knowledge-Aware Neuron Interpretation for Scene Classification",
    "volume": "main",
    "abstract": "Although neural models have achieved remarkable performance, they still encounter doubts due to the intransparency. To this end, model prediction explanation is attracting more and more attentions. However, current methods rarely incorporate external knowledge and still suffer from three limitations: (1) Neglecting concept completeness. Merely selecting concepts may not sufficient for prediction. (2) Lacking concept fusion. Failure to merge semantically-equivalent concepts. (3) Difficult in manipulating model behavior. Lack of verification for explanation on original model. To address these issues, we propose a novel knowledge-aware neuron interpretation framework to explain model predictions for image scene classification. Specifically, for concept completeness, we present core concepts of a scene based on knowledge graph, ConceptNet, to gauge the completeness of concepts. Our method, incorporating complete concepts, effectively provides better prediction explanations compared to baselines. Furthermore, for concept fusion, we introduce a knowledge graph-based method known as Concept Filtering, which produces over 23% point gain on neuron behaviors for neuron interpretation. At last, we propose Model Manipulation, which aims to study whether the core concepts based on ConceptNet could be employed to manipulate model behavior. The results show that core concepts can effectively improve the performance of original model by over 26%",
    "checked": true,
    "id": "e67bc90687a8cb1140af4c1666c797ef3b6f5a20",
    "semantic_title": "knowledge-aware neuron interpretation for scene classification",
    "citation_count": 0,
    "authors": [
      "Yong Guan",
      "Freddy Lécué",
      "Jiaoyan Chen",
      "Ru Li",
      "Jeff Z. Pan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27966": {
    "title": "Self-Supervised Representation Learning with Meta Comprehensive Regularization",
    "volume": "main",
    "abstract": "Self-Supervised Learning (SSL) methods harness the concept of semantic invariance by utilizing data augmentation strategies to produce similar representations for different deformations of the same input. Essentially, the model captures the shared information among multiple augmented views of samples, while disregarding the non-shared information that may be beneficial for downstream tasks. To address this issue, we introduce a module called CompMod with Meta Comprehensive Regularization (MCR), embedded into existing self-supervised frameworks, to make the learned representations more comprehensive. Specifically, we update our proposed model through a bi-level optimization mechanism, enabling it to capture comprehensive features. Additionally, guided by the constrained extraction of features using maximum entropy coding, the self-supervised learning model learns more comprehensive features on top of learning consistent features. In addition, we provide theoretical support for our proposed method from information theory and causal counterfactual perspective. Experimental results show that our method achieves significant improvement in classification, object detection and semantic segmentation tasks on multiple benchmark datasets",
    "checked": true,
    "id": "64826ea91cb06cb27f03c3b6eb334e0a2b5efd3b",
    "semantic_title": "self-supervised representation learning with meta comprehensive regularization",
    "citation_count": 2,
    "authors": [
      "Huijie Guo",
      "Ying Ba",
      "Jie Hu",
      "Lingyu Si",
      "Wenwen Qiang",
      "Lei Shi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27967": {
    "title": "Graph Context Transformation Learning for Progressive Correspondence Pruning",
    "volume": "main",
    "abstract": "Most of existing correspondence pruning methods only concentrate on gathering the context information as much as possible while neglecting effective ways to utilize such information. In order to tackle this dilemma, in this paper we propose Graph Context Transformation Network (GCT-Net) enhancing context information to conduct consensus guidance for progressive correspondence pruning. Specifically, we design the Graph Context Enhance Transformer which first generates the graph network and then transforms it into multi-branch graph contexts. Moreover, it employs self-attention and cross-attention to magnify characteristics of each graph context for emphasizing the unique as well as shared essential information. To further apply the recalibrated graph contexts to the global domain, we propose the Graph Context Guidance Transformer. This module adopts a confident-based sampling strategy to temporarily screen high-confidence vertices for guiding accurate classification by searching global consensus between screened vertices and remaining ones. The extensive experimental results on outlier removal and relative pose estimation clearly demonstrate the superior performance of GCT-Net compared to state-of-the-art methods across outdoor and indoor datasets",
    "checked": true,
    "id": "4f7d087ec9c1cb13cdc060635219909ad0595e71",
    "semantic_title": "graph context transformation learning for progressive correspondence pruning",
    "citation_count": 1,
    "authors": [
      "Junwen Guo",
      "Guobao Xiao",
      "Shiping Wang",
      "Jun Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27968": {
    "title": "Depth-Guided Robust and Fast Point Cloud Fusion NeRF for Sparse Input Views",
    "volume": "main",
    "abstract": "Novel-view synthesis with sparse input views is important for real-world applications like AR/VR and autonomous driving. Recent methods have integrated depth information into NeRFs for sparse input synthesis, leveraging depth prior for geometric and spatial understanding. However, most existing works tend to overlook inaccuracies within depth maps and have low time efficiency. To address these issues, we propose a depth-guided robust and fast point cloud fusion NeRF for sparse inputs. We perceive radiance fields as an explicit voxel grid of features. A point cloud is constructed for each input view, characterized within the voxel grid using matrices and vectors. We accumulate the point cloud of each input view to construct the fused point cloud of the entire scene. Each voxel determines its density and appearance by referring to the point cloud of the entire scene. Through point cloud fusion and voxel grid fine-tuning, inaccuracies in depth values are refined or substituted by those from other views. Moreover, our method can achieve faster reconstruction and greater compactness through effective vector-matrix decomposition. Experimental results underline the superior performance and time efficiency of our approach compared to state-of-the-art baselines",
    "checked": true,
    "id": "736352670e479f57ad1b352dbd4901093e1be6f3",
    "semantic_title": "depth-guided robust and fast point cloud fusion nerf for sparse input views",
    "citation_count": 1,
    "authors": [
      "Shuai Guo",
      "Qiuwen Wang",
      "Yijie Gao",
      "Rong Xie",
      "Li Song"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27969": {
    "title": "Improving Panoptic Narrative Grounding by Harnessing Semantic Relationships and Visual Confirmation",
    "volume": "main",
    "abstract": "Recent advancements in single-stage Panoptic Narrative Grounding (PNG) have demonstrated significant potential. These methods predict pixel-level masks by directly matching pixels and phrases. However, they often neglect the modeling of semantic and visual relationships between phrase-level instances, limiting their ability for complex multi-modal reasoning in PNG. To tackle this issue, we propose XPNG, a \"differentiation-refinement-localization\" reasoning paradigm for accurately locating instances or regions. In XPNG, we introduce a Semantic Context Convolution (SCC) module to leverage semantic priors for generating distinctive features. This well-crafted module employs a combination of dynamic channel-wise convolution and pixel-wise convolution to embed semantic information and establish inter-object relationships guided by semantics. Subsequently, we propose a Visual Context Verification (VCV) module to provide visual cues, eliminating potential space biases introduced by semantics and further refining the visual features generated by the previous module. Extensive experiments on PNG benchmark datasets reveal that our approach achieves state-of-the-art performance, significantly outperforming existing methods by a considerable margin and yielding a 3.9-point improvement in overall metrics. Our codes and results are available at our project webpage: https://github.com/TianyuGoGO/XPNG",
    "checked": true,
    "id": "aa50b841a5553a3ab81a816653b299f948538118",
    "semantic_title": "improving panoptic narrative grounding by harnessing semantic relationships and visual confirmation",
    "citation_count": 2,
    "authors": [
      "Tianyu Guo",
      "Haowei Wang",
      "Yiwei Ma",
      "Jiayi Ji",
      "Xiaoshuai Sun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27970": {
    "title": "Learning to Manipulate Artistic Images",
    "volume": "main",
    "abstract": "Recent advancement in computer vision has significantly lowered the barriers to artistic creation. Exemplar-based image translation methods have attracted much attention due to flexibility and controllability. However, these methods hold assumptions regarding semantics or require semantic information as the input, while accurate semantics is not easy to obtain in artistic images. Besides, these methods suffer from cross-domain artifacts due to training data prior and generate imprecise structure due to feature compression in the spatial domain. In this paper, we propose an arbitrary Style Image Manipulation Network (SIM-Net), which leverages semantic-free information as guidance and a region transportation strategy in a self-supervised manner for image generation. Our method balances computational efficiency and high resolution to a certain extent. Moreover, our method facilitates zero-shot style image manipulation. Both qualitative and quantitative experiments demonstrate the superiority of our method over state-of-the-art methods.Code is available at https://github.com/SnailForce/SIM-Net",
    "checked": true,
    "id": "18ea6cb23b926073408827fc895c3d4971ecd1a5",
    "semantic_title": "learning to manipulate artistic images",
    "citation_count": 0,
    "authors": [
      "Wei Guo",
      "Yuqi Zhang",
      "De Ma",
      "Qian Zheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27971": {
    "title": "PICNN: A Pathway towards Interpretable Convolutional Neural Networks",
    "volume": "main",
    "abstract": "Convolutional Neural Networks (CNNs) have exhibited great performance in discriminative feature learning for complex visual tasks. Besides discrimination power, interpretability is another important yet under-explored property for CNNs. One difficulty in the CNN interpretability is that filters and image classes are entangled. In this paper, we introduce a novel pathway to alleviate the entanglement between filters and image classes. The proposed pathway groups the filters in a late conv-layer of CNN into class-specific clusters. Clusters and classes are in a one-to-one relationship. Specifically, we use the Bernoulli sampling to generate the filter-cluster assignment matrix from a learnable filter-class correspondence matrix. To enable end-to-end optimization, we develop a novel reparameterization trick for handling the non-differentiable Bernoulli sampling. We evaluate the effectiveness of our method on ten widely used network architectures (including nine CNNs and a ViT) and five benchmark datasets. Experimental results have demonstrated that our method PICNN (the combination of standard CNNs with our proposed pathway) exhibits greater interpretability than standard CNNs while achieving higher or comparable discrimination power",
    "checked": true,
    "id": "5f924cc5c719b87d41733a692cc15ffeaccbf7f5",
    "semantic_title": "picnn: a pathway towards interpretable convolutional neural networks",
    "citation_count": 0,
    "authors": [
      "Wengang Guo",
      "Jiayi Yang",
      "Huilin Yin",
      "Qijun Chen",
      "Wei Ye"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27972": {
    "title": "GSN: Generalisable Segmentation in Neural Radiance Field",
    "volume": "main",
    "abstract": "Traditional Radiance Field (RF) representations capture details of a specific scene and must be trained afresh on each scene. Semantic feature fields have been added to RFs to facilitate several segmentation tasks. Generalised RF representations learn the principles of view interpolation. A generalised RF can render new views of an unknown and untrained scene, given a few views. We present a way to distil feature fields into the generalised GNT representation. Our GSN representation generates new views of unseen scenes on the fly along with consistent, per-pixel semantic features. This enables multi-view segmentation of arbitrary new scenes. We show different semantic features being distilled into generalised RFs. Our multi-view segmentation results are on par with methods that use traditional RFs. GSN closes the gap between standard and generalisable RF methods significantly. Project Page: https://vinayak-vg.github.io/GSN/",
    "checked": true,
    "id": "ffa2b13214b2e5a2caaa1cdbfd70f941777677f9",
    "semantic_title": "gsn: generalisable segmentation in neural radiance field",
    "citation_count": 1,
    "authors": [
      "Vinayak Gupta",
      "Rahul Goel",
      "Sirikonda Dhawal",
      "P. J.  Narayanan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27973": {
    "title": "AMD: Autoregressive Motion Diffusion",
    "volume": "main",
    "abstract": "Human motion generation aims to produce plausible human motion sequences according to various conditional inputs, such as text or audio. Despite the feasibility of existing methods in generating motion based on short prompts and simple motion patterns, they encounter difficulties when dealing with long prompts or complex motions. The challenges are two-fold: 1) the scarcity of human motion-captured data for long prompts and complex motions. 2) the high diversity of human motions in the temporal domain and the substantial divergence of distributions from conditional modalities, leading to a many-to-many mapping problem when generating motion with complex and long texts. In this work, we address these gaps by 1) elaborating the first dataset pairing long textual descriptions and 3D complex motions (HumanLong3D), and 2) proposing an autoregressive motion diffusion model (AMD). Specifically, AMD integrates the text prompt at the current timestep with the text prompt and action sequences at the previous timestep as conditional information to predict the current action sequences in an iterative manner. Furthermore, we present its generalization for X-to-Motion with \"No Modality Left Behind\", enabling for the first time the generation of high-definition and high-fidelity human motions based on user-defined modality input",
    "checked": true,
    "id": "50656eea23a65a716315ba88a0f741966d798d5e",
    "semantic_title": "amd: autoregressive motion diffusion",
    "citation_count": 8,
    "authors": [
      "Bo Han",
      "Hao Peng",
      "Minjing Dong",
      "Yi Ren",
      "Yixuan Shen",
      "Chang Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27974": {
    "title": "HuTuMotion: Human-Tuned Navigation of Latent Motion Diffusion Models with Minimal Feedback",
    "volume": "main",
    "abstract": "We introduce HuTuMotion, an innovative approach for generating natural human motions that navigates latent motion diffusion models by leveraging few-shot human feedback. Unlike existing approaches that sample latent variables from a standard normal prior distribution, our method adapts the prior distribution to better suit the characteristics of the data, as indicated by human feedback, thus enhancing the quality of motion generation. Furthermore, our findings reveal that utilizing few-shot feedback can yield performance levels on par with those attained through extensive human feedback. This discovery emphasizes the potential and efficiency of incorporating few-shot human-guided optimization within latent diffusion models for personalized and style-aware human motion generation applications. The experimental results show the significantly superior performance of our method over existing state-of-the-art approaches",
    "checked": true,
    "id": "44d319e3ecd0a5949a4b7c9c5de794f28f5aa7e4",
    "semantic_title": "hutumotion: human-tuned navigation of latent motion diffusion models with minimal feedback",
    "citation_count": 1,
    "authors": [
      "Gaoge Han",
      "Shaoli Huang",
      "Mingming Gong",
      "Jinglei Tang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27975": {
    "title": "MA-Net: Rethinking Neural Unit in the Light of Astrocytes",
    "volume": "main",
    "abstract": "The artificial neuron (N-N) model-based networks have accomplished extraordinary success for various vision tasks. However, as a simplification of the mammal neuron model, their structure is locked during training, resulting in overfitting and over-parameters. The astrocyte, newly explored by biologists, can adaptively modulate neuronal communication by inserting itself between neurons. The communication, between the astrocyte and neuron, is bidirectionally and shows the potential to alleviate issues raised by unidirectional communication in the N-N model. In this paper, we first elaborate on the artificial Multi-Astrocyte-Neuron (MA-N) model, which enriches the functionality of the artificial neuron model. Our MA-N model is formulated at both astrocyte- and neuron-level that mimics the bidirectional communication with temporal and joint mechanisms. Then, we construct the MA-Net network with the MA-N model, whose neural connections can be continuously and adaptively modulated during training. Experiments show that our MA-Net advances new state-of-the-art on multiple tasks while significantly reducing its parameters by connection optimization",
    "checked": true,
    "id": "f5af906b66fadd38d0ebd2c46321d2fbebbef776",
    "semantic_title": "ma-net: rethinking neural unit in the light of astrocytes",
    "citation_count": 0,
    "authors": [
      "Mengqiao Han",
      "Liyuan Pan",
      "Xiabi Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27976": {
    "title": "Dual-Perspective Knowledge Enrichment for Semi-supervised 3D Object Detection",
    "volume": "main",
    "abstract": "Semi-supervised 3D object detection is a promising yet under-explored direction to reduce data annotation costs, especially for cluttered indoor scenes. A few prior works, such as SESS and 3DIoUMatch, attempt to solve this task by utilizing a teacher model to generate pseudo-labels for unlabeled samples. However, the availability of unlabeled samples in the 3D domain is relatively limited compared to its 2D counterpart due to the greater effort required to collect 3D data. Moreover, the loose consistency regularization in SESS and restricted pseudo-label selection strategy in 3DIoUMatch lead to either low-quality supervision or a limited amount of pseudo labels. To address these issues, we present a novel Dual-Perspective Knowledge Enrichment approach named DPKE for semi-supervised 3D object detection. Our DPKE enriches the knowledge of limited training data, particularly unlabeled data, from two perspectives: data-perspective and feature-perspective. Specifically, from the data-perspective, we propose a class-probabilistic data augmentation method that augments the input data with additional instances based on the varying distribution of class probabilities. Our DPKE achieves feature-perspective knowledge enrichment by designing a geometry-aware feature matching method that regularizes feature-level similarity between object proposals from the student and teacher models. Extensive experiments on the two benchmark datasets demonstrate that our DPKE achieves superior performance over existing state-of-the-art approaches under various label ratio conditions. The source code and models will be made available to the public",
    "checked": true,
    "id": "443797f8dff272e35838567a366ee33859d601a3",
    "semantic_title": "dual-perspective knowledge enrichment for semi-supervised 3d object detection",
    "citation_count": 0,
    "authors": [
      "Yucheng Han",
      "Na Zhao",
      "Weiling Chen",
      "Keng Teck Ma",
      "Hanwang Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27977": {
    "title": "Exploiting the Social-Like Prior in Transformer for Visual Reasoning",
    "volume": "main",
    "abstract": "Benefiting from instrumental global dependency modeling of self-attention (SA), transformer-based approaches have become the pivotal choices for numerous downstream visual reasoning tasks, such as visual question answering (VQA) and referring expression comprehension (REC). However, some studies have recently suggested that SA tends to suffer from rank collapse thereby inevitably leads to representation degradation as the transformer layer goes deeper. Inspired by social network theory, we attempt to make an analogy between social behavior and regional information interaction in SA, and harness two crucial notions of structural hole and degree centrality in social network to explore the possible optimization towards SA learning, which naturally deduces two plug-and-play social-like modules. Based on structural hole, the former module allows to make information interaction in SA more structured, which effectively avoids redundant information aggregation and global feature homogenization for better rank remedy, followed by latter module to comprehensively characterize and refine the representation discrimination via considering degree centrality of regions and transitivity of relations. Without bells and whistles, our model outperforms a bunch of baselines by a noticeable margin when considering our social-like prior on five benchmarks in VQA and REC tasks, and a series of explanatory results are showcased to sufficiently reveal the social-like behaviors in SA",
    "checked": true,
    "id": "0eeedd5c5ad085b51dfaffa7386ba7cb539bbf4e",
    "semantic_title": "exploiting the social-like prior in transformer for visual reasoning",
    "citation_count": 0,
    "authors": [
      "Yudong Han",
      "Yupeng Hu",
      "Xuemeng Song",
      "Haoyu Tang",
      "Mingzhu Xu",
      "Liqiang Nie"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27978": {
    "title": "Improving Audio-Visual Segmentation with Bidirectional Generation",
    "volume": "main",
    "abstract": "The aim of audio-visual segmentation (AVS) is to precisely differentiate audible objects within videos down to the pixel level. Traditional approaches often tackle this challenge by combining information from various modalities, where the contribution of each modality is implicitly or explicitly modeled. Nevertheless, the interconnections between different modalities tend to be overlooked in audio-visual modeling. In this paper, inspired by the human ability to mentally simulate the sound of an object and its visual appearance, we introduce a bidirectional generation framework. This framework establishes robust correlations between an object's visual characteristics and its associated sound, thereby enhancing the performance of AVS. To achieve this, we employ a visual-to-audio projection component that reconstructs audio features from object segmentation masks and minimizes reconstruction errors. Moreover, recognizing that many sounds are linked to object movements, we introduce an implicit volumetric motion estimation module to handle temporal dynamics that may be challenging to capture using conventional optical flow methods. To showcase the effectiveness of our approach, we conduct comprehensive experiments and analyses on the widely recognized AVSBench benchmark. As a result, we establish a new state-of-the-art performance level in the AVS benchmark, particularly excelling in the challenging MS3 subset which involves segmenting multiple sound sources. Code is released in: https://github.com/OpenNLPLab/AVS-bidirectional",
    "checked": true,
    "id": "d782a43e36f2cd3c7f633c12251928219454ed95",
    "semantic_title": "improving audio-visual segmentation with bidirectional generation",
    "citation_count": 14,
    "authors": [
      "Dawei Hao",
      "Yuxin Mao",
      "Bowen He",
      "Xiaodong Han",
      "Yuchao Dai",
      "Yiran Zhong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27979": {
    "title": "Hand-Centric Motion Refinement for 3D Hand-Object Interaction via Hierarchical Spatial-Temporal Modeling",
    "volume": "main",
    "abstract": "Hands are the main medium when people interact with the world. Generating proper 3D motion for hand-object interaction is vital for applications such as virtual reality and robotics. Although grasp tracking or object manipulation synthesis can produce coarse hand motion, this kind of motion is inevitably noisy and full of jitter. To address this problem, we propose a data-driven method for coarse motion refinement. First, we design a hand-centric representation to describe the dynamic spatial-temporal relation between hands and objects. Compared to the object-centric representation, our hand-centric representation is straightforward and does not require an ambiguous projection process that converts object-based prediction into hand motion. Second, to capture the dynamic clues of hand-object interaction, we propose a new architecture that models the spatial and temporal structure in a hierarchical manner. Extensive experiments demonstrate that our method outperforms previous methods by a noticeable margin",
    "checked": true,
    "id": "c856badce6a0ade6e6e30abb5cda09b1cde8f607",
    "semantic_title": "hand-centric motion refinement for 3d hand-object interaction via hierarchical spatial-temporal modeling",
    "citation_count": 2,
    "authors": [
      "Yuze Hao",
      "Jianrong Zhang",
      "Tao Zhuo",
      "Fuan Wen",
      "Hehe Fan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27980": {
    "title": "Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "Compared to conventional semantic segmentation with pixel-level supervision, weakly supervised semantic segmentation (WSSS) with image-level labels poses the challenge that it commonly focuses on the most discriminative regions, resulting in a disparity between weakly and fully supervision scenarios. A typical manifestation is the diminished precision on object boundaries, leading to deteriorated accuracy of WSSS. To alleviate this issue, we propose to adaptively partition the image content into certain regions (e.g., confident foreground and background) and uncertain regions (e.g., object boundaries and misclassified categories) for separate processing. For uncertain cues, we propose an adaptive masking strategy and seek to recover the local information with self-distilled knowledge. We further assume that confident regions should be robust enough to preserve the global semantics, and introduce a complementary self-distillation method that constrains semantic consistency between confident regions and an augmented view with the same class labels. Extensive experiments conducted on PASCAL VOC 2012 and MS COCO 2014 demonstrate that our proposed single-stage approach for WSSS not only outperforms state-of-the-art counterparts but also surpasses multi-stage methods that trade complexity for accuracy",
    "checked": true,
    "id": "c74bea3c612db7dfe422426b407c07eec44d2054",
    "semantic_title": "progressive feature self-reinforcement for weakly supervised semantic segmentation",
    "citation_count": 1,
    "authors": [
      "Jingxuan He",
      "Lechao Cheng",
      "Chaowei Fang",
      "Zunlei Feng",
      "Tingting Mu",
      "Mingli Song"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27981": {
    "title": "Prompting Multi-Modal Image Segmentation with Semantic Grouping",
    "volume": "main",
    "abstract": "Multi-modal image segmentation is one of the core issues in computer vision. The main challenge lies in integrating common information between modalities while retaining specific patterns for each modality. Existing methods typically perform full fine-tuning on RGB-based pre-trained parameters to inherit the powerful representation of the foundation model. Although effective, such paradigm is not optimal due to weak transferability and scarce downstream data. Inspired by the recent success of prompt learning in language models, we propose the Grouping Prompt Tuning Framework (GoPT), which introduces explicit semantic grouping to learn modal-related prompts, adapting the frozen pre-trained foundation model to various downstream multi-modal segmentation tasks. Specifically, a class-aware uni-modal prompter is designed to balance intra- and inter-modal semantic propagation by grouping modality-specific class tokens, thereby improving the adaptability of spatial information. Furthermore, an alignment-induced cross-modal prompter is introduced to aggregate class-aware representations and share prompt parameters among different modalities to assist in modeling common statistics. Extensive experiments show the superiority of our GoPT, which achieves SOTA performance on various downstream multi-modal image segmentation tasks by training only < 1% model parameters",
    "checked": true,
    "id": "e92fc5e127f2dfe3832d5d66d7130b6c0a25391d",
    "semantic_title": "prompting multi-modal image segmentation with semantic grouping",
    "citation_count": 3,
    "authors": [
      "Qibin He"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27982": {
    "title": "Low-Latency Space-Time Supersampling for Real-Time Rendering",
    "volume": "main",
    "abstract": "With the rise of real-time rendering and the evolution of display devices, there is a growing demand for post-processing methods that offer high-resolution content in a high frame rate. Existing techniques often suffer from quality and latency issues due to the disjointed treatment of frame supersampling and extrapolation. In this paper, we recognize the shared context and mechanisms between frame supersampling and extrapolation, and present a novel framework, Space-time Supersampling (STSS). By integrating them into a unified framework, STSS can improve the overall quality with lower latency. To implement an efficient architecture, we treat the aliasing and warping holes unified as reshading regions and put forth two key components to compensate the regions, namely Random Reshading Masking (RRM) and Efficient Reshading Module (ERM). Extensive experiments demonstrate that our approach achieves superior visual fidelity compared to state-of-the-art (SOTA) methods. Notably, the performance is achieved within only 4ms, saving up to 75\\% of time against the conventional two-stage pipeline that necessitates 17ms",
    "checked": true,
    "id": "2be825c40596e749062e43bc6a2adf1b42af4da2",
    "semantic_title": "low-latency space-time supersampling for real-time rendering",
    "citation_count": 0,
    "authors": [
      "Ruian He",
      "Shili Zhou",
      "Yuqi Sun",
      "Ri Cheng",
      "Weimin Tan",
      "Bo Yan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27983": {
    "title": "Collaborative Weakly Supervised Video Correlation Learning for Procedure-Aware Instructional Video Analysis",
    "volume": "main",
    "abstract": "Video Correlation Learning (VCL), which aims to analyze the relationships between videos, has been widely studied and applied in various general video tasks. However, applying VCL to instructional videos is still quite challenging due to their intrinsic procedural temporal structure. Specifically, procedural knowledge is critical for accurate correlation analyses on instructional videos. Nevertheless, current procedure-learning methods heavily rely on step-level annotations, which are costly and not scalable. To address this problem, we introduce a weakly supervised framework called Collaborative Procedure Alignment (CPA) for procedure-aware correlation learning on instructional videos. Our framework comprises two core modules: collaborative step mining and frame-to-step alignment. The collaborative step mining module enables simultaneous and consistent step segmentation for paired videos, leveraging the semantic and temporal similarity between frames. Based on the identified steps, the frame-to-step alignment module performs alignment between the frames and steps across videos. The alignment result serves as a measurement of the correlation distance between two videos. We instantiate our framework in two distinct instructional video tasks: sequence verification and action quality assessment. Extensive experiments validate the effectiveness of our approach in providing accurate and interpretable correlation analyses for instructional videos",
    "checked": true,
    "id": "d773f7c84bd82f2c95ce41510760f6e234f8482f",
    "semantic_title": "collaborative weakly supervised video correlation learning for procedure-aware instructional video analysis",
    "citation_count": 2,
    "authors": [
      "Tianyao He",
      "Huabin Liu",
      "Yuxi  Li",
      "Xiao Ma",
      "Cheng Zhong",
      "Yang Zhang",
      "Weiyao Lin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27984": {
    "title": "Frequency-Adaptive Pan-Sharpening with Mixture of Experts",
    "volume": "main",
    "abstract": "Pan-sharpening involves reconstructing missing high-frequency information in multi-spectral images with low spatial resolution, using a higher-resolution panchromatic image as guidance. Although the inborn connection with frequency domain, existing pan-sharpening research has not almost investigated the potential solution upon frequency domain. To this end, we propose a novel Frequency Adaptive Mixture of Experts (FAME) learning framework for pan-sharpening, which consists of three key components: the Adaptive Frequency Separation Prediction Module, the Sub-Frequency Learning Expert Module, and the Expert Mixture Module. In detail, the first leverages the discrete cosine transform to perform frequency separation by predicting the frequency mask. On the basis of generated mask, the second with low-frequency MOE and high-frequency MOE takes account for enabling the effective low-frequency and high-frequency information reconstruction. Followed by, the final fusion module dynamically weights high frequency and low-frequency MOE knowledge to adapt to remote sensing images with significant content variations. Quantitative and qualitative experiments over multiple datasets demonstrate that our method performs the best against other state-of-the-art ones and comprises a strong generalization ability for real-world scenes. Code will be made publicly at https://github.com/alexhe101/FAME-Net",
    "checked": true,
    "id": "fb9853ebc5cf769621e0c7e7df1bf45407702290",
    "semantic_title": "frequency-adaptive pan-sharpening with mixture of experts",
    "citation_count": 2,
    "authors": [
      "Xuanhua He",
      "Keyu Yan",
      "Rui Li",
      "Chengjun Xie",
      "Jie Zhang",
      "Man Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27985": {
    "title": "Enhancing RAW-to-sRGB with Decoupled Style Structure in Fourier Domain",
    "volume": "main",
    "abstract": "RAW to sRGB mapping, which aims to convert RAW images from smartphones into RGB form equivalent to that of Digital Single-Lens Reflex (DSLR) cameras, has become an important area of research. However, current methods often ignore the difference between cell phone RAW images and DSLR camera RGB images, a difference that goes beyond the color matrix and extends to spatial structure due to resolution variations. Recent methods directly rebuild color mapping and spatial structure via shared deep representation, limiting optimal performance. Inspired by Image Signal Processing (ISP) pipeline, which distinguishes image restoration and enhancement, we present a novel Neural ISP framework, named FourierISP. This approach breaks the image down into style and structure within the frequency domain, allowing for independent optimization. FourierISP is comprised of three subnetworks: Phase Enhance Subnet for structural refinement, Amplitude Refine Subnet for color learning, and Color Adaptation Subnet for blending them in a smooth manner. This approach sharpens both color and structure, and extensive evaluations across varied datasets confirm that our approach realizes state-of-the-art results. Code will be available at https://github.com/alexhe101/FourierISP",
    "checked": true,
    "id": "837f645c97bc0431820d9e547d6ccba37065f9c2",
    "semantic_title": "enhancing raw-to-srgb with decoupled style structure in fourier domain",
    "citation_count": 1,
    "authors": [
      "Xuanhua He",
      "Tao Hu",
      "Guoli Wang",
      "Zejin Wang",
      "Run Wang",
      "Qian Zhang",
      "Keyu Yan",
      "Ziyi Chen",
      "Rui Li",
      "Chengjun Xie",
      "Jie Zhang",
      "Man Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27986": {
    "title": "A User-Friendly Framework for Generating Model-Preferred Prompts in Text-to-Image Synthesis",
    "volume": "main",
    "abstract": "Well-designed prompts have demonstrated the potential to guide text-to-image models in generating amazing images. Although existing prompt engineering methods can provide high-level guidance, it is challenging for novice users to achieve the desired results by manually entering prompts due to a discrepancy between novice-user-input prompts and the model-preferred prompts. To bridge the distribution gap between user input behavior and model training datasets, we first construct a novel Coarse-Fine Granularity Prompts dataset (CFP) and propose a novel User-Friendly Fine-Grained Text Generation framework (UF-FGTG) for automated prompt optimization. For CFP, we construct a novel dataset for text-to-image tasks that combines coarse and fine-grained prompts to facilitate the development of automated prompt generation methods. For UF-FGTG, we propose a novel framework that automatically translates user-input prompts into model-preferred prompts. Specifically, we propose a prompt refiner that continually rewrites prompts to empower users to select results that align with their unique needs. Meanwhile, we integrate image-related loss functions from the text-to-image model into the training process of text generation to generate model-preferred prompts. Additionally, we propose an adaptive feature extraction module to ensure diversity in the generated results. Experiments demonstrate that our approach is capable of generating more visually appealing and diverse images than previous state-of-the-art methods, achieving an average improvement of 5% across six quality and aesthetic metrics. Data and code are available at https://github.com/Naylenv/UF-FGTG",
    "checked": true,
    "id": "57866c5ba4eb25130faeaea82b39b70cab23ef98",
    "semantic_title": "a user-friendly framework for generating model-preferred prompts in text-to-image synthesis",
    "citation_count": 0,
    "authors": [
      "Nailei Hei",
      "Qianyu Guo",
      "Zihao Wang",
      "Yan Wang",
      "Haofen Wang",
      "Wenqiang Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27987": {
    "title": "Optimize & Reduce: A Top-Down Approach for Image Vectorization",
    "volume": "main",
    "abstract": "Vector image representation is a popular choice when editability and flexibility in resolution are desired. However, most images are only available in raster form, making raster-to-vector image conversion (vectorization) an important task. Classical methods for vectorization are either domain-specific or yield an abundance of shapes which limits editability and interpretability. Learning-based methods, that use differentiable rendering, have revolutionized vectorization, at the cost of poor generalization to out-of-training distribution domains, and optimization-based counterparts are either slow or produce non-editable and redundant shapes. In this work, we propose Optimize & Reduce (O&R), a top-down approach to vectorization that is both fast and domain-agnostic. O&R aims to attain a compact representation of input images by iteratively optimizing Bezier curve parameters and significantly reducing the number of shapes, using a devised importance measure. We contribute a benchmark of five datasets comprising images from a broad spectrum of image complexities - from emojis to natural-like images. Through extensive experiments on hundreds of images, we demonstrate that our method is domain agnostic and outperforms existing works in both reconstruction and perceptual quality for a fixed number of shapes. Moreover, we show that our algorithm is x10 faster than the state-of-the-art optimization-based method. Our code is publicly available: https://github.com/ajevnisek/optimize-and-reduce",
    "checked": true,
    "id": "aa8cf0a97505629249eb68ef482aa51c755ebd52",
    "semantic_title": "optimize & reduce: a top-down approach for image vectorization",
    "citation_count": 1,
    "authors": [
      "Or Hirschorn",
      "Amir Jevnisek",
      "Shai Avidan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27988": {
    "title": "MotionMix: Weakly-Supervised Diffusion for Controllable Motion Generation",
    "volume": "main",
    "abstract": "Controllable generation of 3D human motions becomes an important topic as the world embraces digital transformation. Existing works, though making promising progress with the advent of diffusion models, heavily rely on meticulously captured and annotated (e.g., text) high-quality motion corpus, a resource-intensive endeavor in the real world. This motivates our proposed MotionMix, a simple yet effective weakly-supervised diffusion model that leverages both noisy and unannotated motion sequences. Specifically, we separate the denoising objectives of a diffusion model into two stages: obtaining conditional rough motion approximations in the initial T-T* steps by learning the noisy annotated motions, followed by the unconditional refinement of these preliminary motions during the last T* steps using unannotated motions. Notably, though learning from two sources of imperfect data, our model does not compromise motion generation quality compared to fully supervised approaches that access gold data. Extensive experiments on several benchmarks demonstrate that our MotionMix, as a versatile framework, consistently achieves state-of-the-art performances on text-to-motion, action-to-motion, and music-to-dance tasks",
    "checked": true,
    "id": "6fee3e7fd37d56b83c85b46d079211ab271a3a5d",
    "semantic_title": "motionmix: weakly-supervised diffusion for controllable motion generation",
    "citation_count": 3,
    "authors": [
      "Nhat M. Hoang",
      "Kehong Gong",
      "Chuan Guo",
      "Michael Bi Mi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27989": {
    "title": "Commonsense for Zero-Shot Natural Language Video Localization",
    "volume": "main",
    "abstract": "Zero-shot Natural Language-Video Localization (NLVL) methods have exhibited promising results in training NLVL models exclusively with raw video data by dynamically generating video segments and pseudo-query annotations. However, existing pseudo-queries often lack grounding in the source video, resulting in unstructured and disjointed content. In this paper, we investigate the effectiveness of commonsense reasoning in zero-shot NLVL. Specifically, we present CORONET, a zero-shot NLVL framework that leverages commonsense to bridge the gap between videos and generated pseudo-queries via a commonsense enhancement module. CORONET employs Graph Convolution Networks (GCN) to encode commonsense information extracted from a knowledge graph, conditioned on the video, and cross-attention mechanisms to enhance the encoded video and pseudo-query representations prior to localization. Through empirical evaluations on two benchmark datasets, we demonstrate that CORONET surpasses both zero-shot and weakly supervised baselines, achieving improvements up to 32.13% across various recall thresholds and up to 6.33% in mIoU. These results underscore the significance of leveraging commonsense reasoning for zero-shot NLVL",
    "checked": true,
    "id": "066ad67338f45d77f4cafbe56fd8969575a19690",
    "semantic_title": "commonsense for zero-shot natural language video localization",
    "citation_count": 1,
    "authors": [
      "Meghana Holla",
      "Ismini Lourentzou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27990": {
    "title": "Learning Subject-Aware Cropping by Outpainting Professional Photos",
    "volume": "main",
    "abstract": "How to frame (or crop) a photo often depends on the image subject and its context; e.g., a human portrait. Recent works have defined the subject-aware image cropping task as a nuanced and practical version of image cropping. We propose a weakly-supervised approach (GenCrop) to learn what makes a high-quality, subject-aware crop from professional stock images. Unlike supervised prior work, GenCrop requires no new manual annotations beyond the existing stock image collection. The key challenge in learning from this data, however, is that the images are already cropped and we do not know what regions were removed. Our insight is to combine a library of stock images with a modern, pre-trained text-to-image diffusion model. The stock image collection provides diversity, and its images serve as pseudo-labels for a good crop. The text-image diffusion model is used to out-paint (i.e., outward inpainting) realistic uncropped images. Using this procedure, we are able to automatically generate a large dataset of cropped-uncropped training pairs to train a cropping model. Despite being weakly-supervised, GenCrop is competitive with state-of-the-art supervised methods and significantly better than comparable weakly-supervised baselines on quantitative and qualitative evaluation metrics",
    "checked": true,
    "id": "9b7e1b05537d96e7dd1a03e526ba0abbd93aadee",
    "semantic_title": "learning subject-aware cropping by outpainting professional photos",
    "citation_count": 0,
    "authors": [
      "James Hong",
      "Lu Yuan",
      "Michaël Gharbi",
      "Matthew Fisher",
      "Kayvon  Fatahalian"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27991": {
    "title": "High-Fidelity Diffusion-Based Image Editing",
    "volume": "main",
    "abstract": "Diffusion models have attained remarkable success in the domains of image generation and editing. It is widely recognized that employing larger inversion and denoising steps in diffusion model leads to improved image reconstruction quality. However, the editing performance of diffusion models tends to be no more satisfactory even with increasing denoising steps. The deficiency in editing could be attributed to the conditional Markovian property of the editing process, where errors accumulate throughout denoising steps. To tackle this challenge, we first propose an innovative framework where a rectifier module is incorporated to modulate diffusion model weights with residual features from the original images, thereby providing compensatory information to bridge the fidelity gap. Furthermore, we introduce a novel learning paradigm aimed at minimizing error propagation during the editing process, which trains the editing procedure in a manner similar to denoising score-matching. Extensive experiments demonstrate that our proposed framework and training strategy achieve high-fidelity reconstruction and editing results across various levels of denoising steps, meanwhile exhibits exceptional performance in terms of both quantitative metric and qualitative assessments. Lastly, we explore our model's generalization though several applications like image-to-image translation and out-of-domain image editing",
    "checked": true,
    "id": "9036af5a0174dc8825aac8f7663dab635674a685",
    "semantic_title": "high-fidelity diffusion-based image editing",
    "citation_count": 1,
    "authors": [
      "Chen Hou",
      "Guoqiang  Wei",
      "Zhibo Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27992": {
    "title": "Domain-Hallucinated Updating for Multi-Domain Face Anti-spoofing",
    "volume": "main",
    "abstract": "Multi-Domain Face Anti-Spoofing (MD-FAS) is a practical setting that aims to update models on new domains using only novel data while ensuring that the knowledge acquired from previous domains is not forgotten. Prior methods utilize the responses from models to represent the previous domain knowledge or map the different domains into separated feature spaces to prevent forgetting. However, due to domain gaps, the responses of new data are not as accurate as those of previous data. Also, without the supervision of previous data, separated feature spaces might be destroyed by new domains while updating, leading to catastrophic forgetting. Inspired by the challenges posed by the lack of previous data, we solve this issue from a new standpoint that generates hallucinated previous data for updating FAS model. To this end, we propose a novel Domain-Hallucinated Updating (DHU) framework to facilitate the hallucination of data. Specifically, Domain Information Explorer learns representative domain information of the previous domains. Then, Domain Information Hallucination module transfers the new domain data to pseudo-previous domain ones. Moreover, Hallucinated Features Joint Learning module is proposed to asymmetrically align the new and pseudo-previous data for real samples via dual levels to learn more generalized features, promoting the results on all domains. Our experimental results and visualizations demonstrate that the proposed method outperforms state-of-the-art competitors in terms of effectiveness",
    "checked": true,
    "id": "5d4fa551f46a7fe692f0f94f3940b46bad5b2b76",
    "semantic_title": "domain-hallucinated updating for multi-domain face anti-spoofing",
    "citation_count": 1,
    "authors": [
      "Chengyang Hu",
      "Ke-Yue Zhang",
      "Taiping Yao",
      "Shice Liu",
      "Shouhong Ding",
      "Xin Tan",
      "Lizhuang Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27993": {
    "title": "QI-IRA: Quantum-Inspired Interactive Ranking Aggregation for Person Re-identification",
    "volume": "main",
    "abstract": "Ranking aggregation (RA), the process of aggregating multiple rankings derived from multiple search strategies, has been proved effective in person re-identification (re-ID) because of a single re-ID method can not always achieve consistent superiority for different scenarios. Existing RA research mainly focus on unsupervised and fully-supervised methods. The former lack external supervision to optimize performance, while the latter are costly because of expensive labeling effort required for training. To address the above challenges, this paper proposes a quantum-inspired interactive ranking aggregation (QI-IRA) method, which (1) utilizes quantum theory to interpret and model the generation and aggregation of multiple basic rankings, (2) approximates or even exceeds the performance of fully-supervised RA methods with much less labeling cost, even as low as only two feedbacks per query on Market1501, MARS and DukeMTMC-VideoReID datasets. Comparative experiments conducted on six public re-ID datasets validate the superiority of the proposed QI-IRA method over existing unsupervised, interactive, and fully-supervised RA approaches",
    "checked": true,
    "id": "a470d11119fec8477a83cab5ef993d091c0bc106",
    "semantic_title": "qi-ira: quantum-inspired interactive ranking aggregation for person re-identification",
    "citation_count": 0,
    "authors": [
      "Chunyu Hu",
      "Hong Zhang",
      "Chao Liang",
      "Hao Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27994": {
    "title": "SpaceGTN: A Time-Agnostic Graph Transformer Network for Handwritten Diagram Recognition and Segmentation",
    "volume": "main",
    "abstract": "Online handwriting recognition is pivotal in domains like note-taking, education, healthcare, and office tasks. Existing diagram recognition algorithms mainly rely on the temporal information of strokes, resulting in a decline in recognition performance when dealing with notes that have been modified or have no temporal information. The current datasets are drawn based on templates and cannot reflect the real free-drawing situation. To address these challenges, we present SpaceGTN, a time-agnostic Graph Transformer Network, leveraging spatial integration and removing the need for temporal data. Extensive experiments on multiple datasets have demonstrated that our method consistently outperforms existing methods and achieves state-of-the-art performance. We also propose a pipeline that seamlessly connects offline and online handwritten diagrams. By integrating a stroke restoration technique with SpaceGTN, it enables intelligent editing of previously uneditable offline diagrams at the stroke level. In addition, we have also launched the first online handwritten diagram dataset, OHSD, which is collected using a free-drawing method and comes with modification annotations",
    "checked": true,
    "id": "d4010fabbecd78c1fcb15d33702d52b5c77258ff",
    "semantic_title": "spacegtn: a time-agnostic graph transformer network for handwritten diagram recognition and segmentation",
    "citation_count": 0,
    "authors": [
      "Haoxiang Hu",
      "Cangjun Gao",
      "Yaokun Li",
      "Xiaoming Deng",
      "YuKun Lai",
      "Cuixia Ma",
      "Yong-Jin Liu",
      "Hongan Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27995": {
    "title": "Learning Explicit Contact for Implicit Reconstruction of Hand-Held Objects from Monocular Images",
    "volume": "main",
    "abstract": "Reconstructing hand-held objects from monocular RGB images is an appealing yet challenging task. In this task, contacts between hands and objects provide important cues for recovering the 3D geometry of the hand-held objects. Though recent works have employed implicit functions to achieve impressive progress, they ignore formulating contacts in their frameworks, which results in producing less realistic object meshes. In this work, we explore how to model contacts in an explicit way to benefit the implicit reconstruction of hand-held objects. Our method consists of two components: explicit contact prediction and implicit shape reconstruction. In the first part, we propose a new subtask of directly estimating 3D hand-object contacts from a single image. The part-level and vertex-level graph-based transformers are cascaded and jointly learned in a coarse-to-fine manner for more accurate contact probabilities. In the second part, we introduce a novel method to diffuse estimated contact states from the hand mesh surface to nearby 3D space and leverage diffused contact probabilities to construct the implicit neural representation for the manipulated object. Benefiting from estimating the interaction patterns between the hand and the object, our method can reconstruct more realistic object meshes, especially for object parts that are in contact with hands. Extensive experiments on challenging benchmarks show that the proposed method outperforms the current state of the arts by a great margin. Our code is publicly available at https://junxinghu.github.io/projects/hoi.html",
    "checked": true,
    "id": "b8767383f31ad1ffaff8f0154e923d8de02de0af",
    "semantic_title": "learning explicit contact for implicit reconstruction of hand-held objects from monocular images",
    "citation_count": 1,
    "authors": [
      "Junxing Hu",
      "Hongwen Zhang",
      "Zerui Chen",
      "Mengcheng Li",
      "Yunlong Wang",
      "Yebin Liu",
      "Zhenan Sun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27996": {
    "title": "DALDet: Depth-Aware Learning Based Object Detection for Autonomous Driving",
    "volume": "main",
    "abstract": "3D object detection achieves good detection performance in autonomous driving. However, it requires substantial computational resources, which prevents its practical application. 2D object detection has less computational burden but lacks spatial and geometric information embedded in depth. Therefore, we present DALDet, an efficient depth-aware learning based 2D detector, achieving high-performance object detection for autonomous driving. We design an efficient one-stage detection framework and seamlessly integrate depth cues into convolutional neural network by introducing depth-aware convolution and depth-aware average pooling, which effectively improve the detector's ability to perceive 3D space. Moreover, we propose a depth-guided loss function for training DALDet, which effectively improves the localization ability of the detector. Due to the use of depth map, DALDet can also output the distance of the object, which is of great importance for driving applications such as obstacle avoidance. Extensive experiments demonstrate the superiority and efficiency of DALDet. In particular, our DALDet ranks 1st on both KITTI Car and Cyclist 2D detection test leaderboards among all 2D detectors with high efficiency as well as yielding competitive performance among many leading 3D detectors. Code will be available at https://github.com/hukefy/DALDet",
    "checked": true,
    "id": "4a134742984a958526b193d23a30a86f66004b48",
    "semantic_title": "daldet: depth-aware learning based object detection for autonomous driving",
    "citation_count": 0,
    "authors": [
      "Ke Hu",
      "Tongbo Cao",
      "Yuan Li",
      "Song Chen",
      "Yi Kang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27997": {
    "title": "COMMA: Co-articulated Multi-Modal Learning",
    "volume": "main",
    "abstract": "Pretrained large-scale vision-language models such as CLIP have demonstrated excellent generalizability over a series of downstream tasks. However, they are sensitive to the variation of input text prompts and need a selection of prompt templates to achieve satisfactory performance. Recently, various methods have been proposed to dynamically learn the prompts as the textual inputs to avoid the requirements of laboring hand-crafted prompt engineering in the fine-tuning process. We notice that these methods are suboptimal in two aspects. First, the prompts of the vision and language branches in these methods are usually separated or uni-directionally correlated. Thus, the prompts of both branches are not fully correlated and may not provide enough guidance to align the representations of both branches. Second, it's observed that most previous methods usually achieve better performance on seen classes but cause performance degeneration on unseen classes compared to CLIP. This is because the essential generic knowledge learned in the pretraining stage is partly forgotten in the fine-tuning process. In this paper, we propose Co-Articulated Multi-Modal Learning (COMMA) to handle the above limitations. Especially, our method considers prompts from both branches to generate the prompts to enhance the representation alignment of both branches. Besides, to alleviate forgetting about the essential knowledge, we minimize the feature discrepancy between the learned prompts and the embeddings of hand-crafted prompts in the pre-trained CLIP in the late transformer layers. We evaluate our method across three representative tasks of generalization to novel classes, new target datasets and unseen domain shifts. Experimental results demonstrate the superiority of our method by exhibiting a favorable performance boost upon all tasks with high efficiency. Code is available at https://github.com/hulianyuyy/COMMA",
    "checked": true,
    "id": "b9eb60c1a1cd28def5c1a67ca56982a24c1f26ef",
    "semantic_title": "comma: co-articulated multi-modal learning",
    "citation_count": 0,
    "authors": [
      "Lianyu Hu",
      "Liqing Gao",
      "Zekang Liu",
      "Chi-Man Pun",
      "Wei Feng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27998": {
    "title": "Latent Space Editing in Transformer-Based Flow Matching",
    "volume": "main",
    "abstract": "This paper strives for image editing via generative models. Flow Matching is an emerging generative modeling technique that offers the advantage of simple and efficient training. Simultaneously, a new transformer-based U-ViT has recently been proposed to replace the commonly used UNet for better scalability and performance in generative modeling. Hence, Flow Matching with a transformer backbone offers the potential for scalable and high-quality generative modeling, but their latent structure and editing ability are as of yet unknown. Hence, we adopt this setting and explore how to edit images through latent space manipulation. We introduce an editing space, which we call u-space, that can be manipulated in a controllable, accumulative, and composable manner. Additionally, we propose a tailored sampling solution to enable sampling with the more efficient adaptive step-size ODE solvers. Lastly, we put forth a straightforward yet powerful method for achieving fine-grained and nuanced editing using text prompts. Our framework is simple and efficient, all while being highly effective at editing images while preserving the essence of the original content. Our code will be publicly available at https://taohu.me/lfm/",
    "checked": true,
    "id": "ca743e75ce090bbf686307e41bd8747661768fbe",
    "semantic_title": "latent space editing in transformer-based flow matching",
    "citation_count": 12,
    "authors": [
      "Vincent Tao Hu",
      "Wei Zhang",
      "Meng Tang",
      "Pascal Mettes",
      "Deli Zhao",
      "Cees  Snoek"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/27999": {
    "title": "BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions",
    "volume": "main",
    "abstract": "Vision Language Models (VLMs), which extend Large Language Models (LLM) by incorporating visual understanding capability, have demonstrated significant advancements in addressing open-ended visual question-answering (VQA) tasks. However, these models cannot accurately interpret images infused with text, a common occurrence in real-world scenarios. Standard procedures for extracting information from images often involve learning a fixed set of query embeddings. These embeddings are designed to encapsulate image contexts and are later used as soft prompt inputs in LLMs. Yet, this process is limited to the token count, potentially curtailing the recognition of scenes with text-rich context. To improve upon them, the present study introduces BLIVA: an augmented version of InstructBLIP with Visual Assistant. BLIVA incorporates the query embeddings from InstructBLIP and also directly projects encoded patch embeddings into the LLM, a technique inspired by LLaVA. This approach assists the model to capture intricate details potentially missed during the query decoding process. Empirical evidence demonstrates that our model, BLIVA, significantly enhances performance in processing text-rich VQA benchmarks (up to 17.76% in OCR-VQA benchmark) and in undertaking general (not particularly text-rich) VQA benchmarks (up to 7.9% in Visual Spatial Reasoning benchmark), and achieved 17.72% overall improvement in a comprehensive multimodal LLM benchmark (MME), comparing to our baseline InstructBLIP. BLIVA demonstrates significant capability in decoding real-world images, irrespective of text presence. To demonstrate the broad industry applications enabled by BLIVA, we evaluate the model using a new dataset comprising YouTube thumbnails paired with question-answer sets across 11 diverse categories. For researchers interested in further exploration, our code and models are freely accessible at https://github.com/mlpc-ucsd/BLIVA",
    "checked": true,
    "id": "30cc95639cffca4ffa8c0eafbc502636c0c88fa5",
    "semantic_title": "bliva: a simple multimodal llm for better handling of text-rich visual questions",
    "citation_count": 55,
    "authors": [
      "Wenbo Hu",
      "Yifan Xu",
      "Yi Li",
      "Weiyue Li",
      "Zeyuan Chen",
      "Zhuowen Tu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28000": {
    "title": "A Dynamic Learning Method towards Realistic Compositional Zero-Shot Learning",
    "volume": "main",
    "abstract": "To tackle the challenge of recognizing images of unseen attribute-object compositions, Compositional Zero-Shot Learning (CZSL) methods have been previously addressed. However, test images in realistic scenarios may also incorporate other forms of unknown factors, such as novel semantic concepts or novel image styles. As previous CZSL works have overlooked this critical issue, in this research, we first propose the Realistic Compositional Zero-Shot Learning (RCZSL) task which considers the various types of unknown factors in an unified experimental setting. To achieve this, we firstly conduct re-labelling on MIT-States and use the pre-trained generative models to obtain images of various domains. Then the entire dataset is split into a training set and a test set, with the latter containing images of unseen concepts, unseen compositions, unseen domains as well as their combinations. Following this, we show that the visual-semantic relationship changes on unseen images, leading us to construct two dynamic modulators to adapt the visual features and composition prototypes in accordance with the input image. We believe that such a dynamic learning method could effectively alleviate the domain shift problem caused by various types of unknown factors. We conduct extensive experiments on benchmark datasets for both the conventional CZSL setting and the proposed RCZSL setting. The effectiveness of our method has been proven by empirical results, which significantly outperformed both our baseline method and state-of-the-art approaches",
    "checked": true,
    "id": "3996ffd3a289ed1fd42990bbf3b696f2493f5819",
    "semantic_title": "a dynamic learning method towards realistic compositional zero-shot learning",
    "citation_count": 0,
    "authors": [
      "Xiaoming Hu",
      "Zilei Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28001": {
    "title": "LF-ViT: Reducing Spatial Redundancy in Vision Transformer for Efficient Image Recognition",
    "volume": "main",
    "abstract": "The Vision Transformer (ViT) excels in accuracy when handling high-resolution images, yet it confronts the challenge of significant spatial redundancy, leading to increased computational and memory requirements. To address this, we present the Localization and Focus Vision Transformer (LF-ViT). This model operates by strategically curtailing computational demands without impinging on performance. In the Localization phase, a reduced-resolution image is processed; if a definitive prediction remains elusive, our pioneering Neighborhood Global Class Attention (NGCA) mechanism is triggered, effectively identifying and spotlighting class-discriminative regions based on initial findings. Subsequently, in the Focus phase, this designated region is used from the original image to enhance recognition. Uniquely, LF-ViT employs consistent parameters across both phases, ensuring seamless end-to-end optimization. Our empirical tests affirm LF-ViT's prowess: it remarkably decreases Deit-S's FLOPs by 63% and concurrently amplifies throughput twofold. Code of this project is at https://github.com/edgeai1/LF-ViT.git",
    "checked": true,
    "id": "b1f0ac0b5e4044251dcdcce78e0046feb43cc7c7",
    "semantic_title": "lf-vit: reducing spatial redundancy in vision transformer for efficient image recognition",
    "citation_count": 0,
    "authors": [
      "Youbing Hu",
      "Yun Cheng",
      "Anqi Lu",
      "Zhiqiang Cao",
      "Dawei Wei",
      "Jie Liu",
      "Zhijun Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28002": {
    "title": "O^2-Recon: Completing 3D Reconstruction of Occluded Objects in the Scene with a Pre-trained 2D Diffusion Model",
    "volume": "main",
    "abstract": "Occlusion is a common issue in 3D reconstruction from RGB-D videos, often blocking the complete reconstruction of objects and presenting an ongoing problem. In this paper, we propose a novel framework, empowered by a 2D diffusion-based in-painting model, to reconstruct complete surfaces for the hidden parts of objects. Specifically, we utilize a pre-trained diffusion model to fill in the hidden areas of 2D images. Then we use these in-painted images to optimize a neural implicit surface representation for each instance for 3D reconstruction. Since creating the in-painting masks needed for this process is tricky, we adopt a human-in-the-loop strategy that involves very little human engagement to generate high-quality masks. Moreover, some parts of objects can be totally hidden because the videos are usually shot from limited perspectives. To ensure recovering these invisible areas, we develop a cascaded network architecture for predicting signed distance field, making use of different frequency bands of positional encoding and maintaining overall smoothness. Besides the commonly used rendering loss, Eikonal loss, and silhouette loss, we adopt a CLIP-based semantic consistency loss to guide the surface from unseen camera angles. Experiments on ScanNet scenes show that our proposed framework achieves state-of-the-art accuracy and completeness in object-level reconstruction from scene-level RGB-D videos. Code: https://github.com/THU-LYJ-Lab/O2-Recon",
    "checked": true,
    "id": "2f39b7f099283a09cc348a1d5fe7b581bad81e7f",
    "semantic_title": "o^2-recon: completing 3d reconstruction of occluded objects in the scene with a pre-trained 2d diffusion model",
    "citation_count": 0,
    "authors": [
      "Yubin Hu",
      "Sheng Ye",
      "Wang Zhao",
      "Matthieu Lin",
      "Yuze He",
      "Yu-Hui Wen",
      "Ying He",
      "Yong-Jin Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28003": {
    "title": "Arbitrary-Scale Video Super-resolution Guided by Dynamic Context",
    "volume": "main",
    "abstract": "We propose a Dynamic Context-Guided Upsampling (DCGU) module for video super-resolution (VSR) that leverages temporal context guidance to achieve efficient and effective arbitrary-scale VSR. While most VSR research focuses on backbone design, the importance of the upsampling part is often overlooked. Existing methods rely on pixelshuffle-based upsampling, which has limited capabilities in handling arbitrary upsampling scales. Recent attempts to replace pixelshuffle-based modules with implicit neural function-based and filter-based approaches suffer from slow inference speeds and limited representation capacity, respectively. To overcome these limitations, our DCGU module predicts non-local sampling locations and content-dependent filter weights, enabling efficient and effective arbitrary-scale VSR. Our proposed multi-granularity location search module efficiently identifies non-local sampling locations across the entire low-resolution grid, and the temporal bilateral filter modulation module integrates content information with the filter weight to enhance textual details. Extensive experiments demonstrate the superiority of our method in terms of performance and speed on arbitrary-scale VSR",
    "checked": true,
    "id": "0ffa19408dc66bb924f802a4f37b01398773bb76",
    "semantic_title": "arbitrary-scale video super-resolution guided by dynamic context",
    "citation_count": 0,
    "authors": [
      "Cong Huang",
      "Jiahao Li",
      "Lei Chu",
      "Dong Liu",
      "Yan Lu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28004": {
    "title": "Dynamic Weighted Combiner for Mixed-Modal Image Retrieval",
    "volume": "main",
    "abstract": "Mixed-Modal Image Retrieval (MMIR) as a flexible search paradigm has attracted wide attention. However, previous approaches always achieve limited performance, due to two critical factors are seriously overlooked. 1) The contribution of image and text modalities is different, but incorrectly treated equally. 2) There exist inherent labeling noises in describing users' intentions with text in web datasets from diverse real-world scenarios, giving rise to overfitting. We propose a Dynamic Weighted Combiner (DWC) to tackle the above challenges, which includes three merits. First, we propose an Editable Modality De-equalizer (EMD) by taking into account the contribution disparity between modalities, containing two modality feature editors and an adaptive weighted combiner. Second, to alleviate labeling noises and data bias, we propose a dynamic soft-similarity label generator (SSG) to implicitly improve noisy supervision. Finally, to bridge modality gaps and facilitate similarity learning, we propose a CLIP-based mutual enhancement module alternately trained by a mixed-modality contrastive loss. Extensive experiments verify that our proposed model significantly outperforms state-of-the-art methods on real-world datasets. The source code is available at https://github.com/fuxianghuang1/DWC",
    "checked": true,
    "id": "8c1660c2c914f19fc7b3038469434b64bf3b1982",
    "semantic_title": "dynamic weighted combiner for mixed-modal image retrieval",
    "citation_count": 1,
    "authors": [
      "Fuxiang Huang",
      "Lei Zhang",
      "Xiaowei Fu",
      "Suqi Song"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28005": {
    "title": "NeuSurf: On-Surface Priors for Neural Surface Reconstruction from Sparse Input Views",
    "volume": "main",
    "abstract": "Recently, neural implicit functions have demonstrated remarkable results in the field of multi-view reconstruction. However, most existing methods are tailored for dense views and exhibit unsatisfactory performance when dealing with sparse views. Several latest methods have been proposed for generalizing implicit reconstruction to address the sparse view reconstruction task, but they still suffer from high training costs and are merely valid under carefully selected perspectives. In this paper, we propose a novel sparse view reconstruction framework that leverages on-surface priors to achieve highly faithful surface reconstruction. Specifically, we design several constraints on global geometry alignment and local geometry refinement for jointly optimizing coarse shapes and fine details. To achieve this, we train a neural network to learn a global implicit field from the on-surface points obtained from SfM and then leverage it as a coarse geometric constraint. To exploit local geometric consistency, we project on-surface points onto seen and unseen views, treating the consistent loss of projected features as a fine geometric constraint. The experimental results with DTU and BlendedMVS datasets in two prevalent sparse settings demonstrate significant improvements over the state-of-the-art methods",
    "checked": true,
    "id": "8e5e0fdd376295306223b2c6267975d3d881143f",
    "semantic_title": "neusurf: on-surface priors for neural surface reconstruction from sparse input views",
    "citation_count": 3,
    "authors": [
      "Han Huang",
      "Yulun Wu",
      "Junsheng Zhou",
      "Ge Gao",
      "Ming Gu",
      "Yu-Shen Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28006": {
    "title": "Seeing Dark Videos via Self-Learned Bottleneck Neural Representation",
    "volume": "main",
    "abstract": "Enhancing low-light videos in a supervised style presents a set of challenges, including limited data diversity, misalignment, and the domain gap introduced through the dataset construction pipeline. Our paper tackles these challenges by constructing a self-learned enhancement approach that gets rid of the reliance on any external training data. The challenge of self-supervised learning lies in fitting high-quality signal representations solely from input signals. Our work designs a bottleneck neural representation mechanism that extracts those signals. More in detail, we encode the frame-wise representation with a compact deep embedding and utilize a neural network to parameterize the video-level manifold consistently. Then, an entropy constraint is applied to the enhanced results based on the adjacent spatial-temporal context to filter out the degraded visual signals, e.g. noise and frame inconsistency. Last, a novel Chromatic Retinex decomposition is proposed to effectively align the reflectance distribution temporally. It benefits the entropy control on different components of each frame and facilitates noise-to-noise training, successfully suppressing the temporal flicker. Extensive experiments demonstrate the robustness and superior effectiveness of our proposed method. Our project is publicly available at: https://huangerbai.github.io/SLBNR/",
    "checked": true,
    "id": "05b780ed15aafbfef5d26125da4867b7ad2cba58",
    "semantic_title": "seeing dark videos via self-learned bottleneck neural representation",
    "citation_count": 0,
    "authors": [
      "Haofeng Huang",
      "Wenhan Yang",
      "Lingyu Duan",
      "Jiaying Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28007": {
    "title": "Combinatorial CNN-Transformer Learning with Manifold Constraints for Semi-supervised Medical Image Segmentation",
    "volume": "main",
    "abstract": "Semi-supervised learning (SSL), as one of the dominant methods, aims at leveraging the unlabeled data to deal with the annotation dilemma of supervised learning, which has attracted much attentions in the medical image segmentation. Most of the existing approaches leverage a unitary network by convolutional neural networks (CNNs) with compulsory consistency of the predictions through small perturbations applied to inputs or models. The penalties of such a learning paradigm are that (1) CNN-based models place severe limitations on global learning; (2) rich and diverse class-level distributions are inhibited. In this paper, we present a novel CNN-Transformer learning framework in the manifold space for semi-supervised medical image segmentation. First, at intra-student level, we propose a novel class-wise consistency loss to facilitate the learning of both discriminative and compact target feature representations. Then, at inter-student level, we align the CNN and Transformer features using a prototype-based optimal transport method. Extensive experiments show that our method outperforms previous state-of-the-art methods on three public medical image segmentation benchmarks",
    "checked": true,
    "id": "f86b36c5b9a2338c6fcef47c74b0b186a35eb61b",
    "semantic_title": "combinatorial cnn-transformer learning with manifold constraints for semi-supervised medical image segmentation",
    "citation_count": 0,
    "authors": [
      "Huimin Huang",
      "Yawen Huang",
      "Shiao Xie",
      "Lanfen Lin",
      "Ruofeng Tong",
      "Yen-Wei Chen",
      "Yuexiang Li",
      "Yefeng Zheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28008": {
    "title": "Sparse Bayesian Deep Learning for Cross Domain Medical Image Reconstruction",
    "volume": "main",
    "abstract": "Cross domain medical image reconstruction aims to address the issue that deep learning models trained solely on one source dataset might not generalize effectively to unseen target datasets from different hospitals. Some recent methods achieve satisfactory reconstruction performance, but often at the expense of extensive parameters and time consumption. To strike a balance between cross domain image reconstruction quality and model computational efficiency, we propose a lightweight sparse Bayesian deep learning method. Notably, we apply a fixed-form variational Bayes (FFVB) approach to quantify pixel-wise uncertainty priors derived from degradation distribution of the source domain. Furthermore, by integrating the uncertainty prior into the posterior sampled through stochastic gradient Langevin dynamics (SGLD), we develop a training strategy that dynamically generates and optimizes the prior distribution on the network weights for each unseen domain. This strategy enhances generalizability and ensures robust reconstruction performance. When evaluated on medical image reconstruction tasks, our proposed approach demonstrates impressive performance across various previously unseen domains",
    "checked": true,
    "id": "b82cc411c64afff5f223826c336ebad5ff16ef32",
    "semantic_title": "sparse bayesian deep learning for cross domain medical image reconstruction",
    "citation_count": 1,
    "authors": [
      "Jiaxin Huang",
      "Qi Wu",
      "Yazhou Ren",
      "Fan Yang",
      "Aodi Yang",
      "Qianqian Yang",
      "Xiaorong Pu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28009": {
    "title": "UniCell: Universal Cell Nucleus Classification via Prompt Learning",
    "volume": "main",
    "abstract": "The recognition of multi-class cell nuclei can significantly facilitate the process of histopathological diagnosis. Numerous pathological datasets are currently available, but their annotations are inconsistent. Most existing methods require individual training on each dataset to deduce the relevant labels and lack the use of common knowledge across datasets, consequently restricting the quality of recognition. In this paper, we propose a universal cell nucleus classification framework (UniCell), which employs a novel prompt learning mechanism to uniformly predict the corresponding categories of pathological images from different dataset domains. In particular, our framework adopts an end-to-end architecture for nuclei detection and classification, and utilizes flexible prediction heads for adapting various datasets. Moreover, we develop a Dynamic Prompt Module (DPM) that exploits the properties of multiple datasets to enhance features. The DPM first integrates the embeddings of datasets and semantic categories, and then employs the integrated prompts to refine image representations, efficiently harvesting the shared knowledge among the related cell types and data sources. Experimental results demonstrate that the proposed method effectively achieves the state-of-the-art results on four nucleus detection and classification benchmarks. Code and models are available at https://github.com/lhaof/UniCell",
    "checked": true,
    "id": "02e7dac3c02e84229689fbe3f63e4dd4cf4df384",
    "semantic_title": "unicell: universal cell nucleus classification via prompt learning",
    "citation_count": 0,
    "authors": [
      "Junjia Huang",
      "Haofeng Li",
      "Xiang Wan",
      "Guanbin Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28010": {
    "title": "SC-NeuS: Consistent Neural Surface Reconstruction from Sparse and Noisy Views",
    "volume": "main",
    "abstract": "The recent neural surface reconstruction approaches using volume rendering have made much progress by achieving impressive surface reconstruction quality, but are still limited to dense and highly accurate posed views. To overcome such drawbacks, this paper pays special attention on the consistent surface reconstruction from sparse views with noisy camera poses. Unlike previous approaches, the key difference of this paper is to exploit the multi-view constraints directly from the explicit geometry of the neural surface, which can be used as effective regularization to jointly learn the neural surface and refine the camera poses. To build effective multi-view constraints, we introduce a fast differentiable on-surface intersection to generate on-surface points, and propose view-consistent losses on such differentiable points to regularize the neural surface learning. Based on this point, we propose a joint learning strategy, named SC-NeuS, to perform geometry-consistent surface reconstruction in an end-to-end manner. With extensive evaluation on public datasets, our SC-NeuS can achieve consistently better surface reconstruction results with fine-grained details than previous approaches, especially from sparse and noisy camera views. The source code is available at https://github.com/zouzx/sc-neus.git",
    "checked": true,
    "id": "43b0c25e8a9970ac8b90ddf6edcf1391c7df1073",
    "semantic_title": "sc-neus: consistent neural surface reconstruction from sparse and noisy views",
    "citation_count": 3,
    "authors": [
      "Shi-Sheng Huang",
      "Zixin Zou",
      "Yichi Zhang",
      "Yan-Pei Cao",
      "Ying Shan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28011": {
    "title": "MFTN: Multi-Level Feature Transfer Network Based on MRI-Transformer for MR Image Super-resolution",
    "volume": "main",
    "abstract": "Due to the unique environment and inherent properties of magnetic resonance imaging (MRI) instruments, MR images typically have lower resolution. Therefore, improving the resolution of MR images is beneficial for assisting doctors in diagnosing the condition. Currently, the existing MR image super-resolution (SR) methods still have the problem of insufficient detail reconstruction. To overcome this issue, this paper proposes a multi-level feature transfer network (MFTN) based on MRI-Transformer to realize SR of low-resolution MRI data. MFTN consists of a multi-scale feature reconstruction network (MFRN) and a multi-level feature extraction branch (MFEB). MFRN is constructed as a pyramid structure to gradually reconstruct image features at different scales by integrating the features obtained from MFEB, and MFEB is constructed to provide detail information at different scales for low resolution MR image SR reconstruction by constructing multiple MRI-Transformer modules. Each MRI-Transformer module is designed to learn the transfer features from the reference image by establishing feature correlations between the reference image and low-resolution MR image. In addition, a contrast learning constraint item is added to the loss function to enhance the texture details of the SR image. A large number of experiments show that our network can effectively reconstruct high-quality MR Images and achieves better performance compared to some state-of-the-art methods. The source code of this work will be released on GitHub",
    "checked": true,
    "id": "b7972f13bd51dc482060cb47f21058d14bef2201",
    "semantic_title": "mftn: multi-level feature transfer network based on mri-transformer for mr image super-resolution",
    "citation_count": 0,
    "authors": [
      "Shuying Huang",
      "Ge Chen",
      "Yong Yang",
      "Xiaozheng Wang",
      "Chenbin Liang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28012": {
    "title": "SDGAN: Disentangling Semantic Manipulation for Facial Attribute Editing",
    "volume": "main",
    "abstract": "Facial attribute editing has garnered significant attention, yet prevailing methods struggle with achieving precise attribute manipulation while preserving irrelevant details and controlling attribute styles. This challenge primarily arises from the strong correlations between different attributes and the interplay between attributes and identity. In this paper, we propose Semantic Disentangled GAN (SDGAN), a novel method addressing this challenge. SDGAN introduces two key concepts: a semantic disentanglement generator that assigns facial representations to distinct attribute-specific editing modules, enabling the decoupling of the facial attribute editing process, and a semantic mask alignment strategy that confines attribute editing to appropriate regions, thereby avoiding undesired modifications. Leveraging these concepts, SDGAN demonstrates accurate attribute editing and achieves high-quality attribute style manipulation through both latent-guided and reference-guided manners. We extensively evaluate our method on the CelebA-HQ database, providing both qualitative and quantitative analyses. Our results establish that SDGAN significantly outperforms state-of-the-art techniques, showcasing the effectiveness of our approach. To foster reproducibility and further research, we will provide the code for our method",
    "checked": true,
    "id": "8cd737b653c99523b6a524ef2c1bcf4de6b3b085",
    "semantic_title": "sdgan: disentangling semantic manipulation for facial attribute editing",
    "citation_count": 1,
    "authors": [
      "Wenmin Huang",
      "Weiqi Luo",
      "Jiwu Huang",
      "Xiaochun Cao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28013": {
    "title": "Frozen CLIP Transformer Is an Efficient Point Cloud Encoder",
    "volume": "main",
    "abstract": "The pretrain-finetune paradigm has achieved great success in NLP and 2D image fields because of the high-quality representation ability and transferability of their pretrained models. However, pretraining such a strong model is difficult in the 3D point cloud field due to the limited amount of point cloud sequences. This paper introduces Efficient Point Cloud Learning (EPCL), an effective and efficient point cloud learner for directly training high-quality point cloud models with a frozen CLIP transformer. Our EPCL connects the 2D and 3D modalities by semantically aligning the image features and point cloud features without paired 2D-3D data. Specifically, the input point cloud is divided into a series of local patches, which are converted to token embeddings by the designed point cloud tokenizer. These token embeddings are concatenated with a task token and fed into the frozen CLIP transformer to learn point cloud representation. The intuition is that the proposed point cloud tokenizer projects the input point cloud into a unified token space that is similar to the 2D images. Comprehensive experiments on 3D detection, semantic segmentation, classification and few-shot learning demonstrate that the CLIP transformer can serve as an efficient point cloud encoder and our method achieves promising performance on both indoor and outdoor benchmarks. In particular, performance gains brought by our EPCL are 19.7 AP50 on ScanNet V2 detection, 4.4 mIoU on S3DIS segmentation and 1.2 mIoU on SemanticKITTI segmentation compared to contemporary pretrained models. Code is available at \\url{https://github.com/XiaoshuiHuang/EPCL}",
    "checked": true,
    "id": "fdb1cbb3ea42a47c8b6eecc94817f5276a4d0ea8",
    "semantic_title": "frozen clip transformer is an efficient point cloud encoder",
    "citation_count": 6,
    "authors": [
      "Xiaoshui Huang",
      "Zhou Huang",
      "Sheng Li",
      "Wentao Qu",
      "Tong He",
      "Yuenan Hou",
      "Yifan Zuo",
      "Wanli Ouyang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28014": {
    "title": "G2L-CariGAN: Caricature Generation from Global Structure to Local Features",
    "volume": "main",
    "abstract": "Existing GAN-based approaches to caricature generation mainly focus on exaggerating a character's global facial structure. This often leads to the failure in highlighting significant facial features such as big eyes and hook nose. To address this limitation, we propose a new approach termed as G2L-CariGAN, which uses feature maps of spatial dimensions instead of latent codes for geometric exaggeration. G2L-CariGAN first exaggerates the global facial structure of the character on a low-dimensional feature map and then exaggerates its local facial features on a high-dimensional feature map. Moreover, we develop a caricature identity loss function based on feature maps, which well retains the character's identity after exaggeration. Our experiments have demonstrated that G2L-CariGAN outperforms the state-of-arts in terms of the quality of exaggerating a character and retaining its identity",
    "checked": true,
    "id": "cd2c4f46aa02c7e2fbaa05931329b0bc24cc7f26",
    "semantic_title": "g2l-carigan: caricature generation from global structure to local features",
    "citation_count": 0,
    "authors": [
      "Xin Huang",
      "Yunfeng Bai",
      "Dong Liang",
      "Feng Tian",
      "Jinyuan Jia"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28015": {
    "title": "3D Visibility-Aware Generalizable Neural Radiance Fields for Interacting Hands",
    "volume": "main",
    "abstract": "Neural radiance fields (NeRFs) are promising 3D representations for scenes, objects, and humans. However, most existing methods require multi-view inputs and per-scene training, which limits their real-life applications. Moreover, current methods focus on single-subject cases, leaving scenes of interacting hands that involve severe inter-hand occlusions and challenging view variations remain unsolved. To tackle these issues, this paper proposes a generalizable visibility-aware NeRF (VA-NeRF) framework for interacting hands. Specifically, given an image of interacting hands as input, our VA-NeRF first obtains a mesh-based representation of hands and extracts their corresponding geometric and textural features. Subsequently, a feature fusion module that exploits the visibility of query points and mesh vertices is introduced to adaptively merge features of both hands, enabling the recovery of features in unseen areas. Additionally, our VA-NeRF is optimized together with a novel discriminator within an adversarial learning paradigm. In contrast to conventional discriminators that predict a single real/fake label for the synthesized image, the proposed discriminator generates a pixel-wise visibility map, providing fine-grained supervision for unseen areas and encouraging the VA-NeRF to improve the visual quality of synthesized images. Experiments on the Interhand2.6M dataset demonstrate that our proposed VA-NeRF outperforms conventional NeRFs significantly. Project Page: https://github.com/XuanHuang0/VANeRF",
    "checked": true,
    "id": "d95a5ad6c6ef4fe08f61430f268ab21cfaf79c04",
    "semantic_title": "3d visibility-aware generalizable neural radiance fields for interacting hands",
    "citation_count": 0,
    "authors": [
      "Xuan Huang",
      "Hanhui Li",
      "Zejun Yang",
      "Zhisheng Wang",
      "Xiaodan Liang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28016": {
    "title": "Sunshine to Rainstorm: Cross-Weather Knowledge Distillation for Robust 3D Object Detection",
    "volume": "main",
    "abstract": "LiDAR-based 3D object detection models inevitably struggle under rainy conditions due to the degraded and noisy scanning signals. Previous research has attempted to address this by simulating the noise from rain to improve the robustness of detection models. However, significant disparities exist between simulated and actual rain-impacted data points. In this work, we propose a novel rain simulation method, termed DRET, that unifies Dynamics and Rainy Environment Theory to provide a cost-effective means of expanding the available realistic rain data for 3D detection training. Furthermore, we present a Sunny-to-Rainy Knowledge Distillation (SRKD) approach to enhance 3D detection under rainy conditions. Extensive experiments on the Waymo-Open-Dataset show that, when combined with the state-of-the-art DSVT model and other classical 3D detectors, our proposed framework demonstrates significant detection accuracy improvements, without losing efficiency. Remarkably, our framework also improves detection capabilities under sunny conditions, therefore offering a robust solution for 3D detection regardless of whether the weather is rainy or sunny",
    "checked": true,
    "id": "d4c33a2a141fe770b48471fa6b7ebfa58fa52523",
    "semantic_title": "sunshine to rainstorm: cross-weather knowledge distillation for robust 3d object detection",
    "citation_count": 1,
    "authors": [
      "Xun Huang",
      "Hai Wu",
      "Xin Li",
      "Xiaoliang Fan",
      "Chenglu Wen",
      "Cheng Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28017": {
    "title": "Structure-CLIP: Towards Scene Graph Knowledge to Enhance Multi-Modal Structured Representations",
    "volume": "main",
    "abstract": "Large-scale vision-language pre-training has achieved significant performance in multi-modal understanding and generation tasks. However, existing methods often perform poorly on image-text matching tasks that require structured representations, i.e., representations of objects, attributes, and relations. The models cannot make a distinction between \"An astronaut rides a horse\" and \"A horse rides an astronaut\". This is because they fail to fully leverage structured knowledge when learning multi-modal representations. In this paper, we present an end-to-end framework Structure-CLIP, which integrates Scene Graph Knowledge (SGK) to enhance multi-modal structured representations. Firstly, we use scene graphs to guide the construction of semantic negative examples, which results in an increased emphasis on learning structured representations. Moreover, a Knowledge-Enhance Encoder (KEE) is proposed to leverage SGK as input to further enhance structured representations. To verify the effectiveness of the proposed framework, we pre-train our model with the aforementioned approaches and conduct experiments on downstream tasks. Experimental results demonstrate that Structure-CLIP achieves state-of-the-art (SOTA) performance on VG-Attribution and VG-Relation datasets, with 12.5% and 4.1% ahead of the multi-modal SOTA model respectively. Meanwhile, the results on MSCOCO indicate that Structure-CLIP significantly enhances the structured representations while maintaining the ability of general representations. Our code is available at https://github.com/zjukg/Structure-CLIP",
    "checked": true,
    "id": "0b5b753aa23be12f24b4592429514df6e53289bc",
    "semantic_title": "structure-clip: towards scene graph knowledge to enhance multi-modal structured representations",
    "citation_count": 4,
    "authors": [
      "Yufeng Huang",
      "Jiji Tang",
      "Zhuo Chen",
      "Rongsheng Zhang",
      "Xinfeng Zhang",
      "Weijie Chen",
      "Zeng Zhao",
      "Zhou Zhao",
      "Tangjie Lv",
      "Zhipeng Hu",
      "Wen Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28018": {
    "title": "Voxel or Pillar: Exploring Efficient Point Cloud Representation for 3D Object Detection",
    "volume": "main",
    "abstract": "Efficient representation of point clouds is fundamental for LiDAR-based 3D object detection. While recent grid-based detectors often encode point clouds into either voxels or pillars, the distinctions between these approaches remain underexplored. In this paper, we quantify the differences between the current encoding paradigms and highlight the limited vertical learning within. To tackle these limitations, we propose a hybrid detection framework named Voxel-Pillar Fusion (VPF), which synergistically combines the unique strengths of both voxels and pillars. To be concrete, we first develop a sparse voxel-pillar encoder that encodes point clouds into voxel and pillar features through 3D and 2D sparse convolutions respectively, and then introduce the Sparse Fusion Layer (SFL), facilitating bidirectional interaction between sparse voxel and pillar features. Our computationally efficient, fully sparse method can be seamlessly integrated into both dense and sparse detectors. Leveraging this powerful yet straightforward representation, VPF delivers competitive performance, achieving real-time inference speeds on the nuScenes and Waymo Open Dataset",
    "checked": true,
    "id": "d29194b63599d0c13c0690268e29123a20e634ef",
    "semantic_title": "voxel or pillar: exploring efficient point cloud representation for 3d object detection",
    "citation_count": 1,
    "authors": [
      "Yuhao Huang",
      "Sanping Zhou",
      "Junjie Zhang",
      "Jinpeng Dong",
      "Nanning Zheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28019": {
    "title": "COMBAT: Alternated Training for Effective Clean-Label Backdoor Attacks",
    "volume": "main",
    "abstract": "Backdoor attacks pose a critical concern to the practice of using third-party data for AI development. The data can be poisoned to make a trained model misbehave when a predefined trigger pattern appears, granting the attackers illegal benefits. While most proposed backdoor attacks are dirty-label, clean-label attacks are more desirable by keeping data labels unchanged to dodge human inspection. However, designing a working clean-label attack is a challenging task, and existing clean-label attacks show underwhelming performance. In this paper, we propose a novel mechanism to develop clean-label attacks with outstanding attack performance. The key component is a trigger pattern generator, which is trained together with a surrogate model in an alternating manner. Our proposed mechanism is flexible and customizable, allowing different backdoor trigger types and behaviors for either single or multiple target labels. Our backdoor attacks can reach near-perfect attack success rates and bypass all state-of-the-art backdoor defenses, as illustrated via comprehensive experiments on standard benchmark datasets. Our code is available at https://github.com/VinAIResearch/COMBAT",
    "checked": true,
    "id": "3fcc029ed4469bb30296228f07237b65a8b4e0c0",
    "semantic_title": "combat: alternated training for effective clean-label backdoor attacks",
    "citation_count": 2,
    "authors": [
      "Tran Huynh",
      "Dang Nguyen",
      "Tung Pham",
      "Anh Tran"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28020": {
    "title": "MagiCapture: High-Resolution Multi-Concept Portrait Customization",
    "volume": "main",
    "abstract": "Large-scale text-to-image models including Stable Diffusion are capable of generating high-fidelity photorealistic portrait images. There is an active research area dedicated to personalizing these models, aiming to synthesize specific subjects or styles using provided sets of reference images. However, despite the plausible results from these personalization methods, they tend to produce images that often fall short of realism and are not yet on a commercially viable level. This is particularly noticeable in portrait image generation, where any unnatural artifact in human faces is easily discernible due to our inherent human bias. To address this, we introduce MagiCapture, a personalization method for integrating subject and style concepts to generate high-resolution portrait images using just a few subject and style references. For instance, given a handful of random selfies, our fine-tuned model can generate high-quality portrait images in specific styles, such as passport or profile photos. The main challenge with this task is the absence of ground truth for the composed concepts, leading to a reduction in the quality of the final output and an identity shift of the source subject. To address these issues, we present a novel Attention Refocusing loss coupled with auxiliary priors, both of which facilitate robust learning within this weakly supervised learning setting. Our pipeline also includes additional post-processing steps to ensure the creation of highly realistic outputs. MagiCapture outperforms other baselines in both quantitative and qualitative evaluations and can also be generalized to other non-human objects",
    "checked": true,
    "id": "f0410acf0c7c1e391023db24b3162ba11fc38d58",
    "semantic_title": "magicapture: high-resolution multi-concept portrait customization",
    "citation_count": 9,
    "authors": [
      "Junha Hyung",
      "Jaeyo Shin",
      "Jaegul Choo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28021": {
    "title": "Rethinking Peculiar Images by Diffusion Models: Revealing Local Minima's Role",
    "volume": "main",
    "abstract": "Recent significant advancements in diffusion models have revolutionized image generation, enabling the synthesis of highly realistic images with text-based guidance. These breakthroughs have paved the way for constructing datasets via generative artificial intelligence (AI), offering immense potential for various applications. However, two critical challenges hinder the widespread adoption of synthesized data: computational cost and the generation of peculiar images. While computational costs have improved through various approaches, the issue of peculiar image generation remains relatively unexplored. Existing solutions rely on heuristics, extra training, or AI-based post-processing to mitigate this problem. In this paper, we present a novel approach to address both issues simultaneously. We establish that both gradient descent and diffusion sampling are specific cases of the generalized expectation maximization algorithm. We hypothesize and empirically demonstrate that peculiar image generation is akin to the local minima problem in optimization. Inspired by optimization techniques, we apply naive momentum and positive-negative momentum to diffusion sampling. Last, we propose new metrics to evaluate the peculiarity. Experimental results show momentum effectively prevents peculiar image generation without extra computation",
    "checked": true,
    "id": "a8cd87b8f225325525f5a4ff9ef82259dfc4908d",
    "semantic_title": "rethinking peculiar images by diffusion models: revealing local minima's role",
    "citation_count": 0,
    "authors": [
      "Jinhyeok Jang",
      "Chan-Hyun Youn",
      "Minsu Jeon",
      "Changha Lee"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28022": {
    "title": "ProxyDet: Synthesizing Proxy Novel Classes via Classwise Mixup for Open-Vocabulary Object Detection",
    "volume": "main",
    "abstract": "Open-vocabulary object detection (OVOD) aims to recognize novel objects whose categories are not included in the training set. In order to classify these unseen classes during training, many OVOD frameworks leverage the zero-shot capability of largely pretrained vision and language models, such as CLIP. To further improve generalization on the unseen novel classes, several approaches proposed to additionally train with pseudo region labeling on the external data sources that contain a substantial number of novel category labels beyond the existing training data. Albeit its simplicity, these pseudo-labeling methods still exhibit limited improvement with regard to the truly unseen novel classes that were not pseudo-labeled. In this paper, we present a novel, yet simple technique that helps generalization on the overall distribution of novel classes. Inspired by our observation that numerous novel classes reside within the convex hull constructed by the base (seen) classes in the CLIP embedding space, we propose to synthesize proxy-novel classes approximating novel classes via linear mixup between a pair of base classes. By training our detector with these synthetic proxy-novel classes, we effectively explore the embedding space of novel classes. The experimental results on various OVOD benchmarks such as LVIS and COCO demonstrate superior performance on novel classes compared to the other state-of-the-art methods. Code is available at https://github.com/clovaai/ProxyDet",
    "checked": true,
    "id": "f1a81194349bfe012b1c01a1fd0b2bcd66889b62",
    "semantic_title": "proxydet: synthesizing proxy novel classes via classwise mixup for open-vocabulary object detection",
    "citation_count": 3,
    "authors": [
      "Joonhyun Jeong",
      "Geondo Park",
      "Jayeon Yoo",
      "Hyungsik Jung",
      "Heesu Kim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28023": {
    "title": "A Diffusion Model with State Estimation for Degradation-Blind Inverse Imaging",
    "volume": "main",
    "abstract": "Solving the task of inverse imaging problems can restore unknown clean images from input measurements that have incomplete information. Utilizing powerful generative models, such as denoising diffusion models, could better tackle the ill-posed issues of inverse problems with the distribution prior of the unknown clean images. We propose a learnable state-estimator-based diffusion model to incorporate the measurements into the reconstruction process. Our method makes efficient use of the pre-trained diffusion models with computational feasibility compared to the conditional diffusion models, which need to be trained from scratch. In addition, our pipeline does not require explicit knowledge of the image degradation operator or make the assumption of its form, unlike many other works that use the pre-trained diffusion models at the test time. The experiments on three typical inverse imaging problems (both linear and non-linear), inpainting, deblurring, and JPEG compression restoration, have comparable results with the state-of-the-art methods",
    "checked": true,
    "id": "a8873bc9e9323d80f37c9d06d64492fbe9cbbb2d",
    "semantic_title": "a diffusion model with state estimation for degradation-blind inverse imaging",
    "citation_count": 0,
    "authors": [
      "Liya Ji",
      "Zhefan Rao",
      "Sinno Jialin Pan",
      "Chenyang Lei",
      "Qifeng Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28024": {
    "title": "SSMG: Spatial-Semantic Map Guided Diffusion Model for Free-Form Layout-to-Image Generation",
    "volume": "main",
    "abstract": "Despite significant progress in Text-to-Image (T2I) generative models, even lengthy and complex text descriptions still struggle to convey detailed controls. In contrast, Layout-to-Image (L2I) generation, aiming to generate realistic and complex scene images from user-specified layouts, has risen to prominence. However, existing methods transform layout information into tokens or RGB images for conditional control in the generative process, leading to insufficient spatial and semantic controllability of individual instances. To address these limitations, we propose a novel Spatial-Semantic Map Guided (SSMG) diffusion model that adopts the feature map, derived from the layout, as guidance. Owing to rich spatial and semantic information encapsulated in well-designed feature maps, SSMG achieves superior generation quality with sufficient spatial and semantic controllability compared to previous works. Additionally, we propose the Relation-Sensitive Attention (RSA) and Location-Sensitive Attention (LSA) mechanisms. The former aims to model the relationships among multiple objects within scenes while the latter is designed to heighten the model's sensitivity to the spatial information embedded in the guidance. Extensive experiments demonstrate that SSMG achieves highly promising results, setting a new state-of-the-art across a range of metrics encompassing fidelity, diversity, and controllability",
    "checked": true,
    "id": "f1442ebd0c1619d190f893d79c9d064149c0c06a",
    "semantic_title": "ssmg: spatial-semantic map guided diffusion model for free-form layout-to-image generation",
    "citation_count": 6,
    "authors": [
      "Chengyou Jia",
      "Minnan Luo",
      "Zhuohang Dang",
      "Guang Dai",
      "Xiaojun Chang",
      "Mengmeng Wang",
      "Jingdong Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28025": {
    "title": "TiMix: Text-Aware Image Mixing for Effective Vision-Language Pre-training",
    "volume": "main",
    "abstract": "Self-supervised Multi-modal Contrastive Learning (SMCL) remarkably advances modern Vision-Language Pre-training (VLP) models by aligning visual and linguistic modalities. Due to noises in web-harvested text-image pairs, however, scaling up training data volume in SMCL presents considerable obstacles in terms of computational cost and data inefficiency. To improve data efficiency in VLP, we propose Text-aware Image Mixing (TiMix), which integrates mix-based data augmentation techniques into SMCL, yielding significant performance improvements without significantly increasing computational overhead. We provide a theoretical analysis of TiMix from a mutual information (MI) perspective, showing that mixed data samples for cross-modal contrastive learning implicitly serve as a regularizer for the contrastive loss. The experimental results demonstrate that TiMix exhibits a comparable performance on downstream tasks, even with a reduced amount of training data and shorter training time, when benchmarked against existing methods. This work empirically and theoretically demonstrates the potential of data mixing for data-efficient and computationally viable VLP, benefiting broader VLP model adoption in practical scenarios. Our code is available on https://github.com/chaoyajiang/TiMiX/tree/main",
    "checked": true,
    "id": "c335be43056c9f7e239c80e8e78721eb562405f7",
    "semantic_title": "timix: text-aware image mixing for effective vision-language pre-training",
    "citation_count": 2,
    "authors": [
      "Chaoya Jiang",
      "Wei Ye",
      "Haiyang Xu",
      "Qinghao Ye",
      "Ming Yan",
      "Ji Zhang",
      "Shikun Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28026": {
    "title": "Revealing the Proximate Long-Tail Distribution in Compositional Zero-Shot Learning",
    "volume": "main",
    "abstract": "Compositional Zero-Shot Learning (CZSL) aims to transfer knowledge from seen state-object pairs to novel unseen pairs. In this process, visual bias caused by the diverse interrelationship of state-object combinations blurs their visual features, hindering the learning of distinguishable class prototypes. Prevailing methods concentrate on disentangling states and objects directly from visual features, disregarding potential enhancements that could arise from a data viewpoint. Experimentally, we unveil the results caused by the above problem closely approximate the long-tailed distribution. As a solution, we transform CZSL into a proximate class imbalance problem. We mathematically deduce the role of class prior within the long-tailed distribution in CZSL. Building upon this insight, we incorporate visual bias caused by compositions into the classifier's training and inference by estimating it as a proximate class prior. This enhancement encourages the classifier to acquire more discernible class prototypes for each composition, thereby achieving more balanced predictions. Experimental results demonstrate that our approach elevates the model's performance to the state-of-the-art level, without introducing additional parameters",
    "checked": true,
    "id": "cd5f19ec0506eff3fd5dae32b236ae6595d4d3a5",
    "semantic_title": "revealing the proximate long-tail distribution in compositional zero-shot learning",
    "citation_count": 0,
    "authors": [
      "Chenyi Jiang",
      "Haofeng Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28027": {
    "title": "MWSIS: Multimodal Weakly Supervised Instance Segmentation with 2D Box Annotations for Autonomous Driving",
    "volume": "main",
    "abstract": "Instance segmentation is a fundamental research in computer vision, especially in autonomous driving. However, manual mask annotation for instance segmentation is quite time-consuming and costly. To address this problem, some prior works attempt to apply weakly supervised manner by exploring 2D or 3D boxes. However, no one has ever successfully segmented 2D and 3D instances simultaneously by only using 2D box annotations, which could further reduce the annotation cost by an order of magnitude. Thus, we propose a novel framework called Multimodal Weakly Supervised Instance Segmentation (MWSIS), which incorporates various fine-grained label correction modules for both 2D and 3D modalities, along with a new multimodal cross-supervision approach. In the 2D pseudo label generation branch, the Instance-based Pseudo Mask Generation (IPG) module utilizes predictions for self-supervised correction. Similarly, in the 3D pseudo label generation branch, the Spatial-based Pseudo Label Generation (SPG) module generates pseudo labels by incorporating the spatial prior information of the point cloud. To further refine the generated pseudo labels, the Point-based Voting Label Correction (PVC) module utilizes historical predictions for correction. Additionally, a Ring Segment-based Label Correction (RSC) module is proposed to refine the predictions by leveraging the depth prior information from the point cloud. Finally, the Consistency Sparse Cross-modal Supervision (CSCS) module reduces the inconsistency of multimodal predictions by response distillation. Particularly, transferring the 3D backbone to downstream tasks not only improves the performance of the 3D detectors, but also outperforms fully supervised instance segmentation with only 5% fully supervised annotations. On the Waymo dataset, the proposed framework demonstrates significant improvements over the baseline, especially achieving 2.59% mAP and 12.75% mAP increases for 2D and 3D instance segmentation tasks, respectively. The code is available at https://github.com/jiangxb98/mwsis-plugin",
    "checked": true,
    "id": "569e269e2e27919ab08c58c39fd4e2c3772ba171",
    "semantic_title": "mwsis: multimodal weakly supervised instance segmentation with 2d box annotations for autonomous driving",
    "citation_count": 0,
    "authors": [
      "Guangfeng Jiang",
      "Jun Liu",
      "Yuzhi Wu",
      "Wenlong Liao",
      "Tao He",
      "Pai Peng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28028": {
    "title": "Transferable Video Moment Localization by Moment-Guided Query Prompting",
    "volume": "main",
    "abstract": "Video moment localization stands as a crucial task within the realm of computer vision, entailing the identification of temporal moments in untrimmed videos that bear semantic relevance to the supplied natural language queries. This work delves into a relatively unexplored facet of the task: the transferability of video moment localization models. This concern is addressed by evaluating moment localization models within a cross-domain transfer setting. In this setup, we curate multiple datasets distinguished by substantial domain gaps. The model undergoes training on one of these datasets, while validation and testing are executed using the remaining datasets. To confront the challenges inherent in this scenario, we draw inspiration from the recently introduced large-scale pre-trained vision-language models. Our focus is on exploring how the strategic utilization of these resources can bolster the capabilities of a model designed for video moment localization. Nevertheless, the distribution of language queries in video moment localization usually diverges from the text used by pre-trained models, exhibiting distinctions in aspects such as length, content, expression, and more. To mitigate the gap, this work proposes a Moment-Guided Query Prompting (MGQP) method for video moment localization. Our key idea is to generate multiple distinct and complementary prompt primitives through stratification of the original queries. Our approach is comprised of a prompt primitive constructor, a multimodal prompt refiner, and a holistic prompt incorporator. We carry out extensive experiments on Charades-STA, TACoS, DiDeMo, and YouCookII datasets, and investigate the efficacy of the proposed method using various pre-trained models, such as CLIP, ActionCLIP, CLIP4Clip, and VideoCLIP. The experimental results demonstrate the effectiveness of our proposed method",
    "checked": true,
    "id": "d2074234484f4d257d1ae9bdbe923c60d8b6c500",
    "semantic_title": "transferable video moment localization by moment-guided query prompting",
    "citation_count": 0,
    "authors": [
      "Hao Jiang",
      "Yang Yizhang",
      "Yadong Mu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28029": {
    "title": "In-Hand 3D Object Reconstruction from a Monocular RGB Video",
    "volume": "main",
    "abstract": "Our work aims to reconstruct a 3D object that is held and rotated by a hand in front of a static RGB camera. Previous methods that use implicit neural representations to recover the geometry of a generic hand-held object from multi-view images achieved compelling results in the visible part of the object. However, these methods falter in accurately capturing the shape within the hand-object contact region due to occlusion. In this paper, we propose a novel method that deals with surface reconstruction under occlusion by incorporating priors of 2D occlusion elucidation and physical contact constraints. For the former, we introduce an object amodal completion network to infer the 2D complete mask of objects under occlusion. To ensure the accuracy and view consistency of the predicted 2D amodal masks, we devise a joint optimization method for both amodal mask refinement and 3D reconstruction. For the latter, we impose penetration and attraction constraints on the local geometry in contact regions. We evaluate our approach on HO3D and HOD datasets and demonstrate that it outperforms the state-of-the-art methods in terms of reconstruction surface quality, with an improvement of 52% on HO3D and 20% on HOD. Project webpage: https://east-j.github.io/ihor",
    "checked": true,
    "id": "5e048334ee6e2d0977bec32f697abf80aed58ac0",
    "semantic_title": "in-hand 3d object reconstruction from a monocular rgb video",
    "citation_count": 1,
    "authors": [
      "Shijian Jiang",
      "Qi Ye",
      "Rengan Xie",
      "Yuchi Huo",
      "Xiang Li",
      "Yang Zhou",
      "Jiming Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28030": {
    "title": "AACP: Aesthetics Assessment of Children's Paintings Based on Self-Supervised Learning",
    "volume": "main",
    "abstract": "The Aesthetics Assessment of Children's Paintings (AACP) is an important branch of the image aesthetics assessment (IAA), playing a significant role in children's education. This task presents unique challenges, such as limited available data and the requirement for evaluation metrics from multiple perspectives. However, previous approaches have relied on training large datasets and subsequently providing an aesthetics score to the image, which is not applicable to AACP. To solve this problem, we construct an aesthetics assessment dataset of children's paintings and a model based on self-supervised learning. 1) We build a novel dataset composed of two parts: the first part contains more than 20k unlabeled images of children's paintings; the second part contains 1.2k images of children's paintings, and each image contains eight attributes labeled by multiple design experts. 2) We design a pipeline that includes a feature extraction module, perception modules and a disentangled evaluation module. 3) We conduct both qualitative and quantitative experiments to compare our model's performance with five other methods using the AACP dataset. Our experiments reveal that our method can accurately capture aesthetic features and achieve state-of-the-art performance",
    "checked": true,
    "id": "816342f5da41cd1b94b7b973cbc8cfc95ec0f3e6",
    "semantic_title": "aacp: aesthetics assessment of children's paintings based on self-supervised learning",
    "citation_count": 0,
    "authors": [
      "Shiqi Jiang",
      "Ning Li",
      "Chen Shi",
      "Liping Guo",
      "Changbo Wang",
      "Chenhui Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28031": {
    "title": "Exploring Self- and Cross-Triplet Correlations for Human-Object Interaction Detection",
    "volume": "main",
    "abstract": "Human-Object Interaction (HOI) detection plays a vital role in scene understanding, which aims to predict the HOI triplet in the form of . Existing methods mainly extract multi-modal features (e.g., appearance, object semantics, human pose) and then fuse them together to directly predict HOI triplets. However, most of these methods focus on seeking for self-triplet aggregation, but ignore the potential cross-triplet dependencies, resulting in ambiguity of action prediction. In this work, we propose to explore Self- and Cross-Triplet Correlations (SCTC) for HOI detection. Specifically, we regard each triplet proposal as a graph where Human, Object represent nodes and Action indicates edge, to aggregate self-triplet correlation. Also, we try to explore cross-triplet dependencies by jointly considering instance-level, semantic-level, and layout-level relations. Besides, we leverage the CLIP model to assist our SCTC obtain interaction-aware feature by knowledge distillation, which provides useful action clues for HOI detection. Extensive experiments on HICO-DET and V-COCO datasets verify the effectiveness of our proposed SCTC",
    "checked": true,
    "id": "b1a8b29ef7e5202a7998e529abfaa919f12b63d2",
    "semantic_title": "exploring self- and cross-triplet correlations for human-object interaction detection",
    "citation_count": 0,
    "authors": [
      "Weibo Jiang",
      "Weihong Ren",
      "Jiandong Tian",
      "Liangqiong Qu",
      "Zhiyong Wang",
      "Honghai Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28032": {
    "title": "Comprehensive Visual Grounding for Video Description",
    "volume": "main",
    "abstract": "The grounding accuracy of existing video captioners is still behind the expectation. The majority of existing methods perform grounded video captioning on sparse entity annotations, whereas the captioning accuracy often suffers from degenerated object appearances on the annotated area such as motion blur and video defocus. Moreover, these methods seldom consider the complex interactions among entities. In this paper, we propose a comprehensive visual grounding network to improve video captioning, by explicitly linking the entities and actions to the visual clues across the video frames. Specifically, the network consists of spatial-temporal entity grounding and action grounding. The proposed entity grounding encourages the attention mechanism to focus on informative spatial areas across video frames, albeit the entity is annotated in only one frame of a video. The action grounding dynamically associates the verbs to related subjects and the corresponding context, which keeps fine-grained spatial and temporal details for action prediction. Both entity grounding and action grounding are formulated as a unified task guided by a soft grounding supervision, which brings architecture simplification and improves training efficiency as well. We conduct extensive experiments on two challenging datasets, and demonstrate significant performance improvements of +2.3 CIDEr on ActivityNet-Entities and +2.2 CIDEr on MSR-VTT compared to state-of-the-arts",
    "checked": true,
    "id": "880279bee3b62c9c515e0d18b0147d1b0eed5d7f",
    "semantic_title": "comprehensive visual grounding for video description",
    "citation_count": 0,
    "authors": [
      "Wenhui Jiang",
      "Yibo Cheng",
      "Linxin Liu",
      "Yuming Fang",
      "Yuxin Peng",
      "Yang Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28033": {
    "title": "Far3D: Expanding the Horizon for Surround-View 3D Object Detection",
    "volume": "main",
    "abstract": "Recently 3D object detection from surround-view images has made notable advancements with its low deployment cost. However, most works have primarily focused on close perception range while leaving long-range detection less explored. Expanding existing methods directly to cover long distances poses challenges such as heavy computation costs and unstable convergence. To address these limitations, this paper proposes a novel sparse query-based framework, dubbed Far3D. By utilizing high-quality 2D object priors, we generate 3D adaptive queries that complement the 3D global queries. To efficiently capture discriminative features across different views and scales for long-range objects, we introduce a perspective-aware aggregation module. Additionally, we propose a range-modulated 3D denoising approach to address query error propagation and mitigate convergence issues in long-range tasks. Significantly, Far3D demonstrates SoTA performance on the challenging Argoverse 2 dataset, covering a wide range of 150 meters, surpassing several LiDAR-based approaches. The code is available at https://github.com/megvii-research/Far3D",
    "checked": true,
    "id": "93c4cac28873829a8de48942c286044bf89cc037",
    "semantic_title": "far3d: expanding the horizon for surround-view 3d object detection",
    "citation_count": 17,
    "authors": [
      "Xiaohui Jiang",
      "Shuailin Li",
      "Yingfei Liu",
      "Shihao Wang",
      "Fan Jia",
      "Tiancai Wang",
      "Lijin Han",
      "Xiangyu Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28034": {
    "title": "Delving into Multimodal Prompting for Fine-Grained Visual Classification",
    "volume": "main",
    "abstract": "Fine-grained visual classification (FGVC) involves categorizing fine subdivisions within a broader category, which poses challenges due to subtle inter-class discrepancies and large intra-class variations. However, prevailing approaches primarily focus on uni-modal visual concepts. Recent advancements in pre-trained vision-language models have demonstrated remarkable performance in various high-level vision tasks, yet the applicability of such models to FGVC tasks remains uncertain. In this paper, we aim to fully exploit the capabilities of cross-modal description to tackle FGVC tasks and propose a novel multimodal prompting solution, denoted as MP-FGVC, based on the contrastive language-image pertaining (CLIP) model. Our MP-FGVC comprises a multimodal prompts scheme and a multimodal adaptation scheme. The former includes Subcategory-specific Vision Prompt (SsVP) and Discrepancy-aware Text Prompt (DaTP), which explicitly highlights the subcategory-specific discrepancies from the perspectives of both vision and language. The latter aligns the vision and text prompting elements in a common semantic space, facilitating cross-modal collaborative reasoning through a Vision-Language Fusion Module (VLFM) for further improvement on FGVC. Moreover, we tailor a two-stage optimization strategy for MP-FGVC to fully leverage the pre-trained CLIP model and expedite efficient adaptation for FGVC. Extensive experiments conducted on four FGVC datasets demonstrate the effectiveness of our MP-FGVC",
    "checked": true,
    "id": "11e3efa08b5db1a8958dfe8119593a4d3f18796a",
    "semantic_title": "delving into multimodal prompting for fine-grained visual classification",
    "citation_count": 3,
    "authors": [
      "Xin Jiang",
      "Hao Tang",
      "Junyao Gao",
      "Xiaoyu Du",
      "Shengfeng He",
      "Zechao Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28035": {
    "title": "MCA: Moment Channel Attention Networks",
    "volume": "main",
    "abstract": "Channel attention mechanisms endeavor to recalibrate channel weights to enhance representation abilities of networks. However, mainstream methods often rely solely on global average pooling as the feature squeezer, which significantly limits the overall potential of models. In this paper, we investigate the statistical moments of feature maps within a neural network. Our findings highlight the critical role of high-order moments in enhancing model capacity. Consequently, we introduce a flexible and comprehensive mechanism termed Extensive Moment Aggregation (EMA) to capture the global spatial context. Building upon this mechanism, we propose the Moment Channel Attention (MCA) framework, which efficiently incorporates multiple levels of moment-based information while minimizing additional computation costs through our Cross Moment Convolution (CMC) module. The CMC module via channel-wise convolution layer to capture multiple order moment information as well as cross channel features. The MCA block is designed to be lightweight and easily integrated into a variety of neural network architectures. Experimental results on classical image classification, object detection, and instance segmentation tasks demonstrate that our proposed method achieves state-of-the-art results, outperforming existing channel attention methods",
    "checked": true,
    "id": "e0f4ef66e1c53b8034d34840d1375cbef7e8dfd7",
    "semantic_title": "mca: moment channel attention networks",
    "citation_count": 0,
    "authors": [
      "Yangbo Jiang",
      "Zhiwei Jiang",
      "Le Han",
      "Zenan Huang",
      "Nenggan Zheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28036": {
    "title": "Towards Robust Image Stitching: An Adaptive Resistance Learning against Compatible Attacks",
    "volume": "main",
    "abstract": "Image stitching seamlessly integrates images captured from varying perspectives into a single wide field-of-view image. Such integration not only broadens the captured scene but also augments holistic perception in computer vision applications. Given a pair of captured images, subtle perturbations and distortions which go unnoticed by the human visual system tend to attack the correspondence matching, impairing the performance of image stitching algorithms. In light of this challenge, this paper presents the first attempt to improve the robustness of image stitching against adversarial attacks. Specifically, we introduce a stitching-oriented attack (SoA), tailored to amplify the alignment loss within overlapping regions, thereby targeting the feature matching procedure. To establish an attack resistant model, we delve into the robustness of stitching architecture and develop an adaptive adversarial training (AAT) to balance attack resistance with stitching precision. In this way, we relieve the gap between the routine adversarial training and benign models, ensuring resilience without quality compromise. Comprehensive evaluation across real-world and synthetic datasets validate the deterioration of SoA on stitching performance. Furthermore, AAT emerges as a more robust solution against adversarial perturbations, delivering superior stitching results. Code is available at: https://github.com/Jzy2017/TRIS",
    "checked": true,
    "id": "3ac562d89872a03e0f4ede662731132e891be26e",
    "semantic_title": "towards robust image stitching: an adaptive resistance learning against compatible attacks",
    "citation_count": 0,
    "authors": [
      "Zhiying Jiang",
      "Xingyuan Li",
      "Jinyuan Liu",
      "Xin Fan",
      "Risheng Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28037": {
    "title": "Instance-Aware Multi-Camera 3D Object Detection with Structural Priors Mining and Self-Boosting Learning",
    "volume": "main",
    "abstract": "Camera-based bird-eye-view (BEV) perception paradigm has made significant progress in the autonomous driving field. Under such a paradigm, accurate BEV representation construction relies on reliable depth estimation for multi-camera images. However, existing approaches exhaustively predict depths for every pixel without prioritizing objects, which are precisely the entities requiring detection in the 3D space. To this end, we propose IA-BEV, which integrates image-plane instance awareness into the depth estimation process within a BEV-based detector. First, a category-specific structural priors mining approach is proposed for enhancing the efficacy of monocular depth generation. Besides, a self-boosting learning strategy is further proposed to encourage the model to place more emphasis on challenging objects in computation-expensive temporal stereo matching. Together they provide advanced depth estimation results for high-quality BEV features construction, benefiting the ultimate 3D detection. The proposed method achieves state-of-the-art performances on the challenging nuScenes benchmark, and extensive experimental results demonstrate the effectiveness of our designs",
    "checked": true,
    "id": "5343856c6bd7b13c282918b933ac69b8a0b41fe8",
    "semantic_title": "instance-aware multi-camera 3d object detection with structural priors mining and self-boosting learning",
    "citation_count": 2,
    "authors": [
      "Yang Jiao",
      "Zequn Jie",
      "Shaoxiang Chen",
      "Lechao Cheng",
      "Jingjing Chen",
      "Lin Ma",
      "Yu-Gang Jiang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28038": {
    "title": "PromptMRG: Diagnosis-Driven Prompts for Medical Report Generation",
    "volume": "main",
    "abstract": "Automatic medical report generation (MRG) is of great research value as it has the potential to relieve radiologists from the heavy burden of report writing. Despite recent advancements, accurate MRG remains challenging due to the need for precise clinical understanding and disease identification. Moreover, the imbalanced distribution of diseases makes the challenge even more pronounced, as rare diseases are underrepresented in training data, making their diagnosis unreliable. To address these challenges, we propose diagnosis-driven prompts for medical report generation (PromptMRG), a novel framework that aims to improve the diagnostic accuracy of MRG with the guidance of diagnosis-aware prompts. Specifically, PromptMRG is based on encoder-decoder architecture with an extra disease classification branch. When generating reports, the diagnostic results from the classification branch are converted into token prompts to explicitly guide the generation process. To further improve the diagnostic accuracy, we design cross-modal feature enhancement, which retrieves similar reports from the database to assist the diagnosis of a query image by leveraging the knowledge from a pre-trained CLIP. Moreover, the disease imbalanced issue is addressed by applying an adaptive logit-adjusted loss to the classification branch based on the individual learning status of each disease, which overcomes the barrier of text decoder's inability to manipulate disease distributions. Experiments on two MRG benchmarks show the effectiveness of the proposed method, where it obtains state-of-the-art clinical efficacy performance on both datasets",
    "checked": true,
    "id": "9dbc47793bc136e4728e3c4717db6694700ef344",
    "semantic_title": "promptmrg: diagnosis-driven prompts for medical report generation",
    "citation_count": 14,
    "authors": [
      "Haibo Jin",
      "Haoxuan Che",
      "Yi Lin",
      "Hao Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28039": {
    "title": "PCE-Palm: Palm Crease Energy Based Two-Stage Realistic Pseudo-Palmprint Generation",
    "volume": "main",
    "abstract": "The lack of large-scale data seriously hinders the development of palmprint recognition. Recent approaches address this issue by generating large-scale realistic pseudo palmprints from Bézier curves. However, the significant difference between Bézier curves and real palmprints limits their effectiveness. In this paper, we divide the Bézier-Real difference into creases and texture differences, thus reducing the generation difficulty. We introduce a new palm crease energy (PCE) domain as a bridge from Bézier curves to real palmprints and propose a two-stage generation model. The first stage generates PCE images (realistic creases) from Bézier curves, and the second stage outputs realistic palmprints (realistic texture) with PCE images as input. In addition, we also design a lightweight plug-and-play line feature enhancement block to facilitate domain transfer and improve recognition performance. Extensive experimental results demonstrate that the proposed method surpasses state-of-the-art methods. Under extremely few data settings like 40 IDs (only 2.5% of the total training set), our model achieves a 29% improvement over RPG-Palm and outperforms ArcFace with 100% training set by more than 6% in terms of TAR@FAR=1e-6",
    "checked": true,
    "id": "d64aa6d7564a1e982cdd5db89469e0d3dd04023a",
    "semantic_title": "pce-palm: palm crease energy based two-stage realistic pseudo-palmprint generation",
    "citation_count": 1,
    "authors": [
      "Jianlong Jin",
      "Lei Shen",
      "Ruixin Zhang",
      "Chenglong Zhao",
      "Ge Jin",
      "Jingyun Zhang",
      "Shouhong Ding",
      "Yang Zhao",
      "Wei Jia"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28040": {
    "title": "SwiftPillars: High-Efficiency Pillar Encoder for Lidar-Based 3D Detection",
    "volume": "main",
    "abstract": "Lidar-based 3D Detection is one of the significant components of Autonomous Driving. However, current methods over-focus on improving the performance of 3D Lidar perception, which causes the architecture of networks becoming complicated and hard to deploy. Thus, the methods are difficult to apply in Autonomous Driving for real-time processing. In this paper, we propose a high-efficiency network, SwiftPillars, which includes Swift Pillar Encoder (SPE) and Multi-scale Aggregation Decoder (MAD). The SPE is constructed by a concise Dual-attention Module with lightweight operators. The Dual-attention Module utilizes feature pooling, matrix multiplication, etc. to speed up point-wise and channel-wise attention extraction and fusion. The MAD interconnects multiple scale features extracted by SPE with minimal computational cost to leverage performance. In our experiments, our proposal accomplishes 61.3% NDS and 53.2% mAP in nuScenes dataset. In addition, we evaluate inference time on several platforms (P4, T4, A2, MLU370, RTX3080), where SwiftPillars achieves up to 13.3ms (75FPS) on NVIDIA Tesla T4. Compared with PointPillars, SwiftPillars is on average 26.58% faster in inference speed with equivalent GPUs and a higher mAP of approximately 3.2% in the nuScenes dataset",
    "checked": true,
    "id": "fe64b08c16231505abdb1b5cf25ffcffbcf9ccff",
    "semantic_title": "swiftpillars: high-efficiency pillar encoder for lidar-based 3d detection",
    "citation_count": 0,
    "authors": [
      "Xin Jin",
      "Kai Liu",
      "Cong Ma",
      "Ruining Yang",
      "Fei Hui",
      "Wei Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28041": {
    "title": "DeS3: Adaptive Attention-Driven Self and Soft Shadow Removal Using ViT Similarity",
    "volume": "main",
    "abstract": "Removing soft and self shadows that lack clear boundaries from a single image is still challenging. Self shadows are shadows that are cast on the object itself. Most existing methods rely on binary shadow masks, without considering the ambiguous boundaries of soft and self shadows. In this paper, we present DeS3, a method that removes hard, soft and self shadows based on adaptive attention and ViT similarity. Our novel ViT similarity loss utilizes features extracted from a pre-trained Vision Transformer. This loss helps guide the reverse sampling towards recovering scene structures. Our adaptive attention is able to differentiate shadow regions from the underlying objects, as well as shadow regions from the object casting the shadow. This capability enables DeS3 to better recover the structures of objects even when they are partially occluded by shadows. Different from existing methods that rely on constraints during the training phase, we incorporate the ViT similarity during the sampling stage. Our method outperforms state-of-the-art methods on the SRD, AISTD, LRSS, USR and UIUC datasets, removing hard, soft, and self shadows robustly. Specifically, our method outperforms the SOTA method by 16% of the RMSE of the whole image on the LRSS dataset",
    "checked": true,
    "id": "de8cfa46816293b2c50b8260366cbb82807caf7e",
    "semantic_title": "des3: adaptive attention-driven self and soft shadow removal using vit similarity",
    "citation_count": 4,
    "authors": [
      "Yeying Jin",
      "Wei Ye",
      "Wenhan Yang",
      "Yuan Yuan",
      "Robby T. Tan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28042": {
    "title": "AMD: Anatomical Motion Diffusion with Interpretable Motion Decomposition and Fusion",
    "volume": "main",
    "abstract": "Generating realistic human motion sequences from text descriptions is a challenging task that requires capturing the rich expressiveness of both natural language and human motion. Recent advances in diffusion models have enabled significant progress in human motion synthesis. However, existing methods struggle to handle text inputs that describe complex or long motions. In this paper, we propose the Adaptable Motion Diffusion (AMD) model, which leverages a Large Language Model (LLM) to parse the input text into a sequence of concise and interpretable anatomical scripts that correspond to the target motion. This process exploits the LLM's ability to provide anatomical guidance for complex motion synthesis. We then devise a two-branch fusion scheme that balances the influence of the input text and the anatomical scripts on the inverse diffusion process, which adaptively ensures the semantic fidelity and diversity of the synthesized motion. Our method can effectively handle texts with complex or long motion descriptions, where existing methods often fail. Experiments on datasets with relatively more complex motions, such as CLCD1 and CLCD2, demonstrate that our AMD significantly outperforms existing state-of-the-art models",
    "checked": true,
    "id": "5cba010c96c3b050a5e58641b9455aa6a745256b",
    "semantic_title": "amd: anatomical motion diffusion with interpretable motion decomposition and fusion",
    "citation_count": 3,
    "authors": [
      "Beibei Jing",
      "Youjia Zhang",
      "Zikai Song",
      "Junqing Yu",
      "Wei Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28043": {
    "title": "Retrieval-Augmented Primitive Representations for Compositional Zero-Shot Learning",
    "volume": "main",
    "abstract": "Compositional zero-shot learning (CZSL) aims to recognize unseen attribute-object compositions by learning from seen compositions. Composing the learned knowledge of seen primitives, i.e., attributes or objects, into novel compositions is critical for CZSL. In this work, we propose to explicitly retrieve knowledge of seen primitives for compositional zero-shot learning. We present a retrieval-augmented method, which augments standard multi-path classification methods with two retrieval modules. Specifically, we construct two databases storing the attribute and object representations of training images, respectively. For an input training/testing image, we use two retrieval modules to retrieve representations of training images with the same attribute and object, respectively. The primitive representations of the input image are augmented by using the retrieved representations, for composition recognition. By referencing semantically similar images, the proposed method is capable of recalling knowledge of seen primitives for compositional generalization. Experiments on three widely-used datasets show the effectiveness of the proposed method",
    "checked": true,
    "id": "b2cb7d8df63d395593fccd6799db3efab7efbeea",
    "semantic_title": "retrieval-augmented primitive representations for compositional zero-shot learning",
    "citation_count": 1,
    "authors": [
      "Chenchen Jing",
      "Yukun Li",
      "Hao Chen",
      "Chunhua Shen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28044": {
    "title": "CrossBind: Collaborative Cross-Modal Identification of Protein Nucleic-Acid-Binding Residues",
    "volume": "main",
    "abstract": "Accurate identification of protein nucleic acid binding residues poses a significant challenge with important implications for various biological processes and drug design. Many typical computational methods for protein analysis rely on a single model that could ignore either the semantic context of the protein or the global 3D geometric information. Consequently, these approaches may result in incomplete or inaccurate protein analysis. To address the above issue, in this paper, we present CrossBind, a novel collaborative cross modal approach for identifying binding residues by exploiting both protein geometric structure and its sequence prior knowledge extracted from a large scale protein language model. Specifically, our multi modal approach leverages a contrastive learning technique and atom wise attention to capture the positional relationships between atoms and residues, thereby incorporating fine grained local geometric knowledge, for better binding residue prediction. Extensive experimental results demonstrate that our approach outperforms the next best state of the art methods, GraphSite and GraphBind, on DNA and RNA datasets by 10.8/17.3% in terms of the harmonic mean of precision and recall (F1 Score) and 11.9/24.8% in Matthews correlation coefficient (MCC), respectively. We release the code at https://github.com/BEAM-Labs/CrossBind",
    "checked": true,
    "id": "8ec37c9732ffb324c162cec6e0cb14d324a31381",
    "semantic_title": "crossbind: collaborative cross-modal identification of protein nucleic-acid-binding residues",
    "citation_count": 0,
    "authors": [
      "Linglin Jing",
      "Sheng Xu",
      "Yifan Wang",
      "Yuzhe Zhou",
      "Tao Shen",
      "Zhigang Ji",
      "Hui Fang",
      "Zhen Li",
      "Siqi Sun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28045": {
    "title": "X4D-SceneFormer: Enhanced Scene Understanding on 4D Point Cloud Videos through Cross-Modal Knowledge Transfer",
    "volume": "main",
    "abstract": "The field of 4D point cloud understanding is rapidly developing with the goal of analyzing dynamic 3D point cloud sequences. However, it remains a challenging task due to the sparsity and lack of texture in point clouds. Moreover, the irregularity of point cloud poses a difficulty in aligning temporal information within video sequences. To address these issues, we propose a novel cross-modal knowledge transfer framework, called X4D-SceneFormer. This framework enhances 4D-Scene understanding by transferring texture priors from RGB sequences using a Transformer architecture with temporal relationship mining. Specifically, the framework is designed with a dual-branch architecture, consisting of an 4D point cloud transformer and a Gradient-aware Image Transformer (GIT). The GIT combines visual texture and temporal correlation features to offer rich semantics and dynamics for better point cloud representation. During training, we employ multiple knowledge transfer techniques, including temporal consistency losses and masked self-attention, to strengthen the knowledge transfer between modalities. This leads to enhanced performance during inference using single-modal 4D point cloud inputs. Extensive experiments demonstrate the superior performance of our framework on various 4D point cloud video understanding tasks, including action recognition, action segmentation and semantic segmentation. The results achieve 1st places, i.e., 85.3% (+7.9%) accuracy and 47.3% (+5.0%) mIoU for 4D action segmentation and semantic segmentation, on the HOI4D challenge, outperforming previous state-of-the-art by a large margin. We release the code at https://github.com/jinglinglingling/X4D",
    "checked": true,
    "id": "dfd491548ac65ba20f25033849b64adf32f17bca",
    "semantic_title": "x4d-sceneformer: enhanced scene understanding on 4d point cloud videos through cross-modal knowledge transfer",
    "citation_count": 1,
    "authors": [
      "Linglin Jing",
      "Ying Xue",
      "Xu Yan",
      "Chaoda Zheng",
      "Dong Wang",
      "Ruimao Zhang",
      "Zhigang Wang",
      "Hui Fang",
      "Bin Zhao",
      "Zhen Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28046": {
    "title": "VVS: Video-to-Video Retrieval with Irrelevant Frame Suppression",
    "volume": "main",
    "abstract": "In content-based video retrieval (CBVR), dealing with large-scale collections, efficiency is as important as accuracy; thus, several video-level feature-based studies have actively been conducted. Nevertheless, owing to the severe difficulty of embedding a lengthy and untrimmed video into a single feature, these studies have been insufficient for accurate retrieval compared to frame-level feature-based studies. In this paper, we show that appropriate suppression of irrelevant frames can provide insight into the current obstacles of the video-level approaches. Furthermore, we propose a Video-to-Video Suppression network (VVS) as a solution. VVS is an end-to-end framework that consists of an easy distractor elimination stage to identify which frames to remove and a suppression weight generation stage to determine the extent to suppress the remaining frames. This structure is intended to effectively describe an untrimmed video with varying content and meaningless information. Its efficacy is proved via extensive experiments, and we show that our approach is not only state-of-the-art in video-level approaches but also has a fast inference time despite possessing retrieval capabilities close to those of frame-level approaches. Code is available at https://github.com/sejong-rcv/VVS",
    "checked": true,
    "id": "1b642a398e7925cd169bbeaf0d50d28c26bf2a5c",
    "semantic_title": "vvs: video-to-video retrieval with irrelevant frame suppression",
    "citation_count": 2,
    "authors": [
      "Won Jo",
      "Geuntaek Lim",
      "Gwangjin Lee",
      "Hyunwoo Kim",
      "Byungsoo Ko",
      "Yukyung Choi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28047": {
    "title": "Rethinking Robustness of Model Attributions",
    "volume": "main",
    "abstract": "For machine learning models to be reliable and trustworthy, their decisions must be interpretable. As these models find increasing use in safety-critical applications, it is important that not just the model predictions but also their explanations (as feature attributions) be robust to small human-imperceptible input perturbations. Recent works have shown that many attribution methods are fragile and have proposed improvements in either these methods or the model training. We observe two main causes for fragile attributions: first, the existing metrics of robustness (e.g., top-k intersection) overpenalize even reasonable local shifts in attribution, thereby making random perturbations to appear as a strong attack, and second, the attribution can be concentrated in a small region even when there are multiple important parts in an image. To rectify this, we propose simple ways to strengthen existing metrics and attribution methods that incorporate locality of pixels in robustness metrics and diversity of pixel locations in attributions. Towards the role of model training in attributional robustness, we empirically observe that adversarially trained models have more robust attributions on smaller datasets, however, this advantage disappears in larger datasets. Code is made available at https://github.com/ksandeshk/LENS",
    "checked": true,
    "id": "ba30fb2e9fa8134abe9bf331a39338e542edfbe9",
    "semantic_title": "rethinking robustness of model attributions",
    "citation_count": 0,
    "authors": [
      "Sandesh Kamath",
      "Sankalp Mittal",
      "Amit Deshpande",
      "Vineeth N Balasubramanian"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28048": {
    "title": "Cross-Constrained Progressive Inference for 3D Hand Pose Estimation with Dynamic Observer-Decision-Adjuster Networks",
    "volume": "main",
    "abstract": "Generalization is very important for pose estimation, especially for 3D pose estimation where small changes in the 2D images could trigger structural changes in the 3D space. To achieve generalization, the system needs to have the capability of detecting estimation errors by double-checking the projection coherence between the 3D and 2D spaces and adapting its network inference process based on this feedback. Current pose estimation is one-time feed-forward and lacks the capability to gather feedback and adapt the inference outcome. To address this problem, we propose to explore the concept of progressive inference where the network learns an observer to continuously detect the prediction error based on constraints matching, as well as an adjuster to refine its inference outcome based on these constraints errors. Within the context of 3D hand pose estimation, we find that this observer-adjuster design is relatively unstable since the observer is operating in the 2D image domain while the adjuster is operating in the 3D domain. To address this issue, we propose to construct two sets of observers-adjusters with complementary constraints from different perspectives. They operate in a dynamic sequential manner controlled by a decision network to progressively improve the 3D pose estimation. We refer to this method as Cross-Constrained Progressive Inference (CCPI). Our extensive experimental results on FreiHAND and HO-3D benchmark datasets demonstrate that the proposed CCPI method is able to significantly improve the generalization capability and performance of 3D hand pose estimation",
    "checked": true,
    "id": "28082160e77c2abdfec696b32369589e9433bf8e",
    "semantic_title": "cross-constrained progressive inference for 3d hand pose estimation with dynamic observer-decision-adjuster networks",
    "citation_count": 0,
    "authors": [
      "Zhehan Kan",
      "Xueting Hu",
      "Zihan Liao",
      "Ke Yu",
      "Zhihai He"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28049": {
    "title": "Catch-Up Mix: Catch-Up Class for Struggling Filters in CNN",
    "volume": "main",
    "abstract": "Deep learning has made significant advances in computer vision, particularly in image classification tasks. Despite their high accuracy on training data, deep learning models often face challenges related to complexity and overfitting. One notable concern is that the model often relies heavily on a limited subset of filters for making predictions. This dependency can result in compromised generalization and an increased vulnerability to minor variations. While regularization techniques like weight decay, dropout, and data augmentation are commonly used to address this issue, they may not directly tackle the reliance on specific filters. Our observations reveal that the heavy reliance problem gets severe when slow-learning filters are deprived of learning opportunities due to fast-learning filters. Drawing inspiration from image augmentation research that combats over-reliance on specific image regions by removing and replacing parts of images, Our idea is to mitigate the problem of over-reliance on strong filters by substituting highly activated features. To this end, we present a novel method called Catch-up Mix, which provides learning opportunities to a wide range of filters during training, focusing on filters that may lag behind. By mixing activation maps with relatively lower norms, Catch-up Mix promotes the development of more diverse representations and reduces reliance on a small subset of filters. Experimental results demonstrate the superiority of our method in various vision classification datasets, providing enhanced robustness",
    "checked": true,
    "id": "8923db58bc7d920fb9a2245897ad7f7ac24e7239",
    "semantic_title": "catch-up mix: catch-up class for struggling filters in cnn",
    "citation_count": 1,
    "authors": [
      "Minsoo Kang",
      "Minkoo Kang",
      "Suhyun Kim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28050": {
    "title": "VLCounter: Text-Aware Visual Representation for Zero-Shot Object Counting",
    "volume": "main",
    "abstract": "Zero-Shot Object Counting~(ZSOC) aims to count referred instances of arbitrary classes in a query image without human-annotated exemplars. To deal with ZSOC, preceding studies proposed a two-stage pipeline: discovering exemplars and counting. However, there remains a challenge of vulnerability to error propagation of the sequentially designed two-stage process. In this work, we propose an one-stage baseline, Visual-Language Baseline (VLBase), exploring the implicit association of the semantic-patch embeddings of CLIP. Subsequently, we extend the VLBase to Visual-language Counter (VLCounter) by incorporating three modules devised to tailor VLBase for object counting. First, we introduce Semantic-conditioned Prompt Tuning (SPT) within the image encoder to acquire target-highlighted representations. Second, Learnable Affine Transformation (LAT) is employed to translate the semantic-patch similarity map to be appropriate for the counting task. Lastly, we transfer the layer-wisely encoded features to the decoder through Segment-aware Skip Connection (SaSC) to keep the generalization capability for unseen classes. Through extensive experiments on FSC147, CARPK, and PUCPR+, we demonstrate the benefits of our end-to-end framework, VLCounter. Code is available at https://github.com/seunggu0305/VLCounter",
    "checked": true,
    "id": "ebd49ede490d2cbd431bbf83790ee70b20c418fd",
    "semantic_title": "vlcounter: text-aware visual representation for zero-shot object counting",
    "citation_count": 3,
    "authors": [
      "Seunggu Kang",
      "WonJun Moon",
      "Euiyeon Kim",
      "Jae-Pil Heo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28051": {
    "title": "StegFormer: Rebuilding the Glory of Autoencoder-Based Steganography",
    "volume": "main",
    "abstract": "Image hiding aims to conceal one or more secret images within a cover image of the same resolution. Due to strict capacity requirements, image hiding is commonly called large-capacity steganography. In this paper, we propose StegFormer, a novel autoencoder-based image-hiding model. StegFormer can conceal one or multiple secret images within a cover image of the same resolution while preserving the high visual quality of the stego image. In addition, to mitigate the limitations of current steganographic models in real-world scenarios, we propose a normalizing training strategy and a restrict loss to improve the reliability of the steganographic models under realistic conditions. Furthermore, we propose an efficient steganographic capacity expansion method to increase the capacity of steganography and enhance the efficiency of secret communication. Through this approach, we can increase the relative payload of StegFormer to 96 bits per pixel without any training strategy modifications. Experiments demonstrate that our StegFormer outperforms existing state-of-the-art (SOTA) models. In the case of single-image steganography, there is an improvement of more than 3 dB and 5 dB in PSNR for secret/recovery image pairs and cover/stego image pairs",
    "checked": true,
    "id": "9f1193ce9320b3ffe1dd262b9426495b45f6d8ed",
    "semantic_title": "stegformer: rebuilding the glory of autoencoder-based steganography",
    "citation_count": 0,
    "authors": [
      "Xiao  Ke",
      "Huanqi Wu",
      "Wenzhong Guo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28052": {
    "title": "Expediting Contrastive Language-Image Pretraining via Self-Distilled Encoders",
    "volume": "main",
    "abstract": "Recent advances in vision language pretraining (VLP) have been largely attributed to the large-scale data collected from the web. However, uncurated dataset contains weakly correlated image-text pairs, causing data inefficiency. To address the issue, knowledge distillation have been explored at the expense of extra image and text momentum encoders to generate teaching signals for misaligned image-text pairs. In this paper, our goal is to resolve the misalignment problem with an efficient distillation framework. To this end, we propose ECLIPSE: Expediting Contrastive Language-Image Pretraining with Self-distilled Encoders. ECLIPSE features a distinctive distillation architecture wherein a shared text encoder is utilized between an online image encoder and a momentum image encoder. This strategic design choice enables the distillation to operate within a unified projected space of text embedding, resulting in better performance. Based on the unified text embedding space, ECLIPSE compensates for the additional computational cost of the momentum image encoder by expediting the online image encoder. Through our extensive experiments, we validate that there is a sweet spot between expedition and distillation where the partial view from the expedited online image encoder interacts complementarily with the momentum teacher. As a result, ECLIPSE outperforms its counterparts while achieving substantial acceleration in inference speed",
    "checked": true,
    "id": "8a0bc3ea8dbf6016e8967cab80b3fce191d019f8",
    "semantic_title": "expediting contrastive language-image pretraining via self-distilled encoders",
    "citation_count": 0,
    "authors": [
      "Bumsoo Kim",
      "Jinhyung Kim",
      "Yeonsik Jo",
      "Seung Hwan Kim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28053": {
    "title": "Weakly Supervised Semantic Segmentation for Driving Scenes",
    "volume": "main",
    "abstract": "State-of-the-art techniques in weakly-supervised semantic segmentation (WSSS) using image-level labels exhibit severe performance degradation on driving scene datasets such as Cityscapes. To address this challenge, we develop a new WSSS framework tailored to driving scene datasets. Based on extensive analysis of dataset characteristics, we employ Contrastive Language-Image Pre-training (CLIP) as our baseline to obtain pseudo-masks. However, CLIP introduces two key challenges: (1) pseudo-masks from CLIP lack in representing small object classes, and (2) these masks contain notable noise. We propose solutions for each issue as follows. (1) We devise Global-Local View Training that seamlessly incorporates small-scale patches during model training, thereby enhancing the model's capability to handle small-sized yet critical objects in driving scenes (e.g., traffic light). (2) We introduce Consistency-Aware Region Balancing (CARB), a novel technique that discerns reliable and noisy regions through evaluating the consistency between CLIP masks and segmentation predictions. It prioritizes reliable pixels over noisy pixels via adaptive loss weighting. Notably, the proposed method achieves 51.8\\% mIoU on the Cityscapes test dataset, showcasing its potential as a strong WSSS baseline on driving scene datasets. Experimental results on CamVid and WildDash2 demonstrate the effectiveness of our method across diverse datasets, even with small-scale datasets or visually challenging conditions. The code is available at https://github.com/k0u-id/CARB",
    "checked": true,
    "id": "a0d46c8ceefdc2ce23438b7b5d8ece8899adafca",
    "semantic_title": "weakly supervised semantic segmentation for driving scenes",
    "citation_count": 1,
    "authors": [
      "Dongseob Kim",
      "Seungho Lee",
      "Junsuk Choe",
      "Hyunjung Shim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28054": {
    "title": "FPRF: Feed-Forward Photorealistic Style Transfer of Large-Scale 3D Neural Radiance Fields",
    "volume": "main",
    "abstract": "We present FPRF, a feed-forward photorealistic style transfer method for large-scale 3D neural radiance fields. FPRF stylizes large-scale 3D scenes with arbitrary, multiple style reference images without additional optimization while preserving multi-view appearance consistency. Prior arts required tedious per-style/-scene optimization and were limited to small-scale 3D scenes. FPRF efficiently stylizes large-scale 3D scenes by introducing a style-decomposed 3D neural radiance field, which inherits AdaIN's feed-forward stylization machinery, supporting arbitrary style reference images. Furthermore, FPRF supports multi-reference stylization with the semantic correspondence matching and local AdaIN, which adds diverse user control for 3D scene styles. FPRF also preserves multi-view consistency by applying semantic matching and style transfer processes directly onto queried features in 3D space. In experiments, we demonstrate that FPRF achieves favorable photorealistic quality 3D scene stylization for large-scale scenes with diverse reference images",
    "checked": true,
    "id": "a53e6d23d131f7f7e4747626ba23482652409f88",
    "semantic_title": "fprf: feed-forward photorealistic style transfer of large-scale 3d neural radiance fields",
    "citation_count": 1,
    "authors": [
      "GeonU Kim",
      "Kim Youwang",
      "Tae-Hyun Oh"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28055": {
    "title": "Let There Be Sound: Reconstructing High Quality Speech from Silent Videos",
    "volume": "main",
    "abstract": "The goal of this work is to reconstruct high quality speech from lip motions alone, a task also known as lip-to-speech. A key challenge of lip-to-speech systems is the one-to-many mapping caused by (1) the existence of homophenes and (2) multiple speech variations, resulting in a mispronounced and over-smoothed speech. In this paper, we propose a novel lip-to-speech system that significantly improves the generation quality by alleviating the one-to-many mapping problem from multiple perspectives. Specifically, we incorporate (1) self-supervised speech representations to disambiguate homophenes, and (2) acoustic variance information to model diverse speech styles. Additionally, to better solve the aforementioned problem, we employ a flow based post-net which captures and refines the details of the generated speech. We perform extensive experiments on two datasets, and demonstrate that our method achieves the generation quality close to that of real human utterance, outperforming existing methods in terms of speech naturalness and intelligibility by a large margin. Synthesised samples are available at our demo page: https://mm.kaist.ac.kr/projects/LTBS",
    "checked": true,
    "id": "330ab71d1b9bcdf37c5688bff6af51278c56d246",
    "semantic_title": "let there be sound: reconstructing high quality speech from silent videos",
    "citation_count": 0,
    "authors": [
      "Ji-Hoon Kim",
      "Jaehun Kim",
      "Joon Son Chung"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28056": {
    "title": "Expand-and-Quantize: Unsupervised Semantic Segmentation Using High-Dimensional Space and Product Quantization",
    "volume": "main",
    "abstract": "Unsupervised semantic segmentation (USS) aims to discover and recognize meaningful categories without any labels. For a successful USS, two key abilities are required: 1) information compression and 2) clustering capability. Previous methods have relied on feature dimension reduction for information compression, however, this approach may hinder the process of clustering. In this paper, we propose a novel USS framework called Expand-and-Quantize Unsupervised Semantic Segmentation (EQUSS), which combines the benefits of high-dimensional spaces for better clustering and product quantization for effective information compression. Our extensive experiments demonstrate that EQUSS achieves state-of-the-art results on three standard benchmarks. In addition, we analyze the entropy of USS features, which is the first step towards understanding USS from the perspective of information theory",
    "checked": true,
    "id": "467227272889a8307c83a1b2ffd1daad6d8d8bfa",
    "semantic_title": "expand-and-quantize: unsupervised semantic segmentation using high-dimensional space and product quantization",
    "citation_count": 0,
    "authors": [
      "Jiyoung Kim",
      "Kyuhong Shim",
      "Insu Lee",
      "Byonghyo Shim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28057": {
    "title": "Sync-NeRF: Generalizing Dynamic NeRFs to Unsynchronized Videos",
    "volume": "main",
    "abstract": "Recent advancements in 4D scene reconstruction using neural radiance fields (NeRF) have demonstrated the ability to represent dynamic scenes from multi-view videos. However, they fail to reconstruct the dynamic scenes and struggle to fit even the training views in unsynchronized settings. It happens because they employ a single latent embedding for a frame while the multi-view images at the same frame were actually captured at different moments. To address this limitation, we introduce time offsets for individual unsynchronized videos and jointly optimize the offsets with NeRF. By design, our method is applicable for various baselines and improves them with large margins. Furthermore, finding the offsets always works as synchronizing the videos without manual effort. Experiments are conducted on the common Plenoptic Video Dataset and a newly built Unsynchronized Dynamic Blender Dataset to verify the performance of our method. Project page: https://seoha-kim.github.io/sync-nerf",
    "checked": true,
    "id": "f4f11c99d253519379796927faef6adcd8acbfe4",
    "semantic_title": "sync-nerf: generalizing dynamic nerfs to unsynchronized videos",
    "citation_count": 2,
    "authors": [
      "Seoha Kim",
      "Jeongmin Bae",
      "Youngsik Yun",
      "Hahyun Lee",
      "Gun Bang",
      "Youngjung Uh"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28058": {
    "title": "Improving Open Set Recognition via Visual Prompts Distilled from Common-Sense Knowledge",
    "volume": "main",
    "abstract": "Open Set Recognition (OSR) poses significant challenges in distinguishing known from unknown classes. In OSR, the overconfidence problem has become a persistent obstacle, where visual recognition models often misclassify unknown objects as known objects with high confidence. This issue stems from the fact that visual recognition models often lack the integration of common-sense knowledge, a feature that is naturally present in language-based models but lacking in visual recognition systems. In this paper, we propose a novel approach to enhance OSR performance by distilling common-sense knowledge into visual prompts. Utilizing text prompts that embody common-sense knowledge about known classes, the proposed visual prompt is learned by extracting semantic common-sense features and aligning them with image features from visual recognition models. The unique aspect of this work is the training of individual visual prompts for each class to encapsulate this common-sense knowledge. Our methodology is model-agnostic, capable of enhancing OSR across various visual recognition models, and computationally light as it focuses solely on training the visual prompts. This research introduces a method for addressing OSR, aiming at a more systematic integration of visual recognition systems with common-sense knowledge. The obtained results indicate an enhancement in recognition accuracy, suggesting the applicability of this approach in practical settings",
    "checked": true,
    "id": "880f8154979949194fa3d1a98065832944aeeb80",
    "semantic_title": "improving open set recognition via visual prompts distilled from common-sense knowledge",
    "citation_count": 2,
    "authors": [
      "Seongyeop Kim",
      "Hyung-Il Kim",
      "Yong Man Ro"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28059": {
    "title": "Gaussian Mixture Proposals with Pull-Push Learning Scheme to Capture Diverse Events for Weakly Supervised Temporal Video Grounding",
    "volume": "main",
    "abstract": "In the weakly supervised temporal video grounding study, previous methods use predetermined single Gaussian proposals which lack the ability to express diverse events described by the sentence query. To enhance the expression ability of a proposal, we propose a Gaussian mixture proposal (GMP) that can depict arbitrary shapes by learning importance, centroid, and range of every Gaussian in the mixture. In learning GMP, each Gaussian is not trained in a feature space but is implemented over a temporal location. Thus the conventional feature-based learning for Gaussian mixture model is not valid for our case. In our special setting, to learn moderately coupled Gaussian mixture capturing diverse events, we newly propose a pull-push learning scheme using pulling and pushing losses, each of which plays an opposite role to the other. The effects of components in our scheme are verified in-depth with extensive ablation studies and the overall scheme achieves state-of-the-art performance. Our code is available at https://github.com/sunoh-kim/pps",
    "checked": true,
    "id": "4ae34510bc501ce418743fda52b0d61f480e8f7a",
    "semantic_title": "gaussian mixture proposals with pull-push learning scheme to capture diverse events for weakly supervised temporal video grounding",
    "citation_count": 0,
    "authors": [
      "Sunoh Kim",
      "Jungchan Cho",
      "Joonsang Yu",
      "YoungJoon Yoo",
      "Jin Young Choi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28060": {
    "title": "PARSAC: Accelerating Robust Multi-Model Fitting with Parallel Sample Consensus",
    "volume": "main",
    "abstract": "We present a real-time method for robust estimation of multiple instances of geometric models from noisy data. Geometric models such as vanishing points, planar homographies or fundamental matrices are essential for 3D scene analysis. Previous approaches discover distinct model instances in an iterative manner, thus limiting their potential for speedup via parallel computation. In contrast, our method detects all model instances independently and in parallel. A neural network segments the input data into clusters representing potential model instances by predicting multiple sets of sample and inlier weights. Using the predicted weights, we determine the model parameters for each potential instance separately in a RANSAC-like fashion. We train the neural network via task-specific loss functions, i.e. we do not require a ground-truth segmentation of the input data. As suitable training data for homography and fundamental matrix fitting is scarce, we additionally present two new synthetic datasets. We demonstrate state-of-the-art performance on these as well as multiple established datasets, with inference times as small as five milliseconds per image",
    "checked": true,
    "id": "f8c34f3fb9719ce3e9a71490884a02aa9f5da828",
    "semantic_title": "parsac: accelerating robust multi-model fitting with parallel sample consensus",
    "citation_count": 4,
    "authors": [
      "Florian Kluger",
      "Bodo Rosenhahn"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28061": {
    "title": "Distribution Matching for Multi-Task Learning of Classification Tasks: A Large-Scale Study on Faces & Beyond",
    "volume": "main",
    "abstract": "Multi-Task Learning (MTL) is a framework, where multiple related tasks are learned jointly and benefit from a shared representation space, or parameter transfer. To provide sufficient learning support, modern MTL uses annotated data with full, or sufficiently large overlap across tasks, i.e., each input sample is annotated for all, or most of the tasks. However, collecting such annotations is prohibitive in many real applications, and cannot benefit from datasets available for individual tasks. In this work, we challenge this setup and show that MTL can be successful with classification tasks with little, or non-overlapping annotations, or when there is big discrepancy in the size of labeled data per task. We explore task-relatedness for co-annotation and co-training, and propose a novel approach, where knowledge exchange is enabled between the tasks via distribution matching. To demonstrate the general applicability of our method, we conducted diverse case studies in the domains of affective computing, face recognition, species recognition, and shopping item classification using nine datasets. Our large-scale study of affective tasks for basic expression recognition and facial action unit detection illustrates that our approach is network agnostic and brings large performance improvements compared to the state-of-the-art in both tasks and across all studied databases. In all case studies, we show that co-training via task-relatedness is advantageous and prevents negative transfer (which occurs when MT model's performance is worse than that of at least one single-task model)",
    "checked": true,
    "id": "16d26f3adaf77d477b60b96081c488ae26abb730",
    "semantic_title": "distribution matching for multi-task learning of classification tasks: a large-scale study on faces & beyond",
    "citation_count": 11,
    "authors": [
      "Dimitrios Kollias",
      "Viktoriia Sharmanska",
      "Stefanos Zafeiriou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28062": {
    "title": "Block Image Compressive Sensing with Local and Global Information Interaction",
    "volume": "main",
    "abstract": "Block image compressive sensing methods, which divide a single image into small blocks for efficient sampling and reconstruction, have achieved significant success. However, these methods process each block locally and thus disregard the global communication among different blocks in the reconstruction step. Existing methods have attempted to address this issue with local filters or by directly reconstructing the entire image, but they have only achieved insufficient communication among adjacent pixels or bypassed the problem. To directly confront the communication problem among blocks and effectively resolve it, we propose a novel approach called Block Reconstruction with Blocks' Communication Network (BRBCN). BRBCN focuses on both local and global information, while further taking their interactions into account. Specifically, BRBCN comprises dual CNN and Transformer architectures, in which CNN is used to reconstruct each block for powerful local processing and Transformer is used to calculate the global communication among all the blocks. Moreover, we propose a global-to-local module (G2L) and a local-to-global module (L2G) to effectively integrate the representations of CNN and Transformer, with which our BRBCN network realizes the bidirectional interaction between local and global information. Extensive experiments show our BRBCN method outperforms existing state-of-the-art methods by a large margin. The code is available at https://github.com/kongxiuxiu/BRBCN",
    "checked": true,
    "id": "aa1ac8cf02a273798f6307e79140dcebd2a22539",
    "semantic_title": "block image compressive sensing with local and global information interaction",
    "citation_count": 0,
    "authors": [
      "Xiaoyu Kong",
      "Yongyong Chen",
      "Feng Zheng",
      "Zhenyu He"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28063": {
    "title": "QDETRv: Query-Guided DETR for One-Shot Object Localization in Videos",
    "volume": "main",
    "abstract": "In this work, we study one-shot video object localization problem that aims to localize instances of unseen objects in the target video using a single query image of the object. Toward addressing this challenging problem, we extend a popular and successful object detection method, namely DETR (Detection Transformer), and introduce a novel approach –query-guided detection transformer for videos (QDETRv). A distinctive feature of QDETRv is its capacity to exploit information from the query image and spatio-temporal context of the target video, which significantly aids in precisely pinpointing the desired object in the video. We incorporate cross-attention mechanisms that capture temporal relationships across adjacent frames to handle the dynamic context in videos effectively. Further, to ensure strong initialization for QDETRv, we also introduce a novel unsupervised pretraining technique tailored to videos. This involves training our model on synthetic object trajectories with an analogous objective as the query-guided localization task. During this pretraining phase, we incorporate recurrent object queries and loss functions that encourage accurate patch feature reconstruction. These additions enable better temporal understanding and robust representation learning. Our experiments show that the proposed model significantly outperforms the competitive baselines on two public benchmarks, VidOR and ImageNet-VidVRD, extended for one-shot open-set localization tasks",
    "checked": true,
    "id": "d072dfeb79b5667d4ddfd5171733d7e309e890c5",
    "semantic_title": "qdetrv: query-guided detr for one-shot object localization in videos",
    "citation_count": 0,
    "authors": [
      "Yogesh Kumar",
      "Saswat Mallick",
      "Anand Mishra",
      "Sowmya Rasipuram",
      "Anutosh Maitra",
      "Roshni Ramnani"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28064": {
    "title": "LaViP: Language-Grounded Visual Prompting",
    "volume": "main",
    "abstract": "We introduce a language-grounded visual prompting method to adapt the visual encoder of vision-language models for downstream tasks. By capitalizing on language integration, we devise a parameter-efficient strategy to adjust the input of the visual encoder, eliminating the need to modify or add to the model's parameters. Due to this design choice, our algorithm can operate even in black-box scenarios, showcasing adaptability in situations where access to the model's parameters is constrained. We will empirically demonstrate that, compared to prior art, grounding visual prompts with language enhances both the accuracy and speed of adaptation. Moreover, our algorithm excels in base-to-novel class generalization, overcoming limitations of visual prompting and exhibiting the capacity to generalize beyond seen classes. We thoroughly assess and evaluate our method across a variety of image recognition datasets, such as EuroSAT, UCF101, DTD, and CLEVR, spanning different learning situations, including few-shot adaptation, base-to-novel class generalization, and transfer learning",
    "checked": true,
    "id": "d808742a12873e428f94a35ee0cc4920e21f848f",
    "semantic_title": "lavip: language-grounded visual prompting",
    "citation_count": 0,
    "authors": [
      "Nilakshan Kunananthaseelan",
      "Jing Zhang",
      "Mehrtash Harandi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28065": {
    "title": "Towards More Faithful Natural Language Explanation Using Multi-Level Contrastive Learning in VQA",
    "volume": "main",
    "abstract": "Natural language explanation in visual question answer (VQA-NLE) aims to explain the decision-making process of models by generating natural language sentences to increase users' trust in the black-box systems. Existing post-hoc methods have achieved significant progress in obtaining a plausible explanation. However, such post-hoc explanations are not always aligned with human logical inference, suffering from the issues on: 1) Deductive unsatisfiability, the generated explanations do not logically lead to the answer; 2) Factual inconsistency, the model falsifies its counterfactual explanation for answers without considering the facts in images; and 3) Semantic perturbation insensitivity, the model can not recognize the semantic changes caused by small perturbations. These problems reduce the faithfulness of explanations generated by models. To address the above issues, we propose a novel self-supervised Multi-level Contrastive Learning based natural language Explanation model (MCLE) for VQA with semantic-level, image-level, and instance-level factual and counterfactual samples. MCLE extracts discriminative features and aligns the feature spaces from explanations with visual question and answer to generate more consistent explanations. We conduct extensive experiments, ablation analysis, and case study to demonstrate the effectiveness of our method on two VQA-NLE benchmarks",
    "checked": true,
    "id": "d9f38c99fdf74ab2d3918ca10bdbcd2cb9fbbfd2",
    "semantic_title": "towards more faithful natural language explanation using multi-level contrastive learning in vqa",
    "citation_count": 2,
    "authors": [
      "Chengen Lai",
      "Shengli  Song",
      "Shiqi Meng",
      "Jingyang Li",
      "Sitong Yan",
      "Guangneng Hu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28066": {
    "title": "MatchDet: A Collaborative Framework for Image Matching and Object Detection",
    "volume": "main",
    "abstract": "Image matching and object detection are two fundamental and challenging tasks, while many related applications consider them two individual tasks (i.e. task-individual). In this paper, a collaborative framework called MatchDet (i.e. task-collaborative) is proposed for image matching and object detection to obtain mutual improvements. To achieve the collaborative learning of the two tasks, we propose three novel modules, including a Weighted Spatial Attention Module (WSAM) for Detector, and Weighted Attention Module (WAM) and Box Filter for Matcher. Specifically, the WSAM highlights the foreground regions of target image to benefit the subsequent detector, the WAM enhances the connection between the foreground regions of pair images to ensure high-quality matches, and Box Filter mitigates the impact of false matches. We evaluate the approaches on a new benchmark with two datasets called Warp-COCO and miniScanNet. Experimental results show our approaches are effective and achieve competitive improvements",
    "checked": true,
    "id": "c965465163735fe082c9d8b5740ac3291c06a587",
    "semantic_title": "matchdet: a collaborative framework for image matching and object detection",
    "citation_count": 0,
    "authors": [
      "Jinxiang Lai",
      "Wenlong Wu",
      "Bin-Bin Gao",
      "Jun Liu",
      "Jiawei Zhan",
      "Congchong Nie",
      "Yi Zeng",
      "Chengjie Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28067": {
    "title": "ViTree: Single-Path Neural Tree for Step-Wise Interpretable Fine-Grained Visual Categorization",
    "volume": "main",
    "abstract": "As computer vision continues to advance and finds widespread applications across various domains, the need for interpretability in deep learning models becomes paramount. Existing methods often resort to post-hoc techniques or prototypes to explain the decision-making process, which can be indirect and lack intrinsic illustration. In this research, we introduce ViTree, a novel approach for fine-grained visual categorization that combines the popular vision transformer as a feature extraction backbone with neural decision trees. By traversing the tree paths, ViTree effectively selects patches from transformer-processed features to highlight informative local regions, thereby refining representations in a step-wise manner. Unlike previous tree-based models that rely on soft distributions or ensembles of paths, ViTree selects a single tree path, offering a clearer and simpler decision-making process. This patch and path selectivity enhances model interpretability of ViTree, enabling better insights into the model's inner workings. Remarkably, extensive experimentation validates that this streamlined approach surpasses various strong competitors and achieves state-of-the-art performance while maintaining exceptional interpretability which is proved by multi-perspective methods. Code can be found at https://github.com/SJTU-DeepVisionLab/ViTree",
    "checked": true,
    "id": "776cf9685aef060c7a36fdd13e35f3610dcbd5f4",
    "semantic_title": "vitree: single-path neural tree for step-wise interpretable fine-grained visual categorization",
    "citation_count": 0,
    "authors": [
      "Danning Lao",
      "Qi Liu",
      "Jiazi Bu",
      "Junchi Yan",
      "Wei Shen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28068": {
    "title": "MaskDiff: Modeling Mask Distribution with Diffusion Probabilistic Model for Few-Shot Instance Segmentation",
    "volume": "main",
    "abstract": "Few-shot instance segmentation extends the few-shot learning paradigm to the instance segmentation task, which tries to segment instance objects from a query image with a few annotated examples of novel categories. Conventional approaches have attempted to address the task via prototype learning, known as point estimation. However, this mechanism depends on prototypes (e.g. mean of K-shot) for prediction, leading to performance instability. To overcome the disadvantage of the point estimation mechanism, we propose a novel approach, dubbed MaskDiff, which models the underlying conditional distribution of a binary mask, which is conditioned on an object region and K-shot information. Inspired by augmentation approaches that perturb data with Gaussian noise for populating low data density regions, we model the mask distribution with a diffusion probabilistic model. We also propose to utilize classifier-free guided mask sampling to integrate category information into the binary mask generation process. Without bells and whistles, our proposed method consistently outperforms state-of-the-art methods on both base and novel classes of the COCO dataset while simultaneously being more stable than existing methods. The source code is available at: https://github.com/minhquanlecs/MaskDiff",
    "checked": true,
    "id": "a86846ad0893595896c36f465cf4f7ce07945247",
    "semantic_title": "maskdiff: modeling mask distribution with diffusion probabilistic model for few-shot instance segmentation",
    "citation_count": 6,
    "authors": [
      "Minh-Quan Le",
      "Tam V. Nguyen",
      "Trung-Nghia Le",
      "Thanh-Toan Do",
      "Minh N. Do",
      "Minh-Triet Tran"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28069": {
    "title": "FRED: Towards a Full Rotation-Equivariance in Aerial Image Object Detection",
    "volume": "main",
    "abstract": "Rotation-equivariance is an essential yet challenging property in oriented object detection. While general object detectors naturally leverage robustness to spatial shifts due to the translation-equivariance of the conventional CNNs, achieving rotation-equivariance remains an elusive goal. Current detectors deploy various alignment techniques to derive rotation-invariant features, but still rely on high capacity models and heavy data augmentation with all possible rotations. In this paper, we introduce a Fully Rotation-Equivariant Oriented Object Detector (FRED), whose entire process from the image to the bounding box prediction is strictly equivariant. Specifically, we decouple the invariant task (object classification) and the equivariant task (object localization) to achieve end-to-end equivariance. We represent the bounding box as a set of rotation-equivariant vectors to implement rotation-equivariant localization. Moreover, we utilized these rotation-equivariant vectors as offsets in the deformable convolution, thereby enhancing the existing advantages of spatial adaptation. Leveraging full rotation-equivariance, our FRED demonstrates higher robustness to image-level rotation compared to existing methods. Furthermore, we show that FRED is one step closer to non-axis aligned learning through our experiments. Compared to state-of-the-art methods, our proposed method delivers comparable performance on DOTA-v1.0 and outperforms by 1.5 mAP on DOTA-v1.5, all while significantly reducing the model parameters to 16%",
    "checked": true,
    "id": "84a04ed8f34145915d416ff61861615d088035da",
    "semantic_title": "fred: towards a full rotation-equivariance in aerial image object detection",
    "citation_count": 1,
    "authors": [
      "Chanho Lee",
      "Jinsu Son",
      "Hyounguk Shon",
      "Yunho Jeon",
      "Junmo Kim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28070": {
    "title": "Domain Generalization with Vital Phase Augmentation",
    "volume": "main",
    "abstract": "Deep neural networks have shown remarkable performance in image classification. However, their performance significantly deteriorates with corrupted input data. Domain generalization methods have been proposed to train robust models against out-of-distribution data. Data augmentation in the frequency domain is one of such approaches that enable a model to learn phase features to establish domain-invariant representations. This approach changes the amplitudes of the input data while preserving the phases. However, using fixed phases leads to susceptibility to phase fluctuations because amplitudes and phase fluctuations commonly occur in out-of-distribution. In this study, to address this problem, we introduce an approach using finite variation of the phases of input data rather than maintaining fixed phases. Based on the assumption that the degree of domain-invariant features varies for each phase, we propose a method to distinguish phases based on this degree. In addition, we propose a method called vital phase augmentation (VIPAug) that applies the variation to the phases differently according to the degree of domain-invariant features of given phases. The model depends more on the vital phases that contain more domain-invariant features for attaining robustness to amplitude and phase fluctuations. We present experimental evaluations of our proposed approach, which exhibited improved performance for both clean and corrupted data. VIPAug achieved SOTA performance on the benchmark CIFAR-10 and CIFAR-100 datasets, as well as near-SOTA performance on the ImageNet-100 and ImageNet datasets. Our code is available at https://github.com/excitedkid/vipaug",
    "checked": true,
    "id": "f75bd014fbe0894f2e5c024f6e67f7956a574517",
    "semantic_title": "domain generalization with vital phase augmentation",
    "citation_count": 0,
    "authors": [
      "Ingyun Lee",
      "Wooju Lee",
      "Hyun Myung"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28071": {
    "title": "Modeling Stereo-Confidence out of the End-to-End Stereo-Matching Network via Disparity Plane Sweep",
    "volume": "main",
    "abstract": "We propose a novel stereo-confidence that can be measured externally to various stereo-matching networks, offering an alternative input modality choice of the cost volume for learning-based approaches, especially in safety-critical systems. Grounded in the foundational concepts of disparity definition and the disparity plane sweep, the proposed stereo-confidence method is built upon the idea that any shift in a stereo-image pair should be updated in a corresponding amount shift in the disparity map. Based on this idea, the proposed stereo-confidence method can be summarized in three folds. 1) Using the disparity plane sweep, multiple disparity maps can be obtained and treated as a 3-D volume (predicted disparity volume), like the cost volume is constructed. 2) One of these disparity maps serves as an anchor, allowing us to define a desirable (or ideal) disparity profile at every spatial point. 3) By comparing the desirable and predicted disparity profiles, we can quantify the level of matching ambiguity between left and right images for confidence measurement. Extensive experimental results using various stereo-matching networks and datasets demonstrate that the proposed stereo-confidence method not only shows competitive performance on its own but also consistent performance improvements when it is used as an input modality for learning-based stereo-confidence methods",
    "checked": true,
    "id": "6bbb7d349aaa1e18f646853793c073f71cd3e69f",
    "semantic_title": "modeling stereo-confidence out of the end-to-end stereo-matching network via disparity plane sweep",
    "citation_count": 0,
    "authors": [
      "Jae Young Lee",
      "Woonghyun Ka",
      "Jaehyun Choi",
      "Junmo Kim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28072": {
    "title": "MFOS: Model-Free & One-Shot Object Pose Estimation",
    "volume": "main",
    "abstract": "Existing learning-based methods for object pose estimation in RGB images are mostly model-specific or category based. They lack the capability to generalize to new object categories at test time, hence severely hindering their practicability and scalability. Notably, recent attempts have been made to solve this issue, but they still require accurate 3D data of the object surface at both train and test time. In this paper, we introduce a novel approach that can estimate in a single forward pass the pose of objects never seen during training, given minimum input. In contrast to existing state-of-the-art approaches, which rely on task-specific modules, our proposed model is entirely based on a transformer architecture, which can benefit from recently proposed 3D-geometry general pretraining. We conduct extensive experiments and report state-of-the-art one-shot performance on the challenging LINEMOD benchmark. Finally, extensive ablations allow us to determine good practices with this relatively new type of architecture in the field",
    "checked": true,
    "id": "9147b582eb4a155a4de9b85baeafa7d07fa4f939",
    "semantic_title": "mfos: model-free & one-shot object pose estimation",
    "citation_count": 2,
    "authors": [
      "JongMin Lee",
      "Yohann Cabon",
      "Romain Brégier",
      "Sungjoo Yoo",
      "Jerome Revaud"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28073": {
    "title": "Noise-Free Optimization in Early Training Steps for Image Super-resolution",
    "volume": "main",
    "abstract": "Recent deep-learning-based single image super-resolution (SISR) methods have shown impressive performance whereas typical methods train their networks by minimizing the pixel-wise distance with respect to a given high-resolution (HR) image. However, despite the basic training scheme being the predominant choice, its use in the context of ill-posed inverse problems has not been thoroughly investigated. In this work, we aim to provide a better comprehension of the underlying constituent by decomposing target HR images into two subcomponents: (1) the optimal centroid which is the expectation over multiple potential HR images, and (2) the inherent noise defined as the residual between the HR image and the centroid. Our findings show that the current training scheme cannot capture the ill-posed nature of SISR and becomes vulnerable to the inherent noise term, especially during early training steps. To tackle this issue, we propose a novel optimization method that can effectively remove the inherent noise term in the early steps of vanilla training by estimating the optimal centroid and directly optimizing toward the estimation. Experimental results show that the proposed method can effectively enhance the stability of vanilla training, leading to overall performance gain. Codes are available at github.com/2minkyulee/ECO",
    "checked": true,
    "id": "ad1edc074309f785580d1ccc5edbec6645543813",
    "semantic_title": "noise-free optimization in early training steps for image super-resolution",
    "citation_count": 1,
    "authors": [
      "MinKyu Lee",
      "Jae-Pil Heo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28074": {
    "title": "Spectrum Translation for Refinement of Image Generation (STIG) Based on Contrastive Learning and Spectral Filter Profile",
    "volume": "main",
    "abstract": "Currently, image generation and synthesis have remarkably progressed with generative models. Despite photo-realistic results, intrinsic discrepancies are still observed in the frequency domain. The spectral discrepancy appeared not only in generative adversarial networks but in diffusion models. In this study, we propose a framework to effectively mitigate the disparity in frequency domain of the generated images to improve generative performance of both GAN and diffusion models. This is realized by spectrum translation for the refinement of image generation (STIG) based on contrastive learning. We adopt theoretical logic of frequency components in various generative networks. The key idea, here, is to refine the spectrum of the generated image via the concept of image-to-image translation and contrastive learning in terms of digital signal processing. We evaluate our framework across eight fake image datasets and various cutting-edge models to demonstrate the effectiveness of STIG. Our framework outperforms other cutting-edges showing significant decreases in FID and log frequency distance of spectrum. We further emphasize that STIG improves image quality by decreasing the spectral anomaly. Additionally, validation results present that the frequency-based deepfake detector confuses more in the case where fake spectrums are manipulated by STIG",
    "checked": true,
    "id": "8286b1a4eb8c70e24b121ed758e7306c2aa44288",
    "semantic_title": "spectrum translation for refinement of image generation (stig) based on contrastive learning and spectral filter profile",
    "citation_count": 2,
    "authors": [
      "Seokjun Lee",
      "Seung-Won Jung",
      "Hyunseok Seo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28075": {
    "title": "Few-Shot Neural Radiance Fields under Unconstrained Illumination",
    "volume": "main",
    "abstract": "In this paper, we introduce a new challenge for synthesizing novel view images in practical environments with limited input multi-view images and varying lighting conditions. Neural radiance fields (NeRF), one of the pioneering works for this task, demand an extensive set of multi-view images taken under constrained illumination, which is often unattainable in real-world settings. While some previous works have managed to synthesize novel views given images with different illumination, their performance still relies on a substantial number of input multi-view images. To address this problem, we suggest ExtremeNeRF, which utilizes multi-view albedo consistency, supported by geometric alignment. Specifically, we extract intrinsic image components that should be illumination-invariant across different views, enabling direct appearance comparison between the input and novel view under unconstrained illumination. We offer thorough experimental results for task evaluation, employing the newly created NeRF Extreme benchmark—the first in-the-wild benchmark for novel view synthesis under multiple viewing directions and varying illuminations",
    "checked": true,
    "id": "b50031ffe7c060fdd9c7fefe313d47b00c73097f",
    "semantic_title": "few-shot neural radiance fields under unconstrained illumination",
    "citation_count": 3,
    "authors": [
      "SeokYeong Lee",
      "JunYong Choi",
      "Seungryong Kim",
      "Ig-Jae Kim",
      "Junghyun Cho"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28076": {
    "title": "Object-Aware Domain Generalization for Object Detection",
    "volume": "main",
    "abstract": "Single-domain generalization (S-DG) aims to generalize a model to unseen environments with a single-source domain. However, most S-DG approaches have been conducted in the field of classification. When these approaches are applied to object detection, the semantic features of some objects can be damaged, which can lead to imprecise object localization and misclassification. To address these problems, we propose an object-aware domain generalization (OA-DG) method for single-domain generalization in object detection. Our method consists of data augmentation and training strategy, which are called OA-Mix and OA-Loss, respectively. OA-Mix generates multi-domain data with multi-level transformation and object-aware mixing strategy. OA-Loss enables models to learn domain-invariant representations for objects and backgrounds from the original and OA-Mixed images. Our proposed method outperforms state-of-the-art works on standard benchmarks. Our code is available at https://github.com/WoojuLee24/OA-DG",
    "checked": true,
    "id": "bf423c74fa7565aa4659263265b76e0657e4e750",
    "semantic_title": "object-aware domain generalization for object detection",
    "citation_count": 1,
    "authors": [
      "Wooju Lee",
      "Dasol Hong",
      "Hyungtae Lim",
      "Hyun Myung"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28077": {
    "title": "Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention",
    "volume": "main",
    "abstract": "Vision Transformer(ViT) is one of the most widely used models in the computer vision field with its great performance on various tasks. In order to fully utilize the ViT-based architecture in various applications, proper visualization methods with a decent localization performance are necessary, but these methods employed in CNN-based models are still not available in ViT due to its unique structure. In this work, we propose an attention-guided visualization method applied to ViT that provides a high-level semantic explanation for its decision. Our method selectively aggregates the gradients directly propagated from the classification output to each self-attention, collecting the contribution of image features extracted from each location of the input image. These gradients are additionally guided by the normalized self-attention scores, which are the pairwise patch correlation scores. They are used to supplement the gradients on the patch-level context information efficiently detected by the self-attention mechanism. This approach of our method provides elaborate high-level semantic explanations with great localization performance only with the class labels. As a result, our method outperforms the previous leading explainability methods of ViT in the weakly-supervised localization task and presents great capability in capturing the full instances of the target class object. Meanwhile, our method provides a visualization that faithfully explains the model, which is demonstrated in the perturbation comparison test",
    "checked": true,
    "id": "ffc855594cad345ea5a1cce2ee27095bec767bc8",
    "semantic_title": "attention guided cam: visual explanations of vision transformer guided by self-attention",
    "citation_count": 0,
    "authors": [
      "Saebom Leem",
      "Hyunseok Seo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28078": {
    "title": "Contrastive Tuning: A Little Help to Make Masked Autoencoders Forget",
    "volume": "main",
    "abstract": "Masked Image Modeling (MIM) methods, like Masked Autoencoders (MAE), efficiently learn a rich representation of the input. However, for adapting to downstream tasks, they require a sufficient amount of labeled data since their rich features code not only objects but also less relevant image background. In contrast, Instance Discrimination (ID) methods focus on objects. In this work, we study how to combine the efficiency and scalability of MIM with the ability of ID to perform downstream classification in the absence of large amounts of labeled data. To this end, we introduce Masked Autoencoder Contrastive Tuning (MAE-CT), a sequential approach that utilizes the implicit clustering of the Nearest Neighbor Contrastive Learning (NNCLR) objective to induce abstraction in the topmost layers of a pre-trained MAE. MAE-CT tunes the rich features such that they form semantic clusters of objects without using any labels. Notably, MAE-CT does not rely on hand-crafted augmentations and frequently achieves its best performances while using only minimal augmentations (crop & flip). Further, MAE-CT is compute efficient as it requires at most 10% overhead compared to MAE re-training. Applied to large and huge Vision Transformer (ViT) models, MAE-CT excels over previous self-supervised methods trained on ImageNet in linear probing, k-NN and low-shot classification accuracy as well as in unsupervised clustering accuracy. With ViT-H/16 MAE-CT achieves a new state-of-the-art in linear probing of 82.2%. Project page: github.com/ml-jku/MAE-CT",
    "checked": true,
    "id": "5e97c57cf3ff5034bc8bf6473c0cd0369fa64816",
    "semantic_title": "contrastive tuning: a little help to make masked autoencoders forget",
    "citation_count": 7,
    "authors": [
      "Johannes Lehner",
      "Benedikt Alkin",
      "Andreas Fürst",
      "Elisabeth Rumetshofer",
      "Lukas Miklautz",
      "Sepp Hochreiter"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28079": {
    "title": "Few-Shot Learning from Augmented Label-Uncertain Queries in Bongard-HOI",
    "volume": "main",
    "abstract": "Detecting human-object interactions (HOI) in a few-shot setting remains a challenge. Existing meta-learning methods struggle to extract representative features for classification due to the limited data, while existing few-shot HOI models rely on HOI text labels for classification. Moreover, some query images may display visual similarity to those outside their class, such as similar backgrounds between different HOI classes. This makes learning more challenging, especially with limited samples. Bongard-HOI epitomizes this HOI few-shot problem, making it the benchmark we focus on in this paper. In our proposed method, we introduce novel label-uncertain query augmentation techniques to enhance the diversity of the query inputs, aiming to distinguish the positive HOI class from the negative ones. As these augmented inputs may or may not have the same class label as the original inputs, their class label is unknown. Those belonging to a different class become hard samples due to their visual similarity to the original ones. Additionally, we introduce a novel pseudo-label generation technique that enables a mean teacher model to learn from the augmented label-uncertain inputs. We propose to augment the negative support set for the student model to enrich the semantic information, fostering diversity that challenges and enhances the student's learning. Experimental results demonstrate that our method sets a new state-of-the-art (SOTA) performance by achieving 68.74% accuracy on the Bongard-HOI benchmark, a significant improvement over the existing SOTA of 66.59%. In our evaluation on HICO-FS, a more general few-shot recognition dataset, our method achieves 73.27% accuracy, outperforming the previous SOTA of 71.20% in the 5- way 5-shot task",
    "checked": true,
    "id": "e66d66b0d3d6c05e63dabb578c9fb991c671992f",
    "semantic_title": "few-shot learning from augmented label-uncertain queries in bongard-hoi",
    "citation_count": 0,
    "authors": [
      "Qinqian Lei",
      "Bo Wang",
      "Robby T. Tan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28080": {
    "title": "Removing Interference and Recovering Content Imaginatively for Visible Watermark Removal",
    "volume": "main",
    "abstract": "Visible watermarks, while instrumental in protecting image copyrights, frequently distort the underlying content, complicating tasks like scene interpretation and image editing. Visible watermark removal aims to eliminate the interference of watermarks and restore the background content. However, existing methods often implement watermark component removal and background restoration tasks within a singular branch, leading to residual watermarks in the predictions and ignoring cases where watermarks heavily obscure the background. To address these limitations, this study introduces the Removing Interference and Recovering Content Imaginatively (RIRCI) framework. RIRCI embodies a two-stage approach: the initial phase centers on discerning and segregating the watermark component, while the subsequent phase focuses on background content restoration. To achieve meticulous background restoration, our proposed model employs a dual-path network capable of fully exploring the intrinsic background information beneath semi-transparent watermarks and peripheral contextual information from unaffected regions. Moreover, a Global and Local Context Interaction module is built upon multi-layer perceptrons and bidirectional feature transformation for comprehensive representation modeling in the background restoration phase. The efficacy of our approach is empirically validated across two large-scale datasets, and our findings reveal a marked enhancement over existing watermark removal techniques",
    "checked": true,
    "id": "3bbf488b113db65996e65dda9d29ca686daf637d",
    "semantic_title": "removing interference and recovering content imaginatively for visible watermark removal",
    "citation_count": 0,
    "authors": [
      "Yicheng Leng",
      "Chaowei Fang",
      "Gen Li",
      "Yixiang Fang",
      "Guanbin Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28081": {
    "title": "Data Roaming and Quality Assessment for Composed Image Retrieval",
    "volume": "main",
    "abstract": "The task of Composed Image Retrieval (CoIR) involves queries that combine image and text modalities, allowing users to express their intent more effectively. However, current CoIR datasets are orders of magnitude smaller compared to other vision and language (V&L) datasets. Additionally, some of these datasets have noticeable issues, such as queries containing redundant modalities. To address these shortcomings, we introduce the Large Scale Composed Image Retrieval (LaSCo) dataset, a new CoIR dataset which is ten times larger than existing ones. Pre-training on our LaSCo, shows a noteworthy improvement in performance, even in zero-shot. Furthermore, we propose a new approach for analyzing CoIR datasets and methods, which detects modality redundancy or necessity, in queries. We also introduce a new CoIR baseline, the Cross-Attention driven Shift Encoder (CASE). This baseline allows for early fusion of modalities using a cross-attention module and employs an additional auxiliary task during training. Our experiments demonstrate that this new baseline outperforms the current state-of-the-art methods on established benchmarks like FashionIQ and CIRR",
    "checked": true,
    "id": "1c205939f60ab42079f63738fa755f2eda026a03",
    "semantic_title": "data roaming and quality assessment for composed image retrieval",
    "citation_count": 9,
    "authors": [
      "Matan Levy",
      "Rami Ben-Ari",
      "Nir Darshan",
      "Dani Lischinski"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28082": {
    "title": "Point Transformer with Federated Learning for Predicting Breast Cancer HER2 Status from Hematoxylin and Eosin-Stained Whole Slide Images",
    "volume": "main",
    "abstract": "Directly predicting human epidermal growth factor receptor 2 (HER2) status from widely available hematoxylin and eosin (HE)-stained whole slide images (WSIs) can reduce technical costs and expedite treatment selection. Accurately predicting HER2 requires large collections of multi-site WSIs. Federated learning enables collaborative training of these WSIs without gigabyte-size WSIs transportation and data privacy concerns. However, federated learning encounters challenges in addressing label imbalance in multi-site WSIs from the real world. Moreover, existing WSI classification methods cannot simultaneously exploit local context information and long-range dependencies in the site-end feature representation of federated learning. To address these issues, we present a point transformer with federated learning for multi-site HER2 status prediction from HE-stained WSIs. Our approach incorporates two novel designs. We propose a dynamic label distribution strategy and an auxiliary classifier, which helps to establish a well-initialized model and mitigate label distribution variations across sites. Additionally, we propose a farthest cosine sampling based on cosine distance. It can sample the most distinctive features and capture the long-range dependencies. Extensive experiments and analysis show that our method achieves state-of-the-art performance at four sites with a total of 2687 WSIs. Furthermore, we demonstrate that our model can generalize to two unseen sites with 229 WSIs. Code is available at: https://github.com/boyden/PointTransformerFL",
    "checked": true,
    "id": "faf54324a75571bd8cd50e826b2d5480c608d583",
    "semantic_title": "point transformer with federated learning for predicting breast cancer her2 status from hematoxylin and eosin-stained whole slide images",
    "citation_count": 1,
    "authors": [
      "Bao Li",
      "Zhenyu Liu",
      "Lizhi Shao",
      "Bensheng Qiu",
      "Hong Bu",
      "Jie Tian"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28083": {
    "title": "Unsupervised Cross-Domain Image Retrieval via Prototypical Optimal Transport",
    "volume": "main",
    "abstract": "Unsupervised cross-domain image retrieval (UCIR) aims to retrieve images sharing the same category across diverse domains without relying on labeled data. Prior approaches have typically decomposed the UCIR problem into two distinct tasks: intra-domain representation learning and cross-domain feature alignment. However, these segregated strategies overlook the potential synergies between these tasks. This paper introduces ProtoOT, a novel Optimal Transport formulation explicitly tailored for UCIR, which integrates intra-domain feature representation learning and cross-domain alignment into a unified framework. ProtoOT leverages the strengths of the K-means clustering method to effectively manage distribution imbalances inherent in UCIR. By utilizing K-means for generating initial prototypes and approximating class marginal distributions, we modify the constraints in Optimal Transport accordingly, significantly enhancing its performance in UCIR scenarios. Furthermore, we incorporate contrastive learning into the ProtoOT framework to further improve representation learning. This encourages local semantic consistency among features with similar semantics, while also explicitly enforcing separation between features and unmatched prototypes, thereby enhancing global discriminativeness. ProtoOT surpasses existing state-of-the-art methods by a notable margin across benchmark datasets. Notably, on DomainNet, ProtoOT achieves an average P@200 enhancement of 24.44%, and on Office-Home, it demonstrates a P@15 improvement of 12.12%. Code is available at https://github.com/HCVLAB/ProtoOT",
    "checked": true,
    "id": "f9da402462d6845fca253e7d0f2c563cd8b0e483",
    "semantic_title": "unsupervised cross-domain image retrieval via prototypical optimal transport",
    "citation_count": 1,
    "authors": [
      "Bin Li",
      "Ye Shi",
      "Qian Yu",
      "Jingya Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28084": {
    "title": "Semantic-Guided Generative Image Augmentation Method with Diffusion Models for Image Classification",
    "volume": "main",
    "abstract": "Existing image augmentation methods consist of two categories: perturbation-based methods and generative methods. Perturbation-based methods apply pre-defined perturbations to augment an original image, but only locally vary the image, thus lacking image diversity. In contrast, generative methods bring more image diversity in the augmented images but may not preserve semantic consistency, thus may incorrectly change the essential semantics of the original image. To balance image diversity and semantic consistency in augmented images, we propose SGID, a Semantic-guided Generative Image augmentation method with Diffusion models for image classification. Specifically, SGID employs diffusion models to generate augmented images with good image diversity. More importantly, SGID takes image labels and captions as guidance to maintain semantic consistency between the augmented and original images. Experimental results show that SGID outperforms the best augmentation baseline by 1.72% on ResNet-50 (from scratch), 0.33% on ViT (ImageNet-21k), and 0.14% on CLIP-ViT (LAION-2B). Moreover, SGID can be combined with other image augmentation baselines and further improves the overall performance. We demonstrate the semantic consistency and image diversity of SGID through quantitative human and automated evaluations, as well as qualitative case studies",
    "checked": true,
    "id": "1e2c20d1c77aa2413cfaf362a77ec627462033f6",
    "semantic_title": "semantic-guided generative image augmentation method with diffusion models for image classification",
    "citation_count": 1,
    "authors": [
      "Bohan Li",
      "Xiao Xu",
      "Xinghao Wang",
      "Yutai Hou",
      "Yunlong Feng",
      "Feng Wang",
      "Xuanliang Zhang",
      "Qingfu Zhu",
      "Wanxiang Che"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28085": {
    "title": "One at a Time: Progressive Multi-Step Volumetric Probability Learning for Reliable 3D Scene Perception",
    "volume": "main",
    "abstract": "Numerous studies have investigated the pivotal role of reliable 3D volume representation in scene perception tasks, such as multi-view stereo (MVS) and semantic scene completion (SSC). They typically construct 3D probability volumes directly with geometric correspondence, attempting to fully address the scene perception tasks in a single forward pass. However, such a single-step solution makes it hard to learn accurate and convincing volumetric probability, especially in challenging regions like unexpected occlusions and complicated light reflections. Therefore, this paper proposes to decompose the complicated 3D volume representation learning into a sequence of generative steps to facilitate fine and reliable scene perception. Considering the recent advances achieved by strong generative diffusion models, we introduce a multi-step learning framework, dubbed as VPD, dedicated to progressively refining the Volumetric Probability in a Diffusion process. Specifically, we first build a coarse probability volume from input images with the off-the-shelf scene perception baselines, which is then conditioned as the basic geometry prior before being fed into a 3D diffusion UNet, to progressively achieve accurate probability distribution modeling. To handle the corner cases in challenging areas, a Confidence-Aware Contextual Collaboration (CACC) module is developed to correct the uncertain regions for reliable volumetric learning based on multi-scale contextual contents. Moreover, an Online Filtering (OF) strategy is designed to maintain representation consistency for stable diffusion sampling. Extensive experiments are conducted on scene perception tasks including multi-view stereo (MVS) and semantic scene completion (SSC), to validate the efficacy of our method in learning reliable volumetric representations. Notably, for the SSC task, our work stands out as the first to surpass LiDAR-based methods on the SemanticKITTI dataset",
    "checked": true,
    "id": "76cac517fdfb1b721265b38a7d90549258fe2339",
    "semantic_title": "one at a time: progressive multi-step volumetric probability learning for reliable 3d scene perception",
    "citation_count": 1,
    "authors": [
      "Bohan Li",
      "Yasheng Sun",
      "Jingxin Dong",
      "Zheng Zhu",
      "Jinming Liu",
      "Xin Jin",
      "Wenjun Zeng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28086": {
    "title": "AE-NeRF: Audio Enhanced Neural Radiance Field for Few Shot Talking Head Synthesis",
    "volume": "main",
    "abstract": "Audio-driven talking head synthesis is a promising topic with wide applications in digital human, film making and virtual reality. Recent NeRF-based approaches have shown superiority in quality and fidelity compared to previous studies. However, when it comes to few-shot talking head generation, a practical scenario where only few seconds of talking video is available for one identity, two limitations emerge: 1) they either have no base model, which serves as a facial prior for fast convergence, or ignore the importance of audio when building the prior; 2) most of them overlook the degree of correlation between different face regions and audio, e.g., mouth is audio related, while ear is audio independent. In this paper, we present Audio Enhanced Neural Radiance Field (AE-NeRF) to tackle the above issues, which can generate realistic portraits of a new speaker with few-shot dataset. Specifically, we introduce an Audio Aware Aggregation module into the feature fusion stage of the reference scheme, where the weight is determined by the similarity of audio between reference and target image. Then, an Audio-Aligned Face Generation strategy is proposed to model the audio related and audio independent regions respectively, with a dual-NeRF framework. Extensive experiments have shown AE-NeRF surpasses the state-of-the-art on image fidelity, audio-lip synchronization, and generalization ability, even in limited training set or training iterations",
    "checked": true,
    "id": "29f10d0e14e00e4749bb6003917035a66d44c746",
    "semantic_title": "ae-nerf: audio enhanced neural radiance field for few shot talking head synthesis",
    "citation_count": 2,
    "authors": [
      "Dongze Li",
      "Kang Zhao",
      "Wei Wang",
      "Bo Peng",
      "Yingya Zhang",
      "Jing Dong",
      "Tieniu Tan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28087": {
    "title": "Monocular 3D Hand Mesh Recovery via Dual Noise Estimation",
    "volume": "main",
    "abstract": "Current parametric models have made notable progress in 3D hand pose and shape estimation. However, due to the fixed hand topology and complex hand poses, current models are hard to generate meshes that are aligned with the image well. To tackle this issue, we introduce a dual noise estimation method in this paper. Given a single-view image as input, we first adopt a baseline parametric regressor to obtain the coarse hand meshes. We assume the mesh vertices and their image-plane projections are noisy, and can be associated in a unified probabilistic model. We then learn the distributions of noise to refine mesh vertices and their projections. The refined vertices are further utilized to refine camera parameters in a closed-form manner. Consequently, our method obtains well-aligned and high-quality 3D hand meshes. Extensive experiments on the large-scale Interhand2.6M dataset demonstrate that the proposed method not only improves the performance of its baseline by more than 10% but also achieves state-of-the-art performance. Project page: https://github.com/hanhuili/DNE4Hand",
    "checked": true,
    "id": "83f618c9633a1fefcf0e6fdea3f1a01e8ace02c2",
    "semantic_title": "monocular 3d hand mesh recovery via dual noise estimation",
    "citation_count": 0,
    "authors": [
      "Hanhui Li",
      "Xiaojian Lin",
      "Xuan Huang",
      "Zejun Yang",
      "Zhisheng Wang",
      "Xiaodan Liang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28088": {
    "title": "Point2Real: Bridging the Gap between Point Cloud and Realistic Image for Open-World 3D Recognition",
    "volume": "main",
    "abstract": "Recognition in open-world scenarios is an important and challenging field, where Vision-Language Pre-training paradigms have greatly impacted the 2D domain. This inspires a growing interest in introducing 2D pre-trained models, such as CLIP, into the 3D domain to enhance the ability of point cloud understanding. Considering the difference between discrete 3D point clouds and real-world 2D images, reducing the domain gap is crucial. Some recent works project point clouds onto a 2D plane to enable 3D zero-shot capabilities without training. However, this simplistic approach leads to an unclear or even distorted geometric structure, limiting the potential of 2D pre-trained models in 3D. To address the domain gap, we propose Point2Real, a training-free framework based on the realistic rendering technique to automate the transformation of the 3D point cloud domain into the Vision-Language domain. Specifically, Point2Real leverages a shape recovery module that devises an iterative ball-pivoting algorithm to convert point clouds into meshes, narrowing the gap in shape at first. To simulate photo-realistic images, a set of refined textures as candidates is applied for rendering, where the CLIP confidence is utilized to select the suitable one. Moreover, to tackle the viewpoint challenge, a heuristic multi-view adapter is implemented for feature aggregation, which exploits the depth surface as an effective indicator of view-specific discriminability for recognition. We conduct experiments on ModelNet10, ModelNet40, and ScanObjectNN datasets, and the results demonstrate that Point2Real outperforms other approaches in zero-shot and few-shot tasks by a large margin",
    "checked": true,
    "id": "f26f2c4d136cc802a6a156610deededd07f638ce",
    "semantic_title": "point2real: bridging the gap between point cloud and realistic image for open-world 3d recognition",
    "citation_count": 0,
    "authors": [
      "Hanxuan Li",
      "Bin Fu",
      "Ruiping Wang",
      "Xilin Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28089": {
    "title": "Gradual Residuals Alignment: A Dual-Stream Framework for GAN Inversion and Image Attribute Editing",
    "volume": "main",
    "abstract": "GAN-based image attribute editing firstly leverages GAN Inversion to project real images into the latent space of GAN and then manipulates corresponding latent codes. Recent inversion methods mainly utilize additional high-bit features to improve image details preservation, as low-bit codes cannot faithfully reconstruct source images, leading to the loss of details. However, during editing, existing works fail to accurately complement the lost details and suffer from poor editability. The main reason is they inject all the lost details indiscriminately at one time, which inherently induces the position and quantity of details to overfit source images, resulting in inconsistent content and artifacts in edited images. This work argues that details should be gradually injected into both the reconstruction and editing process in a multi-stage coarse-to-fine manner for better detail preservation and high editability. Therefore, a novel dual-stream framework is proposed to accurately complement details at each stage. The Reconstruction Stream is employed to embed coarse-to-fine lost details into residual features and then adaptively add them to the GAN generator. In the Editing Stream, residual features are accurately aligned by our Selective Attention mechanism and then injected into the editing process in a multi-stage manner. Extensive experiments have shown the superiority of our framework in both reconstruction accuracy and editing quality compared with existing methods",
    "checked": true,
    "id": "e950bff27ab92c14b4b12d6d99a765498e70b0ac",
    "semantic_title": "gradual residuals alignment: a dual-stream framework for gan inversion and image attribute editing",
    "citation_count": 1,
    "authors": [
      "Hao Li",
      "Mengqi Huang",
      "Lei Zhang",
      "Bo Hu",
      "Yi Liu",
      "Zhendong Mao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28090": {
    "title": "Towards Automated Chinese Ancient Character Restoration: A Diffusion-Based Method with a New Dataset",
    "volume": "main",
    "abstract": "Automated Chinese ancient character restoration (ACACR) remains a challenging task due to its historical significance and aesthetic complexity. Existing methods are constrained by non-professional masks and even overfitting when training on small-scale datasets, which hinder their interdisciplinary application to traditional fields. In this paper, we are proud to introduce the Chinese Ancient Rubbing and Manuscript Character Dataset (ARMCD), which consists of 15,553 real-world ancient single-character images with 42 rubbings and manuscripts, covering the works of over 200 calligraphy artists spanning from 200 to 1,800 AD. We are also dedicated to providing professional synthetic masks by extracting localized erosion from real eroded images. Moreover, we propose DiffACR (Diffusion model for automated Chinese Ancient Character Restoration), a diffusion-based method for the ACACR task. Specifically, we regard the synthesis of eroded images as a special form of cold diffusion on uneroded ones and extract the prior mask directly from the eroded images. Our experiments demonstrate that our method comprehensively outperforms most existing methods on the proposed ARMCD. Dataset and code are available at https://github.com/lhl322001/DiffACR",
    "checked": true,
    "id": "1397ed489c67b496008ad0587669ce9dede3d285",
    "semantic_title": "towards automated chinese ancient character restoration: a diffusion-based method with a new dataset",
    "citation_count": 0,
    "authors": [
      "Haolong Li",
      "Chenghao Du",
      "Ziheng Jiang",
      "Yifan Zhang",
      "Jiawei Ma",
      "Chen Ye"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28091": {
    "title": "Learning Deformable Hypothesis Sampling for Accurate PatchMatch Multi-View Stereo",
    "volume": "main",
    "abstract": "This paper introduces a learnable Deformable Hypothesis Sampler (DeformSampler) to address the challenging issue of noisy depth estimation in faithful PatchMatch multi-view stereo (MVS). We observe that the heuristic depth hypothesis sampling modes employed by PatchMatch MVS solvers are insensitive to (i) the piece-wise smooth distribution of depths across the object surface and (ii) the implicit multi-modal distribution of depth prediction probabilities along the ray direction on the surface points. Accordingly, we develop DeformSampler to learn distribution-sensitive sample spaces to (i) propagate depths consistent with the scene's geometry across the object surface and (ii) fit a Laplace Mixture model that approaches the point-wise probabilities distribution of the actual depths along the ray direction. We integrate DeformSampler into a learnable PatchMatch MVS system to enhance depth estimation in challenging areas, such as piece-wise discontinuous surface boundaries and weakly-textured regions. Experimental results on DTU and Tanks & Temples datasets demonstrate its superior performance and generalization capabilities compared to state-of-the-art competitors. Code is available at https://github.com/Geo-Tell/DS-PMNet",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongjie Li",
      "Yao Guo",
      "Xianwei Zheng",
      "Hanjiang Xiong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28092": {
    "title": "Catalyst for Clustering-Based Unsupervised Object Re-identification: Feature Calibration",
    "volume": "main",
    "abstract": "Clustering-based methods are emerging as a ubiquitous technology in unsupervised object Re-Identification (ReID), which alternate between pseudo-label generation and representation learning. Recent advances in this field mainly fall into two groups: pseudo-label correction and robust representation learning. Differently, in this work, we improve unsupervised object ReID from feature calibration, a completely different but complementary insight from the current approaches. Specifically, we propose to insert a conceptually simple yet empirically powerful Feature Calibration Module (FCM) before pseudo-label generation. In practice, FCM calibrates the features using a nonparametric graph attention network, enforcing similar instances to move together in the feature space while allowing dissimilar instances to separate. As a result, we can generate more reliable pseudo-labels using the calibrated features and further improve subsequent representation learning. FCM is simple, effective, parameter-free, training-free, plug-and-play, and can be considered as a catalyst, increasing the 'chemical reaction' between pseudo-label generation and representation learning. Moreover, it maintains the efficiency of testing time with negligible impact on training time. In this paper, we insert FCM into a simple baseline. Experiments across different scenarios and benchmarks show that FCM consistently improves the baseline (e.g., 8.2% mAP gain on MSMT17), and achieves the new state-of-the-art results. Code is available at: https://github.com/lhf12278/FCM-ReID",
    "checked": true,
    "id": "5b6d65bf0757486adfb6b57e3e73a096115ffa6b",
    "semantic_title": "catalyst for clustering-based unsupervised object re-identification: feature calibration",
    "citation_count": 1,
    "authors": [
      "Huafeng Li",
      "Qingsong Hu",
      "Zhanxuan Hu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28093": {
    "title": "EAN: An Efficient Attention Module Guided by Normalization for Deep Neural Networks",
    "volume": "main",
    "abstract": "Deep neural networks (DNNs) have achieved remarkable success in various fields, and two powerful techniques, feature normalization and attention mechanisms, have been widely used to enhance model performance. However, they are usually considered as two separate approaches or combined in a simplistic manner. In this paper, we investigate the intrinsic relationship between feature normalization and attention mechanisms and propose an Efficient Attention module guided by Normalization, dubbed EAN. Instead of using costly fully-connected layers for attention learning, EAN leverages the strengths of feature normalization and incorporates an Attention Generation (AG) unit to re-calibrate features. The proposed AG unit exploits the normalization component as a measure of the importance of distinct features and generates an attention mask using GroupNorm, L2 Norm, and Adaptation operations. By employing a grouping, AG unit and aggregation strategy, EAN is established, offering a unified module that harnesses the advantages of both normalization and attention, while maintaining minimal computational overhead. Furthermore, EAN serves as a plug-and-play module that can be seamlessly integrated with classic backbone architectures. Extensive quantitative evaluations on various visual tasks demonstrate that EAN achieves highly competitive performance compared to the current state-of-the-art attention methods while sustaining lower model complexity",
    "checked": true,
    "id": "5a0b5d323adea5053380b7583dc12aa27a720813",
    "semantic_title": "ean: an efficient attention module guided by normalization for deep neural networks",
    "citation_count": 0,
    "authors": [
      "Jiafeng Li",
      "Zelin Li",
      "Ying Wen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28094": {
    "title": "Label-Efficient Few-Shot Semantic Segmentation with Unsupervised Meta-Training",
    "volume": "main",
    "abstract": "The goal of this paper is to alleviate the training cost for few-shot semantic segmentation (FSS) models. Despite that FSS in nature improves model generalization to new concepts using only a handful of test exemplars, it relies on strong supervision from a considerable amount of labeled training data for base classes. However, collecting pixel-level annotations is notoriously expensive and time-consuming, and small-scale training datasets convey low information density that limits test-time generalization. To resolve the issue, we take a pioneering step towards label-efficient training of FSS models from fully unlabeled training data, or additionally a few labeled samples to enhance the performance. This motivates an approach based on a novel unsupervised meta-training paradigm. In particular, the approach first distills pre-trained unsupervised pixel embedding into compact semantic clusters from which a massive number of pseudo meta-tasks is constructed. To mitigate the noise in the pseudo meta-tasks, we further advocate a robust Transformer-based FSS model with a novel prototype-based cross-attention design. Extensive experiments have been conducted on two standard benchmarks, i.e., PASCAL-5i and COCO-20i, and the results show that our method produces impressive performance without any annotations, and is comparable to fully supervised competitors even using only 20% of the annotations. Our code is available at: https://github.com/SSSKYue/UMTFSS",
    "checked": true,
    "id": "4dd949b1703c07a1e1909715f9d85d1c71bf04e8",
    "semantic_title": "label-efficient few-shot semantic segmentation with unsupervised meta-training",
    "citation_count": 0,
    "authors": [
      "Jianwu Li",
      "Kaiyue Shi",
      "Guo-Sen Xie",
      "Xiaofeng Liu",
      "Jian Zhang",
      "Tianfei Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28095": {
    "title": "FedDiv: Collaborative Noise Filtering for Federated Learning with Noisy Labels",
    "volume": "main",
    "abstract": "Federated Learning with Noisy Labels (F-LNL) aims at seeking an optimal server model via collaborative distributed learning by aggregating multiple client models trained with local noisy or clean samples. On the basis of a federated learning framework, recent advances primarily adopt label noise filtering to separate clean samples from noisy ones on each client, thereby mitigating the negative impact of label noise. However, these prior methods do not learn noise filters by exploiting knowledge across all clients, leading to sub-optimal and inferior noise filtering performance and thus damaging training stability. In this paper, we present FedDiv to tackle the challenges of F-LNL. Specifically, we propose a global noise filter called Federated Noise Filter for effectively identifying samples with noisy labels on every client, thereby raising stability during local training sessions. Without sacrificing data privacy, this is achieved by modeling the global distribution of label noise across all clients. Then, in an effort to make the global model achieve higher performance, we introduce a Predictive Consistency based Sampler to identify more credible local data for local model training, thus preventing noise memorization and further boosting the training stability. Extensive experiments on CIFAR-10, CIFAR-100, and Clothing1M demonstrate that FedDiv achieves superior performance over state-of-the-art F-LNL methods under different label noise settings for both IID and non-IID data partitions. Source code is publicly available at https://github.com/lijichang/FLNL-FedDiv",
    "checked": true,
    "id": "064dcb85d67231075a768cbfed65264402c72b91",
    "semantic_title": "feddiv: collaborative noise filtering for federated learning with noisy labels",
    "citation_count": 4,
    "authors": [
      "Jichang Li",
      "Guanbin Li",
      "Hui Cheng",
      "Zicheng Liao",
      "Yizhou Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28096": {
    "title": "Fully Data-Driven Pseudo Label Estimation for Pointly-Supervised Panoptic Segmentation",
    "volume": "main",
    "abstract": "The core of pointly-supervised panoptic segmentation is estimating accurate dense pseudo labels from sparse point labels to train the panoptic head. Previous works generate pseudo labels mainly based on hand-crafted rules, such as connecting multiple points into polygon masks, or assigning the label information of labeled pixels to unlabeled pixels based on the artificially defined traversing distance. The accuracy of pseudo labels is limited by the quality of the hand-crafted rules (polygon masks are rough at object contour regions, and the traversing distance error will result in wrong pseudo labels). To overcome the limitation of hand-crafted rules, we estimate pseudo labels with a fully data-driven pseudo label branch, which is optimized by point labels end-to-end and predicts more accurate pseudo labels than previous methods. We also train an auxiliary semantic branch with point labels, it assists the training of the pseudo label branch by transferring semantic segmentation knowledge through shared parameters. Experiments on Pascal VOC and MS COCO demonstrate that our approach is effective and shows state-of-the-art performance compared with related works. Codes are available at https://github.com/BraveGroup/FDD",
    "checked": true,
    "id": "da3d0e947f958111a73b7c031635adf4239cfddd",
    "semantic_title": "fully data-driven pseudo label estimation for pointly-supervised panoptic segmentation",
    "citation_count": 0,
    "authors": [
      "Jing Li",
      "Junsong Fan",
      "Yuran Yang",
      "Shuqi Mei",
      "Jun Xiao",
      "Zhaoxiang Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28097": {
    "title": "FAVOR: Full-Body AR-Driven Virtual Object Rearrangement Guided by Instruction Text",
    "volume": "main",
    "abstract": "Rearrangement operations form the crux of interactions between humans and their environment. The ability to generate natural, fluid sequences of this operation is of essential value in AR/VR and CG. Bridging a gap in the field, our study introduces FAVOR: a novel dataset for Full-body AR-driven Virtual Object Rearrangement that uniquely employs motion capture systems and AR eyeglasses. Comprising 3k diverse motion rearrangement sequences and 7.17 million interaction data frames, this dataset breaks new ground in research data. We also present a pipeline FAVORITE for producing digital human rearrangement motion sequences guided by instructions. Experimental results, both qualitative and quantitative, suggest that this dataset and pipeline deliver high-quality motion sequences. Our dataset, code, and appendix are available at https://kailinli.github.io/FAVOR",
    "checked": true,
    "id": "dbc48d6b266151e6f89382536079ef0e624445f7",
    "semantic_title": "favor: full-body ar-driven virtual object rearrangement guided by instruction text",
    "citation_count": 2,
    "authors": [
      "Kailin Li",
      "Lixin Yang",
      "Zenan Lin",
      "Jian Xu",
      "Xinyu Zhan",
      "Yifei Zhao",
      "Pengxiang Zhu",
      "Wenxiong Kang",
      "Kejian Wu",
      "Cewu Lu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28098": {
    "title": "Panoptic Scene Graph Generation with Semantics-Prototype Learning",
    "volume": "main",
    "abstract": "Panoptic Scene Graph Generation (PSG) parses objects and predicts their relationships (predicate) to connect human language and visual scenes. However, different language preferences of annotators and semantic overlaps between predicates lead to biased predicate annotations in the dataset, i.e. different predicates for the same object pairs. Biased predicate annotations make PSG models struggle in constructing a clear decision plane among predicates, which greatly hinders the real application of PSG models. To address the intrinsic bias above, we propose a novel framework named ADTrans to adaptively transfer biased predicate annotations to informative and unified ones. To promise consistency and accuracy during the transfer process, we propose to observe the invariance degree of representations in each predicate class, and learn unbiased prototypes of predicates with different intensities. Meanwhile, we continuously measure the distribution changes between each presentation and its prototype, and constantly screen potentially biased data. Finally, with the unbiased predicate-prototype representation embedding space, biased annotations are easily identified. Experiments show that ADTrans significantly improves the performance of benchmark models, achieving a new state-of-the-art performance, and shows great generalization and effectiveness on multiple datasets. Our code is released at https://github.com/lili0415/PSG-biased-annotation",
    "checked": true,
    "id": "ad15b5abc44d4e2520bb16c0cf06c31ed461b599",
    "semantic_title": "panoptic scene graph generation with semantics-prototype learning",
    "citation_count": 19,
    "authors": [
      "Li Li",
      "Wei Ji",
      "Yiming Wu",
      "Mengze Li",
      "You Qin",
      "Lina Wei",
      "Roger Zimmermann"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28099": {
    "title": "SpectralNeRF: Physically Based Spectral Rendering with Neural Radiance Field",
    "volume": "main",
    "abstract": "In this paper, we propose SpectralNeRF, an end-to-end Neural Radiance Field (NeRF)-based architecture for high-quality physically based rendering from a novel spectral perspective. We modify the classical spectral rendering into two main steps, 1) the generation of a series of spectrum maps spanning different wavelengths, 2) the combination of these spectrum maps for the RGB output. Our SpectralNeRF follows these two steps through the proposed multi-layer perceptron (MLP)-based architecture (SpectralMLP) and Spectrum Attention UNet (SAUNet). Given the ray origin and the ray direction, the SpectralMLP constructs the spectral radiance field to obtain spectrum maps of novel views, which are then sent to the SAUNet to produce RGB images of white-light illumination. Applying NeRF to build up the spectral rendering is a more physically-based way from the perspective of ray-tracing. Further, the spectral radiance fields decompose difficult scenes and improve the performance of NeRF-based methods. Comprehensive experimental results demonstrate the proposed SpectralNeRF is superior to recent NeRF-based methods when synthesizing new views on synthetic and real datasets. The codes and datasets are available at https://github.com/liru0126/SpectralNeRF",
    "checked": true,
    "id": "b368b958476a002dc7d1205e97d131cec15cbb56",
    "semantic_title": "spectralnerf: physically based spectral rendering with neural radiance field",
    "citation_count": 1,
    "authors": [
      "Ru Li",
      "Jia Liu",
      "Guanghui Liu",
      "Shengping Zhang",
      "Bing Zeng",
      "Shuaicheng Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28100": {
    "title": "GridFormer: Point-Grid Transformer for Surface Reconstruction",
    "volume": "main",
    "abstract": "Implicit neural networks have emerged as a crucial technology in 3D surface reconstruction. To reconstruct continuous surfaces from discrete point clouds, encoding the input points into regular grid features (plane or volume) has been commonly employed in existing approaches. However, these methods typically use the grid as an index for uniformly scattering point features. Compared with the irregular point features, the regular grid features may sacrifice some reconstruction details but improve efficiency. To take full advantage of these two types of features, we introduce a novel and high-efficiency attention mechanism between the grid and point features named Point-Grid Transformer (GridFormer). This mechanism treats the grid as a transfer point connecting the space and point cloud. Our method maximizes the spatial expressiveness of grid features and maintains computational efficiency. Furthermore, optimizing predictions over the entire space could potentially result in blurred boundaries. To address this issue, we further propose a boundary optimization strategy incorporating margin binary cross-entropy loss and boundary sampling. This approach enables us to achieve a more precise representation of the object structure. Our experiments validate that our method is effective and outperforms the state-of-the-art approaches under widely used benchmarks by producing more precise geometry reconstructions. The code is available at https://github.com/list17/GridFormer",
    "checked": true,
    "id": "527753455847ee9ac6bc2a9b98d4e5406b8a5157",
    "semantic_title": "gridformer: point-grid transformer for surface reconstruction",
    "citation_count": 1,
    "authors": [
      "Shengtao Li",
      "Ge Gao",
      "Yudong Liu",
      "Yu-Shen Liu",
      "Ming Gu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28101": {
    "title": "Adaptive Uncertainty-Based Learning for Text-Based Person Retrieval",
    "volume": "main",
    "abstract": "Text-based person retrieval aims at retrieving a specific pedestrian image from a gallery based on textual descriptions. The primary challenge is how to overcome the inherent heterogeneous modality gap in the situation of significant intra-class variation and minimal inter-class variation. Existing approaches commonly employ vision-language pre-training or attention mechanisms to learn appropriate cross-modal alignments from noise inputs. Despite commendable progress, current methods inevitably suffer from two defects: 1) Matching ambiguity, which mainly derives from unreliable matching pairs; 2) One-sided cross-modal alignments, stemming from the absence of exploring one-to-many correspondence, i.e., coarse-grained semantic alignment. These critical issues significantly deteriorate retrieval performance. To this end, we propose a novel framework termed Adaptive Uncertainty-based Learning (AUL) for text-based person retrieval from the uncertainty perspective. Specifically, our AUL framework consists of three key components: 1) Uncertainty-aware Matching Filtration that leverages Subjective Logic to effectively mitigate the disturbance of unreliable matching pairs and select high-confidence cross-modal matches for training; 2) Uncertainty-based Alignment Refinement, which not only simulates coarse-grained alignments by constructing uncertainty representations but also performs progressive learning to incorporate coarse- and fine-grained alignments properly; 3) Cross-modal Masked Modeling that aims at exploring more comprehensive relations between vision and language. Extensive experiments demonstrate that our AUL method consistently achieves state-of-the-art performance on three benchmark datasets in supervised, weakly supervised, and domain generalization settings. Our code is available at https://github.com/CFM-MSG/Code-AUL",
    "checked": true,
    "id": "7b2dff4bec69aa6511590608ce8bfb200ba70215",
    "semantic_title": "adaptive uncertainty-based learning for text-based person retrieval",
    "citation_count": 1,
    "authors": [
      "Shenshen Li",
      "Chen He",
      "Xing Xu",
      "Fumin Shen",
      "Yang Yang",
      "Heng Tao Shen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28102": {
    "title": "Learning Continuous Implicit Field with Local Distance Indicator for Arbitrary-Scale Point Cloud Upsampling",
    "volume": "main",
    "abstract": "Point cloud upsampling aims to generate dense and uniformly distributed point sets from a sparse point cloud, which plays a critical role in 3D computer vision. Previous methods typically split a sparse point cloud into several local patches, upsample patch points, and merge all upsampled patches. However, these methods often produce holes, outliers or non-uniformity due to the splitting and merging process which does not maintain consistency among local patches.To address these issues, we propose a novel approach that learns an unsigned distance field guided by local priors for point cloud upsampling. Specifically, we train a local distance indicator (LDI) that predicts the unsigned distance from a query point to a local implicit surface. Utilizing the learned LDI, we learn an unsigned distance field to represent the sparse point cloud with patch consistency. At inference time, we randomly sample queries around the sparse point cloud, and project these query points onto the zero-level set of the learned implicit field to generate a dense point cloud. We justify that the implicit field is naturally continuous, which inherently enables the application of arbitrary-scale upsampling without necessarily retraining for various scales. We conduct comprehensive experiments on both synthetic data and real scans, and report state-of-the-art results under widely used benchmarks. Project page: https://lisj575.github.io/APU-LDI",
    "checked": true,
    "id": "d45bcd82fd2859475f06115da115a2b62acd6900",
    "semantic_title": "learning continuous implicit field with local distance indicator for arbitrary-scale point cloud upsampling",
    "citation_count": 2,
    "authors": [
      "Shujuan Li",
      "Junsheng Zhou",
      "Baorui Ma",
      "Yu-Shen Liu",
      "Zhizhong Han"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28103": {
    "title": "Long-Tailed Learning as Multi-Objective Optimization",
    "volume": "main",
    "abstract": "Real-world data is extremely imbalanced and presents a long-tailed distribution, resulting in models biased towards classes with sufficient samples and performing poorly on rare classes. Recent methods propose to rebalance classes but they undertake the seesaw dilemma (what is increasing performance on tail classes may decrease that of head classes, and vice versa). In this paper, we argue that the seesaw dilemma is derived from the gradient imbalance of different classes, in which gradients of inappropriate classes are set to important for updating, thus prone to overcompensation or undercompensation on tail classes. To achieve ideal compensation, we formulate long-tailed recognition as a multi-objective optimization problem, which fairly respects the contributions of head and tail classes simultaneously. For efficiency, we propose a Gradient-Balancing Grouping (GBG) strategy to gather the classes with similar gradient directions, thus approximately making every update under a Pareto descent direction. Our GBG method drives classes with similar gradient directions to form a more representative gradient and provides ideal compensation to the tail classes. Moreover, we conduct extensive experiments on commonly used benchmarks in long-tailed learning and demonstrate the superiority of our method over existing SOTA methods. Our code is released at https://github.com/WickyLee1998/GBG_v1",
    "checked": true,
    "id": "7056b7b7be998425fdac5fde072d6f9b7f027138",
    "semantic_title": "long-tailed learning as multi-objective optimization",
    "citation_count": 1,
    "authors": [
      "Weiqi Li",
      "Fan Lyu",
      "Fanhua Shang",
      "Liang Wan",
      "Wei Feng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28104": {
    "title": "Temporal-Distributed Backdoor Attack against Video Based Action Recognition",
    "volume": "main",
    "abstract": "Deep neural networks (DNNs) have achieved tremendous success in various applications including video action recognition, yet remain vulnerable to backdoor attacks (Trojans). The backdoor-compromised model will mis-classify to the target class chosen by the attacker when a test instance (from a non-target class) is embedded with a specific trigger, while maintaining high accuracy on attack-free instances. Although there are extensive studies on backdoor attacks against image data, the susceptibility of video-based systems under backdoor attacks remains largely unexplored. Current studies are direct extensions of approaches proposed for image data, e.g., the triggers are independently embedded within the frames, which tend to be detectable by existing defenses. In this paper, we introduce a simple yet effective backdoor attack against video data. Our proposed attack, adding perturbations in a transformed domain, plants an imperceptible, temporally distributed trigger across the video frames, and is shown to be resilient to existing defensive strategies. The effectiveness of the proposed attack is demonstrated by extensive experiments with various well-known models on two video recognition benchmarks, UCF101 and HMDB51, and a sign language recognition benchmark, Greek Sign Language (GSL) dataset. We delve into the impact of several influential factors on our proposed attack and identify an intriguing effect termed \"collateral damage\" through extensive studies",
    "checked": true,
    "id": "a717ebfe821d1f8defd22abc73ce992334c56b0b",
    "semantic_title": "temporal-distributed backdoor attack against video based action recognition",
    "citation_count": 4,
    "authors": [
      "Xi Li",
      "Songhe Wang",
      "Ruiquan Huang",
      "Mahanth Gowda",
      "George Kesidis"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28105": {
    "title": "DI-V2X: Learning Domain-Invariant Representation for Vehicle-Infrastructure Collaborative 3D Object Detection",
    "volume": "main",
    "abstract": "Vehicle-to-Everything (V2X) collaborative perception has recently gained significant attention due to its capability to enhance scene understanding by integrating information from various agents, e.g., vehicles, and infrastructure. However, current works often treat the information from each agent equally, ignoring the inherent domain gap caused by the utilization of different LiDAR sensors of each agent, thus leading to suboptimal performance. In this paper, we propose DI-V2X, that aims to learn Domain-Invariant representations through a new distillation framework to mitigate the domain discrepancy in the context of V2X 3D object detection. DI-V2X comprises three essential components: a domain-mixing instance augmentation (DMA) module, a progressive domain-invariant distillation (PDD) module, and a domain-adaptive fusion (DAF) module. Specifically, DMA builds a domain-mixing 3D instance bank for the teacher and student models during training, resulting in aligned data representation. Next, PDD encourages the student models from different domains to gradually learn a domain-invariant feature representation towards the teacher, where the overlapping regions between agents are employed as guidance to facilitate the distillation process. Furthermore, DAF closes the domain gap between the students by incorporating calibration-aware domain-adaptive attention. Extensive experiments on the challenging DAIR-V2X and V2XSet benchmark datasets demonstrate DI-V2X achieves remarkable performance, outperforming all the previous V2X models. Code is available at https://github.com/Serenos/DI-V2X",
    "checked": true,
    "id": "7205815c010275d481a52004434821143d8b8c0b",
    "semantic_title": "di-v2x: learning domain-invariant representation for vehicle-infrastructure collaborative 3d object detection",
    "citation_count": 6,
    "authors": [
      "Xiang Li",
      "Junbo Yin",
      "Wei Li",
      "Chengzhong Xu",
      "Ruigang Yang",
      "Jianbing Shen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28106": {
    "title": "Multi-Modality Affinity Inference for Weakly Supervised 3D Semantic Segmentation",
    "volume": "main",
    "abstract": "3D point cloud semantic segmentation has a wide range of applications. Recently, weakly supervised point cloud segmentation methods have been proposed, aiming to alleviate the expensive and laborious manual annotation process by leveraging scene-level labels. However, these methods have not effectively exploited the rich geometric information (such as shape and scale) and appearance information (such as color and texture) present in RGB-D scans. Furthermore, current approaches fail to fully leverage the point affinity that can be inferred from the feature extraction network, which is crucial for learning from weak scene-level labels. Additionally, previous work overlooks the detrimental effects of the long-tailed distribution of point cloud data in weakly supervised 3D semantic segmentation. To this end, this paper proposes a simple yet effective scene-level weakly supervised point cloud segmentation method with a newly introduced multi-modality point affinity inference module. The point affinity proposed in this paper is characterized by features from multiple modalities (e.g., point cloud and RGB), and is further refined by normalizing the classifier weights to alleviate the detrimental effects of long-tailed distribution without the need of the prior of category distribution. Extensive experiments on the ScanNet and S3DIS benchmarks verify the effectiveness of our proposed method, which outperforms the state-of-the-art by ~4% to ~ 6% mIoU. Codes are released at https://github.com/Sunny599/AAAI24-3DWSSG-MMA",
    "checked": true,
    "id": "7712928cc56933648473c3c6349209996fdfba07",
    "semantic_title": "multi-modality affinity inference for weakly supervised 3d semantic segmentation",
    "citation_count": 1,
    "authors": [
      "Xiawei Li",
      "Qingyuan Xu",
      "Jing Zhang",
      "Tianyi Zhang",
      "Qian Yu",
      "Lu Sheng",
      "Dong Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28107": {
    "title": "IINet: Implicit Intra-inter Information Fusion for Real-Time Stereo Matching",
    "volume": "main",
    "abstract": "Recently, there has been a growing interest in 3D CNN-based stereo matching methods due to their remarkable accuracy. However, the high complexity of 3D convolution makes it challenging to strike a balance between accuracy and speed. Notably, explicit 3D volumes contain considerable redundancy. In this study, we delve into more compact 2D implicit network to eliminate redundancy and boost real-time performance. However, simply replacing explicit 3D networks with 2D implicit networks causes issues that can lead to performance degradation, including the loss of structural information, the quality decline of inter-image information, as well as the inaccurate regression caused by low-level features. To address these issues, we first integrate intra-image information to fuse with inter-image information, facilitating propagation guided by structural cues. Subsequently, we introduce the Fast Multi-scale Score Volume (FMSV) and Confidence Based Filtering (CBF) to efficiently acquire accurate multi-scale, noise-free inter-image information. Furthermore, combined with the Residual Context-aware Upsampler (RCU), our Intra-Inter Fusing network is meticulously designed to enhance information transmission on both feature-level and disparity-level, thereby enabling accurate and robust regression. Experimental results affirm the superiority of our network in terms of both speed and accuracy compared to all other fast methods",
    "checked": true,
    "id": "f9fef9ef3d2b6a4e9a6a01aabdf35d65c19af2b2",
    "semantic_title": "iinet: implicit intra-inter information fusion for real-time stereo matching",
    "citation_count": 3,
    "authors": [
      "Ximeng Li",
      "Chen Zhang",
      "Wanjuan Su",
      "Wenbing Tao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28108": {
    "title": "Causal Representation Learning via Counterfactual Intervention",
    "volume": "main",
    "abstract": "Existing causal representation learning methods are based on the causal graph they build. However, due to the omission of bias within the causal graph, they essentially encourage models to learn biased causal effects in latent space. In this paper, we propose a novel causally disentangling framework that aims to learn unbiased causal effects. We first introduce inductive and dataset biases into traditional causal graph for the physical concepts of interest. Then, we eliminate the negative effects from these two biases by counterfactual intervention with reweighted loss function for learning unbiased causal effects. Finally, we employ the causal effects into the VAE to endow the latent representations with causality. In particular, we highlight that removing biases in this paper is regarded as a part of learning process for unbiased causal effects, which is crucial for causal disentanglement performance improvement. Through extensive experiments on real-world and synthetic datasets, we show that our method outperforms different baselines and obtains the state-of-the-art results for achieving causal representation learning",
    "checked": true,
    "id": "9a7768fd651aad877f3ffb16d0f3da1aa48abb54",
    "semantic_title": "causal representation learning via counterfactual intervention",
    "citation_count": 1,
    "authors": [
      "Xiutian Li",
      "Siqi Sun",
      "Rui Feng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28109": {
    "title": "Bi-ViT: Pushing the Limit of Vision Transformer Quantization",
    "volume": "main",
    "abstract": "Vision transformers (ViTs) quantization offers a promising prospect to facilitate deploying large pre-trained networks on resource-limited devices. Fully-binarized ViTs (Bi-ViT) that pushes the quantization of ViTs to its limit remain largely unexplored and a very challenging task yet, due to their unacceptable performance. Through extensive empirical analyses, we identify the severe drop in ViT binarization is caused by attention distortion in self-attention, which technically stems from the gradient vanishing and ranking disorder. To address these issues, we first introduce a learnable scaling factor to reactivate the vanished gradients and illustrate its effectiveness through theoretical and experimental analyses. We then propose a ranking-aware distillation method to rectify the disordered ranking in a teacher-student framework. Bi-ViT achieves significant improvements over popular DeiT and Swin backbones in terms of Top-1 accuracy and FLOPs. For example, with DeiT-Tiny and Swin-Tiny, our method significantly outperforms baselines by 22.1% and 21.4% respectively, while 61.5x and 56.1x theoretical acceleration in terms of FLOPs compared with real-valued counterparts on ImageNet. Our codes and models are attached on https://github.com/YanjingLi0202/Bi-ViT/",
    "checked": true,
    "id": "b48a85980deb5f1baa64d862b9f0e4e62124e4de",
    "semantic_title": "bi-vit: pushing the limit of vision transformer quantization",
    "citation_count": 1,
    "authors": [
      "Yanjing Li",
      "Sheng Xu",
      "Mingbao Lin",
      "Xianbin Cao",
      "Chuanjian Liu",
      "Xiao Sun",
      "Baochang Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28110": {
    "title": "Harnessing Edge Information for Improved Robustness in Vision Transformers",
    "volume": "main",
    "abstract": "Deep Neural Networks (DNNs) have demonstrated remarkable accuracy in vision classification tasks. However, they exhibit vulnerability to additional noises known as adversarial attacks. Previous studies hypothesize that this vulnerability might stem from the fact that high-accuracy DNNs heavily rely on irrelevant and non-robust features, such as textures and the background. In this work, we reveal that edge information extracted from images can provide relevant and robust features related to shapes and the foreground. These features assist pretrained DNNs in achieving improved adversarial robustness without compromising their accuracy on clean images. A lightweight and plug-and-play EdgeNet is proposed, which can be seamlessly integrated into existing pretrained DNNs, including Vision Transformers, a recent family of state-of-the-art models for vision classification. Our EdgeNet can process edges derived from either clean nature images or noisy adversarial images, yielding robust features which can be injected into the intermediate layers of the frozen backbone DNNs. The cost of obtaining such edges using conventional edge detection algorithms (e.g., Canny edge detector) is marginal, and the cost of training the EdgeNet is equivalent to that of fine-tuning the backbone network with techniques such as Adapter",
    "checked": true,
    "id": "f57183918c68c1571bcf172a7085b206302065a4",
    "semantic_title": "harnessing edge information for improved robustness in vision transformers",
    "citation_count": 0,
    "authors": [
      "Yanxi Li",
      "Chengbin  Du",
      "Chang Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28111": {
    "title": "Multi-Region Text-Driven Manipulation of Diffusion Imagery",
    "volume": "main",
    "abstract": "Text-guided image manipulation has attracted significant attention recently. Prevailing techniques concentrate on image attribute editing for individual objects, however, encountering challenges when it comes to multi-object editing. The main reason is the lack of consistency constraints on the spatial layout. This work presents a multi-region guided image manipulation framework, enabling manipulation through region-level textual prompts. With MultiDiffusion as a baseline, we are dedicated to the automatic generation of a rational multi-object spatial distribution, where disparate regions are fused as a unified entity. To mitigate interference from regional fusion, we employ an off-the-shelf model (CLIP) to impose region-aware spatial guidance on multi-object manipulation. Moreover, when applied to the StableDiffusion, the presence of quality-related yet object-agnostic lengthy words hampers the manipulation. To ensure focus on meaningful object-specific words for efficient guidance and generation, we introduce a keyword selection method. Furthermore, we demonstrate a downstream application of our method for multi-region inversion, which is tailored for manipulating multiple objects in real images. Our approach, compatible with variants of Stable Diffusion models, is readily applicable for manipulating diverse objects in extensive images with high-quality generation, showing superb image control capabilities. Code is available at https://github.com/liyiming09/multi-region-guided-diffusion",
    "checked": true,
    "id": "64b8f1b57fd0f79dbabb814f541f7b0ede2103eb",
    "semantic_title": "multi-region text-driven manipulation of diffusion imagery",
    "citation_count": 0,
    "authors": [
      "Yiming Li",
      "Peng  Zhou",
      "Jun Sun",
      "Yi Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28112": {
    "title": "Direct May Not Be the Best: An Incremental Evolution View of Pose Generation",
    "volume": "main",
    "abstract": "Pose diversity is an inherent representative characteristic of 2D images. Due to the 3D to 2D projection mechanism, there is evident content discrepancy among distinct pose images. This is the main obstacle bothering pose transformation related researches. To deal with this challenge, we propose a fine-grained incremental evolution centered pose generation framework, rather than traditional direct one-to-one in a rush. Since proposed approach actually bypasses the theoretical difficulty of directly modeling dramatic non-linear variation, the incurred content distortion and blurring could be effectively constrained, at the same time the various individual pose details, especially clothes texture, could be precisely maintained. In order to systematically guide the evolution course, both global and incremental evolution constraints are elaborately designed and merged into the overall framework. And a novel triple-path knowledge fusion structure is worked out to take full advantage of all available valuable knowledge to conduct high-quality pose synthesis. In addition, our framework could generate a series of valuable by-products, namely the various intermediate poses. Extensive experiments have been conducted to verify the effectiveness of the proposed approach. Code is available at https://github.com/Xiaofei-CN/Incremental-Evolution-Pose-Generation",
    "checked": true,
    "id": "560153fbadf7dd147fee5688cbd13f445118fc15",
    "semantic_title": "direct may not be the best: an incremental evolution view of pose generation",
    "citation_count": 0,
    "authors": [
      "Yuelong Li",
      "Tengfei Xiao",
      "Lei Geng",
      "Jianming Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28113": {
    "title": "FocalDreamer: Text-Driven 3D Editing via Focal-Fusion Assembly",
    "volume": "main",
    "abstract": "While text-3D editing has made significant strides in leveraging score distillation sampling, emerging approaches still fall short in delivering separable, precise and consistent outcomes that are vital to content creation. In response, we introduce FocalDreamer, a framework that merges base shape with editable parts according to text prompts for fine-grained editing within desired regions. Specifically, equipped with geometry union and dual-path rendering, FocalDreamer assembles independent 3D parts into a complete object, tailored for convenient instance reuse and part-wise control. We propose geometric focal loss and style consistency regularization, which encourage focal fusion and congruent overall appearance. Furthermore, FocalDreamer generates high-fidelity geometry and PBR textures which are compatible with widely-used graphics engines. Extensive experiments have highlighted the superior editing capabilities of FocalDreamer in both quantitative and qualitative evaluations",
    "checked": true,
    "id": "b89cdd726f41fcbbaa076f6a3a2bc420ab6643bf",
    "semantic_title": "focaldreamer: text-driven 3d editing via focal-fusion assembly",
    "citation_count": 35,
    "authors": [
      "Yuhan Li",
      "Yishun Dou",
      "Yue Shi",
      "Yu Lei",
      "Xuanhong Chen",
      "Yi Zhang",
      "Peng  Zhou",
      "Bingbing Ni"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28114": {
    "title": "SAVSR: Arbitrary-Scale Video Super-Resolution via a Learned Scale-Adaptive Network",
    "volume": "main",
    "abstract": "Deep learning-based video super-resolution (VSR) networks have gained significant performance improvements in recent years. However, existing VSR networks can only support a fixed integer scale super-resolution task, and when we want to perform VSR at multiple scales, we need to train several models. This implementation certainly increases the consumption of computational and storage resources, which limits the application scenarios of VSR techniques. In this paper, we propose a novel Scale-adaptive Arbitrary-scale Video Super-Resolution network (SAVSR), which is the first work focusing on spatial VSR at arbitrary scales including both non-integer and asymmetric scales. We also present an omni-dimensional scale-attention convolution, which dynamically adapts according to the scale of the input to extract inter-frame features with stronger representational power. Moreover, the proposed spatio-temporal adaptive arbitrary-scale upsampling performs VSR tasks using both temporal features and scale information. And we design an iterative bi-directional architecture for implicit feature alignment. Experiments at various scales on the benchmark datasets show that the proposed SAVSR outperforms state-of-the-art (SOTA) methods at non-integer and asymmetric scales. The source code is available at https://github.com/Weepingchestnut/SAVSR",
    "checked": true,
    "id": "e9dbfe1b8ff3c62ba98c12f13d2d03f07c805a24",
    "semantic_title": "savsr: arbitrary-scale video super-resolution via a learned scale-adaptive network",
    "citation_count": 1,
    "authors": [
      "Zekun Li",
      "Hongying Liu",
      "Fanhua Shang",
      "Yuanyuan Liu",
      "Liang Wan",
      "Wei Feng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28115": {
    "title": "Sampling-Resilient Multi-Object Tracking",
    "volume": "main",
    "abstract": "Multi-Object Tracking (MOT) is a cornerstone operator for video surveillance applications. To enable real-time processing of large-scale live video streams, we study an interesting scenario called down-sampled MOT, which performs object tracking only on a small subset of video frames. The problem is challenging for state-of-the-art MOT methods, which exhibit significant performance degradation under high frame reduction ratios. In this paper, we devise a sampling-resilient tracker with a novel sparse-observation Kalman filter (SOKF). It integrates an LSTM network to capture non-linear and dynamic motion patterns caused by sparse observations. Since the LSTM-based state transition is not compatible with the original noise estimation mechanism, we propose new estimation strategies based on Bayesian neural networks and derive the optimal Kalman gain for SOKF. To associate the detected bounding boxes robustly, we also propose a comprehensive similarity metric that systematically integrates multiple spatial matching signals. Experiments on three benchmark datasets show that our proposed tracker achieves the best trade-off between efficiency and accuracy. With the same tracking accuracy, we reduce the total processing time of ByteTrack by 2× in MOT17 and 3× in DanceTrack",
    "checked": true,
    "id": "ea3fe33fc17255d73d5964b0c6845558e723ae14",
    "semantic_title": "sampling-resilient multi-object tracking",
    "citation_count": 0,
    "authors": [
      "Zepeng Li",
      "Dongxiang  Zhang",
      "Sai Wu",
      "Mingli Song",
      "Gang Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28116": {
    "title": "Object-Aware Adaptive-Positivity Learning for Audio-Visual Question Answering",
    "volume": "main",
    "abstract": "This paper focuses on the Audio-Visual Question Answering (AVQA) task that aims to answer questions derived from untrimmed audible videos. To generate accurate answers, an AVQA model is expected to find the most informative audio-visual clues relevant to the given questions. In this paper, we propose to explicitly consider fine-grained visual objects in video frames (object-level clues) and explore the multi-modal relations (\\textit{i.e.}, the object, audio, and question) in terms of feature interaction and model optimization. For the former, we present an end-to-end object-oriented network that adopts a question-conditioned clue discovery module to concentrate audio/visual modalities on respective keywords of the question and designs a modality-conditioned clue collection module to highlight closely associated audio segments or visual objects. For model optimization, we propose an object-aware adaptive-positivity learning strategy that selects the highly semantic-matched multi-modal pair as \\textit{positivity}. Specifically, we design two object-aware contrastive loss functions to identify the highly relevant question-object pairs and audio-object pairs, respectively. These selected pairs are constrained to have larger similarity values than the mismatched pairs. The positivity-selecting process is adaptive as the positivity pairs selected in each video frame may be different. These two object-aware objectives help the model understand \\textit{which objects are exactly relevant to the question} and \\textit{which are making sounds}. Extensive experiments on the MUSIC-AVQA dataset demonstrate the proposed method is effective in finding favorable audio-visual clues and also achieves new state-of-the-art question-answering performance. The code is available at https://github.com/zhangbin-ai/APL",
    "checked": true,
    "id": "06f25d3b3112b7cfa5c9395e184644fdbb896d76",
    "semantic_title": "object-aware adaptive-positivity learning for audio-visual question answering",
    "citation_count": 2,
    "authors": [
      "Zhangbin Li",
      "Dan Guo",
      "Jinxing Zhou",
      "Jing Zhang",
      "Meng Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28117": {
    "title": "Hypercorrelation Evolution for Video Class-Incremental Learning",
    "volume": "main",
    "abstract": "Video class-incremental learning aims to recognize new actions while restricting the catastrophic forgetting of old ones, whose representative samples can only be saved in limited memory. Semantically variable subactions are susceptible to class confusion due to data imbalance. While existing methods address the problem by estimating and distilling the spatio-temporal knowledge, we further explores that the refinement of hierarchical correlations is crucial for the alignment of spatio-temporal features. To enhance the adaptability on evolved actions, we proposes a hierarchical aggregation strategy, in which hierarchical matching matrices are combined and jointly optimized to selectively store and retrieve relevant features from previous tasks. Meanwhile, a correlation refinement mechanism is presented to reinforce the bias on informative exemplars according to online hypercorrelation distribution. Experimental results demonstrate the effectiveness of the proposed method on three standard video class-incremental learning benchmarks, outperforming state-of-the-art methods. Code is available at: https://github.com/Lsen991031/HCE",
    "checked": true,
    "id": "a9ccbdce0f213d6b88862af1aed16f0ec018a158",
    "semantic_title": "hypercorrelation evolution for video class-incremental learning",
    "citation_count": 0,
    "authors": [
      "Sen Liang",
      "Kai Zhu",
      "Wei Zhai",
      "Zhiheng Liu",
      "Yang Cao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28118": {
    "title": "CoSTA: End-to-End Comprehensive Space-Time Entanglement for Spatio-Temporal Video Grounding",
    "volume": "main",
    "abstract": "This paper studies the spatio-temporal video grounding task, which aims to localize a spatio-temporal tube in an untrimmed video based on the given text description of an event. Existing one-stage approaches suffer from insufficient space-time interaction in two aspects: i) less precise prediction of event temporal boundaries, and ii) inconsistency in object prediction for the same event across adjacent frames. To address these issues, we propose a framework of Comprehensive Space-Time entAnglement (CoSTA) to densely entangle space-time multi-modal features for spatio-temporal localization. Specifically, we propose a space-time collaborative encoder to extract comprehensive video features and leverage Transformer to perform spatio-temporal multi-modal understanding. Our entangled decoder couples temporal boundary prediction and spatial localization via an entangled query, boasting an enhanced ability to capture object-event relationships. We conduct extensive experiments on the challenging benchmarks of HC-STVG and VidSTG, where CoSTA outperforms existing state-of-the-art methods, demonstrating its effectiveness for this task",
    "checked": true,
    "id": "6c854ba13bc8a68252e1dbbdf91fe581fcfa1da0",
    "semantic_title": "costa: end-to-end comprehensive space-time entanglement for spatio-temporal video grounding",
    "citation_count": 0,
    "authors": [
      "Yaoyuan Liang",
      "Xiao Liang",
      "Yansong Tang",
      "Zhao Yang",
      "Ziran Li",
      "Jingang Wang",
      "Wenbo Ding",
      "Shao-Lun Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28119": {
    "title": "Any-Stereo: Arbitrary Scale Disparity Estimation for Iterative Stereo Matching",
    "volume": "main",
    "abstract": "Due to unaffordable computational costs, the regularized disparity in iterative stereo matching is typically maintained at a lower resolution than the input. To regress the full resolution disparity, most stereo methods resort to convolutions to decode a fixed-scale output. However, they are inadequate for recovering vital high-frequency information lost during downsampling, limiting their performance on full-resolution prediction. In this paper, we introduce AnyStereo, an accurate and efficient disparity upsampling module with implicit neural representation for the iterative stereo pipeline. By modeling the disparity as a continuous representation over 2D spatial coordinates, subtle details can emerge from the latent space at arbitrary resolution. To further complement the missing information and details in the latent code, we propose two strategies: intra-scale similarity unfolding and cross-scale feature alignment. The former unfolds the neighbor relationships, while the latter introduces the context in high-resolution feature maps. The proposed AnyStereo can seamlessly replace the upsampling module in most iterative stereo models, improving their ability to capture fine details and generate arbitrary-scale disparities even with fewer parameters. With our method, the iterative stereo pipeline establishes a new state-of-the-art performance. The code is available at https://github.com/Zhaohuai-L/Any-Stereo",
    "checked": true,
    "id": "98d1f09c6ad0fadff7df5085365d25c6b01775b9",
    "semantic_title": "any-stereo: arbitrary scale disparity estimation for iterative stereo matching",
    "citation_count": 2,
    "authors": [
      "Zhaohuai Liang",
      "Changhe Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28120": {
    "title": "Impartial Adversarial Distillation: Addressing Biased Data-Free Knowledge Distillation via Adaptive Constrained Optimization",
    "volume": "main",
    "abstract": "Data-Free Knowledge Distillation (DFKD) enables knowledge transfer from a pretrained teacher to a light-weighted student without original training data. Existing works are limited by a strong assumption that samples used to pretrain the teacher model are balanced, which is, however, unrealistic for many real-world tasks. In this work, we investigated a pragmatic yet under-explored problem: how to perform DFKD from a teacher model pretrained from imbalanced data. We observe a seemingly counter-intuitive phenomenon, i.e., adversarial DFKD algorithms favour minority classes, while causing a disastrous impact on majority classes. We theoretically prove that a biased teacher could cause severe disparity on different groups of synthetic data in adversarial distillation, which further exacerbates the mode collapse of a generator and consequently degenerates the overall accuracy of a distilled student model. To tackle this problem, we propose a class-adaptive regularization method, aiming to encourage impartial representation learning of a generator among different classes under a constrained learning formulation. We devise a primal-dual algorithm to solve the target optimization problem. Through extensive experiments, we show that our method mitigates the biased learning of majority classes in DFKD and improves the overall performance compared with baselines. Code will be available at https://github.com/ldpbuaa/ipad",
    "checked": true,
    "id": "429e080c5209da6fcb0de0008b00fa8ce43d2306",
    "semantic_title": "impartial adversarial distillation: addressing biased data-free knowledge distillation via adaptive constrained optimization",
    "citation_count": 0,
    "authors": [
      "Dongping Liao",
      "Xitong Gao",
      "Chengzhong Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28121": {
    "title": "VLM2Scene: Self-Supervised Image-Text-LiDAR Learning with Foundation Models for Autonomous Driving Scene Understanding",
    "volume": "main",
    "abstract": "Vision and language foundation models (VLMs) have showcased impressive capabilities in 2D scene understanding. However, their latent potential in elevating the understanding of 3D autonomous driving scenes remains untapped. In this paper, we propose VLM2Scene, which exploits the potential of VLMs to enhance 3D self-supervised representation learning through our proposed image-text-LiDAR contrastive learning strategy. Specifically, in the realm of autonomous driving scenes, the inherent sparsity of LiDAR point clouds poses a notable challenge for point-level contrastive learning methods. This method often grapples with limitations tied to a restricted receptive field and the presence of noisy points. To tackle this challenge, our approach emphasizes region-level learning, leveraging regional masks without semantics derived from the vision foundation model. This approach capitalizes on valuable contextual information to enhance the learning of point cloud representations. First, we introduce Region Caption Prompts to generate fine-grained language descriptions for the corresponding regions, utilizing the language foundation model. These region prompts then facilitate the establishment of positive and negative text-point pairs within the contrastive loss framework. Second, we propose a Region Semantic Concordance Regularization, which involves a semantic-filtered region learning and a region semantic assignment strategy. The former aims to filter the false negative samples based on the semantic distance, and the latter mitigates potential inaccuracies in pixel semantics, thereby enhancing overall semantic consistency. Extensive experiments on representative autonomous driving datasets demonstrate that our self-supervised method significantly outperforms other counterparts. Codes are available at https://github.com/gbliao/VLM2Scene",
    "checked": true,
    "id": "96148eb50d850813da8ae97158e0897b2fd81317",
    "semantic_title": "vlm2scene: self-supervised image-text-lidar learning with foundation models for autonomous driving scene understanding",
    "citation_count": 4,
    "authors": [
      "Guibiao Liao",
      "Jiankun Li",
      "Xiaoqing Ye"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28122": {
    "title": "Text-to-Image Generation for Abstract Concepts",
    "volume": "main",
    "abstract": "Recent years have witnessed the substantial progress of large-scale models across various domains, such as natural language processing and computer vision, facilitating the expression of concrete concepts. Unlike concrete concepts that are usually directly associated with physical objects, expressing abstract concepts through natural language requires considerable effort since they are characterized by intricate semantics and connotations. An alternative approach is to leverage images to convey rich visual information as a supplement. Nevertheless, existing Text-to-Image (T2I) models are primarily trained on concrete physical objects and often struggle to visualize abstract concepts. Inspired by the three-layer artwork theory that identifies critical factors, intent, object and form during artistic creation, we propose a framework of Text-to-Image generation for Abstract Concepts (TIAC). The abstract concept is clarified into a clear intent with a detailed definition to avoid ambiguity. LLMs then transform it into semantic-related physical objects, and the concept-dependent form is retrieved from an LLM-extracted form pattern set. Information from these three aspects will be integrated to generate prompts for T2I models via LLM. Evaluation results from human assessments and our newly designed metric concept score demonstrate the effectiveness of our framework in creating images that can sufficiently express abstract concepts",
    "checked": true,
    "id": "0d38f1edac66b4645cf5fa05abaf9d92cba5d5d3",
    "semantic_title": "text-to-image generation for abstract concepts",
    "citation_count": 4,
    "authors": [
      "Jiayi Liao",
      "Xu Chen",
      "Qiang Fu",
      "Lun Du",
      "Xiangnan He",
      "Xiang Wang",
      "Shi Han",
      "Dongmei Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28123": {
    "title": "VSFormer: Visual-Spatial Fusion Transformer for Correspondence Pruning",
    "volume": "main",
    "abstract": "Correspondence pruning aims to find correct matches (inliers) from an initial set of putative correspondences, which is a fundamental task for many applications. The process of finding is challenging, given the varying inlier ratios between scenes/image pairs due to significant visual differences. However, the performance of the existing methods is usually limited by the problem of lacking visual cues (e.g., texture, illumination, structure) of scenes. In this paper, we propose a Visual-Spatial Fusion Transformer (VSFormer) to identify inliers and recover camera poses accurately. Firstly, we obtain highly abstract visual cues of a scene with the cross attention between local features of two-view images. Then, we model these visual cues and correspondences by a joint visual-spatial fusion module, simultaneously embedding visual cues into correspondences for pruning. Additionally, to mine the consistency of correspondences, we also design a novel module that combines the KNN-based graph and the transformer, effectively capturing both local and global contexts. Extensive experiments have demonstrated that the proposed VSFormer outperforms state-of-the-art methods on outdoor and indoor benchmarks. Our code is provided at the following repository: https://github.com/sugar-fly/VSFormer",
    "checked": true,
    "id": "d8ecf82bd3bfc17ba2e7875672377a51b01e36c6",
    "semantic_title": "vsformer: visual-spatial fusion transformer for correspondence pruning",
    "citation_count": 1,
    "authors": [
      "Tangfei Liao",
      "Xiaoqin Zhang",
      "Li Zhao",
      "Tao Wang",
      "Guobao Xiao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28124": {
    "title": "NightRain: Nighttime Video Deraining via Adaptive-Rain-Removal and Adaptive-Correction",
    "volume": "main",
    "abstract": "Existing deep-learning-based methods for nighttime video deraining rely on synthetic data due to the absence of real-world paired data. However, the intricacies of the real world, particularly with the presence of light effects and low-light regions affected by noise, create significant domain gaps, hampering synthetic-trained models in removing rain streaks properly and leading to over-saturation and color shifts. Motivated by this, we introduce NightRain, a novel nighttime video deraining method with adaptive-rain-removal and adaptive-correction. Our adaptive-rain-removal uses unlabeled rain videos to enable our model to derain real-world rain videos, particularly in regions affected by complex light effects. The idea is to allow our model to obtain rain-free regions based on the confidence scores. Once rain-free regions and the corresponding regions from our input are obtained, we can have region-based paired real data. These paired data are used to train our model using a teacher-student framework, allowing the model to iteratively learn from less challenging regions to more challenging regions. Our adaptive-correction aims to rectify errors in our model's predictions, such as over-saturation and color shifts. The idea is to learn from clear night input training videos based on the differences or distance between those input videos and their corresponding predictions. Our model learns from these differences, compelling our model to correct the errors. From extensive experiments, our method demonstrates state-of-the-art performance. It achieves a PSNR of 26.73dB, surpassing existing nighttime video deraining methods by a substantial margin of 13.7%",
    "checked": true,
    "id": "bed73f894a89efd988b2f84a483d246c80d21031",
    "semantic_title": "nightrain: nighttime video deraining via adaptive-rain-removal and adaptive-correction",
    "citation_count": 2,
    "authors": [
      "Beibei Lin",
      "Yeying Jin",
      "Wending Yan",
      "Wei Ye",
      "Yuan Yuan",
      "Shunli Zhang",
      "Robby T. Tan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28125": {
    "title": "Unsupervised Pan-Sharpening via Mutually Guided Detail Restoration",
    "volume": "main",
    "abstract": "Pan-sharpening is a task that aims to super-resolve the low-resolution multispectral (LRMS) image with the guidance of a corresponding high-resolution panchromatic (PAN) image. The key challenge in pan-sharpening is to accurately modeling the relationship between the MS and PAN images. While supervised deep learning methods are commonly employed to address this task, the unavailability of ground-truth severely limits their effectiveness. In this paper, we propose a mutually guided detail restoration method for unsupervised pan-sharpening. Specifically, we treat pan-sharpening as a blind image deblurring task, in which the blur kernel can be estimated by a CNN. Constrained by the blur kernel, the pan-sharpened image retains spectral information consistent with the LRMS image. Once the pan-sharpened image is obtained, the PAN image is blurred using a pre-defined blur operator. The pan-sharpened image, in turn, is used to guide the detail restoration of the blurred PAN image. By leveraging the mutual guidance between MS and PAN images, the pan-sharpening network can implicitly learn the spatial relationship between the two modalities. Extensive experiments show that the proposed method significantly outperforms existing unsupervised pan-sharpening methods",
    "checked": true,
    "id": "342b3c768e052a5f296b993970114d334425d1c6",
    "semantic_title": "unsupervised pan-sharpening via mutually guided detail restoration",
    "citation_count": 0,
    "authors": [
      "Huangxing Lin",
      "Yuhang Dong",
      "Xinghao Ding",
      "Tianpeng Liu",
      "Yongxiang Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28126": {
    "title": "Gramformer: Learning Crowd Counting via Graph-Modulated Transformer",
    "volume": "main",
    "abstract": "Transformer has been popular in recent crowd counting work since it breaks the limited receptive field of traditional CNNs. However, since crowd images always contain a large number of similar patches, the self-attention mechanism in Transformer tends to find a homogenized solution where the attention maps of almost all patches are identical. In this paper, we address this problem by proposing Gramformer: a graph-modulated transformer to enhance the network by adjusting the attention and input node features respectively on the basis of two different types of graphs. Firstly, an attention graph is proposed to diverse attention maps to attend to complementary information. The graph is building upon the dissimilarities between patches, modulating the attention in an anti-similarity fashion. Secondly, a feature-based centrality encoding is proposed to discover the centrality positions or importance of nodes. We encode them with a proposed centrality indices scheme to modulate the node features and similarity relationships. Extensive experiments on four challenging crowd counting datasets have validated the competitiveness of the proposed method. Code is available at https://github.com/LoraLinH/Gramformer",
    "checked": true,
    "id": "42d4057b9f03b538349c7c66a02fdc6c84c066f1",
    "semantic_title": "gramformer: learning crowd counting via graph-modulated transformer",
    "citation_count": 2,
    "authors": [
      "Hui Lin",
      "Zhiheng Ma",
      "Xiaopeng Hong",
      "Qinnan Shangguan",
      "Deyu Meng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28127": {
    "title": "Weakly Supervised Open-Vocabulary Object Detection",
    "volume": "main",
    "abstract": "Despite weakly supervised object detection (WSOD) being a promising step toward evading strong instance-level annotations, its capability is confined to closed-set categories within a single training dataset. In this paper, we propose a novel weakly supervised open-vocabulary object detection framework, namely WSOVOD, to extend traditional WSOD to detect novel concepts and utilize diverse datasets with only image-level annotations. To achieve this, we explore three vital strategies, including dataset-level feature adaptation, image-level salient object localization, and region-level vision-language alignment. First, we perform data-aware feature extraction to produce an input-conditional coefficient, which is leveraged into dataset attribute prototypes to identify dataset bias and help achieve cross-dataset generalization. Second, a customized location-oriented weakly supervised region proposal network is proposed to utilize high-level semantic layouts from the category-agnostic segment anything model to distinguish object boundaries. Lastly, we introduce a proposal-concept synchronized multiple-instance network, i.e., object mining and refinement with visual-semantic alignment, to discover objects matched to the text embeddings of concepts. Extensive experiments on Pascal VOC and MS COCO demonstrate that the proposed WSOVOD achieves new state-of-the-art compared with previous WSOD methods in both close-set object localization and detection tasks. Meanwhile, WSOVOD enables cross-dataset and open-vocabulary learning to achieve on-par or even better performance than well-established fully-supervised open-vocabulary object detection (FSOVOD)",
    "checked": true,
    "id": "3beff9defd7666a197ab78bb0ab337e7dcf24598",
    "semantic_title": "weakly supervised open-vocabulary object detection",
    "citation_count": 2,
    "authors": [
      "Jianghang Lin",
      "Yunhang Shen",
      "Bingquan Wang",
      "Shaohui Lin",
      "Ke Li",
      "Liujuan Cao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28128": {
    "title": "Spot the Error: Non-autoregressive Graphic Layout Generation with Wireframe Locator",
    "volume": "main",
    "abstract": "Layout generation is a critical step in graphic design to achieve meaningful compositions of elements. Most previous works view it as a sequence generation problem by concatenating element attribute tokens (i.e., category, size, position). So far the autoregressive approach (AR) has achieved promising results, but is still limited in global context modeling and suffers from error propagation since it can only attend to the previously generated tokens. Recent non-autoregressive attempts (NAR) have shown competitive results, which provides a wider context range and the flexibility to refine with iterative decoding. However, current works only use simple heuristics to recognize erroneous tokens for refinement which is inaccurate. This paper first conducts an in-depth analysis to better understand the difference between the AR and NAR framework. Furthermore, based on our observation that pixel space is more sensitive in capturing spatial patterns of graphic layouts (e.g., overlap, alignment), we propose a learning-based locator to detect erroneous tokens which takes the wireframe image rendered from the generated layout sequence as input. We show that it serves as a complementary modality to the element sequence in object space and contributes greatly to the overall performance. Experiments on two public datasets show that our approach outperforms both AR and NAR baselines. Extensive studies further prove the effectiveness of different modules with interesting findings. Our code will be available at https://github.com/ffffatgoose/SpotError",
    "checked": true,
    "id": "e4c7f5f83b417335b2eb7b44d87037fe24b8f538",
    "semantic_title": "spot the error: non-autoregressive graphic layout generation with wireframe locator",
    "citation_count": 1,
    "authors": [
      "Jieru Lin",
      "Danqing Huang",
      "Tiejun Zhao",
      "Dechen Zhan",
      "Chin-Yew Lin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28129": {
    "title": "M2SD:Multiple Mixing Self-Distillation for Few-Shot Class-Incremental Learning",
    "volume": "main",
    "abstract": "Few-shot Class-incremental learning (FSCIL) is a challenging task in machine learning that aims to recognize new classes from a limited number of instances while preserving the ability to classify previously learned classes without retraining the entire model. This presents challenges in updating the model with new classes using limited training data, particularly in balancing acquiring new knowledge while retaining the old. We propose a novel method named Multiple Mxing Self-Distillation (M2SD) during the training phase to address these issues. Specifically, we propose a dual-branch structure that facilitates the expansion of the entire feature space to accommodate new classes. Furthermore, we introduce a feature enhancement component that can pass additional enhanced information back to the base network by self-distillation, resulting in improved classification performance upon adding new classes. After training, we discard both structures, leaving only the primary network to classify new class instances. Extensive experiments demonstrate that our approach achieves superior performance over previous state-of-the-art methods",
    "checked": false,
    "id": "73f7b7e32da246f9a232a9cf313ea7932b1d8c25",
    "semantic_title": "m2sd: multiple mixing self-distillation for few-shot class-incremental learning",
    "citation_count": 0,
    "authors": [
      "Jinhao Lin",
      "Ziheng Wu",
      "Weifeng Lin",
      "Jun Huang",
      "RongHua Luo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28130": {
    "title": "EDA: Evolving and Distinct Anchors for Multimodal Motion Prediction",
    "volume": "main",
    "abstract": "Motion prediction is a crucial task in autonomous driving, and one of its major challenges lands in the multimodality of future behaviors. Many successful works have utilized mixture models which require identification of positive mixture components, and correspondingly fall into two main lines: prediction-based and anchor-based matching. The prediction clustering phenomenon in prediction-based matching makes it difficult to pick representative trajectories for downstream tasks, while the anchor-based matching suffers from a limited regression capability. In this paper, we introduce a novel paradigm, named Evolving and Distinct Anchors (EDA), to define the positive and negative components for multimodal motion prediction based on mixture models. We enable anchors to evolve and redistribute themselves under specific scenes for an enlarged regression capacity. Furthermore, we select distinct anchors before matching them with the ground truth, which results in impressive scoring performance. Our approach enhances all metrics compared to the baseline MTR, particularly with a notable relative reduction of 13.5% in Miss Rate, resulting in state-of-the-art performance on the Waymo Open Motion Dataset. Appendix and code are available at https://github.com/Longzhong-Lin/EDA",
    "checked": true,
    "id": "7aada1cf092ea79ed373f6bd81ff76b3d0a23a16",
    "semantic_title": "eda: evolving and distinct anchors for multimodal motion prediction",
    "citation_count": 2,
    "authors": [
      "Longzhong Lin",
      "Xuewu Lin",
      "Tianwei Lin",
      "Lichao Huang",
      "Rong Xiong",
      "Yue Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28131": {
    "title": "PTUS: Photo-Realistic Talking Upper-Body Synthesis via 3D-Aware Motion Decomposition Warping",
    "volume": "main",
    "abstract": "Talking upper-body synthesis is a promising task due to its versatile potential for video creation and consists of animating the body and face from a source image with the motion from a given driving video. However, prior synthesis approaches fall short in addressing this task and have been either limited to animating heads of a target person only, or have animated the upper body but neglected the synthesis of precise facial details. To tackle this task, we propose a Photo-realistic Talking Upper-body Synthesis method via 3D-aware motion decomposition warping, named PTUS, to both precisely synthesize the upper body as well as recover the details of the face such as blinking and lip synchronization. In particular, the motion decomposition mechanism consists of a face-body motion decomposition, which decouples the 3D motion estimation of the face and body, and a local-global motion decomposition, which decomposes the 3D face motion into global and local motions resulting in the transfer of facial expression. The 3D-aware warping module transfers the large-scale and subtle 3D motions to the extracted 3D depth-aware features in a coarse-tofine manner. Moreover, we present a new dataset, Talking-UB, which includes upper-body images with high-resolution faces, addressing the limitations of prior datasets that either consist of only facial images or upper-body images with blurry faces. Experimental results demonstrate that our proposed method can synthesize high-quality videos that preserve facial details, and achieves superior results compared to state-of-the-art cross-person motion transfer approaches. Code and collected dataset are released in https://github.com/cooluoluo/PTUS",
    "checked": true,
    "id": "eae107ee26815b25ab9f82ff58a280f8e14bf983",
    "semantic_title": "ptus: photo-realistic talking upper-body synthesis via 3d-aware motion decomposition warping",
    "citation_count": 0,
    "authors": [
      "Luoyang Lin",
      "Zutao Jiang",
      "Xiaodan Liang",
      "Liqian Ma",
      "Michael C. Kampffmeyer",
      "Xiaochun Cao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28132": {
    "title": "Exploring Temporal Feature Correlation for Efficient and Stable Video Semantic Segmentation",
    "volume": "main",
    "abstract": "This paper tackles the problem of efficient and stable video semantic segmentation. While stability has been under-explored, prevalent work in efficient video semantic segmentation uses the keyframe paradigm. They efficiently process videos by only recomputing the low-level features and reusing high-level features computed at selected keyframes. In addition, the reused features stabilize the predictions across frames, thereby improving video consistency. However, dynamic scenes in the video can easily lead to misalignments between reused and recomputed features, which hampers performance. Moreover, relying on feature reuse to improve prediction consistency is brittle; an erroneous alignment of the features can easily lead to unstable predictions. Therefore, the keyframe paradigm exhibits a dilemma between stability and performance. We address this efficiency and stability challenge using a novel yet simple Temporal Feature Correlation (TFC) module. It uses the cosine similarity between two frames' low-level features to inform the semantic label's consistency across frames. Specifically, we selectively reuse label-consistent features across frames through linear interpolation and update others through sparse multi-scale deformable attention. As a result, we no longer directly reuse features to improve stability and thus effectively solve feature misalignment. This work provides a significant step towards efficient and stable video semantic segmentation. On the VSPW dataset, our method significantly improves the prediction consistency of image-based methods while being as fast and accurate",
    "checked": true,
    "id": "4933214f5c731c554699e7fa44c4e3bb560ac4c6",
    "semantic_title": "exploring temporal feature correlation for efficient and stable video semantic segmentation",
    "citation_count": 0,
    "authors": [
      "Matthieu Lin",
      "Jenny Sheng",
      "Yubin Hu",
      "Yangguang Li",
      "Lu Qi",
      "Andrew Zhao",
      "Gao Huang",
      "Yong-Jin Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28133": {
    "title": "Boosting Adversarial Transferability across Model Genus by Deformation-Constrained Warping",
    "volume": "main",
    "abstract": "Adversarial examples generated by a surrogate model typically exhibit limited transferability to unknown target systems. To address this problem, many transferability enhancement approaches (e.g., input transformation and model augmentation) have been proposed. However, they show poor performances in attacking systems having different model genera from the surrogate model. In this paper, we propose a novel and generic attacking strategy, called Deformation-Constrained Warping Attack (DeCoWA), that can be effectively applied to cross model genus attack. Specifically, DeCoWA firstly augments input examples via an elastic deformation, namely Deformation-Constrained Warping (DeCoW), to obtain rich local details of the augmented input. To avoid severe distortion of global semantics led by random deformation, DeCoW further constrains the strength and direction of the warping transformation by a novel adaptive control strategy. Extensive experiments demonstrate that the transferable examples crafted by our DeCoWA on CNN surrogates can significantly hinder the performance of Transformers (and vice versa) on various tasks, including image classification, video action recognition, and audio recognition. Code is made available at https://github.com/LinQinLiang/DeCoWA",
    "checked": true,
    "id": "c8aa4f78499b4def6738d7687d660f0de742815d",
    "semantic_title": "boosting adversarial transferability across model genus by deformation-constrained warping",
    "citation_count": 0,
    "authors": [
      "Qinliang Lin",
      "Cheng Luo",
      "Zenghao Niu",
      "Xilin He",
      "Weicheng Xie",
      "Yuanbo Hou",
      "Linlin Shen",
      "Siyang Song"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28134": {
    "title": "A Fixed-Point Approach to Unified Prompt-Based Counting",
    "volume": "main",
    "abstract": "Existing class-agnostic counting models typically rely on a single type of prompt, e.g., box annotations. This paper aims to establish a comprehensive prompt-based counting framework capable of generating density maps for concerned objects indicated by various prompt types, such as box, point, and text. To achieve this goal, we begin by converting prompts from different modalities into prompt masks without requiring training. These masks are then integrated into a class-agnostic counting methodology for predicting density maps. Furthermore, we introduce a fixed-point inference along with an associated loss function to improve counting accuracy, all without introducing new parameters. The effectiveness of this method is substantiated both theoretically and experimentally. Additionally, a contrastive training scheme is implemented to mitigate dataset bias inherent in current class-agnostic counting datasets, a strategy whose effectiveness is confirmed by our ablation study. Our model excels in prominent class-agnostic datasets and exhibits superior performance in cross-dataset adaptation tasks",
    "checked": true,
    "id": "7708a8618ce479c0d7b1592bd9ba1ad9ced3b3d0",
    "semantic_title": "a fixed-point approach to unified prompt-based counting",
    "citation_count": 1,
    "authors": [
      "Wei Lin",
      "Antoni B. Chan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28135": {
    "title": "Boosting Multiple Instance Learning Models for Whole Slide Image Classification: A Model-Agnostic Framework Based on Counterfactual Inference",
    "volume": "main",
    "abstract": "Multiple instance learning is an effective paradigm for whole slide image (WSI) classification, where labels are only provided at the bag level. However, instance-level prediction is also crucial as it offers insights into fine-grained regions of interest. Existing multiple instance learning methods either solely focus on training a bag classifier or have the insufficient capability of exploring instance prediction. In this work, we propose a novel model-agnostic framework to boost existing multiple instance learning models, to improve the WSI classification performance in both bag and instance levels. Specifically, we propose a counterfactual inference-based sub-bag assessment method and a hierarchical instance searching strategy to help to search reliable instances and obtain their accurate pseudo labels. Furthermore, an instance classifier is well-trained to produce accurate predictions. The instance embedding it generates is treated as a prompt to refine the instance feature for bag prediction. This framework is model-agnostic, capable of adapting to existing multiple instance learning models, including those without specific mechanisms like attention. Extensive experiments on three datasets demonstrate the competitive performance of our method. Code will be available at https://github.com/centurion-crawler/CIMIL",
    "checked": true,
    "id": "e64a031f900a1fb9cfc2c9604c72c3f7bbfe44f6",
    "semantic_title": "boosting multiple instance learning models for whole slide image classification: a model-agnostic framework based on counterfactual inference",
    "citation_count": 0,
    "authors": [
      "Weiping Lin",
      "Zhenfeng Zhuang",
      "Lequan Yu",
      "Liansheng Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28136": {
    "title": "Relightable and Animatable Neural Avatars from Videos",
    "volume": "main",
    "abstract": "Lightweight creation of 3D digital avatars is a highly desirable but challenging task. With only sparse videos of a person under unknown illumination, we propose a method to create relightable and animatable neural avatars, which can be used to synthesize photorealistic images of humans under novel viewpoints, body poses, and lighting. The key challenge here is to disentangle the geometry, material of the clothed body, and lighting, which becomes more difficult due to the complex geometry and shadow changes caused by body motions. To solve this ill-posed problem, we propose novel techniques to better model the geometry and shadow changes. For geometry change modeling, we propose an invertible deformation field, which helps to solve the inverse skinning problem and leads to better geometry quality. To model the spatial and temporal varying shading cues, we propose a pose-aware part-wise light visibility network to estimate light occlusion. Extensive experiments on synthetic and real datasets show that our approach reconstructs high-quality geometry and generates realistic shadows under different body poses. Code and data are available at https://wenbin-lin.github.io/RelightableAvatar-page",
    "checked": true,
    "id": "af945274d2c61debdac3eaa2d4f1c7fa405a1a60",
    "semantic_title": "relightable and animatable neural avatars from videos",
    "citation_count": 4,
    "authors": [
      "Wenbin Lin",
      "Chengwei Zheng",
      "Jun-Hai Yong",
      "Feng Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28137": {
    "title": "TD²-Net: Toward Denoising and Debiasing for Video Scene Graph Generation",
    "volume": "main",
    "abstract": "Dynamic scene graph generation (SGG) focuses on detecting objects in a video and determining their pairwise relationships. Existing dynamic SGG methods usually suffer from several issues, including 1) Contextual noise, as some frames might contain occluded and blurred objects. 2) Label bias, primarily due to the high imbalance between a few positive relationship samples and numerous negative ones. Additionally, the distribution of relationships exhibits a long-tailed pattern. To address the above problems, in this paper, we introduce a network named TD2-Net that aims at denoising and debiasing for dynamic SGG. Specifically, we first propose a denoising spatio-temporal transformer module that enhances object representation with robust contextual information. This is achieved by designing a differentiable Top-K object selector that utilizes the gumbel-softmax sampling strategy to select the relevant neighborhood for each object. Second, we introduce an asymmetrical reweighting loss to relieve the issue of label bias. This loss function integrates asymmetry focusing factors and the volume of samples to adjust the weights assigned to individual samples. Systematic experimental results demonstrate the superiority of our proposed TD2-Net over existing state-of-the-art approaches on Action Genome databases. In more detail, TD2-Net outperforms the second-best competitors by 12.7% on mean-Recall@10 for predicate classification",
    "checked": true,
    "id": "a8d009ad768cc0e34624374fd277fecda013860b",
    "semantic_title": "td²-net: toward denoising and debiasing for video scene graph generation",
    "citation_count": 0,
    "authors": [
      "Xin Lin",
      "Chong Shi",
      "Yibing Zhan",
      "Zuopeng Yang",
      "Yaqi Wu",
      "Dacheng Tao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28138": {
    "title": "Ced-NeRF: A Compact and Efficient Method for Dynamic Neural Radiance Fields",
    "volume": "main",
    "abstract": "Rendering photorealistic dynamic scenes has been a focus of recent research, with applications in virtual and augmented reality. While the Neural Radiance Field (NeRF) has shown remarkable rendering quality for static scenes, achieving real-time rendering of dynamic scenes remains challenging due to expansive computation for the time dimension. The incorporation of explicit-based methods, specifically voxel grids, has been proposed to accelerate the training and rendering of neural radiance fields with hybrid representation. However, employing a hybrid representation for dynamic scenes results in overfitting due to fast convergence, which can result in artifacts (e.g., floaters, noisy geometric) on novel views. To address this, we propose a compact and efficient method for dynamic neural radiance fields, namely Ced-NeRF which only require a small number of additional parameters to construct a hybrid representation of dynamic NeRF. Evaluation of dynamic scene datasets shows that our Ced-NeRF achieves fast rendering speeds while maintaining high-quality rendering results. Our method outperforms the current state-of-the-art methods in terms of quality, training and rendering speed",
    "checked": true,
    "id": "9b7d52f65fcc5184929a8e20c141752c5a094299",
    "semantic_title": "ced-nerf: a compact and efficient method for dynamic neural radiance fields",
    "citation_count": 0,
    "authors": [
      "Youtian Lin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28139": {
    "title": "TagCLIP: A Local-to-Global Framework to Enhance Open-Vocabulary Multi-Label Classification of CLIP without Training",
    "volume": "main",
    "abstract": "Contrastive Language-Image Pre-training (CLIP) has demonstrated impressive capabilities in open-vocabulary classification. The class token in the image encoder is trained to capture the global features to distinguish different text descriptions supervised by contrastive loss, making it highly effective for single-label classification. However, it shows poor performance on multi-label datasets because the global feature tends to be dominated by the most prominent class and the contrastive nature of softmax operation aggravates it. In this study, we observe that the multi-label classification results heavily rely on discriminative local features but are overlooked by CLIP. As a result, we dissect the preservation of patch-wise spatial information in CLIP and proposed a local-to-global framework to obtain image tags. It comprises three steps: (1) patch-level classification to obtain coarse scores; (2) dual-masking attention refinement (DMAR) module to refine the coarse scores; (3) class-wise reidentification (CWR) module to remedy predictions from a global perspective. This framework is solely based on frozen CLIP and significantly enhances its multi-label classification performance on various benchmarks without dataset-specific training. Besides, to comprehensively assess the quality and practicality of generated tags, we extend their application to the downstream task, i.e., weakly supervised semantic segmentation (WSSS) with generated tags as image-level pseudo labels. Experiments demonstrate that this classify-then-segment paradigm dramatically outperforms other annotation-free segmentation methods and validates the effectiveness of generated tags. Our code is available at https://github.com/linyq2117/TagCLIP",
    "checked": true,
    "id": "a08bbdbf885c7a6fb5499dccae24982770abd909",
    "semantic_title": "tagclip: a local-to-global framework to enhance open-vocabulary multi-label classification of clip without training",
    "citation_count": 2,
    "authors": [
      "Yuqi Lin",
      "Minghao Chen",
      "Kaipeng Zhang",
      "Hengjia Li",
      "Mingming Li",
      "Zheng Yang",
      "Dongqin Lv",
      "Binbin Lin",
      "Haifeng Liu",
      "Deng Cai"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28140": {
    "title": "Independency Adversarial Learning for Cross-Modal Sound Separation",
    "volume": "main",
    "abstract": "The sound mixture separation is still challenging due to heavy sound overlapping and disturbance from noise. Unsupervised separation would significantly increase the difficulty. As sound overlapping always hinders accurate sound separation, we propose an Independency Adversarial Learning based Cross-Modal Sound Separation (IAL-CMS) approach, where IAL employs adversarial learning to minimize the correlation of separated sound elements, exploring high sound independence; CMS performs cross-modal sound separation, incorporating audio-visual consistent feature learning and interactive cross-attention learning to emphasize the semantic consistency among cross-modal features. Both audio-visual consistency and audio consistency are kept to guarantee accurate separation. The consistency and sound independence ensure the decomposition of overlapping mixtures into unrelated and distinguishable sound elements. The proposed approach is evaluated on MUSIC, VGGSound, and AudioSet. Extensive experiments certify that our approach outperforms existing approaches in supervised and unsupervised scenarios",
    "checked": true,
    "id": "a2db1363fee616485ba3ac17710fe62463bee073",
    "semantic_title": "independency adversarial learning for cross-modal sound separation",
    "citation_count": 0,
    "authors": [
      "Zhenkai Lin",
      "Yanli Ji",
      "Yang Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28141": {
    "title": "BEV-MAE: Bird's Eye View Masked Autoencoders for Point Cloud Pre-training in Autonomous Driving Scenarios",
    "volume": "main",
    "abstract": "Existing LiDAR-based 3D object detection methods for autonomous driving scenarios mainly adopt the training-from-scratch paradigm. Unfortunately, this paradigm heavily relies on large-scale labeled data, whose collection can be expensive and time-consuming. Self-supervised pre-training is an effective and desirable way to alleviate this dependence on extensive annotated data. In this work, we present BEV-MAE, an efficient masked autoencoder pre-training framework for LiDAR-based 3D object detection in autonomous driving. Specifically, we propose a bird's eye view (BEV) guided masking strategy to guide the 3D encoder learning feature representation in a BEV perspective and avoid complex decoder design during pre-training. Furthermore, we introduce a learnable point token to maintain a consistent receptive field size of the 3D encoder with fine-tuning for masked point cloud inputs. Based on the property of outdoor point clouds in autonomous driving scenarios, i.e., the point clouds of distant objects are more sparse, we propose point density prediction to enable the 3D encoder to learn location information, which is essential for object detection. Experimental results show that BEV-MAE surpasses prior state-of-the-art self-supervised methods and achieves a favorably pre-training efficiency. Furthermore, based on TransFusion-L, BEV-MAE achieves new state-of-the-art LiDAR-based 3D object detection results, with 73.6 NDS and 69.6 mAP on the nuScenes benchmark. The source code will be released at https://github.com/VDIGPKU/BEV-MAE",
    "checked": true,
    "id": "b58f0b97f13ce8b327285c77828858457e86862a",
    "semantic_title": "bev-mae: bird's eye view masked autoencoders for point cloud pre-training in autonomous driving scenarios",
    "citation_count": 7,
    "authors": [
      "Zhiwei Lin",
      "Yongtao Wang",
      "Shengxiang Qi",
      "Nan Dong",
      "Ming-Hsuan Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28142": {
    "title": "Focus Stacking with High Fidelity and Superior Visual Effects",
    "volume": "main",
    "abstract": "Focus stacking is a technique in computational photography, and it synthesizes a single all-in-focus image from different focal plane images. It is difficult for previous works to produce a high-quality all-in-focus image that meets two goals: high-fidelity to its source images and good visual effects without defects or abnormalities. This paper proposes a novel method based on optical imaging process analysis and modeling. Based on a foreground segmentation - diffusion elimination architecture, the foreground segmentation makes most of the areas in full-focus images heritage information from the source images to achieve high fidelity; diffusion elimination models the physical imaging process and is specially used to solve the transition region (TR) problem that is a long-term neglected issue and degrades visual effects of synthesized images. Based on extensive experiments on simulated dataset, existing realistic dataset and our proposed BetaFusion dataset, the results show that our proposed method can generate high-quality all-in-focus images by achieving two goals simultaneously, especially can successfully solve the TR problem and eliminate the visual effect degradation of synthesized images caused by the TR problem",
    "checked": true,
    "id": "100a0c96cbe9898a6369552297f9f69d150d043c",
    "semantic_title": "focus stacking with high fidelity and superior visual effects",
    "citation_count": 0,
    "authors": [
      "Bo Liu",
      "Bin Hu",
      "Xiuli Bi",
      "Weisheng Li",
      "Bin Xiao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28143": {
    "title": "DeepBranchTracer: A Generally-Applicable Approach to Curvilinear Structure Reconstruction Using Multi-Feature Learning",
    "volume": "main",
    "abstract": "Curvilinear structures, which include line-like continuous objects, are fundamental geometrical elements in image-based applications. Reconstructing these structures from images constitutes a pivotal research area in computer vision. However, the complex topology and ambiguous image evidence render this process a challenging task. In this paper, we introduce DeepBranchTracer, a novel method that learns both external image features and internal geometric characteristics to reconstruct curvilinear structures. Firstly, we formulate the curvilinear structures extraction as a geometric attribute estimation problem. Then, a curvilinear structure feature learning network is designed to extract essential branch attributes, including the image features of centerline and boundary, and the geometric features of direction and radius. Finally, utilizing a multi-feature fusion tracing strategy, our model iteratively traces the entire branch by integrating the extracted image and geometric features. We extensively evaluated our model on both 2D and 3D datasets, demonstrating its superior performance over existing segmentation and reconstruction methods in terms of accuracy and continuity",
    "checked": true,
    "id": "e6e59bcf20541a6a7e26dd2108f120803c0f6736",
    "semantic_title": "deepbranchtracer: a generally-applicable approach to curvilinear structure reconstruction using multi-feature learning",
    "citation_count": 0,
    "authors": [
      "Chao Liu",
      "Ting Zhao",
      "Nenggan Zheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28144": {
    "title": "Decoupling Degradations with Recurrent Network for Video Restoration in Under-Display Camera",
    "volume": "main",
    "abstract": "Under-display camera (UDC) systems are the foundation of full-screen display devices in which the lens mounts under the display. The pixel array of light-emitting diodes used for display diffracts and attenuates incident light, causing various degradations as the light intensity changes. Unlike general video restoration which recovers video by treating different degradation factors equally, video restoration for UDC systems is more challenging that concerns removing diverse degradation over time while preserving temporal consistency. In this paper, we introduce a novel video restoration network, called D2RNet, specifically designed for UDC systems. It employs a set of Decoupling Attention Modules (DAM) that effectively separate the various video degradation factors. More specifically, a soft mask generation function is proposed to formulate each frame into flare and haze based on the diffraction arising from incident light of different intensities, followed by the proposed flare and haze removal components that leverage long- and short-term feature learning to handle the respective degradations. Such a design offers an targeted and effective solution to eliminating various types of degradation in UDC systems. We further extend our design into multi-scale to overcome the scale-changing of degradation that often occur in long-range videos. To demonstrate the superiority of D2RNet, we propose a large-scale UDC video benchmark by gathering HDR videos and generating realistically degraded videos using the point spread function measured by a commercial UDC system. Extensive quantitative and qualitative evaluations demonstrate the superiority of D2RNet compared to other state-of-the-art video restoration and UDC image restoration methods",
    "checked": true,
    "id": "5939c181212d8ebeee3e81b3b77660aed6b682f4",
    "semantic_title": "decoupling degradations with recurrent network for video restoration in under-display camera",
    "citation_count": 1,
    "authors": [
      "Chengxu Liu",
      "Xuan Wang",
      "Yuanting Fan",
      "Shuai Li",
      "Xueming Qian"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28145": {
    "title": "Unsupervised Domain Adaptative Temporal Sentence Localization with Mutual Information Maximization",
    "volume": "main",
    "abstract": "Temporal sentence localization (TSL) aims to localize a target segment in a video according to a given sentence query. Though respectable works have made decent achievements in this task, they severely rely on abundant yet expensive manual annotations for training. Moreover, these trained data-dependent models usually can not generalize well to unseen scenarios because of the inherent domain shift. To facilitate this issue, in this paper, we target another more practical but challenging setting: unsupervised domain adaptative temporal sentence localization (UDA-TSL), which explores whether the localization knowledge can be transferred from a fully-annotated data domain (source domain) to a new unannotated data domain (target domain). Particularly, we propose an effective and novel baseline for UDA-TSL to bridge the multi-modal gap across different domains and learn the potential correspondence between the video-query pairs in target domain. We first develop separate modality-specific domain adaptation modules to smoothly balance the minimization of the domain shifts in cross-dataset video and query domains. Then, to fully exploit the semantic correspondence of both modalities in target domain for unsupervised localization, we devise a mutual information learning module to adaptively align the video-query pairs which are more likely to be relevant in target domain, leading to more truly aligned target pairs and ensuring the discriminability of target features. In this way, our model can learn domain-invariant and semantic-aligned cross-modal representations. Three sets of migration experiments show that our model achieves competitive performance compared to existing methods",
    "checked": true,
    "id": "745e525afca4ba6fe5d7e07473288c36400da5bd",
    "semantic_title": "unsupervised domain adaptative temporal sentence localization with mutual information maximization",
    "citation_count": 0,
    "authors": [
      "Daizong Liu",
      "Xiang Fang",
      "Xiaoye Qu",
      "Jianfeng Dong",
      "He Yan",
      "Yang Yang",
      "Pan Zhou",
      "Yu Cheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28146": {
    "title": "Explicitly Perceiving and Preserving the Local Geometric Structures for 3D Point Cloud Attack",
    "volume": "main",
    "abstract": "Deep learning models for point clouds have shown to be vulnerable to adversarial attacks, which have received increasing attention in various safety-critical applications such as autonomous driving, robotics, and surveillance. Existing 3D attack methods generally employ global distance losses to implicitly constrain the point-wise perturbations for optimization. However, these simple losses are quite difficult to accurately measure and restrict the proper 3D geometry as point clouds are highly structured. Although few recent works try to exploit additional shape-aware surface knowledge to globally constrain the point position, they still fail to preserve the detailed point-to-point geometric dependency in different local regions. To this end, in this paper, we propose a novel Multi-grained Geometry-aware Attack (MGA), which explicitly captures the local topology characteristics in different 3D regions for adversarial constraint. Specifically, we first develop multi-scale spectral local filter banks adapting to different 3D object shapes to explore potential geometric structures in local regions. Considering that objects may contain complex geometries, we then extend each filter bank into multi-layer ones to gradually capture the topology contexts of the same region in a coarse-to-fine manner. Hence, the focused local geometric structures will be highlighted in the coefficients calculated by the filtering process. At last, by restricting these coefficients between benign and adversarial samples, our MGA is able to properly measure and preserve the detailed geometry contexts in the whole 3D object with trivial perturbations. Extensive experiments demonstrate that our attack can achieve superior performance on various 3D classification models, with satisfying adversarial imperceptibility and strong resistance to different defense methods",
    "checked": true,
    "id": "20581baf00920643d80076f3dc7b976469e58771",
    "semantic_title": "explicitly perceiving and preserving the local geometric structures for 3d point cloud attack",
    "citation_count": 0,
    "authors": [
      "Daizong Liu",
      "Wei Hu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28147": {
    "title": "Adv-Diffusion: Imperceptible Adversarial Face Identity Attack via Latent Diffusion Model",
    "volume": "main",
    "abstract": "Adversarial attacks involve adding perturbations to the source image to cause misclassification by the target model, which demonstrates the potential of attacking face recognition models. Existing adversarial face image generation methods still can't achieve satisfactory performance because of low transferability and high detectability. In this paper, we propose a unified framework Adv-Diffusion that can generate imperceptible adversarial identity perturbations in the latent space but not the raw pixel space, which utilizes strong inpainting capabilities of the latent diffusion model to generate realistic adversarial images. Specifically, we propose the identity-sensitive conditioned diffusion generative model to generate semantic perturbations in the surroundings. The designed adaptive strength-based adversarial perturbation algorithm can ensure both attack transferability and stealthiness. Extensive qualitative and quantitative experiments on the public FFHQ and CelebA-HQ datasets prove the proposed method achieves superior performance compared with the state-of-the-art methods without an extra generative model training process. The source code is available at https://github.com/kopper-xdu/Adv-Diffusion",
    "checked": true,
    "id": "82664094523e5b3efa5f073b6a10c7131d73d1fe",
    "semantic_title": "adv-diffusion: imperceptible adversarial face identity attack via latent diffusion model",
    "citation_count": 0,
    "authors": [
      "Decheng Liu",
      "Xijun Wang",
      "Chunlei Peng",
      "Nannan Wang",
      "Ruimin Hu",
      "Xinbo Gao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28148": {
    "title": "Multi-View Dynamic Reflection Prior for Video Glass Surface Detection",
    "volume": "main",
    "abstract": "Recent research has shown significant interest in image-based glass surface detection (GSD). However, detecting glass surfaces in dynamic scenes remains largely unexplored due to the lack of a high-quality dataset and an effective video glass surface detection (VGSD) method. In this paper, we propose the first VGSD approach. Our key observation is that reflections frequently appear on glass surfaces, but they change dynamically as the camera moves. Based on this observation, we propose to offset the excessive dependence on a single uncertainty reflection via joint modeling of temporal and spatial reflection cues. To this end, we propose the VGSD-Net with two novel modules: a Location-aware Reflection Extraction (LRE) module and a Context-enhanced Reflection Integration (CRI) module, for the position-aware reflection feature extraction and the spatial-temporal reflection cues integration, respectively. We have also created the first large-scale video glass surface dataset (VGSD-D), consisting of 19,166 image frames with accurately-annotated glass masks extracted from 297 videos. Extensive experiments demonstrate that VGSD-Net outperforms state-of-the-art approaches adapted from related fields. Code and dataset will be available at https://github.com/fawnliu/VGSD",
    "checked": true,
    "id": "c0e277a7e8b94dd53f97fcf47005c71d916ff7e8",
    "semantic_title": "multi-view dynamic reflection prior for video glass surface detection",
    "citation_count": 1,
    "authors": [
      "Fang Liu",
      "Yuhao Liu",
      "Jiaying Lin",
      "Ke Xu",
      "Rynson W.H. Lau"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28149": {
    "title": "Grab What You Need: Rethinking Complex Table Structure Recognition with Flexible Components Deliberation",
    "volume": "main",
    "abstract": "Recently, Table Structure Recognition (TSR) task, aiming at identifying table structure into machine readable formats, has received increasing interest in the community. While impressive success, most single table component-based methods can not perform well on unregularized table cases distracted by not only complicated inner structure but also exterior capture distortion. In this paper, we raise it as Complex TSR problem, where the performance degeneration of existing methods is attributable to their inefficient component usage and redundant post-processing. To mitigate it, we shift our perspective from table component extraction towards the efficient multiple components leverage, which awaits further exploration in the field. Specifically, we propose a seminal method, termed GrabTab, equipped with newly proposed Component Deliberator, to handle various types of tables in a unified framework. Thanks to its progressive deliberation mechanism, our GrabTab can flexibly accommodate to most complex tables with reasonable components selected but without complicated post-processing involved. Quantitative experimental results on public benchmarks demonstrate that our method significantly outperforms the state-of-the-arts, especially under more challenging scenes",
    "checked": true,
    "id": "2c267f43aca9bfb2f01e5400fca14545c4aa14b3",
    "semantic_title": "grab what you need: rethinking complex table structure recognition with flexible components deliberation",
    "citation_count": 2,
    "authors": [
      "Hao Liu",
      "Xin Li",
      "Mingming Gong",
      "Bing Liu",
      "Yunfei Wu",
      "Deqiang Jiang",
      "Yinsong Liu",
      "Xing Sun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28150": {
    "title": "DiDA: Disambiguated Domain Alignment for Cross-Domain Retrieval with Partial Labels",
    "volume": "main",
    "abstract": "Driven by generative AI and the Internet, there is an increasing availability of a wide variety of images, leading to the significant and popular task of cross-domain image retrieval. To reduce annotation costs and increase performance, this paper focuses on an untouched but challenging problem, i.e., cross-domain image retrieval with partial labels (PCIR). Specifically, PCIR faces great challenges due to the ambiguous supervision signal and the domain gap. To address these challenges, we propose a novel method called disambiguated domain alignment (DiDA) for cross-domain retrieval with partial labels. In detail, DiDA elaborates a novel prototype-score unitization learning mechanism (PSUL) to extract common discriminative representations by simultaneously disambiguating the partial labels and narrowing the domain gap. Additionally, DiDA proposes a prototype-based domain alignment mechanism (PBDA) to further bridge the inherent cross-domain discrepancy. Attributed to PSUL and PBDA, our DiDA effectively excavates domain-invariant discrimination for cross-domain image retrieval. We demonstrate the effectiveness of DiDA through comprehensive experiments on three benchmarks, comparing it to existing state-of-the-art methods. Code available: https://github.com/lhrrrrrr/DiDA",
    "checked": true,
    "id": "9a08bc3699ec1ac3c8139c0e4b4ebd726a29f680",
    "semantic_title": "dida: disambiguated domain alignment for cross-domain retrieval with partial labels",
    "citation_count": 1,
    "authors": [
      "Haoran Liu",
      "Ying Ma",
      "Ming Yan",
      "Yingke Chen",
      "Dezhong Peng",
      "Xu Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28151": {
    "title": "Test-Time Personalization with Meta Prompt for Gaze Estimation",
    "volume": "main",
    "abstract": "Despite the recent remarkable achievement in gaze estimation, efficient and accurate personalization of gaze estimation without labels is a practical problem but rarely touched on in the literature. To achieve efficient personalization, we take inspiration from the recent advances in Natural Language Processing (NLP) by updating a negligible number of parameters, \"prompts\", at the test time. Specifically, the prompt is additionally attached without perturbing original network and can contain less than 1% of a ResNet-18's parameters. Our experiments show high efficiency of the prompt tuning approach. The proposed one can be 10 times faster in terms of adaptation speed than the methods compared. However, it is non-trivial to update the prompt for personalized gaze estimation without labels. At the test time, it is essential to ensure that the minimizing of particular unsupervised loss leads to the goals of minimizing gaze estimation error. To address this difficulty, we propose to meta-learn the prompt to ensure that its updates align with the goal. Our experiments show that the meta-learned prompt can be effectively adapted even with a simple symmetry loss. In addition, we experiment on four cross-dataset validations to show the remarkable advantages of the proposed method",
    "checked": true,
    "id": "6923ebb5c4b376eb6e7c088376af5601ed6e292e",
    "semantic_title": "test-time personalization with meta prompt for gaze estimation",
    "citation_count": 2,
    "authors": [
      "Huan Liu",
      "Julia Qi",
      "Zhenhao Li",
      "Mohammad Hassanpour",
      "Yang Wang",
      "Konstantinos N. Plataniotis",
      "Yuanhao Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28152": {
    "title": "M3SOT: Multi-Frame, Multi-Field, Multi-Space 3D Single Object Tracking",
    "volume": "main",
    "abstract": "3D Single Object Tracking (SOT) stands a forefront task of computer vision, proving essential for applications like autonomous driving. Sparse and occluded data in scene point clouds introduce variations in the appearance of tracked objects, adding complexity to the task. In this research, we unveil M3SOT, a novel 3D SOT framework, which synergizes multiple input frames (template sets), multiple receptive fields (continuous contexts), and multiple solution spaces (distinct tasks) in ONE model. Remarkably, M3SOT pioneers in modeling temporality, contexts, and tasks directly from point clouds, revisiting a perspective on the key factors influencing SOT. To this end, we design a transformer-based network centered on point cloud targets in the search area, aggregating diverse contextual representations and propagating target cues by employing historical frames. As M3SOT spans varied processing perspectives, we've streamlined the network—trimming its depth and optimizing its structure—to ensure a lightweight and efficient deployment for SOT applications. We posit that, backed by practical construction, M3SOT sidesteps the need for complex frameworks and auxiliary components to deliver sterling results. Extensive experiments on benchmarks such as KITTI, nuScenes, and Waymo Open Dataset demonstrate that M3SOT achieves state-of-the-art performance at 38 FPS. Our code and models are available at https://github.com/ywu0912/TeamCode.git",
    "checked": true,
    "id": "a1c51a4763b621426ec59d0f9d9a2bc9c712f133",
    "semantic_title": "m3sot: multi-frame, multi-field, multi-space 3d single object tracking",
    "citation_count": 1,
    "authors": [
      "Jiaming Liu",
      "Yue Wu",
      "Maoguo Gong",
      "Qiguang Miao",
      "Wenping Ma",
      "Cai Xu",
      "Can Qin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28153": {
    "title": "Unsupervised Continual Anomaly Detection with Contrastively-Learned Prompt",
    "volume": "main",
    "abstract": "Unsupervised Anomaly Detection (UAD) with incremental training is crucial in industrial manufacturing, as unpredictable defects make obtaining sufficient labeled data infeasible. However, continual learning methods primarily rely on supervised annotations, while the application in UAD is limited due to the absence of supervision. Current UAD methods train separate models for different classes sequentially, leading to catastrophic forgetting and a heavy computational burden. To address this issue, we introduce a novel Unsupervised Continual Anomaly Detection framework called UCAD, which equips the UAD with continual learning capability through contrastively-learned prompts. In the proposed UCAD, we design a Continual Prompting Module (CPM) by utilizing a concise key-prompt-knowledge memory bank to guide task-invariant 'anomaly' model predictions using task-specific 'normal' knowledge. Moreover, Structure-based Contrastive Learning (SCL) is designed with the Segment Anything Model (SAM) to improve prompt learning and anomaly segmentation results. Specifically, by treating SAM's masks as structure, we draw features within the same mask closer and push others apart for general feature representations. We conduct comprehensive experiments and set the benchmark on unsupervised continual anomaly detection and segmentation, demonstrating that our method is significantly better than anomaly detection methods, even with rehearsal training. The code will be available at https://github.com/shirowalker/UCAD",
    "checked": true,
    "id": "32e0218de758ec85a5531834bbaa60448032971b",
    "semantic_title": "unsupervised continual anomaly detection with contrastively-learned prompt",
    "citation_count": 1,
    "authors": [
      "Jiaqi Liu",
      "Kai Wu",
      "Qiang Nie",
      "Ying Chen",
      "Bin-Bin Gao",
      "Yong Liu",
      "Jinbao Wang",
      "Chengjie Wang",
      "Feng Zheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28154": {
    "title": "Region-Aware Exposure Consistency Network for Mixed Exposure Correction",
    "volume": "main",
    "abstract": "Exposure correction aims to enhance images suffering from improper exposure to achieve satisfactory visual effects. Despite recent progress, existing methods generally mitigate either overexposure or underexposure in input images, and they still struggle to handle images with mixed exposure, i.e., one image incorporates both overexposed and underexposed regions. The mixed exposure distribution is non-uniform and leads to varying representation, which makes it challenging to address in a unified process. In this paper, we introduce an effective Region-aware Exposure Correction Network (RECNet) that can handle mixed exposure by adaptively learning and bridging different regional exposure representations. Specifically, to address the challenge posed by mixed exposure disparities, we develop a region-aware de-exposure module that effectively translates regional features of mixed exposure scenarios into an exposure-invariant feature space. Simultaneously, as de-exposure operation inevitably reduces discriminative information, we introduce a mixed-scale restoration unit that integrates exposure-invariant features and unprocessed features to recover local information. To further achieve a uniform exposure distribution in the global image, we propose an exposure contrastive regularization strategy under the constraints of intra-regional exposure consistency and inter-regional exposure continuity. Extensive experiments are conducted on various datasets, and the experimental results demonstrate the superiority and generalization of our proposed method. The code is released at: https://github.com/kravrolens/RECNet",
    "checked": true,
    "id": "04ac6a5d9892840e2a2520fe7f601fa188e05267",
    "semantic_title": "region-aware exposure consistency network for mixed exposure correction",
    "citation_count": 1,
    "authors": [
      "Jin Liu",
      "Huiyuan Fu",
      "Chuanming Wang",
      "Huadong Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28155": {
    "title": "R3CD: Scene Graph to Image Generation with Relation-Aware Compositional Contrastive Control Diffusion",
    "volume": "main",
    "abstract": "Image generation tasks have achieved remarkable performance using large-scale diffusion models. However, these models are limited to capturing the abstract relations (viz., interactions excluding positional relations) among multiple entities of complex scene graphs. Two main problems exist: 1) fail to depict more concise and accurate interactions via abstract relations; 2) fail to generate complete entities. To address that, we propose a novel Relation-aware Compositional Contrastive Control Diffusion method, dubbed as R3CD, that leverages large-scale diffusion models to learn abstract interactions from scene graphs. Herein, a scene graph transformer based on node and edge encoding is first designed to perceive both local and global information from input scene graphs, whose embeddings are initialized by a T5 model. Then a joint contrastive loss based on attention maps and denoising steps is developed to control the diffusion model to understand and further generate images, whose spatial structures and interaction features are consistent with a priori relation. Extensive experiments are conducted on two datasets: Visual Genome and COCO-Stuff, and demonstrate that the proposal outperforms existing models both in quantitative and qualitative metrics to generate more realistic and diverse images according to different scene graph specifications",
    "checked": true,
    "id": "1084b1a30c195cef4b4a3125aa8f8d26180550ee",
    "semantic_title": "r3cd: scene graph to image generation with relation-aware compositional contrastive control diffusion",
    "citation_count": 1,
    "authors": [
      "Jinxiu Liu",
      "Qi Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28156": {
    "title": "DifAttack: Query-Efficient Black-Box Adversarial Attack via Disentangled Feature Space",
    "volume": "main",
    "abstract": "This work investigates efficient score-based black-box adversarial attacks with high Attack Success Rate (ASR) and good generalizability. We design a novel attack method based on a Disentangled Feature space, called DifAttack, which differs significantly from the existing ones operating over the entire feature space. Specifically, DifAttack firstly disentangles an image's latent feature into an adversarial feature and a visual feature, where the former dominates the adversarial capability of an image, while the latter largely determines its visual appearance. We train an autoencoder for the disentanglement by using pairs of clean images and their Adversarial Examples (AEs) generated from available surrogate models via white-box attack methods. Eventually, DifAttack iteratively optimizes the adversarial feature according to the query feedback from the victim model until a successful AE is generated, while keeping the visual feature unaltered. In addition, due to the avoidance of using surrogate models' gradient information when optimizing AEs for black-box models, our proposed DifAttack inherently possesses better attack capability in the open-set scenario, where the training dataset of the victim model is unknown. Extensive experimental results demonstrate that our method achieves significant improvements in ASR and query efficiency simultaneously, especially in the targeted attack and open-set scenarios. The code is available The code is available at https://github.com/csjunjun/DifAttack.git",
    "checked": false,
    "id": "b2355f0db07e7c8564d2bff1a9699fba2a886ea3",
    "semantic_title": "difattack: query-efficient black-box attack via disentangled feature space",
    "citation_count": 1,
    "authors": [
      "Jun Liu",
      "Jiantao Zhou",
      "Jiandian Zeng",
      "Jinyu Tian"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28157": {
    "title": "Frequency Shuffling and Enhancement for Open Set Recognition",
    "volume": "main",
    "abstract": "Open-Set Recognition (OSR) aims to accurately identify known classes while effectively rejecting unknown classes to guarantee reliability. Most existing OSR methods focus on learning in the spatial domain, where subtle texture and global structure are potentially intertwined. Empirical studies have shown that DNNs trained in the original spatial domain are inclined to over-perceive subtle texture. The biased semantic perception could lead to catastrophic over-confidence when predicting both known and unknown classes. To this end, we propose an innovative approach by decomposing the spatial domain to the frequency domain to separately consider global (low-frequency) and subtle (high-frequency) information, named Frequency Shuffling and Enhancement (FreSH). To alleviate the overfitting of subtle texture, we introduce the High-Frequency Shuffling (HFS) strategy that generates diverse high-frequency information and promotes the capture of low-frequency invariance. Moreover, to enhance the perception of global structure, we propose the Low-Frequency Residual (LFR) learning procedure that constructs a composite feature space, integrating low-frequency and original spatial features. Experiments on various benchmarks demonstrate that the proposed FreSH consistently trumps the state-of-the-arts by a considerable margin",
    "checked": true,
    "id": "a8392919afe17505242d047ce78ba3bfe82813be",
    "semantic_title": "frequency shuffling and enhancement for open set recognition",
    "citation_count": 1,
    "authors": [
      "Lijun Liu",
      "Rui Wang",
      "Yuan Wang",
      "Lihua Jing",
      "Chuan Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28158": {
    "title": "KPA-Tracker: Towards Robust and Real-Time Category-Level Articulated Object 6D Pose Tracking",
    "volume": "main",
    "abstract": "Our life is populated with articulated objects. Current category-level articulation estimation works largely focus on predicting part-level 6D poses on static point cloud observations. In this paper, we tackle the problem of category-level online robust and real-time 6D pose tracking of articulated objects, where we propose KPA-Tracker, a novel 3D KeyPoint based Articulated object pose Tracker. Given an RGB-D image or a partial point cloud at the current frame as well as the estimated per-part 6D poses from the last frame, our KPA-Tracker can effectively update the poses with learned 3D keypoints between the adjacent frames. Specifically, we first canonicalize the input point cloud and formulate the pose tracking as an inter-frame pose increment estimation task. To learn consistent and separate 3D keypoints for every rigid part, we build KPA-Gen that outputs the high-quality ordered 3D keypoints in an unsupervised manner. During pose tracking on the whole video, we further propose a keypoint-based articulation tracking algorithm that mines keyframes as reference for accurate pose updating. We provide extensive experiments on validating our KPA-Tracker on various datasets ranging from synthetic point cloud observation to real-world scenarios, which demonstrates the superior performance and robustness of the KPA-Tracker. We believe that our work has the potential to be applied in many fields including robotics, embodied intelligence and augmented reality. All the datasets and codes are available at https://github.com/hhhhhar/KPA-Tracker",
    "checked": true,
    "id": "18c970b92e891dc55cf5dd9d0bd01ba8f6f18edd",
    "semantic_title": "kpa-tracker: towards robust and real-time category-level articulated object 6d pose tracking",
    "citation_count": 0,
    "authors": [
      "Liu Liu",
      "Anran Huang",
      "Qi Wu",
      "Dan Guo",
      "Xun Yang",
      "Meng Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28159": {
    "title": "UVAGaze: Unsupervised 1-to-2 Views Adaptation for Gaze Estimation",
    "volume": "main",
    "abstract": "Gaze estimation has become a subject of growing interest in recent research. Most of the current methods rely on single-view facial images as input. Yet, it is hard for these approaches to handle large head angles, leading to potential inaccuracies in the estimation. To address this issue, adding a second-view camera can help better capture eye appearance. However, existing multi-view methods have two limitations. 1) They require multi-view annotations for training, which are expensive. 2) More importantly, during testing, the exact positions of the multiple cameras must be known and match those used in training, which limits the application scenario. To address these challenges, we propose a novel 1-view-to-2-views (1-to-2 views) adaptation solution in this paper, the Unsupervised 1-to-2 Views Adaptation framework for Gaze estimation (UVAGaze). Our method adapts a traditional single-view gaze estimator for flexibly placed dual cameras. Here, the \"flexibly\" means we place the dual cameras in arbitrary places regardless of the training data, without knowing their extrinsic parameters. Specifically, the UVAGaze builds a dual-view mutual supervision adaptation strategy, which takes advantage of the intrinsic consistency of gaze directions between both views. In this way, our method can not only benefit from common single-view pre-training, but also achieve more advanced dual-view gaze estimation. The experimental results show that a single-view estimator, when adapted for dual views, can achieve much higher accuracy, especially in cross-dataset settings, with a substantial improvement of 47.0%. Project page: https://github.com/MickeyLLG/UVAGaze",
    "checked": true,
    "id": "91922a69ac7cc34659771fec1a481a9a3087708b",
    "semantic_title": "uvagaze: unsupervised 1-to-2 views adaptation for gaze estimation",
    "citation_count": 2,
    "authors": [
      "Ruicong Liu",
      "Feng Lu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28160": {
    "title": "Compact HD Map Construction via Douglas-Peucker Point Transformer",
    "volume": "main",
    "abstract": "High-definition (HD) map construction requires a comprehensive understanding of traffic environments, encompassing centimeter-level localization and rich semantic information. Previous works face challenges in redundant point representation or high-complexity curve modeling. In this paper, we present a flexible yet effective map element detector that synthesizes hierarchical information with a compact Douglas-Peucker (DP) point representation in a transformer architecture for robust and reliable predictions. Specifically, our proposed representation approximates class-agnostic map elements with DP points, which are sparsely located in crucial positions of structures and can get rid of redundancy and complexity. Besides, we design a position constraint with uncertainty to avoid potential ambiguities. Moreover, pairwise-point shape matching constraints are proposed to balance local structural information of different scales. Experiments on the public nuScenes dataset demonstrate that our method overwhelms current SOTAs. Extensive ablation studies validate each component of our methods. Codes will be released at https://github.com/sweety121/DPFormer",
    "checked": true,
    "id": "ae55213e149c7f68fa915ee6de2b7699c989a298",
    "semantic_title": "compact hd map construction via douglas-peucker point transformer",
    "citation_count": 0,
    "authors": [
      "Ruixin Liu",
      "Zejian Yuan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28161": {
    "title": "Primitive-Based 3D Human-Object Interaction Modelling and Programming",
    "volume": "main",
    "abstract": "Embedding Human and Articulated Object Interaction (HAOI) in 3D is an important direction for a deeper human activity understanding. Different from previous works that use parametric and CAD models to represent humans and objects, in this work, we propose a novel 3D geometric primitive-based language to encode both humans and objects. Given our new paradigm, humans and objects are all compositions of primitives instead of heterogeneous entities. Thus, mutual information learning may be achieved between the limited 3D data of humans and different object categories. Moreover, considering the simplicity of the expression and the richness of the information it contains, we choose the superquadric as the primitive representation. To explore an effective embedding of HAOI for the machine, we build a new benchmark on 3D HAOI consisting of primitives together with their images and propose a task requiring machines to recover 3D HAOI using primitives from images. Moreover, we propose a baseline of single-view 3D reconstruction on HAOI. We believe this primitive-based 3D HAOI representation would pave the way for 3D HAOI studies. Our code and data are available at https://mvig-rhos.com/p3haoi",
    "checked": true,
    "id": "afe1fb2de7adb9ad3231cb7b68d8c023066de887",
    "semantic_title": "primitive-based 3d human-object interaction modelling and programming",
    "citation_count": 1,
    "authors": [
      "Siqi Liu",
      "Yong-Lu Li",
      "Zhou Fang",
      "Xinpeng Liu",
      "Yang You",
      "Cewu Lu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28162": {
    "title": "Fast Inter-frame Motion Prediction for Compressed Dynamic Point Cloud Attribute Enhancement",
    "volume": "main",
    "abstract": "Recent years have witnessed the success of deep learning methods in quality enhancement of compressed point cloud. However, existing methods focus on geometry and attribute enhancement of single-frame point cloud. This paper proposes a novel compressed quality enhancement method for dynamic point cloud (DAE-MP). Specifically, we propose a fast inter-frame motion prediction module (IFMP) to explicitly estimate motion displacement and achieve inter-frame feature alignment. To maintain motion continuity between consecutive frames, we propose a motion consistency loss for supervised learning. Furthermore, a frequency component separation and fusion module is designed to extract rich frequency features adaptively. To the best of our knowledge, the proposed method is the first deep learning-based work to enhance the quality for compressed dynamic point cloud. Experimental results show that the proposed method can greatly improve the quality of compressed dynamic point cloud and provide a fast and efficient motion prediction plug-in for large-scale point cloud. For dynamic point cloud attribute with severely compressed artifact, our proposed DAE-MP method achieves up to 0.52dB (PSNR) performance gain. Moreover, the proposed IFMP module has a certain real-time processing ability for calculating the motion offset between dynamic point cloud frame",
    "checked": true,
    "id": "d434d5b7d201d8858dfee23faf81a159c1b0e3ab",
    "semantic_title": "fast inter-frame motion prediction for compressed dynamic point cloud attribute enhancement",
    "citation_count": 0,
    "authors": [
      "Wang Liu",
      "Wei Gao",
      "Xingming Mu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28163": {
    "title": "RWMS: Reliable Weighted Multi-Phase for Semi-supervised Segmentation",
    "volume": "main",
    "abstract": "Semantic segmentation is one of the tasks concerned in the field of computer vision. However, the cost of capturing large numbers of pixel-level annotations is expensive. Semi-supervised learning can utilize labeled and unlabeled data, providing new ideas for solving the problem of insufficient labeled data. In this work, we propose a data-reliability weighted multi-phase learning method for semi-supervised segmentation (RWMS). Under the framework of self-training, we train two different teacher models to evaluate the reliability of pseudo labels. By selecting reliable data at the image level and reweighting pseudo labels at the pixel level, multi-phase training is guided to focus on more reliable knowledge. Besides, we also inject strong data augmentations on unlabeled images while training. Through extensive experiments, we demonstrate that our method performs remarkably well compared to baseline methods and substantially outperforms them, more than 3% on VOC and Cityscapes",
    "checked": true,
    "id": "0350080ec0db8dcb8d7a4b6530ab7ff1e9db8f2e",
    "semantic_title": "rwms: reliable weighted multi-phase for semi-supervised segmentation",
    "citation_count": 0,
    "authors": [
      "Wensi Liu",
      "Xiao-Yu Tang",
      "Chong Yang",
      "Chunjie Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28164": {
    "title": "Learning Real-World Image De-weathering with Imperfect Supervision",
    "volume": "main",
    "abstract": "Real-world image de-weathering aims at removing various undesirable weather-related artifacts. Owing to the impossibility of capturing image pairs concurrently, existing real-world de-weathering datasets often exhibit inconsistent illumination, position, and textures between the ground-truth images and the input degraded images, resulting in imperfect supervision. Such non-ideal supervision negatively affects the training process of learning-based de-weathering methods. In this work, we attempt to address the problem with a unified solution for various inconsistencies. Specifically, inspired by information bottleneck theory, we first develop a Consistent Label Constructor (CLC) to generate a pseudo-label as consistent as possible with the input degraded image while removing most weather-related degradation. In particular, multiple adjacent frames of the current input are also fed into CLC to enhance the pseudo-label. Then we combine the original imperfect labels and pseudo-labels to jointly supervise the de-weathering model by the proposed Information Allocation Strategy (IAS). During testing, only the de-weathering model is used for inference. Experiments on two real-world de-weathering datasets show that our method helps existing de-weathering models achieve better performance. Code is available at https://github.com/1180300419/imperfect-deweathering",
    "checked": true,
    "id": "38e8159a2642477b69c282039076b4a00ca91e41",
    "semantic_title": "learning real-world image de-weathering with imperfect supervision",
    "citation_count": 0,
    "authors": [
      "Xiaohui Liu",
      "Zhilu Zhang",
      "Xiaohe Wu",
      "Chaoyu Feng",
      "Xiaotao Wang",
      "Lei Lei",
      "Wangmeng Zuo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28165": {
    "title": "Differentiable Auxiliary Learning for Sketch Re-Identification",
    "volume": "main",
    "abstract": "Sketch re-identification (Re-ID) seeks to match pedestrians' photos from surveillance videos with corresponding sketches. However, we observe that existing works still have two critical limitations: (i) cross- and intra-modality discrepancies hinder the extraction of modality-shared features, (ii) standard triplet loss fails to constrain latent feature distribution in each modality with inadequate samples. To overcome the above issues, we propose a differentiable auxiliary learning network (DALNet) to explore a robust auxiliary modality for Sketch Re-ID. Specifically, for (i) we construct an auxiliary modality by using a dynamic auxiliary generator (DAG) to bridge the gap between sketch and photo modalities. The auxiliary modality highlights the described person in photos to mitigate background clutter and learns sketch style through style refinement. Moreover, a modality interactive attention module (MIA) is presented to align the features and learn the invariant patterns of two modalities by auxiliary modality. To address (ii), we propose a multi-modality collaborative learning scheme (MMCL) to align the latent distribution of three modalities. An intra-modality circle loss in MMCL brings learned global and modality-shared features of the same identity closer in the case of insufficient samples within each modality. Extensive experiments verify the superior performance of our DALNet over the state-of-the-art methods for Sketch Re-ID, and the generalization in sketch-based image retrieval and sketch-photo face recognition tasks",
    "checked": true,
    "id": "d02409a63b18078b9aeba8f1ece86c01683c89f8",
    "semantic_title": "differentiable auxiliary learning for sketch re-identification",
    "citation_count": 0,
    "authors": [
      "Xingyu Liu",
      "Xu Cheng",
      "Haoyu Chen",
      "Hao Yu",
      "Guoying Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28166": {
    "title": "Keypoint Fusion for RGB-D Based 3D Hand Pose Estimation",
    "volume": "main",
    "abstract": "Previous 3D hand pose estimation methods primarily rely on a single modality, either RGB or depth, and the comprehensive utilization of the dual modalities has not been extensively explored. RGB and depth data provide complementary information and thus can be fused to enhance the robustness of 3D hand pose estimation. However, there exist two problems for applying existing fusion methods in 3D hand pose estimation: redundancy of dense feature fusion and ambiguity of visual features. First, pixel-wise feature interactions introduce high computational costs and ineffective calculations of invalid pixels. Second, visual features suffer from ambiguity due to color and texture similarities, as well as depth holes and noise caused by frequent hand movements, which interferes with modeling cross-modal correlations. In this paper, we propose Keypoint-Fusion for RGB-D based 3D hand pose estimation, which leverages the unique advantages of dual modalities to mutually eliminate the feature ambiguity, and performs cross-modal feature fusion in a more efficient way. Specifically, we focus cross-modal fusion on sparse yet informative spatial regions (i.e. keypoints). Meanwhile, by explicitly extracting relatively more reliable information as disambiguation evidence, depth modality provides 3D geometric information for RGB feature pixels, and RGB modality complements the precise edge information lost due to the depth noise. Keypoint-Fusion achieves state-of-the-art performance on two challenging hand datasets, significantly decreasing the error compared with previous single-modal methods",
    "checked": true,
    "id": "85ba446ddc0879269841881b11bff9aa046f81f3",
    "semantic_title": "keypoint fusion for rgb-d based 3d hand pose estimation",
    "citation_count": 0,
    "authors": [
      "Xingyu Liu",
      "Pengfei Ren",
      "Yuanyuan Gao",
      "Jingyu Wang",
      "Haifeng Sun",
      "Qi Qi",
      "Zirui Zhuang",
      "Jianxin Liao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28167": {
    "title": "CAVEN: An Embodied Conversational Agent for Efficient Audio-Visual Navigation in Noisy Environments",
    "volume": "main",
    "abstract": "Audio-visual navigation of an agent towards locating an audio goal is a challenging task especially when the audio is sporadic or the environment is noisy. In this paper, we present CAVEN, a Conversation-based Audio-Visual Embodied Navigation framework in which the agent may interact with a human/oracle for solving the task of navigating to an audio goal. Specifically, CAVEN is modeled as a budget-aware partially observable semi-Markov decision process that implicitly learns the uncertainty in the audio-based navigation policy to decide when and how the agent may interact with the oracle. Our CAVEN agent can engage in fully-bidirectional natural language conversations by producing relevant questions and interpret free-form, potentially noisy responses from the oracle based on the audio-visual context. To enable such a capability, CAVEN is equipped with: i) a trajectory forecasting network that is grounded in audio-visual cues to produce a potential trajectory to the estimated goal, and (ii) a natural language based question generation and reasoning network to pose an interactive question to the oracle or interpret the oracle's response to produce navigation instructions. To train the interactive modules, we present a large scale dataset: AVN-Instruct, based on the Landmark-RxR dataset. To substantiate the usefulness of conversations, we present experiments on the benchmark audio-goal task using the SoundSpaces simulator under various noisy settings. Our results reveal that our fully-conversational approach leads to nearly an order-of-magnitude improvement in success rate, especially in localizing new sound sources and against methods that use only uni-directional interaction",
    "checked": true,
    "id": "f68bf1c9b0bd05f5421c9c42aabecdd26c91b612",
    "semantic_title": "caven: an embodied conversational agent for efficient audio-visual navigation in noisy environments",
    "citation_count": 0,
    "authors": [
      "Xiulong Liu",
      "Sudipta Paul",
      "Moitreya Chatterjee",
      "Anoop Cherian"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28168": {
    "title": "DeepCalliFont: Few-Shot Chinese Calligraphy Font Synthesis by Integrating Dual-Modality Generative Models",
    "volume": "main",
    "abstract": "Few-shot font generation, especially for Chinese calligraphy fonts, is a challenging and ongoing problem. With the help of prior knowledge that is mainly based on glyph consistency assumptions, some recently proposed methods can synthesize high-quality Chinese glyph images. However, glyphs in calligraphy font styles often do not meet these assumptions. To address this problem, we propose a novel model, DeepCalliFont, for few-shot Chinese calligraphy font synthesis by integrating dual-modality generative models. Specifically, the proposed model consists of image synthesis and sequence generation branches, generating consistent results via a dual-modality representation learning strategy. The two modalities (i.e., glyph images and writing sequences) are properly integrated using a feature recombination module and a rasterization loss function. Furthermore, a new pre-training strategy is adopted to improve the performance by exploiting large amounts of uni-modality data. Both qualitative and quantitative experiments have been conducted to demonstrate the superiority of our method to other state-of-the-art approaches in the task of few-shot Chinese calligraphy font synthesis. The source code can be found at https://github.com/lsflyt-pku/DeepCalliFont",
    "checked": true,
    "id": "b3fb0653c137d42ab742feb4005c1130ef8fa13b",
    "semantic_title": "deepcallifont: few-shot chinese calligraphy font synthesis by integrating dual-modality generative models",
    "citation_count": 0,
    "authors": [
      "Yitian Liu",
      "Zhouhui Lian"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28169": {
    "title": "Stable Unlearnable Example: Enhancing the Robustness of Unlearnable Examples via Stable Error-Minimizing Noise",
    "volume": "main",
    "abstract": "The open sourcing of large amounts of image data promotes the development of deep learning techniques. Along with this comes the privacy risk of these image datasets being exploited by unauthorized third parties to train deep learning models for commercial or illegal purposes. To avoid the abuse of data, a poisoning-based technique, \"unlearnable example\", has been proposed to significantly degrade the generalization performance of models by adding imperceptible noise to the data. To further enhance its robustness against adversarial training, existing works leverage iterative adversarial training on both the defensive noise and the surrogate model. However, it still remains unknown whether the robustness of unlearnable examples primarily comes from the effect of enhancement in the surrogate model or the defensive noise. Observing that simply removing the adversarial perturbation on the training process of the defensive noise can improve the performance of robust unlearnable examples, we identify that solely the surrogate model's robustness contributes to the performance. Furthermore, we found a negative correlation exists between the robustness of defensive noise and the protection performance, indicating defensive noise's instability issue. Motivated by this, to further boost the robust unlearnable example, we introduce Stable Error-Minimizing noise (SEM), which trains the defensive noise against random perturbation instead of the time-consuming adversarial perturbation to improve the stability of defensive noise. Through comprehensive experiments, we demonstrate that SEM achieves a new state-of-the-art performance on CIFAR-10, CIFAR-100, and ImageNet Subset regarding both effectiveness and efficiency",
    "checked": true,
    "id": "322a3eeed77cf1b6a48bf1b32e5f11455aa6d8d8",
    "semantic_title": "stable unlearnable example: enhancing the robustness of unlearnable examples via stable error-minimizing noise",
    "citation_count": 0,
    "authors": [
      "Yixin Liu",
      "Kaidi Xu",
      "Xun Chen",
      "Lichao Sun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28170": {
    "title": "Scaling and Masking: A New Paradigm of Data Sampling for Image and Video Quality Assessment",
    "volume": "main",
    "abstract": "Quality assessment of images and videos emphasizes both local details and global semantics, whereas general data sampling methods (e.g., resizing, cropping or grid-based fragment) fail to catch them simultaneously. To address the deficiency, current approaches have to adopt multi-branch models and take as input the multi-resolution data, which burdens the model complexity. In this work, instead of stacking up models, a more elegant data sampling method (named as SAMA, scaling and masking) is explored, which compacts both the local and global content in a regular input size. The basic idea is to scale the data into a pyramid first, and reduce the pyramid into a regular data dimension with a masking strategy. Benefiting from the spatial and temporal redundancy in images and videos, the processed data maintains the multi-scale characteristics with a regular input size, thus can be processed by a single-branch model. We verify the sampling method in image and video quality assessment. Experiments show that our sampling method can improve the performance of current single-branch models significantly, and achieves competitive performance to the multi-branch models without extra model complexity. The source code will be available at https://github.com/Sissuire/SAMA",
    "checked": true,
    "id": "229780122cfe2f75f8d55d7005228c008a543eed",
    "semantic_title": "scaling and masking: a new paradigm of data sampling for image and video quality assessment",
    "citation_count": 2,
    "authors": [
      "Yongxu Liu",
      "Yinghui Quan",
      "Guoyao Xiao",
      "Aobo Li",
      "Jinjian Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28171": {
    "title": "Implicit Modeling of Non-rigid Objects with Cross-Category Signals",
    "volume": "main",
    "abstract": "Deep implicit functions (DIFs) have emerged as a potent and articulate means of representing 3D shapes. However, methods modeling object categories or non-rigid entities have mainly focused on single-object scenarios. In this work, we propose MODIF, a multi-object deep implicit function that jointly learns the deformation fields and instance-specific latent codes for multiple objects at once. Our emphasis is on non-rigid, non-interpenetrating entities such as organs. To effectively capture the interrelation between these entities and ensure precise, collision-free representations, our approach facilitates signaling between category-specific fields to adequately rectify shapes. We also introduce novel inter-object supervision: an attraction-repulsion loss is formulated to refine contact regions between objects. Our approach is demonstrated on various medical benchmarks, involving modeling different groups of intricate anatomical entities. Experimental results illustrate that our model can proficiently learn the shape representation of each organ and their relations to others, to the point that shapes missing from unseen instances can be consistently recovered by our method. Finally, MODIF can also propagate semantic information throughout the population via accurate point correspondences",
    "checked": true,
    "id": "3f77f46dc1d8fc28bb343b4ea6bb35b332bb3c5e",
    "semantic_title": "implicit modeling of non-rigid objects with cross-category signals",
    "citation_count": 1,
    "authors": [
      "Yuchun Liu",
      "Benjamin Planche",
      "Meng Zheng",
      "Zhongpai Gao",
      "Pierre Sibut-Bourde",
      "Fan Yang",
      "Terrence Chen",
      "Ziyan Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28172": {
    "title": "Recasting Regional Lighting for Shadow Removal",
    "volume": "main",
    "abstract": "Removing shadows requires an understanding of both lighting conditions and object textures in a scene. Existing methods typically learn pixel-level color mappings between shadow and non-shadow images, in which the joint modeling of lighting and object textures is implicit and inadequate. We observe that in a shadow region, the degradation degree of object textures depends on the local illumination, while simply enhancing the local illumination cannot fully recover the attenuated textures. Based on this observation, we propose to condition the restoration of attenuated textures on the corrected local lighting in the shadow region. Specifically, We first design a shadow-aware decomposition network to estimate the illumination and reflectance layers of shadow regions explicitly. We then propose a novel bilateral correction network to recast the lighting of shadow regions in the illumination layer via a novel local lighting correction module, and to restore the textures conditioned on the corrected illumination layer via a novel illumination-guided texture restoration module. We further annotate pixel-wise shadow masks for the public SRD dataset, which originally contains only image pairs. Experiments on three benchmarks show that our method outperforms existing state-of-the-art shadow removal methods. Project page in: yuhaoliu7456.github.io/RRL-Net",
    "checked": true,
    "id": "35076f0d473822a78f017e995596e54696b2579b",
    "semantic_title": "recasting regional lighting for shadow removal",
    "citation_count": 4,
    "authors": [
      "Yuhao Liu",
      "Zhanghan Ke",
      "Ke Xu",
      "Fang Liu",
      "Zhenwei Wang",
      "Rynson W.H. Lau"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28173": {
    "title": "Rolling-Unet: Revitalizing MLP's Ability to Efficiently Extract Long-Distance Dependencies for Medical Image Segmentation",
    "volume": "main",
    "abstract": "Medical image segmentation methods based on deep learning network are mainly divided into CNN and Transformer. However, CNN struggles to capture long-distance dependencies, while Transformer suffers from high computational complexity and poor local feature learning. To efficiently extract and fuse local features and long-range dependencies, this paper proposes Rolling-Unet, which is a CNN model combined with MLP. Specifically, we propose the core R-MLP module, which is responsible for learning the long-distance dependency in a single direction of the whole image. By controlling and combining R-MLP modules in different directions, OR-MLP and DOR-MLP modules are formed to capture long-distance dependencies in multiple directions. Further, Lo2 block is proposed to encode both local context information and long-distance dependencies without excessive computational burden. Lo2 block has the same parameter size and computational complexity as a 3×3 convolution. The experimental results on four public datasets show that Rolling-Unet achieves superior performance compared to the state-of-the-art methods",
    "checked": true,
    "id": "be5a18eb08247898f2708a62b1646341e61ed626",
    "semantic_title": "rolling-unet: revitalizing mlp's ability to efficiently extract long-distance dependencies for medical image segmentation",
    "citation_count": 2,
    "authors": [
      "Yutong Liu",
      "Haijiang Zhu",
      "Mengting Liu",
      "Huaiyuan Yu",
      "Zihan Chen",
      "Jie Gao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28174": {
    "title": "Advancing Video Synchronization with Fractional Frame Analysis: Introducing a Novel Dataset and Model",
    "volume": "main",
    "abstract": "Multiple views play a vital role in 3D pose estimation tasks. Ideally, multi-view 3D pose estimation tasks should directly utilize naturally collected videos for pose estimation. However, due to the constraints of video synchronization, existing methods often use expensive hardware devices to synchronize the initiation of cameras, which restricts most 3D pose collection scenarios to indoor settings. Some recent works learn deep neural networks to align desynchronized datasets derived from synchronized cameras and can only produce frame-level accuracy. For fractional frame video synchronization, this work proposes an Inter-Frame and Intra-Frame Desynchronized Dataset (IFID), which labels fractional time intervals between two video clips. IFID is the first dataset that annotates inter-frame and intra-frame intervals, with a total of 382,500 video clips annotated, making it the largest dataset to date. We also develop a novel model based on the Transformer architecture, named InSynFormer, for synchronizing inter-frame and intra-frame. Extensive experimental evaluations demonstrate its promising performance. The dataset and source code of the model are available at https://github.com/yuxuan-cser/InSynFormer",
    "checked": true,
    "id": "206cbdbb8a4e654013a81d680060bafb31138dd9",
    "semantic_title": "advancing video synchronization with fractional frame analysis: introducing a novel dataset and model",
    "citation_count": 0,
    "authors": [
      "Yuxuan Liu",
      "Haizhou Ai",
      "Junliang Xing",
      "Xuri Li",
      "Xiaoyi Wang",
      "Pin Tao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28175": {
    "title": "FedCD: Federated Semi-Supervised Learning with Class Awareness Balance via Dual Teachers",
    "volume": "main",
    "abstract": "Recent advancements in deep learning have greatly improved the efficiency of auxiliary medical diagnostics. However, concerns over patient privacy and data annotation costs restrict the viability of centralized training models. In response, federated semi-supervised learning has garnered substantial attention from medical institutions. However, it faces challenges arising from knowledge discrepancies among local clients and class imbalance in non-independent and identically distributed data. Existing methods like class balance adaptation for addressing class imbalance often overlook low-confidence yet valuable rare samples in unlabeled data and may compromise client privacy. To address these issues, we propose a novel framework with class awareness balance and dual teacher distillation called FedCD. FedCD introduces a global-local framework to balance and purify global and local knowledge. Additionally, we introduce a novel class awareness balance module to effectively explore potential rare classes and encourage balanced learning in unlabeled clients. Importantly, our approach prioritizes privacy protection by only exchanging network parameters during communication. Experimental results on two medical datasets under various settings demonstrate the effectiveness of FedCD. The code is available at https://github.com/YunzZ-Liu/FedCD",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzhi Liu",
      "Huisi Wu",
      "Jing Qin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28176": {
    "title": "BLADE: Box-Level Supervised Amodal Segmentation through Directed Expansion",
    "volume": "main",
    "abstract": "Perceiving the complete shape of occluded objects is essential for human and machine intelligence. While the amodal segmentation task is to predict the complete mask of partially occluded objects, it is time-consuming and labor-intensive to annotate the pixel-level ground truth amodal masks. Box-level supervised amodal segmentation addresses this challenge by relying solely on ground truth bounding boxes and instance classes as supervision, thereby alleviating the need for exhaustive pixel-level annotations. Nevertheless, current box-level methodologies encounter limitations in generating low-resolution masks and imprecise boundaries, failing to meet the demands of practical real-world applications. We present a novel solution to tackle this problem by introducing a directed expansion approach from visible masks to corresponding amodal masks. Our approach involves a hybrid end-to-end network based on the overlapping region - the area where different instances intersect. Diverse segmentation strategies are applied for overlapping regions and non-overlapping regions according to distinct characteristics. To guide the expansion of visible masks, we introduce an elaborately-designed connectivity loss for overlapping regions, which leverages correlations with visible masks and facilitates accurate amodal segmentation. Experiments are conducted on several challenging datasets and the results show that our proposed method can outperform existing state-of-the-art methods with large margins",
    "checked": true,
    "id": "ed3921555dfad3d3edf36852177ee1711d9a512b",
    "semantic_title": "blade: box-level supervised amodal segmentation through directed expansion",
    "citation_count": 2,
    "authors": [
      "Zhaochen Liu",
      "Zhixuan Li",
      "Tingting Jiang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28177": {
    "title": "Towards Balanced Alignment: Modal-Enhanced Semantic Modeling for Video Moment Retrieval",
    "volume": "main",
    "abstract": "Video Moment Retrieval (VMR) aims to retrieve temporal segments in untrimmed videos corresponding to a given language query by constructing cross-modal alignment strategies. However, these existing strategies are often sub-optimal since they ignore the modality imbalance problem, i.e., the semantic richness inherent in videos far exceeds that of a given limited-length sentence. Therefore, in pursuit of better alignment, a natural idea is enhancing the video modality to filter out query-irrelevant semantics, and enhancing the text modality to capture more segment-relevant knowledge. In this paper, we introduce Modal-Enhanced Semantic Modeling (MESM), a novel framework for more balanced alignment through enhancing features at two levels. First, we enhance the video modality at the frame-word level through word reconstruction. This strategy emphasizes the portions associated with query words in frame-level features while suppressing irrelevant parts. Therefore, the enhanced video contains less redundant semantics and is more balanced with the textual modality. Second, we enhance the textual modality at the segment-sentence level by learning complementary knowledge from context sentences and ground-truth segments. With the knowledge added to the query, the textual modality thus maintains more meaningful semantics and is more balanced with the video modality. By implementing two levels of MESM, the semantic information from both modalities is more balanced to align, thereby bridging the modality gap. Experiments on three widely used benchmarks, including the out-of-distribution settings, show that the proposed framework achieves a new start-of-the-art performance with notable generalization ability (e.g., 4.42% and 7.69% average gains of R1@0.7 on Charades-STA and Charades-CG). The code will be available at https://github.com/lntzm/MESM",
    "checked": true,
    "id": "c0e3c3effa1b5311ba1ee47c358fbb75b11fac7f",
    "semantic_title": "towards balanced alignment: modal-enhanced semantic modeling for video moment retrieval",
    "citation_count": 3,
    "authors": [
      "Zhihang Liu",
      "Jun Li",
      "Hongtao Xie",
      "Pandeng Li",
      "Jiannan Ge",
      "Sun-Ao Liu",
      "Guoqing Jin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28178": {
    "title": "Improving Cross-Modal Alignment with Synthetic Pairs for Text-Only Image Captioning",
    "volume": "main",
    "abstract": "Although image captioning models have made significant advancements in recent years, the majority of them heavily depend on high-quality datasets containing paired images and texts which are costly to acquire. Previous works leverage the CLIP's cross-modal association ability for image captioning, relying solely on textual information under unsupervised settings. However, not only does a modality gap exist between CLIP text and image features, but a discrepancy also arises between training and inference due to the unavailability of real-world images, which hinders the cross-modal alignment in text-only captioning. This paper proposes a novel method to address these issues by incorporating synthetic image-text pairs. A pre-trained text-to-image model is deployed to obtain images that correspond to textual data, and the pseudo features of generated images are optimized toward the real ones in the CLIP embedding space. Furthermore, textual information is gathered to represent image features, resulting in the image features with various semantics and the bridged modality gap. To unify training and inference, synthetic image features would serve as the training prefix for the language decoder, while real images are used for inference. Additionally, salient objects in images are detected as assistance to enhance the learning of modality alignment. Experimental results demonstrate that our method obtains the state-of-the-art performance on benchmark datasets",
    "checked": true,
    "id": "5c4f8effb87d7db31ff5b53ead15b921d60a103e",
    "semantic_title": "improving cross-modal alignment with synthetic pairs for text-only image captioning",
    "citation_count": 1,
    "authors": [
      "Zhiyue Liu",
      "Jinyuan Liu",
      "Fanrong Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28179": {
    "title": "Cell Graph Transformer for Nuclei Classification",
    "volume": "main",
    "abstract": "Nuclei classification is a critical step in computer-aided diagnosis with histopathology images. In the past, various methods have employed graph neural networks (GNN) to analyze cell graphs that model inter-cell relationships by considering nuclei as vertices. However, they are limited by the GNN mechanism that only passes messages among local nodes via fixed edges. To address the issue, we develop a cell graph transformer (CGT) that treats nodes and edges as input tokens to enable learnable adjacency and information exchange among all nodes. Nevertheless, training the transformer with a cell graph presents another challenge. Poorly initialized features can lead to noisy self-attention scores and inferior convergence, particularly when processing the cell graphs with numerous connections. Thus, we further propose a novel topology-aware pretraining method that leverages a graph convolutional network (GCN) to learn a feature extractor. The pre-trained features may suppress unreasonable correlations and hence ease the finetuning of CGT. Experimental results suggest that the proposed cell graph transformer with topology-aware pretraining significantly improves the nuclei classification results, and achieves the state-of-the-art performance. Code and models are available at https://github.com/lhaof/CGT",
    "checked": true,
    "id": "026c5535021a83d14532ec9a8fc8f11cfad8889e",
    "semantic_title": "cell graph transformer for nuclei classification",
    "citation_count": 0,
    "authors": [
      "Wei Lou",
      "Guanbin Li",
      "Xiang Wan",
      "Haofeng Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28180": {
    "title": "Detect Any Keypoints: An Efficient Light-Weight Few-Shot Keypoint Detector",
    "volume": "main",
    "abstract": "Recently the prompt-based models have become popular across various language and vision tasks. Following that trend, we perform few-shot keypoint detection (FSKD) by detecting any keypoints in a query image, given the prompts formed by support images and keypoints. FSKD can be applied to detecting keypoints and poses of diverse animal species. In order to maintain flexibility of detecting varying number of keypoints, existing FSKD approaches modulate query feature map per support keypoint, then detect the corresponding keypoint from each modulated feature via a detection head. Such a separation of modulation-detection makes model heavy and slow when the number of keypoints increases. To overcome this issue, we design a novel light-weight detector which combines modulation and detection into one step, with the goal of reducing the computational cost without the drop of performance. Moreover, to bridge the large domain shift of keypoints between seen and unseen species, we further improve our model with mean feature based contrastive learning to align keypoint distributions, resulting in better keypoint representations for FSKD. Compared to the state of the art, our light-weight detector reduces the number of parameters by 50%, training/test time by 50%, and achieves 5.62% accuracy gain on 1-shot novel keypoint detection in the Animal pose dataset. Our model is also robust to the number of keypoints and saves memory when evaluating a large number of keypoints (e.g., 1000) per episode",
    "checked": true,
    "id": "ebd001db8344de32c6aafc1dd2e11b8fee738c38",
    "semantic_title": "detect any keypoints: an efficient light-weight few-shot keypoint detector",
    "citation_count": 2,
    "authors": [
      "Changsheng Lu",
      "Piotr Koniusz"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28181": {
    "title": "TCNet: Continuous Sign Language Recognition from Trajectories and Correlated Regions",
    "volume": "main",
    "abstract": "A key challenge in continuous sign language recognition (CSLR) is to efficiently capture long-range spatial interactions over time from the video input. To address this challenge, we propose TCNet, a hybrid network that effectively models spatio-temporal information from Trajectories and Correlated regions. TCNet's trajectory module transforms frames into aligned trajectories composed of continuous visual tokens. This facilitates extracting region trajectory patterns. In addition, for a query token, self-attention is learned along the trajectory. As such, our network can also focus on fine-grained spatio-temporal patterns, such as finger movement, of a region in motion. TCNet's correlation module utilizes a novel dynamic attention mechanism that filters out irrelevant frame regions. Additionally, it assigns dynamic key-value tokens from correlated regions to each query. Both innovations significantly reduce the computation cost and memory. We perform experiments on four large-scale datasets: PHOENIX14, PHOENIX14-T, CSL, and CSL-Daily. Our results demonstrate that TCNet consistently achieves state-of-the-art performance. For example, we improve over the previous state-of-the-art by 1.5\\% and 1.0\\% word error rate on PHOENIX14 and PHOENIX14-T, respectively. Code is available at https://github.com/hotfinda/TCNet",
    "checked": true,
    "id": "866dec836ea1733952b7b99a0a57e290ff54a114",
    "semantic_title": "tcnet: continuous sign language recognition from trajectories and correlated regions",
    "citation_count": 0,
    "authors": [
      "Hui Lu",
      "Albert Ali Salah",
      "Ronald Poppe"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28182": {
    "title": "MLNet: Mutual Learning Network with Neighborhood Invariance for Universal Domain Adaptation",
    "volume": "main",
    "abstract": "Universal domain adaptation (UniDA) is a practical but challenging problem, in which information about the relation between the source and the target domains is not given for knowledge transfer. Existing UniDA methods may suffer from the problems of overlooking intra-domain variations in the target domain and difficulty in separating between the similar known and unknown class. To address these issues, we propose a novel Mutual Learning Network (MLNet) with neighborhood invariance for UniDA. In our method, confidence-guided invariant feature learning with self-adaptive neighbor selection is designed to reduce the intra-domain variations for more generalizable feature representation. By using the cross-domain mixup scheme for better unknown-class identification, the proposed method compensates for the misidentified known-class errors by mutual learning between the closed-set and open-set classifiers. Extensive experiments on three publicly available benchmarks demonstrate that our method achieves the best results compared to the state-of-the-arts in most cases and significantly outperforms the baseline across all the four settings in UniDA. Code is available at https://github.com/YanzuoLu/MLNet",
    "checked": true,
    "id": "df4662e4cf0d1b2825ae735fd94dc64de3a841e8",
    "semantic_title": "mlnet: mutual learning network with neighborhood invariance for universal domain adaptation",
    "citation_count": 1,
    "authors": [
      "Yanzuo Lu",
      "Meng Shen",
      "Andy J Ma",
      "Xiaohua Xie",
      "Jian-Huang Lai"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28183": {
    "title": "Set Prediction Guided by Semantic Concepts for Diverse Video Captioning",
    "volume": "main",
    "abstract": "Diverse video captioning aims to generate a set of sentences to describe the given video in various aspects. Mainstream methods are trained with independent pairs of a video and a caption from its ground-truth set without exploiting the intra-set relationship, resulting in low diversity of generated captions. Different from them, we formulate diverse captioning into a semantic-concept-guided set prediction (SCG-SP) problem by fitting the predicted caption set to the ground-truth set, where the set-level relationship is fully captured. Specifically, our set prediction consists of two synergistic tasks, i.e., caption generation and an auxiliary task of concept combination prediction providing extra semantic supervision. Each caption in the set is attached to a concept combination indicating the primary semantic content of the caption and facilitating element alignment in set prediction. Furthermore, we apply a diversity regularization term on concepts to encourage the model to generate semantically diverse captions with various concept combinations. These two tasks share multiple semantics-specific encodings as input, which are obtained by iterative interaction between visual features and conceptual queries. The correspondence between the generated captions and specific concept combinations further guarantees the interpretability of our model. Extensive experiments on benchmark datasets show that the proposed SCG-SP achieves state-of-the-art (SOTA) performance under both relevance and diversity metrics",
    "checked": true,
    "id": "a24c4730e18bc1fd88c112cbdd2be2a93e17e34f",
    "semantic_title": "set prediction guided by semantic concepts for diverse video captioning",
    "citation_count": 0,
    "authors": [
      "Yifan Lu",
      "Ziqi Zhang",
      "Chunfeng Yuan",
      "Peng Li",
      "Yan Wang",
      "Bing Li",
      "Weiming Hu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28184": {
    "title": "Entropy Induced Pruning Framework for Convolutional Neural Networks",
    "volume": "main",
    "abstract": "Structured pruning techniques have achieved great compression performance on convolutional neural networks for image classification tasks. However, the majority of existing methods are sensitive with respect to the model parameters, and their pruning results may be unsatisfactory when the original model is trained poorly. That is, they need the original model to be fully trained, to obtain useful weight information. This is time-consuming, and makes the effectiveness of the pruning results dependent on the degree of model optimization. To address the above issue, we propose a novel metric named Average Filter Information Entropy (AFIE). It decomposes the weight matrix of each layer into a low-rank space, and quantifies the filter importance based on the distribution of the normalized eigenvalues. Intuitively, the eigenvalues capture the covariance among filters, and therefore could be a good guide for pruning. Since the distribution of eigenvalues is robust to the updating of parameters, AFIE can yield a stable evaluation for the importance of each filter no matter whether the original model is trained fully. We implement our AFIE-based pruning method for three popular CNN models of AlexNet, VGG-16, and ResNet-50, and test them on three widely-used image datasets MNIST, CIFAR-10, and ImageNet, respectively. The experimental results are encouraging. We surprisingly observe that for our methods, even when the original model is trained with only one epoch, the AFIE score of each filter keeps identical to the results when the model is fully-trained. This fully indicates the effectiveness of the proposed pruning method",
    "checked": true,
    "id": "c4ec5728f427f68c536d53386c479ea076374517",
    "semantic_title": "entropy induced pruning framework for convolutional neural networks",
    "citation_count": 1,
    "authors": [
      "Yiheng Lu",
      "Ziyu Guan",
      "Yaming Yang",
      "Wei Zhao",
      "Maoguo Gong",
      "Cai Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28185": {
    "title": "Pano-NeRF: Synthesizing High Dynamic Range Novel Views with Geometry from Sparse Low Dynamic Range Panoramic Images",
    "volume": "main",
    "abstract": "Panoramic imaging research on geometry recovery and High Dynamic Range (HDR) reconstruction becomes a trend with the development of Extended Reality (XR). Neural Radiance Fields (NeRF) provide a promising scene representation for both tasks without requiring extensive prior data. How- ever, in the case of inputting sparse Low Dynamic Range (LDR) panoramic images, NeRF often degrades with under-constrained geometry and is unable to reconstruct HDR radiance from LDR inputs. We observe that the radiance from each pixel in panoramic images can be modeled as both a signal to convey scene lighting information and a light source to illuminate other pixels. Hence, we propose the irradiance fields from sparse LDR panoramic images, which increases the observation counts for faithful geometry recovery and leverages the irradiance-radiance attenuation for HDR reconstruction. Extensive experiments demonstrate that the irradiance fields outperform state-of-the-art methods on both geometry recovery and HDR reconstruction and validate their effectiveness. Furthermore, we show a promising byproduct of spatially-varying lighting estimation. The code is available at https://github.com/Lu-Zhan/Pano-NeRF",
    "checked": true,
    "id": "03a04495a459c7092ebc234532fa3642d8b62472",
    "semantic_title": "pano-nerf: synthesizing high dynamic range novel views with geometry from sparse low dynamic range panoramic images",
    "citation_count": 0,
    "authors": [
      "Zhan Lu",
      "Qian Zheng",
      "Boxin Shi",
      "Xudong Jiang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28186": {
    "title": "ScanERU: Interactive 3D Visual Grounding Based on Embodied Reference Understanding",
    "volume": "main",
    "abstract": "Aiming to link natural language descriptions to specific regions in a 3D scene represented as 3D point clouds, 3D visual grounding is a very fundamental task for human-robot interaction. The recognition errors can significantly impact the overall accuracy and then degrade the operation of AI systems. Despite their effectiveness, existing methods suffer from the difficulty of low recognition accuracy in cases of multiple adjacent objects with similar appearance. To address this issue, this work intuitively introduces the human-robot interaction as a cue to facilitate the development of 3D visual grounding. Specifically, a new task termed Embodied Reference Understanding (ERU) is first designed for this concern. Then a new dataset called ScanERU is constructed to evaluate the effectiveness of this idea. Different from existing datasets, our ScanERU dataset is the first to cover semi-synthetic scene integration with textual, real-world visual, and synthetic gestural information. Additionally, this paper formulates a heuristic framework based on attention mechanisms and human body movements to enlighten the research of ERU. Experimental results demonstrate the superiority of the proposed method, especially in the recognition of multiple identical objects. Our codes and dataset are available in the ScanERU repository",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyang Lu",
      "Yunqiang Pei",
      "Guoqing Wang",
      "Peiwei Li",
      "Yang Yang",
      "Yinjie  Lei",
      "Heng Tao Shen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28187": {
    "title": "MGNet: Learning Correspondences via Multiple Graphs",
    "volume": "main",
    "abstract": "Learning correspondences aims to find correct correspondences (inliers) from the initial correspondence set with an uneven correspondence distribution and a low inlier rate, which can be regarded as graph data. Recent advances usually use graph neural networks (GNNs) to build a single type of graph or simply stack local graphs into the global one to complete the task. But they ignore the complementary relationship between different types of graphs, which can effectively capture potential relationships among sparse correspondences. To address this problem, we propose MGNet to effectively combine multiple complementary graphs. To obtain information integrating implicit and explicit local graphs, we construct local graphs from implicit and explicit aspects and combine them effectively, which is used to build a global graph. Moreover, we propose Graph Soft Degree Attention (GSDA) to make full use of all sparse correspondence information at once in the global graph, which can capture and amplify discriminative features. Extensive experiments demonstrate that MGNet outperforms state-of-the-art methods in different visual tasks. The code is provided in https://github.com/DAILUANYUAN/MGNet-2024AAAI",
    "checked": true,
    "id": "de68220e07dc47d64c3fbff2920d78393f83fc4d",
    "semantic_title": "mgnet: learning correspondences via multiple graphs",
    "citation_count": 2,
    "authors": [
      "Dai Luanyuan",
      "Xiaoyu Du",
      "Hanwang Zhang",
      "Jinhui Tang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28188": {
    "title": "SCP: Spherical-Coordinate-Based Learned Point Cloud Compression",
    "volume": "main",
    "abstract": "In recent years, the task of learned point cloud compression has gained prominence. An important type of point cloud, LiDAR point cloud, is generated by spinning LiDAR on vehicles. This process results in numerous circular shapes and azimuthal angle invariance features within the point clouds. However, these two features have been largely overlooked by previous methodologies. In this paper, we introduce a model-agnostic method called Spherical-Coordinate-based learned Point cloud compression (SCP), designed to fully leverage the features of circular shapes and azimuthal angle invariance. Additionally, we propose a multi-level Octree for SCP to mitigate the reconstruction error for distant areas within the Spherical-coordinate-based Octree. SCP exhibits excellent universality, making it applicable to various learned point cloud compression techniques. Experimental results demonstrate that SCP surpasses previous state-of-the-art methods by up to 29.14% in point-to-point PSNR BD-Rate",
    "checked": true,
    "id": "f080f34d8356c52a7d63fbab42b24f76bf9bf05d",
    "semantic_title": "scp: spherical-coordinate-based learned point cloud compression",
    "citation_count": 1,
    "authors": [
      "Ao Luo",
      "Linxin Song",
      "Keisuke Nonaka",
      "Kyohei Unno",
      "Heming Sun",
      "Masayuki Goto",
      "Jiro Katto"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28189": {
    "title": "DLCA-Recon: Dynamic Loose Clothing Avatar Reconstruction from Monocular Videos",
    "volume": "main",
    "abstract": "Reconstructing a dynamic human with loose clothing is an important but difficult task. To address this challenge, we propose a method named DLCA-Recon to create human avatars from monocular videos. The distance from loose clothing to the underlying body rapidly changes in every frame when the human freely moves and acts. Previous methods lack effective geometric initialization and constraints for guiding the optimization of deformation to explain this dramatic change, resulting in the discontinuous and incomplete reconstruction surface.To model the deformation more accurately, we propose to initialize an estimated 3D clothed human in the canonical space, as it is easier for deformation fields to learn from the clothed human than from SMPL.With both representations of explicit mesh and implicit SDF, we utilize the physical connection information between consecutive frames and propose a dynamic deformation field (DDF) to optimize deformation fields. DDF accounts for contributive forces on loose clothing to enhance the interpretability of deformations and effectively capture the free movement of loose clothing. Moreover, we propagate SMPL skinning weights to each individual and refine pose and skinning weights during the optimization to improve skinning transformation. Based on more reasonable initialization and DDF, we can simulate real-world physics more accurately. Extensive experiments on public and our own datasets validate that our method can produce superior results for humans with loose clothing compared to the SOTA methods",
    "checked": true,
    "id": "8b53f3de9068be1b4ba97eb58fccf12c233f788b",
    "semantic_title": "dlca-recon: dynamic loose clothing avatar reconstruction from monocular videos",
    "citation_count": 1,
    "authors": [
      "Chunjie Luo",
      "Fei Luo",
      "Yusen Wang",
      "Enxu Zhao",
      "Chunxia Xiao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28190": {
    "title": "Dual-Window Multiscale Transformer for Hyperspectral Snapshot Compressive Imaging",
    "volume": "main",
    "abstract": "Coded aperture snapshot spectral imaging (CASSI) system is an effective manner for hyperspectral snapshot compressive imaging. The core issue of CASSI is to solve the inverse problem for the reconstruction of hyperspectral image (HSI). In recent years, Transformer-based methods achieve promising performance in HSI reconstruction. However, capturing both long-range dependencies and local information while ensuring reasonable computational costs remains a challenging problem. In this paper, we propose a Transformer-based HSI reconstruction method called dual-window multiscale Transformer (DWMT), which is a coarse-to-fine process, reconstructing the global properties of HSI with the long-range dependencies. In our method, we propose a novel U-Net architecture using a dual-branch encoder to refine pixel information and full-scale skip connections to fuse different features, enhancing the extraction of fine-grained features. Meanwhile, we design a novel self-attention mechanism called dual-window multiscale multi-head self-attention (DWM-MSA), which utilizes two different-sized windows to compute self-attention, which can capture the long-range dependencies in a local region at different scales to improve the reconstruction performance. We also propose a novel position embedding method for Transformer, named con-abs position embedding (CAPE), which effectively enhances positional information of the HSIs. Extensive experiments on both the simulated and the real data are conducted to demonstrate the superior performance, stability, and generalization ability of our DWMT. Code of this project is at https://github.com/chenx2000/DWMT",
    "checked": true,
    "id": "36676313809fa414bd42be8f279560691a22367c",
    "semantic_title": "dual-window multiscale transformer for hyperspectral snapshot compressive imaging",
    "citation_count": 1,
    "authors": [
      "Fulin Luo",
      "Xi Chen",
      "Xiuwen Gong",
      "Weiwen Wu",
      "Tan Guo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28191": {
    "title": "Electron Microscopy Images as Set of Fragments for Mitochondrial Segmentation",
    "volume": "main",
    "abstract": "Automatic mitochondrial segmentation enjoys great popularity with the development of deep learning. However, the coarse prediction raised by the presence of regular 3D grids in previous methods regardless of 3D CNN or the vision transformers suggest a possibly sub-optimal feature arrangement. To mitigate this limitation, we attempt to interpret the 3D EM image stacks as a set of interrelated 3D fragments for a better solution. However, it is non-trivial to model the 3D fragments without introducing excessive computational overhead. In this paper, we design a coherent fragment vision transformer (FragViT) combined with affinity learning to manipulate features on 3D fragments yet explore mutual relationships to model fragment-wise context, enjoying locality prior without sacrificing global reception. The proposed FragViT includes a fragment encoder and a hierarchical fragment aggregation module. The fragment encoder is equipped with affinity heads to transform the tokens into fragments with homogeneous semantics, and the multi-layer self-attention is used to explicitly learn inter-fragment relations with long-range dependencies. The hierarchical fragment aggregation module is responsible for hierarchically aggregating fragment-wise prediction back to the final voxel-wise prediction in a progressive manner. Extensive experimental results on the challenging MitoEM, Lucchi, and AC3/AC4 benchmarks demonstrate the effectiveness of the proposed method",
    "checked": true,
    "id": "44937b45698ad57782688160fc3fcfb5b1a5d47b",
    "semantic_title": "electron microscopy images as set of fragments for mitochondrial segmentation",
    "citation_count": 1,
    "authors": [
      "Naisong Luo",
      "Rui Sun",
      "Yuwen Pan",
      "Tianzhu Zhang",
      "Feng Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28192": {
    "title": "DiffusionTrack: Diffusion Model for Multi-Object Tracking",
    "volume": "main",
    "abstract": "Multi-object tracking (MOT) is a challenging vision task that aims to detect individual objects within a single frame and associate them across multiple frames. Recent MOT approaches can be categorized into two-stage tracking-by-detection (TBD) methods and one-stage joint detection and tracking (JDT) methods. Despite the success of these approaches, they also suffer from common problems, such as harmful global or local inconsistency, poor trade-off between robustness and model complexity, and lack of flexibility in different scenes within the same video. In this paper we propose a simple but robust framework that formulates object detection and association jointly as a consistent denoising diffusion process from paired noise boxes to paired ground-truth boxes. This novel progressive denoising diffusion strategy substantially augments the tracker's effectiveness, enabling it to discriminate between various objects. During the training stage, paired object boxes diffuse from paired ground-truth boxes to random distribution, and the model learns detection and tracking simultaneously by reversing this noising process. In inference, the model refines a set of paired randomly generated boxes to the detection and tracking results in a flexible one-step or multi-step denoising diffusion process. Extensive experiments on three widely used MOT benchmarks, including MOT17, MOT20, and DanceTrack, demonstrate that our approach achieves competitive performance compared to the current state-of-the-art methods. Code is available at https://github.com/RainBowLuoCS/DiffusionTrack",
    "checked": true,
    "id": "95b0dcc6d828236da2077ba262d01b265f782acf",
    "semantic_title": "diffusiontrack: diffusion model for multi-object tracking",
    "citation_count": 4,
    "authors": [
      "Run Luo",
      "Zikai Song",
      "Lintao Ma",
      "Jinlin Wei",
      "Wei Yang",
      "Min Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28193": {
    "title": "Devignet: High-Resolution Vignetting Removal via a Dual Aggregated Fusion Transformer with Adaptive Channel Expansion",
    "volume": "main",
    "abstract": "Vignetting commonly occurs as a degradation in images resulting from factors such as lens design, improper lens hood usage, and limitations in camera sensors. This degradation affects image details, color accuracy, and presents challenges in computational photography. Existing vignetting removal algorithms predominantly rely on ideal physics assumptions and hand-crafted parameters, resulting in the ineffective removal of irregular vignetting and suboptimal results. Moreover, the substantial lack of real-world vignetting datasets hinders the objective and comprehensive evaluation of vignetting removal. To address these challenges, we present VigSet, a pioneering dataset for vignetting removal. VigSet includes 983 pairs of both vignetting and vignetting-free high-resolution (over 4k) real-world images under various conditions. In addition, We introduce DeVigNet, a novel frequency-aware Transformer architecture designed for vignetting removal. Through the Laplacian Pyramid decomposition, we propose the Dual Aggregated Fusion Transformer to handle global features and remove vignetting in the low-frequency domain. Additionally, we propose the Adaptive Channel Expansion Module to enhance details in the high-frequency domain. The experiments demonstrate that the proposed model outperforms existing state-of-the-art methods. The code, models, and dataset are available at https://github.com/CXH-Research/DeVigNet",
    "checked": true,
    "id": "1d88bb9fb95e72a73bb11c88614fd5354426fa2f",
    "semantic_title": "devignet: high-resolution vignetting removal via a dual aggregated fusion transformer with adaptive channel expansion",
    "citation_count": 8,
    "authors": [
      "Shenghong Luo",
      "Xuhang Chen",
      "Weiwen Chen",
      "Zinuo Li",
      "Shuqiang Wang",
      "Chi-Man Pun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28194": {
    "title": "AdaFormer: Efficient Transformer with Adaptive Token Sparsification for Image Super-resolution",
    "volume": "main",
    "abstract": "Efficient transformer-based models have made remarkable progress in image super-resolution (SR). Most of these works mainly design elaborate structures to accelerate the inference of the transformer, where all feature tokens are propagated equally. However, they ignore the underlying characteristic of image content, i.e., various image regions have distinct restoration difficulties, especially for large images (2K-8K), failing to achieve adaptive inference. In this work, we propose an adaptive token sparsification transformer (AdaFormer) to speed up the model inference for image SR. Specifically, a texture-relevant sparse attention block with parallel global and local branches is introduced, aiming to integrate informative tokens from the global view instead of only in fixed local windows. Then, an early-exit strategy is designed to progressively halt tokens according to the token importance. To estimate the plausibility of each token, we adopt a lightweight confidence estimator, which is constrained by an uncertainty-guided loss to obtain a binary halting mask about the tokens. Experiments on large images have illustrated that our proposal reduces nearly 90% latency against SwinIR on Test8K, while maintaining a comparable performance",
    "checked": true,
    "id": "020e9051ca419feb9e973dbf9e6d5441a42abbfe",
    "semantic_title": "adaformer: efficient transformer with adaptive token sparsification for image super-resolution",
    "citation_count": 0,
    "authors": [
      "Xiaotong Luo",
      "Zekun Ai",
      "Qiuyuan Liang",
      "Ding Liu",
      "Yuan Xie",
      "Yanyun Qu",
      "Yun Fu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28195": {
    "title": "SkipDiff: Adaptive Skip Diffusion Model for High-Fidelity Perceptual Image Super-resolution",
    "volume": "main",
    "abstract": "It is well-known that image quality assessment usually meets with the problem of perception-distortion (p-d) tradeoff. The existing deep image super-resolution (SR) methods either focus on high fidelity with pixel-level objectives or high perception with generative models. The emergence of diffusion model paves a fresh way for image restoration, which has the potential to offer a brand-new solution for p-d trade-off. We experimentally observed that the perceptual quality and distortion change in an opposite direction with the increase of sampling steps. In light of this property, we propose an adaptive skip diffusion model (SkipDiff), which aims to achieve high-fidelity perceptual image SR with fewer sampling steps. Specifically, it decouples the sampling procedure into coarse skip approximation and fine skip refinement stages. A coarse-grained skip diffusion is first performed as a high-fidelity prior to obtaining a latent approximation of the full diffusion. Then, a fine-grained skip diffusion is followed to further refine the latent sample for promoting perception, where the fine time steps are adaptively learned by deep reinforcement learning. Meanwhile, this approach also enables faster sampling of diffusion model through skipping the intermediate denoising process to shorten the effective steps of the computation. Extensive experimental results show that our SkipDiff achieves superior perceptual quality with plausible reconstruction accuracy and a faster sampling speed",
    "checked": true,
    "id": "92a4cbdc289c28da782b3beaf9989df52266b597",
    "semantic_title": "skipdiff: adaptive skip diffusion model for high-fidelity perceptual image super-resolution",
    "citation_count": 3,
    "authors": [
      "Xiaotong Luo",
      "Yuan Xie",
      "Yanyun Qu",
      "Yun Fu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28196": {
    "title": "Modeling Continuous Motion for 3D Point Cloud Object Tracking",
    "volume": "main",
    "abstract": "The task of 3D single object tracking (SOT) with LiDAR point clouds is crucial for various applications, such as autonomous driving and robotics. However, existing approaches have primarily relied on appearance matching or motion modeling within only two successive frames, thereby overlooking the long-range continuous motion property of objects in 3D space. To address this issue, this paper presents a novel approach that views each tracklet as a continuous stream: at each timestamp, only the current frame is fed into the network to interact with multi-frame historical features stored in a memory bank, enabling efficient exploitation of sequential information. To achieve effective cross-frame message passing, a hybrid attention mechanism is designed to account for both long-range relation modeling and local geometric feature extraction. Furthermore, to enhance the utilization of multi-frame features for robust tracking, a contrastive sequence enhancement strategy is proposed, which uses ground truth tracklets to augment training sequences and promote discrimination against false positives in a contrastive manner. Extensive experiments demonstrate that the proposed method outperforms the state-of-the-art method by significant margins on multiple benchmarks",
    "checked": true,
    "id": "3ee35227402eb42727c67db43386d94a392b602c",
    "semantic_title": "modeling continuous motion for 3d point cloud object tracking",
    "citation_count": 2,
    "authors": [
      "Zhipeng Luo",
      "Gongjie Zhang",
      "Changqing Zhou",
      "Zhonghua Wu",
      "Qingyi Tao",
      "Lewei Lu",
      "Shijian Lu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28197": {
    "title": "SGFormer: Semantic Graph Transformer for Point Cloud-Based 3D Scene Graph Generation",
    "volume": "main",
    "abstract": "In this paper, we propose a novel model called SGFormer, Semantic Graph TransFormer for point cloud-based 3D scene graph generation. The task aims to parse a point cloud-based scene into a semantic structural graph, with the core challenge of modeling the complex global structure. Existing methods based on graph convolutional networks (GCNs) suffer from the over-smoothing dilemma and can only propagate information from limited neighboring nodes. In contrast, SGFormer uses Transformer layers as the base building block to allow global information passing, with two types of newly-designed layers tailored for the 3D scene graph generation task. Specifically, we introduce the graph embedding layer to best utilize the global information in graph edges while maintaining comparable computation costs. Furthermore, we propose the semantic injection layer to leverage linguistic knowledge from large-scale language model (i.e., ChatGPT), to enhance objects' visual features. We benchmark our SGFormer on the established 3DSSG dataset and achieve a 40.94% absolute improvement in relationship prediction's R@50 and an 88.36% boost on the subset with complex scenes over the state-of-the-art. Our analyses further show SGFormer's superiority in the long-tail and zero-shot scenarios. Our source code is available at https://github.com/Andy20178/SGFormer",
    "checked": true,
    "id": "ed7b5b7017caeb69ecce53c7e09fcb79071c2ff3",
    "semantic_title": "sgformer: semantic graph transformer for point cloud-based 3d scene graph generation",
    "citation_count": 0,
    "authors": [
      "Changsheng Lv",
      "Mengshi Qi",
      "Xia Li",
      "Zhengyuan Yang",
      "Huadong Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28198": {
    "title": "Privileged Prior Information Distillation for Image Matting",
    "volume": "main",
    "abstract": "Performance of trimap-free image matting methods is limited when trying to decouple the deterministic and undetermined regions, especially in the scenes where foregrounds are semantically ambiguous, chromaless, or high transmittance. In this paper, we propose a novel framework named Privileged Prior Information Distillation for Image Matting (PPID-IM) that can effectively transfer privileged prior environment-aware information to improve the performance of trimap-free students in solving hard foregrounds. The prior information of trimap regulates only the teacher model during the training stage, while not being fed into the student network during actual inference. To achieve effective privileged cross-modality (i.e. trimap and RGB) information distillation, we introduce a Cross-Level Semantic Distillation (CLSD) module that reinforces the students with more knowledgeable semantic representations and environment-aware information. We also propose an Attention-Guided Local Distillation module that efficiently transfers privileged local attributes from the trimap-based teacher to trimap-free students for the guidance of local-region optimization. Extensive experiments demonstrate the effectiveness and superiority of our PPID on image matting. The code will be released soon",
    "checked": true,
    "id": "88ade9aa07e5ce1165b279201760172b532f5d75",
    "semantic_title": "privileged prior information distillation for image matting",
    "citation_count": 0,
    "authors": [
      "Cheng Lyu",
      "Jiake Xie",
      "Bo Xu",
      "Cheng Lu",
      "Han Huang",
      "Xin Huang",
      "Ming Wu",
      "Chuang Zhang",
      "Yong Tang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28199": {
    "title": "FedST: Federated Style Transfer Learning for Non-IID Image Segmentation",
    "volume": "main",
    "abstract": "Federated learning collaboratively trains machine learning models among different clients while keeping data privacy and has become the mainstream for breaking data silos. However, the non-independently and identically distribution (i.e., Non-IID) characteristic of different image domains among different clients reduces the benefits of federated learning and has become a bottleneck problem restricting the accuracy and generalization of federated models. In this work, we propose a novel federated image segmentation method based on style transfer, FedST, by using a denoising diffusion probabilistic model to achieve feature disentanglement and image synthesis of cross-domain image data between multiple clients. Thus it can share style features among clients while protecting structure features of image data, which effectively alleviates the influence of the Non-IID phenomenon. Experiments prove that our method achieves superior segmentation performance compared to state-of-art methods among four different Non-IID datasets in objective and subjective assessment. The code is available at https://github.com/YoferChen/FedST",
    "checked": true,
    "id": "00feb31827e2699fb60dc229eafcd222753ebaa3",
    "semantic_title": "fedst: federated style transfer learning for non-iid image segmentation",
    "citation_count": 0,
    "authors": [
      "Boyuan Ma",
      "Xiang Yin",
      "Jing Tan",
      "Yongfeng Chen",
      "Haiyou Huang",
      "Hao Wang",
      "Weihua Xue",
      "Xiaojuan Ban"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28200": {
    "title": "SlowTrack: Increasing the Latency of Camera-Based Perception in Autonomous Driving Using Adversarial Examples",
    "volume": "main",
    "abstract": "In Autonomous Driving (AD), real-time perception is a critical component responsible for detecting surrounding objects to ensure safe driving. While researchers have extensively explored the integrity of AD perception due to its safety and security implications, the aspect of availability (real-time performance) or latency has received limited attention. Existing works on latency-based attack have focused mainly on object detection, i.e., a component in camera-based AD perception, overlooking the entire camera-based AD perception, which hinders them to achieve effective system-level effects, such as vehicle crashes. In this paper, we propose SlowTrack, a novel framework for generating adversarial attacks to increase the execution time of camera-based AD perception. We propose a novel two-stage attack strategy along with the three new loss function designs. Our evaluation is conducted on four popular camera-based AD perception pipelines, and the results demonstrate that SlowTrack significantly outperforms existing latency-based attacks while maintaining comparable imperceptibility levels. Furthermore, we perform the evaluation on Baidu Apollo, an industry-grade full-stack AD system, and LGSVL, a production-grade AD simulator, with two scenarios to compare the system-level effects of SlowTrack and existing attacks. Our evaluation results show that the system-level effects can be significantly improved, i.e., the vehicle crash rate of SlowTrack is around 95% on average while existing works only have around 30%",
    "checked": true,
    "id": "18be9d40c787ee945bcaa34ea7bc0a79c45f90e4",
    "semantic_title": "slowtrack: increasing the latency of camera-based perception in autonomous driving using adversarial examples",
    "citation_count": 8,
    "authors": [
      "Chen Ma",
      "Ningfei Wang",
      "Qi Alfred Chen",
      "Chao Shen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28201": {
    "title": "Uncertainty-Aware GAN for Single Image Super Resolution",
    "volume": "main",
    "abstract": "Generative adversarial network (GAN) has become a popular tool in the perceptual-oriented single image super-resolution (SISR) for its excellent capability to hallucinate details. However, the performance of most GAN-based SISR methods is impeded due to the limited discriminative ability of their discriminators. In specific, these discriminators only focus on the global image reconstruction quality and ignore the more fine-grained reconstruction quality for constraining the generator, as they predict the overall realness of an image instead of the pixel-level realness. Here, we first introduce the uncertainty into the GAN and propose an Uncertainty-aware GAN (UGAN) to regularize SISR solutions, where the challenging pixels with large reconstruction uncertainty and importance (e.g., texture and edge) are prioritized for optimization. The uncertainty-aware adversarial training strategy enables the discriminator to capture the pixel-level SR uncertainty, which constrains the generator to focus on image areas with high reconstruction difficulty, meanwhile, it improves the interpretability of the SR. To balance weights of multiple training losses, we introduce an uncertainty-aware loss weighting strategy to adaptively learn the optimal loss weights. Extensive experiments demonstrate the effectiveness of our approach in extracting the SR uncertainty and the superiority of the UGAN over the state-of-the-arts in terms of the reconstruction accuracy and perceptual quality",
    "checked": true,
    "id": "ece0a91d507127b5cf0bb00eded66b9ef30b9aa4",
    "semantic_title": "uncertainty-aware gan for single image super resolution",
    "citation_count": 0,
    "authors": [
      "Chenxi Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28202": {
    "title": "Stitching Segments and Sentences towards Generalization in Video-Text Pre-training",
    "volume": "main",
    "abstract": "Video-language pre-training models have recently achieved remarkable results on various multi-modal downstream tasks. However, most of these models rely on contrastive learning or masking modeling to align global features across modalities, neglecting the local associations between video frames and text tokens. This limits the model's ability to perform fine-grained matching and generalization, especially for tasks that selecting segments in long videos based on query texts. To address this issue, we propose a novel stitching and matching pre-text task for video-language pre-training that encourages fine-grained interactions between modalities. Our task involves stitching video frames or sentences into longer sequences and predicting the positions of cross-model queries in the stitched sequences. The individual frame and sentence representations are thus aligned via the stitching and matching strategy, encouraging the fine-grained interactions between videos and texts. in the stitched sequences for the cross-modal query. We conduct extensive experiments on various benchmarks covering text-to-video retrieval, video question answering, video captioning, and moment retrieval. Our results demonstrate that the proposed method significantly improves the generalization capacity of the video-text pre-training models",
    "checked": true,
    "id": "fe0ef1ec484a728d6a3dcb80ece84e721cf6bd32",
    "semantic_title": "stitching segments and sentences towards generalization in video-text pre-training",
    "citation_count": 1,
    "authors": [
      "Fan Ma",
      "Xiaojie Jin",
      "Heng Wang",
      "Jingjia Huang",
      "Linchao Zhu",
      "Yi Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28203": {
    "title": "Image Captioning with Multi-Context Synthetic Data",
    "volume": "main",
    "abstract": "Image captioning requires numerous annotated image-text pairs, resulting in substantial annotation costs. Recently, large models (e.g. diffusion models and large language models) have excelled in producing high-quality images and text. This potential can be harnessed to create synthetic image-text pairs for training captioning models. Synthetic data can improve cost and time efficiency in data collection, allow for customization to specific domains, bootstrap generalization capability for zero-shot performance, and circumvent privacy concerns associated with real-world data. However, existing methods struggle to attain satisfactory performance solely through synthetic data. We identify the issue as generated images from simple descriptions mostly capture a solitary perspective with limited context, failing to align with the intricate scenes prevalent in real-world imagery. To tackle this, we present an innovative pipeline that introduces multi-context data generation. Beginning with an initial text corpus, our approach employs a large language model to extract multiple sentences portraying the same scene from diverse viewpoints. These sentences are then condensed into a single sentence with multiple contexts. Subsequently, we generate intricate images using the condensed captions through diffusion models. Our model is exclusively trained on synthetic image-text pairs crafted through this process. The effectiveness of our pipeline is validated through experimental results in both the in-domain and cross-domain settings, where it achieves state-of-the-art performance on well-known datasets such as MSCOCO, Flickr30k, and NoCaps",
    "checked": true,
    "id": "96a7ce9510adc87bdce0b6e8bc541dfad7ae7df7",
    "semantic_title": "image captioning with multi-context synthetic data",
    "citation_count": 3,
    "authors": [
      "Feipeng Ma",
      "Yizhou Zhou",
      "Fengyun Rao",
      "Yueyi Zhang",
      "Xiaoyan Sun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28204": {
    "title": "Directed Diffusion: Direct Control of Object Placement through Attention Guidance",
    "volume": "main",
    "abstract": "Text-guided diffusion models such as DALLE-2, Imagen, and Stable Diffusion are able to generate an effectively endless variety of images given only a short text prompt describing the desired image content. In many cases the images are of very high quality. However, these models often struggle to compose scenes containing several key objects such as characters in specified positional relationships. The missing capability to ``direct'' the placement of characters and objects both within and across images is crucial in storytelling, as recognized in the literature on film and animation theory. In this work, we take a particularly straightforward approach to providing the needed direction. Drawing on the observation that the cross-attention maps for prompt words reflect the spatial layout of objects denoted by those words, we introduce an optimization objective that produces ``activation'' at desired positions in these cross-attention maps. The resulting approach is a step toward generalizing the applicability of text-guided diffusion models beyond single images to collections of related images, as in storybooks. Directed Diffusion provides easy high-level positional control over multiple objects, while making use of an existing pre-trained model and maintaining a coherent blend between the positioned objects and the background. Moreover, it requires only a few lines to implement",
    "checked": true,
    "id": "beaa3130f2657bbfe784b2313f20ba66495926ba",
    "semantic_title": "directed diffusion: direct control of object placement through attention guidance",
    "citation_count": 37,
    "authors": [
      "Wan-Duo Kurt Ma",
      "Avisek Lahiri",
      "J. P. Lewis",
      "Thomas Leung",
      "W. Bastiaan Kleijn"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28205": {
    "title": "Unifying Visual and Vision-Language Tracking via Contrastive Learning",
    "volume": "main",
    "abstract": "Single object tracking aims to locate the target object in a video sequence according to the state specified by different modal references, including the initial bounding box (BBOX), natural language (NL), or both (NL+BBOX). Due to the gap between different modalities, most existing trackers are designed for single or partial of these reference settings and overspecialize on the specific modality. Differently, we present a unified tracker called UVLTrack, which can simultaneously handle all three reference settings (BBOX, NL, NL+BBOX) with the same parameters. The proposed UVLTrack enjoys several merits. First, we design a modality-unified feature extractor for joint visual and language feature learning and propose a multi-modal contrastive loss to align the visual and language features into a unified semantic space. Second, a modality-adaptive box head is proposed, which makes full use of the target reference to mine ever-changing scenario features dynamically from video contexts and distinguish the target in a contrastive way, enabling robust performance in different reference settings. Extensive experimental results demonstrate that UVLTrack achieves promising performance on seven visual tracking datasets, three vision-language tracking datasets, and three visual grounding datasets. Codes and models will be open-sourced at https://github.com/OpenSpaceAI/UVLTrack",
    "checked": true,
    "id": "cffbe52113aa32aa8f97621abe4db4052cae4aa1",
    "semantic_title": "unifying visual and vision-language tracking via contrastive learning",
    "citation_count": 2,
    "authors": [
      "Yinchao Ma",
      "Yuyang Tang",
      "Wenfei Yang",
      "Tianzhu Zhang",
      "Jinpeng Zhang",
      "Mengxue Kang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28206": {
    "title": "Follow Your Pose: Pose-Guided Text-to-Video Generation Using Pose-Free Videos",
    "volume": "main",
    "abstract": "Generating text-editable and pose-controllable character videos have an imperious demand in creating various digital human. Nevertheless, this task has been restricted by the absence of a comprehensive dataset featuring paired video-pose captions and the generative prior models for videos. In this work, we design a novel two-stage training scheme that can utilize easily obtained datasets (i.e., image pose pair and pose-free video) and the pre-trained text-to-image (T2I) model to obtain the pose-controllable character videos. Specifically, in the first stage, only the keypoint image pairs are used only for a controllable text-to-image generation. We learn a zero-initialized convolutional encoder to encode the pose information. In the second stage, we finetune the motion of the above network via a pose-free video dataset by adding the learnable temporal self-attention and reformed cross-frame self-attention blocks. Powered by our new designs, our method successfully generates continuously pose-controllable character videos while keeps the editing and concept composition ability of the pre-trained T2I model. The code and models are available on https://follow-your-pose.github.io/",
    "checked": true,
    "id": "ee73edebd42626d9c2d91e35fd2ed3cdb0fb26d0",
    "semantic_title": "follow your pose: pose-guided text-to-video generation using pose-free videos",
    "citation_count": 90,
    "authors": [
      "Yue Ma",
      "Yingqing He",
      "Xiaodong Cun",
      "Xintao Wang",
      "Siran Chen",
      "Xiu Li",
      "Qifeng Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28207": {
    "title": "Let All Be Whitened: Multi-Teacher Distillation for Efficient Visual Retrieval",
    "volume": "main",
    "abstract": "Visual retrieval aims to search for the most relevant visual items, e.g., images and videos, from a candidate gallery with a given query item. Accuracy and efficiency are two competing objectives in retrieval tasks. Instead of crafting a new method pursuing further improvement on accuracy, in this paper we propose a multi-teacher distillation framework Whiten-MTD, which is able to transfer knowledge from off-the-shelf pre-trained retrieval models to a lightweight student model for efficient visual retrieval. Furthermore, we discover that the similarities obtained by different retrieval models are diversified and incommensurable, which makes it challenging to jointly distill knowledge from multiple models. Therefore, we propose to whiten the output of teacher models before fusion, which enables effective multi-teacher distillation for retrieval models. Whiten-MTD is conceptually simple and practically effective. Extensive experiments on two landmark image retrieval datasets and one video retrieval dataset demonstrate the effectiveness of our proposed method, and its good balance of retrieval performance and efficiency. Our source code is released at https://github.com/Maryeon/whiten_mtd",
    "checked": true,
    "id": "31943a0c96ffaea90290e9ddadb9908fa1d5b5ad",
    "semantic_title": "let all be whitened: multi-teacher distillation for efficient visual retrieval",
    "citation_count": 1,
    "authors": [
      "Zhe Ma",
      "Jianfeng Dong",
      "Shouling Ji",
      "Zhenguang Liu",
      "Xuhong Zhang",
      "Zonghui Wang",
      "Sifeng He",
      "Feng Qian",
      "Xiaobo Zhang",
      "Lei Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28208": {
    "title": "Cross-Layer and Cross-Sample Feature Optimization Network for Few-Shot Fine-Grained Image Classification",
    "volume": "main",
    "abstract": "Recently, a number of Few-Shot Fine-Grained Image Classification (FS-FGIC) methods have been proposed, but they primarily focus on better fine-grained feature extraction while overlooking two important issues. The first one is how to extract discriminative features for Fine-Grained Image Classification tasks while reducing trivial and non-generalizable sample level noise introduced in this procedure, to overcome the over-fitting problem under the setting of Few-Shot Learning. The second one is how to achieve satisfying feature matching between limited support and query samples with variable spatial positions and angles. To address these issues, we propose a novel Cross-layer and Cross-sample feature optimization Network for FS-FGIC, C2-Net for short. The proposed method consists of two main modules: Cross-Layer Feature Refinement (CLFR) module and Cross-Sample Feature Adjustment (CSFA) module. The CLFR module further refines the extracted features while integrating outputs from multiple layers to suppress sample-level feature noise interference. Additionally, the CSFA module addresses the feature mismatch between query and support samples through both channel activation and position matching operations. Extensive experiments have been conducted on five fine-grained benchmark datasets, and the results show that the C2-Net outperforms other state-of-the-art methods by a significant margin in most cases. Our code is available at: https://github.com/zenith0923/C2-Net",
    "checked": true,
    "id": "f83566ec176d360f8ec67cebd0f6f20d1cea0ac4",
    "semantic_title": "cross-layer and cross-sample feature optimization network for few-shot fine-grained image classification",
    "citation_count": 1,
    "authors": [
      "Zhen-Xiang Ma",
      "Zhen-Duo Chen",
      "Li-Jun Zhao",
      "Zi-Chao Zhang",
      "Xin  Luo",
      "Xin-Shun Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28209": {
    "title": "LMD: Faster Image Reconstruction with Latent Masking Diffusion",
    "volume": "main",
    "abstract": "As a class of fruitful approaches, diffusion probabilistic models (DPMs) have shown excellent advantages in high-resolution image reconstruction. On the other hand, masked autoencoders (MAEs), as popular self-supervised vision learners, have demonstrated simpler and more effective image reconstruction and transfer capabilities on downstream tasks. However, they all require extremely high training costs, either due to inherent high temporal-dependence (i.e., excessively long diffusion steps) or due to artificially low spatial-dependence (i.e., human-formulated high mask ratio, such as 0.75). To the end, this paper presents LMD, a faster image reconstruction framework with Latent Masking Diffusion. First, we propose to project and reconstruct images in latent space through a pre-trained variational autoencoder, which is theoretically more efficient than in the pixel-based space. Then, we combine the advantages of MAEs and DPMs to design a progressive masking diffusion model, which gradually increases the masking proportion by three different schedulers and reconstructs the latent features from simple to difficult, without sequentially performing denoising diffusion as in DPMs or using fixed high masking ratio as in MAEs, so as to alleviate the high training time-consumption predicament. Our approach allows for learning high-capacity models and accelerate their training (by 3x or more) and barely reduces the original accuracy. Inference speed in downstream tasks also significantly outperforms the previous approaches",
    "checked": true,
    "id": "e9568414a40162d1417a9405b5275c62fe476b91",
    "semantic_title": "lmd: faster image reconstruction with latent masking diffusion",
    "citation_count": 3,
    "authors": [
      "Zhiyuan Ma",
      "Zhihuan Yu",
      "Jianjun Li",
      "Bowen Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28210": {
    "title": "AdapEdit: Spatio-Temporal Guided Adaptive Editing Algorithm for Text-Based Continuity-Sensitive Image Editing",
    "volume": "main",
    "abstract": "With the great success of text-conditioned diffusion models in creative text-to-image generation, various text-driven image editing approaches have attracted the attentions of many researchers. However, previous works mainly focus on discreteness-sensitive instructions such as adding, removing or replacing specific objects, background elements or global styles (i.e., \"hard editing\"), while generally ignoring subject-binding but semantically fine-changing continuity-sensitive instructions such as actions, poses or adjectives, and so on (i.e., \"soft editing\"), which hampers generative AI from generating user-customized visual contents. To mitigate this predicament, we propose a spatio-temporal guided adaptive editing algorithm AdapEdit, which realizes adaptive image editing by introducing a soft-attention strategy to dynamically vary the guiding degree from the editing conditions to visual pixels from both temporal and spatial perspectives. Note our approach has a significant advantage in preserving model priors and does not require model training, fine-tuning, extra data, or optimization. We present our results over a wide variety of raw images and editing instructions, demonstrating competitive performance and showing it significantly outperforms the previous approaches. Code is available: https://github.com/AnonymousPony/adap-edit",
    "checked": true,
    "id": "9a800e47315c73a559f26ed30142bdcf8f4fe58e",
    "semantic_title": "adapedit: spatio-temporal guided adaptive editing algorithm for text-based continuity-sensitive image editing",
    "citation_count": 2,
    "authors": [
      "Zhiyuan Ma",
      "Guoli Jia",
      "Bowen Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28211": {
    "title": "Pay Attention to Target: Relation-Aware Temporal Consistency for Domain Adaptive Video Semantic Segmentation",
    "volume": "main",
    "abstract": "Video semantic segmentation has achieved conspicuous achievements attributed to the development of deep learning, but suffers from labor-intensive annotated training data gathering. To alleviate the data-hunger issue, domain adaptation approaches are developed in the hope of adapting the model trained on the labeled synthetic videos to the real videos in the absence of annotations. By analyzing the dominant paradigm consistency regularization in the domain adaptation task, we find that the bottlenecks exist in previous methods from the perspective of pseudo-labels. To take full advantage of the information contained in the pseudo-labels and empower more effective supervision signals, we propose a coherent PAT network including a target domain focalizer and relation-aware temporal consistency. The proposed PAT network enjoys several merits. First, the target domain focalizer is responsible for paying attention to the target domain, and increasing the accessibility of pseudo-labels in consistency training. Second, the relation-aware temporal consistency aims at modeling the inter-class consistent relationship across frames to equip the model with effective supervision signals. Extensive experimental results on two challenging benchmarks demonstrate that our method performs favorably against state-of-the-art domain adaptive video semantic segmentation methods",
    "checked": true,
    "id": "d97d8024e8458d1153d93f1fa916fd2899da8ec5",
    "semantic_title": "pay attention to target: relation-aware temporal consistency for domain adaptive video semantic segmentation",
    "citation_count": 2,
    "authors": [
      "Huayu Mai",
      "Rui Sun",
      "Yuan Wang",
      "Tianzhu Zhang",
      "Feng Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28212": {
    "title": "Improving Automatic VQA Evaluation Using Large Language Models",
    "volume": "main",
    "abstract": "8 years after the visual question answering (VQA) task was proposed, accuracy remains the primary metric for automatic evaluation. VQA Accuracy has been effective so far in the IID evaluation setting. However, our community is undergoing a shift towards open-ended generative models and OOD evaluation. In this new paradigm, the existing VQA Accuracy metric is overly stringent and underestimates the performance of VQA systems. Thus, there is a need to develop more robust automatic VQA metrics that serve as a proxy for human judgment. In this work, we propose to leverage the in-context learning capabilities of instruction-tuned large language models (LLMs) to build a better VQA metric. We formulate VQA evaluation as an answer-rating task where the LLM is instructed to score the accuracy of a candidate answer given a set of reference answers. We demonstrate the proposed metric better correlates with human judgment compared to existing metrics across several VQA models and benchmarks. We hope wide adoption of our metric will contribute to better estimating the research progress on the VQA task. We plan to release the evaluation code and collected human judgments",
    "checked": true,
    "id": "62e633f4b5cf8bc573e496602d3aa6e5919bbe61",
    "semantic_title": "improving automatic vqa evaluation using large language models",
    "citation_count": 5,
    "authors": [
      "Oscar Mañas",
      "Benno Krojer",
      "Aishwarya Agrawal"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28213": {
    "title": "Inconsistency-Based Data-Centric Active Open-Set Annotation",
    "volume": "main",
    "abstract": "Active learning, a method to reduce labeling effort for training deep neural networks, is often limited by the assumption that all unlabeled data belong to known classes. This closed-world assumption fails in practical scenarios with unknown classes in the data, leading to active open-set annotation challenges. Existing methods struggle with this uncertainty. We introduce NEAT, a novel, computationally efficient, data-centric active learning approach for open-set data. NEAT differentiates and labels known classes from a mix of known and unknown classes, using a clusterability criterion and a consistency mea- sure that detects inconsistencies between model predictions and feature distribution. In contrast to recent learning-centric solutions, NEAT shows superior performance in active open- set annotation, as our experiments confirm. Additional details on the further evaluation metrics, implementation, and archi- tecture of our method can be found in the public document at https://arxiv.org/pdf/2401.04923.pdf",
    "checked": true,
    "id": "52b21ed2c15b32e78a448bf4fd75b373533a828c",
    "semantic_title": "inconsistency-based data-centric active open-set annotation",
    "citation_count": 0,
    "authors": [
      "Ruiyu Mao",
      "Ouyang Xu",
      "Yunhui Guo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28214": {
    "title": "Progressive High-Frequency Reconstruction for Pan-Sharpening with Implicit Neural Representation",
    "volume": "main",
    "abstract": "Pan-sharpening aims to leverage the high-frequency signal of the panchromatic (PAN) image to enhance the resolution of its corresponding multi-spectral (MS) image. However, deep neural networks (DNNs) tend to prioritize learning the low-frequency components during the training process, which limits the restoration of high-frequency edge details in MS images. To overcome this limitation, we treat pan-sharpening as a coarse-to-fine high-frequency restoration problem and propose a novel method for achieving high-quality restoration of edge information in MS images. Specifically, to effectively obtain fine-grained multi-scale contextual features, we design a Band-limited Multi-scale High-frequency Generator (BMHG) that generates high-frequency signals from the PAN image within different bandwidths. During training, higher-frequency signals are progressively injected into the MS image, and corresponding residual blocks are introduced into the network simultaneously. This design enables gradients to flow from later to earlier blocks smoothly, encouraging intermediate blocks to concentrate on missing details. Furthermore, to address the issue of pixel position misalignment arising from multi-scale features fusion, we propose a Spatial-spectral Implicit Image Function (SIIF) that employs implicit neural representation to effectively represent and fuse spatial and spectral features in the continuous domain. Extensive experiments on different datasets demonstrate that our method outperforms existing approaches in terms of quantitative and visual measurements for high-frequency detail recovery",
    "checked": true,
    "id": "38bf14038044e0b6105835d57c11c940a2a6f649",
    "semantic_title": "progressive high-frequency reconstruction for pan-sharpening with implicit neural representation",
    "citation_count": 1,
    "authors": [
      "Ge Meng",
      "Jingjia Huang",
      "Yingying Wang",
      "Zhenqi Fu",
      "Xinghao Ding",
      "Yue Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28215": {
    "title": "NaMa: Neighbor-Aware Multi-Modal Adaptive Learning for Prostate Tumor Segmentation on Anisotropic MR Images",
    "volume": "main",
    "abstract": "Accurate segmentation of prostate tumors from multi-modal magnetic resonance (MR) images is crucial for diagnosis and treatment of prostate cancer. However, the robustness of existing segmentation methods is limited, mainly because these methods 1) fail to adaptively assess subject-specific information of each MR modality for accurate tumor delineation, and 2) lack effective utilization of inter-slice information across thick slices in MR images to segment tumor as a whole 3D volume. In this work, we propose a two-stage neighbor-aware multi-modal adaptive learning network (NaMa) for accurate prostate tumor segmentation from multi-modal anisotropic MR images. In particular, in the first stage, we apply subject-specific multi-modal fusion in each slice by developing a novel modality-informativeness adaptive learning (MIAL) module for selecting and adaptively fusing informative representation of each modality based on inter-modality correlations. In the second stage, we exploit inter-slice feature correlations to derive volumetric tumor segmentation. Specifically, we first use a Unet variant with sequence layers to coarsely capture slice relationship at a global scale, and further generate an activation map for each slice. Then, we introduce an activation mapping guidance (AMG) module to refine slice-wise representation (via information from adjacent slices) for consistent tumor segmentation across neighboring slices. Besides, during the network training, we further apply a random mask strategy to each MR modality to improve feature representation efficiency. Experiments on both in-house and public (PICAI) multi-modal prostate tumor datasets show that our proposed NaMa performs better than state-of-the-art methods",
    "checked": true,
    "id": "ca1a12bf119b0dff9e255fc3175020ef10fd66b4",
    "semantic_title": "nama: neighbor-aware multi-modal adaptive learning for prostate tumor segmentation on anisotropic mr images",
    "citation_count": 1,
    "authors": [
      "Runqi Meng",
      "Xiao Zhang",
      "Shijie Huang",
      "Yuning Gu",
      "Guiqin Liu",
      "Guangyu Wu",
      "Nizhuan Wang",
      "Kaicong Sun",
      "Dinggang Shen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28216": {
    "title": "ConVQG: Contrastive Visual Question Generation with Multimodal Guidance",
    "volume": "main",
    "abstract": "Asking questions about visual environments is a crucial way for intelligent agents to understand rich multi-faceted scenes, raising the importance of Visual Question Generation (VQG) systems. Apart from being grounded to the image, existing VQG systems can use textual constraints, such as expected answers or knowledge triplets, to generate focused questions. These constraints allow VQG systems to specify the question content or leverage external commonsense knowledge that can not be obtained from the image content only. However, generating focused questions using textual constraints while enforcing a high relevance to the image content remains a challenge, as VQG systems often ignore one or both forms of grounding. In this work, we propose Contrastive Visual Question Generation (ConVQG), a method using a dual contrastive objective to discriminate questions generated using both modalities from those based on a single one. Experiments on both knowledge-aware and standard VQG benchmarks demonstrate that ConVQG outperforms the state-of-the-art methods and generates image-grounded, text-guided, and knowledge-rich questions. Our human evaluation results also show preference for ConVQG questions compared to non-contrastive baselines",
    "checked": true,
    "id": "e0db3ab50b52252f7cec373dd2157378fb88546c",
    "semantic_title": "convqg: contrastive visual question generation with multimodal guidance",
    "citation_count": 1,
    "authors": [
      "Li Mi",
      "Syrielle Montariol",
      "Javiera Castillo Navarro",
      "Xianjie Dai",
      "Antoine Bosselut",
      "Devis Tuia"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28217": {
    "title": "Out-of-Distribution Detection in Long-Tailed Recognition with Calibrated Outlier Class Learning",
    "volume": "main",
    "abstract": "Existing out-of-distribution (OOD) methods have shown great success on balanced datasets but become ineffective in long-tailed recognition (LTR) scenarios where 1) OOD samples are often wrongly classified into head classes and/or 2) tail-class samples are treated as OOD samples. To address these issues, current studies fit a prior distribution of auxiliary/pseudo OOD data to the long-tailed in-distribution (ID) data. However, it is difficult to obtain such an accurate prior distribution given the unknowingness of real OOD samples and heavy class imbalance in LTR. A straightforward solution to avoid the requirement of this prior is to learn an outlier class to encapsulate the OOD samples. The main challenge is then to tackle the aforementioned confusion between OOD samples and head/tail-class samples when learning the outlier class. To this end, we introduce a novel calibrated outlier class learning (COCL) approach, in which 1) a debiased large margin learning method is introduced in the outlier class learning to distinguish OOD samples from both head and tail classes in the representation space and 2) an outlier-class-aware logit calibration method is defined to enhance the long-tailed classification confidence. Extensive empirical results on three popular benchmarks CIFAR10-LT, CIFAR100-LT, and ImageNet-LT demonstrate that COCL substantially outperforms existing state-of-the-art OOD detection methods in LTR while being able to improve the classification accuracy on ID data. Code is available at https://github.com/mala-lab/COCL",
    "checked": true,
    "id": "f0f220240fc752b6b3c56464d96aeb322f221ef0",
    "semantic_title": "out-of-distribution detection in long-tailed recognition with calibrated outlier class learning",
    "citation_count": 4,
    "authors": [
      "Wenjun Miao",
      "Guansong Pang",
      "Xiao Bai",
      "Tianqi Li",
      "Jin Zheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28218": {
    "title": "BCLNet: Bilateral Consensus Learning for Two-View Correspondence Pruning",
    "volume": "main",
    "abstract": "Correspondence pruning aims to establish reliable correspondences between two related images and recover relative camera motion. Existing approaches often employ a progressive strategy to handle the local and global contexts, with a prominent emphasis on transitioning from local to global, resulting in the neglect of interactions between different contexts. To tackle this issue, we propose a parallel context learning strategy that involves acquiring bilateral consensus for the two-view correspondence pruning task. In our approach, we design a distinctive self-attention block to capture global context and parallel process it with the established local context learning module, which enables us to simultaneously capture both local and global consensuses. By combining these local and global consensuses, we derive the required bilateral consensus. We also design a recalibration block, reducing the influence of erroneous consensus information and enhancing the robustness of the model. The culmination of our efforts is the Bilateral Consensus Learning Network (BCLNet), which efficiently estimates camera pose and identifies inliers (true correspondences). Extensive experiments results demonstrate that our network not only surpasses state-of-the-art methods on benchmark datasets but also showcases robust generalization abilities across various feature extraction techniques. Noteworthily, BCLNet obtains significant improvement gains over the second best method on unknown outdoor dataset, and obviously accelerates model training speed",
    "checked": true,
    "id": "6385e97a4cc5d7f5ab5966c0200366b2e88c6dea",
    "semantic_title": "bclnet: bilateral consensus learning for two-view correspondence pruning",
    "citation_count": 3,
    "authors": [
      "Xiangyang Miao",
      "Guobao Xiao",
      "Shiping Wang",
      "Jun Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28219": {
    "title": "Understanding the Role of the Projector in Knowledge Distillation",
    "volume": "main",
    "abstract": "In this paper we revisit the efficacy of knowledge distillation as a function matching and metric learning problem. In doing so we verify three important design decisions, namely the normalisation, soft maximum function, and projection layers as key ingredients. We theoretically show that the projector implicitly encodes information on past examples, enabling relational gradients for the student. We then show that the normalisation of representations is tightly coupled with the training dynamics of this projector, which can have a large impact on the students performance. Finally, we show that a simple soft maximum function can be used to address any significant capacity gap problems. Experimental results on various benchmark datasets demonstrate that using these insights can lead to superior or comparable performance to state-of-the-art knowledge distillation techniques, despite being much more computationally efficient. In particular, we obtain these results across image classification (CIFAR100 and ImageNet), object detection (COCO2017), and on more difficult distillation objectives, such as training data efficient transformers, whereby we attain a 77.2% top-1 accuracy with DeiT-Ti on ImageNet. Code and models are publicly available",
    "checked": true,
    "id": "d24a0b0bda424d011b24228ba90163421f7a31f2",
    "semantic_title": "understanding the role of the projector in knowledge distillation",
    "citation_count": 6,
    "authors": [
      "Roy Miles",
      "Krystian Mikolajczyk"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28220": {
    "title": "Robust Blind Text Image Deblurring via Maximum Consensus Framework",
    "volume": "main",
    "abstract": "The blind text image deblurring problem presents a formidable challenge, requiring the recovery of a clean and sharp text image from a blurry version with an unknown blur kernel. Sparsity-based strategies have demonstrated their efficacy by emphasizing the sparse priors of the latent image and kernel. However, these existing strategies have largely neglected the influence of additional noise, imposing limitations on their performance. To overcome this limitation, we propose a novel framework designed to effectively mitigate the impact of extensive noise prevalent in blurred images. Our approach centers around a robust Maximum Consensus Framework, wherein we optimize the quantity of interest from the noisy blurry image based on the maximum consensus criterion. Furthermore, we propose the integration of the Alternating Direction Method of Multipliers (ADMM) and the Half-Quadratic Splitting (HQS) method to address the computationally intractable L0 norm problem. This innovative strategy enables improvements in the deblurring performance of blurry text images with the additional synthetic noise. Experimental evaluations conducted on various noisy blurry text images demonstrate the superiority of the proposed approach over existing methods",
    "checked": true,
    "id": "98485ce11520599be3bc4bb25bf7dbcbb4337cf6",
    "semantic_title": "robust blind text image deblurring via maximum consensus framework",
    "citation_count": 0,
    "authors": [
      "Zijian Min",
      "Gundu Mohamed Hassan",
      "Geun-Sik Jo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28221": {
    "title": "Knowledge Guided Semi-supervised Learning for Quality Assessment of User Generated Videos",
    "volume": "main",
    "abstract": "Perceptual quality assessment of user generated content (UGC) videos is challenging due to the requirement of large scale human annotated videos for training. In this work, we address this challenge by first designing a self-supervised Spatio-Temporal Visual Quality Representation Learning (ST-VQRL) framework to generate robust quality aware features for videos. Then, we propose a dual-model based Semi Supervised Learning (SSL) method specifically designed for the Video Quality Assessment (SSL-VQA) task, through a novel knowledge transfer of quality predictions between the two models. Our SSL-VQA method uses the ST-VQRL backbone to produce robust performances across various VQA datasets including cross-database settings, despite being learned with limited human annotated videos. Our model improves the state-of-the-art performance when trained only with limited data by around 10%, and by around 15% when unlabelled data is also used in SSL. Source codes and checkpoints are available at https://github.com/Shankhanil006/SSL-VQA",
    "checked": true,
    "id": "19d1d0bd73e391db945c11f5aae1dbe267f38e82",
    "semantic_title": "knowledge guided semi-supervised learning for quality assessment of user generated videos",
    "citation_count": 0,
    "authors": [
      "Shankhanil Mitra",
      "Rajiv Soundararajan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28222": {
    "title": "Bridging the Gap between 2D and 3D Visual Question Answering: A Fusion Approach for 3D VQA",
    "volume": "main",
    "abstract": "In 3D Visual Question Answering (3D VQA), the scarcity of fully annotated data and limited visual content diversity hampers the generalization to novel scenes and 3D concepts (e.g., only around 800 scenes are utilized in ScanQA and SQA dataset). Current approaches resort supplement 3D reasoning with 2D information. However, these methods face challenges: either they use top-down 2D views that introduce overly complex and sometimes question-irrelevant visual clues, or they rely on globally aggregated scene/image-level representations from 2D VLMs, losing the fine-grained vision-language correlations. To overcome these limitations, our approach utilizes question-conditional 2D view selection procedure, pinpointing semantically relevant 2D inputs for crucial visual clues. We then integrate this 2D knowledge into the 3D-VQA system via a two-branch Transformer structure. This structure, featuring a Twin-Transformer design, compactly combines 2D and 3D modalities and captures fine-grained correlations between modalities, allowing them mutually augmenting each other. Integrating proposed mechanisms above, we present BridgeQA, that offers a fresh perspective on multi-modal transformer-based architectures for 3D-VQA. Experiments validate that BridgeQA achieves state-of-the-art on 3D-VQA datasets and significantly outperforms existing solutions. Code is available at https://github.com/matthewdm0816/BridgeQA",
    "checked": true,
    "id": "c6a1744f928dd5ddf8791c768fb651c3df01c440",
    "semantic_title": "bridging the gap between 2d and 3d visual question answering: a fusion approach for 3d vqa",
    "citation_count": 1,
    "authors": [
      "Wentao Mo",
      "Yang Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28223": {
    "title": "Augmented Commonsense Knowledge for Remote Object Grounding",
    "volume": "main",
    "abstract": "The vision-and-language navigation (VLN) task necessitates an agent to perceive the surroundings, follow natural language instructions, and act in photo-realistic unseen environments. Most of the existing methods employ the entire image or object features to represent navigable viewpoints. However, these representations are insufficient for proper action prediction, especially for the REVERIE task, which uses concise high-level instructions, such as \"Bring me the blue cushion in the master bedroom\". To address enhancing representation, we propose an augmented commonsense knowledge model (ACK) to leverage commonsense information as a spatio-temporal knowledge graph for improving agent navigation. Specifically, the proposed approach involves constructing a knowledge base by retrieving commonsense information from ConceptNet, followed by a refinement module to remove noisy and irrelevant knowledge. We further present ACK which consists of knowledge graph-aware cross-modal and concept aggregation modules to enhance visual representation and visual-textual data alignment by integrating visible objects, commonsense knowledge, and concept history, which includes object and knowledge temporal information. Moreover, we add a new pipeline for the commonsense-based decision-making process which leads to more accurate local action prediction. Experimental results demonstrate our proposed model noticeably outperforms the baseline and archives the state-of-the-art on the REVERIE benchmark. The source code is available at https://github.com/Bahram-Mohammadi/ACK",
    "checked": true,
    "id": "2050b081d35e4ba4432c93383afda0d6d72eb0cc",
    "semantic_title": "augmented commonsense knowledge for remote object grounding",
    "citation_count": 0,
    "authors": [
      "Bahram Mohammadi",
      "Yicong Hong",
      "Yuankai Qi",
      "Qi Wu",
      "Shirui Pan",
      "Javen Qinfeng Shi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28224": {
    "title": "Recurrent Partial Kernel Network for Efficient Optical Flow Estimation",
    "volume": "main",
    "abstract": "Optical flow estimation is a challenging task consisting of predicting per-pixel motion vectors between images. Recent methods have employed larger and more complex models to improve the estimation accuracy. However, this impacts the widespread adoption of optical flow methods and makes it harder to train more general models since the optical flow data is hard to obtain. This paper proposes a small and efficient model for optical flow estimation. We design a new spatial recurrent encoder that extracts discriminative features at a significantly reduced size. Unlike standard recurrent units, we utilize Partial Kernel Convolution (PKConv) layers to produce variable multi-scale features with a single shared block. We also design efficient Separable Large Kernels (SLK) to capture large context information with low computational cost. Experiments on public benchmarks show that we achieve state-of-the-art generalization performance while requiring significantly fewer parameters and memory than competing methods. Our model ranks first in the Spring benchmark without finetuning, improving the results by over 10% while requiring an order of magnitude fewer FLOPs and over four times less memory than the following published method without finetuning. The code is available at github.com/hmorimitsu/ptlflow/tree/main/ptlflow/models/rpknet",
    "checked": true,
    "id": "481279b11b3cc08be9fed241655c568cd6c10d07",
    "semantic_title": "recurrent partial kernel network for efficient optical flow estimation",
    "citation_count": 1,
    "authors": [
      "Henrique Morimitsu",
      "Xiaobin Zhu",
      "Xiangyang Ji",
      "Xu-Cheng Yin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28225": {
    "title": "TETRIS: Towards Exploring the Robustness of Interactive Segmentation",
    "volume": "main",
    "abstract": "Interactive segmentation methods rely on user inputs to iteratively update the selection mask. A click specifying the object of interest is arguably the most simple and intuitive interaction type, and thereby the most common choice for interactive segmentation. However, user clicking patterns in the interactive segmentation context remain unexplored. Accordingly, interactive segmentation evaluation strategies rely more on intuition and common sense rather than empirical studies (e.g., assuming that users tend to click in the center of the area with the largest error). In this work, we conduct a real-user study to investigate real user clicking patterns. This study reveals that the intuitive assumption made in the common evaluation strategy may not hold. As a result, interactive segmentation models may show high scores in the standard benchmarks, but it does not imply that they would perform well in a real world scenario. To assess the applicability of interactive segmentation methods, we propose a novel evaluation strategy providing a more comprehensive analysis of a model's performance. To this end, we propose a methodology for finding extreme user inputs by a direct optimization in a white-box adversarial attack on the interactive segmentation model. Based on the performance with such adversarial user inputs, we assess the robustness of interactive segmentation models w.r.t click positions. Besides, we introduce a novel benchmark for measuring the robustness of interactive segmentation, and report the results of an extensive evaluation of dozens of models",
    "checked": true,
    "id": "34f98bb32cb28d7e8cb176d54c83969a53e3fb6a",
    "semantic_title": "tetris: towards exploring the robustness of interactive segmentation",
    "citation_count": 0,
    "authors": [
      "Andrey Moskalenko",
      "Vlad Shakhuro",
      "Anna Vorontsova",
      "Anton Konushin",
      "Anton Antonov",
      "Alexander Krapukhin",
      "Denis Shepelev",
      "Konstantin Soshin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28226": {
    "title": "T2I-Adapter: Learning Adapters to Dig Out More Controllable Ability for Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "The incredible generative ability of large-scale text-to-image (T2I) models has demonstrated strong power of learning complex structures and meaningful semantics. However, relying solely on text prompts cannot fully take advantage of the knowledge learned by the model, especially when flexible and accurate controlling (e.g., structure and color) is needed. In this paper, we aim to ``dig out\" the capabilities that T2I models have implicitly learned, and then explicitly use them to control the generation more granularly. Specifically, we propose to learn low-cost T2I-Adapters to align internal knowledge in T2I models with external control signals, while freezing the original large T2I models. In this way, we can train various adapters according to different conditions, achieving rich control and editing effects in the color and structure of the generation results. Further, the proposed T2I-Adapters have attractive properties of practical value, such as composability and generalization ability. Extensive experiments demonstrate that our T2I-Adapter has promising generation quality and a wide range of applications. Our code is available at https://github.com/TencentARC/T2I-Adapter",
    "checked": true,
    "id": "58842cdca3ea68f7b9e638b288fc247a6f26dafc",
    "semantic_title": "t2i-adapter: learning adapters to dig out more controllable ability for text-to-image diffusion models",
    "citation_count": 518,
    "authors": [
      "Chong Mou",
      "Xintao Wang",
      "Liangbin Xie",
      "Yanze Wu",
      "Jian Zhang",
      "Zhongang Qi",
      "Ying Shan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28227": {
    "title": "Semi-supervised Open-World Object Detection",
    "volume": "main",
    "abstract": "Conventional open-world object detection (OWOD) problem setting first distinguishes known and unknown classes and then later incrementally learns the unknown objects when introduced with labels in the subsequent tasks. However, the current OWOD formulation heavily relies on the external human oracle for knowledge input during the incremental learning stages. Such reliance on run-time makes this formulation less realistic in a real-world deployment. To address this, we introduce a more realistic formulation, named semi-supervised open-world detection (SS-OWOD), that reduces the annotation cost by casting the incremental learning stages of OWOD in a semi-supervised manner. We demonstrate that the performance of the state-of-the-art OWOD detector dramatically deteriorates in the proposed SS-OWOD setting. Therefore, we introduce a novel SS-OWOD detector, named SS-OWFormer, that utilizes a feature-alignment scheme to better align the object query representations between the original and augmented images to leverage the large unlabeled and few labeled data. We further introduce a pseudo-labeling scheme for unknown detection that exploits the inherent capability of decoder object queries to capture object-specific information. On the COCO dataset, our SS-OWFormer using only 50% of the labeled data achieves detection performance that is on par with the state-of-the-art (SOTA) OWOD detector using all the 100% of labeled data. Further, our SS-OWFormer achieves an absolute gain of 4.8% in unknown recall over the SOTA OWOD detector. Lastly, we demonstrate the effectiveness of our SS-OWOD problem setting and approach for remote sensing object detection, proposing carefully curated splits and baseline performance evaluations. Our experiments on 4 datasets including MS COCO, PASCAL, Objects365 and DOTA demonstrate the effectiveness of our approach. Our source code, models and splits are available here https://github.com/sahalshajim/SS-OWFormer",
    "checked": true,
    "id": "05bc25ab4752935f4a36aaa5c395d6b2278687d0",
    "semantic_title": "semi-supervised open-world object detection",
    "citation_count": 1,
    "authors": [
      "Sahal Shaji Mullappilly",
      "Abhishek Singh Gehlot",
      "Rao Muhammad Anwer ",
      "Fahad Shahbaz Khan",
      "Hisham Cholakkal"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28228": {
    "title": "Adversarial Attacks on the Interpretation of Neuron Activation Maximization",
    "volume": "main",
    "abstract": "Feature visualization is one of the most popular techniques used to interpret the internal behavior of individual units of trained deep neural networks. Based on activation maximization, they consist of finding synthetic or natural inputs that maximize neuron activations. This paper introduces an optimization framework that aims to deceive feature visualization through adversarial model manipulation. It consists of finetuning a pre-trained model with a specifically introduced loss that aims to maintain model performance, while also significantly changing feature visualization. We provide evidence of the success of this manipulation on several pre-trained models for the classification task with ImageNet",
    "checked": true,
    "id": "fdeebf187c390becf9890ccd37e0f189fa3929fd",
    "semantic_title": "adversarial attacks on the interpretation of neuron activation maximization",
    "citation_count": 6,
    "authors": [
      "Geraldin Nanfack",
      "Alexander Fulleringer",
      "Jonathan Marty",
      "Michael Eickenberg",
      "Eugene Belilovsky"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28229": {
    "title": "ColNeRF: Collaboration for Generalizable Sparse Input Neural Radiance Field",
    "volume": "main",
    "abstract": "Neural Radiance Fields (NeRF) have demonstrated impressive potential in synthesizing novel views from dense input, however, their effectiveness is challenged when dealing with sparse input. Existing approaches that incorporate additional depth or semantic supervision can alleviate this issue to an extent. However, the process of supervision collection is not only costly but also potentially inaccurate. In our work, we introduce a novel model: the Collaborative Neural Radiance Fields (ColNeRF) designed to work with sparse input. The collaboration in ColNeRF includes the cooperation among sparse input source images and the cooperation among the output of the NeRF. Through this, we construct a novel collaborative module that aligns information from various views and meanwhile imposes self-supervised constraints to ensure multi-view consistency in both geometry and appearance. A Collaborative Cross-View Volume Integration module (CCVI) is proposed to capture complex occlusions and implicitly infer the spatial location of objects. Moreover, we introduce self-supervision of target rays projected in multiple directions to ensure geometric and color consistency in adjacent regions. Benefiting from the collaboration at the input and output ends, ColNeRF is capable of capturing richer and more generalized scene representation, thereby facilitating higher-quality results of the novel view synthesis. Our extensive experimental results demonstrate that ColNeRF outperforms state-of-the-art sparse input generalizable NeRF methods. Furthermore, our approach exhibits superiority in fine-tuning towards adapting to new scenes, achieving competitive performance compared to per-scene optimized NeRF-based methods while significantly reducing computational costs. Our code is available at: https://github.com/eezkni/ColNeRF",
    "checked": true,
    "id": "a3708c910496a03396735b204139702fbd9c6b00",
    "semantic_title": "colnerf: collaboration for generalizable sparse input neural radiance field",
    "citation_count": 1,
    "authors": [
      "Zhangkai Ni",
      "Peiqi Yang",
      "Wenhan Yang",
      "Hanli Wang",
      "Lin Ma",
      "Sam Kwong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28230": {
    "title": "Wavelet-Driven Spatiotemporal Predictive Learning: Bridging Frequency and Time Variations",
    "volume": "main",
    "abstract": "Spatiotemporal predictive learning is a paradigm that empowers models to learn spatial and temporal patterns by predicting future frames from past frames in an unsupervised manner. This method typically uses recurrent units to capture long-term dependencies, but these units often come with high computational costs and limited performance in real-world scenes. This paper presents an innovative Wavelet-based SpatioTemporal (WaST) framework, which extracts and adaptively controls both low and high-frequency components at image and feature levels via 3D discrete wavelet transform for faster processing while maintaining high-quality predictions. We propose a Time-Frequency Aware Translator uniquely crafted to efficiently learn short- and long-range spatiotemporal information by individually modeling spatial frequency and temporal variations. Meanwhile, we design a wavelet-domain High-Frequency Focal Loss that effectively supervises high-frequency variations. Extensive experiments across various real-world scenarios, such as driving scene prediction, traffic flow prediction, human motion capture, and weather forecasting, demonstrate that our proposed WaST achieves state-of-the-art performance over various spatiotemporal prediction methods",
    "checked": true,
    "id": "b4d2527c78839dcb903bbf1b178df2c1fbcbc30d",
    "semantic_title": "wavelet-driven spatiotemporal predictive learning: bridging frequency and time variations",
    "citation_count": 0,
    "authors": [
      "Xuesong Nie",
      "Yunfeng Yan",
      "Siyuan Li",
      "Cheng Tan",
      "Xi Chen",
      "Haoyuan Jin",
      "Zhihang Zhu",
      "Stan Z. Li",
      "Donglian Qi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28231": {
    "title": "Painterly Image Harmonization by Learning from Painterly Objects",
    "volume": "main",
    "abstract": "Given a composite image with photographic object and painterly background, painterly image harmonization targets at stylizing the composite object to be compatible with the background. Despite the competitive performance of existing painterly harmonization works, they did not fully leverage the painterly objects in artistic paintings. In this work, we explore learning from painterly objects for painterly image harmonization. In particular, we learn a mapping from background style and object information to object style based on painterly objects in artistic paintings. With the learnt mapping, we can hallucinate the target style of composite object, which is used to harmonize encoder feature maps to produce the harmonized image. Extensive experiments on the benchmark dataset demonstrate the effectiveness of our proposed method",
    "checked": true,
    "id": "11a78080ba3b10ed1891e03ba3efb28007784e6f",
    "semantic_title": "painterly image harmonization by learning from painterly objects",
    "citation_count": 0,
    "authors": [
      "Li Niu",
      "Junyan Cao",
      "Yan Hong",
      "Liqing Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28232": {
    "title": "Progressive Painterly Image Harmonization from Low-Level Styles to High-Level Styles",
    "volume": "main",
    "abstract": "Painterly image harmonization aims to harmonize a photographic foreground object on the painterly background. Different from previous auto-encoder based harmonization networks, we develop a progressive multi-stage harmonization network, which harmonizes the composite foreground from low-level styles (e.g., color, simple texture) to high-level styles (e.g., complex texture). Our network has better interpretability and harmonization performance. Moreover, we design an early-exit strategy to automatically decide the proper stage to exit, which can skip the unnecessary and even harmful late stages. Extensive experiments on the benchmark dataset demonstrate the effectiveness of our progressive harmonization network",
    "checked": true,
    "id": "97c970e6e16837b7194167591ad6f0c990606e22",
    "semantic_title": "progressive painterly image harmonization from low-level styles to high-level styles",
    "citation_count": 0,
    "authors": [
      "Li Niu",
      "Yan Hong",
      "Junyan Cao",
      "Liqing Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28233": {
    "title": "Domain Generalizable Person Search Using Unreal Dataset",
    "volume": "main",
    "abstract": "Collecting and labeling real datasets to train the person search networks not only requires a lot of time and effort, but also accompanies privacy issues. The weakly-supervised and unsupervised domain adaptation methods have been proposed to alleviate the labeling burden for target datasets, however, their generalization capability is limited. We introduce a novel person search method based on the domain generalization framework, that uses an automatically labeled unreal dataset only for training but is applicable to arbitrary unseen real datasets. To alleviate the domain gaps when transferring the knowledge from the unreal source dataset to the real target datasets, we estimate the fidelity of person instances which is then used to train the end-to-end network adaptively. Moreover, we devise a domain-invariant feature learning scheme to encourage the network to suppress the domain-related features. Experimental results demonstrate that the proposed method provides the competitive performance to existing person search methods even though it is applicable to arbitrary unseen datasets without any prior knowledge and re-training burdens",
    "checked": true,
    "id": "ff19129891c3a8d9ee706da59f558c9f9929f32d",
    "semantic_title": "domain generalizable person search using unreal dataset",
    "citation_count": 0,
    "authors": [
      "Minyoung Oh",
      "Duhyun Kim",
      "Jae-Young Sim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28234": {
    "title": "OctOcc: High-Resolution 3D Occupancy Prediction with Octree",
    "volume": "main",
    "abstract": "3D semantic occupancy has garnered considerable attention due to its abundant structural information encompassing the entire scene in autonomous driving. However, existing 3D occupancy prediction methods contend with the constraint of low-resolution 3D voxel features arising from the limitation of computational memory. To address this limitation and achieve a more fine-grained representation of 3D scenes, we propose OctOcc, a novel octree-based approach for 3D semantic occupancy prediction. OctOcc is conceptually rooted in the observation that the vast majority of 3D space is left unoccupied. Capitalizing on this insight, we endeavor to cultivate memory-efficient high-resolution 3D occupancy predictions by mitigating superfluous cross-attentions. Specifically, we devise a hierarchical octree structure that selectively generates finer-grained cross-attentions solely in potentially occupied regions. Extending our inquiry beyond 3D space, we identify analogous redundancies within another side of cross attentions, 2D images. Consequently, a 2D image feature filtering network is conceived to expunge extraneous regions. Experimental results demonstrate that the proposed OctOcc significantly outperforms existing methods on nuScenes and SemanticKITTI datasets with limited memory consumption",
    "checked": true,
    "id": "21036ff70cfce9ab1c24af4110be38bbf9a073be",
    "semantic_title": "octocc: high-resolution 3d occupancy prediction with octree",
    "citation_count": 0,
    "authors": [
      "Wenzhe Ouyang",
      "Xiaolin Song",
      "Bailan Feng",
      "Zenglin Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28235": {
    "title": "NeSyFOLD: A Framework for Interpretable Image Classification",
    "volume": "main",
    "abstract": "Deep learning models such as CNNs have surpassed human performance in computer vision tasks such as image classi- fication. However, despite their sophistication, these models lack interpretability which can lead to biased outcomes re- flecting existing prejudices in the data. We aim to make pre- dictions made by a CNN interpretable. Hence, we present a novel framework called NeSyFOLD to create a neurosym- bolic (NeSy) model for image classification tasks. The model is a CNN with all layers following the last convolutional layer replaced by a stratified answer set program (ASP) derived from the last layer kernels. The answer set program can be viewed as a rule-set, wherein the truth value of each pred- icate depends on the activation of the corresponding kernel in the CNN. The rule-set serves as a global explanation for the model and is interpretable. We also use our NeSyFOLD framework with a CNN that is trained using a sparse kernel learning technique called Elite BackProp (EBP). This leads to a significant reduction in rule-set size without compromising accuracy or fidelity thus improving scalability of the NeSy model and interpretability of its rule-set. Evaluation is done on datasets with varied complexity and sizes. We also pro- pose a novel algorithm for labelling the predicates in the rule- set with meaningful semantic concept(s) learnt by the CNN. We evaluate the performance of our \"semantic labelling algo- rithm\" to quantify the efficacy of the semantic labelling for both the NeSy model and the NeSy-EBP model",
    "checked": true,
    "id": "736a1ac552143bc799539957fe17c7dd2401711d",
    "semantic_title": "nesyfold: a framework for interpretable image classification",
    "citation_count": 3,
    "authors": [
      "Parth Padalkar",
      "Huaduo Wang",
      "Gopal Gupta"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28236": {
    "title": "Semi-Supervised Blind Image Quality Assessment through Knowledge Distillation and Incremental Learning",
    "volume": "main",
    "abstract": "Blind Image Quality Assessment (BIQA) aims to simulate human assessment of image quality. It has a great demand for labeled data, which is often insufficient in practice. Some researchers employ unsupervised methods to address this issue, which is challenging to emulate the human subjective system. To this end, we introduce a unified framework that combines semi-supervised and incremental learning to address the mentioned issue. Specifically, when training data is limited, semi-supervised learning is necessary to infer extensive unlabeled data. To facilitate semi-supervised learning, we use knowledge distillation to assign pseudo-labels to unlabeled data, preserving analytical capability. To gradually improve the quality of pseudo labels, we introduce incremental learning. However, incremental learning can lead to catastrophic forgetting. We employ Experience Replay by selecting representative samples during multiple rounds of semi-supervised learning, to alleviate forgetting and ensure model stability. Experimental results show that the proposed approach achieves state-of-the-art performance across various benchmark datasets. After being trained on the LIVE dataset, our method can be directly transferred to the CSIQ dataset. Compared with other methods, it significantly outperforms unsupervised methods on the CSIQ dataset with a marginal performance drop (-0.002) on the LIVE dataset. In conclusion, our proposed method demonstrates its potential to tackle the challenges in real-world production processes",
    "checked": true,
    "id": "cba4f45ae01254b78e48dc88cfad855c7e042835",
    "semantic_title": "semi-supervised blind image quality assessment through knowledge distillation and incremental learning",
    "citation_count": 0,
    "authors": [
      "Wensheng Pan",
      "Timin Gao",
      "Yan Zhang",
      "Xiawu Zheng",
      "Yunhang Shen",
      "Ke Li",
      "Runze Hu",
      "Yutao Liu",
      "Pingyang Dai"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28237": {
    "title": "Less Is More: Label Recommendation for Weakly Supervised Point Cloud Semantic Segmentation",
    "volume": "main",
    "abstract": "Weak supervision has proven to be an effective strategy for reducing the burden of annotating semantic segmentation tasks in 3D space. However, unconstrained or heuristic weakly supervised annotation forms may lead to suboptimal label efficiency. To address this issue, we propose a novel label recommendation framework for weakly supervised point cloud semantic segmentation. Distinct from pre-training and active learning, the label recommendation framework consists of three stages: inductive bias learning, recommendations for points to be labeled, and point cloud semantic segmentation learning. In practice, we first introduce the point cloud upsampling task to induct inductive bias from structural information. During the recommendation stage, we present a cross-scene clustering strategy to generate centers of clustering as recommended points. Then we introduce a recommended point positions attention module LabelAttention to model the long-range dependency under sparse annotations. Additionally, we employ position encoding to enhance the spatial awareness of semantic features. Throughout the framework, the useful information obtained from inductive bias learning is propagated to subsequent semantic segmentation networks in the form of label positions. Experimental results demonstrate that our framework outperforms weakly supervised point cloud semantic segmentation methods and other methods for labeling efficiency on S3DIS and ScanNetV2, even at an extremely low label rate",
    "checked": false,
    "id": "ec674fcb54e8d0f822f6dda186c8fcff2246b34c",
    "semantic_title": "image understands point cloud: weakly supervised 3d semantic segmentation via association learning",
    "citation_count": 4,
    "authors": [
      "Zhiyi Pan",
      "Nan Zhang",
      "Wei Gao",
      "Shan Liu",
      "Ge Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28238": {
    "title": "patchDPCC: A Patchwise Deep Compression Framework for Dynamic Point Clouds",
    "volume": "main",
    "abstract": "When compressing point clouds, point-based deep learning models operate points in a continuous space, which has a chance to minimize the geometric fidelity loss introduced by voxelization in preprocessing. But these methods could hardly scale to inputs with arbitrary points. Furthermore, the point cloud frames are individually compressed, failing the conventional wisdom of leveraging inter-frame similarity. In this work, we propose a patchwise compression framework called patchDPCC, which consists of a patch group generation module and a point-based compression model. Algorithms are developed to generate patches from different frames representing the same object, and more importantly, these patches are regulated to have the same number of points. We also incorporate a feature transfer module in the compression model, which refines the feature quality by exploiting the inter-frame similarity. Our model generates point-wise features for entropy coding, which guarantees the reconstruction speed. The evaluation on the MPEG 8i dataset shows that our method improves the compression ratio by 47.01% and 85.22% when compared to PCGCv2 and V-PCC with the same reconstruction quality, which is 9% and 16% better than that D-DPCC does. Our method also achieves the fastest decoding speed among the learning-based compression models",
    "checked": true,
    "id": "be0a8898cb5ede3215213c3d4d340754015544c8",
    "semantic_title": "patchdpcc: a patchwise deep compression framework for dynamic point clouds",
    "citation_count": 0,
    "authors": [
      "Zirui Pan",
      "Mengbai Xiao",
      "Xu Han",
      "Dongxiao Yu",
      "Guanghui Zhang",
      "Yao Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28239": {
    "title": "LISR: Learning Linear 3D Implicit Surface Representation Using Compactly Supported Radial Basis Functions",
    "volume": "main",
    "abstract": "Implicit 3D surface reconstruction of an object from its partial and noisy 3D point cloud scan is the classical geometry processing and 3D computer vision problem. In the literature, various 3D shape representations have been developed, differing in memory efficiency and shape retrieval effectiveness, such as volumetric, parametric, and implicit surfaces. Radial basis functions provide memory-efficient parameterization of the implicit surface. However, we show that training a neural network using the mean squared error between the ground-truth implicit surface and the linear basis-based implicit surfaces does not converge to the global solution. In this work, we propose locally supported compact radial basis functions for a linear representation of the implicit surface. This representation enables us to generate 3D shapes with arbitrary topologies at any resolution due to their continuous nature. We then propose a neural network architecture for learning the linear implicit shape representation of the 3D surface of an object. We learn linear implicit shapes within a supervised learning framework using ground truth Signed-Distance Field (SDF) data for guidance. The classical strategies face difficulties in finding linear implicit shapes from a given 3D point cloud due to numerical issues (requires solving inverse of a large matrix) in basis and query point selection. The proposed approach achieves better Chamfer distance and comparable F-score than the state-of-the-art approach on the benchmark dataset. We also show the effectiveness of the proposed approach by using it for the 3D shape completion task",
    "checked": true,
    "id": "3dd59295ea89a9d1b985b0afbd4fabe54c2d7488",
    "semantic_title": "lisr: learning linear 3d implicit surface representation using compactly supported radial basis functions",
    "citation_count": 0,
    "authors": [
      "Atharva Pandey",
      "Vishal Yadav",
      "Rajendra Nagar",
      "Santanu Chaudhury"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28240": {
    "title": "RadarMOSEVE: A Spatial-Temporal Transformer Network for Radar-Only Moving Object Segmentation and Ego-Velocity Estimation",
    "volume": "main",
    "abstract": "Moving object segmentation (MOS) and Ego velocity estimation (EVE) are vital capabilities for mobile systems to achieve full autonomy. Several approaches have attempted to achieve MOSEVE using a LiDAR sensor. However, LiDAR sensors are typically expensive and susceptible to adverse weather conditions. Instead, millimeter-wave radar (MWR) has gained popularity in robotics and autonomous driving for real applications due to its cost-effectiveness and resilience to bad weather. Nonetheless, publicly available MOSEVE datasets and approaches using radar data are limited. Some existing methods adopt point convolutional networks from LiDAR-based approaches, ignoring the specific artifacts and the valuable radial velocity information of radar measurements, leading to suboptimal performance. In this paper, we propose a novel transformer network that effectively addresses the sparsity and noise issues and leverages the radial velocity measurements of radar points using our devised radar self- and cross-attention mechanisms. Based on that, our method achieves accurate EVE of the robot and performs MOS using only radar data simultaneously. To thoroughly evaluate the MOSEVE performance of our method, we annotated the radar points in the public View-of-Delft (VoD) dataset and additionally constructed a new radar dataset in various environments. The experimental results demonstrate the superiority of our approach over existing state-of-the-art methods. The code is available at https://github.com/ORCAUboat/RadarMOSEVE",
    "checked": true,
    "id": "0679c1cdd548736a9dc688335e8627047afbfcfe",
    "semantic_title": "radarmoseve: a spatial-temporal transformer network for radar-only moving object segmentation and ego-velocity estimation",
    "citation_count": 0,
    "authors": [
      "Changsong Pang",
      "Xieyuanli Chen",
      "Yimin Liu",
      "Huimin Lu",
      "Yuwei Cheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28241": {
    "title": "NeBLa: Neural Beer-Lambert for 3D Reconstruction of Oral Structures from Panoramic Radiographs",
    "volume": "main",
    "abstract": "Panoramic radiography (Panoramic X-ray, PX) is a widely used imaging modality for dental examination. However, PX only provides a flattened 2D image, lacking in a 3D view of the oral structure. In this paper, we propose NeBLa (Neural Beer-Lambert) to estimate 3D oral structures from real-world PX. NeBLa tackles full 3D reconstruction for varying subjects (patients) where each reconstruction is based only on a single panoramic image. We create an intermediate representation called simulated PX (SimPX) from 3D Cone-beam computed tomography (CBCT) data based on the Beer-Lambert law of X-ray rendering and rotational principles of PX imaging. SimPX aims at not only truthfully simulating PX, but also facilitates the reverting process back to 3D data. We propose a novel neural model based on ray tracing which exploits both global and local input features to convert SimPX to 3D output. At inference, a real PX image is translated to a SimPX-style image with semantic regularization, and the translated image is processed by generation module to produce high-quality outputs. Experiments show that NeBLa outperforms prior state-of-the-art in reconstruction tasks both quantitatively and qualitatively. Unlike prior methods, NeBLa does not require any prior information such as the shape of dental arches, nor the matched PX-CBCT dataset for training, which is difficult to obtain in clinical practice. Our code is available at https://github.com/sihwa-park/nebla",
    "checked": true,
    "id": "518be69aa20c66e6ebea8dac82a247c20bb66583",
    "semantic_title": "nebla: neural beer-lambert for 3d reconstruction of oral structures from panoramic radiographs",
    "citation_count": 1,
    "authors": [
      "Sihwa Park",
      "Seongjun Kim",
      "Doeyoung Kwon",
      "Yohan Jang",
      "In-Seok Song",
      "Seung Jun Baek"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28242": {
    "title": "Task-Disruptive Background Suppression for Few-Shot Segmentation",
    "volume": "main",
    "abstract": "Few-shot segmentation aims to accurately segment novel target objects within query images using only a limited number of annotated support images. The recent works exploit support background as well as its foreground to precisely compute the dense correlations between query and support. However, they overlook the characteristics of the background that generally contains various types of objects. In this paper, we highlight this characteristic of background which can bring problematic cases as follows: (1) when the query and support backgrounds are dissimilar and (2) when objects in the support background are similar to the target object in the query. Without any consideration of the above cases, adopting the entire support background leads to a misprediction of the query foreground as background. To address this issue, we propose Task-disruptive Background Suppression(TBS), a module to suppress those disruptive support background features based on two spatial-wise scores: query-relevant and target-relevant scores. The former aims to mitigate the impact of unshared features solely existing in the support background, while the latter aims to reduce the influence of target-similar support background features. Based on these two scores, we define a query background relevant score that captures the similarity between the backgrounds of the query and the support, and utilize it to scale support background features to adaptively restrict the impact of disruptive support backgrounds. Our proposed method achieves state-of-the-art performance on standard few-shot segmentation benchmarks. Our official code is available at github.com/SuhoPark0706/TBSNet",
    "checked": true,
    "id": "bec47ac3b2fbe26faff0fa128d91b404b235a085",
    "semantic_title": "task-disruptive background suppression for few-shot segmentation",
    "citation_count": 1,
    "authors": [
      "Suho Park",
      "SuBeen Lee",
      "Sangeek Hyun",
      "Hyun Seok Seong",
      "Jae-Pil Heo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28243": {
    "title": "SA²VP: Spatially Aligned-and-Adapted Visual Prompt",
    "volume": "main",
    "abstract": "As a prominent parameter-efficient fine-tuning technique in NLP, prompt tuning is being explored its potential in computer vision. Typical methods for visual prompt tuning follow the sequential modeling paradigm stemming from NLP, which represents an input image as a flattened sequence of token embeddings and then learns a set of unordered parameterized tokens prefixed to the sequence representation as the visual prompts for task adaptation of large vision models. While such sequential modeling paradigm of visual prompt has shown great promise, there are two potential limitations. First, the learned visual prompts cannot model the underlying spatial relations in the input image, which is crucial for image encoding. Second, since all prompt tokens play the same role of prompting for all image tokens without distinction, it lacks the fine-grained prompting capability, i.e., individual prompting for different image tokens. In this work, we propose the Spatially Aligned-and-Adapted Visual Prompt model (SA^2VP), which learns a two-dimensional prompt token map with equal (or scaled) size to the image token map, thereby being able to spatially align with the image map. Each prompt token is designated to prompt knowledge only for the spatially corresponding image tokens. As a result, our model can conduct individual prompting for different image tokens in a fine-grained manner. Moreover, benefiting from the capability of preserving the spatial structure by the learned prompt token map, our SA^2VP is able to model the spatial relations in the input image, leading to more effective prompting. Extensive experiments on three challenging benchmarks for image classification demonstrate the superiority of our model over other state-of-the-art methods for visual prompt tuning. Code is available at https://github.com/tommy-xq/SA2VP",
    "checked": false,
    "id": "90007063f5a263ba249cc860d939b2f9a1b39ed1",
    "semantic_title": "sa2vp: spatially aligned-and-adapted visual prompt",
    "citation_count": 0,
    "authors": [
      "Wenjie Pei",
      "Tongqi Xia",
      "Fanglin Chen",
      "Jinsong Li",
      "Jiandong Tian",
      "Guangming Lu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28244": {
    "title": "ConditionVideo: Training-Free Condition-Guided Video Generation",
    "volume": "main",
    "abstract": "Recent works have successfully extended large-scale text-to-image models to the video domain, producing promising results but at a high computational cost and requiring a large amount of video data. In this work, we introduce ConditionVideo, a training-free approach to text-to-video generation based on the provided condition, video, and input text, by leveraging the power of off-the-shelf text-to-image generation methods (e.g., Stable Diffusion). ConditionVideo generates realistic dynamic videos from random noise or given scene videos. Our method explicitly disentangles the motion representation into condition-guided and scenery motion components. To this end, the ConditionVideo model is designed with a UNet branch and a control branch. To improve temporal coherence, we introduce sparse bi-directional spatial-temporal attention (sBiST-Attn). The 3D control network extends the conventional 2D controlnet model, aiming to strengthen conditional generation accuracy by additionally leveraging the bi-directional frames in the temporal domain. Our method exhibits superior performance in terms of frame consistency, clip score, and conditional accuracy, outperforming other compared methods",
    "checked": false,
    "id": "725929d0ebf07e6a4a3c061d9ffcee7936958e31",
    "semantic_title": "conditionvideo: training-free condition-guided text-to-video generation",
    "citation_count": 6,
    "authors": [
      "Bo Peng",
      "Xinyuan Chen",
      "Yaohui Wang",
      "Chaochao Lu",
      "Yu Qiao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28245": {
    "title": "ViTEraser: Harnessing the Power of Vision Transformers for Scene Text Removal with SegMIM Pretraining",
    "volume": "main",
    "abstract": "Scene text removal (STR) aims at replacing text strokes in natural scenes with visually coherent backgrounds. Recent STR approaches rely on iterative refinements or explicit text masks, resulting in high complexity and sensitivity to the accuracy of text localization. Moreover, most existing STR methods adopt convolutional architectures while the potential of vision Transformers (ViTs) remains largely unexplored. In this paper, we propose a simple-yet-effective ViT-based text eraser, dubbed ViTEraser. Following a concise encoder-decoder framework, ViTEraser can easily incorporate various ViTs to enhance long-range modeling. Specifically, the encoder hierarchically maps the input image into the hidden space through ViT blocks and patch embedding layers, while the decoder gradually upsamples the hidden features to the text-erased image with ViT blocks and patch splitting layers. As ViTEraser implicitly integrates text localization and inpainting, we propose a novel end-to-end pretraining method, termed SegMIM, which focuses the encoder and decoder on the text box segmentation and masked image modeling tasks, respectively. Experimental results demonstrate that ViTEraser with SegMIM achieves state-of-the-art performance on STR by a substantial margin and exhibits strong generalization ability when extended to other tasks, e.g., tampered scene text detection. Furthermore, we comprehensively explore the architecture, pretraining, and scalability of the ViT-based encoder-decoder for STR, which provides deep insights into the application of ViT to the STR field. Code is available at https://github.com/shannanyinxiang/ViTEraser",
    "checked": true,
    "id": "fee6fb37ce240cc147bf88c37b6d9d7cfaf1edf4",
    "semantic_title": "viteraser: harnessing the power of vision transformers for scene text removal with segmim pretraining",
    "citation_count": 2,
    "authors": [
      "Dezhi Peng",
      "Chongyu Liu",
      "Yuliang Liu",
      "Lianwen Jin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28246": {
    "title": "FRIH: Fine-Grained Region-Aware Image Harmonization",
    "volume": "main",
    "abstract": "Image harmonization aims to generate a more realistic appearance of foreground and background for a composite image. All the existing methods perform the same harmonization process for the whole foreground. However, the implanted foreground always contains different appearance patterns. Existing solutions ignore the difference of each color block and lose some specific details. Therefore, we propose a novel global-local two stages framework for Fine-grained Region-aware Image Harmonization (FRIH). In the first stage, the whole input foreground mask is used to make a global coarse-grained harmonization. In the second stage, we adaptively cluster the input foreground mask into several submasks. Each submask and the coarsely adjusted image are concatenated respectively and fed into a lightweight cascaded module, refining the global harmonization result. Moreover, we further design a fusion prediction module to generate the final result, utilizing the different degrees of harmonization results comprehensively. Without bells and whistles, our FRIH achieves a competitive performance on iHarmony4 dataset with a lightweight model",
    "checked": true,
    "id": "5f22c41e8c57ae8537632ec20227ba37868dabbc",
    "semantic_title": "frih: fine-grained region-aware image harmonization",
    "citation_count": 6,
    "authors": [
      "Jinlong Peng",
      "Zekun Luo",
      "Liang Liu",
      "Boshen Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28247": {
    "title": "Navigating Open Set Scenarios for Skeleton-Based Action Recognition",
    "volume": "main",
    "abstract": "In real-world scenarios, human actions often fall outside the distribution of training data, making it crucial for models to recognize known actions and reject unknown ones. However, using pure skeleton data in such open-set conditions poses challenges due to the lack of visual background cues and the distinct sparse structure of body pose sequences. In this paper, we tackle the unexplored Open-Set Skeleton-based Action Recognition (OS-SAR) task and formalize the benchmark on three skeleton-based datasets. We assess the performance of seven established open-set approaches on our task and identify their limits and critical generalization issues when dealing with skeleton information.To address these challenges, we propose a distance-based cross-modality ensemble method that leverages the cross-modal alignment of skeleton joints, bones, and velocities to achieve superior open-set recognition performance. We refer to the key idea as CrossMax - an approach that utilizes a novel cross-modality mean max discrepancy suppression mechanism to align latent spaces during training and a cross-modality distance-based logits refinement method during testing. CrossMax outperforms existing approaches and consistently yields state-of-the-art results across all datasets and backbones. We will release the benchmark, code, and models to the community",
    "checked": true,
    "id": "535bb9caf568b1892edca4f1217dca6d4d42ba8d",
    "semantic_title": "navigating open set scenarios for skeleton-based action recognition",
    "citation_count": 5,
    "authors": [
      "Kunyu Peng",
      "Cheng Yin",
      "Junwei Zheng",
      "Ruiping Liu",
      "David Schneider",
      "Jiaming Zhang",
      "Kailun Yang",
      "M. Saquib Sarfraz",
      "Rainer Stiefelhagen",
      "Alina Roitberg"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28248": {
    "title": "LaneGraph2Seq: Lane Topology Extraction with Language Model via Vertex-Edge Encoding and Connectivity Enhancement",
    "volume": "main",
    "abstract": "Understanding road structures is crucial for autonomous driving. Intricate road structures are often depicted using lane graphs, which include centerline curves and connections forming a Directed Acyclic Graph (DAG). Accurate extraction of lane graphs relies on precisely estimating vertex and edge information within the DAG. Recent research highlights Transformer-based language models' impressive sequence prediction abilities, making them effective for learning graph representations when graph data are encoded as sequences. However, existing studies focus mainly on modeling vertices explicitly, leaving edge information simply embedded in the network. Consequently, these approaches fall short in the task of lane graph extraction. To address this, we introduce LaneGraph2Seq, a novel approach for lane graph extraction. It leverages a language model with vertex-edge encoding and connectivity enhancement. Our serialization strategy includes a vertex-centric depth-first traversal and a concise edge-based partition sequence. Additionally, we use classifier-free guidance combined with nucleus sampling to improve lane connectivity. We validate our method on prominent datasets, nuScenes and Argoverse 2, showcasing consistent and compelling results. Our LaneGraph2Seq approach demonstrates superior performance compared to state-of-the-art techniques in lane graph extraction",
    "checked": true,
    "id": "59b5050f02c84742f0136d0793e1d480d58066d7",
    "semantic_title": "lanegraph2seq: lane topology extraction with language model via vertex-edge encoding and connectivity enhancement",
    "citation_count": 0,
    "authors": [
      "Renyuan Peng",
      "Xinyue Cai",
      "Hang Xu",
      "Jiachen Lu",
      "Feng Wen",
      "Wei Zhang",
      "Li Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28249": {
    "title": "Data Adaptive Traceback for Vision-Language Foundation Models in Image Classification",
    "volume": "main",
    "abstract": "Vision-language foundation models have been incredibly successful in a wide range of downstream computer vision tasks using adaptation methods. However, due to the high cost of obtaining pre-training datasets, pairs with weak image-text correlation in the data exist in large numbers. We call them weak-paired samples. Due to the limitations of these weak-paired samples, the pre-training model are unable to mine all the knowledge from pre-training data. The existing adaptation methods do not consider the missing knowledge, which may lead to crucial task-related knowledge for the downstream tasks being ignored. To address this issue, we propose a new adaptation framework called Data Adaptive Traceback (DAT). Specifically, we utilize a zero-shot-based method to extract the most downstream task-related subset of the pre-training data to enable the downstream tasks. Furthermore, we adopt a pseudo-label-based semi-supervised technique to reuse the pre-training images and a vision-language contrastive learning method to address the confirmation bias issue in semi-supervised learning. We conduct extensive experiments that show our proposed DAT approach meaningfully improves various benchmark datasets' performance over traditional adaptation methods by simply",
    "checked": true,
    "id": "a4c8a1947adddf5ec4665afdd45edc0535ddee50",
    "semantic_title": "data adaptive traceback for vision-language foundation models in image classification",
    "citation_count": 0,
    "authors": [
      "Wenshuo Peng",
      "Kaipeng Zhang",
      "Yue Yang",
      "Hao Zhang",
      "Yu Qiao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28250": {
    "title": "SAM-PARSER: Fine-Tuning SAM Efficiently by Parameter Space Reconstruction",
    "volume": "main",
    "abstract": "Segment Anything Model (SAM) has received remarkable attention as it offers a powerful and versatile solution for object segmentation in images. However, fine-tuning SAM for downstream segmentation tasks under different scenarios remains a challenge, as the varied characteristics of different scenarios naturally requires diverse model parameter spaces. Most existing fine-tuning methods attempt to bridge the gaps among different scenarios by introducing a set of new parameters to modify SAM's original parameter space. Unlike these works, in this paper, we propose fine-tuning SAM efficiently by parameter space reconstruction (SAM-PARSER), which introduce nearly zero trainable parameters during fine-tuning. In SAM-PARSER, we assume that SAM's original parameter space is relatively complete, so that its bases are able to reconstruct the parameter space of a new scenario. We obtain the bases by matrix decomposition, and fine-tuning the coefficients to reconstruct the parameter space tailored to the new scenario by an optimal linear combination of the bases. Experimental results show that SAM-PARSER exhibits superior segmentation performance across various scenarios, while reducing the number of trainable parameters by approximately 290 times compared with current parameter-efficient fine-tuning methods",
    "checked": true,
    "id": "ad59ceccb324f7fb33bdcfbb3caf421fe5b4b044",
    "semantic_title": "sam-parser: fine-tuning sam efficiently by parameter space reconstruction",
    "citation_count": 8,
    "authors": [
      "Zelin Peng",
      "Zhengqin Xu",
      "Zhilin Zeng",
      "Xiaokang Yang",
      "Wei Shen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28251": {
    "title": "Relational Distant Supervision for Image Captioning without Image-Text Pairs",
    "volume": "main",
    "abstract": "Unsupervised image captioning aims to generate descriptions of images without relying on any image-sentence pairs for training. Most existing works use detected visual objects or concepts as bridge to connect images and texts. Considering that the relationship between objects carries more information, we use the object relationship as a more accurate connection between images and texts. In this paper, we adapt the idea of distant supervision that extracts the knowledge about object relationships from an external corpus and imparts them to images to facilitate inferring visual object relationships, without introducing any extra pre-trained relationship detectors. Based on these learned informative relationships, we construct pseudo image-sentence pairs for captioning model training. Specifically, our method consists of three modules: (1) a relationship learning module that learns to infer relationships from images under the distant supervision; (2) a relationship-to-sentence module that transforms the inferred relationships into sentences to generate pseudo image-sentence pairs; (3) an image captioning module that is trained by using the generated image-sentence pairs. Promising results on three datasets show that our method outperforms the state-of-the-art methods of unsupervised image captioning",
    "checked": true,
    "id": "ddd64e0b3111a26e91cf7cde8e567cc27da311c8",
    "semantic_title": "relational distant supervision for image captioning without image-text pairs",
    "citation_count": 0,
    "authors": [
      "Yayun Qi",
      "Wentian Zhao",
      "Xinxiao Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28252": {
    "title": "Bias-Conflict Sample Synthesis and Adversarial Removal Debias Strategy for Temporal Sentence Grounding in Video",
    "volume": "main",
    "abstract": "Temporal Sentence Grounding in Video (TSGV) is troubled by dataset bias issue, which is caused by the uneven temporal distribution of the target moments for samples with similar semantic components in input videos or query texts. Existing methods resort to utilizing prior knowledge about bias to artificially break this uneven distribution, which only removes a limited amount of significant language biases. In this work, we propose the bias-conflict sample synthesis and adversarial removal debias strategy (BSSARD), which dynamically generates bias-conflict samples by explicitly leveraging potentially spurious correlations between single-modality features and the temporal position of the target moments. Through adversarial training, its bias generators continuously introduce biases and generate bias-conflict samples to deceive its grounding model. Meanwhile, the grounding model continuously eliminates the introduced biases, which requires it to model multi-modality alignment information. BSSARD will cover most kinds of coupling relationships and disrupt language and visual biases simultaneously. Extensive experiments on Charades-CD and ActivityNet-CD demonstrate the promising debiasing capability of BSSARD. Source codes are available at https://github.com/qzhb/BSSARD",
    "checked": true,
    "id": "6c74fe6f2d5d901758c2f8d419a444b50b5a0fad",
    "semantic_title": "bias-conflict sample synthesis and adversarial removal debias strategy for temporal sentence grounding in video",
    "citation_count": 0,
    "authors": [
      "Zhaobo Qi",
      "Yibo Yuan",
      "Xiaowen Ruan",
      "Shuhui Wang",
      "Weigang Zhang",
      "Qingming Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28253": {
    "title": "NuScenes-QA: A Multi-Modal Visual Question Answering Benchmark for Autonomous Driving Scenario",
    "volume": "main",
    "abstract": "We introduce a novel visual question answering (VQA) task in the context of autonomous driving, aiming to answer natural language questions based on street-view clues. Compared to traditional VQA tasks, VQA in autonomous driving scenario presents more challenges. Firstly, the raw visual data are multi-modal, including images and point clouds captured by camera and LiDAR, respectively. Secondly, the data are multi-frame due to the continuous, real-time acquisition. Thirdly, the outdoor scenes exhibit both moving foreground and static background. Existing VQA benchmarks fail to adequately address these complexities. To bridge this gap, we propose NuScenes-QA, the first benchmark for VQA in the autonomous driving scenario, encompassing 34K visual scenes and 460K question-answer pairs. Specifically, we leverage existing 3D detection annotations to generate scene graphs and design question templates manually. Subsequently, the question-answer pairs are generated programmatically based on these templates. Comprehensive statistics prove that our NuScenes-QA is a balanced large-scale benchmark with diverse question formats. Built upon it, we develop a series of baselines that employ advanced 3D detection and VQA techniques. Our extensive experiments highlight the challenges posed by this new task. Codes and dataset are available at https://github.com/qiantianwen/NuScenes-QA",
    "checked": true,
    "id": "ee156428803c5bd6e7372f6b27d74bcf88390db3",
    "semantic_title": "nuscenes-qa: a multi-modal visual question answering benchmark for autonomous driving scenario",
    "citation_count": 47,
    "authors": [
      "Tianwen Qian",
      "Jingjing Chen",
      "Linhai Zhuo",
      "Yang Jiao",
      "Yu-Gang Jiang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28254": {
    "title": "X-RefSeg3D: Enhancing Referring 3D Instance Segmentation via Structured Cross-Modal Graph Neural Networks",
    "volume": "main",
    "abstract": "Referring 3D instance segmentation is a challenging task aimed at accurately segmenting a target instance within a 3D scene based on a given referring expression. However, previous methods have overlooked the distinct roles played by different words in referring expressions. Additionally, they have failed to incorporate the positional relationship within referring expressions with the spatial correlations in 3D scenes. To alleviate these issues, we present a novel model called X-RefSeg3D, which constructs a cross-modal graph for the input 3D scene and unites textual and spatial relationships for reasoning via graph neural networks. Our approach begins by capturing object-specific text features, which are then fused with the instance features to construct a comprehensive cross-modal scene graph. Subsequently, we integrate the obtained cross-modal features into graph neural networks, leveraging the K-nearest algorithm to derive explicit instructions from expressions and factual relationships in scenes. This enables the effective capture of higher-order relationships among instances, thereby enhancing feature fusion and facilitating reasoning. Finally, the refined feature undergoes a matching module to compute the ultimate matching score. Experimental results on ScanRefer demonstrate the effectiveness of our method, surpassing previous approaches by a substantial margin of +3.67% in terms of mIOU",
    "checked": true,
    "id": "3656fe765de786f52708a4a81ed05623019a85d1",
    "semantic_title": "x-refseg3d: enhancing referring 3d instance segmentation via structured cross-modal graph neural networks",
    "citation_count": 5,
    "authors": [
      "Zhipeng Qian",
      "Yiwei Ma",
      "Jiayi Ji",
      "Xiaoshuai Sun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28255": {
    "title": "BARET: Balanced Attention Based Real Image Editing Driven by Target-Text Inversion",
    "volume": "main",
    "abstract": "Image editing approaches with diffusion models have been rapidly developed, yet their applicability are subject to requirements such as specific editing types (e.g., foreground or background object editing, style transfer), multiple conditions (e.g., mask, sketch, caption), and time consuming fine-tuning of diffusion models. For alleviating these limitations and realizing efficient real image editing, we propose a novel editing technique that only requires an input image and target text for various editing types including non-rigid edits without fine-tuning diffusion model. Our method contains three novelties: (I) Target-text Inversion Schedule (TTIS) is designed to fine-tune the input target text embedding to achieve fast image reconstruction without image caption and acceleration of convergence. (II) Progressive Transition Scheme applies progressive linear interpolation between target text embedding and its fine-tuned version to generate transition embedding for maintaining non-rigid editing capability. (III) Balanced Attention Module (BAM) balances the tradeoff between textual description and image semantics. By the means of combining self-attention map from reconstruction process and cross-attention map from transition process, the guidance of target text embeddings in diffusion process is optimized. In order to demonstrate editing capability, effectiveness and efficiency of the proposed BARET, we have conducted extensive qualitative and quantitative experiments. Moreover, results derived from user study and ablation study further prove the superiority over other methods",
    "checked": false,
    "id": "35020a4da2a0d1e3b782974e8b01c0bdb1a2ffaf",
    "semantic_title": "baret : balanced attention based real image editing driven by target-text inversion",
    "citation_count": 0,
    "authors": [
      "Yuming Qiao",
      "Fanyi Wang",
      "Jingwen Su",
      "Yanhao Zhang",
      "Yunjie Yu",
      "Siyu Wu",
      "Guo-Jun Qi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28256": {
    "title": "High-Fidelity 3D Head Avatars Reconstruction through Spatially-Varying Expression Conditioned Neural Radiance Field",
    "volume": "main",
    "abstract": "One crucial aspect of 3D head avatar reconstruction lies in the details of facial expressions. Although recent NeRF-based photo-realistic 3D head avatar methods achieve high-quality avatar rendering, they still encounter challenges retaining intricate facial expression details because they overlook the potential of specific expression variations at different spatial positions when conditioning the radiance field. Motivated by this observation, we introduce a novel Spatially-Varying Expression (SVE) conditioning. The SVE can be obtained by a simple MLP-based generation network, encompassing both spatial positional features and global expression information. Benefiting from rich and diverse information of the SVE at different positions, the proposed SVE-conditioned NeRF can deal with intricate facial expressions and achieve realistic rendering and geometry details of high-fidelity 3D head avatars. Additionally, to further elevate the geometric and rendering quality, we introduce a new coarse-to-fine training strategy, including a geometry initialization strategy at the coarse stage and an adaptive importance sampling strategy at the fine stage. Extensive experiments indicate that our method outperforms other state-of-the-art (SOTA) methods in rendering and geometry quality on mobile phone-collected and public datasets. Code and data can be found at https://github.com/minghanqin/AvatarSVE",
    "checked": true,
    "id": "325561d087b3d1552b52e5d9d33d07119fa62450",
    "semantic_title": "high-fidelity 3d head avatars reconstruction through spatially-varying expression conditioned neural radiance field",
    "citation_count": 1,
    "authors": [
      "Minghan Qin",
      "Yifan Liu",
      "Yuelang Xu",
      "Xiaochen Zhao",
      "Yebin Liu",
      "Haoqian Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28257": {
    "title": "Text2City: One-Stage Text-Driven Urban Layout Regeneration",
    "volume": "main",
    "abstract": "Regenerating urban layout is an essential process for urban regeneration. In this paper, we propose a new task called text-driven urban layout regeneration, which provides an intuitive input modal - text - for users to specify the regeneration, instead of designing complex rules. Given the target region to be regenerated, we propose a one-stage text-driven urban layout regeneration model, Text2City, to jointly and progressively regenerate the urban layout (i.e., road and building layouts) based on textual layout descriptions and surrounding context (i.e., urban layouts and functions of the surrounding regions). Text2City first extracts road and building attributes from the textual layout description to guide the regeneration. It includes a novel one-stage joint regenerator network based on the conditioned denoising diffusion probabilistic models (DDPMs) and prior knowledge exchange. To harmonize the regenerated layouts through joint optimization, we propose the interactive & enhanced guidance module for self-enhancement and prior knowledge exchange between road and building layouts during the regeneration. We also design a series of constraints from attribute-, geometry- and pixel-levels to ensure rational urban layout generation. To train our model, we build a large-scale dataset containing urban layouts and layout descriptions, covering 147K regions. Qualitative and quantitative evaluations show that our proposed method outperforms the baseline methods in regenerating desirable urban layouts that meet the textual descriptions",
    "checked": true,
    "id": "5181489c629a5f8feddcd08bb60e1b33d114c2e8",
    "semantic_title": "text2city: one-stage text-driven urban layout regeneration",
    "citation_count": 1,
    "authors": [
      "Yiming Qin",
      "Nanxuan Zhao",
      "Bin Sheng",
      "Rynson W.H. Lau"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28258": {
    "title": "Empowering CAM-Based Methods with Capability to Generate Fine-Grained and High-Faithfulness Explanations",
    "volume": "main",
    "abstract": "Recently, the explanation of neural network models has garnered considerable research attention. In computer vision, CAM (Class Activation Map)-based methods and LRP (Layer-wise Relevance Propagation) method are two common explanation methods. However, since most CAM-based methods can only generate global weights, they can only generate coarse-grained explanations at a deep layer. LRP and its variants, on the other hand, can generate fine-grained explanations. But the faithfulness of the explanations is too low. To address these challenges, in this paper, we propose FG-CAM (Fine-Grained CAM), which extends CAM-based methods to enable generating fine-grained and high-faithfulness explanations. FG-CAM uses the relationship between two adjacent layers of feature maps with resolution differences to gradually increase the explanation resolution, while finding the contributing pixels and filtering out the pixels that do not contribute. Our method not only solves the shortcoming of CAM-based methods without changing their characteristics, but also generates fine-grained explanations that have higher faithfulness than LRP and its variants. We also present FG-CAM with denoising, which is a variant of FG-CAM and is able to generate less noisy explanations with almost no change in explanation faithfulness. Experimental results show that the performance of FG-CAM is almost unaffected by the explanation resolution. FG-CAM outperforms existing CAM-based methods significantly in both shallow and intermediate layers, and outperforms LRP and its variants significantly in the input layer. Our code is available at https://github.com/dongmo-qcq/FG-CAM",
    "checked": true,
    "id": "b4f9919572e4ffbb8bfd7e491fc5e6005ab97965",
    "semantic_title": "empowering cam-based methods with capability to generate fine-grained and high-faithfulness explanations",
    "citation_count": 1,
    "authors": [
      "Changqing Qiu",
      "Fusheng Jin",
      "Yining Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28259": {
    "title": "High-Order Structure Based Middle-Feature Learning for Visible-Infrared Person Re-identification",
    "volume": "main",
    "abstract": "Visible-infrared person re-identification (VI-ReID) aims to retrieve images of the same persons captured by visible (VIS) and infrared (IR) cameras. Existing VI-ReID methods ignore high-order structure information of features while being relatively difficult to learn a reasonable common feature space due to the large modality discrepancy between VIS and IR images. To address the above problems, we propose a novel high-order structure based middle-feature learning network (HOS-Net) for effective VI-ReID. Specifically, we first leverage a short- and long-range feature extraction (SLE) module to effectively exploit both short-range and long-range features. Then, we propose a high-order structure learning (HSL) module to successfully model the high-order relationship across different local features of each person image based on a whitened hypergraph network. This greatly alleviates model collapse and enhances feature representations. Finally, we develop a common feature space learning (CFL) module to learn a discriminative and reasonable common feature space based on middle features generated by aligning features from different modalities and ranges. In particular, a modality-range identity-center contrastive (MRIC) loss is proposed to reduce the distances between the VIS, IR, and middle features, smoothing the training process. Extensive experiments on the SYSU-MM01, RegDB, and LLCM datasets show that our HOS-Net achieves superior state-of-the-art performance. Our code is available at https://github.com/Jaulaucoeng/HOS-Net",
    "checked": true,
    "id": "f597c6434120f0c10a40e6d710160e1ce9e28d09",
    "semantic_title": "high-order structure based middle-feature learning for visible-infrared person re-identification",
    "citation_count": 0,
    "authors": [
      "Liuxiang Qiu",
      "Si Chen",
      "Yan Yan",
      "Jing-Hao Xue",
      "Da-Han Wang",
      "Shunzhi Zhu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28260": {
    "title": "Mining Fine-Grained Image-Text Alignment for Zero-Shot Captioning via Text-Only Training",
    "volume": "main",
    "abstract": "Image captioning aims at generating descriptive and meaningful textual descriptions of images, enabling a broad range of vision-language applications. Prior works have demonstrated that harnessing the power of Contrastive Image Language Pre-training (CLIP) offers a promising approach to achieving zero-shot captioning, eliminating the need for expensive caption annotations. However, the widely observed modality gap in the latent space of CLIP harms the performance of zero-shot captioning by breaking the alignment between paired image-text features. To address this issue, we conduct an analysis on the CLIP latent space which leads to two findings. Firstly, we observe that the CLIP's visual feature of image subregions can achieve closer proximity to the paired caption due to the inherent information loss in text descriptions. In addition, we show that the modality gap between a paired image-text can be empirically modeled as a zero-mean Gaussian distribution. Motivated by the findings, we propose a novel zero-shot image captioning framework with text-only training to reduce the modality gap. In particular, we introduce a subregion feature aggregation to leverage local region information, which produces a compact visual representation for matching text representation. Moreover, we incorporate a noise injection and CLIP reranking strategy to boost captioning performance. We also extend our framework to build a zero-shot VQA pipeline, demonstrating its generality. Through extensive experiments on common captioning and VQA datasets such as MSCOCO, Flickr30k and VQAV2, we show that our method achieves remarkable performance improvements. Code is available at https://github.com/Artanic30/MacCap",
    "checked": true,
    "id": "08d669b23413700acefb0f3b2b1512f8a299f4ab",
    "semantic_title": "mining fine-grained image-text alignment for zero-shot captioning via text-only training",
    "citation_count": 0,
    "authors": [
      "Longtian Qiu",
      "Shan Ning",
      "Xuming He"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28261": {
    "title": "HiHPQ: Hierarchical Hyperbolic Product Quantization for Unsupervised Image Retrieval",
    "volume": "main",
    "abstract": "Existing unsupervised deep product quantization methods primarily aim for the increased similarity between different views of the identical image, whereas the delicate multi-level semantic similarities preserved between images are overlooked. Moreover, these methods predominantly focus on the Euclidean space for computational convenience, compromising their ability to map the multi-level semantic relationships between images effectively. To mitigate these shortcomings, we propose a novel unsupervised product quantization method dubbed Hierarchical Hyperbolic Product Quantization (HiHPQ), which learns quantized representations by incorporating hierarchical semantic similarity within hyperbolic geometry. Specifically, we propose a hyperbolic product quantizer, where the hyperbolic codebook attention mechanism and the quantized contrastive learning on the hyperbolic product manifold are introduced to expedite quantization. Furthermore, we propose a hierarchical semantics learning module, designed to enhance the distinction between similar and non-matching images for a query by utilizing the extracted hierarchical semantics as an additional training supervision. Experiments on benchmark image datasets show that our proposed method outperforms state-of-the-art baselines",
    "checked": true,
    "id": "2d6182ec7a94bae41e22eab12febfd1a2e3d6367",
    "semantic_title": "hihpq: hierarchical hyperbolic product quantization for unsupervised image retrieval",
    "citation_count": 1,
    "authors": [
      "Zexuan Qiu",
      "Jiahong Liu",
      "Yankai Chen",
      "Irwin King"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28262": {
    "title": "S2CycleDiff: Spatial-Spectral-Bilateral Cycle-Diffusion Framework for Hyperspectral Image Super-resolution",
    "volume": "main",
    "abstract": "Hyperspectral image super-resolution (HISR) is a technique that can break through the limitation of imaging mechanism to obtain the hyperspectral image (HSI) with high spatial resolution. Although some progress has been achieved by existing methods, most of them directly learn the spatial-spectral joint mapping between the observed images and the target high-resolution HSI (HrHSI), failing to fully reserve the spectral distribution of low-resolution HSI (LrHSI) and the spatial distribution of high-resolution multispectral imagery (HrMSI). To this end, we propose a spatial-spectral-bilateral cycle-diffusion framework (S2CycleDiff) for HISR, which can step-wise generate the HrHSI with high spatial-spectral fidelity by learning the conditional distribution of spatial and spectral super-resolution processes bilaterally. Specifically, a customized conditional cycle-diffusion framework is designed as the backbone to achieve the spatial-spectral-bilateral super-resolution by repeated refinement, wherein the spatial/spectral guided pyramid denoising (SGPD) module seperately takes HrMSI and LrHSI as the guiding factors to achieve the spatial details injection and spectral correction. The outputs of the conditional cycle-diffusion framework are fed into a complementary fusion block to integrate the spatial and spectral details to generate the desired HrHSI. Experiments have been conducted on three widely used datasets to demonstrate the superiority of the proposed method over state-of-the-art HISR methods. The code is available at https://github.com/Jiahuiqu/S2CycleDiff",
    "checked": true,
    "id": "d577e53bf21d40f8700f5a34860a20b905c3ade8",
    "semantic_title": "s2cyclediff: spatial-spectral-bilateral cycle-diffusion framework for hyperspectral image super-resolution",
    "citation_count": 1,
    "authors": [
      "Jiahui Qu",
      "Jie He",
      "Wenqian Dong",
      "Jingyu Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28263": {
    "title": "E2HQV: High-Quality Video Generation from Event Camera via Theory-Inspired Model-Aided Deep Learning",
    "volume": "main",
    "abstract": "The bio-inspired event cameras or dynamic vision sensors are capable of asynchronously capturing per-pixel brightness changes (called event-streams) in high temporal resolution and high dynamic range. However, the non-structural spatial-temporal event-streams make it challenging for providing intuitive visualization with rich semantic information for human vision. It calls for events-to-video (E2V) solutions which take event-streams as input and generate high quality video frames for intuitive visualization. However, current solutions are predominantly data-driven without considering the prior knowledge of the underlying statistics relating event-streams and video frames. It highly relies on the non-linearity and generalization capability of the deep neural networks, thus, is struggling on reconstructing detailed textures when the scenes are complex. In this work, we propose E2HQV, a novel E2V paradigm designed to produce high-quality video frames from events. This approach leverages a model-aided deep learning framework, underpinned by a theory-inspired E2V model, which is meticulously derived from the fundamental imaging principles of event cameras. To deal with the issue of state-reset in the recurrent components of E2HQV, we also design a temporal shift embedding module to further improve the quality of the video frames. Comprehensive evaluations on the real world event camera datasets validate our approach, with E2HQV, notably outperforming state-of-the-art approaches, e.g., surpassing the second best by over 40% for some evaluation metrics",
    "checked": true,
    "id": "2fdddd153261888696c2a6dd02f4f2148102bade",
    "semantic_title": "e2hqv: high-quality video generation from event camera via theory-inspired model-aided deep learning",
    "citation_count": 0,
    "authors": [
      "Qiang Qu",
      "Yiran Shen",
      "Xiaoming Chen",
      "Yuk Ying Chung",
      "Tongliang Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28264": {
    "title": "BLiRF: Bandlimited Radiance Fields for Dynamic Scene Modeling",
    "volume": "main",
    "abstract": "Inferring the 3D structure of a non-rigid dynamic scene from a single moving camera is an under-constrained problem. Inspired by the remarkable progress of neural radiance fields (NeRFs) in photo-realistic novel view synthesis of static scenes, it has also been extended to dynamic settings. Such methods heavily rely on implicit neural priors to regularize the problem. In this work, we take a step back and investigate how current implementations may entail deleterious effects including limited expressiveness, entanglement of light and density fields, and sub-optimal motion localization. Further, we devise a factorisation-based framework that represents the scene as a composition of bandlimited, high-dimensional signals. We demonstrate compelling results across complex dynamic scenes that involve changes in lighting, texture and long-range dynamics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sameera Ramasinghe",
      "Violetta Shevchenko",
      "Gil Avraham",
      "Anton van den Hengel"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28265": {
    "title": "Cross-Sentence Gloss Consistency for Continuous Sign Language Recognition",
    "volume": "main",
    "abstract": "Continuous sign language recognition (CSLR) aims to recognize gloss sequences from continuous sign videos. Recent works enhance the gloss representation consistency by mining correlations between visual and contextual modules within individual sentences. However, there still remain much richer correlations among glosses across different sentences. In this paper, we present a simple yet effective Cross-Sentence Gloss Consistency (CSGC), which enforces glosses belonging to a same category to be more consistent in representation than those belonging to different categories, across all training sentences. Specifically, in CSGC, a prototype is maintained for each gloss category and benefits the gloss discrimination in a contrastive way. Thanks to the well-distinguished gloss prototype, an auxiliary similarity classifier is devised to enhance the recognition clues, thus yielding more accurate results. Extensive experiments conducted on three CSLR datasets show that our proposed CSGC significantly boosts the performance of CSLR, surpassing existing state-of-the-art works by large margins (i.e., 1.6% on PHOENIX14, 2.4% on PHOENIX14-T, and 5.7% on CSL-Daily)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Rao",
      "Ke Sun",
      "Xiaohan Wang",
      "Qi Wang",
      "Bang Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28266": {
    "title": "Forecasting Bimanual Object Manipulation Sequences from Unimanual Observations",
    "volume": "main",
    "abstract": "Learning to forecast bimanual object manipulation sequences from unimanual observations has broad applications in assistive robots and augmented reality. This challenging task requires us to first infer motion from the missing arm and the object it would have been manipulating were the person bimanual, then forecast the human and object motion while maintaining hand-object contact during manipulation. Previous attempts model the hand-object interactions only implicitly, and thus tend to produce unrealistic motion where the objects float in air. We address this with a novel neural network that (i) identifies and forecasts the pose for only the objects undergoing motion through an object motion module and (ii) refines human pose predictions by encouraging hand-object contact during manipulation through an ensemble of human pose predictors. The components are also designed to be generic enough for use in both unimanual and bimanual contexts. Our approach outperforms the state-of-the-art pose forecasting methods on bimanual manipulation datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haziq Razali",
      "Yiannis Demiris"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28267": {
    "title": "Multi-Step Denoising Scheduled Sampling: Towards Alleviating Exposure Bias for Diffusion Models",
    "volume": "main",
    "abstract": "Denoising Diffusion Probabilistic Models (DDPMs) have achieved significant success in generation tasks. Nevertheless, the exposure bias issue, i.e., the natural discrepancy between the training (the output of each step is calculated individually by a given input) and inference (the output of each step is calculated based on the input iteratively obtained based on the model), harms the performance of DDPMs. To our knowledge, few works have tried to tackle this issue by modifying the training process for DDPMs, but they still perform unsatisfactorily due to 1) partially modeling the discrepancy and 2) ignoring the prediction error accumulation. To address the above issues, in this paper, we propose a multi-step denoising scheduled sampling (MDSS) strategy to alleviate the exposure bias for DDPMs. Analyzing the formulations of the training and inference of DDPMs, MDSS 1) comprehensively considers the discrepancy influence of prediction errors on the output of the model (the Gaussian noise) and the output of the step (the calculated input signal of the next step), and 2) efficiently models the prediction error accumulation by using multiple iterations of a mathematical formulation initialized from one-step prediction error obtained from the model. The experimental results, compared with previous works, demonstrate that our approach is more effective in mitigating exposure bias in DDPM, DDIM, and DPM-solver. In particular, MDSS achieves an FID score of 3.86 in 100 sample steps of DDIM on the CIFAR-10 dataset, whereas the second best obtains 4.78. The code will be available on GitHub",
    "checked": true,
    "id": "7a97dc8a97053cf77a01fe947d2463bc868d2d1e",
    "semantic_title": "multi-step denoising scheduled sampling: towards alleviating exposure bias for diffusion models",
    "citation_count": 0,
    "authors": [
      "Zhiyao Ren",
      "Yibing Zhan",
      "Liang Ding",
      "Gaoang Wang",
      "Chaoyue Wang",
      "Zhongyi Fan",
      "Dacheng Tao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28268": {
    "title": "CRA-PCN: Point Cloud Completion with Intra- and Inter-level Cross-Resolution Transformers",
    "volume": "main",
    "abstract": "Point cloud completion is an indispensable task for recovering complete point clouds due to incompleteness caused by occlusion, limited sensor resolution, etc. The family of coarse-to-fine generation architectures has recently exhibited great success in point cloud completion and gradually became mainstream. In this work, we unveil one of the key ingredients behind these methods: meticulously devised feature extraction operations with explicit cross-resolution aggregation. We present Cross-Resolution Transformer that efficiently performs cross-resolution aggregation with local attention mechanisms. With the help of our recursive designs, the proposed operation can capture more scales of features than common aggregation operations, which is beneficial for capturing fine geometric characteristics. While prior methodologies have ventured into various manifestations of inter-level cross-resolution aggregation, the effectiveness of intra-level one and their combination has not been analyzed. With unified designs, Cross-Resolution Transformer can perform intra- or inter-level cross-resolution aggregation by switching inputs. We integrate two forms of Cross-Resolution Transformers into one up-sampling block for point generation, and following the coarse-to-fine manner, we construct CRA-PCN to incrementally predict complete shapes with stacked up-sampling blocks. Extensive experiments demonstrate that our method outperforms state-of-the-art methods by a large margin on several widely used benchmarks. Codes are available at https://github.com/EasyRy/CRA-PCN",
    "checked": true,
    "id": "e143cacc0d81e28877335ffb871fbcd95858d469",
    "semantic_title": "cra-pcn: point cloud completion with intra- and inter-level cross-resolution transformers",
    "citation_count": 0,
    "authors": [
      "Yi Rong",
      "Haoran Zhou",
      "Lixin Yuan",
      "Cheng Mei",
      "Jiahao Wang",
      "Tong Lu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28269": {
    "title": "Entropic Open-Set Active Learning",
    "volume": "main",
    "abstract": "Active Learning (AL) aims to enhance the performance of deep models by selecting the most informative samples for annotation from a pool of unlabeled data. Despite impressive performance in closed-set settings, most AL methods fail in real-world scenarios where the unlabeled data contains unknown categories. Recently, a few studies have attempted to tackle the AL problem for the open-set setting. However, these methods focus more on selecting known samples and do not efficiently utilize unknown samples obtained during AL rounds. In this work, we propose an Entropic Open-set AL (EOAL) framework which leverages both known and unknown distributions effectively to select informative samples during AL rounds. Specifically, our approach employs two different entropy scores. One measures the uncertainty of a sample with respect to the known-class distributions. The other measures the uncertainty of the sample with respect to the unknown-class distributions. By utilizing these two entropy scores we effectively separate the known and unknown samples from the unlabeled data resulting in better sampling. Through extensive experiments, we show that the proposed method outperforms existing state-of-the-art methods on CIFAR-10, CIFAR-100, and TinyImageNet datasets. Code is available at https://github.com/bardisafa/EOAL",
    "checked": true,
    "id": "886747cfbabdfa6b52beaf0a0bd003b7ec39c662",
    "semantic_title": "entropic open-set active learning",
    "citation_count": 3,
    "authors": [
      "Bardia Safaei",
      "Vibashan VS",
      "Celso M. de Melo",
      "Vishal M. Patel"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28270": {
    "title": "Generating Images of Rare Concepts Using Pre-trained Diffusion Models",
    "volume": "main",
    "abstract": "Text-to-image diffusion models can synthesize high quality images, but they have various limitations. Here we highlight a common failure mode of these models, namely, generating uncommon concepts and structured concepts like hand palms. We show that their limitation is partly due to the long-tail nature of their training data: web-crawled data sets are strongly unbalanced, causing models to under-represent concepts from the tail of the distribution. We characterize the effect of unbalanced training data on text-to-image models and offer a remedy. We show that rare concepts can be correctly generated by carefully selecting suitable generation seeds in the noise space, using a small reference set of images, a technique that we call SeedSelect. SeedSelect does not require retraining or finetuning the diffusion model. We assess the faithfulness, quality and diversity of SeedSelect in creating rare objects and generating complex formations like hand images, and find it consistently achieves superior performance. We further show the advantage of SeedSelect in semantic data augmentation. Generating semantically appropriate images can successfully improve performance in few-shot recognition benchmarks, for classes from the head and from the tail of the training data of diffusion models",
    "checked": true,
    "id": "241536d067624791d590af8b94f3f80a99328233",
    "semantic_title": "generating images of rare concepts using pre-trained diffusion models",
    "citation_count": 18,
    "authors": [
      "Dvir Samuel",
      "Rami Ben-Ari",
      "Simon Raviv",
      "Nir Darshan",
      "Gal Chechik"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28271": {
    "title": "RG-GAN: Dynamic Regenerative Pruning for Data-Efficient Generative Adversarial Networks",
    "volume": "main",
    "abstract": "Training Generative Adversarial Networks (GAN) to generate high-quality images typically requires large datasets. Network pruning during training has recently emerged as a significant advancement for data-efficient GAN. However, simple and straightforward pruning can lead to the risk of losing key information, resulting in suboptimal results due to GAN's competitive dynamics between generator (G) and discriminator (D). Addressing this, we present RG-GAN, a novel approach that marks the first incorporation of dynamic weight regeneration and pruning in GAN training to improve the quality of the generated samples, even with limited data. Specifically, RG-GAN initiates layer-wise dynamic pruning by removing less important weights to the quality of the generated images. While pruning enhances efficiency, excessive sparsity within layers can pose a risk of model collapse. To mitigate this issue, RG-GAN applies a dynamic regeneration method to reintroduce specific weights when they become important, ensuring a balance between sparsity and image quality. Though effective, the sparse network achieved through this process might eliminate some weights important to the combined G and D performance, a crucial aspect for achieving stable and effective GAN training. RG-GAN addresses this loss of weights by integrating learned sparse network weights back into the dense network at the previous stage during a follow-up regeneration step. Our results consistently demonstrate RG-GAN's robust performance across a variety of scenarios, including different GAN architectures, datasets, and degrees of data scarcity, reinforcing its value as a generic training methodology. Results also show that data augmentation exhibits improved performance in conjunction with RG-GAN. Furthermore, RG-GAN can achieve fewer parameters without compromising, and even enhancing, the quality of the generated samples. Code can be found at this link: https://github.com/IntellicentAI-Lab/RG-GAN",
    "checked": true,
    "id": "33e8b0e8a7d4e37f6b0462c6345bab921c593d03",
    "semantic_title": "rg-gan: dynamic regenerative pruning for data-efficient generative adversarial networks",
    "citation_count": 0,
    "authors": [
      "Divya Saxena",
      "Jiannong Cao",
      "Jiahao Xu",
      "Tarun Kulshrestha"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28272": {
    "title": "SeTformer Is What You Need for Vision and Language",
    "volume": "main",
    "abstract": "The dot product self-attention (DPSA) is a fundamental component of transformers. However, scaling them to long sequences, like documents or high-resolution images, becomes prohibitively expensive due to the quadratic time and memory complexities arising from the softmax operation. Kernel methods are employed to simplify computations by approximating softmax but often lead to performance drops compared to softmax attention. We propose SeTformer, a novel transformer where DPSA is purely replaced by Self-optimal Transport (SeT) for achieving better performance and computational efficiency. SeT is based on two essential softmax properties: maintaining a non-negative attention matrix and using a nonlinear reweighting mechanism to emphasize important tokens in input sequences. By introducing a kernel cost function for optimal transport, SeTformer effectively satisfies these properties. In particular, with small and base-sized models, SeTformer achieves impressive top-1 accuracies of 84.7% and 86.2% on ImageNet-1K. In object detection, SeTformer-base outperforms the FocalNet counterpart by +2.2 mAP, using 38% fewer parameters and 29% fewer FLOPs. In semantic segmentation, our base-size model surpasses NAT by +3.5 mIoU with 33% fewer parameters. SeTformer also achieves state-of-the-art results in language modeling on the GLUE benchmark. These findings highlight SeTformer applicability for vision and language tasks",
    "checked": true,
    "id": "98f41341c87448f32065eff170222274b3e5a906",
    "semantic_title": "setformer is what you need for vision and language",
    "citation_count": 0,
    "authors": [
      "Pourya Shamsolmoali",
      "Masoumeh Zareapoor",
      "Eric Granger",
      "Michael Felsberg"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28273": {
    "title": "Multi-Domain Multi-Scale Diffusion Model for Low-Light Image Enhancement",
    "volume": "main",
    "abstract": "Diffusion models have achieved remarkable progress in low-light image enhancement. However, there remain two practical limitations: (1) existing methods mainly focus on the spatial domain for the diffusion process, while neglecting the essential features in the frequency domain; (2) conventional patch-based sampling strategy inevitably leads to severe checkerboard artifacts due to the uneven overlapping. To address these limitations in one go, we propose a Multi-Domain Multi-Scale (MDMS) diffusion model for low-light image enhancement. In particular, we introduce a spatial-frequency fusion module to seamlessly integrates spatial and frequency information. By leveraging the Multi-Domain Learning (MDL) paradigm, our proposed model is endowed with the capability to adaptively facilitate noise distribution learning, thereby enhancing the quality of the generated images. Meanwhile, we propose a Multi-Scale Sampling (MSS) strategy that follows a divide-ensemble manner by merging the restored patches under different resolutions. Such a multi-scale learning paradigm explicitly derives patch information from different granularities, thus leading to smoother boundaries. Furthermore, we empirically adopt the Bright Channel Prior (BCP) which indicates natural statistical regularity as an additional restoration guidance. Experimental results on LOL and LOLv2 datasets demonstrate that our method achieves state-of-the-art performance for the low-light image enhancement task. Codes are available at https://github.com/Oliiveralien/MDMS",
    "checked": true,
    "id": "f779a1a0d293aca6d753cc4c6d67c1ddbc147899",
    "semantic_title": "multi-domain multi-scale diffusion model for low-light image enhancement",
    "citation_count": 0,
    "authors": [
      "Kai Shang",
      "Mingwen  Shao",
      "Chao Wang",
      "Yuanshuo Cheng",
      "Shuigen Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28274": {
    "title": "Polyper: Boundary Sensitive Polyp Segmentation",
    "volume": "main",
    "abstract": "We present a new boundary sensitive framework for polyp segmentation, termed Polyper.Our method is motivated by a clinical approach that seasoned medical practitioners often leverage the inherent features of interior polyp regions to tackle blurred boundaries.Inspired by this, we propose to explicitly leverages boundary regions to bolster the model's boundary discrimination capability while minimizing computational resource wastage. Our approach first extracts low-confidence boundary regions and high-confidence prediction regions from an initial segmentation map through differentiable morphological operators.Then, we design the boundary sensitive attention that concentrates on augmenting the features near the boundary regions using the high-confidence prediction region's characteristics to generate good segmentation results.Our proposed method can be seamlessly integrated with classical encoder networks, like ResNet-50, MiT-B1, and Swin Transformer.To evaludate the effectiveness of Polyper, we conduct experiments on five publicly available challenging datasets, and receive state-of-the-art performance on all of them. Code is available at https://github.com/haoshao-nku/medical_seg.git",
    "checked": true,
    "id": "17f4c28040d098497e21492b6a19916e47d6e812",
    "semantic_title": "polyper: boundary sensitive polyp segmentation",
    "citation_count": 0,
    "authors": [
      "Hao Shao",
      "Yang Zhang",
      "Qibin Hou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28275": {
    "title": "Collaborative Consortium of Foundation Models for Open-World Few-Shot Learning",
    "volume": "main",
    "abstract": "Open-World Few-Shot Learning (OFSL) is a crucial research field dedicated to accurately identifying target samples in scenarios where data is limited and labels are unreliable. This research holds significant practical implications and is highly relevant to real-world applications. Recently, the advancements in foundation models like CLIP and DINO have showcased their robust representation capabilities even in resource-constrained settings with scarce data. This realization has brought about a transformative shift in focus, moving away from \"building models from scratch\" towards \"effectively harnessing the potential of foundation models to extract pertinent prior knowledge suitable for OFSL and utilizing it sensibly\". Motivated by this perspective, we introduce the Collaborative Consortium of Foundation Models (CO3), which leverages CLIP, DINO, GPT-3, and DALL-E to collectively address the OFSL problem. CO3 comprises four key blocks: (1) the Label Correction Block (LC-Block) corrects unreliable labels, (2) the Data Augmentation Block (DA-Block) enhances available data, (3) the Feature Extraction Block (FE-Block) extracts multi-modal features, and (4) the Text-guided Fusion Adapter (TeFu-Adapter) integrates multiple features while mitigating the impact of noisy labels through semantic constraints. Only the adapter's parameters are adjustable, while the others remain frozen. Through collaboration among these foundation models, CO3 effectively unlocks their potential and unifies their capabilities to achieve state-of-the-art performance on multiple benchmark datasets. https://github.com/The-Shuai/CO3",
    "checked": true,
    "id": "90668de8b1c5dcb0471444e3177dc28e20fce5d4",
    "semantic_title": "collaborative consortium of foundation models for open-world few-shot learning",
    "citation_count": 2,
    "authors": [
      "Shuai Shao",
      "Yu Bai",
      "Yan Wang",
      "Baodi Liu",
      "Bin Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28276": {
    "title": "FaceCoresetNet: Differentiable Coresets for Face Set Recognition",
    "volume": "main",
    "abstract": "In set-based face recognition, we aim to compute the most discriminative descriptor from an unbounded set of images and videos showing a single person. A discriminative descriptor balances two policies when aggregating information from a given set. The first is a quality-based policy: emphasizing high-quality and down-weighting low-quality images. The second is a diversity-based policy: emphasizing unique images in the set and down-weighting multiple occurrences of similar images as found in video clips which can overwhelm the set representation. This work frames face-set representation as a differentiable coreset selection problem. Our model learns how to select a small coreset of the input set that balances quality and diversity policies using a learned metric parameterized by the face quality, optimized end-to-end. The selection process is a differentiable farthest-point sampling (FPS) realized by approximating the non-differentiable Argmax operation with differentiable sampling from the Gumbel-Softmax distribution of distances. The small coreset is later used as queries in a self and cross-attention architecture to enrich the descriptor with information from the whole set. Our model is order-invariant and linear in the input set size. We set a new SOTA to set face verification on the IJB-B and IJB-C datasets. Our code is publicly available at https://github.com/ligaripash/FaceCoresetNet",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gil Shapira",
      "Yosi Keller"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28277": {
    "title": "Decouple Content and Motion for Conditional Image-to-Video Generation",
    "volume": "main",
    "abstract": "The goal of conditional image-to-video (cI2V) generation is to create a believable new video by beginning with the condition, i.e., one image and text. The previous cI2V generation methods conventionally perform in RGB pixel space, with limitations in modeling motion consistency and visual continuity. Additionally, the efficiency of generating videos in pixel space is quite low. In this paper, we propose a novel approach to address these challenges by disentangling the target RGB pixels into two distinct components: spatial content and temporal motions. Specifically, we predict temporal motions which include motion vector and residual based on a 3D-UNet diffusion model. By explicitly modeling temporal motions and warping them to the starting image, we improve the temporal consistency of generated videos. This results in a reduction of spatial redundancy, emphasizing temporal details. Our proposed method achieves performance improvements by disentangling content and motion, all without introducing new structural complexities to the model. Extensive experiments on various datasets confirm our approach's superior performance over the majority of state-of-the-art methods in both effectiveness and efficiency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cuifeng Shen",
      "Yulu Gan",
      "Chen Chen",
      "Xiongwei Zhu",
      "Lele Cheng",
      "Tingting Gao",
      "Jinzhi Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28278": {
    "title": "GroundVLP: Harnessing Zero-Shot Visual Grounding from Vision-Language Pre-training and Open-Vocabulary Object Detection",
    "volume": "main",
    "abstract": "Visual grounding, a crucial vision-language task involving the understanding of the visual context based on the query expression, necessitates the model to capture the interactions between objects, as well as various spatial and attribute information. However, the annotation data of visual grounding task is limited due to its time-consuming and labor-intensive annotation process, resulting in the trained models being constrained from generalizing its capability to a broader domain. To address this challenge, we propose GroundVLP, a simple yet effective zero-shot method that harnesses visual grounding ability from the existing models trained from image-text pairs and pure object detection data, both of which are more conveniently obtainable and offer a broader domain compared to visual grounding annotation data. GroundVLP proposes a fusion mechanism that combines the heatmap from GradCAM and the object proposals of open-vocabulary detectors. We demonstrate that the proposed method significantly outperforms other zero-shot methods on RefCOCO/+/g datasets, surpassing prior zero-shot state-of-the-art by approximately 28% on the test split of RefCOCO and RefCOCO+. Furthermore, GroundVLP performs comparably to or even better than some non-VLP-based supervised models on the Flickr30k entities dataset. Our code is available at https://github.com/om-ai-lab/GroundVLP",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haozhan Shen",
      "Tiancheng Zhao",
      "Mingwei Zhu",
      "Jianwei Yin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28279": {
    "title": "Automatic Radiology Reports Generation via Memory Alignment Network",
    "volume": "main",
    "abstract": "The automatic generation of radiology reports is of great significance, which can reduce the workload of doctors and improve the accuracy and reliability of medical diagnosis and treatment, and has attracted wide attention in recent years. Cross-modal mapping between images and text, a key component of generating high-quality reports, is challenging due to the lack of corresponding annotations. Despite its importance, previous studies have often overlooked it or lacked adequate designs for this crucial component. In this paper, we propose a method with memory alignment embedding to assist the model in aligning visual and textual features to generate a coherent and informative report. Specifically, we first get the memory alignment embedding by querying the memory matrix, where the query is derived from a combination of the visual features and their corresponding positional embeddings. Then the alignment between the visual and textual features can be guided by the memory alignment embedding during the generation process. The comparison experiments with other alignment methods show that the proposed alignment method is less costly and more effective. The proposed approach achieves better performance than state-of-the-art approaches on two public datasets IU X-Ray and MIMIC-CXR, which further demonstrates the effectiveness of the proposed alignment method",
    "checked": true,
    "id": "07a1521b03f38a43bddfcff50db98d4caa9a5e5a",
    "semantic_title": "automatic radiology reports generation via memory alignment network",
    "citation_count": 0,
    "authors": [
      "Hongyu Shen",
      "Mingtao Pei",
      "Juncai Liu",
      "Zhaoxing Tian"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28280": {
    "title": "CGMGM: A Cross-Gaussian Mixture Generative Model for Few-Shot Semantic Segmentation",
    "volume": "main",
    "abstract": "Few-shot semantic segmentation (FSS) aims to segment unseen objects in a query image using a few pixel-wise annotated support images, thus expanding the capabilities of semantic segmentation. The main challenge lies in extracting sufficient information from the limited support images to guide the segmentation process. Conventional methods typically address this problem by generating single or multiple prototypes from the support images and calculating their cosine similarity to the query image. However, these methods often fail to capture meaningful information for modeling the de facto joint distribution of pixel and category. Consequently, they result in incomplete segmentation of foreground objects and mis-segmentation of the complex background. To overcome this issue, we propose the Cross Gaussian Mixture Generative Model (CGMGM), a novel Gaussian Mixture Models~(GMMs)-based FSS method, which establishes the joint distribution of pixel and category in both the support and query images. Specifically, our method initially matches the feature representations of the query image with those of the support images to generate and refine an initial segmentation mask. It then employs GMMs to accurately model the joint distribution of foreground and background using the support masks and the initial segmentation mask. Subsequently, a parametric decoder utilizes the posterior probability of pixels in the query image, by applying the Bayesian theorem, to the joint distribution, to generate the final segmentation mask. Experimental results on PASCAL-5i and COCO-20i datasets demonstrate our CGMGM's effectiveness and superior performance compared to the state-of-the-art methods",
    "checked": true,
    "id": "a6da546174299f0d60352e6e8ff50bfed2ee25dd",
    "semantic_title": "cgmgm: a cross-gaussian mixture generative model for few-shot semantic segmentation",
    "citation_count": 0,
    "authors": [
      "Junao Shen",
      "Kun Kuang",
      "Jiaheng Wang",
      "Xinyu Wang",
      "Tian Feng",
      "Wei Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28281": {
    "title": "Learn How to See: Collaborative Embodied Learning for Object Detection and Camera Adjusting",
    "volume": "main",
    "abstract": "Passive object detectors, trained on large-scale static datasets, often overlook the feedback from object detection to image acquisition. Embodied vision and active detection mitigate this issue by interacting with the environment. Nevertheless, the materialization of activeness hinges on resource-intensive data collection and annotation. To tackle these challenges, we propose a collaborative student-teacher framework. Technically, a replay buffer is built based on the trajectory data to encapsulate the relationship of state, action, and reward. In addition, the student network diverges from reinforcement learning by redefining sequential decision pathways using a GPT structure enriched with causal self-attention. Moreover, the teacher network establishes a subtle state-reward mapping based on adjacent benefit differences, providing reliable rewards for student adaptively self-tuning with the vast unlabeled replay buffer data. Additionally, an innovative yet straightforward benefit reference value is proposed within the teacher network, adding to its effectiveness and simplicity. Leveraging a flexible replay buffer and embodied collaboration between teacher and student, the framework learns to see before detection with shallower features and shorter inference steps. Experiments highlight significant advantages of our algorithm over state-of-the-art detectors. The code is released at https://github.com/lydonShen/STF",
    "checked": true,
    "id": "6ee7c0ce4eca777a892486f980251069dc665055",
    "semantic_title": "learn how to see: collaborative embodied learning for object detection and camera adjusting",
    "citation_count": 0,
    "authors": [
      "Lingdong Shen",
      "Chunlei Huo",
      "Nuo Xu",
      "Chaowei Han",
      "Zichen Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28282": {
    "title": "Distributed Manifold Hashing for Image Set Classification and Retrieval",
    "volume": "main",
    "abstract": "Conventional image set methods typically learn from image sets stored in one location. However, in real-world applications, image sets are often distributed or collected across different positions. Learning from such distributed image sets presents a challenge that has not been studied thus far. Moreover, efficiency is seldom addressed in large-scale image set applications. To fulfill these gaps, this paper proposes Distributed Manifold Hashing (DMH), which models distributed image sets as a connected graph. DMH employs Riemannian manifold to effectively represent each image set and further suggests learning hash code for each image set to achieve efficient computation and storage. DMH is formally formulated as a distributed learning problem with local consistency constraint on global variables among neighbor nodes, and can be optimized in parallel. Extensive experiments on three benchmark datasets demonstrate that DMH achieves highly competitive accuracies in a distributed setting and provides faster classification and retrieval than state-of-the-arts",
    "checked": true,
    "id": "f077ae63c209b7f3695d4cd925a7362172f54659",
    "semantic_title": "distributed manifold hashing for image set classification and retrieval",
    "citation_count": 0,
    "authors": [
      "Xiaobo Shen",
      "Peizhuo Song",
      "Yun-Hao Yuan",
      "Yuhui Zheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28283": {
    "title": "Controllable 3D Face Generation with Conditional Style Code Diffusion",
    "volume": "main",
    "abstract": "Generating photorealistic 3D faces from given conditions is a challenging task. Existing methods often rely on time-consuming one-by-one optimization approaches, which are not efficient for modeling the same distribution content, e.g., faces. Additionally, an ideal controllable 3D face generation model should consider both facial attributes and expressions. Thus we propose a novel approach called TEx-Face(TExt & Expression-to-Face) that addresses these challenges by dividing the task into three components, i.e., 3D GAN Inversion, Conditional Style Code Diffusion, and 3D Face Decoding. For 3D GAN inversion, we introduce two methods, which aim to enhance the representation of style codes and alleviate 3D inconsistencies. Furthermore, we design a style code denoiser to incorporate multiple conditions into the style code and propose a data augmentation strategy to address the issue of insufficient paired visual-language data. Extensive experiments conducted on FFHQ, CelebA-HQ, and CelebA-Dialog demonstrate the promising performance of our TEx-Face in achieving the efficient and controllable generation of photorealistic 3D faces. The code will be publicly available",
    "checked": true,
    "id": "346670ed8f4c284d89f150dd33f1a769dd5bd7f4",
    "semantic_title": "controllable 3d face generation with conditional style code diffusion",
    "citation_count": 4,
    "authors": [
      "Xiaolong Shen",
      "Jianxin Ma",
      "Chang Zhou",
      "Zongxin Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28284": {
    "title": "Adaptive Integration of Partial Label Learning and Negative Learning for Enhanced Noisy Label Learning",
    "volume": "main",
    "abstract": "There has been significant attention devoted to the effectiveness of various domains, such as semi-supervised learning, contrastive learning, and meta-learning, in enhancing the performance of methods for noisy label learning (NLL) tasks. However, most existing methods still depend on prior assumptions regarding clean samples amidst different sources of noise (e.g., a pre-defined drop rate or a small subset of clean samples). In this paper, we propose a simple yet powerful idea called NPN, which revolutionizes Noisy label learning by integrating Partial label learning (PLL) and Negative learning (NL). Toward this goal, we initially decompose the given label space adaptively into the candidate and complementary labels, thereby establishing the conditions for PLL and NL. We propose two adaptive data-driven paradigms of label disambiguation for PLL: hard disambiguation and soft disambiguation. Furthermore, we generate reliable complementary labels using all non-candidate labels for NL to enhance model robustness through indirect supervision. To maintain label reliability during the later stage of model training, we introduce a consistency regularization term that encourages agreement between the outputs of multiple augmentations. Experiments conducted on both synthetically corrupted and real-world noisy datasets demonstrate the superiority of NPN compared to other state-of-the-art (SOTA) methods. The source code has been made available at https://github.com/NUST-Machine-Intelligence-Laboratory/NPN",
    "checked": true,
    "id": "caa8e1e2ef611cff4f8005d1391c6f465c9f2b82",
    "semantic_title": "adaptive integration of partial label learning and negative learning for enhanced noisy label learning",
    "citation_count": 6,
    "authors": [
      "Mengmeng Sheng",
      "Zeren Sun",
      "Zhenhuang Cai",
      "Tao Chen",
      "Yichao Zhou",
      "Yazhou Yao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28285": {
    "title": "Transformer-Based No-Reference Image Quality Assessment via Supervised Contrastive Learning",
    "volume": "main",
    "abstract": "Image Quality Assessment (IQA) has long been a research hotspot in the field of image processing, especially No-Reference Image Quality Assessment (NR-IQA). Due to the powerful feature extraction ability, existing Convolution Neural Network (CNN) and Transformers based NR-IQA methods have achieved considerable progress. However, they still exhibit limited capability when facing unknown authentic distortion datasets. To further improve NR-IQA performance, in this paper, a novel supervised contrastive learning (SCL) and Transformer-based NR-IQA model SaTQA is proposed. We first train a model on a large-scale synthetic dataset by SCL (no image subjective score is required) to extract degradation features of images with various distortion types and levels. To further extract distortion information from images, we propose a backbone network incorporating the Multi-Stream Block (MSB) by combining the CNN inductive bias and Transformer long-term dependence modeling capability. Finally, we propose the Patch Attention Block (PAB) to obtain the final distorted image quality score by fusing the degradation features learned from contrastive learning with the perceptual distortion information extracted by the backbone network. Experimental results on six standard IQA datasets show that SaTQA outperforms the state-of-the-art methods for both synthetic and authentic datasets. Code is available at https://github.com/I2-Multimedia-Lab/SaTQA",
    "checked": true,
    "id": "552ef6992843277e10ffc7d08ffa389b9bb1f52d",
    "semantic_title": "transformer-based no-reference image quality assessment via supervised contrastive learning",
    "citation_count": 2,
    "authors": [
      "Jinsong Shi",
      "Pan Gao",
      "Jie Qin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28286": {
    "title": "Explicit Visual Prompts for Visual Object Tracking",
    "volume": "main",
    "abstract": "How to effectively exploit spatio-temporal information is crucial to capture target appearance changes in visual tracking. However, most deep learning-based trackers mainly focus on designing a complicated appearance model or template updating strategy, while lacking the exploitation of context between consecutive frames and thus entailing the when-and-how-to-update dilemma. To address these issues, we propose a novel explicit visual prompts framework for visual tracking, dubbed EVPTrack. Specifically, we utilize spatio-temporal tokens to propagate information between consecutive frames without focusing on updating templates. As a result, we cannot only alleviate the challenge of when-to-update, but also avoid the hyper-parameters associated with updating strategies. Then, we utilize the spatio-temporal tokens to generate explicit visual prompts that facilitate inference in the current frame. The prompts are fed into a transformer encoder together with the image tokens without additional processing. Consequently, the efficiency of our model is improved by avoiding how-to-update. In addition, we consider multi-scale information as explicit visual prompts, providing multiscale template features to enhance the EVPTrack's ability to handle target scale changes. Extensive experimental results on six benchmarks (i.e., LaSOT, LaSOText, GOT-10k, UAV123, TrackingNet, and TNL2K.) validate that our EVPTrack can achieve competitive performance at a real-time speed by effectively exploiting both spatio-temporal and multi-scale information. Code and models are available at https://github.com/GXNU-ZhongLab/EVPTrack",
    "checked": true,
    "id": "11285524ad300043c53e7e003e0406d8fd555de3",
    "semantic_title": "explicit visual prompts for visual object tracking",
    "citation_count": 2,
    "authors": [
      "Liangtao Shi",
      "Bineng Zhong",
      "Qihua Liang",
      "Ning Li",
      "Shengping Zhang",
      "Xianxian Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28287": {
    "title": "Evidential Uncertainty-Guided Mitochondria Segmentation for 3D EM Images",
    "volume": "main",
    "abstract": "Recent advances in deep learning have greatly improved the segmentation of mitochondria from Electron Microscopy (EM) images. However, suffering from variations in mitochondrial morphology, imaging conditions, and image noise, existing methods still exhibit high uncertainty in their predictions. Moreover, in view of our findings, predictions with high levels of uncertainty are often accompanied by inaccuracies such as ambiguous boundaries and amount of false positive segments. To deal with the above problems, we propose a novel approach for mitochondria segmentation in 3D EM images that leverages evidential uncertainty estimation, which for the first time integrates evidential uncertainty to enhance the performance of segmentation. To be more specific, our proposed method not only provides accurate segmentation results, but also estimates associated uncertainty. Then, the estimated uncertainty is used to help improve the segmentation performance by an uncertainty rectification module, which leverages uncertainty maps and multi-scale information to refine the segmentation. Extensive experiments conducted on four challenging benchmarks demonstrate the superiority of our proposed method over existing approaches",
    "checked": true,
    "id": "2709604df35d686d1469faaba80cd0652a53dee5",
    "semantic_title": "evidential uncertainty-guided mitochondria segmentation for 3d em images",
    "citation_count": 0,
    "authors": [
      "Ruohua Shi",
      "Lingyu Duan",
      "Tiejun Huang",
      "Tingting Jiang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28288": {
    "title": "Towards Squeezing-Averse Virtual Try-On via Sequential Deformation",
    "volume": "main",
    "abstract": "In this paper, we first investigate a visual quality degradation problem observed in recent high-resolution virtual try-on approach. The tendency is empirically found that the textures of clothes are squeezed at the sleeve, as visualized in the upper row of Fig.1(a). A main reason for the issue arises from a gradient conflict between two popular losses, the Total Variation (TV) and adversarial losses. Specifically, the TV loss aims to disconnect boundaries between the sleeve and torso in a warped clothing mask, whereas the adversarial loss aims to combine between them. Such contrary objectives feedback the misaligned gradients to a cascaded appearance flow estimation, resulting in undesirable squeezing artifacts. To reduce this, we propose a Sequential Deformation (SD-VITON) that disentangles the appearance flow prediction layers into TV objective-dominant (TVOB) layers and a task-coexistence (TACO) layer. Specifically, we coarsely fit the clothes onto a human body via the TVOB layers, and then keep on refining via the TACO layer. In addition, the bottom row of Fig.1(a) shows a different type of squeezing artifacts around the waist. To address it, we further propose that we first warp the clothes into a tucked-out shirts style, and then partially erase the texture from the warped clothes without hurting the smoothness of the appearance flows. Experimental results show that our SD-VITON successfully resolves both types of artifacts and outperforms the baseline methods. Source code will be available at https://github.com/SHShim0513/SD-VITON",
    "checked": true,
    "id": "5117dd8c25841e60dee0b5661e83d87ad9f1264f",
    "semantic_title": "towards squeezing-averse virtual try-on via sequential deformation",
    "citation_count": 4,
    "authors": [
      "Sang-Heon Shim",
      "Jiwoo Chung",
      "Jae-Pil Heo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28289": {
    "title": "DPA-P2PNet: Deformable Proposal-Aware P2PNet for Accurate Point-Based Cell Detection",
    "volume": "main",
    "abstract": "Point-based cell detection (PCD), which pursues high-performance cell sensing under low-cost data annotation, has garnered increased attention in computational pathology community. Unlike mainstream PCD methods that rely on intermediate density map representations, the Point-to-Point network (P2PNet) has recently emerged as an end-to-end solution for PCD, demonstrating impressive cell detection accuracy and efficiency. Nevertheless, P2PNet is limited to decoding from a single-level feature map due to the scale-agnostic property of point proposals, which is insufficient to leverage multi-scale information. Moreover, the spatial distribution of pre-set point proposals is biased from that of cells, leading to inaccurate cell localization. To lift these limitations, we present DPA-P2PNet in this work. The proposed method directly extracts multi-scale features for decoding according to the coordinates of point proposals on hierarchical feature maps. On this basis, we further devise deformable point proposals to mitigate the positional bias between proposals and potential cells to promote cell localization. Inspired by practical pathological diagnosis that usually combines high-level tissue structure and low-level cell morphology for accurate cell classification, we propose a multi-field-of-view (mFoV) variant of DPA-P2PNet to accommodate additional large FoV images with tissue information as model input. Finally, we execute the first self-supervised pre-training on immunohistochemistry histopathology image data and evaluate the suitability of four representative self-supervised methods on the PCD task. Experimental results on three benchmarks and a large-scale and real-world interval dataset demonstrate the superiority of our proposed models over the state-of-the-art counterparts. Codes and pre-trained weights are available at https://github.com/windygoo/DPA-P2PNet",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongyi Shui",
      "Sunyi Zheng",
      "Chenglu Zhu",
      "Shichuan Zhang",
      "Xiaoxuan Yu",
      "Honglin Li",
      "Jingxiong Li",
      "Pingyi Chen",
      "Lin Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28290": {
    "title": "DVANet: Disentangling View and Action Features for Multi-View Action Recognition",
    "volume": "main",
    "abstract": "In this work, we present a novel approach to multi-view action recognition where we guide learned action representations to be separated from view-relevant information in a video. When trying to classify action instances captured from multiple viewpoints, there is a higher degree of difficulty due to the difference in background, occlusion, and visibility of the captured action from different camera angles. To tackle the various problems introduced in multi-view action recognition, we propose a novel configuration of learnable transformer decoder queries, in conjunction with two supervised contrastive losses, to enforce the learning of action features that are robust to shifts in viewpoints. Our disentangled feature learning occurs in two stages: the transformer decoder uses separate queries to separately learn action and view information, which are then further disentangled using our two contrastive losses. We show that our model and method of training significantly outperforms all other uni-modal models on four multi-view action recognition datasets: NTU RGB+D, NTU RGB+D 120, PKU-MMD, and N-UCLA. Compared to previous RGB works, we see maximal improvements of 1.5%, 4.8%, 2.2%, and 4.8% on each dataset, respectively. Our code can be found here: https://github.com/NyleSiddiqui/MultiView_Actions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nyle Siddiqui",
      "Praveen Tirupattur",
      "Mubarak Shah"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28291": {
    "title": "Learning to Approximate Adaptive Kernel Convolution on Graphs",
    "volume": "main",
    "abstract": "Various Graph Neural Networks (GNN) have been successful in analyzing data in non-Euclidean spaces, however, they have limitations such as oversmoothing, i.e., information becomes excessively averaged as the number of hidden layers increases. The issue stems from the intrinsic formulation of conventional graph convolution where the nodal features are aggregated from a direct neighborhood per layer across the entire nodes in the graph. As setting different number of hidden layers per node is infeasible, recent works leverage a diffusion kernel to redefine the graph structure and incorporate information from farther nodes. Unfortunately, such approaches suffer from heavy diagonalization of a graph Laplacian or learning a large transform matrix. In this regards, we propose a diffusion learning framework where the range of feature aggregation is controlled by the scale of a diffusion kernel. For efficient computation, we derive closed-form derivatives of approximations of the graph convolution with respect to the scale, so that node-wise range can be adaptively learned.With a downstream classifier, the entire framework is made trainable in an end-to-end manner. Our model is tested on various standard datasets for node-wise classification for the state-of-the-art performance, and it is also validated on a real-world brain network data for graph classifications to demonstrate its practicality for Alzheimer classification",
    "checked": true,
    "id": "2df8d9807bc09bcad6625b9dcb7f43db30e7dd8e",
    "semantic_title": "learning to approximate adaptive kernel convolution on graphs",
    "citation_count": 0,
    "authors": [
      "Jaeyoon Sim",
      "Sooyeon Jeon",
      "InJun Choi",
      "Guorong Wu",
      "Won Hwa Kim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28292": {
    "title": "Semi-supervised Active Learning for Video Action Detection",
    "volume": "main",
    "abstract": "In this work, we focus on label efficient learning for video action detection. We develop a novel semi-supervised active learning approach which utilizes both labeled as well as un- labeled data along with informative sample selection for ac- tion detection. Video action detection requires spatio-temporal localization along with classification, which poses several challenges for both active learning (informative sample se- lection) as well as semi-supervised learning (pseudo label generation). First, we propose NoiseAug, a simple augmenta- tion strategy which effectively selects informative samples for video action detection. Next, we propose fft-attention, a novel technique based on high-pass filtering which enables effective utilization of pseudo label for SSL in video action detection by emphasizing on relevant activity region within a video. We evaluate the proposed approach on three different bench- mark datasets, UCF-101-24, JHMDB-21, and Youtube-VOS. First, we demonstrate its effectiveness on video action detec- tion where the proposed approach outperforms prior works in semi-supervised and weakly-supervised learning along with several baseline approaches in both UCF101-24 and JHMDB- 21. Next, we also show its effectiveness on Youtube-VOS for video object segmentation demonstrating its generalization capability for other dense prediction tasks in videos",
    "checked": true,
    "id": "c0523e8928af5d6e9bb1f1d0cc4e27088e9b1732",
    "semantic_title": "semi-supervised active learning for video action detection",
    "citation_count": 1,
    "authors": [
      "Ayush Singh",
      "Aayush J Rana",
      "Akash Kumar",
      "Shruti Vyas",
      "Yogesh Singh Rawat"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28293": {
    "title": "DeblurSR: Event-Based Motion Deblurring under the Spiking Representation",
    "volume": "main",
    "abstract": "We present DeblurSR, a novel motion deblurring approach that converts a blurry image into a sharp video. DeblurSR utilizes event data to compensate for motion ambiguities and exploits the spiking representation to parameterize the sharp output video as a mapping from time to intensity. Our key contribution, the Spiking Representation (SR), is inspired by the neuromorphic principles determining how biological neurons communicate with each other in living organisms. We discuss why the spikes can represent sharp edges and how the spiking parameters are interpreted from the neuromorphic perspective. DeblurSR has higher output quality and requires fewer computing resources than state-of-the-art event-based motion deblurring methods. We additionally show that our approach easily extends to video super-resolution when combined with recent advances in implicit neural representation",
    "checked": true,
    "id": "d0cf364708054edc121d35c16f8326294df893e5",
    "semantic_title": "deblursr: event-based motion deblurring under the spiking representation",
    "citation_count": 2,
    "authors": [
      "Chen Song",
      "Chandrajit Bajaj",
      "Qixing Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28294": {
    "title": "Multi-Cross Sampling and Frequency-Division Reconstruction for Image Compressed Sensing",
    "volume": "main",
    "abstract": "Deep Compressed Sensing (DCS) has attracted considerable interest due to its superior quality and speed compared to traditional CS algorithms. However, current approaches employ simplistic convolutional downsampling to acquire measurements, making it difficult to retain high-level features of the original signal for better image reconstruction. Furthermore, these approaches often overlook the presence of both high- and low-frequency information within the network, despite their critical role in achieving high-quality reconstruction. To address these challenges, we propose a novel Multi-Cross Sampling and Frequency Division Network (MCFD-Net) for image CS. The Dynamic Multi-Cross Sampling (DMCS) module, a sampling network of MCFD-Net, incorporates pyramid cross convolution and dual-branch sampling with multi-level pooling. Additionally, it introduces an attention mechanism between perception blocks to enhance adaptive learning effects. In the second deep reconstruction stage, we design a Frequency Division Reconstruction Module (FDRM). This module employs a discrete wavelet transform to extract high- and low-frequency information from images. It then applies multi-scale convolution and self-similarity attention compensation separately to both types of information before merging the output reconstruction results. The MCFD-Net integrates the DMCS and FDRM to construct an end-to-end learning network. Extensive CS experiments conducted on multiple benchmark datasets demonstrate that our MCFD-Net outperforms state-of-the-art approaches, while also exhibiting superior noise robustness",
    "checked": true,
    "id": "d33f72ce953b2a23f4da57617ab310dd03554841",
    "semantic_title": "multi-cross sampling and frequency-division reconstruction for image compressed sensing",
    "citation_count": 0,
    "authors": [
      "Heping Song",
      "Jingyao Gong",
      "Hongying Meng",
      "Yuping Lai"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28295": {
    "title": "Generalizable Fourier Augmentation for Unsupervised Video Object Segmentation",
    "volume": "main",
    "abstract": "The performance of existing unsupervised video object segmentation methods typically suffers from severe performance degradation on test videos when tested in out-of-distribution scenarios. The primary reason is that the test data in real- world may not follow the independent and identically distribution (i.i.d.) assumption, leading to domain shift. In this paper, we propose a generalizable fourier augmentation method during training to improve the generalization ability of the model. To achieve this, we perform Fast Fourier Transform (FFT) over the intermediate spatial domain features in each layer to yield corresponding frequency representations, including amplitude components (encoding scene-aware styles such as texture, color, contrast of the scene) and phase components (encoding rich semantics). We produce a variety of style features via Gaussian sampling to augment the training data, thereby improving the generalization capability of the model. To further improve the cross-domain generalization performance of the model, we design a phase feature update strategy via exponential moving average using phase features from past frames in an online update manner, which could help the model to learn cross-domain-invariant features. Extensive experiments show that our proposed method achieves the state-of-the-art performance on popular benchmarks",
    "checked": true,
    "id": "5fa2e285956f489164cf0798ee37fce86a083923",
    "semantic_title": "generalizable fourier augmentation for unsupervised video object segmentation",
    "citation_count": 1,
    "authors": [
      "Huihui Song",
      "Tiankang Su",
      "Yuhui Zheng",
      "Kaihua Zhang",
      "Bo Liu",
      "Dong Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28296": {
    "title": "Semantic-Aware Autoregressive Image Modeling for Visual Representation Learning",
    "volume": "main",
    "abstract": "The development of autoregressive modeling (AM) in computer vision lags behind natural language processing (NLP) in self-supervised pre-training. This is mainly caused by the challenge that images are not sequential signals and lack a natural order when applying autoregressive modeling. In this study, inspired by human beings' way of grasping an image, i.e., focusing on the main object first, we present a semantic-aware autoregressive image modeling (SemAIM) method to tackle this challenge. The key insight of SemAIM is to autoregressively model images from the semantic patches to the less semantic patches. To this end, we first calculate a semantic-aware permutation of patches according to their feature similarities and then perform the autoregression procedure based on the permutation. In addition, considering that the raw pixels of patches are low-level signals and are not ideal prediction targets for learning high-level semantic representation, we also explore utilizing the patch features as the prediction targets. Extensive experiments are conducted on a broad range of downstream tasks, including image classification, object detection, and instance/semantic segmentation, to evaluate the performance of SemAIM. The results demonstrate SemAIM achieves state-of-the-art performance compared with other self-supervised methods. Specifically, with ViT-B, SemAIM achieves 84.1% top-1 accuracy for fine-tuning on ImageNet, 51.3% AP and 45.4% AP for object detection and instance segmentation on COCO, which outperforms the vanilla MAE by 0.5%, 1.0%, and 0.5%, respectively. Code is available at https://github.com/skyoux/SemAIM",
    "checked": true,
    "id": "b98f6725341e75df30f9d61e284972c6b11a7e78",
    "semantic_title": "semantic-aware autoregressive image modeling for visual representation learning",
    "citation_count": 0,
    "authors": [
      "Kaiyou Song",
      "Shan Zhang",
      "Tong Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28297": {
    "title": "Self-Prompt Mechanism for Few-Shot Image Recognition",
    "volume": "main",
    "abstract": "Few-shot learning poses a formidable challenge as it necessitates effective recognition of novel classes based on a limited set of examples. Recent studies have sought to address the challenge of rare samples by tuning visual features through the utilization of external text prompts. However, the performance of these methods is constrained due to the inherent modality gap between the prompt text and image features. Instead of naively utilizing the external semantic information generated from text to guide the training of the image encoder, we propose a novel self-prompt mechanism (SPM) to adaptively adjust the neural network according to unseen data. Specifically, SPM involves a systematic selection of intrinsic semantic features generated by the image encoder across spatial and channel dimensions, thereby engendering self-prompt information. Subsequently, upon backpropagation of this self-prompt information to the deeper layers of the neural network, it effectively steers the network toward the learning and adaptation of new samples. Meanwhile, we propose a novel parameter-efficient tuning method that exclusively fine-tunes the parameters relevant to self-prompt (prompts are no more than 2% of the total parameters), and the incorporation of additional learnable parameters as self-prompt ensures the retention of prior knowledge through frozen encoder weights. Therefore, our method is highly suited for few-shot recognition tasks that require both information retention and adaptive adjustment of network parameters with limited labeling data constraints. Extensive experiments demonstrate the effectiveness of the proposed SPM in both 5-way 1-shot and 5-way 5-shot settings for standard single-domain and cross-domain few-shot recognition datasets, respectively. Our code is available at https://github.com/codeshop715/SPM",
    "checked": true,
    "id": "276276bc5e525a5854d9dfdd0b98bb41faaa2ace",
    "semantic_title": "self-prompt mechanism for few-shot image recognition",
    "citation_count": 0,
    "authors": [
      "Mingchen Song",
      "Huiqiang Wang",
      "Guoqiang Zhong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28298": {
    "title": "Diverse Person: Customize Your Own Dataset for Text-Based Person Search",
    "volume": "main",
    "abstract": "Text-based person search is a challenging task aimed at locating specific target pedestrians through text descriptions. Recent advancements have been made in this field, but there remains a deficiency in datasets tailored for text-based person search. The creation of new, real-world datasets is hindered by concerns such as the risk of pedestrian privacy leakage and the substantial costs of annotation. In this paper, we introduce a framework, named Diverse Person (DP), to achieve efficient and high-quality text-based person search data generation without involving privacy concerns. Specifically, we propose to leverage available images of clothing and accessories as reference attribute images to edit the original dataset images through diffusion models. Additionally, we employ a Large Language Model (LLM) to produce annotations that are both high in quality and stylistically consistent with those found in real-world datasets. Extensive experimental results demonstrate that the baseline models trained with our DP can achieve new state-of-the-art results on three public datasets, with performance improvements up to 4.82%, 2.15%, and 2.28% on CUHK-PEDES, ICFG-PEDES, and RSTPReid in terms of Rank-1 accuracy, respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zifan Song",
      "Guosheng Hu",
      "Cairong Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28299": {
    "title": "V2Meow: Meowing to the Visual Beat via Video-to-Music Generation",
    "volume": "main",
    "abstract": "Video-to-music generation demands both a temporally localized high-quality listening experience and globally aligned video-acoustic signatures. While recent music generation models excel at the former through advanced audio codecs, the exploration of video-acoustic signatures has been confined to specific visual scenarios. In contrast, our research confronts the challenge of learning globally aligned signatures between video and music directly from paired music and videos, without explicitly modeling domain-specific rhythmic or semantic relationships. We propose V2Meow, a video-to-music generation system capable of producing high-quality music audio for a diverse range of video input types using a multi-stage autoregressive model. Trained on 5k hours of music audio clips paired with video frames mined from in-the-wild music videos, V2Meow is competitive with previous domain-specific models when evaluated in a zero-shot manner. It synthesizes high-fidelity music audio waveforms solely by conditioning on pre-trained general-purpose visual features extracted from video frames, with optional style control via text prompts. Through both qualitative and quantitative evaluations, we demonstrate that our model outperforms various existing music generation systems in terms of visual-audio correspondence and audio quality. Music samples are available at tinyurl.com/v2meow",
    "checked": true,
    "id": "0dec46a8d2d120ffd008f2f40587d92267d42a40",
    "semantic_title": "v2meow: meowing to the visual beat via video-to-music generation",
    "citation_count": 1,
    "authors": [
      "Kun Su",
      "Judith Yue Li",
      "Qingqing Huang",
      "Dima Kuzmin",
      "Joonseok Lee",
      "Chris Donahue",
      "Fei Sha",
      "Aren Jansen",
      "Yu Wang",
      "Mauro Verzetti",
      "Timo Denk"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28300": {
    "title": "F³-Pruning: A Training-Free and Generalized Pruning Strategy towards Faster and Finer Text-to-Video Synthesis",
    "volume": "main",
    "abstract": "Recently Text-to-Video (T2V) synthesis has undergone a breakthrough by training transformers or diffusion models on large-scale datasets. Nevertheless, inferring such large models incurs huge costs. Previous inference acceleration works either require costly retraining or are model-specific. To address this issue, instead of retraining we explore the inference process of two mainstream T2V models using transformers and diffusion models. The exploration reveals the redundancy in temporal attention modules of both models, which are commonly utilized to establish temporal relations among frames. Consequently, we propose a training-free and generalized pruning strategy called F3-Pruning to prune redundant temporal attention weights. Specifically, when aggregate temporal attention values are ranked below a certain ratio, corresponding weights will be pruned. Extensive experiments on three datasets using a classic transformer-based model CogVideo and a typical diffusion-based model Tune-A-Video verify the effectiveness of F3-Pruning in inference acceleration, quality assurance and broad applicability",
    "checked": false,
    "id": "38e4bbd4d8bfaa6a8216f8fbe44edaa07e2eb997",
    "semantic_title": "f3-pruning: a training-free and generalized pruning strategy towards faster and finer text-to-video synthesis",
    "citation_count": 0,
    "authors": [
      "Sitong Su",
      "Jianzhi Liu",
      "Lianli Gao",
      "Jingkuan Song"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28301": {
    "title": "A Unified Environmental Network for Pedestrian Trajectory Prediction",
    "volume": "main",
    "abstract": "Accurately predicting pedestrian movements in complex environments is challenging due to social interactions, scene constraints, and pedestrians' multimodal behaviors. Sequential models like long short-term memory fail to effectively integrate scene features to make predicted trajectories comply with scene constraints due to disparate feature modalities of scene and trajectory. Though existing convolution neural network (CNN) models can extract scene features, they are ineffective in mapping these features into scene constraints for pedestrians and struggle to model pedestrian interactions due to the loss of target pedestrian information. To address these issues, we propose a unified environmental network based on CNN for pedestrian trajectory prediction. We introduce a polar-based method to reflect the distance and direction relationship between any position in the environment and the target pedestrian. This enables us to simultaneously model scene constraints and pedestrian social interactions in the form of feature maps. Additionally, we capture essential local features in the feature map, characterizing potential multimodal movements of pedestrians at each time step to prevent redundant predicted trajectories. We verify the performance of our proposed model on four trajectory prediction datasets, encompassing both short-term and long-term predictions. The experimental results demonstrate the superiority of our approach over existing methods",
    "checked": true,
    "id": "8782d41e4c07017917dcfdf602395bc81cced307",
    "semantic_title": "a unified environmental network for pedestrian trajectory prediction",
    "citation_count": 0,
    "authors": [
      "Yuchao Su",
      "Yuanman Li",
      "Wei Wang",
      "Jiantao Zhou",
      "Xia Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28302": {
    "title": "LRANet: Towards Accurate and Efficient Scene Text Detection with Low-Rank Approximation Network",
    "volume": "main",
    "abstract": "Recently, regression-based methods, which predict parameterized text shapes for text localization, have gained popularity in scene text detection. However, the existing parameterized text shape methods still have limitations in modeling arbitrary-shaped texts due to ignoring the utilization of text-specific shape information. Moreover, the time consumption of the entire pipeline has been largely overlooked, leading to a suboptimal overall inference speed. To address these issues, we first propose a novel parameterized text shape method based on low-rank approximation. Unlike other shape representation methods that employ data-irrelevant parameterization, our approach utilizes singular value decomposition and reconstructs the text shape using a few eigenvectors learned from labeled text contours. By exploring the shape correlation among different text contours, our method achieves consistency, compactness, simplicity, and robustness in shape representation. Next, we propose a dual assignment scheme for speed acceleration. It adopts a sparse assignment branch to accelerate the inference speed, and meanwhile, provides ample supervised signals for training through a dense assignment branch. Building upon these designs, we implement an accurate and efficient arbitrary-shaped text detector named LRANet. Extensive experiments are conducted on several challenging benchmarks, demonstrating the superior accuracy and efficiency of LRANet compared to state-of-the-art methods. Code is available at: https://github.com/ychensu/LRANet.git",
    "checked": true,
    "id": "10967969add42d50c3dcc91abc4ffcd0ee7b815a",
    "semantic_title": "lranet: towards accurate and efficient scene text detection with low-rank approximation network",
    "citation_count": 0,
    "authors": [
      "Yuchen Su",
      "Zhineng Chen",
      "Zhiwen Shao",
      "Yuning Du",
      "Zhilong Ji",
      "Jinfeng Bai",
      "Yong Zhou",
      "Yu-Gang Jiang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28303": {
    "title": "Spatial-Semantic Collaborative Cropping for User Generated Content",
    "volume": "main",
    "abstract": "A large amount of User Generated Content (UGC) is uploaded to the Internet daily and displayed to people world-widely through the client side (mobile and PC). This requires the cropping algorithms to produce the aesthetic thumbnail within a specific aspect ratio on different devices. However, existing image cropping works mainly focus on landmark or landscape images, which fail to model the relations among the multi-objects with the complex background in UGC. Besides, previous methods merely consider the aesthetics of the cropped images while ignoring the content integrity, which is crucial for UGC cropping. In this paper, we propose a Spatial-Semantic Collaborative cropping network (S2CNet) for arbitrary user generated content accompanied by a new cropping benchmark. Specifically, we first mine the visual genes of the potential objects. Then, the suggested adaptive attention graph recasts this task as a procedure of information association over visual nodes. The underlying spatial and semantic relations are ultimately centralized to the crop candidate through differentiable message passing, which helps our network efficiently to preserve both the aesthetics and the content integrity. Extensive experiments on the proposed UGCrop5K and other public datasets demonstrate the superiority of our approach over state-of-the-art counterparts",
    "checked": true,
    "id": "b736737bacbcce92b5ca9f29376fa5957abf3aa1",
    "semantic_title": "spatial-semantic collaborative cropping for user generated content",
    "citation_count": 0,
    "authors": [
      "Yukun Su",
      "Yiwen Cao",
      "Jingliang Deng",
      "Fengyun Rao",
      "Qingyao Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28304": {
    "title": "TR-DETR: Task-Reciprocal Transformer for Joint Moment Retrieval and Highlight Detection",
    "volume": "main",
    "abstract": "Video moment retrieval (MR) and highlight detection (HD) based on natural language queries are two highly related tasks, which aim to obtain relevant moments within videos and highlight scores of each video clip. Recently, several methods have been devoted to building DETR-based networks to solve both MR and HD jointly. These methods simply add two separate task heads after multi-modal feature extraction and feature interaction, achieving good performance. Nevertheless, these approaches underutilize the reciprocal relationship between two tasks. In this paper, we propose a task-reciprocal transformer based on DETR (TR-DETR) that focuses on exploring the inherent reciprocity between MR and HD. Specifically, a local-global multi-modal alignment module is first built to align features from diverse modalities into a shared latent space. Subsequently, a visual feature refinement is designed to eliminate query-irrelevant information from visual features for modal interaction. Finally, a task cooperation module is constructed to refine the retrieval pipeline and the highlight score prediction process by utilizing the reciprocity between MR and HD. Comprehensive experiments on QVHighlights, Charades-STA and TVSum datasets demonstrate that TR-DETR outperforms existing state-of-the-art methods. Codes are available at https://github.com/mingyao1120/TR-DETR",
    "checked": true,
    "id": "04cb3420254844735ce65fe245deaf035d8e9fca",
    "semantic_title": "tr-detr: task-reciprocal transformer for joint moment retrieval and highlight detection",
    "citation_count": 3,
    "authors": [
      "Hao Sun",
      "Mingyao Zhou",
      "Wenjing Chen",
      "Wei Xie"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28305": {
    "title": "UniAP: Towards Universal Animal Perception in Vision via Few-Shot Learning",
    "volume": "main",
    "abstract": "Animal visual perception is an important technique for automatically monitoring animal health, understanding animal behaviors, and assisting animal-related research. However, it is challenging to design a deep learning-based perception model that can freely adapt to different animals across various perception tasks, due to the varying poses of a large diversity of animals, lacking data on rare species, and the semantic inconsistency of different tasks. We introduce UniAP, a novel Universal Animal Perception model that leverages few-shot learning to enable cross-species perception among various visual tasks. Our proposed model takes support images and labels as prompt guidance for a query image. Images and labels are processed through a Transformer-based encoder and a lightweight label encoder, respectively. Then a matching module is designed for aggregating information between prompt guidance and the query image, followed by a multi-head label decoder to generate outputs for various tasks. By capitalizing on the shared visual characteristics among different animals and tasks, UniAP enables the transfer of knowledge from well-studied species to those with limited labeled data or even unseen species. We demonstrate the effectiveness of UniAP through comprehensive experiments in pose estimation, segmentation, and classification tasks on diverse animal species, showcasing its ability to generalize and adapt to new classes with minimal labeled examples",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meiqi Sun",
      "Zhonghan Zhao",
      "Wenhao Chai",
      "Hanjun Luo",
      "Shidong Cao",
      "Yanting Zhang",
      "Jenq-Neng Hwang",
      "Gaoang Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28306": {
    "title": "CFR-ICL: Cascade-Forward Refinement with Iterative Click Loss for Interactive Image Segmentation",
    "volume": "main",
    "abstract": "The click-based interactive segmentation aims to extract the object of interest from an image with the guidance of user clicks. Recent work has achieved great overall performance by employing feedback from the output. However, in most state-of-the-art approaches, 1) the inference stage involves inflexible heuristic rules and requires a separate refinement model, and 2) the number of user clicks and model performance cannot be balanced. To address the challenges, we propose a click-based and mask-guided interactive image segmentation framework containing three novel components: Cascade-Forward Refinement (CFR), Iterative Click Loss (ICL), and SUEM image augmentation. The CFR offers a unified inference framework to generate segmentation results in a coarse-to-fine manner. The proposed ICL allows model training to improve segmentation and reduce user interactions simultaneously. The proposed SUEM augmentation is a comprehensive way to create large and diverse training sets for interactive image segmentation. Extensive experiments demonstrate the state-of-the-art performance of the proposed approach on five public datasets. Remarkably, our model reduces by 33.2%, and 15.5% the number of clicks required to surpass an IoU of 0.95 in the previous state-of-the-art approach on the Berkeley and DAVIS sets, respectively",
    "checked": true,
    "id": "ab13e7ee06b977f41f9709ee955435fb6dfd043e",
    "semantic_title": "cfr-icl: cascade-forward refinement with iterative click loss for interactive image segmentation",
    "citation_count": 8,
    "authors": [
      "Shoukun Sun",
      "Min Xian",
      "Fei Xu",
      "Luca Capriotti",
      "Tiankai Yao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28307": {
    "title": "RL-SeqISP: Reinforcement Learning-Based Sequential Optimization for Image Signal Processing",
    "volume": "main",
    "abstract": "Hardware image signal processing (ISP), aiming at converting RAW inputs to RGB images, consists of a series of processing blocks, each with multiple parameters. Traditionally, ISP parameters are manually tuned in isolation by imaging experts according to application-specific quality and performance metrics, which is time-consuming and biased towards human perception due to complex interaction with the output image. Since the relationship between any single parameter's variation and the output performance metric is a complex, non-linear function, optimizing such a large number of ISP parameters is challenging. To address this challenge, we propose a novel Sequential ISP parameter optimization model, called the RL-SeqISP model, which utilizes deep reinforcement learning to jointly optimize all ISP parameters for a variety of imaging applications. Concretely, inspired by the sequential tuning process of human experts, the proposed model can progressively enhance image quality by seamlessly integrating information from both the image feature space and the parameter space. Furthermore, a dynamic parameter optimization module is introduced to avoid ISP parameters getting stuck into local optima, which is able to more effectively guarantee the optimal parameters resulting from the sequential learning strategy. These merits of the RL-SeqISP model as well as its high efficiency are substantiated by comprehensive experiments on a wide range of downstream tasks, including two visual analysis tasks (instance segmentation and object detection), and image quality assessment (IQA), as compared with representative methods both quantitatively and qualitatively. In particular, even using only 10% of the training data, our model outperforms other SOTA methods by an average of 7% mAP on two visual analysis tasks",
    "checked": true,
    "id": "0603e458e115ae89a549929945fcc15352605fe5",
    "semantic_title": "rl-seqisp: reinforcement learning-based sequential optimization for image signal processing",
    "citation_count": 0,
    "authors": [
      "Xinyu Sun",
      "Zhikun Zhao",
      "Lili Wei",
      "Congyan Lang",
      "Mingxuan Cai",
      "Longfei Han",
      "Juan Wang",
      "Bing Li",
      "Yuxuan Guo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28308": {
    "title": "PathAsst: A Generative Foundation AI Assistant towards Artificial General Intelligence of Pathology",
    "volume": "main",
    "abstract": "As advances in large language models (LLMs) and multimodal techniques continue to mature, the development of general-purpose multimodal large language models (MLLMs) has surged, offering significant applications in interpreting natural images. However, the field of pathology has largely remained untapped, particularly in gathering high-quality data and designing comprehensive model frameworks. To bridge the gap in pathology MLLMs, we present PathAsst, a multimodal generative foundation AI assistant to revolutionize diagnostic and predictive analytics in pathology. The development of PathAsst involves three pivotal steps: data acquisition, CLIP model adaptation, and the training of PathAsst's multimodal generative capabilities. Firstly, we collect over 207K high-quality pathology image-text pairs from authoritative sources. Leveraging the advanced power of ChatGPT, we generate over 180K instruction-following samples. Furthermore, we devise additional instruction-following data specifically tailored for invoking eight pathology-specific sub-models we prepared, allowing the PathAsst to effectively collaborate with these models, enhancing its diagnostic ability. Secondly, by leveraging the collected data, we construct PathCLIP, a pathology-dedicated CLIP, to enhance PathAsst's capabilities in interpreting pathology images. Finally, we integrate PathCLIP with the Vicuna-13b and utilize pathology-specific instruction-tuning data to enhance the multimodal generation capacity of PathAsst and bolster its synergistic interactions with sub-models. The experimental results of PathAsst show the potential of harnessing AI-powered generative foundation model to improve pathology diagnosis and treatment processes. We open-source our dataset, as well as a comprehensive toolkit for extensive pathology data collection and preprocessing at https://github.com/superjamessyx/Generative-Foundation-AI-Assistant-for-Pathology",
    "checked": true,
    "id": "3c50ef336232da0885ef61da386c98eac964b7cd",
    "semantic_title": "pathasst: a generative foundation ai assistant towards artificial general intelligence of pathology",
    "citation_count": 14,
    "authors": [
      "Yuxuan Sun",
      "Chenglu Zhu",
      "Sunyi Zheng",
      "Kai Zhang",
      "Lin Sun",
      "Zhongyi Shui",
      "Yunlong Zhang",
      "Honglin Li",
      "Lin Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28309": {
    "title": "FG-EmoTalk: Talking Head Video Generation with Fine-Grained Controllable Facial Expressions",
    "volume": "main",
    "abstract": "Although deep generative models have greatly improved one-shot video-driven talking head generation, few studies address fine-grained controllable facial expression editing, which is crucial for practical applications. Existing methods rely on a fixed set of predefined discrete emotion labels or simply copy expressions from input videos. This is limiting as expressions are complex, and methods using only emotion labels cannot generate fine-grained, accurate or mixed expressions. Generating talking head video with precise expressions is also difficult using 3D model-based approaches, as 3DMM only models facial movements and tends to produce deviations. In this paper, we propose a novel framework enabling fine-grained facial expression editing in talking face generation. Our goal is to achieve expression control by manipulating the intensities of individual facial Action Units (AUs) or groups. First, compared with existing methods which decouple the face into pose and expression, we propose a disentanglement scheme to isolates three components from the human face, namely, appearance, pose, and expression. Second, we propose to use input AUs to control muscle group intensities in the generated face, and integrate the AUs features with the disentangled expression latent code. Finally, we present a self-supervised training strategy with well-designed constraints. Experiments show our method achieves fine-grained expression control, produces high-quality talking head videos and outperforms baseline methods",
    "checked": true,
    "id": "78396d47cff7a5d2e1330d9f75a2619bb7e1b713",
    "semantic_title": "fg-emotalk: talking head video generation with fine-grained controllable facial expressions",
    "citation_count": 2,
    "authors": [
      "Zhaoxu Sun",
      "Yuze Xuan",
      "Fang Liu",
      "Yang Xiang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28310": {
    "title": "Frequency-Aware Deepfake Detection: Improving Generalizability through Frequency Space Domain Learning",
    "volume": "main",
    "abstract": "This research addresses the challenge of developing a universal deepfake detector that can effectively identify unseen deepfake images despite limited training data. Existing frequency-based paradigms have relied on frequency-level artifacts introduced during the up-sampling in GAN pipelines to detect forgeries. However, the rapid advancements in synthesis technology have led to specific artifacts for each generation model. Consequently, these detectors have exhibited a lack of proficiency in learning the frequency domain and tend to overfit to the artifacts present in the training data, leading to suboptimal performance on unseen sources. To address this issue, we introduce a novel frequency-aware approach called FreqNet, centered around frequency domain learning, specifically designed to enhance the generalizability of deepfake detectors. Our method forces the detector to continuously focus on high-frequency information, exploiting high-frequency representation of features across spatial and channel dimensions. Additionally, we incorporate a straightforward frequency domain learning module to learn source-agnostic features. It involves convolutional layers applied to both the phase spectrum and amplitude spectrum between the Fast Fourier Transform (FFT) and Inverse Fast Fourier Transform (iFFT). Extensive experimentation involving 17 GANs demonstrates the effectiveness of our proposed method, showcasing state-of-the-art performance (+9.8\\%) while requiring fewer parameters. The code is available at https://github.com/chuangchuangtan/FreqNet-DeepfakeDetection",
    "checked": true,
    "id": "a19241eaaa6c35f8763b4c3bf5c754b9fdb09331",
    "semantic_title": "frequency-aware deepfake detection: improving generalizability through frequency space domain learning",
    "citation_count": 2,
    "authors": [
      "Chuangchuang Tan",
      "Yao Zhao",
      "Shikui Wei",
      "Guanghua Gu",
      "Ping Liu",
      "Yunchao Wei"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28311": {
    "title": "Compound Text-Guided Prompt Tuning via Image-Adaptive Cues",
    "volume": "main",
    "abstract": "Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable generalization capabilities to downstream tasks. However, existing prompt tuning based frameworks need to parallelize learnable textual inputs for all categories, suffering from massive GPU memory consumption when there is a large number of categories in the target dataset. Moreover, previous works require to include category names within prompts, exhibiting subpar performance when dealing with ambiguous category names. To address these shortcomings, we propose Compound Text-Guided Prompt Tuning (TGP-T) that significantly reduces resource demand while achieving superior performance. We introduce text supervision to the optimization of prompts, which enables two benefits: 1) releasing the model reliance on the pre-defined category names during inference, thereby enabling more flexible prompt generation; 2) reducing the number of inputs to the text encoder, which decreases GPU memory consumption significantly. Specifically, we found that compound text supervisions, i.e., category-wise and content-wise, is highly effective, since they provide inter-class separability and capture intra-class variations, respectively. Moreover, we condition the prompt generation on visual features through a module called Bonder, which facilitates the alignment between prompts and visual features. Extensive experiments on few-shot recognition and domain generalization demonstrate that TGP-T achieves superior performance with consistently lower training costs. It reduces GPU memory usage by 93% and attains a 2.5% performance gain on 16-shot ImageNet. The code is available at https://github.com/EricTan7/TGP-T",
    "checked": true,
    "id": "a05b33072a8330413947eab0833ed2fd21de4963",
    "semantic_title": "compound text-guided prompt tuning via image-adaptive cues",
    "citation_count": 1,
    "authors": [
      "Hao Tan",
      "Jun Li",
      "Yizhuang Zhou",
      "Jun Wan",
      "Zhen Lei",
      "Xiangyu Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28312": {
    "title": "Occluded Person Re-identification via Saliency-Guided Patch Transfer",
    "volume": "main",
    "abstract": "While generic person re-identification has made remarkable improvement in recent years, these methods are designed under the assumption that the entire body of the person is available. This assumption brings about a significant performance degradation when suffering from occlusion caused by various obstacles in real-world applications. To address this issue, data-driven strategies have emerged to enhance the model's robustness to occlusion. Following the random erasing paradigm, these strategies typically employ randomly generated noise to supersede randomly selected image regions to simulate obstacles. However, the random strategy is not sensitive to location and content, meaning they cannot mimic real-world occlusion cases in application scenarios. To overcome this limitation and fully exploit the real scene information in datasets, this paper proposes a more intuitive and effective data-driven strategy named Saliency-Guided Patch Transfer (SPT). Combined with the vision transformer, SPT divides person instances and background obstacles using salient patch selection. By transferring person instances to different background obstacles, SPT can easily generate photo-realistic occluded samples. Furthermore, we propose an occlusion-aware Intersection over Union (OIoU) with mask-rolling to filter the more suitable combination and a class-ignoring strategy to achieve more stable processing. Extensive experimental evaluations conducted on occluded and holistic person re-identification benchmarks demonstrate that SPT provides a significant performance gain among different ViT-based ReID algorithms on occluded ReID",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Tan",
      "Jiaer Xia",
      "Wenfeng Liu",
      "Pingyang Dai",
      "Yongjian Wu",
      "Liujuan Cao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28313": {
    "title": "Style2Talker: High-Resolution Talking Head Generation with Emotion Style and Art Style",
    "volume": "main",
    "abstract": "Although automatically animating audio-driven talking heads has recently received growing interest, previous efforts have mainly concentrated on achieving lip synchronization with the audio, neglecting two crucial elements for generating expressive videos: emotion style and art style. In this paper, we present an innovative audio-driven talking face generation method called Style2Talker. It involves two stylized stages, namely Style-E and Style-A, which integrate text-controlled emotion style and picture-controlled art style into the final output. In order to prepare the scarce emotional text descriptions corresponding to the videos, we propose a labor-free paradigm that employs large-scale pretrained models to automatically annotate emotional text labels for existing audio-visual datasets. Incorporating the synthetic emotion texts, the Style-E stage utilizes a large-scale CLIP model to extract emotion representations, which are combined with the audio, serving as the condition for an efficient latent diffusion model designed to produce emotional motion coefficients of a 3DMM model. Moving on to the Style-A stage, we develop a coefficient-driven motion generator and an art-specific style path embedded in the well-known StyleGAN. This allows us to synthesize high-resolution artistically stylized talking head videos using the generated emotional motion coefficients and an art style source picture. Moreover, to better preserve image details and avoid artifacts, we provide StyleGAN with the multi-scale content features extracted from the identity image and refine its intermediate feature maps by the designed content encoder and refinement network, respectively. Extensive experimental results demonstrate our method outperforms existing state-of-the-art methods in terms of audio-lip synchronization and performance of both emotion style and art style",
    "checked": true,
    "id": "5e95dfb1e2c929b8b9608cb04f9e9bb0d7179c74",
    "semantic_title": "style2talker: high-resolution talking head generation with emotion style and art style",
    "citation_count": 5,
    "authors": [
      "Shuai Tan",
      "Bin Ji",
      "Ye Pan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28314": {
    "title": "Say Anything with Any Style",
    "volume": "main",
    "abstract": "Generating stylized talking head with diverse head motions is crucial for achieving natural-looking videos but still remains challenging. Previous works either adopt a regressive method to capture the speaking style, resulting in a coarse style that is averaged across all training data, or employ a universal network to synthesize videos with different styles which causes suboptimal performance. To address these, we propose a novel dynamic-weight method, namely Say Anything with Any Style (SAAS), which queries the discrete style representation via a generative model with a learned style codebook. Specifically, we develop a multi-task VQ-VAE that incorporates three closely related tasks to learn a style codebook as a prior for style extraction. This discrete prior, along with the generative model, enhances the precision and robustness when extracting the speaking styles of the given style clips. By utilizing the extracted style, a residual architecture comprising a canonical branch and style-specific branch is employed to predict the mouth shapes conditioned on any driving audio while transferring the speaking style from the source to any desired one. To adapt to different speaking styles, we steer clear of employing a universal network by exploring an elaborate HyperStyle to produce the style-specific weights offset for the style branch. Furthermore, we construct a pose generator and a pose codebook to store the quantized pose representation, allowing us to sample diverse head motions aligned with the audio and the extracted style. Experiments demonstrate that our approach surpasses state-of-the-art methods in terms of both lip-synchronization and stylized expression. Besides, we extend our SAAS to video-driven style editing field and achieve satisfactory performance as well",
    "checked": true,
    "id": "994aa25f02ad040ab1db47ea1a9c3ec8dd36575b",
    "semantic_title": "say anything with any style",
    "citation_count": 4,
    "authors": [
      "Shuai Tan",
      "Bin Ji",
      "Yu Ding",
      "Ye Pan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28315": {
    "title": "Semantic-Aware Data Augmentation for Text-to-Image Synthesis",
    "volume": "main",
    "abstract": "Data augmentation has been recently leveraged as an effective regularizer in various vision-language deep neural networks. However, in text-to-image synthesis (T2Isyn), current augmentation wisdom still suffers from the semantic mismatch between augmented paired data. Even worse, semantic collapse may occur when generated images are less semantically constrained. In this paper, we develop a novel Semantic-aware Data Augmentation (SADA) framework dedicated to T2Isyn. In particular, we propose to augment texts in the semantic space via an Implicit Textual Semantic Preserving Augmentation, in conjunction with a specifically designed Image Semantic Regularization Loss as Generated Image Semantic Conservation, to cope well with semantic mismatch and collapse. As one major contribution, we theoretically show that Implicit Textual Semantic Preserving Augmentation can certify better text-image consistency while Image Semantic Regularization Loss regularizing the semantics of generated images would avoid semantic collapse and enhance image quality. Extensive experiments validate that SADA enhances text-image consistency and improves image quality significantly in T2Isyn models across various backbones. Especially, incorporating SADA during the tuning process of Stable Diffusion models also yields performance improvements",
    "checked": true,
    "id": "d766020b683b3c1567be4ea28d74a71d150ded47",
    "semantic_title": "semantic-aware data augmentation for text-to-image synthesis",
    "citation_count": 0,
    "authors": [
      "Zhaorui Tan",
      "Xi Yang",
      "Kaizhu Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28316": {
    "title": "Data-Free Generalized Zero-Shot Learning",
    "volume": "main",
    "abstract": "Deep learning models have the ability to extract rich knowledge from large-scale datasets. However, the sharing of data has become increasingly challenging due to concerns regarding data copyright and privacy. Consequently, this hampers the effective transfer of knowledge from existing data to novel downstream tasks and concepts. Zero-shot learning (ZSL) approaches aim to recognize new classes by transferring semantic knowledge learned from base classes. However, traditional generative ZSL methods often require access to real images from base classes and rely on manually annotated attributes, which presents challenges in terms of data restrictions and model scalability. To this end, this paper tackles a challenging and practical problem dubbed as data-free zero-shot learning (DFZSL), where only the CLIP-based base classes data pre-trained classifier is available for zero-shot classification. Specifically, we propose a generic framework for DFZSL, which consists of three main components. Firstly, to recover the virtual features of the base data, we model the CLIP features of base class images as samples from a von Mises-Fisher (vMF) distribution based on the pre-trained classifier. Secondly, we leverage the text features of CLIP as low-cost semantic information and propose a feature-language prompt tuning (FLPT) method to further align the virtual image features and textual features. Thirdly, we train a conditional generative model using the well-aligned virtual image features and corresponding semantic text features, enabling the generation of new classes features and achieve better zero-shot generalization. Our framework has been evaluated on five commonly used benchmarks for generalized ZSL, as well as 11 benchmarks for the base-to-new ZSL. The results demonstrate the superiority and effectiveness of our approach. Our code is available in https://github.com/ylong4/DFZSL",
    "checked": true,
    "id": "bfe41c89aa91358126823d22cca98f111ba42fec",
    "semantic_title": "data-free generalized zero-shot learning",
    "citation_count": 1,
    "authors": [
      "Bowen Tang",
      "Jing Zhang",
      "Long Yan",
      "Qian Yu",
      "Lu Sheng",
      "Dong Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28317": {
    "title": "Offline and Online Optical Flow Enhancement for Deep Video Compression",
    "volume": "main",
    "abstract": "Video compression relies heavily on exploiting the temporal redundancy between video frames, which is usually achieved by estimating and using the motion information. The motion information is represented as optical flows in most of the existing deep video compression networks. Indeed, these networks often adopt pre-trained optical flow estimation networks for motion estimation. The optical flows, however, may be less suitable for video compression due to the following two factors. First, the optical flow estimation networks were trained to perform inter-frame prediction as accurately as possible, but the optical flows themselves may cost too many bits to encode. Second, the optical flow estimation networks were trained on synthetic data, and may not generalize well enough to real-world videos. We address the twofold limitations by enhancing the optical flows in two stages: offline and online. In the offline stage, we fine-tune a trained optical flow estimation network with the motion information provided by a traditional (non-deep) video compression scheme, e.g. H.266/VVC, as we believe the motion information of H.266/VVC achieves a better rate-distortion trade-off. In the online stage, we further optimize the latent features of the optical flows with a gradient descent-based algorithm for the video to be compressed, so as to enhance the adaptivity of the optical flows. We conduct experiments on two state-of-the-art deep video compression schemes, DCVC and DCVC-DC. Experimental results demonstrate that the proposed offline and online enhancement together achieves on average 13.4% bitrate saving for DCVC and 4.1% bitrate saving for DCVC-DC on the tested videos, without increasing the model or computational complexity of the decoder side",
    "checked": true,
    "id": "939886a6127213a02b13abd9eb741e0a0d307faa",
    "semantic_title": "offline and online optical flow enhancement for deep video compression",
    "citation_count": 2,
    "authors": [
      "Chuanbo Tang",
      "Xihua Sheng",
      "Zhuoyuan Li",
      "Haotian Zhang",
      "Li Li",
      "Dong Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28318": {
    "title": "Manifold Constraints for Imperceptible Adversarial Attacks on Point Clouds",
    "volume": "main",
    "abstract": "Adversarial attacks on 3D point clouds often exhibit unsatisfactory imperceptibility, which primarily stems from the disregard for manifold-aware distortion, i.e., distortion of the underlying 2-manifold surfaces. In this paper, we develop novel manifold constraints to reduce such distortion, aiming to enhance the imperceptibility of adversarial attacks on 3D point clouds. Specifically, we construct a bijective manifold mapping between point clouds and a simple parameter shape using an invertible auto-encoder. Consequently, manifold-aware distortion during attacks can be captured within the parameter space. By enforcing manifold constraints that preserve local properties of the parameter shape, manifold-aware distortion is effectively mitigated, ultimately leading to enhanced imperceptibility. Extensive experiments demonstrate that integrating manifold constraints into conventional adversarial attack solutions yields superior imperceptibility, outperforming the state-of-the-art methods",
    "checked": true,
    "id": "2ec718c9784cb6dd825aa57f50c62b4d1793fd3c",
    "semantic_title": "manifold constraints for imperceptible adversarial attacks on point clouds",
    "citation_count": 0,
    "authors": [
      "Keke Tang",
      "Xu He",
      "Weilong Peng",
      "Jianpeng Wu",
      "Yawen Shi",
      "Daizong Liu",
      "Pan Zhou",
      "Wenping Wang",
      "Zhihong Tian"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28319": {
    "title": "Once and for All: Universal Transferable Adversarial Perturbation against Deep Hashing-Based Facial Image Retrieval",
    "volume": "main",
    "abstract": "Deep Hashing (DH)-based image retrieval has been widely applied to face-matching systems due to its accuracy and efficiency. However, this convenience comes with an increased risk of privacy leakage. DH models inherit the vulnerability to adversarial attacks, which can be used to prevent the retrieval of private images. Existing adversarial attacks against DH typically target a single image or a specific class of images, lacking universal adversarial perturbation for the entire hash dataset. In this paper, we propose the first universal transferable adversarial perturbation against DH-based facial image retrieval, a single perturbation can protect all images. Specifically, we explore the relationship between clusters learned by different DH models and define the optimization objective of universal perturbation as leaving from the overall hash center. To mitigate the challenge of single-objective optimization, we randomly obtain sub-cluster centers and further propose sub-task-based meta-learning to aid in overall optimization. We test our method with popular facial datasets and DH models, indicating impressive cross-image, -identity, -model, and -scheme universal anti-retrieval performance. Compared to state-of-the-art methods, our performance is competitive in white-box settings and exhibits significant improvements of 10%-70% in transferability in all black-box settings",
    "checked": true,
    "id": "ecc659fabd80db05ad2bc627f6cb13367f3113e6",
    "semantic_title": "once and for all: universal transferable adversarial perturbation against deep hashing-based facial image retrieval",
    "citation_count": 0,
    "authors": [
      "Long Tang",
      "Dengpan Ye",
      "Yunna Lv",
      "Chuanxi Chen",
      "Yunming Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28320": {
    "title": "Prior and Prediction Inverse Kernel Transformer for Single Image Defocus Deblurring",
    "volume": "main",
    "abstract": "Defocus blur, due to spatially-varying sizes and shapes, is hard to remove. Existing methods either are unable to effectively handle irregular defocus blur or fail to generalize well on other datasets. In this work, we propose a divide-and-conquer approach to tackling this issue, which gives rise to a novel end-to-end deep learning method, called prior-and-prediction inverse kernel transformer (P2IKT), for single image defocus deblurring. Since most defocus blur can be approximated as Gaussian blur or its variants, we construct an inverse Gaussian kernel module in our method to enhance its generalization ability. At the same time, an inverse kernel prediction module is introduced in order to flexibly address the irregular blur that cannot be approximated by Gaussian blur. We further design a scale recurrent transformer, which estimates mixing coefficients for adaptively combining the results from the two modules and runs the scale recurrent ``coarse-to-fine\" procedure for progressive defocus deblurring. Extensive experimental results demonstrate that our P2IKT outperforms previous methods in terms of PSNR on multiple defocus deblurring datasets",
    "checked": true,
    "id": "62567e11cd48b1ffc1ba43ffa8380782d00e5db2",
    "semantic_title": "prior and prediction inverse kernel transformer for single image defocus deblurring",
    "citation_count": 0,
    "authors": [
      "Peng Tang",
      "Zhiqiang Xu",
      "Chunlai Zhou",
      "Pengfei Wei",
      "Peng Han",
      "Xin Cao",
      "Tobias Lasser"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28321": {
    "title": "Semantic Lens: Instance-Centric Semantic Alignment for Video Super-resolution",
    "volume": "main",
    "abstract": "As a critical clue of video super-resolution (VSR), inter-frame alignment significantly impacts overall performance. However, accurate pixel-level alignment is a challenging task due to the intricate motion interweaving in the video. In response to this issue, we introduce a novel paradigm for VSR named Semantic Lens, predicated on semantic priors drawn from degraded videos. Specifically, video is modeled as instances, events, and scenes via a Semantic Extractor. Those semantics assist the Pixel Enhancer in understanding the recovered contents and generating more realistic visual results. The distilled global semantics embody the scene information of each frame, while the instance-specific semantics assemble the spatial-temporal contexts related to each instance. Furthermore, we devise a Semantics-Powered Attention Cross-Embedding (SPACE) block to bridge the pixel-level features with semantic knowledge, composed of a Global Perspective Shifter (GPS) and an Instance-Specific Semantic Embedding Encoder (ISEE). Concretely, the GPS module generates pairs of affine transformation parameters for pixel-level feature modulation conditioned on global semantics. After that the ISEE module harnesses the attention mechanism to align the adjacent frames in the instance-centric semantic space. In addition, we incorporate a simple yet effective pre-alignment module to alleviate the difficulty of model training. Extensive experiments demonstrate the superiority of our model over existing state-of-the-art VSR methods",
    "checked": true,
    "id": "e062f75dbfbb38a378b9c208b0a7562f0938a04d",
    "semantic_title": "semantic lens: instance-centric semantic alignment for video super-resolution",
    "citation_count": 0,
    "authors": [
      "Qi Tang",
      "Yao Zhao",
      "Meiqin Liu",
      "Jian Jin",
      "Chao Yao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28322": {
    "title": "Boosting Residual Networks with Group Knowledge",
    "volume": "main",
    "abstract": "Recent research understands the residual networks from a new perspective of the implicit ensemble model. From this view, previous methods such as stochastic depth and stimulative training have further improved the performance of the residual network by sampling and training of its subnets. However, they both use the same supervision for all subnets of different capacities and neglect the valuable knowledge generated by subnets during training. In this manuscript, we mitigate the significant knowledge distillation gap caused by using the same kind of supervision and advocate leveraging the subnets to provide diverse knowledge. Based on this motivation, we propose a group knowledge based training framework for boosting the performance of residual networks. Specifically, we implicitly divide all subnets into hierarchical groups by subnet-in-subnet sampling, aggregate the knowledge of different subnets in each group during training, and exploit upper-level group knowledge to supervise lower-level subnet group. Meanwhile, we also develop a subnet sampling strategy that naturally samples larger subnets, which are found to be more helpful than smaller subnets in boosting performance for hierarchical groups. Compared with typical subnet training and other methods, our method achieves the best efficiency and performance trade-offs on multiple datasets and network structures. The code is at https://github.com/tsj-001/AAAI24-GKT",
    "checked": true,
    "id": "496e58f431b638573c55d39bb8f5801e252cffbf",
    "semantic_title": "boosting residual networks with group knowledge",
    "citation_count": 2,
    "authors": [
      "Shengji Tang",
      "Peng Ye",
      "Baopu Li",
      "Weihao Lin",
      "Tao Chen",
      "Tong He",
      "Chong Yu",
      "Wanli Ouyang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28323": {
    "title": "Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models",
    "volume": "main",
    "abstract": "The popularity of pre-trained large models has revolutionized downstream tasks across diverse fields, such as language, vision, and multi-modality. To minimize the adaption cost for downstream tasks, many Parameter-Efficient Fine-Tuning (PEFT) techniques are proposed for language and 2D image pre-trained models. However, the specialized PEFT method for 3D pre-trained models is still under-explored. To this end, we introduce Point-PEFT, a novel framework for adapting point cloud pre-trained models with minimal learnable parameters. Specifically, for a pre-trained 3D model, we freeze most of its parameters, and only tune the newly added PEFT modules on downstream tasks, which consist of a Point-prior Prompt and a Geometry-aware Adapter. The Point-prior Prompt adopts a set of learnable prompt tokens, for which we propose to construct a memory bank with domain-specific knowledge, and utilize a parameter-free attention to enhance the prompt tokens. The Geometry-aware Adapter aims to aggregate point cloud features within spatial neighborhoods to capture fine-grained geometric information through local interactions. Extensive experiments indicate that our Point-PEFT can achieve better performance than the full fine-tuning on various downstream tasks, while using only 5% of the trainable parameters, demonstrating the efficiency and effectiveness of our approach. Code is released at https://github.com/Ivan-Tang-3D/Point-PEFT",
    "checked": true,
    "id": "65be1daf946237606c6583483780cea13d4cb852",
    "semantic_title": "point-peft: parameter-efficient fine-tuning for 3d pre-trained models",
    "citation_count": 5,
    "authors": [
      "Yiwen Tang",
      "Ray Zhang",
      "Zoey Guo",
      "Xianzheng Ma",
      "Bin Zhao",
      "Zhigang Wang",
      "Dong Wang",
      "Xuelong Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28324": {
    "title": "Context-I2W: Mapping Images to Context-Dependent Words for Accurate Zero-Shot Composed Image Retrieval",
    "volume": "main",
    "abstract": "Different from the Composed Image Retrieval task that requires expensive labels for training task-specific models, Zero-Shot Composed Image Retrieval (ZS-CIR) involves diverse tasks with a broad range of visual content manipulation intent that could be related to domain, scene, object, and attribute. The key challenge for ZS-CIR tasks is to learn a more accurate image representation that has adaptive attention to the reference image for various manipulation descriptions. In this paper, we propose a novel context-dependent mapping network, named Context-I2W, for adaptively converting description-relevant Image information into a pseudo-word token composed of the description for accurate ZS-CIR. Specifically, an Intent View Selector first dynamically learns a rotation rule to map the identical image to a task-specific manipulation view. Then a Visual Target Extractor further captures local information covering the main targets in ZS-CIR tasks under the guidance of multiple learnable queries. The two complementary modules work together to map an image to a context-dependent pseudo-word token without extra supervision. Our model shows strong generalization ability on four ZS-CIR tasks, including domain conversion, object composition, object manipulation, and attribute manipulation. It obtains consistent and significant performance boosts ranging from 1.88% to 3.60% over the best methods and achieves new state-of-the-art results on ZS-CIR. Our code is available at https://anonymous.4open.science/r/Context-I2W-4224/",
    "checked": true,
    "id": "869acd5d303474fdd1dbf98c6aa6c8f8244a66b4",
    "semantic_title": "context-i2w: mapping images to context-dependent words for accurate zero-shot composed image retrieval",
    "citation_count": 6,
    "authors": [
      "Yuanmin Tang",
      "Jing Yu",
      "Keke Gai",
      "Jiamin Zhuang",
      "Gang Xiong",
      "Yue Hu",
      "Qi Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28325": {
    "title": "Generative-Based Fusion Mechanism for Multi-Modal Tracking",
    "volume": "main",
    "abstract": "Generative models (GMs) have received increasing research interest for their remarkable capacity to achieve comprehensive understanding. However, their potential application in the domain of multi-modal tracking has remained unexplored. In this context, we seek to uncover the potential of harnessing generative techniques to address the critical challenge, information fusion, in multi-modal tracking. In this paper, we delve into two prominent GM techniques, namely, Conditional Generative Adversarial Networks (CGANs) and Diffusion Models (DMs). Different from the standard fusion process where the features from each modality are directly fed into the fusion block, we combine these multi-modal features with random noise in the GM framework, effectively transforming the original training samples into harder instances. This design excels at extracting discriminative clues from the features, enhancing the ultimate tracking performance. Based on this, we conduct extensive experiments across two multi-modal tracking tasks, three baseline methods, and four challenging benchmarks. The experimental results demonstrate that the proposed generative-based fusion mechanism achieves state-of-the-art performance by setting new records on GTOT, LasHeR and RGBD1K. Code will be available at https://github.com/Zhangyong-Tang/GMMT",
    "checked": true,
    "id": "f6b77ab279dc833f6eeba63571bc10a531b45c3b",
    "semantic_title": "generative-based fusion mechanism for multi-modal tracking",
    "citation_count": 4,
    "authors": [
      "Zhangyong Tang",
      "Tianyang Xu",
      "Xiaojun Wu",
      "Xue-Feng Zhu",
      "Josef Kittler"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28326": {
    "title": "Shadow Generation with Decomposed Mask Prediction and Attentive Shadow Filling",
    "volume": "main",
    "abstract": "Image composition refers to inserting a foreground object into a background image to obtain a composite image. In this work, we focus on generating plausible shadows for the inserted foreground object to make the composite image more realistic. To supplement the existing small-scale dataset, we create a large-scale dataset called RdSOBA with rendering techniques. Moreover, we design a two-stage network named DMASNet with decomposed mask prediction and attentive shadow filling. Specifically, in the first stage, we decompose shadow mask prediction into box prediction and shape prediction. In the second stage, we attend to reference background shadow pixels to fill the foreground shadow. Abundant experiments prove that our DMASNet achieves better visual effects and generalizes well to real composite images",
    "checked": true,
    "id": "3cf8fd268223f180914086b6976119f1319cec4e",
    "semantic_title": "shadow generation with decomposed mask prediction and attentive shadow filling",
    "citation_count": 1,
    "authors": [
      "Xinhao Tao",
      "Junyan Cao",
      "Yan Hong",
      "Li Niu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28327": {
    "title": "Towards Efficient and Effective Text-to-Video Retrieval with Coarse-to-Fine Visual Representation Learning",
    "volume": "main",
    "abstract": "In recent years, text-to-video retrieval methods based on CLIP have experienced rapid development. The primary direction of evolution is to exploit the much wider gamut of visual and textual cues to achieve alignment. Concretely, those methods with impressive performance often design a heavy fusion block for sentence (words)-video (frames) interaction, regardless of the prohibitive computation complexity. Nevertheless, these approaches are not optimal in terms of feature utilization and retrieval efficiency. To address this issue, we adopt multi-granularity visual feature learning, ensuring the model's comprehensiveness in capturing visual content features spanning from abstract to detailed levels during the training phase. To better leverage the multi-granularity features, we devise a two-stage retrieval architecture in the retrieval phase. This solution ingeniously balances the coarse and fine granularity of retrieval content. Moreover, it also strikes a harmonious equilibrium between retrieval effectiveness and efficiency. Specifically, in training phase, we design a parameter-free text-gated interaction block (TIB) for fine-grained video representation learning and embed an extra Pearson Constraint to optimize cross-modal representation learning. In retrieval phase, we use coarse-grained video representations for fast recall of top-k candidates, which are then reranked by fine-grained video representations. Extensive experiments on four benchmarks demonstrate the efficiency and effectiveness. Notably, our method achieves comparable performance with the current state-of-the-art methods while being nearly 50 times faster",
    "checked": true,
    "id": "5551dc0d5de947d6f2a270037f4ee48bd30dcc3c",
    "semantic_title": "towards efficient and effective text-to-video retrieval with coarse-to-fine visual representation learning",
    "citation_count": 0,
    "authors": [
      "Kaibin Tian",
      "Yanhua Cheng",
      "Yi Liu",
      "Xinglin Hou",
      "Quan Chen",
      "Han Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28328": {
    "title": "Open-Vocabulary Video Relation Extraction",
    "volume": "main",
    "abstract": "A comprehensive understanding of videos is inseparable from describing the action with its contextual action-object interactions. However, many current video understanding tasks prioritize general action classification and overlook the actors and relationships that shape the nature of the action, resulting in a superficial understanding of the action. Motivated by this, we introduce Open-vocabulary Video Relation Extraction (OVRE), a novel task that views action understanding through the lens of action-centric relation triplets. OVRE focuses on pairwise relations that take part in the action and describes these relation triplets with natural languages. Moreover, we curate the Moments-OVRE dataset, which comprises 180K videos with action-centric relation triplets, sourced from a multi-label action classification dataset. With Moments-OVRE, we further propose a cross-modal mapping model to generate relation triplets as a sequence. Finally, we benchmark existing cross-modal generation models on the new task of OVRE. Our code and dataset are available at https://github.com/Iriya99/OVRE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wentao Tian",
      "Zheng Wang",
      "Yuqian Fu",
      "Jingjing Chen",
      "Lechao Cheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28329": {
    "title": "Divide and Conquer: Hybrid Pre-training for Person Search",
    "volume": "main",
    "abstract": "Large-scale pre-training has proven to be an effective method for improving performance across different tasks. Current person search methods use ImageNet pre-trained models for feature extraction, yet it is not an optimal solution due to the gap between the pre-training task and person search task (as a downstream task). Therefore, in this paper, we focus on pre-training for person search, which involves detecting and re-identifying individuals simultaneously. Although labeled data for person search is scarce, datasets for two sub-tasks person detection and re-identification are relatively abundant. To this end, we propose a hybrid pre-training framework specifically designed for person search using sub-task data only. It consists of a hybrid learning paradigm that handles data with different kinds of supervisions, and an intra-task alignment module that alleviates domain discrepancy under limited resources. To the best of our knowledge, this is the first work that investigates how to support full-task pre-training using sub-task data. Extensive experiments demonstrate that our pre-trained model can achieve significant improvements across diverse protocols, such as person search method, fine-tuning data, pre-training data and model backbone. For example, our model improves ResNet50 based NAE by 10.3% relative improvement w.r.t. mAP. Our code and pre-trained models are released for plug-and-play usage to the person search community (https://github.com/personsearch/PretrainPS)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanling Tian",
      "Di Chen",
      "Yunan Liu",
      "Jian Yang",
      "Shanshan Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28330": {
    "title": "Taxonomy Driven Fast Adversarial Training",
    "volume": "main",
    "abstract": "Adversarial training (AT) is an effective defense method against gradient-based attacks to enhance the robustness of neural networks. Among them, single-step AT has emerged as a hotspot topic due to its simplicity and efficiency, requiring only one gradient propagation in generating adversarial examples. Nonetheless, the problem of catastrophic overfitting (CO) that causes training collapse remains poorly understood, and there exists a gap between the robust accuracy achieved through single- and multi-step AT. In this paper, we present a surprising finding that the taxonomy of adversarial examples reveals the truth of CO. Based on this conclusion, we propose taxonomy driven fast adversarial training (TDAT) which jointly optimizes learning objective, loss function, and initialization method, thereby can be regarded as a new paradigm of single-step AT. Compared with other fast AT methods, TDAT can boost the robustness of neural networks, alleviate the influence of misclassified examples, and prevent CO during the training process while requiring almost no additional computational and memory resources. Our method achieves robust accuracy improvement of 1.59%, 1.62%, 0.71%, and 1.26% on CIFAR-10, CIFAR-100, Tiny ImageNet, and ImageNet-100 datasets, when against projected gradient descent PGD10 attack with perturbation budget 8/255. Furthermore, our proposed method also achieves state-of-the-art robust accuracy against other attacks. Code is available at https://github.com/bookman233/TDAT",
    "checked": true,
    "id": "94f1e0726197375c517f967be8f9b63675f9ab43",
    "semantic_title": "taxonomy driven fast adversarial training",
    "citation_count": 0,
    "authors": [
      "Kun Tong",
      "Chengze Jiang",
      "Jie Gui",
      "Yuan Cao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28331": {
    "title": "End-to-End Real-Time Vanishing Point Detection with Transformer",
    "volume": "main",
    "abstract": "In this paper, we propose a novel transformer-based end-to-end real-time vanishing point detection method, which is named Vanishing Point TRansformer (VPTR). The proposed method can directly regress the locations of vanishing points from given images. To achieve this goal, we pose vanishing point detection as a point object detection task on the Gaussian hemisphere with region division. Considering low-level features always provide more geometric information which can contribute to accurate vanishing point prediction, we propose a clear architecture where vanishing point queries in the decoder can directly gather multi-level features from CNN backbone with deformable attention in VPTR. Our method does not rely on line detection or Manhattan world assumption, which makes it more flexible to use. VPTR runs at an inferring speed of 140 FPS on one NVIDIA 3090 card. Experimental results on synthetic and real-world datasets demonstrate that our method can be used in both natural and structural scenes, and is superior to other state-of-the-art methods on the balance of accuracy and efficiency",
    "checked": true,
    "id": "7e678435b660a9d1ccfd46d7e75891300b71c231",
    "semantic_title": "end-to-end real-time vanishing point detection with transformer",
    "citation_count": 0,
    "authors": [
      "Xin Tong",
      "Shi Peng",
      "Yufei Guo",
      "Xuhui Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28332": {
    "title": "Discrete Cycle-Consistency Based Unsupervised Deep Graph Matching",
    "volume": "main",
    "abstract": "We contribute to the sparsely populated area of unsupervised deep graph matching with application to keypoint matching in images. Contrary to the standard supervised approach, our method does not require ground truth correspondences between keypoint pairs. Instead, it is self-supervised by enforcing consistency of matchings between images of the same object category. As the matching and the consistency loss are discrete, their derivatives cannot be straightforwardly used for learning. We address this issue in a principled way by building our method upon the recent results on black-box differentiation of combinatorial solvers. This makes our method exceptionally flexible, as it is compatible with arbitrary network architectures and combinatorial solvers. Our experimental evaluation suggests that our technique sets a new state-of-the-art for unsupervised graph matching",
    "checked": true,
    "id": "faf26581e1c953f3fb4bc671a330af5c5ecbd71f",
    "semantic_title": "discrete cycle-consistency based unsupervised deep graph matching",
    "citation_count": 0,
    "authors": [
      "Siddharth Tourani",
      "Muhammad Haris Khan",
      "Carsten Rother",
      "Bogdan Savchynskyy"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28333": {
    "title": "A Unified Masked Autoencoder with Patchified Skeletons for Motion Synthesis",
    "volume": "main",
    "abstract": "The synthesis of human motion has traditionally been addressed through task-dependent models that focus on specific challenges, such as predicting future motions or filling in intermediate poses conditioned on known key-poses. In this paper, we present a novel task-independent model called UNIMASK-M, which can effectively address these challenges using a unified architecture. Our model obtains comparable or better performance than the state-of-the-art in each field. Inspired by Vision Transformers (ViTs), our UNIMASK-M model decomposes a human pose into body parts to leverage the spatio-temporal relationships existing in human motion. Moreover, we reformulate various pose-conditioned motion synthesis tasks as a reconstruction problem with different masking patterns given as input. By explicitly informing our model about the masked joints, our UNIMASK-M becomes more robust to occlusions. Experimental results show that our model successfully forecasts human motion on the Human3.6M dataset while achieving state-of-the-art results in motion inbetweening on the LaFAN1 dataset for long transition periods",
    "checked": true,
    "id": "937792056c5c7dc2d637ac5aaf0c792002ebe427",
    "semantic_title": "a unified masked autoencoder with patchified skeletons for motion synthesis",
    "citation_count": 2,
    "authors": [
      "Esteve Valls Mascaró",
      "Hyemin Ahn",
      "Dongheui Lee"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28334": {
    "title": "CoVR: Learning Composed Video Retrieval from Web Video Captions",
    "volume": "main",
    "abstract": "Composed Image Retrieval (CoIR) has recently gained popularity as a task that considers both text and image queries together, to search for relevant images in a database. Most CoIR approaches require manually annotated datasets, comprising image-text-image triplets, where the text describes a modification from the query image to the target image. However, manual curation of CoIR triplets is expensive and prevents scalability. In this work, we instead propose a scalable automatic dataset creation methodology that generates triplets given video-caption pairs, while also expanding the scope of the task to include composed video retrieval (CoVR). To this end, we mine paired videos with a similar caption from a large database, and leverage a large language model to generate the corresponding modification text. Applying this methodology to the extensive WebVid2M collection, we automatically construct our WebVid-CoVR dataset, resulting in 1.6 million triplets. Moreover, we introduce a new benchmark for CoVR with a manually annotated evaluation set, along with baseline results. Our experiments further demonstrate that training a CoVR model on our dataset effectively transfers to CoIR, leading to improved state-of-the-art performance in the zero-shot setup on both the CIRR and FashionIQ benchmarks. Our code, datasets, and models are publicly available at https://imagine.enpc.fr/~ventural/covr",
    "checked": true,
    "id": "1b6e40c46f2e680620cf70218ae4edbc895d305f",
    "semantic_title": "covr: learning composed video retrieval from web video captions",
    "citation_count": 18,
    "authors": [
      "Lucas Ventura",
      "Antoine Yang",
      "Cordelia Schmid",
      "Gül Varol"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28335": {
    "title": "Supervision Interpolation via LossMix: Generalizing Mixup for Object Detection and Beyond",
    "volume": "main",
    "abstract": "The success of data mixing augmentations in image classification tasks has been well-received. However, these techniques cannot be readily applied to object detection due to challenges such as spatial misalignment, foreground/background distinction, and plurality of instances. To tackle these issues, we first introduce a novel conceptual framework called Supervision Interpolation (SI), which offers a fresh perspective on interpolation-based augmentations by relaxing and generalizing Mixup. Based on SI, we propose LossMix, a simple yet versatile and effective regularization that enhances the performance and robustness of object detectors and more. Our key insight is that we can effectively regularize the training on mixed data by interpolating their loss errors instead of ground truth labels. Empirical results on the PASCAL VOC and MS COCO datasets demonstrate that LossMix can consistently outperform state-of-the-art methods widely adopted for detection. Furthermore, by jointly leveraging LossMix with unsupervised domain adaptation, we successfully improve existing approaches and set a new state of the art for cross-domain object detection",
    "checked": true,
    "id": "f1e7642741e16a978a33e8a44dc99e46034b972a",
    "semantic_title": "supervision interpolation via lossmix: generalizing mixup for object detection and beyond",
    "citation_count": 0,
    "authors": [
      "Thanh Vu",
      "Baochen Sun",
      "Bodi Yuan",
      "Alex Ngai",
      "Yueqi Li",
      "Jan-Michael  Frahm"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28336": {
    "title": "Integrated Decision Gradients: Compute Your Attributions Where the Model Makes Its Decision",
    "volume": "main",
    "abstract": "Attribution algorithms are frequently employed to explain the decisions of neural network models. Integrated Gradients (IG) is an influential attribution method due to its strong axiomatic foundation. The algorithm is based on integrating the gradients along a path from a reference image to the input image. Unfortunately, it can be observed that gradients computed from regions where the output logit changes minimally along the path provide poor explanations for the model decision, which is called the saturation effect problem. In this paper, we propose an attribution algorithm called integrated decision gradients (IDG). The algorithm focuses on integrating gradients from the region of the path where the model makes its decision, i.e., the portion of the path where the output logit rapidly transitions from zero to its final value. This is practically realized by scaling each gradient by the derivative of the output logit with respect to the path. The algorithm thereby provides a principled solution to the saturation problem. Additionally, we minimize the errors within the Riemann sum approximation of the path integral by utilizing non-uniform subdivisions determined by adaptive sampling. In the evaluation on ImageNet, it is demonstrated that IDG outperforms IG, Left-IG, Guided IG, and adversarial gradient integration both qualitatively and quantitatively using standard insertion and deletion metrics across three common models",
    "checked": true,
    "id": "a47a96acfdf239b6236ecad92d901fec524b9a0c",
    "semantic_title": "integrated decision gradients: compute your attributions where the model makes its decision",
    "citation_count": 1,
    "authors": [
      "Chase Walker",
      "Sumit Jha",
      "Kenny Chen",
      "Rickard Ewetz"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28337": {
    "title": "HISR: Hybrid Implicit Surface Representation for Photorealistic 3D Human Reconstruction",
    "volume": "main",
    "abstract": "Neural reconstruction and rendering strategies have demonstrated state-of-the-art performances due, in part, to their ability to preserve high level shape details. Existing approaches, however, either represent objects as implicit surface functions or neural volumes and still struggle to recover shapes with heterogeneous materials, in particular human skin, hair or clothes. To this aim, we present a new hybrid implicit surface representation to model human shapes. This representation is composed of two surface layers that represent opaque and translucent regions on the clothed human body. We segment different regions automatically using visual cues and learn to reconstruct two signed distance functions (SDFs). We perform surface-based rendering on opaque regions (e.g., body, face, clothes) to preserve high-fidelity surface normals and volume rendering on translucent regions (e.g., hair). Experiments demonstrate that our approach obtains state-of-the-art results on 3D human reconstructions, and also shows competitive performances on other objects",
    "checked": true,
    "id": "c9bf842fa98ab28c4d19701a2fa4136542bf1aa2",
    "semantic_title": "hisr: hybrid implicit surface representation for photorealistic 3d human reconstruction",
    "citation_count": 1,
    "authors": [
      "Angtian Wang",
      "Yuanlu Xu",
      "Nikolaos Sarafianos",
      "Robert Maier",
      "Edmond Boyer",
      "Alan Yuille",
      "Tony Tung"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28338": {
    "title": "VIGC: Visual Instruction Generation and Correction",
    "volume": "main",
    "abstract": "The integration of visual encoders and large language models (LLMs) has driven recent progress in multimodal large language models (MLLMs). However, the scarcity of high-quality instruction-tuning data for vision-language tasks remains a challenge. The current leading paradigm, such as LLaVA, relies on language-only GPT-4 to generate data, which requires pre-annotated image captions and detection bounding boxes, suffering from understanding image details. A practical solution to this problem would be to utilize the available multimodal large language models to generate instruction data for vision-language tasks. However, it's worth noting that the currently accessible MLLMs are not as powerful as their LLM counterparts, as they tend to produce inadequate responses and generate false information. As a solution for addressing the current issue, this paper proposes the Visual Instruction Generation and Correction (VIGC) framework that enables multimodal large language models to generate instruction-tuning data and progressively enhance its quality on-the-fly. Specifically, Visual Instruction Generation (VIG) guides the vision-language model to generate diverse instruction-tuning data. To ensure generation quality, Visual Instruction Correction (VIC) adopts an iterative update mechanism to correct any inaccuracies in data produced by VIG, effectively reducing the risk of hallucination. Leveraging the diverse, high-quality data generated by VIGC, we finetune mainstream models and validate data quality based on various evaluations. Experimental results demonstrate that VIGC not only compensates for the shortcomings of language-only data generation methods, but also effectively enhances the benchmark performance. The models, datasets, and code are available at https://opendatalab.github.io/VIGC",
    "checked": true,
    "id": "e47d276bad18f441950c8136672ae6864e95323f",
    "semantic_title": "vigc: visual instruction generation and correction",
    "citation_count": 32,
    "authors": [
      "Bin Wang",
      "Fan Wu",
      "Xiao Han",
      "Jiahui Peng",
      "Huaping Zhong",
      "Pan Zhang",
      "Xiaoyi Dong",
      "Weijia Li",
      "Wei Li",
      "Jiaqi Wang",
      "Conghui He"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28339": {
    "title": "Low-Light Face Super-resolution via Illumination, Structure, and Texture Associated Representation",
    "volume": "main",
    "abstract": "Human face captured at night or in dimly lit environments has become a common practice, accompanied by complex low-light and low-resolution degradations. However, the existing face super-resolution (FSR) technologies and derived cascaded schemes are inadequate to recover credible textures. In this paper, we propose a novel approach that decomposes the restoration task into face structural fidelity maintaining and texture consistency learning. The former aims to enhance the quality of face images while improving the structural fidelity, while the latter focuses on eliminating perturbations and artifacts caused by low-light degradation and reconstruction. Based on this, we develop a novel low-light low-resolution face super-resolution framework. Our method consists of two steps: an illumination correction face super-resolution network (IC-FSRNet) for lighting the face and recovering the structural information, and a detail enhancement model (DENet) for improving facial details, thus making them more visually appealing and easier to analyze. As the relighted regions could provide complementary information to boost face super-resolution and vice versa, we introduce the mutual learning to harness the informative components from relighted regions and reconstruction, and achieve the iterative refinement. In addition, DENet equipped with diffusion probabilistic model is built to further improve face image visual quality. Experiments demonstrate that the proposed joint optimization framework achieves significant improvements in reconstruction quality and perceptual quality over existing two-stage sequential solutions. Code is available at https://github.com/wcy-cs/IC-FSRDENet",
    "checked": true,
    "id": "2123ffe273cf03765191df8a93ff46b19bd3c4f6",
    "semantic_title": "low-light face super-resolution via illumination, structure, and texture associated representation",
    "citation_count": 0,
    "authors": [
      "Chenyang Wang",
      "Junjun Jiang",
      "Kui Jiang",
      "Xianming Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28340": {
    "title": "SelfPromer: Self-Prompt Dehazing Transformers with Depth-Consistency",
    "volume": "main",
    "abstract": "This work presents an effective depth-consistency Self-Prompt Transformer, terms as SelfPromer, for image dehazing. It is motivated by an observation that the estimated depths of an image with haze residuals and its clear counterpart vary. Enforcing the depth consistency of dehazed images with clear ones, therefore, is essential for dehazing. For this purpose, we develop a prompt based on the features of depth differences between the hazy input images and corresponding clear counterparts that can guide dehazing models for better restoration. Specifically, we first apply deep features extracted from the input images to the depth difference features for generating the prompt that contains the haze residual information in the input. Then we propose a prompt embedding module that is designed to perceive the haze residuals, by linearly adding the prompt to the deep features. Further, we develop an effective prompt attention module to pay more attention to haze residuals for better removal. By incorporating the prompt, prompt embedding, and prompt attention into an encoder-decoder network based on VQGAN, we can achieve better perception quality. As the depths of clear images are not available at inference, and the dehazed images with one-time feed-forward execution may still contain a portion of haze residuals, we propose a new continuous self-prompt inference that can iteratively correct the dehazing model towards better haze-free image generation. Extensive experiments show that our SelfPromer performs favorably against the state-of-the-art approaches on both synthetic and real-world datasets in terms of perception metrics including NIQE, PI, and PIQE. The source codes will be made available at https://github.com/supersupercong/SelfPromer",
    "checked": true,
    "id": "026553eae8d0abc2b9c6c089e24137fb077de65d",
    "semantic_title": "selfpromer: self-prompt dehazing transformers with depth-consistency",
    "citation_count": 10,
    "authors": [
      "Cong Wang",
      "Jinshan Pan",
      "Wanyu Lin",
      "Jiangxin Dong",
      "Wei Wang",
      "Xiao-Ming Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28341": {
    "title": "Correlation Matching Transformation Transformers for UHD Image Restoration",
    "volume": "main",
    "abstract": "This paper proposes UHDformer, a general Transformer for Ultra-High-Definition (UHD) image restoration. UHDformer contains two learning spaces: (a) learning in high-resolution space and (b) learning in low-resolution space. The former learns multi-level high-resolution features and fuses low-high features and reconstructs the residual images, while the latter explores more representative features learning from the high-resolution ones to facilitate better restoration. To better improve feature representation in low-resolution space, we propose to build feature transformation from the high-resolution space to the low-resolution one. To that end, we propose two new modules: Dual-path Correlation Matching Transformation module (DualCMT) and Adaptive Channel Modulator (ACM). The DualCMT selects top C/r (r is greater or equal to 1 which controls the squeezing level) correlation channels from the max-pooling/mean-pooling high-resolution features to replace low-resolution ones in Transformers, which can effectively squeeze useless content to improve the feature representation in low-resolution space to facilitate better recovery. The ACM is exploited to adaptively modulate multi-level high-resolution features, enabling to provide more useful features to low-resolution space for better learning. Experimental results show that our UHDformer reduces about ninety-seven percent model sizes compared with most state-of-the-art methods while significantly improving performance under different training sets on 3 UHD image restoration tasks, including low-light image enhancement, image dehazing, and image deblurring. The source codes will be made available at https://github.com/supersupercong/UHDformer",
    "checked": true,
    "id": "11ab15d3bd9753e22192dd93eabf83f7272a08ee",
    "semantic_title": "correlation matching transformation transformers for uhd image restoration",
    "citation_count": 3,
    "authors": [
      "Cong Wang",
      "Jinshan Pan",
      "Wei Wang",
      "Gang Fu",
      "Siyuan Liang",
      "Mengzhu Wang",
      "Xiao-Ming Wu",
      "Jun Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28342": {
    "title": "EulerMormer: Robust Eulerian Motion Magnification via Dynamic Filtering within Transformer",
    "volume": "main",
    "abstract": "Video Motion Magnification (VMM) aims to break the resolution limit of human visual perception capability and reveal the imperceptible minor motion that contains valuable information in the macroscopic domain. However, challenges arise in this task due to photon noise inevitably introduced by photographic devices and spatial inconsistency in amplification, leading to flickering artifacts in static fields and motion blur and distortion in dynamic fields in the video. Existing methods focus on explicit motion modeling without emphasizing prioritized denoising during the motion magnification process. This paper proposes a novel dynamic filtering strategy to achieve static-dynamic field adaptive denoising. Specifically, based on Eulerian theory, we separate texture and shape to extract motion representation through inter-frame shape differences, expecting to leverage these subdivided features to solve this task finely. Then, we introduce a novel dynamic filter that eliminates noise cues and preserves critical features in the motion magnification and amplification generation phases. Overall, our unified framework, EulerMormer, is a pioneering effort to first equip with Transformer in learning-based VMM. The core of the dynamic filter lies in a global dynamic sparse cross-covariance attention mechanism that explicitly removes noise while preserving vital information, coupled with a multi-scale dual-path gating mechanism that selectively regulates the dependence on different frequency features to reduce spatial attenuation and complement motion boundaries. We demonstrate extensive experiments that EulerMormer achieves more robust video motion magnification from the Eulerian perspective, significantly outperforming state-of-the-art methods. The source code is available at https://github.com/VUT-HFUT/EulerMormer",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fei Wang",
      "Dan Guo",
      "Kun Li",
      "Meng Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28343": {
    "title": "Learning to Learn Better Visual Prompts",
    "volume": "main",
    "abstract": "Prompt tuning provides a low-cost way of adapting vision-language models (VLMs) for various downstream vision tasks without requiring updating the huge pre-trained parameters. Dispensing with the conventional manual crafting of prompts, the recent prompt tuning method of Context Optimization (CoOp) introduces adaptable vectors as text prompts. Nevertheless, several previous works point out that the CoOp-based approaches are easy to overfit to the base classes and hard to generalize to novel classes. In this paper, we reckon that the prompt tuning works well only in the base classes because of the limited capacity of the adaptable vectors. The scale of the pre-trained model is hundreds times the scale of the adaptable vector, thus the learned vector has a very limited ability to absorb the knowledge of novel classes. To minimize this excessive overfitting of textual knowledge on the base class, we view prompt tuning as learning to learn (LoL) and learn the prompt in the way of meta-learning, the training manner of dividing the base classes into many different subclasses could fully exert the limited capacity of prompt tuning and thus transfer it power to recognize the novel classes. To be specific, we initially perform fine-tuning on the base class based on the CoOp method for pre-trained CLIP. Subsequently, predicated on the fine-tuned CLIP model, we carry out further fine-tuning in an N-way K-shot manner from the perspective of meta-learning on the base classes. We finally apply the learned textual vector and VLM for unseen classes.Extensive experiments on benchmark datasets validate the efficacy of our meta-learning-informed prompt tuning, affirming its role as a robust optimization strategy for VLMs",
    "checked": true,
    "id": "c267e53f823a2ef9e9e6bbc26196d68b789fb4c2",
    "semantic_title": "learning to learn better visual prompts",
    "citation_count": 1,
    "authors": [
      "Fengxiang Wang",
      "Wanrong Huang",
      "Shaowu Yang",
      "Qi Fan",
      "Long Lan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28344": {
    "title": "MuST: Robust Image Watermarking for Multi-Source Tracing",
    "volume": "main",
    "abstract": "In recent years, with the popularity of social media applications, massive digital images are available online, which brings great convenience to image recreation. However, the use of unauthorized image materials in multi-source composite images is still inadequately regulated, which may cause significant loss and discouragement to the copyright owners of the source image materials. Ideally, deep watermarking techniques could provide a solution for protecting these copyrights based on their encoder-noise-decoder training strategy. Yet existing image watermarking schemes, which are mostly designed for single images, cannot well address the copyright protection requirements in this scenario, since the multi-source image composing process commonly includes distortions that are not well investigated in previous methods, e.g., the extreme downsizing. To meet such demands, we propose MuST, a multi-source tracing robust watermarking scheme, whose architecture includes a multi-source image detector and minimum external rectangle operation for multiple watermark resynchronization and extraction. Furthermore, we constructed an image material dataset covering common image categories and designed the simulation model of the multi-source image composing process as the noise layer. Experiments demonstrate the excellent performance of MuST in tracing sources of image materials from the composite images compared with SOTA watermarking methods, which could maintain the extraction accuracy above 98% to trace the sources of at least 3 different image materials while keeping the average PSNR of watermarked image materials higher than 42.51 dB. We released our code on https://github.com/MrCrims/MuST",
    "checked": true,
    "id": "5ee6562ed0c2a4edd1b3ecf81eaef618d6cd87e6",
    "semantic_title": "must: robust image watermarking for multi-source tracing",
    "citation_count": 3,
    "authors": [
      "Guanjie Wang",
      "Zehua Ma",
      "Chang Liu",
      "Xi Yang",
      "Han Fang",
      "Weiming Zhang",
      "Nenghai Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28345": {
    "title": "LION: Implicit Vision Prompt Tuning",
    "volume": "main",
    "abstract": "Despite recent promising performances across a range of vision tasks, vision Transformers still have an issue of high computational costs. Recently, vision prompt learning has provided an economical solution to this problem without fine-tuning the whole large-scale model. However, the efficiency and effectiveness of existing models are still far from satisfactory due to the parameter cost of extensive prompt blocks and tricky prompt framework designs. In this paper, we propose a light-weight prompt framework named impLicit vIsion prOmpt tuNing (LION), which is motivated by deep implicit models with stable low memory costs for various complex tasks. In particular, we merely insect two equilibrium implicit layers in two ends of the pre-trained backbone with parameters frozen. Moreover, according to the lottery hypothesis, we further prune the parameters to relieve the computation burden in implicit layers. Various experiments have validated that our LION obtains promising performances on a wide range of datasets. Most importantly, LION reduces up to 11.5 % of training parameter numbers while obtaining higher performance than the state-of-the-art VPT, especially under challenging scenes. Furthermore, we find that our proposed LION has an excellent generalization performance, making it an easy way to boost transfer learning in the future",
    "checked": true,
    "id": "29a7644583d7042c4476af126f3fe0a372897abe",
    "semantic_title": "lion: implicit vision prompt tuning",
    "citation_count": 8,
    "authors": [
      "Haixin Wang",
      "Jianlong Chang",
      "Yihang Zhai",
      "Xiao Luo",
      "Jinan Sun",
      "Zhouchen Lin",
      "Qi Tian"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28346": {
    "title": "B-spine: Learning B-spline Curve Representation for Robust and Interpretable Spinal Curvature Estimation",
    "volume": "main",
    "abstract": "Spinal curvature estimation is important to the diagnosis and treatment of the scoliosis. Existing methods face several issues such as the need of expensive annotations on the vertebral landmarks and being sensitive to the image quality. It is challenging to achieve robust estimation and obtain interpretable results, especially for low-quality images which are blurry and hazy. In this paper, we propose B-Spine, a novel deep learning pipeline to learn B-spline curve representation of the spine and estimate the Cobb angles for spinal curvature estimation from low-quality X-ray images. Given a low quality input, a novel SegRefine network which employs the unpaired image-to-image translation is proposed to generate a high quality spine mask from the initial segmentation result. Next, a novel mask-based B-spline prediction model is proposed to predict the B-spline curve for the spine centerline. Finally, the Cobb angles are estimated by a hybrid approach which combines the curve slope analysis and a curve based regression model. We conduct quantitative and qualitative comparisons with the representative and SOTA learning-based methods on the public AASCE2019 dataset and our new proposed JLU-CJUH dataset which contains more challenging low-quality images. The superior performance on both datasets shows our method can achieve both robustness and interpretability for spinal curvature estimation",
    "checked": true,
    "id": "464cef125792b6d70d8cb4f6ee45159cb771babe",
    "semantic_title": "b-spine: learning b-spline curve representation for robust and interpretable spinal curvature estimation",
    "citation_count": 0,
    "authors": [
      "Hao Wang",
      "Qiang Song",
      "Ruofeng Yin",
      "Rui Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28347": {
    "title": "ViLT-CLIP: Video and Language Tuning CLIP with Multimodal Prompt Learning and Scenario-Guided Optimization",
    "volume": "main",
    "abstract": "Pre-trained vision-language(V-L) models such as CLIP have demonstrated impressive Zero-Shot performance in many downstream tasks. Since adopting contrastive video-text pairs methods like CLIP to video tasks is limited by its high cost and scale, recent approaches focus on efficiently transferring the image-based CLIP to the video domain. A major finding is that fine-tuning the pre-trained model to achieve strong fully supervised performance leads to low zero shot, few shot, and base to novel generalization. Instead, freezing the backbone network to maintain generalization ability weakens fully supervised performance. Otherwise, no single prompt tuning branch consistently performs optimally. In this work, we proposed a multimodal prompt learning scheme that balances supervised and generalized performance. Our prompting approach contains three sections: 1) Independent prompt on both the vision and text branches to learn the language and visual contexts. 2) Inter-modal prompt mapping to ensure mutual synergy. 3) Reducing the discrepancy between the hand-crafted prompt (a video of a person doing [CLS]) and the learnable prompt, to alleviate the forgetting about essential video scenarios. Extensive validation of fully supervised, zero-shot, few-shot, base-to-novel generalization settings for video recognition indicates that the proposed approach achieves competitive performance with less commute cost",
    "checked": true,
    "id": "c8c19be4d36462c3ee3c8a2867668f0435a73127",
    "semantic_title": "vilt-clip: video and language tuning clip with multimodal prompt learning and scenario-guided optimization",
    "citation_count": 0,
    "authors": [
      "Hao Wang",
      "Fang Liu",
      "Licheng   Jiao",
      "Jiahao Wang",
      "Zehua Hao",
      "Shuo Li",
      "Lingling Li",
      "Puhua Chen",
      "Xu Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28348": {
    "title": "Triple Feature Disentanglement for One-Stage Adaptive Object Detection",
    "volume": "main",
    "abstract": "In recent advancements concerning Domain Adaptive Object Detection (DAOD), unsupervised domain adaptation techniques have proven instrumental. These methods enable enhanced detection capabilities within unlabeled target domains by mitigating distribution differences between source and target domains. A subset of DAOD methods employs disentangled learning to segregate Domain-Specific Representations (DSR) and Domain-Invariant Representations (DIR), with ultimate predictions relying on the latter. Current practices in disentanglement, however, often lead to DIR containing residual domain-specific information. To address this, we introduce the Multi-level Disentanglement Module (MDM) that progressively disentangles DIR, enhancing comprehensive disentanglement. Additionally, our proposed Cyclic Disentanglement Module (CDM) facilitates DSR separation. To refine the process further, we employ the Categorical Features Disentanglement Module (CFDM) to isolate DIR and DSR, coupled with category alignment across scales for improved source-target domain alignment. Given its practical suitability, our model is constructed upon the foundational framework of the Single Shot MultiBox Detector (SSD), which is a one-stage object detection approach. Experimental validation highlights the effectiveness of our method, demonstrating its state-of-the-art performance across three benchmark datasets",
    "checked": true,
    "id": "8c371864fa71255b34a660803efb14c3255fa5e5",
    "semantic_title": "triple feature disentanglement for one-stage adaptive object detection",
    "citation_count": 0,
    "authors": [
      "Haoan Wang",
      "Shilong Jia",
      "Tieyong Zeng",
      "Guixu Zhang",
      "Zhi Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28349": {
    "title": "Neural Physical Simulation with Multi-Resolution Hash Grid Encoding",
    "volume": "main",
    "abstract": "We explore the generalization of the implicit representation in the physical simulation task. Traditional time-dependent partial differential equations (PDEs) solvers for physical simulation often adopt the grid or mesh for spatial discretization, which is memory-consuming for high resolution and lack of adaptivity. Many implicit representations like local extreme machine or Siren are proposed but they are still too compact to suffer from limited accuracy in handling local details and a long time of convergence. We contribute a neural simulation framework based on multi-resolution hash grid representation to introduce hierarchical consideration of global and local information, simultaneously. Furthermore, we propose two key strategies: 1) a numerical gradient method for computing high-order derivatives with boundary conditions; 2) a range analysis sample method for fast neural geometry boundary sampling with dynamic topologies. Our method shows much higher accuracy and strong flexibility for various simulation problems: e.g., large elastic deformations, complex fluid dynamics, and multi-scale phenomena which remain challenging for existing neural physical solvers",
    "checked": true,
    "id": "505429797912ad300f596241bf768d4959cbf109",
    "semantic_title": "neural physical simulation with multi-resolution hash grid encoding",
    "citation_count": 2,
    "authors": [
      "Haoxiang Wang",
      "Tao Yu",
      "Tianwei Yang",
      "Hui Qiao",
      "Qionghai Dai"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28350": {
    "title": "Deep Unfolded Network with Intrinsic Supervision for Pan-Sharpening",
    "volume": "main",
    "abstract": "Existing deep pan-sharpening methods lack the learning of complementary information between PAN and MS modalities in the intermediate layers, and exhibit low interpretability due to their black-box designs. To this end, an interpretable deep unfolded network with intrinsic supervision for pan-sharpening is proposed. Building upon the observation degradation process, it formulates the pan-sharpening task as a variational model minimization with spatial consistency prior and spectral projection prior. The former prior requires a joint component decomposition of PAN and MS images to extract intrinsic features. By being supervised in the intermediate layers, it can selectively provide high-frequency information for spatial enhancement. The latter prior constrains the intensity correlation between MS and PAN images derived from physical observations, so as to improve spectral fidelity. To further enhance the transparency of network design, we develop an iterative solution algorithm following the half-quadratic splitting to unfold the deep model. It rigorously adheres to the variational model, significantly enhancing the interpretability behind network design and efficiently alternating the optimization of the network. Extensive experiments demonstrate the advantages of our method compared to state-of-the-arts, showcasing its remarkable generalization capability to real-world scenes. Our code is publicly available at https://github.com/Baixuzx7/DISPNet",
    "checked": true,
    "id": "782024b542b1fa60ed734fc490a0ab7061876c59",
    "semantic_title": "deep unfolded network with intrinsic supervision for pan-sharpening",
    "citation_count": 1,
    "authors": [
      "Hebaixu Wang",
      "Meiqi Gong",
      "Xiaoguang Mei",
      "Hao Zhang",
      "Jiayi Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28351": {
    "title": "Continuous Piecewise-Affine Based Motion Model for Image Animation",
    "volume": "main",
    "abstract": "Image animation aims to bring static images to life according to driving videos and create engaging visual content that can be used for various purposes such as animation, entertainment, and education. Recent unsupervised methods utilize affine and thin-plate spline transformations based on keypoints to transfer the motion in driving frames to the source image. However, limited by the expressive power of the transformations used, these methods always produce poor results when the gap between the motion in the driving frame and the source image is large. To address this issue, we propose to model motion from the source image to the driving frame in highly-expressive diffeomorphism spaces. Firstly, we introduce Continuous Piecewise-Affine based (CPAB) transformation to model the motion and present a well-designed inference algorithm to generate CPAB transformation from control keypoints. Secondly, we propose a SAM-guided keypoint semantic loss to further constrain the keypoint extraction process and improve the semantic consistency between the corresponding keypoints on the source and driving images. Finally, we design a structure alignment loss to align the structure-related features extracted from driving and generated images, thus helping the generator generate results that are more consistent with the driving action. Extensive experiments on four datasets demonstrate the effectiveness of our method against state-of-the-art competitors quantitatively and qualitatively. Code will be publicly available at: https://github.com/DevilPG/AAAI2024-CPABMM",
    "checked": true,
    "id": "8172765b02f2b56ddc420e6bd74d6bb621a5a140",
    "semantic_title": "continuous piecewise-affine based motion model for image animation",
    "citation_count": 3,
    "authors": [
      "Hexiang Wang",
      "Fengqi Liu",
      "Qianyu Zhou",
      "Ran Yi",
      "Xin Tan",
      "Lizhuang Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28352": {
    "title": "Temporal Adaptive RGBT Tracking with Modality Prompt",
    "volume": "main",
    "abstract": "RGBT tracking has been widely used in various fields such as robotics, surveillance processing, and autonomous driving. Existing RGBT trackers fully explore the spatial information between the template and the search region and locate the target based on the appearance matching results. However, these RGBT trackers have very limited exploitation of temporal information, either ignoring temporal information or exploiting it through online sampling and training. The former struggles to cope with the object state changes, while the latter neglects the correlation between spatial and temporal information. To alleviate these limitations, we propose a novel Temporal Adaptive RGBT Tracking framework, named as TATrack. TATrack has a spatio-temporal two-stream structure and captures temporal information by an online updated template, where the two-stream structure refers to the multi-modal feature extraction and cross-modal interaction for the initial template and the online update template respectively. TATrack contributes to comprehensively exploit spatio-temporal information and multi-modal information for target localization. In addition, we design a spatio-temporal interaction (STI) mechanism that bridges two branches and enables cross-modal interaction to span longer time scales. Extensive experiments on three popular RGBT tracking benchmarks show that our method achieves state-of-the-art performance, while running at real-time speed",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyu Wang",
      "Xiaotao Liu",
      "Yifan Li",
      "Meng Sun",
      "Dian Yuan",
      "Jing Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28353": {
    "title": "SAUI: Scale-Aware Unseen Imagineer for Zero-Shot Object Detection",
    "volume": "main",
    "abstract": "Zero-shot object detection (ZSD) aims to localize and classify unseen objects without access to their training annotations. As a prevailing solution to ZSD, generation-based methods synthesize unseen visual features by taking seen features as reference and class semantic embeddings as guideline. Although previous works continuously improve the synthesis quality, they fail to consider the scale-varying nature of unseen objects. The generation process is preformed over a single scale of object features and thus lacks scale-diversity among synthesized features. In this paper, we reveal the scale-varying challenge in ZSD and propose a Scale-Aware Unseen Imagineer (SAUI) to lead the way of a novel scale-aware ZSD paradigm. To obtain multi-scale features of seen-class objects, we design a specialized coarse-to-fine extractor to capture features through multiple scale-views. To generate unseen features scale by scale, we innovate a Series-GAN synthesizer along with three scale-aware contrastive components to imagine separable, diverse and robust scale-wise unseen features. Extensive experiments on PASCAL VOC, COCO and DIOR datasets demonstrate SAUI's better performance in different scenarios, especially for scale-varying and small objects. Notably, SAUI achieves the new state-of-the art performance on COCO and DIOR",
    "checked": true,
    "id": "f271a239b989268dd76f5a8aa4152fa1da79dc13",
    "semantic_title": "saui: scale-aware unseen imagineer for zero-shot object detection",
    "citation_count": 0,
    "authors": [
      "Jiahao Wang",
      "Caixia Yan",
      "Weizhan Zhang",
      "Huan Liu",
      "Hao Sun",
      "Qinghua Zheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28354": {
    "title": "Omnidirectional Image Super-resolution via Bi-projection Fusion",
    "volume": "main",
    "abstract": "With the rapid development of virtual reality, omnidirectional images (ODIs) have attracted much attention from both the industrial community and academia. However, due to storage and transmission limitations, the resolution of current ODIs is often insufficient to provide an immersive virtual reality experience. Previous approaches address this issue using conventional 2D super-resolution techniques on equirectangular projection without exploiting the unique geometric properties of ODIs. In particular, the equirectangular projection (ERP) provides a complete field-of-view but introduces significant distortion, while the cubemap projection (CMP) can reduce distortion yet has a limited field-of-view. In this paper, we present a novel Bi-Projection Omnidirectional Image Super-Resolution (BPOSR) network to take advantage of the geometric properties of the above two projections. Then, we design two tailored attention methods for these projections: Horizontal Striped Transformer Block (HSTB) for ERP and Perspective Shift Transformer Block (PSTB) for CMP. Furthermore, we propose a fusion module to make these projections complement each other. Extensive experiments demonstrate that BPOSR achieves state-of-the-art performance on omnidirectional image super-resolution. The code is available at https://github.com/W-JG/BPOSR",
    "checked": true,
    "id": "161200d9229c0f0c6e72b61fcd84431d973718c5",
    "semantic_title": "omnidirectional image super-resolution via bi-projection fusion",
    "citation_count": 1,
    "authors": [
      "Jiangang Wang ",
      "Yuning Cui",
      "Yawen Li",
      "Wenqi Ren",
      "Xiaochun Cao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28355": {
    "title": "Adaptive FSS: A Novel Few-Shot Segmentation Framework via Prototype Enhancement",
    "volume": "main",
    "abstract": "The Few-Shot Segmentation (FSS) aims to accomplish the novel class segmentation task with a few annotated images. Current FSS research based on meta-learning focuses on designing a complex interaction mechanism between the query and support feature. However, unlike humans who can rapidly learn new things from limited samples, the existing approach relies solely on fixed feature matching to tackle new tasks, lacking adaptability. In this paper, we propose a novel framework based on the adapter mechanism, namely Adaptive FSS, which can efficiently adapt the existing FSS model to the novel classes. In detail, we design the Prototype Adaptive Module (PAM), which utilizes accurate category information provided by the support set to derive class prototypes, enhancing class-specific information in the multi-stage representation. In addition, our approach is compatible with diverse FSS methods with different backbones by simply inserting PAM between the layers of the encoder. Experiments demonstrate that our method effectively improves the performance of the FSS models (e.g., MSANet, HDMNet, FPTrans, and DCAMA) and achieves new state-of-the-art (SOTA) results (i.e., 72.4% and 79.1% mIoU on PASCAL-5i 1-shot and 5-shot settings, 52.7% and 60.0% mIoU on COCO-20i 1-shot and 5-shot settings). Our code is available at https://github.com/jingw193/AdaptiveFSS",
    "checked": true,
    "id": "cbf702d3245231e56c8d5fd8def136233b23fd95",
    "semantic_title": "adaptive fss: a novel few-shot segmentation framework via prototype enhancement",
    "citation_count": 2,
    "authors": [
      "Jing Wang",
      "Jiangyun Li",
      "Chen Chen",
      "Yisi Zhang",
      "Haoran Shen",
      "Tianxiang Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28356": {
    "title": "PointAttN: You Only Need Attention for Point Cloud Completion",
    "volume": "main",
    "abstract": "Point cloud completion referring to completing 3D shapes from partial 3D point clouds is a fundamental problem for 3D point cloud analysis tasks. Benefiting from the development of deep neural networks, researches on point cloud completion have made great progress in recent years. However, the explicit local region partition like kNNs involved in existing methods makes them sensitive to the density distribution of point clouds. Moreover, it serves limited receptive fields that prevent capturing features from long-range context information. To solve the problems, we leverage the cross-attention and self-attention mechanisms to design novel neural network for point cloud completion with implicit local region partition. Two basic units Geometric Details Perception (GDP) and Self-Feature Augment (SFA) are proposed to establish the structural relationships directly among points in a simple yet effective way via attention mechanism. Then based on GDP and SFA, we construct a new framework with popular encoder-decoder architecture for point cloud completion. The proposed framework, namely PointAttN, is simple, neat and effective, which can precisely capture the structural information of 3D shapes and predict complete point clouds with detailed geometry. Experimental results demonstrate that our PointAttN outperforms state-of-the-art methods on multiple challenging benchmarks. Code is available at: https://github.com/ohhhyeahhh/PointAttN",
    "checked": true,
    "id": "0684ebd7bc08e37c88d8aab6d5d2d708dfd2ec55",
    "semantic_title": "pointattn: you only need attention for point cloud completion",
    "citation_count": 20,
    "authors": [
      "Jun Wang",
      "Ying Cui",
      "Dongyan Guo",
      "Junxia Li",
      "Qingshan Liu",
      "Chunhua Shen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28357": {
    "title": "EarthVQA: Towards Queryable Earth via Relational Reasoning-Based Remote Sensing Visual Question Answering",
    "volume": "main",
    "abstract": "Earth vision research typically focuses on extracting geospatial object locations and categories but neglects the exploration of relations between objects and comprehensive reasoning. Based on city planning needs, we develop a multi-modal multi-task VQA dataset (EarthVQA) to advance relational reasoning-based judging, counting, and comprehensive analysis. The EarthVQA dataset contains 6000 images, corresponding semantic masks, and 208,593 QA pairs with urban and rural governance requirements embedded. As objects are the basis for complex relational reasoning, we propose a Semantic OBject Awareness framework (SOBA) to advance VQA in an object-centric way. To preserve refined spatial locations and semantics, SOBA leverages a segmentation network for object semantics generation. The object-guided attention aggregates object interior features via pseudo masks, and bidirectional cross-attention further models object external relations hierarchically. To optimize object counting, we propose a numerical difference loss that dynamically adds difference penalties, unifying the classification and regression tasks. Experimental results show that SOBA outperforms both advanced general and remote sensing methods. We believe this dataset and framework provide a strong benchmark for Earth vision's complex analysis. The project page is at https://Junjue-Wang.github.io/homepage/EarthVQA",
    "checked": true,
    "id": "f081cc7d09246bbdfb0bdae0e6191a42aa918c92",
    "semantic_title": "earthvqa: towards queryable earth via relational reasoning-based remote sensing visual question answering",
    "citation_count": 3,
    "authors": [
      "Junjue Wang",
      "Zhuo Zheng",
      "Zihang Chen",
      "Ailong Ma",
      "Yanfei Zhong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28358": {
    "title": "Semi-supervised Class-Agnostic Motion Prediction with Pseudo Label Regeneration and BEVMix",
    "volume": "main",
    "abstract": "Class-agnostic motion prediction methods aim to comprehend motion within open-world scenarios, holding significance for autonomous driving systems. However, training a high-performance model in a fully-supervised manner always requires substantial amounts of manually annotated data, which can be both expensive and time-consuming to obtain. To address this challenge, our study explores the potential of semi-supervised learning (SSL) for class-agnostic motion prediction. Our SSL framework adopts a consistency-based self-training paradigm, enabling the model to learn from unlabeled data by generating pseudo labels through test-time inference. To improve the quality of pseudo labels, we propose a novel motion selection and re-generation module. This module effectively selects reliable pseudo labels and re-generates unreliable ones. Furthermore, we propose two data augmentation strategies: temporal sampling and BEVMix. These strategies facilitate consistency regularization in SSL. Experiments conducted on nuScenes demonstrate that our SSL method can surpass the self-supervised approach by a large margin by utilizing only a tiny fraction of labeled data. Furthermore, our method exhibits comparable performance to weakly and some fully supervised methods. These results highlight the ability of our method to strike a favorable balance between annotation costs and performance. Code will be available at https://github.com/kwwcv/SSMP",
    "checked": true,
    "id": "74ca051b676466c16a97ee9e017272f46f5654bf",
    "semantic_title": "semi-supervised class-agnostic motion prediction with pseudo label regeneration and bevmix",
    "citation_count": 3,
    "authors": [
      "Kewei Wang",
      "Yizheng Wu",
      "Zhiyu Pan",
      "Xingyi Li",
      "Ke Xian",
      "Zhe Wang",
      "Zhiguo Cao",
      "Guosheng Lin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28359": {
    "title": "Multi-Domain Incremental Learning for Face Presentation Attack Detection",
    "volume": "main",
    "abstract": "Previous face Presentation Attack Detection (PAD) methods aim to improve the effectiveness of cross-domain tasks. However, in real-world scenarios, the original training data of the pre-trained model is not available due to data privacy or other reasons. Under these constraints, general methods for fine-tuning single-target domain data may lose previously learned knowledge, leading to a catastrophic forgetting problem. To address these issues, we propose a multi-domain incremental learning (MDIL) method for PAD, which not only learns knowledge well from the new domain but also maintains the performance of previous domains stably. Specifically, we propose an adaptive domain-specific experts (ADE) framework based on the vision transformer to preserve the discriminability of previous domains. Furthermore, an asymmetric classifier is designed to keep the output distribution of different classifiers consistent, thereby improving the generalization ability. Extensive experiments show that our proposed method achieves state-of-the-art performance compared to prior methods of incremental learning. Excitingly, under more stringent setting conditions, our method approximates or even outperforms the DA/DG-based methods",
    "checked": true,
    "id": "74b7ea037698f9e7db8f4ac56f202fa5f3ba2f9a",
    "semantic_title": "multi-domain incremental learning for face presentation attack detection",
    "citation_count": 4,
    "authors": [
      "Keyao Wang",
      "Guosheng Zhang",
      "Haixiao Yue",
      "Ajian Liu",
      "Gang Zhang",
      "Haocheng Feng",
      "Junyu Han",
      "Errui Ding",
      "Jingdong Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28360": {
    "title": "AltNeRF: Learning Robust Neural Radiance Field via Alternating Depth-Pose Optimization",
    "volume": "main",
    "abstract": "Neural Radiance Fields (NeRF) have shown promise in generating realistic novel views from sparse scene images. However, existing NeRF approaches often encounter challenges due to the lack of explicit 3D supervision and imprecise camera poses, resulting in suboptimal outcomes. To tackle these issues, we propose AltNeRF---a novel framework designed to create resilient NeRF representations using self-supervised monocular depth estimation (SMDE) from monocular videos, without relying on known camera poses. SMDE in AltNeRF masterfully learns depth and pose priors to regulate NeRF training. The depth prior enriches NeRF's capacity for precise scene geometry depiction, while the pose prior provides a robust starting point for subsequent pose refinement. Moreover, we introduce an alternating algorithm that harmoniously melds NeRF outputs into SMDE through a consistence-driven mechanism, thus enhancing the integrity of depth priors. This alternation empowers AltNeRF to progressively refine NeRF representations, yielding the synthesis of realistic novel views. Extensive experiments showcase the compelling capabilities of AltNeRF in generating high-fidelity and robust novel views that closely resemble reality",
    "checked": true,
    "id": "b5e4d0c81b2ae3436483d873010c98d5aa789d81",
    "semantic_title": "altnerf: learning robust neural radiance field via alternating depth-pose optimization",
    "citation_count": 1,
    "authors": [
      "Kun Wang",
      "Zhiqiang Yan",
      "Huang Tian",
      "Zhenyu Zhang",
      "Xiang Li",
      "Jun Li",
      "Jian Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28361": {
    "title": "A Multimodal, Multi-Task Adapting Framework for Video Action Recognition",
    "volume": "main",
    "abstract": "Recently, the rise of large-scale vision-language pretrained models like CLIP, coupled with the technology of Parameter-Efficient FineTuning (PEFT), has captured substantial attraction in video action recognition. Nevertheless, prevailing approaches tend to prioritize strong supervised performance at the expense of compromising the models' generalization capabilities during transfer. In this paper, we introduce a novel Multimodal, Multi-task CLIP adapting framework named M2-CLIP to address these challenges, preserving both high supervised performance and robust transferability. Firstly, to enhance the individual modality architectures, we introduce multimodal adapters to both the visual and text branches. Specifically, we design a novel visual TED-Adapter, that performs global Temporal Enhancement and local temporal Difference modeling to improve the temporal representation capabilities of the visual encoder. Moreover, we adopt text encoder adapters to strengthen the learning of semantic label information. Secondly, we design a multi-task decoder with a rich set of supervisory signals, including the original contrastive learning head, a cross-modal classification head, a cross-modal masked language modeling head, and a visual classification head. This multi-task decoder adeptly satisfies the need for strong supervised performance within a multimodal framework. Experimental results validate the efficacy of our approach, demonstrating exceptional performance in supervised learning while maintaining strong generalization in zero-shot scenarios",
    "checked": false,
    "id": "64fa029d31186d63ada4f7d34e518017d41737b9",
    "semantic_title": "m2-clip: a multimodal, multi-task adapting framework for video action recognition",
    "citation_count": 0,
    "authors": [
      "Mengmeng Wang",
      "Jiazheng Xing",
      "Boyuan Jiang",
      "Jun Chen",
      "Jianbiao Mei",
      "Xingxing Zuo",
      "Guang Dai",
      "Jingdong Wang",
      "Yong Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28362": {
    "title": "msLPCC: A Multimodal-Driven Scalable Framework for Deep LiDAR Point Cloud Compression",
    "volume": "main",
    "abstract": "LiDAR sensors are widely used in autonomous driving, and the growing storage and transmission demands have made LiDAR point cloud compression (LPCC) a hot research topic. To address the challenges posed by the large-scale and uneven-distribution (spatial and categorical) of LiDAR point data, this paper presents a new multimodal-driven scalable LPCC framework. For the large-scale challenge, we decouple the original LiDAR data into multi-layer point subsets, compress and transmit each layer separately, so as to ensure the reconstruction quality requirement under different scenarios. For the uneven-distribution challenge, we extract, align, and fuse heterologous feature representations, including point modality with position information, depth modality with spatial distance information, and segmentation modality with category information. Extensive experimental results on the benchmark SemanticKITTI database validate that our method outperforms 14 recent representative LPCC methods",
    "checked": true,
    "id": "1c70efbcfea61beb47bee20c50d753e2db0fd303",
    "semantic_title": "mslpcc: a multimodal-driven scalable framework for deep lidar point cloud compression",
    "citation_count": 0,
    "authors": [
      "Miaohui Wang",
      "Runnan Huang",
      "Hengjin Dong",
      "Di Lin",
      "Yun Song",
      "Wuyuan Xie"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28363": {
    "title": "Cycle-Consistency Learning for Captioning and Grounding",
    "volume": "main",
    "abstract": "We present that visual grounding and image captioning, which perform as two mutually inverse processes, can be bridged together for collaborative training by careful designs. By consolidating this idea, we introduce CyCo, a cyclic-consistent learning framework to ameliorate the independent training pipelines of visual grounding and image captioning. The proposed framework (1) allows the semi-weakly supervised training of visual grounding; (2) improves the performance of fully supervised visual grounding; (3) yields a general captioning model that can describe arbitrary image regions. Extensive experiments show that our fully supervised grounding model achieves state-of-the-art performance, and the semi-weakly supervised one also exhibits competitive performance compared to the fully supervised counterparts. Our image captioning model has the capability to freely describe image regions and meanwhile shows impressive performance on prevalent captioning benchmarks",
    "checked": true,
    "id": "2be74ac5974847bd295441bf2067df26a08d6700",
    "semantic_title": "cycle-consistency learning for captioning and grounding",
    "citation_count": 0,
    "authors": [
      "Ning Wang",
      "Jiajun Deng",
      "Mingbo Jia"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28364": {
    "title": "Compositional Text-to-Image Synthesis with Attention Map Control of Diffusion Models",
    "volume": "main",
    "abstract": "Recent text-to-image (T2I) diffusion models show outstanding performance in generating high-quality images conditioned on textual prompts. However, they fail to semantically align the generated images with the prompts due to their limited compositional capabilities, leading to attribute leakage, entity leakage, and missing entities. In this paper, we propose a novel attention mask control strategy based on predicted object boxes to address these issues. In particular, we first train a BoxNet to predict a box for each entity that possesses the attribute specified in the prompt. Then, depending on the predicted boxes, a unique mask control is applied to the cross- and self-attention maps. Our approach produces a more semantically accurate synthesis by constraining the attention regions of each token in the prompt to the image. In addition, the proposed method is straightforward and effective and can be readily integrated into existing cross-attention-based T2I generators. We compare our approach to competing methods and demonstrate that it can faithfully convey the semantics of the original text to the generated content and achieve high availability as a ready-to-use plugin. Please refer to https://github.com/OPPO-Mente-Lab/attention-mask-control",
    "checked": true,
    "id": "3fd516043d29e683d66c6d51aa641a3a9fdef8e0",
    "semantic_title": "compositional text-to-image synthesis with attention map control of diffusion models",
    "citation_count": 35,
    "authors": [
      "Ruichen Wang",
      "Zekang Chen",
      "Chen Chen",
      "Jian Ma",
      "Haonan Lu",
      "Xiaodong Lin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28365": {
    "title": "AGS: Affordable and Generalizable Substitute Training for Transferable Adversarial Attack",
    "volume": "main",
    "abstract": "In practical black-box attack scenarios, most of the existing transfer-based attacks employ pretrained models (e.g. ResNet50) as the substitute models. Unfortunately, these substitute models are not always appropriate for transfer-based attacks. Firstly, these models are usually trained on a largescale annotated dataset, which is extremely expensive and time-consuming to construct. Secondly, the primary goal of these models is to perform a specific task, such as image classification, which is not developed for adversarial attacks. To tackle the above issues, i.e., high cost and over-fitting on taskspecific models, we propose an Affordable and Generalizable Substitute (AGS) training framework tailored for transferbased adversarial attack. Specifically, we train the substitute model from scratch by our proposed adversary-centric constrastive learning. This proposed learning mechanism introduces another sample with slight adversarial perturbations as an additional positive view of the input image, and then encourages the adversarial view and two benign views to interact comprehensively with each other. To further boost the generalizability of the substitute model, we propose adversarial invariant learning to maintain the representations of the adversarial example invariants under augmentations with various strengths. Our AGS model can be trained solely with unlabeled and out-of domain data and avoid overfitting to any task-specific models, because of its inherently self-supervised nature. Extensive experiments demonstrate that our AGS achieves comparable or superior performance compared to substitute models pretrained on the complete ImageNet training set, when executing attacks across a diverse range of target models, including ViTs, robustly trained models, object detection and segmentation models. Our source codes are available at https://github.com/lwmming/AGS",
    "checked": true,
    "id": "0df9394e0b8e38d163211731f79ad2137b6825d7",
    "semantic_title": "ags: affordable and generalizable substitute training for transferable adversarial attack",
    "citation_count": 0,
    "authors": [
      "Ruikui Wang",
      "Yuanfang Guo",
      "Yunhong Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28366": {
    "title": "DocNLC: A Document Image Enhancement Framework with Normalized and Latent Contrastive Representation for Multiple Degradations",
    "volume": "main",
    "abstract": "Document Image Enhancement (DIE) remains challenging due to the prevalence of multiple degradations in document images captured by cameras. In this paper, we respond an interesting question: can the performance of pre-trained models and downstream DIE models be improved if they are bootstrapped using different degradation types of the same semantic samples and their high-dimensional features with ambiguous inter-class distance? To this end, we propose an effective contrastive learning paradigm for DIE — a Document image enhancement framework with Normalization and Latent Contrast (DocNLC). While existing DIE methods focus on eliminating one type of degradation, DocNLC considers the relationship between different types of degradation while utilizing both direct and latent contrasts to constrain content consistency, thus achieving a unified treatment of multiple types of degradation. Specifically, we devise a latent contrastive learning module to enforce explicit decorrelation of the normalized representations of different degradation types and to minimize the redundancy between them. Comprehensive experiments show that our method outperforms state-of-the-art DIE models in both pre-training and fine-tuning stages on four publicly available independent datasets. In addition, we discuss the potential benefits of DocNLC for downstream tasks. Our code is released at https://github.com/RylonW/DocNLC",
    "checked": true,
    "id": "b784e3138cb5b14d073e9a0be189bef0b3e67de0",
    "semantic_title": "docnlc: a document image enhancement framework with normalized and latent contrastive representation for multiple degradations",
    "citation_count": 0,
    "authors": [
      "Ruilu Wang",
      "Yang Xue",
      "Lianwen Jin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28367": {
    "title": "Towards Evidential and Class Separable Open Set Object Detection",
    "volume": "main",
    "abstract": "Detecting in open-world scenarios poses a formidable challenge for models intended for real-world deployment. The advanced closed set object detectors achieve impressive performance under the closed set setting, but often produce overconfident misprediction on unknown objects due to the lack of supervision. In this paper, we propose a novel Evidential Object Detector (EOD) to formulate the Open Set Object Detection (OSOD) problem from the perspective of Evidential Deep Learning (EDL) theory, which quantifies classification uncertainty by placing the Dirichlet Prior over the categorical distribution parameters. The task-specific customized evidential framework, equipped with meticulously designed model architecture and loss function, effectively bridges the gap between EDL theory and detection tasks. Moreover, we utilize contrastive learning as an implicit means of evidential regularization and to encourage the class separation in the latent space. Alongside, we innovatively model the background uncertainty to further improve the unknown discovery ability. Extensive experiments on benchmark datasets demonstrate the outperformance of the proposed method over existing ones",
    "checked": true,
    "id": "c36e20ef2460691f420ea5855e9967fab02dba93",
    "semantic_title": "towards evidential and class separable open set object detection",
    "citation_count": 0,
    "authors": [
      "Ruofan Wang",
      "Rui-Wei Zhao",
      "Xiaobo Zhang",
      "Rui Feng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28368": {
    "title": "Suppressing Uncertainty in Gaze Estimation",
    "volume": "main",
    "abstract": "Uncertainty in gaze estimation manifests in two aspects: 1) low-quality images caused by occlusion, blurriness, inconsistent eye movements, or even non-face images; 2) uncorrected labels resulting from the misalignment between the labeled and actual gaze points during the annotation process. Allowing these uncertainties to participate in training hinders the improvement of gaze estimation. To tackle these challenges, in this paper, we propose an effective solution, named Suppressing Uncertainty in Gaze Estimation (SUGE), which introduces a novel triplet-label consistency measurement to estimate and reduce the uncertainties. Specifically, for each training sample, we propose to estimate a novel ``neighboring label'' calculated by a linearly weighted projection from the neighbors to capture the similarity relationship between image features and their corresponding labels, which can be incorporated with the predicted pseudo label and ground-truth label for uncertainty estimation. By modeling such triplet-label consistency, we can largely reduce the negative effects of unqualified images and wrong labels through our designed sample weighting and label correction strategies. Experimental results on the gaze estimation benchmarks indicate that our proposed SUGE achieves state-of-the-art performance",
    "checked": true,
    "id": "d01582918a41c63cb47c3aeab6805b52f762681c",
    "semantic_title": "suppressing uncertainty in gaze estimation",
    "citation_count": 0,
    "authors": [
      "Shijing Wang",
      "Yaping Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28369": {
    "title": "What Effects the Generalization in Visual Reinforcement Learning: Policy Consistency with Truncated Return Prediction",
    "volume": "main",
    "abstract": "In visual Reinforcement Learning (RL), the challenge of generalization to new environments is paramount. This study pioneers a theoretical analysis of visual RL generalization, establishing an upper bound on the generalization objective, encompassing policy divergence and Bellman error components. Motivated by this analysis, we propose maintaining the cross-domain consistency for each policy in the policy space, which can reduce the divergence of the learned policy during the test. In practice, we introduce the Truncated Return Prediction (TRP) task, promoting cross-domain policy consistency by predicting truncated returns of historical trajectories. Moreover, we also propose a Transformer-based predictor for this auxiliary task. Extensive experiments on DeepMind Control Suite and Robotic Manipulation tasks demonstrate that TRP achieves state-of-the-art generalization performance. We further demonstrate that TRP outperforms previous methods in terms of sample efficiency during training",
    "checked": true,
    "id": "c1cc76ec4bd03ae1c9967498f22c2c54bc10d654",
    "semantic_title": "what effects the generalization in visual reinforcement learning: policy consistency with truncated return prediction",
    "citation_count": 2,
    "authors": [
      "Shuo Wang",
      "Zhihao Wu",
      "Xiaobo Hu",
      "Jinwen Wang",
      "Youfang Lin",
      "Kai Lv"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28370": {
    "title": "DeepAccident: A Motion and Accident Prediction Benchmark for V2X Autonomous Driving",
    "volume": "main",
    "abstract": "Safety is the primary priority of autonomous driving. Nevertheless, no published dataset currently supports the direct and explainable safety evaluation for autonomous driving. In this work, we propose DeepAccident, a large-scale dataset generated via a realistic simulator containing diverse accident scenarios that frequently occur in real-world driving. The proposed DeepAccident dataset includes 57K annotated frames and 285K annotated samples, approximately 7 times more than the large-scale nuScenes dataset with 40k annotated samples. In addition, we propose a new task, end-to-end motion and accident prediction, which can be used to directly evaluate the accident prediction ability for different autonomous driving algorithms. Furthermore, for each scenario, we set four vehicles along with one infrastructure to record data, thus providing diverse viewpoints for accident scenarios and enabling V2X (vehicle-to-everything) research on perception and prediction tasks. Finally, we present a baseline V2X model named V2XFormer that demonstrates superior performance for motion and accident prediction and 3D object detection compared to the single-vehicle model",
    "checked": true,
    "id": "9e5b8170110cba6291797d8524abf007dcbc226c",
    "semantic_title": "deepaccident: a motion and accident prediction benchmark for v2x autonomous driving",
    "citation_count": 28,
    "authors": [
      "Tianqi Wang",
      "Sukmin Kim",
      "Ji Wenxuan",
      "Enze Xie",
      "Chongjian Ge",
      "Junsong Chen",
      "Zhenguo Li",
      "Ping Luo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28371": {
    "title": "Semantic-Guided Novel Category Discovery",
    "volume": "main",
    "abstract": "The Novel Category Discovery problem aims to cluster an unlabeled set with the help of a labeled set consisting of disjoint but related classes. However, existing models treat class names as discrete one-hot labels and ignore the semantic understanding of these classes. In this paper, we propose a new setting named Semantic-guided Novel Category Discovery (SNCD), which requires the model to not only cluster the unlabeled images but also semantically recognize these images based on a set of their class names. The first challenge we confront pertains to effectively leveraging the class names of unlabeled images, given the inherent gap between the visual and linguistic domains. To address this issue, we incorporate a semantic-aware recognition mechanism. This is achieved by constructing dynamic class-wise visual prototypes as well as a semantic similarity matrix that enables the projection of visual features into the semantic space. The second challenge originates from the granularity disparity between the classification and clustering tasks. To deal with this, we develop a semantic-aware clustering process to facilitate the exchange of knowledge between the two tasks. Through extensive experiments, we demonstrate the mutual benefits of the recognition and clustering tasks, which can be jointly optimized. Experimental results on multiple datasets confirm the effectiveness of our proposed method. Our code is available at https://github.com/wang-weishuai/Semantic-guided-NCD",
    "checked": true,
    "id": "d7c0e5cad9a36baf70241786dcb3963b5e9235ab",
    "semantic_title": "semantic-guided novel category discovery",
    "citation_count": 0,
    "authors": [
      "Weishuai Wang",
      "Ting Lei",
      "Qingchao Chen",
      "Yang Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28372": {
    "title": "HARDVS: Revisiting Human Activity Recognition with Dynamic Vision Sensors",
    "volume": "main",
    "abstract": "The main streams of human activity recognition (HAR) algorithms are developed based on RGB cameras which usually suffer from illumination, fast motion, privacy preservation, and large energy consumption. Meanwhile, the biologically inspired event cameras attracted great interest due to their unique features, such as high dynamic range, dense temporal but sparse spatial resolution, low latency, low power, etc. As it is a newly arising sensor, even there is no realistic large-scale dataset for HAR. Considering its great practical value, in this paper, we propose a large-scale benchmark dataset to bridge this gap, termed HARDVS, which contains 300 categories and more than 100K event sequences. We evaluate and report the performance of multiple popular HAR algorithms, which provide extensive baselines for future works to compare. More importantly, we propose a novel spatial-temporal feature learning and fusion framework, termed ESTF, for event stream based human activity recognition. It first projects the event streams into spatial and temporal embeddings using StemNet, then, encodes and fuses the dual-view representations using Transformer networks. Finally, the dual features are concatenated and fed into a classification head for activity prediction. Extensive experiments on multiple datasets fully validated the effectiveness of our model. Both the dataset and source code will be released at https://github.com/Event-AHU/HARDVS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Wang",
      "Zongzhen Wu",
      "Bo Jiang",
      "Zhimin Bao",
      "Lin Zhu",
      "Guoqi Li",
      "Yaowei Wang",
      "Yonghong Tian"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28373": {
    "title": "Structural Information Guided Multimodal Pre-training for Vehicle-Centric Perception",
    "volume": "main",
    "abstract": "Understanding vehicles in images is important for various applications such as intelligent transportation and self-driving system. Existing vehicle-centric works typically pre-train models on large-scale classification datasets and then fine-tune them for specific downstream tasks. However, they neglect the specific characteristics of vehicle perception in different tasks and might thus lead to sub-optimal performance. To address this issue, we propose a novel vehicle-centric pre-training framework called VehicleMAE, which incorporates the structural information including the spatial structure from vehicle profile information and the semantic structure from informative high-level natural language descriptions for effective masked vehicle appearance reconstruction. To be specific, we explicitly extract the sketch lines of vehicles as a form of the spatial structure to guide vehicle reconstruction. The more comprehensive knowledge distilled from the CLIP big model based on the similarity between the paired/unpaired vehicle image-text sample is further taken into consideration to help achieve a better understanding of vehicles. A large-scale dataset is built to pre-train our model, termed Autobot1M, which contains about 1M vehicle images and 12693 text information. Extensive experiments on four vehicle-based downstream tasks fully validated the effectiveness of our VehicleMAE. The source code and pre-trained models will be released at https://github.com/Event-AHU/VehicleMAE",
    "checked": true,
    "id": "6b204642e9a231932b9e72b2e401d7a5bc17a51e",
    "semantic_title": "structural information guided multimodal pre-training for vehicle-centric perception",
    "citation_count": 1,
    "authors": [
      "Xiao Wang",
      "Wentao Wu",
      "Chenglong Li",
      "Zhicheng Zhao",
      "Zhe Chen",
      "Yukai Shi",
      "Jin Tang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28374": {
    "title": "ICAR: Image-Based Complementary Auto Reasoning",
    "volume": "main",
    "abstract": "Scene-aware Complementary Item Retrieval (CIR) is a challenging task which requires to generate a set of compatible items across domains. Due to the subjectivity, it is difficult to set up a rigorous standard for both data collection and learning objectives. To address this challenging task, we propose a visual compatibility concept, composed of similarity (resembling in color, geometry, texture, and etc.) and complementarity (different items like table vs chair completing a group). Based on this notion, we propose a compatibility learning framework, a category-aware Flexible Bidirectional Transformer (FBT), for visual ``scene-based set compatibility reasoning'' with the cross-domain visual similarity input and auto-regressive complementary item generation. We introduce a ``Flexible Bidirectional Transformer (FBT),'' consisting of an encoder with flexible masking, a category prediction arm, and an auto-regressive visual embedding prediction arm. And the inputs for FBT are cross-domain visual similarity invariant embeddings, making this framework quite generalizable. Furthermore, our proposed FBT model learns the inter-object compatibility from a large set of scene images in a self-supervised way. Compared with the SOTA methods, this approach achieves up to 5.3% and 9.6% in FITB score and 22.3% and 31.8% SFID improvement on fashion and furniture, respectively",
    "checked": true,
    "id": "98ed96c1f25974f835c03e3a1acd01c8451186df",
    "semantic_title": "icar: image-based complementary auto reasoning",
    "citation_count": 0,
    "authors": [
      "Xijun Wang",
      "Anqi Liang",
      "Junbang Liang",
      "Ming Lin",
      "Yu Lou",
      "Shan Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28375": {
    "title": "GCNext: Towards the Unity of Graph Convolutions for Human Motion Prediction",
    "volume": "main",
    "abstract": "The past few years has witnessed the dominance of Graph Convolutional Networks (GCNs) over human motion prediction. Various styles of graph convolutions have been proposed, with each one meticulously designed and incorporated into a carefully-crafted network architecture. This paper breaks the limits of existing knowledge by proposing Universal Graph Convolution (UniGC), a novel graph convolution concept that re-conceptualizes different graph convolutions as its special cases. Leveraging UniGC on network-level, we propose GCNext, a novel GCN-building paradigm that dynamically determines the best-fitting graph convolutions both sample-wise and layer-wise. GCNext offers multiple use cases, including training a new GCN from scratch or refining a preexisting GCN. Experiments on Human3.6M, AMASS, and 3DPW datasets show that, by incorporating unique module-to-network designs, GCNext yields up to 9x lower computational cost than existing GCN methods, on top of achieving state-of-the-art performance. Our code is available at https://github.com/BradleyWang0416/GCNext",
    "checked": true,
    "id": "9604c4275b0f2be260842e191287c64ad8f45a09",
    "semantic_title": "gcnext: towards the unity of graph convolutions for human motion prediction",
    "citation_count": 3,
    "authors": [
      "Xinshun Wang",
      "Qiongjie Cui",
      "Chen Chen",
      "Mengyuan Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28376": {
    "title": "CL2CM: Improving Cross-Lingual Cross-Modal Retrieval via Cross-Lingual Knowledge Transfer",
    "volume": "main",
    "abstract": "Cross-lingual cross-modal retrieval has garnered increasing attention recently, which aims to achieve the alignment between vision and target language (V-T) without using any annotated V-T data pairs. Current methods employ machine translation (MT) to construct pseudo-parallel data pairs, which are then used to learn a multi-lingual and multi-modal embedding space that aligns visual and target-language representations. However, the large heterogeneous gap between vision and text, along with the noise present in target language translations, poses significant challenges in effectively aligning their representations. To address these challenges, we propose a general framework, Cross-Lingual to Cross-Modal (CL2CM), which improves the alignment between vision and target language using cross-lingual transfer. This approach allows us to fully leverage the merits of multi-lingual pre-trained models (e.g., mBERT) and the benefits of the same modality structure, i.e., smaller gap, to provide reliable and comprehensive semantic correspondence (knowledge) for the cross-modal network. We evaluate our proposed approach on two multilingual image-text datasets, Multi30K and MSCOCO, and one video-text dataset, VATEX. The results clearly demonstrate the effectiveness of our proposed method and its high potential for large-scale retrieval",
    "checked": true,
    "id": "ad8b899d359240fca7c623cb0c6b8b03309e48fa",
    "semantic_title": "cl2cm: improving cross-lingual cross-modal retrieval via cross-lingual knowledge transfer",
    "citation_count": 0,
    "authors": [
      "Yabing Wang",
      "Fan Wang",
      "Jianfeng Dong",
      "Hao Luo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28377": {
    "title": "OSFFNet: Omni-Stage Feature Fusion Network for Lightweight Image Super-Resolution",
    "volume": "main",
    "abstract": "Recently, several lightweight methods have been proposed to implement single-image super-resolution (SISR) on resource-constrained devices. However, these methods primarily focus on simplifying network structures without the full utilization of shallow features. The fact remains that shallow features encompass crucial details for the super-resolution task, including edges, textures, and colors. Therefore, developing a novel architecture that can effectively integrate features from different levels and capitalize on their mutual complementarity is necessary. We first analyze the relationship between multi-stage features and the restoration tasks in a classic lightweight SR method. Based on these observations, we propose an Omni-Stage Feature Fusion (OSFF) architecture, which incorporates Original Image Stacked Initialisation, Shallow Feature Global Connection, and Multi-Receptive Field Dynamic Fusion. An Attention-Enhanced Feature Distillation module is also designed to enhance the model performance. Finally, leveraging these contributions, we construct an Omni-Stage Feature Fusion Network (OSFFNet). Through extensive experiments on various benchmark datasets, the proposed model outperforms state-of-the-art methods. Notably, it achieves a 0.26dB PSNR improvement over the second-best method for x2 SR on the Urban100 dataset",
    "checked": true,
    "id": "caed01f239bb764137276e8e664d10b6e3757573",
    "semantic_title": "osffnet: omni-stage feature fusion network for lightweight image super-resolution",
    "citation_count": 1,
    "authors": [
      "Yang Wang",
      "Tao Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28378": {
    "title": "Prompting Segmentation with Sound Is Generalizable Audio-Visual Source Localizer",
    "volume": "main",
    "abstract": "Never having seen an object and heard its sound simultaneously, can the model still accurately localize its visual position from the input audio? In this work, we concentrate on the Audio-Visual Localization and Segmentation tasks but under the demanding zero-shot and few-shot scenarios. To achieve this goal, different from existing approaches that mostly employ the encoder-fusion-decoder paradigm to decode localization information from the fused audio-visual feature, we introduce the encoder-prompt-decoder paradigm, aiming to better fit the data scarcity and varying data distribution dilemmas with the help of abundant knowledge from pre-trained models. Specifically, we first propose to construct a Semantic-aware Audio Prompt (SAP) to help the visual foundation model focus on sounding objects, meanwhile, the semantic gap between the visual and audio modalities is also encouraged to shrink. Then, we develop a Correlation Adapter (ColA) to keep minimal training efforts as well as maintain adequate knowledge of the visual foundation model. By equipping with these means, extensive experiments demonstrate that this new paradigm outperforms other fusion-based methods in both the unseen class and cross-dataset settings. We hope that our work can further promote the generalization study of Audio-Visual Localization and Segmentation in practical application scenarios. Project page: https://github.com/GeWu-Lab/Generalizable-Audio-Visual-Segmentation",
    "checked": true,
    "id": "b6f2bf38db13fe0d9a9f6387bd4b8dfab7e5d040",
    "semantic_title": "prompting segmentation with sound is generalizable audio-visual source localizer",
    "citation_count": 7,
    "authors": [
      "Yaoting Wang",
      "Weisong Liu",
      "Guangyao Li",
      "Jian Ding",
      "Di Hu",
      "Xi Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28379": {
    "title": "Mask-Homo: Pseudo Plane Mask-Guided Unsupervised Multi-Homography Estimation",
    "volume": "main",
    "abstract": "Homography estimation is a fundamental problem in computer vision. Previous works mainly focus on estimating either a single homography, or multiple homographies based on mesh grid division of the image. In practical scenarios, single homography is inadequate and often leads to a compromised result for multiple planes; while mesh grid multi-homography damages the plane distribution of the scene, and does not fully address the restriction to use homography. In this work, we propose a novel semantics guided multi-homography estimation framework, Mask-Homo, to provide an explicit solution to the multi-plane depth disparity problem. First, a pseudo plane mask generation module is designed to obtain multiple correlated regions that follow the plane distribution of the scene. Then, multiple local homography transformations, each of which aligns a correlated region precisely, are predicted and corresponding warped images are fused to obtain the final result. Furthermore, a new metric, Mask-PSNR, is proposed for more comprehensive evaluation of alignment. Extensive experiments are conducted to verify the effectiveness of the proposed method. Our code is available at https://github.com/SAITPublic/MaskHomo",
    "checked": true,
    "id": "8a301168faf69d2fb95f00f827393465bdd8d27d",
    "semantic_title": "mask-homo: pseudo plane mask-guided unsupervised multi-homography estimation",
    "citation_count": 1,
    "authors": [
      "Yasi Wang",
      "Hong Liu",
      "Chao Zhang",
      "Lu Xu",
      "Qiang Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28380": {
    "title": "PointPatchMix: Point Cloud Mixing with Patch Scoring",
    "volume": "main",
    "abstract": "Data augmentation is an effective regularization strategy for mitigating overfitting in deep neural networks, and it plays a crucial role in 3D vision tasks, where the point cloud data is relatively limited. While mixing-based augmentation has shown promise for point clouds, previous methods mix point clouds either on block level or point level, which has constrained their ability to strike a balance between generating diverse training samples and preserving the local characteristics of point clouds. The significance of each part component of the point clouds has not been fully considered, as not all parts contribute equally to the classification task, and some parts may contain unimportant or redundant information. To overcome these challenges, we propose PointPatchMix, a novel approach that mixes point clouds at the patch level and integrates a patch scoring module to generate content-based targets for mixed point clouds. Our approach preserves local features at the patch level, while the patch scoring module assigns targets based on the content-based significance score from a pre-trained teacher model. We evaluate PointPatchMix on two benchmark datasets including ModelNet40 and ScanObjectNN, and demonstrate significant improvements over various baselines in both synthetic and real-world datasets, as well as few-shot settings. With Point-MAE as our baseline, our model surpasses previous methods by a significant margin. Furthermore, our approach shows strong generalization across various point cloud methods and enhances the robustness of the baseline model. Code is available at https://jiazewang.com/projects/pointpatchmix.html",
    "checked": true,
    "id": "f9aeaca9177dc9518ec3d1f8226f4df0b1192823",
    "semantic_title": "pointpatchmix: point cloud mixing with patch scoring",
    "citation_count": 3,
    "authors": [
      "Yi Wang",
      "Jiaze Wang",
      "Jinpeng Li",
      "Zixu Zhao",
      "Guangyong Chen",
      "Anfeng Liu",
      "Pheng Ann Heng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28381": {
    "title": "Data Distribution Distilled Generative Model for Generalized Zero-Shot Recognition",
    "volume": "main",
    "abstract": "In the realm of Zero-Shot Learning (ZSL), we address biases in Generalized Zero-Shot Learning (GZSL) models, which favor seen data. To counter this, we introduce an end-to-end generative GZSL framework called D3GZSL. This framework respects seen and synthesized unseen data as in-distribution and out-of-distribution data, respectively, for a more balanced model. D3GZSL comprises two core modules: in-distribution dual space distillation (ID2SD) and out-of-distribution batch distillation (O2DBD). ID2SD aligns teacher-student outcomes in embedding and label spaces, enhancing learning coherence. O2DBD introduces low-dimensional out-of-distribution representations per batch sample, capturing shared structures between seen and un seen categories. Our approach demonstrates its effectiveness across established GZSL benchmarks, seamlessly integrating into mainstream generative frameworks. Extensive experiments consistently showcase that D3GZSL elevates the performance of existing generative GZSL methods, under scoring its potential to refine zero-shot learning practices. The code is available at: https://github.com/PJBQ/D3GZSL.git",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yijie Wang",
      "Mingjian Hong",
      "Luwen Huangfu",
      "Sheng Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28382": {
    "title": "SiMA-Hand: Boosting 3D Hand-Mesh Reconstruction by Single-to-Multi-View Adaptation",
    "volume": "main",
    "abstract": "Estimating 3D hand mesh from RGB images is a longstanding track, in which occlusion is one of the most challenging problems. Existing attempts towards this task often fail when the occlusion dominates the image space. In this paper, we propose SiMA-Hand, aiming to boost the mesh reconstruction performance by Single-to-Multi-view Adaptation. First, we design a multi-view hand reconstructor to fuse information across multiple views by holistically adopting feature fusion at image, joint, and vertex levels. Then, we introduce a single-view hand reconstructor equipped with SiMA. Though taking only one view as input at inference, the shape and orientation features in the single-view reconstructor can be enriched by learning non-occluded knowledge from the extra views at training, enhancing the reconstruction precision on the occluded regions. We conduct experiments on the Dex-YCB and HanCo benchmarks with challenging object- and self-caused occlusion cases, manifesting that SiMA-Hand consistently achieves superior performance over the state of the arts. Code will be released on https://github.com/JoyboyWang/SiMA-Hand Pytorch",
    "checked": true,
    "id": "ca30047165b9b7e795cd50b8371d27fb764353a9",
    "semantic_title": "sima-hand: boosting 3d hand-mesh reconstruction by single-to-multi-view adaptation",
    "citation_count": 0,
    "authors": [
      "Yinqiao Wang",
      "Hao Xu",
      "Pheng Ann Heng",
      "Chi-Wing Fu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28383": {
    "title": "SQLdepth: Generalizable Self-Supervised Fine-Structured Monocular Depth Estimation",
    "volume": "main",
    "abstract": "Recently, self-supervised monocular depth estimation has gained popularity with numerous applications in autonomous driving and robotics. However, existing solutions primarily seek to estimate depth from immediate visual features, and struggle to recover fine-grained scene details. In this paper, we introduce SQLdepth, a novel approach that can effectively learn fine-grained scene structure priors from ego-motion. In SQLdepth, we propose a novel Self Query Layer (SQL) to build a self-cost volume and infer depth from it, rather than inferring depth from feature maps. We show that, the self-cost volume is an effective inductive bias for geometry learning, which implicitly models the single-frame scene geometry, with each slice of it indicating a relative distance map between points and objects in a latent space. Experimental results on KITTI and Cityscapes show that our method attains remarkable state-of-the-art performance, and showcases computational efficiency, reduced training complexity, and the ability to recover fine-grained scene details. Moreover, the self-matching-oriented relative distance querying in SQL improves the robustness and zero-shot generalization capability of SQLdepth. Code is available at https://github.com/hisfog/SfMNeXt-Impl",
    "checked": true,
    "id": "2d87ad25d0ad74c40bd9ad90e5e61b1764f592cf",
    "semantic_title": "sqldepth: generalizable self-supervised fine-structured monocular depth estimation",
    "citation_count": 10,
    "authors": [
      "Youhong Wang",
      "Yunji Liang",
      "Hao Xu",
      "Shaohui Jiao",
      "Hongkai Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28384": {
    "title": "H2GFormer: Horizontal-to-Global Voxel Transformer for 3D Semantic Scene Completion",
    "volume": "main",
    "abstract": "3D Semantic Scene Completion (SSC) has emerged as a novel task in vision-based holistic 3D scene understanding. Its objective is to densely predict the occupancy and category of each voxel in a 3D scene based on input from either LiDAR or images. Currently, many transformer-based semantic scene completion frameworks employ simple yet popular Cross-Attention and Self-Attention mechanisms to integrate and infer dense geometric and semantic information of voxels. However, they overlook the distinctions among voxels in the scene, especially in outdoor scenarios where the horizontal direction contains more variations. And voxels located at object boundaries and within the interior of objects exhibit varying levels of positional significance. To address this issue, we propose a transformer-based SSC framework called H2GFormer that incorporates a horizontal-to-global approach. This framework takes into full consideration the variations of voxels in the horizontal direction and the characteristics of voxels on object boundaries. We introduce a horizontal window-to-global attention (W2G) module that effectively fuses semantic information by first diffusing it horizontally from reliably visible voxels and then propagating the semantic understanding to global voxels, ensuring a more reliable fusion of semantic-aware features. Moreover, an Internal-External Position Awareness Loss (IoE-PALoss) is utilized during network training to emphasize the critical positions within the transition regions between objects. The experiments conducted on the SemanticKITTI dataset demonstrate that H2GFormer exhibits superior performance in both geometric and semantic completion tasks. Our code is available on https://github.com/Ryanwy1/H2GFormer",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Wang",
      "Chao Tong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28385": {
    "title": "Exploring Diverse Representations for Open Set Recognition",
    "volume": "main",
    "abstract": "Open set recognition (OSR) requires the model to classify samples that belong to closed sets while rejecting unknown samples during test. Currently, generative models often perform better than discriminative models in OSR, but recent studies show that generative models may be computationally infeasible or unstable on complex tasks. In this paper, we provide insights into OSR and find that learning supplementary representations can theoretically reduce the open space risk. Based on the analysis, we propose a new model, namely Multi-Expert Diverse Attention Fusion (MEDAF), that learns diverse representations in a discriminative way. MEDAF consists of multiple experts that are learned with an attention diversity regularization term to ensure the attention maps are mutually different. The logits learned by each expert are adaptively fused and used to identify the unknowns through the score function. We show that the differences in attention maps can lead to diverse representations so that the fused representations can well handle the open space. Extensive experiments are conducted on standard and OSR large-scale benchmarks. Results show that the proposed discriminative method can outperform existing generative models by up to 9.5% on AUROC and achieve new state-of-the-art performance with little computational cost. Our method can also seamlessly integrate existing classification models. Code is available at https://github.com/Vanixxz/MEDAF",
    "checked": true,
    "id": "156907b8c73708c0be4377a5fda4730827db71c7",
    "semantic_title": "exploring diverse representations for open set recognition",
    "citation_count": 1,
    "authors": [
      "Yu Wang",
      "Junxian Mu",
      "Pengfei Zhu",
      "Qinghua Hu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28386": {
    "title": "SMILEtrack: SiMIlarity LEarning for Occlusion-Aware Multiple Object Tracking",
    "volume": "main",
    "abstract": "Despite recent progress in Multiple Object Tracking (MOT), several obstacles such as occlusions, similar objects, and complex scenes remain an open challenge. Meanwhile, a systematic study of the cost-performance tradeoff for the popular tracking-by-detection paradigm is still lacking. This paper introduces SMILEtrack, an innovative object tracker that effectively addresses these challenges by integrating an efficient object detector with a Siamese network-based Similarity Learning Module (SLM). The technical contributions of SMILETrack are twofold. First, we propose an SLM that calculates the appearance similarity between two objects, overcoming the limitations of feature descriptors in Separate Detection and Embedding (SDE) models. The SLM incorporates a Patch Self-Attention (PSA) block inspired by the vision Transformer, which generates reliable features for accurate similarity matching. Second, we develop a Similarity Matching Cascade (SMC) module with a novel GATE function for robust object matching across consecutive video frames, further enhancing MOT performance. Together, these innovations help SMILETrack achieve an improved trade-off between the cost (e.g., running speed) and performance (e.g., tracking accuracy) over several existing state-of-the-art benchmarks, including the popular BYTETrack method. SMILETrack outperforms BYTETrack by 0.4-0.8 MOTA and 2.1-2.2 HOTA points on MOT17 and MOT20 datasets. Code is available at http://github.com/pingyang1117/SMILEtrack_official",
    "checked": true,
    "id": "5106d6e330ddb6f409b6bd04e46488e1860ca223",
    "semantic_title": "smiletrack: similarity learning for occlusion-aware multiple object tracking",
    "citation_count": 7,
    "authors": [
      "Yu-Hsiang Wang",
      "Jun-Wei Hsieh",
      "Ping-Yang Chen",
      "Ming-Ching Chang",
      "Hung-Hin So",
      "Xin Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28387": {
    "title": "Learning Hierarchical Prompt with Structured Linguistic Knowledge for Vision-Language Models",
    "volume": "main",
    "abstract": "Prompt learning has become a prevalent strategy for adapting vision-language foundation models to downstream tasks. As large language models (LLMs) have emerged, recent studies have explored the use of category-related descriptions as input to enhance prompt effectiveness. Nevertheless, conventional descriptions fall short of structured information that effectively represents the interconnections among entities or attributes linked to a particular category. To address this limitation and prioritize harnessing structured knowledge, this paper advocates for leveraging LLMs to build a graph for each description to model the entities and attributes describing the category, as well as their correlations. Preexisting prompt tuning methods exhibit inadequacies in managing this structured knowledge. Consequently, we propose a novel approach called Hierarchical Prompt Tuning (HPT), which enables simultaneous modeling of both structured and conventional linguistic knowledge. Specifically, we introduce a relationship-guided attention module to capture pair-wise associations among entities and attributes for low-level prompt learning. In addition, by incorporating high-level and global-level prompts modeling overall semantics, the proposed hierarchical structure forges cross-level interlinks and empowers the model to handle more complex and long-term relationships. Extensive experiments demonstrate that our HPT shows strong effectiveness and generalizes much better than existing SOTA methods. Our code is available at https://github.com/Vill-Lab/2024-AAAI-HPT",
    "checked": true,
    "id": "3ad899df69a4d10d99e9e9bce9f194542847f156",
    "semantic_title": "learning hierarchical prompt with structured linguistic knowledge for vision-language models",
    "citation_count": 0,
    "authors": [
      "Yubin Wang",
      "Xinyang Jiang",
      "De Cheng",
      "Dongsheng Li",
      "Cairong Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28388": {
    "title": "TOP-ReID: Multi-Spectral Object Re-identification with Token Permutation",
    "volume": "main",
    "abstract": "Multi-spectral object Re-identification (ReID) aims to retrieve specific objects by leveraging complementary information from different image spectra. It delivers great advantages over traditional single-spectral ReID in complex visual environment. However, the significant distribution gap among different image spectra poses great challenges for effective multi-spectral feature representations. In addition, most of current Transformer-based ReID methods only utilize the global feature of class tokens to achieve the holistic retrieval, ignoring the local discriminative ones. To address the above issues, we step further to utilize all the tokens of Transformers and propose a cyclic token permutation framework for multi-spectral object ReID, dubbled TOP-ReID. More specifically, we first deploy a multi-stream deep network based on vision Transformers to preserve distinct information from different image spectra. Then, we propose a Token Permutation Module (TPM) for cyclic multi-spectral feature aggregation. It not only facilitates the spatial feature alignment across different image spectra, but also allows the class token of each spectrum to perceive the local details of other spectra. Meanwhile, we propose a Complementary Reconstruction Module (CRM), which introduces dense token-level reconstruction constraints to reduce the distribution gap across different image spectra. With the above modules, our proposed framework can generate more discriminative multi-spectral features for robust object ReID. Extensive experiments on three ReID benchmarks (i.e., RGBNT201, RGBNT100 and MSVR310) verify the effectiveness of our methods. The code is available at https://github.com/924973292/TOP-ReID",
    "checked": true,
    "id": "003f338c7e22aef38db267120e138bcffa802ef9",
    "semantic_title": "top-reid: multi-spectral object re-identification with token permutation",
    "citation_count": 3,
    "authors": [
      "Yuhao Wang",
      "Xuehu Liu",
      "Pingping Zhang",
      "Hu Lu",
      "Zhengzheng Tu",
      "Huchuan Lu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28389": {
    "title": "GMMFormer: Gaussian-Mixture-Model Based Transformer for Efficient Partially Relevant Video Retrieval",
    "volume": "main",
    "abstract": "Given a text query, partially relevant video retrieval (PRVR) seeks to find untrimmed videos containing pertinent moments in a database. For PRVR, clip modeling is essential to capture the partial relationship between texts and videos. Current PRVR methods adopt scanning-based clip construction to achieve explicit clip modeling, which is information-redundant and requires a large storage overhead. To solve the efficiency problem of PRVR methods, this paper proposes GMMFormer, a Gaussian-Mixture-Model based Transformer which models clip representations implicitly. During frame interactions, we incorporate Gaussian-Mixture-Model constraints to focus each frame on its adjacent frames instead of the whole video. Then generated representations will contain multi-scale clip information, achieving implicit clip modeling. In addition, PRVR methods ignore semantic differences between text queries relevant to the same video, leading to a sparse embedding space. We propose a query diverse loss to distinguish these text queries, making the embedding space more intensive and contain more semantic information. Extensive experiments on three large-scale video datasets (i.e., TVR, ActivityNet Captions, and Charades-STA) demonstrate the superiority and efficiency of GMMFormer",
    "checked": true,
    "id": "599c19d4eaefcc3f345ca77ceefa67004ad3b4d4",
    "semantic_title": "gmmformer: gaussian-mixture-model based transformer for efficient partially relevant video retrieval",
    "citation_count": 2,
    "authors": [
      "Yuting Wang",
      "Jinpeng Wang",
      "Bin Chen",
      "Ziyun Zeng",
      "Shu-Tao Xia"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28390": {
    "title": "Out of Thin Air: Exploring Data-Free Adversarial Robustness Distillation",
    "volume": "main",
    "abstract": "Adversarial Robustness Distillation (ARD) is a promising task to solve the issue of limited adversarial robustness of small capacity models while optimizing the expensive computational costs of Adversarial Training (AT). Despite the good robust performance, the existing ARD methods are still impractical to deploy in natural high-security scenes due to these methods rely entirely on original or publicly available data with a similar distribution. In fact, these data are almost always private, specific, and distinctive for scenes that require high robustness. To tackle these issues, we propose a challenging but significant task called Data-Free Adversarial Robustness Distillation (DFARD), which aims to train small, easily deployable, robust models without relying on data. We demonstrate that the challenge lies in the lower upper bound of knowledge transfer information, making it crucial to mining and transferring knowledge more efficiently. Inspired by human education, we design a plug-and-play Interactive Temperature Adjustment (ITA) strategy to improve the efficiency of knowledge transfer and propose an Adaptive Generator Balance (AGB) module to retain more data information. Our method uses adaptive hyperparameters to avoid a large number of parameter tuning, which significantly outperforms the combination of existing techniques. Meanwhile, our method achieves stable and reliable performance on multiple benchmarks",
    "checked": true,
    "id": "fa0a6362d0e98f48bb5bdec739d7e792aaaec7f3",
    "semantic_title": "out of thin air: exploring data-free adversarial robustness distillation",
    "citation_count": 5,
    "authors": [
      "Yuzheng Wang",
      "Zhaoyu Chen",
      "Dingkang Yang",
      "Pinxue Guo",
      "Kaixun Jiang",
      "Wenqiang Zhang",
      "Lizhe Qi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28391": {
    "title": "QAGait: Revisit Gait Recognition from a Quality Perspective",
    "volume": "main",
    "abstract": "Gait recognition is a promising biometric method that aims to identify pedestrians from their unique walking patterns. Silhouette modality, renowned for its easy acquisition, simple structure, sparse representation, and convenient modeling, has been widely employed in controlled in-the-lab research. However, as gait recognition rapidly advances from in-the-lab to in-the-wild scenarios, various conditions raise significant challenges for silhouette modality, including 1) unidentifiable low-quality silhouettes (abnormal segmentation, severe occlusion, or even non-human shape), and 2) identifiable but challenging silhouettes (background noise, non-standard posture, slight occlusion). To address these challenges, we revisit gait recognition pipeline and approach gait recognition from a quality perspective, namely QAGait. Specifically, we propose a series of cost-effective quality assessment strategies, including Maxmial Connect Area and Template Match to eliminate background noises and unidentifiable silhouettes, Alignment strategy to handle non-standard postures. We also propose two quality-aware loss functions to integrate silhouette quality into optimization within the embedding space. Extensive experiments demonstrate our QAGait can guarantee both gait reliability and performance enhancement. Furthermore, our quality assessment strategies can seamlessly integrate with existing gait datasets, showcasing our superiority. Code is available at https://github.com/wzb-bupt/QAGait",
    "checked": true,
    "id": "ef60c7a8224783531317b9fe76c8edd3968add8a",
    "semantic_title": "qagait: revisit gait recognition from a quality perspective",
    "citation_count": 1,
    "authors": [
      "Zengbin Wang",
      "Saihui Hou",
      "Man Zhang",
      "Xu Liu",
      "Chunshui Cao",
      "Yongzhen Huang",
      "Peipei Li",
      "Shibiao Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28392": {
    "title": "Enhancing Hyperspectral Images via Diffusion Model and Group-Autoencoder Super-resolution Network",
    "volume": "main",
    "abstract": "Existing hyperspectral image (HSI) super-resolution (SR) methods struggle to effectively capture the complex spectral-spatial relationships and low-level details, while diffusion models represent a promising generative model known for their exceptional performance in modeling complex relations and learning high and low-level visual features. The direct application of diffusion models to HSI SR is hampered by challenges such as difficulties in model convergence and protracted inference time. In this work, we introduce a novel Group-Autoencoder (GAE) framework that synergistically combines with the diffusion model to construct a highly effective HSI SR model (DMGASR). Our proposed GAE framework encodes high-dimensional HSI data into low-dimensional latent space where the diffusion model works, thereby alleviating the difficulty of training the diffusion model while maintaining band correlation and considerably reducing inference time. Experimental results on both natural and remote sensing hyperspectral datasets demonstrate that the proposed method is superior to other state-of-the-art methods both visually and metrically",
    "checked": true,
    "id": "3c9fecab8f797cdc8d6f331ca9460347e0187f95",
    "semantic_title": "enhancing hyperspectral images via diffusion model and group-autoencoder super-resolution network",
    "citation_count": 0,
    "authors": [
      "Zhaoyang Wang",
      "Dongyang Li",
      "Mingyang Zhang",
      "Hao Luo",
      "Maoguo Gong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28393": {
    "title": "SkyScript: A Large and Semantically Diverse Vision-Language Dataset for Remote Sensing",
    "volume": "main",
    "abstract": "Remote sensing imagery, despite its broad applications in helping achieve Sustainable Development Goals and tackle climate change, has not yet benefited from the recent advancements of versatile, task-agnostic vision language models (VLMs). A key reason is that the large-scale, semantically diverse image-text dataset required for developing VLMs is still absent for remote sensing images. Unlike natural images, remote sensing images and their associated text descriptions cannot be efficiently collected from the public Internet at scale. In this work, we bridge this gap by using geo-coordinates to automatically connect open, unlabeled remote sensing images with rich semantics covered in OpenStreetMap, and thus construct SkyScript, a comprehensive vision-language dataset for remote sensing images, comprising 2.6 million image-text pairs covering 29K distinct semantic tags. With continual pre-training on this dataset, we obtain a VLM that surpasses baseline models with a 6.2% average accuracy gain in zero-shot scene classification across seven benchmark datasets. It also demonstrates the ability of zero-shot transfer for fine-grained object attribute classification and cross-modal retrieval. We hope this dataset can support the advancement of VLMs for various multi-modal tasks in remote sensing, such as open-vocabulary classification, retrieval, captioning, and text-to-image synthesis",
    "checked": true,
    "id": "bacdcd622dabe746005d6713e6d40c3ee15c158f",
    "semantic_title": "skyscript: a large and semantically diverse vision-language dataset for remote sensing",
    "citation_count": 12,
    "authors": [
      "Zhecheng Wang",
      "Rajanie Prabha",
      "Tianyuan Huang",
      "Jiajun Wu",
      "Ram Rajagopal"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28394": {
    "title": "DTMFormer: Dynamic Token Merging for Boosting Transformer-Based Medical Image Segmentation",
    "volume": "main",
    "abstract": "Despite the great potential in capturing long-range dependency, one rarely-explored underlying issue of transformer in medical image segmentation is attention collapse, making it often degenerate into a bypass module in CNN-Transformer hybrid architectures. This is due to the high computational complexity of vision transformers requiring extensive training data while well-annotated medical image data is relatively limited, resulting in poor convergence. In this paper, we propose a plug-n-play transformer block with dynamic token merging, named DTMFormer, to avoid building long-range dependency on redundant and duplicated tokens and thus pursue better convergence. Specifically, DTMFormer consists of an attention-guided token merging (ATM) module to adaptively cluster tokens into fewer semantic tokens based on feature and dependency similarity and a light token reconstruction module to fuse ordinary and semantic tokens. In this way, as self-attention in ATM is calculated based on fewer tokens, DTMFormer is of lower complexity and more friendly to converge. Extensive experiments on publicly-available datasets demonstrate the effectiveness of DTMFormer working as a plug-n-play module for simultaneous complexity reduction and performance improvement. We believe it will inspire future work on rethinking transformers in medical image segmentation. Code: https://github.com/iam-nacl/DTMFormer",
    "checked": true,
    "id": "b9919c4ee86e616d035b8470200cf8f020070332",
    "semantic_title": "dtmformer: dynamic token merging for boosting transformer-based medical image segmentation",
    "citation_count": 0,
    "authors": [
      "Zhehao Wang",
      "Xian Lin",
      "Nannan Wu",
      "Li Yu",
      "Kwang-Ting Cheng",
      "Zengqiang Yan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28395": {
    "title": "SGNet: Structure Guided Network via Gradient-Frequency Awareness for Depth Map Super-resolution",
    "volume": "main",
    "abstract": "Depth super-resolution (DSR) aims to restore high-resolution (HR) depth from low-resolution (LR) one, where RGB image is often used to promote this task. Recent image guided DSR approaches mainly focus on spatial domain to rebuild depth structure. However, since the structure of LR depth is usually blurry, only considering spatial domain is not very sufficient to acquire satisfactory results. In this paper, we propose structure guided network (SGNet), a method that pays more attention to gradient and frequency domains, both of which have the inherent ability to capture high-frequency structure. Specifically, we first introduce the gradient calibration module (GCM), which employs the accurate gradient prior of RGB to sharpen the LR depth structure. Then we present the Frequency Awareness Module (FAM) that recursively conducts multiple spectrum differencing blocks (SDB), each of which propagates the precise high-frequency components of RGB into the LR depth. Extensive experimental results on both real and synthetic datasets demonstrate the superiority of our SGNet, reaching the state-of-the-art (see Fig. 1). Codes and pre-trained models are available at https://github.com/yanzq95/SGNet",
    "checked": true,
    "id": "5a76e4da481763199dce4a5d3f3cb904c1d218d4",
    "semantic_title": "sgnet: structure guided network via gradient-frequency awareness for depth map super-resolution",
    "citation_count": 3,
    "authors": [
      "Zhengxue Wang",
      "Zhiqiang Yan",
      "Jian Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28396": {
    "title": "Vision Transformer Off-the-Shelf: A Surprising Baseline for Few-Shot Class-Agnostic Counting",
    "volume": "main",
    "abstract": "Class-agnostic counting (CAC) aims to count objects of interest from a query image given few exemplars. This task is typically addressed by extracting the features of query image and exemplars respectively and then matching their feature similarity, leading to an extract-then-match paradigm. In this work, we show that CAC can be simplified in an extract-and-match manner, particularly using a vision transformer (ViT) where feature extraction and similarity matching are executed simultaneously within the self-attention. We reveal the rationale of such simplification from a decoupled view of the self-attention.The resulting model, termed CACViT, simplifies the CAC pipeline into a single pretrained plain ViT. Further, to compensate the loss of the scale and the order-of-magnitude information due to resizing and normalization in plain ViT, we present two effective strategies for scale and magnitude embedding. Extensive experiments on the FSC147 and the CARPK datasets show that CACViT significantly outperforms state-of-the-art CAC approaches in both effectiveness (23.60% error reduction) and generalization, which suggests CACViT provides a concise and strong baseline for CAC. Code will be available",
    "checked": true,
    "id": "9a5bb6fb91740981adfccc5d7a6b847068c2749b",
    "semantic_title": "vision transformer off-the-shelf: a surprising baseline for few-shot class-agnostic counting",
    "citation_count": 1,
    "authors": [
      "Zhicheng Wang",
      "Liwen Xiao",
      "Zhiguo Cao",
      "Hao Lu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28397": {
    "title": "Existence Is Chaos: Enhancing 3D Human Motion Prediction with Uncertainty Consideration",
    "volume": "main",
    "abstract": "Human motion prediction is consisting in forecasting future body poses from historically observed sequences. It is a longstanding challenge due to motion's complex dynamics and uncertainty. Existing methods focus on building up complicated neural networks to model the motion dynamics. The predicted results are required to be strictly similar to the training samples with L2 loss in current training pipeline. However, little attention has been paid to the uncertainty property which is crucial to the prediction task. We argue that the recorded motion in training data could be an observation of possible future, rather than a predetermined result. In addition, existing works calculate the predicted error on each future frame equally during training, while recent work indicated that different frames could play different roles. In this work, a novel computationally efficient encoder-decoder model with uncertainty consideration is proposed, which could learn proper characteristics for future frames by a dynamic function. Experimental results on benchmark datasets demonstrate that our uncertainty consideration approach has obvious advantages both in quantity and quality. Moreover, the proposed method could produce motion sequences with much better quality that avoids the intractable shaking artefacts. We believe our work could provide a novel perspective to consider the uncertainty quality for the general motion prediction task and encourage the studies in this field. The code will be available in https://github.com/Motionpre/Adaptive-Salient-Loss-SAGGB",
    "checked": true,
    "id": "d62d40a1537d7f059b03aeb0816846ef1655b242",
    "semantic_title": "existence is chaos: enhancing 3d human motion prediction with uncertainty consideration",
    "citation_count": 0,
    "authors": [
      "Zhihao Wang",
      "Yulin Zhou",
      "Ningyu Zhang",
      "Xiaosong Yang",
      "Jun Xiao",
      "Zhao Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28398": {
    "title": "Heterogeneous Test-Time Training for Multi-Modal Person Re-identification",
    "volume": "main",
    "abstract": "Multi-modal person re-identification (ReID) seeks to mitigate challenging lighting conditions by incorporating diverse modalities. Most existing multi-modal ReID methods concentrate on leveraging complementary multi-modal information via fusion or interaction. However, the relationships among heterogeneous modalities and the domain traits of unlabeled test data are rarely explored. In this paper, we propose a Heterogeneous Test-time Training (HTT) framework for multi-modal person ReID. We first propose a Cross-identity Inter-modal Margin (CIM) loss to amplify the differentiation among distinct identity samples. Moreover, we design a Multi-modal Test-time Training (MTT) strategy to enhance the generalization of the model by leveraging the relationships in the heterogeneous modalities and the information existing in the test data. Specifically, in the training stage, we utilize the CIM loss to further enlarge the distance between anchor and negative by forcing the inter-modal distance to maintain the margin, resulting in an enhancement of the discriminative capacity of the ultimate descriptor. Subsequently, since the test data contains characteristics of the target domain, we adapt the MTT strategy to optimize the network before the inference by using self-supervised tasks designed based on relationships among modalities. Experimental results on benchmark multi-modal ReID datasets RGBNT201, Market1501-MM, RGBN300, and RGBNT100 validate the effectiveness of the proposed method. The codes can be found at https://github.com/ziwang1121/HTT",
    "checked": true,
    "id": "ca4892933dda10e0ed4110d35509c2e2ae7ecfd5",
    "semantic_title": "heterogeneous test-time training for multi-modal person re-identification",
    "citation_count": 0,
    "authors": [
      "Zi Wang",
      "Huaibo Huang",
      "Aihua Zheng",
      "Ran He"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28399": {
    "title": "Fine-Grained Prototypes Distillation for Few-Shot Object Detection",
    "volume": "main",
    "abstract": "Few-shot object detection (FSOD) aims at extending a generic detector for novel object detection with only a few training examples. It attracts great concerns recently due to the practical meanings. Meta-learning has been demonstrated to be an effective paradigm for this task. In general, methods based on meta-learning employ an additional support branch to encode novel examples (a.k.a. support images) into class prototypes, which are then fused with query branch to facilitate the model prediction. However, the class-level prototypes are difficult to precisely generate, and they also lack detailed information, leading to instability in performance. New methods are required to capture the distinctive local context for more robust novel object detection. To this end, we propose to distill the most representative support features into fine-grained prototypes. These prototypes are then assigned into query feature maps based on the matching results, modeling the detailed feature relations between two branches. This process is realized by our Fine-Grained Feature Aggregation (FFA) module. Moreover, in terms of high-level feature fusion, we propose Balanced Class-Agnostic Sampling (B-CAS) strategy and Non-Linear Fusion (NLF) module from differenct perspectives. They are complementary to each other and depict the high-level feature relations more effectively. Extensive experiments on PASCAL VOC and MS COCO benchmarks show that our method sets a new state-of-the-art performance in most settings. Our code is available at https://github.com/wangchen1801/FPD",
    "checked": true,
    "id": "18d1b0241aab29429f13f4439e98ed1878f3ff88",
    "semantic_title": "fine-grained prototypes distillation for few-shot object detection",
    "citation_count": 1,
    "authors": [
      "Zichen Wang",
      "Bo Yang",
      "Haonan Yue",
      "Zhenghao Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28400": {
    "title": "Semantic Complete Scene Forecasting from a 4D Dynamic Point Cloud Sequence",
    "volume": "main",
    "abstract": "We study a new problem of semantic complete scene forecasting (SCSF) in this work. Given a 4D dynamic point cloud sequence, our goal is to forecast the complete scene corresponding to the future next frame along with its semantic labels. To tackle this challenging problem, we properly model the synergetic relationship between future forecasting and semantic scene completion through a novel network named SCSFNet. SCSFNet leverages a hybrid geometric representation for high-resolution complete scene forecasting. To leverage multi-frame observation as well as the understanding of scene dynamics to ease the completion task, SCSFNet introduces an attention-based skip connection scheme. To ease the need to model occlusion variations and to better focus on the occluded part, SCSFNet utilizes auxiliary visibility grids to guide the forecasting task. To evaluate the effectiveness of SCSFNet, we conduct experiments on various benchmarks including two large-scale indoor benchmarks we contributed and the outdoor SemanticKITTI benchmark. Extensive experiments show SCSFNet outperforms baseline methods on multiple metrics by a large margin, and also prove the synergy between future forecasting and semantic scene completion.The project page with code is available at scsfnet.github.io",
    "checked": true,
    "id": "7557ccfc7e99600a3fec111687cab3b971156ca0",
    "semantic_title": "semantic complete scene forecasting from a 4d dynamic point cloud sequence",
    "citation_count": 2,
    "authors": [
      "Zifan Wang",
      "Zhuorui Ye",
      "Haoran Wu",
      "Junyu Chen",
      "Li Yi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28401": {
    "title": "Enhanced Fine-Grained Motion Diffusion for Text-Driven Human Motion Synthesis",
    "volume": "main",
    "abstract": "The emergence of text-driven motion synthesis technique provides animators with great potential to create efficiently. However, in most cases, textual expressions only contain general and qualitative motion descriptions, while lack fine depiction and sufficient intensity, leading to the synthesized motions that either (a) semantically compliant but uncontrollable over specific pose details, or (b) even deviates from the provided descriptions, bringing animators with undesired cases. In this paper, we propose DiffKFC, a conditional diffusion model for text-driven motion synthesis with KeyFrames Collaborated, enabling realistic generation with collaborative and efficient dual-level control: coarse guidance at semantic level, with only few keyframes for direct and fine-grained depiction down to body posture level. Unlike existing inference-editing diffusion models that incorporate conditions without training, our conditional diffusion model is explicitly trained and can fully exploit correlations among texts, keyframes and the diffused target frames. To preserve the control capability of discrete and sparse keyframes, we customize dilated mask attention modules where only partial valid tokens participate in local-to-global attention, indicated by the dilated keyframe mask. Additionally, we develop a simple yet effective smoothness prior, which steers the generated frames towards seamless keyframe transitions at inference. Extensive experiments show that our model not only achieves state-of-the-art performance in terms of semantic fidelity, but more importantly, is able to satisfy animator requirements through fine-grained guidance without tedious labor",
    "checked": true,
    "id": "8eff8c7784c4f9f43377013f14b68aac6c3f1558",
    "semantic_title": "enhanced fine-grained motion diffusion for text-driven human motion synthesis",
    "citation_count": 4,
    "authors": [
      "Dong Wei",
      "Xiaoning Sun",
      "Huaijiang Sun",
      "Shengxiang Hu",
      "Bin Li",
      "Weiqing Li",
      "Jianfeng Lu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28402": {
    "title": "Image as a Language: Revisiting Scene Text Recognition via Balanced, Unified and Synchronized Vision-Language Reasoning Network",
    "volume": "main",
    "abstract": "Scene text recognition is inherently a vision-language task. However, previous works have predominantly focused either on extracting more robust visual features or designing better language modeling. How to effectively and jointly model vision and language to mitigate heavy reliance on a single modality remains a problem. In this paper, aiming to enhance vision-language reasoning in scene text recognition, we present a balanced, unified and synchronized vision-language reasoning network (BUSNet). Firstly, revisiting the image as a language by balanced concatenation along length dimension alleviates the issue of over-reliance on vision or language. Secondly, BUSNet learns an ensemble of unified external and internal vision-language model with shared weight by masked modality modeling (MMM). Thirdly, a novel vision-language reasoning module (VLRM) with synchronized vision-language decoding capacity is proposed. Additionally, BUSNet achieves improved performance through iterative reasoning, which utilizes the vision-language prediction as a new language input. Extensive experiments indicate that BUSNet achieves state-of-the-art performance on several mainstream benchmark datasets and more challenge datasets for both synthetic and real training data compared to recent outstanding methods. Code and dataset will be available at https://github.com/jjwei66/BUSNet",
    "checked": true,
    "id": "cf68ad4127ceacdf36714270ffd527942d28affb",
    "semantic_title": "image as a language: revisiting scene text recognition via balanced, unified and synchronized vision-language reasoning network",
    "citation_count": 0,
    "authors": [
      "Jiajun Wei",
      "Hongjian Zhan",
      "Yue Lu",
      "Xiao Tu",
      "Bing Yin",
      "Cong Liu",
      "Umapada Pal"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28403": {
    "title": "WeakPCSOD: Overcoming the Bias of Box Annotations for Weakly Supervised Point Cloud Salient Object Detection",
    "volume": "main",
    "abstract": "Point cloud salient object detection (PCSOD) is a newly proposed task in 3D dense segmentation. However, the acquisition of accurate 3D dense annotations comes at a high cost, severely limiting the progress of PCSOD. To address this issue, we propose the first weakly supervised PCSOD (named WeakPCSOD) model, which relies solely on cheap 3D bounding box annotations. In WeakPCSOD, we extract noise-free supervision from coarse 3D bounding boxes while mitigating shape biases inherent in box annotations. To achieve this, we introduce a novel mask-to-box (M2B) transformation and a color consistency (CC) loss. The M2B transformation, from a shape perspective, disentangles predictions from labels, enabling the extraction of noiseless supervision from labels while preserving object shapes independently of the box bias. From an appearance perspective, we further introduce the CC loss to provide dense supervision, which mitigates the non-unique predictions stemming from weak supervision and substantially reduces prediction variability. Furthermore, we employ a self-training (ST) strategy to enhance performance by utilizing high-confidence pseudo labels. Notably, the M2B transformation, CC loss, and ST strategy are seamlessly integrated into any model and incur no computational costs for inference. Extensive experiments demonstrate the effectiveness of our WeakPCSOD model, even comparable to fully supervised models utilizing dense annotations",
    "checked": true,
    "id": "2c21c7743d871e9497bad2e0c6cbf7b1d3ad35a7",
    "semantic_title": "weakpcsod: overcoming the bias of box annotations for weakly supervised point cloud salient object detection",
    "citation_count": 0,
    "authors": [
      "Jun Wei",
      "S. Kevin Zhou",
      "Shuguang Cui",
      "Zhen Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28404": {
    "title": "RetouchFormer: Semi-supervised High-Quality Face Retouching Transformer with Prior-Based Selective Self-Attention",
    "volume": "main",
    "abstract": "Face retouching is to beautify a face image, while preserving the image content as much as possible. It is a promising yet challenging task to remove face imperfections and fill with normal skin. Generic image enhancement methods are hampered by the lack of imperfection localization, which often results in incomplete removal of blemishes at large scales. To address this issue, we propose a transformer-based approach, RetouchFormer, which simultaneously identify imperfections and synthesize realistic content in the corresponding regions. Specifically, we learn a latent dictionary to capture the clean face priors, and predict the imperfection regions via a reconstruction-oriented localization module. Also based on this, we can realize face retouching by explicitly suppressing imperfections in our selective self-attention computation, such that local content will be synthesized from normal skin. On the other hand, multi-scale feature tokens lead to increased flexibility in dealing with the imperfections at various scales. The design elements bring greater effectiveness and efficiency. RetouchFormer outperforms the advanced face retouching methods and synthesizes clean face images with high fidelity in our list of extensive experiments performed",
    "checked": true,
    "id": "56baa6a6a43403611466d8a05185fd67c60921ed",
    "semantic_title": "retouchformer: semi-supervised high-quality face retouching transformer with prior-based selective self-attention",
    "citation_count": 0,
    "authors": [
      "Xue Wen",
      "Lianxin Xie",
      "Le Jiang",
      "Tianyi Chen",
      "Si Wu",
      "Cheng Liu",
      "Hau-San Wong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28405": {
    "title": "Mean Teacher DETR with Masked Feature Alignment: A Robust Domain Adaptive Detection Transformer Framework",
    "volume": "main",
    "abstract": "Unsupervised domain adaptation object detection(UDAOD) research on Detection Transformer(DETR) mainly focuses on feature alignment and existing methods can be divided into two kinds, each of which has its unresolved issues. One-stage feature alignment methods can easily lead to performance fluctuation and training stagnation. Two-stage feature alignment method based on mean teacher comprises a pretraining stage followed by a self-training stage, each facing problems in obtaining reliable pretrained model and achieving consistent performance gains. Methods mentioned above have not yet explore how to utilize the third related domain such as target-like domain to assist adaptation. To address these issues, we propose a two-stage framework named MTM, i.e. Mean Teacher-DETR with Masked Feature Alignment. In the pretraining stage, we utilize labeled target-like images produced by image style transfer to avoid performance fluctuation. In the self-training stage, we leverage unlabeled target images by pseudo labels based on mean teacher and propose a module called Object Queries Knowledge Transfer(OQKT) to ensure consistent performance gains of the student model. Most importantly, we propose masked feature alignment methods including Masked Domain Query-based Feature Alignment(MDQFA) and Masked Token-wise Feature Alignment(MTWFA) to alleviate domain shift in a more robust way, which not only prevent training stagnation and lead to a robust pretrained model in the pretraining stage, but also enhance the model's target performance in the self-training stage. Experiments on three challenging scenarios and a theoretical analysis verify the effectiveness of MTM",
    "checked": true,
    "id": "702413ffe52fb57e38d55176672694bfa57975eb",
    "semantic_title": "mean teacher detr with masked feature alignment: a robust domain adaptive detection transformer framework",
    "citation_count": 1,
    "authors": [
      "Weixi Weng",
      "Chun Yuan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28406": {
    "title": "Keep the Faith: Faithful Explanations in Convolutional Neural Networks for Case-Based Reasoning",
    "volume": "main",
    "abstract": "Explaining predictions of black-box neural networks is crucial when applied to decision-critical tasks. Thus, attribution maps are commonly used to identify important image regions, despite prior work showing that humans prefer explanations based on similar examples. To this end, ProtoPNet learns a set of class-representative feature vectors (prototypes) for case-based reasoning. During inference, similarities of latent features to prototypes are linearly classified to form predictions and attribution maps are provided to explain the similarity. In this work, we evaluate whether architectures for case-based reasoning fulfill established axioms required for faithful explanations using the example of ProtoPNet. We show that such architectures allow the extraction of faithful explanations. However, we prove that the attribution maps used to explain the similarities violate the axioms. We propose a new procedure to extract explanations for trained ProtoPNets, named ProtoPFaith. Conceptually, these explanations are Shapley values, calculated on the similarity scores of each prototype. They allow to faithfully answer which prototypes are present in an unseen image and quantify each pixel's contribution to that presence, thereby complying with all axioms. The theoretical violations of ProtoPNet manifest in our experiments on three datasets (CUB-200-2011, Stanford Dogs, RSNA) and five architectures (ConvNet, ResNet, ResNet50, WideResNet50, ResNeXt50). Our experiments show a qualitative difference between the explanations given by ProtoPNet and ProtoPFaith. Additionally, we quantify the explanations with the Area Over the Perturbation Curve, on which ProtoPFaith outperforms ProtoPNet on all experiments by a factor >10^3",
    "checked": true,
    "id": "78300a85c644d45e7c8ea4202ad0d367031a06a5",
    "semantic_title": "keep the faith: faithful explanations in convolutional neural networks for case-based reasoning",
    "citation_count": 3,
    "authors": [
      "Tom Nuno Wolf",
      "Fabian Bongratz",
      "Anne-Marie Rickmann",
      "Sebastian Pölsterl",
      "Christian Wachinger"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28407": {
    "title": "Factorized Diffusion Autoencoder for Unsupervised Disentangled Representation Learning",
    "volume": "main",
    "abstract": "Unsupervised disentangled representation learning aims to recover semantically meaningful factors from real-world data without supervision, which is significant for model generalization and interpretability. Current methods mainly rely on assumptions of independence or informativeness of factors, regardless of interpretability. Intuitively, visually interpretable concepts better align with human-defined factors. However, exploiting visual interpretability as inductive bias is still under-explored. Inspired by the observation that most explanatory image factors can be represented by ``content + mask'', we propose a content-mask factorization network (CMFNet) to decompose an image into different groups of content codes and masks, which are further combined as content masks to represent different visual concepts. To ensure informativeness of the representations, the CMFNet is jointly learned with a generator conditioned on the content masks for reconstructing the input image. The conditional generator employs a diffusion model to leverage its robust distribution modeling capability. Our model is called the Factorized Diffusion Autoencoder (FDAE). To enhance disentanglement of visual concepts, we propose a content decorrelation loss and a mask entropy loss to decorrelate content masks in latent space and spatial space, respectively. Experiments on Shapes3d, MPI3D and Cars3d show that our method achieves advanced performance and can generate visually interpretable concept-specific masks. Source code and supplementary materials are available at https://github.com/wuancong/FDAE",
    "checked": true,
    "id": "893e87ef529edaaedb507c845b776a0d5b3d334e",
    "semantic_title": "factorized diffusion autoencoder for unsupervised disentangled representation learning",
    "citation_count": 1,
    "authors": [
      "Ancong Wu",
      "Wei-Shi Zheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28408": {
    "title": "3D-STMN: Dependency-Driven Superpoint-Text Matching Network for End-to-End 3D Referring Expression Segmentation",
    "volume": "main",
    "abstract": "In 3D Referring Expression Segmentation (3D-RES), the earlier approach adopts a two-stage paradigm, extracting segmentation proposals and then matching them with referring expressions. However, this conventional paradigm encounters significant challenges, most notably in terms of the generation of lackluster initial proposals and a pronounced deceleration in inference speed. Recognizing these limitations, we introduce an innovative end-to-end Superpoint-Text Matching Network (3D-STMN) that is enriched by dependency-driven insights. One of the keystones of our model is the Superpoint-Text Matching (STM) mechanism. Unlike traditional methods that navigate through instance proposals, STM directly correlates linguistic indications with their respective superpoints, clusters of semantically related points. This architectural decision empowers our model to efficiently harness cross-modal semantic relationships, primarily leveraging densely annotated superpoint-text pairs, as opposed to the more sparse instance-text pairs. In pursuit of enhancing the role of text in guiding the segmentation process, we further incorporate the Dependency-Driven Interaction (DDI) module to deepen the network's semantic comprehension of referring expressions. Using the dependency trees as a beacon, this module discerns the intricate relationships between primary terms and their associated descriptors in expressions, thereby elevating both the localization and segmentation capacities. Comprehensive experiments on the ScanRefer benchmark reveal that our model not only sets new performance standards, registering an mIoU gain of 11.7 points but also achieves a staggering enhancement in inference speed, surpassing traditional methods by 95.7 times. The code and models are available at https://github.com/sosppxo/3D-STMN",
    "checked": true,
    "id": "1c509635c1c01e31c6d6e1c493d70ed0b1e8218d",
    "semantic_title": "3d-stmn: dependency-driven superpoint-text matching network for end-to-end 3d referring expression segmentation",
    "citation_count": 5,
    "authors": [
      "Changli Wu",
      "Yiwei Ma",
      "Qi Chen",
      "Haowei Wang",
      "Gen Luo",
      "Jiayi Ji",
      "Xiaoshuai Sun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28409": {
    "title": "SCD-Net: Spatiotemporal Clues Disentanglement Network for Self-Supervised Skeleton-Based Action Recognition",
    "volume": "main",
    "abstract": "Contrastive learning has achieved great success in skeleton-based action recognition. However, most existing approaches encode the skeleton sequences as entangled spatiotemporal representations and confine the contrasts to the same level of representation. Instead, this paper introduces a novel contrastive learning framework, namely Spatiotemporal Clues Disentanglement Network (SCD-Net). Specifically, we integrate the decoupling module with a feature extractor to derive explicit clues from spatial and temporal domains respectively. As for the training of SCD-Net, with a constructed global anchor, we encourage the interaction between the anchor and extracted clues. Further, we propose a new masking strategy with structural constraints to strengthen the contextual associations, leveraging the latest development from masked image modelling into the proposed SCD-Net. We conduct extensive evaluations on the NTU-RGB+D (60&120) and PKU-MMD (I&II) datasets, covering various downstream tasks such as action recognition, action retrieval, transfer learning, and semi-supervised learning. The experimental results demonstrate the effectiveness of our method, which outperforms the existing state-of-the-art (SOTA) approaches significantly. Our code and supplementary material can be found at https://github.com/cong-wu/SCD-Net",
    "checked": true,
    "id": "38c48d17c1fb4f1cc1e7b9eeec174be782c53d8a",
    "semantic_title": "scd-net: spatiotemporal clues disentanglement network for self-supervised skeleton-based action recognition",
    "citation_count": 0,
    "authors": [
      "Cong Wu",
      "Xiao-Jun Wu",
      "Josef Kittler",
      "Tianyang Xu",
      "Sara Ahmed",
      "Muhammad Awais",
      "Zhenhua Feng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28410": {
    "title": "G-NAS: Generalizable Neural Architecture Search for Single Domain Generalization Object Detection",
    "volume": "main",
    "abstract": "In this paper, we focus on a realistic yet challenging task, Single Domain Generalization Object Detection (S-DGOD), where only one source domain's data can be used for training object detectors, but have to generalize multiple distinct target domains. In S-DGOD, both high-capacity fitting and generalization abilities are needed due to the task's complexity. Differentiable Neural Architecture Search (NAS) is known for its high capacity for complex data fitting and we propose to leverage Differentiable NAS to solve S-DGOD. However, it may confront severe over-fitting issues due to the feature imbalance phenomenon, where parameters optimized by gradient descent are biased to learn from the easy-to-learn features, which are usually non-causal and spuriously correlated to ground truth labels, such as the features of background in object detection data. Consequently, this leads to serious performance degradation, especially in generalizing to unseen target domains with huge domain gaps between the source domain and target domains. To address this issue, we propose the Generalizable loss (G-loss), which is an OoD-aware objective, preventing NAS from over-fitting by using gradient descent to optimize parameters not only on a subset of easy-to-learn features but also the remaining predictive features for generalization, and the overall framework is named G-NAS. Experimental results on the S-DGOD urban-scene datasets demonstrate that the proposed G-NAS achieves SOTA performance compared to baseline methods. Codes are available at https://github.com/wufan-cse/G-NAS",
    "checked": true,
    "id": "ee55a0fb99b64e3cec9b735bfab09c9e32562b0b",
    "semantic_title": "g-nas: generalizable neural architecture search for single domain generalization object detection",
    "citation_count": 0,
    "authors": [
      "Fan Wu",
      "Jinling Gao",
      "Lanqing Hong",
      "Xinbing Wang",
      "Chenghu Zhou",
      "Nanyang Ye"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28411": {
    "title": "Multiscale Low-Frequency Memory Network for Improved Feature Extraction in Convolutional Neural Networks",
    "volume": "main",
    "abstract": "Deep learning and Convolutional Neural Networks (CNNs) have driven major transformations in diverse research areas. However, their limitations in handling low-frequency in-formation present obstacles in certain tasks like interpreting global structures or managing smooth transition images. Despite the promising performance of transformer struc-tures in numerous tasks, their intricate optimization com-plexities highlight the persistent need for refined CNN en-hancements using limited resources. Responding to these complexities, we introduce a novel framework, the Mul-tiscale Low-Frequency Memory (MLFM) Network, with the goal to harness the full potential of CNNs while keep-ing their complexity unchanged. The MLFM efficiently preserves low-frequency information, enhancing perfor-mance in targeted computer vision tasks. Central to our MLFM is the Low-Frequency Memory Unit (LFMU), which stores various low-frequency data and forms a parallel channel to the core network. A key advantage of MLFM is its seamless compatibility with various prevalent networks, requiring no alterations to their original core structure. Testing on ImageNet demonstrated substantial accuracy improvements in multiple 2D CNNs, including ResNet, MobileNet, EfficientNet, and ConvNeXt. Furthermore, we showcase MLFM's versatility beyond traditional image classification by successfully integrating it into image-to-image translation tasks, specifically in semantic segmenta-tion networks like FCN and U-Net. In conclusion, our work signifies a pivotal stride in the journey of optimizing the ef-ficacy and efficiency of CNNs with limited resources. This research builds upon the existing CNN foundations and paves the way for future advancements in computer vision. Our codes are available at https://github.com/AlphaWuSeu/MLFM",
    "checked": true,
    "id": "8610a67272e0d806471a36935a427a981018cee9",
    "semantic_title": "multiscale low-frequency memory network for improved feature extraction in convolutional neural networks",
    "citation_count": 0,
    "authors": [
      "Fuzhi Wu",
      "Jiasong Wu",
      "Youyong Kong",
      "Chunfeng Yang",
      "Guanyu Yang",
      "Huazhong Shu",
      "Guy Carrault",
      "Lotfi Senhadji"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28412": {
    "title": "Learning from History: Task-agnostic Model Contrastive Learning for Image Restoration",
    "volume": "main",
    "abstract": "Contrastive learning has emerged as a prevailing paradigm for high-level vision tasks, which, by introducing properly negative samples, has also been exploited for low-level vision tasks to achieve a compact optimization space to account for their ill-posed nature. However, existing methods rely on manually predefined and task-oriented negatives, which often exhibit pronounced task-specific biases. To address this challenge, our paper introduces an innovative method termed 'learning from history', which dynamically generates negative samples from the target model itself. Our approach, named Model Contrastive Learning for Image Restoration (MCLIR), rejuvenates latency models as negative models, making it compatible with diverse image restoration tasks. We propose the Self-Prior guided Negative loss (SPN) to enable it. This approach significantly enhances existing models when retrained with the proposed model contrastive paradigm. The results show significant improvements in image restoration across various tasks and architectures. For example, models retrained with SPN outperform the original FFANet and DehazeFormer by 3.41 and 0.57 dB on the RESIDE indoor dataset for image dehazing. Similarly, they achieve notable improvements of 0.47 dB on SPA-Data over IDT for image deraining and 0.12 dB on Manga109 for a 4x scale super-resolution over lightweight SwinIR, respectively. Code and retrained models are available at https://github.com/Aitical/MCLIR",
    "checked": true,
    "id": "8ce208dfd1259d087e0195f5677cbf2865411ba8",
    "semantic_title": "learning from history: task-agnostic model contrastive learning for image restoration",
    "citation_count": 3,
    "authors": [
      "Gang Wu",
      "Junjun Jiang",
      "Kui Jiang",
      "Xianming Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28413": {
    "title": "Hybrid-Supervised Dual-Search: Leveraging Automatic Learning for Loss-Free Multi-Exposure Image Fusion",
    "volume": "main",
    "abstract": "Multi-exposure image fusion (MEF) has emerged as a prominent solution to address the limitations of digital imaging in representing varied exposure levels. Despite its advancements, the field grapples with challenges, notably the reliance on manual designs for network structures and loss functions, and the constraints of utilizing simulated reference images as ground truths. Consequently, current methodologies often suffer from color distortions and exposure artifacts, further complicating the quest for authentic image representation. In addressing these challenges, this paper presents a Hybrid-Supervised Dual-Search approach for MEF, dubbed HSDS-MEF, which introduces a bi-level optimization search scheme for automatic design of both network structures and loss functions. More specifically, we harness a unique dual research mechanism rooted in a novel weighted structure refinement architecture search. Besides, a hybrid supervised contrast constraint seamlessly guides and integrates with searching process, facilitating a more adaptive and comprehensive search for optimal loss functions. We realize the state-of-the-art performance in comparison to various competitive schemes, yielding a 10.61% and 4.38% improvement in Visual Information Fidelity (VIF) for general and no-reference scenarios, respectively, while providing results with high contrast, rich details and colors. The code is available at https://github.com/RollingPlain/HSDS_MEF",
    "checked": true,
    "id": "536f12f9da927a9deacf4bc5e51ebb08ef6c1632",
    "semantic_title": "hybrid-supervised dual-search: leveraging automatic learning for loss-free multi-exposure image fusion",
    "citation_count": 1,
    "authors": [
      "Guanyao Wu",
      "Hongming Fu",
      "Jinyuan Liu",
      "Long Ma",
      "Xin Fan",
      "Risheng Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28414": {
    "title": "When to Grow? A Fitting Risk-Aware Policy for Layer Growing in Deep Neural Networks",
    "volume": "main",
    "abstract": "Neural growth is the process of growing a small neural network to a large network and has been utilized to accelerate the training of deep neural networks. One crucial aspect of neural growth is determining the optimal growth timing. However, few studies investigate this systematically. Our study reveals that neural growth inherently exhibits a regularization effect, whose intensity is influenced by the chosen policy for growth timing. While this regularization effect may mitigate the overfitting risk of the model, it may lead to a notable accuracy drop when the model underfits. Yet, current approaches have not addressed this issue due to their lack of consideration of the regularization effect from neural growth. Motivated by these findings, we propose an under/over fitting risk-aware growth timing policy, which automatically adjusts the growth timing informed by the level of potential under/overfitting risks to address both risks. Comprehensive experiments conducted using CIFAR-10/100 and ImageNet datasets show that the proposed policy achieves accuracy improvements of up to 1.3% in models prone to underfitting while achieving similar accuracies in models suffering from overfitting compared to the existing methods",
    "checked": true,
    "id": "38ea00c22ed511ed377065449f6d6a783485a309",
    "semantic_title": "when to grow? a fitting risk-aware policy for layer growing in deep neural networks",
    "citation_count": 1,
    "authors": [
      "Haihang Wu",
      "Wei Wang",
      "Tamasha Malepathirana",
      "Damith Senanayake",
      "Denny Oetomo",
      "Saman Halgamuge"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28415": {
    "title": "p-Laplacian Adaptation for Generative Pre-trained Vision-Language Models",
    "volume": "main",
    "abstract": "Vision-Language models (VLMs) pre-trained on large corpora have demonstrated notable success across a range of downstream tasks. In light of the rapidly increasing size of pre-trained VLMs, parameter-efficient transfer learning (PETL) has garnered attention as a viable alternative to full fine-tuning. One such approach is the adapter, which introduces a few trainable parameters into the pre-trained models while preserving the original parameters during adaptation. In this paper, we present a novel modeling framework that recasts adapter tuning after attention as a graph message passing process on attention graphs, where the projected query and value features and attention matrix constitute the node features and the graph adjacency matrix, respectively. Within this framework, tuning adapters in VLMs necessitates handling heterophilic graphs, owing to the disparity between the projected query and value space. To address this challenge, we propose a new adapter architecture, p-adapter, which employs p-Laplacian message passing in Graph Neural Networks (GNNs). Specifically, the attention weights are re-normalized based on the features, and the features are then aggregated using the calibrated attention matrix, enabling the dynamic exploitation of information with varying frequencies in the heterophilic attention graphs. We conduct extensive experiments on different pre-trained VLMs and multi-modal tasks, including visual question answering, visual entailment, and image captioning. The experimental results validate our method's significant superiority over other PETL methods. Our code is available at https://github.com/wuhy68/p-Adapter/",
    "checked": true,
    "id": "47bc27e5f090c58f43c091789c2d06eeac601269",
    "semantic_title": "p-laplacian adaptation for generative pre-trained vision-language models",
    "citation_count": 0,
    "authors": [
      "Haoyuan Wu",
      "Xinyun Zhang",
      "Peng Xu",
      "Peiyu Liao",
      "Xufeng Yao",
      "Bei Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28416": {
    "title": "Task-Adaptive Prompted Transformer for Cross-Domain Few-Shot Learning",
    "volume": "main",
    "abstract": "Cross-Domain Few-Shot Learning (CD-FSL) aims at recognizing samples in novel classes from unseen domains that are vastly different from training classes, with few labeled samples. However, the large domain gap between training and novel classes makes previous FSL methods perform poorly. To address this issue, we propose MetaPrompt, a Task-adaptive Prompted Transformer model for CD-FSL, by jointly exploiting prompt learning and the parameter generation framework. The proposed MetaPrompt enjoys several merits. First, a task-conditioned prompt generator is established upon attention mechanisms. It can flexibly produce a task-adaptive prompt with arbitrary length for unseen tasks, by selectively gathering task characteristics from the contextualized support embeddings. Second, the task-adaptive prompt is attached to Vision Transformer to facilitate fast task adaptation, steering the task-agnostic representation to incorporate task knowledge. To our best knowledge, this is the first work to exploit a prompt-based parameter generation mechanism for CD-FSL. Extensive experimental results on the Meta-Dataset benchmark demonstrate that our method achieves superior results against state-of-the-art methods",
    "checked": true,
    "id": "78555152f5806f3ce54f9fd0ee12ae3c905b2cb8",
    "semantic_title": "task-adaptive prompted transformer for cross-domain few-shot learning",
    "citation_count": 0,
    "authors": [
      "Jiamin Wu",
      "Xin Liu",
      "Xiaotian Yin",
      "Tianzhu Zhang",
      "Yongdong Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28417": {
    "title": "SyFormer: Structure-Guided Synergism Transformer for Large-Portion Image Inpainting",
    "volume": "main",
    "abstract": "Image inpainting is in full bloom accompanied by the progress of convolutional neural networks (CNNs) and transformers, revolutionizing the practical management of abnormity disposal, image editing, etc. However, due to the ever-mounting image resolutions and missing areas, the challenges of distorted long-range dependencies from cluttered background distributions and reduced reference information in image domain inevitably rise, which further cause severe performance degradation. To address the challenges, we propose a novel large-portion image inpainting approach, namely the Structure-Guided Synergism Transformer (SyFormer), to rectify the discrepancies in feature representation and enrich the structural cues from limited reference. Specifically, we devise a dual-routing filtering module that employs a progressive filtering strategy to eliminate invalid noise interference and establish global-level texture correlations. Simultaneously, the structurally compact perception module maps an affinity matrix within the introduced structural priors from a structure-aware generator, assisting in matching and filling the corresponding patches of large-proportionally damaged images. Moreover, we carefully assemble the aforementioned modules to achieve feature complementarity. Finally, a feature decoding alignment scheme is introduced in the decoding process, which meticulously achieves texture amalgamation across hierarchical features. Extensive experiments are conducted on two publicly available datasets, i.e., CelebA-HQ and Places2, to qualitatively and quantitatively demonstrate the superiority of our model over state-of-the-arts",
    "checked": true,
    "id": "377450ae7c4c41aa048425b931fd4b95e76aaf49",
    "semantic_title": "syformer: structure-guided synergism transformer for large-portion image inpainting",
    "citation_count": 1,
    "authors": [
      "Jie Wu",
      "Yuchao Feng",
      "Honghui Xu",
      "Chuanmeng Zhu",
      "Jianwei Zheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28418": {
    "title": "MedSegDiff-V2: Diffusion-Based Medical Image Segmentation with Transformer",
    "volume": "main",
    "abstract": "The Diffusion Probabilistic Model (DPM) has recently gained popularity in the field of computer vision, thanks to its image generation applications, such as Imagen, Latent Diffusion Models, and Stable Diffusion, which have demonstrated impressive capabilities and sparked much discussion within the community. Recent investigations have further unveiled the utility of DPM in the domain of medical image analysis, as underscored by the commendable performance exhibited by the medical image segmentation model across various tasks. Although these models were originally underpinned by a UNet architecture, there exists a potential avenue for enhancing their performance through the integration of vision transformer mechanisms. However, we discovered that simply combining these two models resulted in subpar performance. To effectively integrate these two cutting-edge techniques for the Medical image segmentation, we propose a novel Transformer-based Diffusion framework, called MedSegDiff-V2. We verify its effectiveness on 20 medical image segmentation tasks with different image modalities. Through comprehensive evaluation, our approach demonstrates superiority over prior state-of-the-art (SOTA) methodologies. Code is released at https://github.com/KidsWithTokens/MedSegDiff",
    "checked": false,
    "id": "3f77a62ae888c3b816eabd354a6dd0fc6b9528ea",
    "semantic_title": "medsegdiff-v2: diffusion based medical image segmentation with transformer",
    "citation_count": 58,
    "authors": [
      "Junde Wu",
      "Wei Ji",
      "Huazhu Fu",
      "Min Xu",
      "Yueming Jin",
      "Yanwu Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28419": {
    "title": "Selective and Orthogonal Feature Activation for Pedestrian Attribute Recognition",
    "volume": "main",
    "abstract": "Pedestrian Attribute Recognition (PAR) involves identifying the attributes of individuals in person images. Existing PAR methods typically rely on CNNs as the backbone network to extract pedestrian features. However, CNNs process only one adjacent region at a time, leading to the loss of long-range inter-relations between different attribute-specific regions. To address this limitation, we leverage the Vision Transformer (ViT) instead of CNNs as the backbone for PAR, aiming to model long-range relations and extract more robust features. However, PAR suffers from an inherent attribute imbalance issue, causing ViT to naturally focus more on attributes that appear frequently in the training set and ignore some pedestrian attributes that appear less. The native features extracted by ViT are not able to tolerate the imbalance attribute distribution issue. To tackle this issue, we propose two novel components: the Selective Feature Activation Method (SFAM) and the Orthogonal Feature Activation Loss. SFAM smartly suppresses the more informative attribute-specific features, compelling the PAR model to capture discriminative features from regions that are easily overlooked. The proposed loss enforces an orthogonal constraint on the original feature extracted by ViT and the suppressed features from SFAM, promoting the complementarity of features in space. We conduct experiments on several benchmark PAR datasets, including PETA, PA100K, RAPv1, and RAPv2, demonstrating the effectiveness of our method. Specifically, our method outperforms existing state-of-the-art approaches by GRL, IAA-Caps, ALM, and SSC in terms of mA on the four datasets, respectively",
    "checked": true,
    "id": "34535c494cd45dbb50a9d6e60095aedff4d5a318",
    "semantic_title": "selective and orthogonal feature activation for pedestrian attribute recognition",
    "citation_count": 0,
    "authors": [
      "Junyi Wu",
      "Yan Huang",
      "Min Gao",
      "Yuzhen Niu",
      "Mingjing Yang",
      "Zhipeng Gao",
      "Jianqiang Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28420": {
    "title": "Swift-Mapping: Online Neural Implicit Dense Mapping in Urban Scenes",
    "volume": "main",
    "abstract": "Online dense mapping of urban scenes is of paramount importance for scene understanding of autonomous navigation. Traditional online dense mapping methods fuse sensor measurements (vision, lidar, etc.) across time and space via explicit geometric correspondence. Recently, NeRF-based methods have proved the superiority of neural implicit representations by high-fidelity reconstruction of large-scale city scenes. However, it remains an open problem how to integrate powerful neural implicit representations into online dense mapping. Existing methods are restricted to constrained indoor environments and are too computationally expensive to meet online requirements. To this end, we propose Swift-Mapping, an online neural implicit dense mapping framework in urban scenes. We introduce a novel neural implicit octomap (NIO) structure that provides efficient neural representation for large and dynamic urban scenes while retaining online update capability. Based on that, we propose an online neural dense mapping framework that effectively manages and updates neural octree voxel features. Our approach achieves SOTA reconstruction accuracy while being more than 10x faster in reconstruction speed, demonstrating the superior performance of our method in both accuracy and efficiency",
    "checked": true,
    "id": "f6f3c4e200b7a116cd70fcf5d8548e3f9674fd0e",
    "semantic_title": "swift-mapping: online neural implicit dense mapping in urban scenes",
    "citation_count": 0,
    "authors": [
      "Ke Wu",
      "Kaizhao Zhang",
      "Mingzhe Gao",
      "Jieru Zhao",
      "Zhongxue Gan",
      "Wenchao Ding"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28421": {
    "title": "CPN: Complementary Proposal Network for Unconstrained Text Detection",
    "volume": "main",
    "abstract": "Existing methods for scene text detection can be divided into two paradigms: segmentation-based and anchor-based. While Segmentation-based methods are well-suited for irregular shapes, they struggle with compact or overlapping layouts. Conversely, anchor-based approaches excel for complex layouts but suffer from irregular shapes. To strengthen their merits and overcome their respective demerits, we propose a Complementary Proposal Network (CPN) that seamlessly and parallelly integrates semantic and geometric information for superior performance. The CPN comprises two efficient networks for proposal generation: the Deformable Morphology Semantic Network, which generates semantic proposals employing an innovative deformable morphological operator, and the Balanced Region Proposal Network, which produces geometric proposals with pre-defined anchors. To further enhance the complementarity, we introduce an Interleaved Feature Attention module that enables semantic and geometric features to interact deeply before proposal generation. By leveraging both complementary proposals and features, CPN outperforms state-of-the-art approaches with significant margins under comparable computation cost. Specifically, our approach achieves improvements of 3.6%, 1.3% and 1.0% on challenging benchmarks ICDAR19-ArT, IC15, and MSRA-TD500, respectively. Code for our method will be released",
    "checked": true,
    "id": "1accf4f8f1b0b65abc099372036e3df905e8dfc2",
    "semantic_title": "cpn: complementary proposal network for unconstrained text detection",
    "citation_count": 0,
    "authors": [
      "Longhuang Wu",
      "Shangxuan Tian",
      "Youxin Wang",
      "Pengfei Xiong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28422": {
    "title": "Toward Open-Set Human Object Interaction Detection",
    "volume": "main",
    "abstract": "This work is oriented toward the task of open-set Human Object Interaction (HOI) detection. The challenge lies in identifying completely new, out-of-domain relationships, as opposed to in-domain ones which have seen improvements in zero-shot HOI detection. To address this challenge, we introduce a simple Disentangled HOI Detection (DHD) model for detecting novel relationships by integrating an open-set object detector with a Visual Language Model (VLM). We utilize a disentangled image-text contrastive learning metric for training and connect the bottom-up visual features to text embeddings through lightweight unary and pair-wise adapters. Our model can benefit from the open-set object detector and the VLM to detect novel action categories and combine actions with novel object categories. We further present the VG-HOI dataset, a comprehensive benchmark with over 17k HOI relationships for open-set scenarios. Experimental results show that our model can detect unknown action classes and combine unknown object classes. Furthermore, it can generalize to over 17k HOI classes while being trained on just 600 HOI classes",
    "checked": true,
    "id": "db40cc0e3f6f6cb2823cc54af91f6f228a223469",
    "semantic_title": "toward open-set human object interaction detection",
    "citation_count": 1,
    "authors": [
      "Mingrui Wu",
      "Yuqi Liu",
      "Jiayi Ji",
      "Xiaoshuai Sun",
      "Rongrong Ji"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28423": {
    "title": "VadCLIP: Adapting Vision-Language Models for Weakly Supervised Video Anomaly Detection",
    "volume": "main",
    "abstract": "The recent contrastive language-image pre-training (CLIP) model has shown great success in a wide range of image-level tasks, revealing remarkable ability for learning powerful visual representations with rich semantics. An open and worthwhile problem is efficiently adapting such a strong model to the video domain and designing a robust video anomaly detector. In this work, we propose VadCLIP, a new paradigm for weakly supervised video anomaly detection (WSVAD) by leveraging the frozen CLIP model directly without any pre-training and fine-tuning process. Unlike current works that directly feed extracted features into the weakly supervised classifier for frame-level binary classification, VadCLIP makes full use of fine-grained associations between vision and language on the strength of CLIP and involves dual branch. One branch simply utilizes visual features for coarse-grained binary classification, while the other fully leverages the fine-grained language-image alignment. With the benefit of dual branch, VadCLIP achieves both coarse-grained and fine-grained video anomaly detection by transferring pre-trained knowledge from CLIP to WSVAD task. We conduct extensive experiments on two commonly-used benchmarks, demonstrating that VadCLIP achieves the best performance on both coarse-grained and fine-grained WSVAD, surpassing the state-of-the-art methods by a large margin. Specifically, VadCLIP achieves 84.51% AP and 88.02% AUC on XD-Violence and UCF-Crime, respectively. Code and features are released at https://github.com/nwpu-zxr/VadCLIP",
    "checked": true,
    "id": "a58a1eb932372f70039dbfb0b49af84de855d18e",
    "semantic_title": "vadclip: adapting vision-language models for weakly supervised video anomaly detection",
    "citation_count": 16,
    "authors": [
      "Peng Wu",
      "Xuerong Zhou",
      "Guansong Pang",
      "Lingru Zhou",
      "Qingsen Yan",
      "Peng Wang",
      "Yanning Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28424": {
    "title": "Temporal Correlation Vision Transformer for Video Person Re-Identification",
    "volume": "main",
    "abstract": "Video Person Re-Identification (Re-ID) is a task of retrieving persons from multi-camera surveillance systems. Despite the progress made in leveraging spatio-temporal information in videos, occlusion in dense crowds still hinders further progress. To address this issue, we propose a Temporal Correlation Vision Transformer (TCViT) for video person Re-ID. TCViT consists of a Temporal Correlation Attention (TCA) module and a Learnable Temporal Aggregation (LTA) module. The TCA module is designed to reduce the impact of non-target persons by relative state, while the LTA module is used to aggregate frame-level features based on their completeness. Specifically, TCA is a parameter-free module that first aligns frame-level features to restore semantic coherence in videos and then enhances the features of the target person according to temporal correlation. Additionally, unlike previous methods that treat each frame equally with a pooling layer, LTA introduces a lightweight learnable module to weigh and aggregate frame-level features under the guidance of a classification score. Extensive experiments on four prevalent benchmarks demonstrate that our method achieves state-of-the-art performance in video Re-ID",
    "checked": true,
    "id": "2afdeb690d5b0ab29913332c67af97c7f0a09e30",
    "semantic_title": "temporal correlation vision transformer for video person re-identification",
    "citation_count": 0,
    "authors": [
      "Pengfei Wu",
      "Le Wang",
      "Sanping Zhou",
      "Gang Hua",
      "Changyin Sun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28425": {
    "title": "Point-to-Spike Residual Learning for Energy-Efficient 3D Point Cloud Classification",
    "volume": "main",
    "abstract": "Spiking neural networks (SNNs) have revolutionized neural learning and are making remarkable strides in image analysis and robot control tasks with ultra-low power consumption advantages. Inspired by this success, we investigate the application of spiking neural networks to 3D point cloud processing. We present a point-to-spike residual learning network for point cloud classification, which operates on points with binary spikes rather than floating-point numbers. Specifically, we first design a spatial-aware kernel point spiking neuron to relate spiking generation to point position in 3D space. On this basis, we then design a 3D spiking residual block for effective feature learning based on spike sequences. By stacking the 3D spiking residual blocks, we build the point-to-spike residual classification network, which achieves low computation cost and low accuracy loss on two benchmark datasets, ModelNet40 and ScanObjectNN. Moreover, the classifier strikes a good balance between classification accuracy and biological characteristics, allowing us to explore the deployment of 3D processing to neuromorphic chips for developing energy-efficient 3D robotic perception systems",
    "checked": true,
    "id": "e5e9120c1781f11631c2f51d4b151e41c9e6562d",
    "semantic_title": "point-to-spike residual learning for energy-efficient 3d point cloud classification",
    "citation_count": 0,
    "authors": [
      "Qiaoyun Wu",
      "Quanxiao Zhang",
      "Chunyu Tan",
      "Yun Zhou",
      "Changyin Sun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28426": {
    "title": "Segment beyond View: Handling Partially Missing Modality for Audio-Visual Semantic Segmentation",
    "volume": "main",
    "abstract": "Augmented Reality (AR) devices, emerging as prominent mobile interaction platforms, face challenges in user safety, particularly concerning oncoming vehicles. While some solutions leverage onboard camera arrays, these cameras often have limited field-of-view (FoV) with front or downward perspectives. Addressing this, we propose a new out-of-view semantic segmentation task and Segment Beyond View (SBV), a novel audio-visual semantic segmentation method. SBV supplements the visual modality, which miss the information beyond FoV, with the auditory information using a teacher-student distillation model (Omni2Ego). The model consists of a vision teacher utilising panoramic information, an auditory teacher with 8-channel audio, and an audio-visual student that takes views with limited FoV and binaural audio as input and produce semantic segmentation for objects outside FoV. SBV outperforms existing models in comparative evaluations and shows a consistent performance across varying FoV ranges and in monaural audio settings",
    "checked": true,
    "id": "953a94188f3113e0b13c5526a527b679d1bf2b44",
    "semantic_title": "segment beyond view: handling partially missing modality for audio-visual semantic segmentation",
    "citation_count": 1,
    "authors": [
      "Renjie Wu",
      "Hu Wang",
      "Feras Dayoub",
      "Hsiang-Ting Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28427": {
    "title": "Towards Transferable Adversarial Attacks with Centralized Perturbation",
    "volume": "main",
    "abstract": "Adversarial transferability enables black-box attacks on unknown victim deep neural networks (DNNs), rendering attacks viable in real-world scenarios. Current transferable attacks create adversarial perturbation over the entire image, resulting in excessive noise that overfit the source model. Concentrating perturbation to dominant image regions that are model-agnostic is crucial to improving adversarial efficacy. However, limiting perturbation to local regions in the spatial domain proves inadequate in augmenting transferability. To this end, we propose a transferable adversarial attack with fine-grained perturbation optimization in the frequency domain, creating centralized perturbation. We devise a systematic pipeline to dynamically constrain perturbation optimization to dominant frequency coefficients. The constraint is optimized in parallel at each iteration, ensuring the directional alignment of perturbation optimization with model prediction. Our approach allows us to centralize perturbation towards sample-specific important frequency features, which are shared by DNNs, effectively mitigating source model overfitting. Experiments demonstrate that by dynamically centralizing perturbation on dominating frequency coefficients, crafted adversarial examples exhibit stronger transferability, and allowing them to bypass various defenses",
    "checked": true,
    "id": "7e59fdd13e3e9c8387d2a124adf47c05a6aeda8c",
    "semantic_title": "towards transferable adversarial attacks with centralized perturbation",
    "citation_count": 4,
    "authors": [
      "Shangbo Wu",
      "Yu-an Tan",
      "Yajie Wang",
      "Ruinan Ma",
      "Wencong Ma",
      "Yuanzhang Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28428": {
    "title": "CLIM: Contrastive Language-Image Mosaic for Region Representation",
    "volume": "main",
    "abstract": "Detecting objects accurately from a large or open vocabulary necessitates the vision-language alignment on region representations. However, learning such a region-text alignment by obtaining high-quality box annotations with text labels or descriptions is expensive and infeasible. In contrast, collecting image-text pairs is simpler but lacks precise object location information to associate regions with texts. In this paper, we propose a novel approach called Contrastive Language-Image Mosaic (CLIM), which leverages large-scale image-text pairs effectively for aligning region and text representations. CLIM combines multiple images into a mosaicked image and treats each image as a ‘pseudo region'. The feature of each pseudo region is extracted and trained to be similar to the corresponding text embedding while dissimilar from others by a contrastive loss, enabling the model to learn the region-text alignment without costly box annotations. As a generally applicable approach, CLIM consistently improves different open-vocabulary object detection methods that use caption supervision. Furthermore, CLIM can effectively enhance the region representation of vision-language models, thus providing stronger backbones for open-vocabulary object detectors. Our experimental results demonstrate that CLIM improves different baseline open-vocabulary object detectors by a large margin on both OV-COCO and OV-LVIS benchmarks. The code is available at https://github.com/wusize/CLIM",
    "checked": true,
    "id": "cd59951ce99b68c3a398b90ebf826ecf6f9bb647",
    "semantic_title": "clim: contrastive language-image mosaic for region representation",
    "citation_count": 6,
    "authors": [
      "Size Wu",
      "Wenwei Zhang",
      "Lumin Xu",
      "Sheng Jin",
      "Wentao Liu",
      "Chen Change Loy"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28429": {
    "title": "SphereDiffusion: Spherical Geometry-Aware Distortion Resilient Diffusion Model",
    "volume": "main",
    "abstract": "Controllable spherical panoramic image generation holds substantial applicative potential across a variety of domains. However, it remains a challenging task due to the inherent spherical distortion and geometry characteristics, resulting in low-quality content generation. In this paper, we introduce a novel framework of SphereDiffusion to address these unique challenges, for better generating high-quality and precisely controllable spherical panoramic images. For the spherical distortion characteristic, we embed the semantics of the distorted object with text encoding, then explicitly construct the relationship with text-object correspondence to better use the pre-trained knowledge of the planar images. Meanwhile, we employ a deformable technique to mitigate the semantic deviation in latent space caused by spherical distortion. For the spherical geometry characteristic, in virtue of spherical rotation invariance, we improve the data diversity and optimization objectives in the training process, enabling the model to better learn the spherical geometry characteristic. Furthermore, we enhance the denoising process of the diffusion model, enabling it to effectively use the learned geometric characteristic to ensure the boundary continuity of the generated images. With these specific techniques, experiments on Structured3D dataset show that SphereDiffusion significantly improves the quality of controllable spherical image generation and relatively reduces around 35% FID on average",
    "checked": true,
    "id": "24cf69ed1f704a266abd6a456a8c8f2394284cbd",
    "semantic_title": "spherediffusion: spherical geometry-aware distortion resilient diffusion model",
    "citation_count": 0,
    "authors": [
      "Tao Wu",
      "Xuewei Li",
      "Zhongang Qi",
      "Di Hu",
      "Xintao Wang",
      "Ying Shan",
      "Xi Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28430": {
    "title": "LRS: Enhancing Adversarial Transferability through Lipschitz Regularized Surrogate",
    "volume": "main",
    "abstract": "The transferability of adversarial examples is of central importance to transfer-based black-box adversarial attacks. Previous works for generating transferable adversarial examples focus on attacking given pretrained surrogate models while the connections between surrogate models and adversarial trasferability have been overlooked. In this paper, we propose Lipschitz Regularized Surrogate (LRS) for transfer-based black-box attacks, a novel approach that transforms surrogate models towards favorable adversarial transferability. Using such transformed surrogate models, any existing transfer-based black-box attack can run without any change, yet achieving much better performance. Specifically, we impose Lipschitz regularization on the loss landscape of surrogate models to enable a smoother and more controlled optimization process for generating more transferable adversarial examples. In addition, this paper also sheds light on the connection between the inner properties of surrogate models and adversarial transferability, where three factors are identified: smaller local Lipschitz constant, smoother loss landscape, and stronger adversarial robustness. We evaluate our proposed LRS approach by attacking state-of-the-art standard deep neural networks and defense models. The results demonstrate significant improvement on the attack success rates and transferability. Our code is available at https://github.com/TrustAIoT/LRS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Wu",
      "Tie Luo",
      "Donald C. Wunsch II"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28431": {
    "title": "CR-SAM: Curvature Regularized Sharpness-Aware Minimization",
    "volume": "main",
    "abstract": "The capacity to generalize to future unseen data stands as one of the utmost crucial attributes of deep neural networks. Sharpness-Aware Minimization (SAM) aims to enhance the generalizability by minimizing worst-case loss using one-step gradient ascent as an approximation. However, as training progresses, the non-linearity of the loss landscape increases, rendering one-step gradient ascent less effective. On the other hand, multi-step gradient ascent will incur higher training cost. In this paper, we introduce a normalized Hessian trace to accurately measure the curvature of loss landscape on both training and test sets. In particular, to counter excessive non-linearity of loss landscape, we propose Curvature Regularized SAM (CR-SAM), integrating the normalized Hessian trace as a SAM regularizer. Additionally, we present an efficient way to compute the trace via finite differences with parallelism. Our theoretical analysis based on PAC-Bayes bounds establishes the regularizer's efficacy in reducing generalization error. Empirical evaluation on CIFAR and ImageNet datasets shows that CR-SAM consistently enhances classification performance for ResNet and Vision Transformer (ViT) models across various datasets. Our code is available at https://github.com/TrustAIoT/CR-SAM",
    "checked": true,
    "id": "f791ddf8078b633b8c3dcee1804e2a541d728417",
    "semantic_title": "cr-sam: curvature regularized sharpness-aware minimization",
    "citation_count": 0,
    "authors": [
      "Tao Wu",
      "Tie Luo",
      "Donald C. Wunsch II"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28432": {
    "title": "Semi-supervised 3D Object Detection with PatchTeacher and PillarMix",
    "volume": "main",
    "abstract": "Semi-supervised learning aims to leverage numerous unlabeled data to improve the model performance. Current semi-supervised 3D object detection methods typically use a teacher to generate pseudo labels for a student, and the quality of the pseudo labels is essential for the final performance. In this paper, we propose PatchTeacher, which focuses on partial scene 3D object detection to provide high-quality pseudo labels for the student. Specifically, we divide a complete scene into a series of patches and feed them to our PatchTeacher sequentially. PatchTeacher leverages the low memory consumption advantage of partial scene detection to process point clouds with a high-resolution voxelization, which can minimize the information loss of quantization and extract more fine-grained features. However, it is non-trivial to train a detector on fractions of the scene. Therefore, we introduce three key techniques, i.e., Patch Normalizer, Quadrant Align, and Fovea Selection, to improve the performance of PatchTeacher. Moreover, we devise PillarMix, a strong data augmentation strategy that mixes truncated pillars from different LiDAR scans to generate diverse training samples and thus help the model learn more general representation. Extensive experiments conducted on Waymo and ONCE datasets verify the effectiveness and superiority of our method and we achieve new state-of-the-art results, surpassing existing methods by a large margin. Codes are available at https://github.com/LittlePey/PTPM",
    "checked": true,
    "id": "9b4b77c766a30aaba0463289b7cca5b676cb719b",
    "semantic_title": "semi-supervised 3d object detection with patchteacher and pillarmix",
    "citation_count": 0,
    "authors": [
      "Xiaopei Wu",
      "Liang Peng",
      "Liang Xie",
      "Yuenan Hou",
      "Binbin Lin",
      "Xiaoshui Huang",
      "Haifeng Liu",
      "Deng Cai",
      "Wanli Ouyang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28433": {
    "title": "Text-Based Occluded Person Re-identification via Multi-Granularity Contrastive Consistency Learning",
    "volume": "main",
    "abstract": "Text-based Person Re-identification (T-ReID), which aims at retrieving a specific pedestrian image from a collection of images via text-based information, has received significant attention. However, previous research has overlooked a challenging yet practical form of T-ReID: dealing with image galleries mixed with occluded and inconsistent personal visuals, instead of ideal visuals with a full-body and clear view. Its major challenges lay in the insufficiency of benchmark datasets and the enlarged semantic gap incurred by arbitrary occlusions and modality gap between text description and visual representation of the target person. To alleviate these issues, we first design an Occlusion Generator (OGor) for the automatic generation of artificial occluded images from generic surveillance images. Then, a fine-granularity token selection mechanism is proposed to minimize the negative impact of occlusion for robust feature learning, and a novel multi-granularity contrastive consistency alignment framework is designed to leverage intra-/inter-granularity of visual-text representations for semantic alignment of occluded visuals and query texts. Experimental results demonstrate that our method exhibits superior performance. We believe this work could inspire the community to investigate more dedicated designs for implementing T-ReID in real-world scenarios. The source code is available at https://github.com/littlexinyi/MGCC",
    "checked": true,
    "id": "287bb3a5f8cec049af4d0a9f01b53cb35bdbc7af",
    "semantic_title": "text-based occluded person re-identification via multi-granularity contrastive consistency learning",
    "citation_count": 1,
    "authors": [
      "Xinyi Wu",
      "Wentao Ma",
      "Dan Guo",
      "Tongqing Zhou",
      "Shan Zhao",
      "Zhiping Cai"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28434": {
    "title": "CMG-Net: Robust Normal Estimation for Point Clouds via Chamfer Normal Distance and Multi-Scale Geometry",
    "volume": "main",
    "abstract": "This work presents an accurate and robust method for estimating normals from point clouds. In contrast to predecessor approaches that minimize the deviations between the annotated and the predicted normals directly, leading to direction inconsistency, we first propose a new metric termed Chamfer Normal Distance to address this issue. This not only mitigates the challenge but also facilitates network training and substantially enhances the network robustness against noise. Subsequently, we devise an innovative architecture that encompasses Multi-scale Local Feature Aggregation and Hierarchical Geometric Information Fusion. This design empowers the network to capture intricate geometric details more effectively and alleviate the ambiguity in scale selection. Extensive experiments demonstrate that our method achieves the state-of-the-art performance on both synthetic and real-world datasets, particularly in scenarios contaminated by noise. Our implementation is available at https://github.com/YingruiWoo/CMG-Net_Pytorch",
    "checked": true,
    "id": "7afe72f027f93223fff7bf9ca320b99a36013c9d",
    "semantic_title": "cmg-net: robust normal estimation for point clouds via chamfer normal distance and multi-scale geometry",
    "citation_count": 1,
    "authors": [
      "Yingrui Wu",
      "Mingyang Zhao",
      "Keqiang Li",
      "Weize Quan",
      "Tianqi Yu",
      "Jianfeng Yang",
      "Xiaohong Jia",
      "Dong-Ming Yan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28435": {
    "title": "WaveFormer: Wavelet Transformer for Noise-Robust Video Inpainting",
    "volume": "main",
    "abstract": "Video inpainting aims to fill in the missing regions of the video frames with plausible content. Benefiting from the outstanding long-range modeling capacity, the transformer-based models have achieved unprecedented performance regarding inpainting quality. Essentially, coherent contents from all the frames along both spatial and temporal dimensions are concerned by a patch-wise attention module, and then the missing contents are generated based on the attention-weighted summation. In this way, attention retrieval accuracy has become the main bottleneck to improve the video inpainting performance, where the factors affecting attention calculation should be explored to maximize the advantages of transformer. Towards this end, in this paper, we theoretically certificate that noise is the culprit that entangles the process of attention calculation. Meanwhile, we propose a novel wavelet transformer network with noise robustness for video inpainting, named WaveFormer. Unlike existing transformer-based methods that utilize the whole embeddings to calculate the attention, our WaveFormer first separates the noise existing in the embedding into high-frequency components by introducing the Discrete Wavelet Transform (DWT), and then adopts clean low-frequency components to calculate the attention. In this way, the impact of noise on attention computation can be greatly mitigated and the missing content regarding different frequencies can be generated by sharing the calculated attention. Extensive experiments validate the superior performance of our method over state-of-the-art baselines both qualitatively and quantitatively",
    "checked": true,
    "id": "ec727ad3f4eeb118406ad8a182c2e216151a7afb",
    "semantic_title": "waveformer: wavelet transformer for noise-robust video inpainting",
    "citation_count": 3,
    "authors": [
      "Zhiliang Wu",
      "Changchang Sun",
      "Hanyu Xuan",
      "Gaowen Liu",
      "Yan Yan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28436": {
    "title": "FD3D: Exploiting Foreground Depth Map for Feature-Supervised Monocular 3D Object Detection",
    "volume": "main",
    "abstract": "Monocular 3D object detection usually adopts direct or hierarchical label supervision. Recently, the distillation supervision transfers the spatial knowledge from LiDAR- or stereo-based teacher networks to monocular detectors, but remaining the domain gap. To mitigate this issue and pursue adequate label manipulation, we exploit Foreground Depth map for feature-supervised monocular 3D object detection named FD3D, which develops the high-quality instructive intermediate features to conduct desirable auxiliary feature supervision with only the original image and annotation foreground object-wise depth map (AFOD) as input. Furthermore, we build up our instructive feature generation network to create instructive spatial features based on the sufficient correlation between image features and pre-processed AFOD, where AFOD provides the attention focus only on foreground objects to achieve clearer guidance in the detection task. Moreover, we apply the auxiliary feature supervision from the pixel and distribution level to achieve comprehensive spatial knowledge guidance. Extensive experiments demonstrate that our method achieves state-of-the-art performance on both the KITTI and nuScenes datasets, with no external data and no extra inference computational cost. We also conduct quantitative and qualitative studies to reveal the effectiveness of our designs",
    "checked": true,
    "id": "9e4194ddfdd6cd0c9b717874ba70c457ec54f823",
    "semantic_title": "fd3d: exploiting foreground depth map for feature-supervised monocular 3d object detection",
    "citation_count": 0,
    "authors": [
      "Zizhang Wu",
      "Yuanzhu Gan",
      "Yunzhe Wu",
      "Ruihao Wang",
      "Xiaoquan Wang",
      "Jian Pu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28437": {
    "title": "Attention Disturbance and Dual-Path Constraint Network for Occluded Person Re-identification",
    "volume": "main",
    "abstract": "Occluded person re-identification (Re-ID) aims to address the potential occlusion problem when matching occluded or holistic pedestrians from different camera views. Many methods use the background as artificial occlusion and rely on attention networks to exclude noisy interference. However, the significant discrepancy between simple background occlusion and realistic occlusion can negatively impact the generalization of the network. To address this issue, we propose a novel transformer-based Attention Disturbance and Dual-Path Constraint Network (ADP) to enhance the generalization of attention networks. Firstly, to imitate real-world obstacles, we introduce an Attention Disturbance Mask (ADM) module that generates an offensive noise, which can distract attention like a realistic occluder, as a more complex form of occlusion. Secondly, to fully exploit these complex occluded images, we develop a DualPath Constraint Module (DPC) that can obtain preferable supervision information from holistic images through dualpath interaction. With our proposed method, the network can effectively circumvent a wide variety of occlusions using the basic ViT baseline. Comprehensive experimental evaluations conducted on person re-ID benchmarks demonstrate the superiority of ADP over state-of-the-art methods",
    "checked": true,
    "id": "330398a1e851110eb839dca6fd2d6469b7f4e9cc",
    "semantic_title": "attention disturbance and dual-path constraint network for occluded person re-identification",
    "citation_count": 1,
    "authors": [
      "Jiaer Xia",
      "Lei Tan",
      "Pingyang Dai",
      "Mingbo Zhao",
      "Yongjian Wu",
      "Liujuan Cao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28438": {
    "title": "Locality Preserving Refinement for Shape Matching with Functional Maps",
    "volume": "main",
    "abstract": "In this paper, we address the nonrigid shape matching with outliers by a novel and effective pointwise map refinement method, termed Locality Preserving Refinement. For accurate pointwise conversion from a given functional map, our method formulates a two-step procedure. Firstly, starting with noisy point-to-point correspondences, we identify inliers by leveraging the neighborhood support, which yields a closed-form solution with linear time complexity. After obtained the reliable correspondences of inliers, we refine the pointwise correspondences for outliers using local linear embedding, which operates in an adaptive spectral similarity space to further eliminate the ambiguities that are difficult to handle in the functional space. By refining pointwise correspondences with local consistency thus embedding geometric constraints into functional spaces, our method achieves considerable improvement in accuracy with linearithmic time and space cost. Extensive experiments on public benchmarks demonstrate the superiority of our method over the state-of-the-art methods. Our code is publicly available at https://github.com/XiaYifan1999/LOPR",
    "checked": true,
    "id": "4ee189554171c9a21a599746f64fa7b10502e932",
    "semantic_title": "locality preserving refinement for shape matching with functional maps",
    "citation_count": 0,
    "authors": [
      "Yifan Xia",
      "Yifan Lu",
      "Yuan Gao",
      "Jiayi Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28439": {
    "title": "SocialCVAE: Predicting Pedestrian Trajectory via Interaction Conditioned Latents",
    "volume": "main",
    "abstract": "Pedestrian trajectory prediction is the key technology in many applications for providing insights into human behavior and anticipating human future motions. Most existing empirical models are explicitly formulated by observed human behaviors using explicable mathematical terms with deterministic nature, while recent work has focused on developing hybrid models combined with learning-based techniques for powerful expressiveness while maintaining explainability. However, the deterministic nature of the learned steering behaviors from the empirical models limits the models' practical performance. To address this issue, this work proposes the social conditional variational autoencoder (SocialCVAE) for predicting pedestrian trajectories, which employs a CVAE to explore behavioral uncertainty in human motion decisions. SocialCVAE learns socially reasonable motion randomness by utilizing a socially explainable interaction energy map as the CVAE's condition, which illustrates the future occupancy of each pedestrian's local neighborhood area. The energy map is generated using an energy-based interaction model, which anticipates the energy cost (i.e., repulsion intensity) of pedestrians' interactions with neighbors. Experimental results on two public benchmarks including 25 scenes demonstrate that SocialCVAE significantly improves prediction accuracy compared with the state-of-the-art methods, with up to 16.85% improvement in Average Displacement Error (ADE) and 69.18% improvement in Final Displacement Error (FDE). Code is available at: https://github.com/ViviXiang/SocialCVAE",
    "checked": true,
    "id": "d583664c01c11ddba35430033a22fa1b2abaef7d",
    "semantic_title": "socialcvae: predicting pedestrian trajectory via interaction conditioned latents",
    "citation_count": 0,
    "authors": [
      "Wei Xiang",
      "Haoteng YIN",
      "He Wang",
      "Xiaogang Jin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28440": {
    "title": "Dynamic Semantic-Based Spatial Graph Convolution Network for Skeleton-Based Human Action Recognition",
    "volume": "main",
    "abstract": "Graph convolutional networks (GCNs) have attracted great attention and achieved remarkable performance in skeleton-based action recognition. However, most of the previous works are designed to refine skeleton topology without considering the types of different joints and edges, making them infeasible to represent the semantic information. In this paper, we proposed a dynamic semantic-based graph convolution network (DS-GCN) for skeleton-based human action recognition, where the joints and edge types were encoded in the skeleton topology in an implicit way. Specifically, two semantic modules, the joints type-aware adaptive topology and the edge type-aware adaptive topology, were proposed. Combining proposed semantics modules with temporal convolution, a powerful framework named DS-GCN was developed for skeleton-based action recognition. Extensive experiments in two datasets, NTU-RGB+D and Kinetics-400 show that the proposed semantic modules were generalized enough to be utilized in various backbones for boosting recognition accuracy. Meanwhile, the proposed DS-GCN notably outperformed state-of-the-art methods. The code is released here https://github.com/davelailai/DS-GCN",
    "checked": true,
    "id": "f01af0e8687d5af721a657b324d9e74f2264fc56",
    "semantic_title": "dynamic semantic-based spatial graph convolution network for skeleton-based human action recognition",
    "citation_count": 1,
    "authors": [
      "Jianyang Xie",
      "Yanda Meng",
      "Yitian Zhao",
      "Anh Nguyen",
      "Xiaoyun Yang",
      "Yalin Zheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28441": {
    "title": "G2P-DDM: Generating Sign Pose Sequence from Gloss Sequence with Discrete Diffusion Model",
    "volume": "main",
    "abstract": "The Sign Language Production (SLP) project aims to automatically translate spoken languages into sign sequences. Our approach focuses on the transformation of sign gloss sequences into their corresponding sign pose sequences (G2P). In this paper, we present a novel solution for this task by converting the continuous pose space generation problem into a discrete sequence generation problem. We introduce the Pose-VQVAE framework, which combines Variational Autoencoders (VAEs) with vector quantization to produce a discrete latent representation for continuous pose sequences. Additionally, we propose the G2P-DDM model, a discrete denoising diffusion architecture for length-varied discrete sequence data, to model the latent prior. To further enhance the quality of pose sequence generation in the discrete space, we present the CodeUnet model to leverage spatial-temporal information. Lastly, we develop a heuristic sequential clustering method to predict variable lengths of pose sequences for corresponding gloss sequences. Our results show that our model outperforms state-of-the-art G2P models on the public SLP evaluation benchmark. For more generated results, please visit our project page: https://slpdiffusier.github.io/g2p-ddm",
    "checked": true,
    "id": "dce74859a3a8573a2f5f59afae9bfde7b56d20df",
    "semantic_title": "g2p-ddm: generating sign pose sequence from gloss sequence with discrete diffusion model",
    "citation_count": 5,
    "authors": [
      "Pan Xie",
      "Qipeng Zhang",
      "Peng Taiying",
      "Hao Tang",
      "Yao Du",
      "Zexian Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28442": {
    "title": "Towards Understanding Future: Consistency Guided Probabilistic Modeling for Action Anticipation",
    "volume": "main",
    "abstract": "Action anticipation aims to infer the action in the unobserved segment (future segment) with the observed segment (past segment). Existing methods focus on learning key past semantics to predict the future, but they do not model the temporal continuity between the past and the future. However, past actions are always highly uncertain in anticipating the unobserved future. The absence of temporal continuity smoothing in the video's past-and-future segments may result in an inconsistent anticipation of future action. In this work, we aim to smooth the global semantics changes in the past and future segments. We propose a Consistency-guided Probabilistic Model (CPM), which focuses on learning the globally temporal probabilistic consistency to inhibit the unexpected temporal consistency. The CPM is deployed on the Transformer architecture, which includes three modules of future semantics estimation, global semantics estimation, and global distribution estimation involving the learning of past-to-future semantics, past-and-future semantics, and semantically probabilistic distributions. To achieve the smoothness of temporal continuity, we follow the principle of variational analysis and describe two probabilistic distributions, i.e., a past-aware distribution and a global-aware distribution, which help to estimate the evidence lower bound of future anticipation. In this study, we maximize the evidence lower bound of future semantics by reducing the distribution distance between the above two distributions for model optimization. Extensive experiments demonstrate that the effectiveness of our method and the CPM achieves state-of-the-art performance on Epic-Kitchen100, Epic-Kitchen55, and EGTEA-GAZE",
    "checked": true,
    "id": "10dc6d17bf160c51788bee46334b2ced1104c872",
    "semantic_title": "towards understanding future: consistency guided probabilistic modeling for action anticipation",
    "citation_count": 0,
    "authors": [
      "Zhao Xie",
      "Yadong Shi",
      "Kewei Wu",
      "Yaru Cheng",
      "Dan Guo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28443": {
    "title": "Towards Detailed Text-to-Motion Synthesis via Basic-to-Advanced Hierarchical Diffusion Model",
    "volume": "main",
    "abstract": "Text-guided motion synthesis aims to generate 3D human motion that not only precisely reflects the textual description but reveals the motion details as much as possible. Pioneering methods explore the diffusion model for text-to-motion synthesis and obtain significant superiority. However, these methods conduct diffusion processes either on the raw data distribution or the low-dimensional latent space, which typically suffer from the problem of modality inconsistency or detail-scarce. To tackle this problem, we propose a novel Basic-to-Advanced Hierarchical Diffusion Model, named B2A-HDM, to collaboratively exploit low-dimensional and high-dimensional diffusion models for high quality detailed motion synthesis. Specifically, the basic diffusion model in low-dimensional latent space provides the intermediate denoising result that to be consistent with the textual description, while the advanced diffusion model in high-dimensional latent space focuses on the following detail-enhancing denoising process. Besides, we introduce a multi-denoiser framework for the advanced diffusion model to ease the learning of high-dimensional model and fully explore the generative potential of the diffusion model. Quantitative and qualitative experiment results on two text-to-motion benchmarks (HumanML3D and KIT-ML) demonstrate that B2A-HDM can outperform existing state-of-the-art methods in terms of fidelity, modality consistency, and diversity",
    "checked": true,
    "id": "68425626b799169c4649603b4aed9fb59f039212",
    "semantic_title": "towards detailed text-to-motion synthesis via basic-to-advanced hierarchical diffusion model",
    "citation_count": 5,
    "authors": [
      "Zhenyu Xie",
      "Yang Wu",
      "Xuehao Gao",
      "Zhongqian Sun",
      "Wei Yang",
      "Xiaodan Liang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28444": {
    "title": "Learning by Erasing: Conditional Entropy Based Transferable Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "Detecting OOD inputs is crucial to deploy machine learning models to the real world safely. However, existing OOD detection methods require an in-distribution (ID) dataset to retrain the models. In this paper, we propose a Deep Generative Models (DGMs) based transferable OOD detection that does not require retraining on the new ID dataset. We first establish and substantiate two hypotheses on DGMs: DGMs exhibit a predisposition towards acquiring low-level features, in preference to semantic information; the lower bound of DGM's log-likelihoods is tied to the conditional entropy between the model input and target output. Drawing on the aforementioned hypotheses, we present an innovative image-erasing strategy, which is designed to create distinct conditional entropy distributions for each individual ID dataset. By training a DGM on a complex dataset with the proposed image-erasing strategy, the DGM could capture the discrepancy of conditional entropy distribution for varying ID datasets, without re-training. We validate the proposed method on the five datasets and show that, without retraining, our method achieves comparable performance to the state-of-the-art group-based OOD detection methods. The project codes will be open-sourced on our project website",
    "checked": true,
    "id": "85125a1c350654da4140c3372240a14082f68387",
    "semantic_title": "learning by erasing: conditional entropy based transferable out-of-distribution detection",
    "citation_count": 0,
    "authors": [
      "Meng Xing",
      "Zhiyong Feng",
      "Yong Su",
      "Changjae Oh"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28445": {
    "title": "Unsupervised Action Segmentation via Fast Learning of Semantically Consistent Actoms",
    "volume": "main",
    "abstract": "Action segmentation serves as a pivotal component in comprehending videos, encompassing the learning of a sequence of semantically consistent action units known as actoms. Conventional methodologies tend to require a significant consumption of time for both training and learning phases. This paper introduces an innovative unsupervised framework for action segmentation in video, characterized by its fast learning capability and absence of mandatory training. The core idea involves splitting the video into distinct actoms, which are then merging together based on shared actions. The key challenge here is to prevent the inadvertent creation of singular actoms that attempt to represent multiple actions during the splitting phase. Additionally, it is crucial to avoid situations where actoms associated with the same action are incorrectly grouped into multiple clusters during the merging phase. In this paper, we present a method for calculating the similarity between adjacent frames under a subspace assumption. Then, we employ a local minimum searching procedure, which effectively splits the video into coherent actoms aligned with their semantic meaning and provides us an action segmentation proposal. Subsequently, we calculate a spatio-temporal similarity between actoms, followed by developing a merging process to merge actoms representing identical actions within the action segmentation proposals. Our approach is evaluated on four benchmark datasets, and the results demonstrate that our method achieves state-of-the-art performance. Besides, our method also achieves the optimal balance between accuracy and learning time when compared to existing unsupervised techniques. Code is available at https://github.com/y66y/SaM",
    "checked": true,
    "id": "4441254c8a55bbe0936a7bb7f2935bad384bb23f",
    "semantic_title": "unsupervised action segmentation via fast learning of semantically consistent actoms",
    "citation_count": 1,
    "authors": [
      "Zheng Xing",
      "Weibing Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28446": {
    "title": "SPEAL: Skeletal Prior Embedded Attention Learning for Cross-Source Point Cloud Registration",
    "volume": "main",
    "abstract": "Point cloud registration, a fundamental task in 3D computer vision, has remained largely unexplored in cross-source point clouds and unstructured scenes. The primary challenges arise from noise, outliers, and variations in scale and density. However, neglected geometric natures of point clouds restricts the performance of current methods. In this paper, we propose a novel method termed SPEAL to leverage skeletal representations for effective learning of intrinsic topologies of point clouds, facilitating robust capture of geometric intricacy. Specifically, we design the Skeleton Extraction Module to extract skeleton points and skeletal features in an unsupervised manner, which is inherently robust to noise and density variances. Then, we propose the Skeleton-Aware GeoTransformer to encode high-level skeleton-aware features. It explicitly captures the topological natures and inter-point-cloud skeletal correlations with the noise-robust and density-invariant skeletal representations. Next, we introduce the Correspondence Dual-Sampler to facilitate correspondences by augmenting the correspondence set with skeletal correspondences. Furthermore, we construct a challenging novel cross-source point cloud dataset named KITTI CrossSource for benchmarking cross-source point cloud registration methods. Extensive quantitative and qualitative experiments are conducted to demonstrate our approach's superiority and robustness on both cross-source and same-source datasets. To the best of our knowledge, our approach is the first to facilitate point cloud registration with skeletal geometric priors",
    "checked": true,
    "id": "53d524485aa8572a0baf5f2a63bac717d5e581de",
    "semantic_title": "speal: skeletal prior embedded attention learning for cross-source point cloud registration",
    "citation_count": 2,
    "authors": [
      "Kezheng Xiong",
      "Maoji Zheng",
      "Qingshan Xu",
      "Chenglu Wen",
      "Siqi Shen",
      "Cheng Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28447": {
    "title": "Patched Line Segment Learning for Vector Road Mapping",
    "volume": "main",
    "abstract": "This paper presents a novel approach to computing vector road maps from satellite remotely sensed images, building upon a well-defined Patched Line Segment (PaLiS) representation for road graphs that holds geometric significance. Unlike prevailing methods that derive road vector representations from satellite images using binary masks or keypoints, our method employs line segments. These segments not only convey road locations but also capture their orientations, making them a robust choice for representation. More precisely, given an input image, we divide it into non-overlapping patches and predict a suitable line segment within each patch. This strategy enables us to capture spatial and structural cues from these patch-based line segments, simplifying the process of constructing the road network graph without the necessity of additional neural networks for connectivity. In our experiments, we demonstrate how an effective representation of a road graph significantly enhances the performance of vector road mapping on established benchmarks, without requiring extensive modifications to the neural network architecture. Furthermore, our method achieves state-of-the-art performance with just 6 GPU hours of training, leading to a substantial 32-fold reduction in training costs in terms of GPU hours",
    "checked": true,
    "id": "08d3ca3aac610be99ec46a444544b1b47170c1cb",
    "semantic_title": "patched line segment learning for vector road mapping",
    "citation_count": 1,
    "authors": [
      "Jiakun Xu",
      "Bowen Xu",
      "Gui-Song Xia",
      "Liang Dong",
      "Nan Xue"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28448": {
    "title": "MuLTI: Efficient Video-and-Language Understanding with Text-Guided MultiWay-Sampler and Multiple Choice Modeling",
    "volume": "main",
    "abstract": "Video-and-language understanding has a variety of applications in the industry, such as video question answering, text-video retrieval, and multi-label classification. Existing video-and-language understanding methods generally adopt heavy multi-modal encoders and feature fusion modules, which consume high computational costs. Specially, they have difficulty dealing with dense video frames or long text prevalent in industrial applications. This paper proposes MuLTI, a highly accurate and efficient video-and-language understanding model that achieves efficient and effective feature fusion and rapid adaptation to downstream tasks. Specifically, we design a Text-Guided MultiWay-Sampler based on adapt-pooling residual mapping and self-attention modules to sample long sequences and fuse multi-modal features, which reduces the computational costs and addresses performance degradation caused by previous samplers. Therefore, MuLTI can handle longer sequences with limited computational costs. Then, to further enhance the model's performance and fill in the lack of pretraining tasks in the video question answering, we propose a new pretraining task named Multiple Choice Modeling. This task bridges the gap between pretraining and downstream tasks and improves the model's ability to align video and text features. Benefiting from the efficient feature fusion module and the new pretraining task, MuLTI achieves state-of-the-art performance on multiple datasets. Implementation and pretrained models will be released",
    "checked": true,
    "id": "5694968f75064691b28c868c6985521fcd7d0c62",
    "semantic_title": "multi: efficient video-and-language understanding with text-guided multiway-sampler and multiple choice modeling",
    "citation_count": 1,
    "authors": [
      "Jiaqi Xu",
      "Bo Liu",
      "Yunkuo Chen",
      "Mengli Cheng",
      "Xing Shi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28449": {
    "title": "Regulating Intermediate 3D Features for Vision-Centric Autonomous Driving",
    "volume": "main",
    "abstract": "Multi-camera perception tasks have gained significant attention in the field of autonomous driving. However, existing frameworks based on Lift-Splat-Shoot (LSS) in the multi-camera setting cannot produce suitable dense 3D features due to the projection nature and uncontrollable densification process. To resolve this problem, we propose to regulate intermediate dense 3D features with the help of volume rendering. Specifically, we employ volume rendering to process the dense 3D features to obtain corresponding 2D features (e.g., depth maps, semantic maps), which are supervised by associated labels in the training. This manner regulates the generation of dense 3D features on the feature level, providing appropriate dense and unified features for multiple perception tasks. Therefore, our approach is termed Vampire, stands for ``Volume rendering As Multi-camera Perception Intermediate feature REgulator''. Experimental results on the Occ3D and nuScenes datasets demonstrate that Vampire facilitates fine-grained and appropriate extraction of dense 3D features, and is competitive with existing SOTA methods across diverse downstream perception tasks like 3D occupancy prediction, LiDAR segmentation and 3D objection detection, while utilizing moderate GPU resources. We provide a video demonstration in the supplementary materials and Codes are available at github.com/cskkxjk/Vampire",
    "checked": true,
    "id": "ea05f10e998d8c7c4037244984818b272c5e1d3a",
    "semantic_title": "regulating intermediate 3d features for vision-centric autonomous driving",
    "citation_count": 5,
    "authors": [
      "Junkai Xu",
      "Liang Peng",
      "Haoran Cheng",
      "Linxuan Xia",
      "Qi Zhou",
      "Dan Deng",
      "Wei Qian",
      "Wenxiao Wang",
      "Deng Cai"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28450": {
    "title": "ZOOM: Learning Video Mirror Detection with Extremely-Weak Supervision",
    "volume": "main",
    "abstract": "Mirror detection is an active research topic in computer vision. However, all existing mirror detectors learn mirror representations from large-scale pixel-wise datasets, which are tedious and expensive to obtain. Although weakly-supervised learning has been widely explored in related topics, we note that popular weak supervision signals (e.g., bounding boxes, scribbles, points) still require some efforts from the user to locate the target objects, with a strong assumption that the images to annotate always contain the target objects. Such an assumption may result in the over-segmentation of mirrors. Our key idea of this work is that the existence of mirrors over a time period may serve as a weak supervision to train a mirror detector, for two reasons. First, if a network can predict the existence of mirrors, it can essentially locate the mirrors. Second, we observe that the reflected contents of a mirror tend to be similar to those in adjacent frames, but exhibit considerable contrast to regions in far-away frames (e.g., non-mirror frames). To this end, in this paper, we propose ZOOM, the first method to learn robust mirror representations from extremely-weak annotations of per-frame ZerO-One Mirror indicators in videos. The key insight of ZOOM is to model the similarity and contrast (between mirror and non-mirror regions) in temporal variations to locate and segment the mirrors. To this end, we propose a novel fusion strategy to leverage temporal consistency information for mirror localization, and a novel temporal similarity-contrast modeling module for mirror segmentation. We construct a new video mirror dataset for training and evaluation. Experimental results under new and standard metrics show that ZOOM performs favorably against existing fully-supervised mirror detection methods",
    "checked": true,
    "id": "c6a79334f5521fbbd2f749d07d3f8aae0854ae91",
    "semantic_title": "zoom: learning video mirror detection with extremely-weak supervision",
    "citation_count": 0,
    "authors": [
      "Ke Xu",
      "Tsun Wai Siu",
      "Rynson W.H. Lau"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28451": {
    "title": "Weakly Supervised Multimodal Affordance Grounding for Egocentric Images",
    "volume": "main",
    "abstract": "To enhance the interaction between intelligent systems and the environment, locating the affordance regions of objects is crucial. These regions correspond to specific areas that provide distinct functionalities. Humans often acquire the ability to identify these regions through action demonstrations and verbal instructions. In this paper, we present a novel multimodal framework that extracts affordance knowledge from exocentric images, which depict human-object interactions, as well as from accompanying textual descriptions that describe the performed actions. The extracted knowledge is then transferred to egocentric images. To achieve this goal, we propose the HOI-Transfer Module, which utilizes local perception to disentangle individual actions within exocentric images. This module effectively captures localized features and correlations between actions, leading to valuable affordance knowledge. Additionally, we introduce the Pixel-Text Fusion Module, which fuses affordance knowledge by identifying regions in egocentric images that bear resemblances to the textual features defining affordances. We employ a Weakly Supervised Multimodal Affordance (WSMA) learning approach, utilizing image-level labels for training. Through extensive experiments, we demonstrate the superiority of our proposed method in terms of evaluation metrics and visual results when compared to existing affordance grounding models. Furthermore, ablation experiments confirm the effectiveness of our approach. Code:https://github.com/xulingjing88/WSMA",
    "checked": true,
    "id": "611e93aa3b9d2ab96fef181ec65bae38b4665bc6",
    "semantic_title": "weakly supervised multimodal affordance grounding for egocentric images",
    "citation_count": 0,
    "authors": [
      "Lingjing Xu",
      "Yang Gao",
      "Wenfeng Song",
      "Aimin Hao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28452": {
    "title": "Gaze from Origin: Learning for Generalized Gaze Estimation by Embedding the Gaze Frontalization Process",
    "volume": "main",
    "abstract": "Gaze estimation aims to accurately estimate the direction or position at which a person is looking. With the development of deep learning techniques, a number of gaze estimation methods have been proposed and achieved state-of-the-art performance. However, these methods are limited to within-dataset settings, whose performance drops when tested on unseen datasets. We argue that this is caused by infinite and continuous gaze labels. To alleviate this problem, we propose using gaze frontalization as an auxiliary task to constrain gaze estimation. Based on this, we propose a novel gaze domain generalization framework named Gaze Frontalization-based Auxiliary Learning (GFAL) Framework which embeds the gaze frontalization process, i.e., guiding the feature so that the eyeball can rotate and look at the front (camera), without any target domain information during training. Experimental results show that our proposed framework is able to achieve state-of-the-art performance on gaze domain generalization task, which is competitive with or even superior to the SOTA gaze unsupervised domain adaptation methods",
    "checked": true,
    "id": "c02af026c7268a6f2871ab66dee92920919ea806",
    "semantic_title": "gaze from origin: learning for generalized gaze estimation by embedding the gaze frontalization process",
    "citation_count": 0,
    "authors": [
      "Mingjie Xu",
      "Feng Lu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28453": {
    "title": "HACDR-Net: Heterogeneous-Aware Convolutional Network for Diabetic Retinopathy Multi-Lesion Segmentation",
    "volume": "main",
    "abstract": "Diabetic Retinopathy (DR), the leading cause of blindness in diabetic patients, is diagnosed by the condition of retinal multiple lesions. As a difficult task in medical image segmentation, DR multi-lesion segmentation faces the main concerns as follows. On the one hand, retinal lesions vary in location, shape, and size. On the other hand, because some lesions occupy only a very small part of the entire fundus image, the high proportion of background leads to difficulties in lesion segmentation. To solve the above problems, we propose a heterogeneous-aware convolutional network (HACDR-Net) that composes heterogeneous cross-convolution, heterogeneous modulated deformable convolution, and optional near-far-aware convolution. Our network introduces an adaptive aggregation module to summarize the heterogeneous feature maps and get diverse lesion areas in the heterogeneous receptive field along the channels and space. In addition, to solve the problem of the highly imbalanced proportion of focal areas, we design a new medical image segmentation loss function, Noise Adjusted Loss (NALoss). NALoss balances the predictive feature distribution of background and lesion by jointing Gaussian noise and hard example mining, thus enhancing awareness of lesions. We conduct the experiments on the public datasets IDRiD and DDR, and the experimental results show that the proposed method achieves better performance than other state-of-the-art methods. The code is open-sourced on github.com/xqh180110910537/HACDR-Net",
    "checked": true,
    "id": "6fd05d4cca24b3d10df74903318ef1dee5c04460",
    "semantic_title": "hacdr-net: heterogeneous-aware convolutional network for diabetic retinopathy multi-lesion segmentation",
    "citation_count": 0,
    "authors": [
      "QiHao Xu",
      "Xiaoling Luo",
      "Chao Huang",
      "Chengliang Liu",
      "Jie Wen",
      "Jialei Wang",
      "Yong Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28454": {
    "title": "Learning Invariant Inter-pixel Correlations for Superpixel Generation",
    "volume": "main",
    "abstract": "Deep superpixel algorithms have made remarkable strides by substituting hand-crafted features with learnable ones. Nevertheless, we observe that existing deep superpixel methods, serving as mid-level representation operations, remain sensitive to the statistical properties (e.g., color distribution, high-level semantics) embedded within the training dataset. Consequently, learnable features exhibit constrained discriminative capability, resulting in unsatisfactory pixel grouping performance, particularly in untrainable application scenarios. To address this issue, we propose the Content Disentangle Superpixel (CDS) algorithm to selectively separate the invariant inter-pixel correlations and statistical properties, i.e., style noise. Specifically, We first construct auxiliary modalities that are homologous to the original RGB image but have substantial stylistic variations. Then, driven by mutual information, we propose the local-grid correlation alignment across modalities to reduce the distribution discrepancy of adaptively selected features and learn invariant inter-pixel correlations. Afterwards, we perform global-style mutual information minimization to enforce the separation of invariant content and train data styles. The experimental results on four benchmark datasets demonstrate the superiority of our approach to existing state-of-the-art methods, regarding boundary adherence, generalization, and efficiency. Code and pre-trained model are available at https://github.com/rookiie/CDSpixel",
    "checked": true,
    "id": "e64aa71e6ccaf364dcae726cd8449e820624c141",
    "semantic_title": "learning invariant inter-pixel correlations for superpixel generation",
    "citation_count": 0,
    "authors": [
      "Sen Xu",
      "Shikui Wei",
      "Tao Ruan",
      "Lixin Liao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28455": {
    "title": "Direction-Aware Video Demoiréing with Temporal-Guided Bilateral Learning",
    "volume": "main",
    "abstract": "Moiré patterns occur when capturing images or videos on screens, severely degrading the quality of the captured images or videos. Despite the recent progresses, existing video demoiréing methods neglect the physical characteristics and formation process of moiré patterns, significantly limiting the effectiveness of video recovery. This paper presents a unified framework, DTNet, a direction-aware and temporal-guided bilateral learning network for video demoiréing. DTNet effectively incorporates the process of moiré pattern removal, alignment, color correction, and detail refinement. Our proposed DTNet comprises two primary stages: Frame-level Direction-aware Demoiréing and Alignment (FDDA) and Tone and Detail Refinement (TDR). In FDDA, we employ multiple directional DCT modes to perform the moiré pattern removal process in the frequency domain, effectively detecting the prominent moiré edges. Then, the coarse and fine-grained alignment is applied on the demoiréd features for facilitating the utilization of neighboring information. In TDR, we propose a temporal-guided bilateral learning pipeline to mitigate the degradation of color and details caused by the moiré patterns while preserving the restored frequency information in FDDA. Guided by the aligned temporal features from FDDA, the affine transformations for the recovery of the ultimate clean frames are learned in TDR. Extensive experiments demonstrate that our video demoiréing method outperforms state-of-the-art approaches by 2.3 dB in PSNR, and also delivers a superior visual experience",
    "checked": false,
    "id": "955fc599a30227ee5af7c6c7338250dca0a05e04",
    "semantic_title": "direction-aware video demoireing with temporal-guided bilateral learning",
    "citation_count": 2,
    "authors": [
      "Shuning Xu",
      "Binbin Song",
      "Xiangyu Chen",
      "Jiantao Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28456": {
    "title": "Spectral Prompt Tuning: Unveiling Unseen Classes for Zero-Shot Semantic Segmentation",
    "volume": "main",
    "abstract": "Recently, CLIP has found practical utility in the domain of pixel-level zero-shot segmentation tasks. The present landscape features two-stage methodologies beset by issues such as intricate pipelines and elevated computational costs. While current one-stage approaches alleviate these concerns and incorporate Visual Prompt Training (VPT) to uphold CLIP's generalization capacity, they still fall short in fully harnessing CLIP's potential for pixel-level unseen class demarcation and precise pixel predictions. To further stimulate CLIP's zero-shot dense prediction capability, we propose SPT-SEG, a one-stage approach that improves CLIP's adaptability from image to pixel. Specifically, we initially introduce Spectral Prompt Tuning (SPT), incorporating spectral prompts into the CLIP visual encoder's shallow layers to capture structural intricacies of images, thereby enhancing comprehension of unseen classes. Subsequently, we introduce the Spectral Guided Decoder (SGD), utilizing both high and low-frequency information to steer the network's spatial focus towards more prominent classification features, enabling precise pixel-level prediction outcomes. Through extensive experiments on two public datasets, we demonstrate the superiority of our method over state-of-the-art approaches, performing well across all classes and particularly excelling in handling unseen classes",
    "checked": true,
    "id": "3032ff7224274d8792aabd4003f0c63ac42389b6",
    "semantic_title": "spectral prompt tuning: unveiling unseen classes for zero-shot semantic segmentation",
    "citation_count": 2,
    "authors": [
      "Wenhao Xu",
      "Rongtao Xu",
      "Changwei Wang",
      "Shibiao Xu",
      "Li Guo",
      "Man Zhang",
      "Xiaopeng Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28457": {
    "title": "SCTNet: Single-Branch CNN with Transformer Semantic Information for Real-Time Segmentation",
    "volume": "main",
    "abstract": "Recent real-time semantic segmentation methods usually adopt an additional semantic branch to pursue rich long-range context. However, the additional branch incurs undesirable computational overhead and slows inference speed. To eliminate this dilemma, we propose SCTNet, a single branch CNN with transformer semantic information for real-time segmentation. SCTNet enjoys the rich semantic representations of an inference-free semantic branch while retaining the high efficiency of lightweight single branch CNN. SCTNet utilizes a transformer as the training-only semantic branch considering its superb ability to extract long-range context. With the help of the proposed transformer-like CNN block CFBlock and the semantic information alignment module, SCTNet could capture the rich semantic information from the transformer branch in training. During the inference, only the single branch CNN needs to be deployed. We conduct extensive experiments on Cityscapes, ADE20K, and COCO-Stuff-10K, and the results show that our method achieves the new state-of-the-art performance. The code and model is available at https://github.com/xzz777/SCTNet",
    "checked": true,
    "id": "27b44b66fdbdeebe3284a48587428dc439a33d79",
    "semantic_title": "sctnet: single-branch cnn with transformer semantic information for real-time segmentation",
    "citation_count": 3,
    "authors": [
      "Zhengze Xu",
      "Dongyue Wu",
      "Changqian Yu",
      "Xiangxiang Chu",
      "Nong Sang",
      "Changxin Gao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28458": {
    "title": "Chain of Generation: Multi-Modal Gesture Synthesis via Cascaded Conditional Control",
    "volume": "main",
    "abstract": "This study aims to improve the generation of 3D gestures by utilizing multimodal information from human speech. Previous studies have focused on incorporating additional modalities to enhance the quality of generated gestures. However, these methods perform poorly when certain modalities are missing during inference. To address this problem, we suggest using speech-derived multimodal priors to improve gesture generation. We introduce a novel method that separates priors from speech and employs multimodal priors as constraints for generating gestures. Our approach utilizes a chain-like modeling method to generate facial blendshapes, body movements, and hand gestures sequentially. Specifically, we incorporate rhythm cues derived from facial deformation and stylization prior based on speech emotions, into the process of generating gestures. By incorporating multimodal priors, our method improves the quality of generated gestures and eliminate the need for expensive setup preparation during inference. Extensive experiments and user studies confirm that our proposed approach achieves state-of-the-art performance",
    "checked": true,
    "id": "4d95c58b9779b0e585d4a8cf6afae509ca176e00",
    "semantic_title": "chain of generation: multi-modal gesture synthesis via cascaded conditional control",
    "citation_count": 4,
    "authors": [
      "Zunnan Xu",
      "Yachao Zhang",
      "Sicheng Yang",
      "Ronghui Li",
      "Xiu Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28459": {
    "title": "Decoupled Contrastive Learning for Long-Tailed Recognition",
    "volume": "main",
    "abstract": "Supervised Contrastive Loss (SCL) is popular in visual representation learning. Given an anchor image, SCL pulls two types of positive samples, i.e., its augmentation and other images from the same class together, while pushes negative images apart to optimize the learned embedding. In the scenario of long-tailed recognition, where the number of samples in each class is imbalanced, treating two types of positive samples equally leads to the biased optimization for intra-category distance. In addition, similarity relationship among negative samples, that are ignored by SCL, also presents meaningful semantic cues. To improve the performance on long-tailed recognition, this paper addresses those two issues of SCL by decoupling the training objective. Specifically, it decouples two types of positives in SCL and optimizes their relations toward different objectives to alleviate the influence of the imbalanced dataset. We further propose a patch-based self distillation to transfer knowledge from head to tail classes to relieve the under-representation of tail classes. It uses patch-based features to mine shared visual patterns among different instances and leverages a self distillation procedure to transfer such knowledge. Experiments on different long-tailed classification benchmarks demonstrate the superiority of our method. For instance, it achieves the 57.7% top-1 accuracy on the ImageNet-LT dataset. Combined with the ensemble-based method, the performance can be further boosted to 59.7%, which substantially outperforms many recent works. Our code will be released",
    "checked": true,
    "id": "83d85f92f7f21a3bdef232da28b7f0f5d68077cd",
    "semantic_title": "decoupled contrastive learning for long-tailed recognition",
    "citation_count": 0,
    "authors": [
      "Shiyu Xuan",
      "Shiliang Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28460": {
    "title": "Revisiting Gradient Pruning: A Dual Realization for Defending against Gradient Attacks",
    "volume": "main",
    "abstract": "Collaborative learning (CL) is a distributed learning framework that aims to protect user privacy by allowing users to jointly train a model by sharing their gradient updates only. However, gradient inversion attacks (GIAs), which recover users' training data from shared gradients, impose severe privacy threats to CL. Existing defense methods adopt different techniques, e.g., differential privacy, cryptography, and perturbation defenses, to defend against the GIAs. Nevertheless, all current defense methods suffer from a poor trade-off between privacy, utility, and efficiency. To mitigate the weaknesses of existing solutions, we propose a novel defense method, Dual Gradient Pruning (DGP), based on gradient pruning, which can improve communication efficiency while preserving the utility and privacy of CL. Specifically, DGP slightly changes gradient pruning with a stronger privacy guarantee. And DGP can also significantly improve communication efficiency with a theoretical analysis of its convergence and generalization. Our extensive experiments show that DGP can effectively defend against the most powerful GIAs and reduce the communication cost without sacrificing the model's utility",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lulu Xue",
      "Shengshan Hu",
      "Ruizhi Zhao",
      "Leo Yu Zhang",
      "Shengqing Hu",
      "Lichao Sun",
      "Dezhong Yao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28461": {
    "title": "A Convolutional Neural Network Interpretable Framework for Human Ventral Visual Pathway Representation",
    "volume": "main",
    "abstract": "Recently, convolutional neural networks (CNNs) have become the best quantitative encoding models for capturing neural activity and hierarchical structure in the ventral visual pathway. However, the weak interpretability of these black-box models hinders their ability to reveal visual representational encoding mechanisms. Here, we propose a convolutional neural network interpretable framework (CNN-IF) aimed at providing a transparent interpretable encoding model for the ventral visual pathway. First, we adapt the feature-weighted receptive field framework to train two high-performing ventral visual pathway encoding models using large-scale functional Magnetic Resonance Imaging (fMRI) in both goal-driven and data-driven approaches. We find that network layer-wise predictions align with the functional hierarchy of the ventral visual pathway. Then, we correspond feature units to voxel units in the brain and successfully quantify the alignment between voxel responses and visual concepts. Finally, we conduct Network Dissection along the ventral visual pathway including the fusiform face area (FFA), and discover variations related to the visual concept of `person'. Our results demonstrate the CNN-IF provides a new perspective for understanding encoding mechanisms in the human ventral visual pathway, and the combination of ante-hoc interpretable structure and post-hoc interpretable approaches can achieve fine-grained voxel-wise correspondence between model and brain. The source code is available at: https://github.com/BIT-YangLab/CNN-IF",
    "checked": true,
    "id": "9c0f0ae771bdd0954c343d419b9ecf288e2c6211",
    "semantic_title": "a convolutional neural network interpretable framework for human ventral visual pathway representation",
    "citation_count": 0,
    "authors": [
      "Mufan Xue",
      "Xinyu Wu",
      "Jinlong Li",
      "Xuesong Li",
      "Guoyuan Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28462": {
    "title": "Self-Supervised 3D Human Mesh Recovery from a Single Image with Uncertainty-Aware Learning",
    "volume": "main",
    "abstract": "Despite achieving impressive improvement in accuracy, most existing monocular 3D human mesh reconstruction methods require large-scale 2D/3D ground-truths for supervision, which limits their applications on unlabeled in-the-wild data that is ubiquitous. To alleviate the reliance on 2D/3D ground-truths, we present a self-supervised 3D human pose and shape reconstruction framework that relies only on self-consistency between intermediate representations of images and projected 2D predictions. Specifically, we extract 2D joints and depth maps from monocular images as proxy inputs, which provides complementary clues to infer accurate 3D human meshes. Furthermore, to reduce the impacts from noisy and ambiguous inputs while better concentrate on the high-quality information, we design an uncertainty-aware module to automatically learn the reliability of the inputs at body-joint level based on the consistency between 2D joints and depth map. Experiments on benchmark datasets show that our approach outperforms other state-of-the-art methods at similar supervision levels",
    "checked": true,
    "id": "fae1e162c02f9fedb27a4ebaa6814556d90e0594",
    "semantic_title": "self-supervised 3d human mesh recovery from a single image with uncertainty-aware learning",
    "citation_count": 0,
    "authors": [
      "Guoli Yan",
      "Zichun Zhong",
      "Jing Hua"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28463": {
    "title": "HORIZON: High-Resolution Semantically Controlled Panorama Synthesis",
    "volume": "main",
    "abstract": "Panorama synthesis endeavors to craft captivating 360-degree visual landscapes, immersing users in the heart of virtual worlds. Nevertheless, contemporary panoramic synthesis techniques grapple with the challenge of semantically guiding the content generation process. Although recent breakthroughs in visual synthesis have unlocked the potential for semantic control in 2D flat images, a direct application of these methods to panorama synthesis yields distorted content. In this study, we unveil an innovative framework for generating high-resolution panoramas, adeptly addressing the issues of spherical distortion and edge discontinuity through sophisticated spherical modeling. Our pioneering approach empowers users with semantic control, harnessing both image and text inputs, while concurrently streamlining the generation of high-resolution panoramas using parallel decoding. We rigorously evaluate our methodology on a diverse array of indoor and outdoor datasets, establishing its superiority over recent related work, in terms of both quantitative and qualitative performance metrics. Our research elevates the controllability, efficiency, and fidelity of panorama synthesis to new levels",
    "checked": true,
    "id": "ac744513f9c2f0d00085fddda2cbffc48a7f6b18",
    "semantic_title": "horizon: high-resolution semantically controlled panorama synthesis",
    "citation_count": 0,
    "authors": [
      "Kun Yan",
      "Lei Ji",
      "Chenfei Wu",
      "Jian Liang",
      "Ming Zhou",
      "Nan Duan",
      "Shuai Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28464": {
    "title": "CF-NeRF: Camera Parameter Free Neural Radiance Fields with Incremental Learning",
    "volume": "main",
    "abstract": "Neural Radiance Fields have demonstrated impressive performance in novel view synthesis. However, NeRF and most of its variants still rely on traditional complex pipelines to provide extrinsic and intrinsic camera parameters, such as COLMAP. Recent works, like NeRFmm, BARF, and L2G-NeRF, directly treat camera parameters as learnable and estimate them through differential volume rendering. However, these methods work for forward-looking scenes with slight motions and fail to tackle the rotation scenario in practice. To overcome this limitation, we propose a novel camera parameter free neural radiance field (CF-NeRF), which incrementally reconstructs 3D representations and recovers the camera parameters inspired by incremental structure from motion. Given a sequence of images, CF-NeRF estimates camera parameters of images one by one and reconstructs the scene through initialization, implicit localization, and implicit optimization. To evaluate our method, we use a challenging real-world dataset, NeRFBuster, which provides 12 scenes under complex trajectories. Results demonstrate that CF-NeRF is robust to rotation and achieves state-of-the-art results without providing prior information and constraints",
    "checked": true,
    "id": "3c3ca076611e0f61db7a957b902191c3502584f1",
    "semantic_title": "cf-nerf: camera parameter free neural radiance fields with incremental learning",
    "citation_count": 2,
    "authors": [
      "Qingsong Yan",
      "Qiang Wang",
      "Kaiyong Zhao",
      "Jie Chen",
      "Bo Li",
      "Xiaowen Chu",
      "Fei Deng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28465": {
    "title": "Referred by Multi-Modality: A Unified Temporal Transformer for Video Object Segmentation",
    "volume": "main",
    "abstract": "Recently, video object segmentation (VOS) referred by multi-modal signals, e.g., language and audio, has evoked increasing attention in both industry and academia. It is challenging for exploring the semantic alignment within modalities and the visual correspondence across frames. However, existing methods adopt separate network architectures for different modalities, and neglect the inter-frame temporal interaction with references. In this paper, we propose MUTR, a Multi-modal Unified Temporal transformer for Referring video object segmentation. With a unified framework for the first time, MUTR adopts a DETR-style transformer and is capable of segmenting video objects designated by either text or audio reference. Specifically, we introduce two strategies to fully explore the temporal relations between videos and multi-modal signals. Firstly, for low-level temporal aggregation before the transformer, we enable the multi-modal references to capture multi-scale visual cues from consecutive video frames. This effectively endows the text or audio signals with temporal knowledge and boosts the semantic alignment between modalities. Secondly, for high-level temporal interaction after the transformer, we conduct inter-frame feature communication for different object embeddings, contributing to better object-wise correspondence for tracking along the video. On Ref-YouTube-VOS and AVSBench datasets with respective text and audio references, MUTR achieves +4.2% and +8.7% J&F improvements to state-of-the-art methods, demonstrating our significance for unified multi-modal VOS. Code is released at https://github.com/OpenGVLab/MUTR",
    "checked": true,
    "id": "24f9c0127dc3033e34eb9dd4ceb562ccf9bb85ad",
    "semantic_title": "referred by multi-modality: a unified temporal transformer for video object segmentation",
    "citation_count": 12,
    "authors": [
      "Shilin Yan",
      "Renrui Zhang",
      "Ziyu Guo",
      "Wenchao Chen",
      "Wei Zhang",
      "Hongyang Li",
      "Yu Qiao",
      "Hao Dong",
      "Zhongjiang He",
      "Peng Gao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28466": {
    "title": "Embracing Language Inclusivity and Diversity in CLIP through Continual Language Learning",
    "volume": "main",
    "abstract": "While vision-language pre-trained models (VL-PTMs) have advanced multimodal research in recent years, their mastery in a few languages like English restricts their applicability in broader communities. To this end, there is an increasing interest in developing multilingual VL models via a joint-learning setup, which, however, could be unrealistic due to expensive costs and data availability. In this work, we propose to extend VL-PTMs' language capacity by continual language learning (CLL), where a model needs to update its linguistic knowledge incrementally without suffering from catastrophic forgetting (CF). We begin our study by introducing a model dubbed CLL-CLIP, which builds upon CLIP, a prevailing VL-PTM that has acquired image-English text alignment. Specifically, CLL-CLIP contains an expandable token embedding layer to handle linguistic differences. It solely trains token embeddings to improve memory stability and is optimized under cross-modal and cross-lingual objectives to learn the alignment between images and multilingual texts. To alleviate CF raised by covariate shift and lexical overlap, we further propose a novel approach that ensures the identical distribution of all token embeddings during initialization and regularizes token embedding learning during training. We construct a CLL benchmark covering 36 languages based on MSCOCO and XM3600 datasets and then evaluate multilingual image-text retrieval performance. Extensive experiments verify the effectiveness of CLL-CLIP and show that our approach can boost CLL-CLIP, e.g., by 6.7% in text-to-image average Recall@1 on XM3600, and improve various state-of-the-art methods consistently. Our code and data are available at https://github.com/yangbang18/CLFM",
    "checked": true,
    "id": "a01ee82dd1afc792ea89a84d3ed2a6ab37c65ed2",
    "semantic_title": "embracing language inclusivity and diversity in clip through continual language learning",
    "citation_count": 1,
    "authors": [
      "Bang Yang",
      "Yong Dai",
      "Xuxin Cheng",
      "Yaowei Li",
      "Asif Raza",
      "Yuexian Zou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28467": {
    "title": "Geometry-Guided Domain Generalization for Monocular 3D Object Detection",
    "volume": "main",
    "abstract": "Monocular 3D object detection (M3OD) is important for autonomous driving. However, existing deep learning-based methods easily suffer from performance degradation in real-world scenarios due to the substantial domain gap between training and testing. M3OD's domain gaps are complex, including camera intrinsic parameters, extrinsic parameters, image appearance, etc. Existing works primarily focus on the domain gaps of camera intrinsic parameters, ignoring other key factors. Moreover, at the feature level, conventional domain invariant learning methods generally cause the negative transfer issue, due to the ignorance of dependency between geometry tasks and domains. To tackle these issues, in this paper, we propose MonoGDG, a geometry-guided domain generalization framework for M3OD, which effectively addresses the domain gap at both camera and feature levels. Specifically, MonoGDG consists of two major components. One is geometry-based image reprojection, which mitigates the impact of camera discrepancy by unifying intrinsic parameters, randomizing camera orientations, and unifying the field of view range. The other is geometry-dependent feature disentanglement, which overcomes the negative transfer problems by incorporating domain-shared and domain-specific features. Additionally, we leverage a depth-disentangled domain discriminator and a domain-aware geometry regression attention mechanism to account for the geometry-domain dependency. Extensive experiments on multiple autonomous driving benchmarks demonstrate that our method achieves state-of-the-art performance in domain generalization for M3OD",
    "checked": true,
    "id": "482672f3ce4ef1a6b851e0bc95dfd44dd3356e23",
    "semantic_title": "geometry-guided domain generalization for monocular 3d object detection",
    "citation_count": 0,
    "authors": [
      "Fan Yang",
      "Hui Chen",
      "Yuwei He",
      "Sicheng Zhao",
      "Chenghao Zhang",
      "Kai Ni",
      "Guiguang Ding"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28468": {
    "title": "Diversity-Authenticity Co-constrained Stylization for Federated Domain Generalization in Person Re-identification",
    "volume": "main",
    "abstract": "This paper tackles the problem of federated domain generalization in person re-identification (FedDG re-ID), aiming to learn a model generalizable to unseen domains with decentralized source domains. Previous methods mainly focus on preventing local overfitting. However, the direction of diversifying local data through stylization for model training is largely overlooked. This direction is popular in domain generalization but will encounter two issues under federated scenario: (1) Most stylization methods require the centralization of multiple domains to generate novel styles but this is not applicable under decentralized constraint. (2) The authenticity of generated data cannot be ensured especially given limited local data, which may impair the model optimization. To solve these two problems, we propose the Diversity-Authenticity Co-constrained Stylization (DACS), which can generate diverse and authentic data for learning robust local model. Specifically, we deploy a style transformation model on each domain to generate novel data with two constraints: (1) A diversity constraint is designed to increase data diversity, which enlarges the Wasserstein distance between the original and transformed data; (2) An authenticity constraint is proposed to ensure data authenticity, which enforces the transformed data to be easily/hardly recognized by the local-side global/local model. Extensive experiments demonstrate the effectiveness of the proposed DACS and show that DACS achieves state-of-the-art performance for FedDG re-ID",
    "checked": true,
    "id": "38404462b45b6f370fe78ab839bd7a7bc5f51136",
    "semantic_title": "diversity-authenticity co-constrained stylization for federated domain generalization in person re-identification",
    "citation_count": 0,
    "authors": [
      "Fengxiang Yang",
      "Zhun Zhong",
      "Zhiming Luo",
      "Yifan He",
      "Shaozi Li",
      "Nicu Sebe"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28469": {
    "title": "Semantic-Aware Transformation-Invariant RoI Align",
    "volume": "main",
    "abstract": "Great progress has been made in learning-based object detection methods in the last decade. Two-stage detectors often have higher detection accuracy than one-stage detectors, due to the use of region of interest (RoI) feature extractors which extract transformation-invariant RoI features for different RoI proposals, making refinement of bounding boxes and prediction of object categories more robust and accurate. However, previous RoI feature extractors can only extract invariant features under limited transformations. In this paper, we propose a novel RoI feature extractor, termed Semantic RoI Align (SRA), which is capable of extracting invariant RoI features under a variety of transformations for two-stage detectors. Specifically, we propose a semantic attention module to adaptively determine different sampling areas by leveraging the global and local semantic relationship within the RoI. We also propose a Dynamic Feature Sampler which dynamically samples features based on the RoI aspect ratio to enhance the efficiency of SRA, and a new position embedding, i.e., Area Embedding, to provide more accurate position information for SRA through an improved sampling area representation. Experiments show that our model significantly outperforms baseline models with slight computational overhead. In addition, it shows excellent generalization ability and can be used to improve performance with various state-of-the-art backbones and detection methods. The code is available at https://github.com/cxjyxxme/SemanticRoIAlign",
    "checked": true,
    "id": "f901ee1be1dc57d9df756b1e74c1d4e4e8f7bad4",
    "semantic_title": "semantic-aware transformation-invariant roi align",
    "citation_count": 0,
    "authors": [
      "Guo-Ye Yang",
      "George Kiyohiro Nakayama",
      "Zi-Kai Xiao",
      "Tai-Jiang Mu",
      "Xiaolei Huang",
      "Shi-Min Hu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28470": {
    "title": "FACL-Attack: Frequency-Aware Contrastive Learning for Transferable Adversarial Attacks",
    "volume": "main",
    "abstract": "Deep neural networks are known to be vulnerable to security risks due to the inherent transferable nature of adversarial examples. Despite the success of recent generative model-based attacks demonstrating strong transferability, it still remains a challenge to design an efficient attack strategy in a real-world strict black-box setting, where both the target domain and model architectures are unknown. In this paper, we seek to explore a feature contrastive approach in the frequency domain to generate adversarial examples that are robust in both cross-domain and cross-model settings. With that goal in mind, we propose two modules that are only employed during the training phase: a Frequency-Aware Domain Randomization (FADR) module to randomize domain-variant low- and high-range frequency components and a Frequency-Augmented Contrastive Learning (FACL) module to effectively separate domain-invariant mid-frequency features of clean and perturbed image. We demonstrate strong transferability of our generated adversarial perturbations through extensive cross-domain and cross-model experiments, while keeping the inference time complexity",
    "checked": true,
    "id": "4a83e0659bbd745ad3425da6c5625583bf0bea52",
    "semantic_title": "facl-attack: frequency-aware contrastive learning for transferable adversarial attacks",
    "citation_count": 0,
    "authors": [
      "Hunmin Yang",
      "Jongoh Jeong",
      "Kuk-Jin Yoon"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28471": {
    "title": "Hybrid-SORT: Weak Cues Matter for Online Multi-Object Tracking",
    "volume": "main",
    "abstract": "Multi-Object Tracking (MOT) aims to detect and associate all desired objects across frames. Most methods accomplish the task by explicitly or implicitly leveraging strong cues (i.e., spatial and appearance information), which exhibit powerful instance-level discrimination. However, when object occlusion and clustering occur, spatial and appearance information will become ambiguous simultaneously due to the high overlap among objects. In this paper, we demonstrate this long-standing challenge in MOT can be efficiently and effectively resolved by incorporating weak cues to compensate for strong cues. Along with velocity direction, we introduce the confidence and height state as potential weak cues. With superior performance, our method still maintains Simple, Online and Real-Time (SORT) characteristics. Also, our method shows strong generalization for diverse trackers and scenarios in a plug-and-play and training-free manner. Significant and consistent improvements are observed when applying our method to 5 different representative trackers. Further, with both strong and weak cues, our method Hybrid-SORT achieves superior performance on diverse benchmarks, including MOT17, MOT20, and especially DanceTrack where interaction and severe occlusion frequently happen with complex motions. The code and models are available at https://github.com/ymzis69/HybridSORT",
    "checked": true,
    "id": "f2dd399edb1cc35340266bd27d8ece57d4759372",
    "semantic_title": "hybrid-sort: weak cues matter for online multi-object tracking",
    "citation_count": 7,
    "authors": [
      "Mingzhan Yang",
      "Guangxin Han",
      "Bin Yan",
      "Wenhua Zhang",
      "Jinqing Qi",
      "Huchuan Lu",
      "Dong Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28472": {
    "title": "Multi-Modal Prompting for Open-Vocabulary Video Visual Relationship Detection",
    "volume": "main",
    "abstract": "Open-vocabulary video visual relationship detection aims to extend video visual relationship detection beyond annotated categories by detecting unseen relationships between objects in videos. Recent progresses in open-vocabulary perception, primarily driven by large-scale image-text pre-trained models like CLIP, have shown remarkable success in recognizing novel objects and semantic categories. However, directly applying CLIP-like models to video visual relationship detection encounters significant challenges due to the substantial gap between images and video object relationships. To address this challenge, we propose a multi-modal prompting method that adapts CLIP well to open-vocabulary video visual relationship detection by prompt-tuning on both visual representation and language input. Specifically, we enhance the image encoder of CLIP by using spatio-temporal visual prompting to capture spatio-temporal contexts, thereby making it suitable for object-level relationship representation in videos. Furthermore, we propose visual-guided language prompting to leverage CLIP's comprehensive semantic knowledge for discovering unseen relationship categories, thus facilitating recognizing novel video relationships. Extensive experiments on two public datasets, VidVRD and VidOR, demonstrate the effectiveness of our method, especially achieving a significant gain of nearly 10% in mAP on novel relationship categories on the VidVRD dataset",
    "checked": true,
    "id": "157b08d93007738d63587ba959aa71f2a35ab73b",
    "semantic_title": "multi-modal prompting for open-vocabulary video visual relationship detection",
    "citation_count": 0,
    "authors": [
      "Shuo Yang",
      "Yongqi Wang",
      "Xiaofeng Ji",
      "Xinxiao Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28473": {
    "title": "Learning Dense Correspondence for NeRF-Based Face Reenactment",
    "volume": "main",
    "abstract": "Face reenactment is challenging due to the need to establish dense correspondence between various face representations for motion transfer. Recent studies have utilized Neural Radiance Field (NeRF) as fundamental representation, which further enhanced the performance of multi-view face reenactment in photo-realism and 3D consistency. However, establishing dense correspondence between different face NeRFs is non-trivial, because implicit representations lack ground-truth correspondence annotations like mesh-based 3D parametric models (e.g., 3DMM) with index-aligned vertexes. Although aligning 3DMM space with NeRF-based face representations can realize motion control, it is sub-optimal for their limited face-only modeling and low identity fidelity. Therefore, we are inspired to ask: Can we learn the dense correspondence between different NeRF-based face representations without a 3D parametric model prior? To address this challenge, we propose a novel framework, which adopts tri-planes as fundamental NeRF representation and decomposes face tri-planes into three components: canonical tri-planes, identity deformations, and motion. In terms of motion control, our key contribution is proposing a Plane Dictionary (PlaneDict) module, which efficiently maps the motion conditions to a linear weighted addition of learnable orthogonal plane bases. To the best of our knowledge, our framework is the first method that achieves one-shot multi-view face reenactment without a 3D parametric model prior. Extensive experiments demonstrate that we produce better results in fine-grained motion control and identity preservation than previous methods",
    "checked": true,
    "id": "693b1f887774e4112af91b0e999276d5145876e0",
    "semantic_title": "learning dense correspondence for nerf-based face reenactment",
    "citation_count": 5,
    "authors": [
      "Songlin Yang",
      "Wei Wang",
      "Yushi Lan",
      "Xiangyu Fan",
      "Bo Peng",
      "Lei Yang",
      "Jing Dong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28474": {
    "title": "Motion Deblurring via Spatial-Temporal Collaboration of Frames and Events",
    "volume": "main",
    "abstract": "Motion deblurring can be advanced by exploiting informative features from supplementary sensors such as event cameras, which can capture rich motion information asynchronously with high temporal resolution. Existing event-based motion deblurring methods neither consider the modality redundancy in spatial fusion nor temporal cooperation between events and frames. To tackle these limitations, a novel spatial-temporal collaboration network (STCNet) is proposed for event-based motion deblurring. Firstly, we propose a differential-modality based cross-modal calibration strategy to suppress redundancy for complementarity enhancement, and then bimodal spatial fusion is achieved with an elaborate cross-modal co-attention mechanism to weight the contributions of them for importance balance. Besides, we present a frame-event mutual spatio-temporal attention scheme to alleviate the errors of relying only on frames to compute cross-temporal similarities when the motion blur is significant, and then the spatio-temporal features from both frames and events are aggregated with the custom cross-temporal coordinate attention. Extensive experiments on both synthetic and real-world datasets demonstrate that our method achieves state-of-the-art performance. Project website: https://github.com/wyang-vis/STCNet",
    "checked": true,
    "id": "7091ba1f978bdda3744c82ecd566ec2c93d18b74",
    "semantic_title": "motion deblurring via spatial-temporal collaboration of frames and events",
    "citation_count": 0,
    "authors": [
      "Wen Yang",
      "Jinjian Wu",
      "Jupo Ma",
      "Leida Li",
      "Guangming Shi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28475": {
    "title": "DGL: Dynamic Global-Local Prompt Tuning for Text-Video Retrieval",
    "volume": "main",
    "abstract": "Text-video retrieval is a critical multi-modal task to find the most relevant video for a text query. Although pretrained models like CLIP have demonstrated impressive potential in this area, the rising cost of fully finetuning these models due to increasing model size continues to pose a problem. To address this challenge, prompt tuning has emerged as an alternative. However, existing works still face two problems when adapting pretrained image-text models to downstream video-text tasks: (1) The visual encoder could only encode frame-level features and failed to extract global-level general video information. (2) Equipping the visual and text encoder with separated prompts failed to mitigate the visual-text modality gap. To this end, we propose DGL, a cross-modal Dynamic prompt tuning method with Global-Local video attention. In contrast to previous prompt tuning methods, we employ the shared latent space to generate local-level text and frame prompts that encourage inter-modal interaction. Furthermore, we propose modeling video in a global-local attention mechanism to capture global video information from the perspective of prompt tuning. Extensive experiments reveal that when only 0.67% parameters are tuned, our cross-modal prompt tuning strategy DGL outperforms or is comparable to fully finetuning methods on MSR-VTT, VATEX, LSMDC, and ActivityNet datasets. Code will be available at https://github.com/knightyxp/DGL",
    "checked": true,
    "id": "1d24f2dfe942e2d67381d8fcfac3423145bd557f",
    "semantic_title": "dgl: dynamic global-local prompt tuning for text-video retrieval",
    "citation_count": 2,
    "authors": [
      "Xiangpeng Yang",
      "Linchao Zhu",
      "Xiaohan Wang",
      "Yi Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28476": {
    "title": "Diverse and Stable 2D Diffusion Guided Text to 3D Generation with Noise Recalibration",
    "volume": "main",
    "abstract": "In recent years, following the success of text guided image generation, text guided 3D generation has gained increasing attention among researchers. Dreamfusion is a notable approach that enhances generation quality by utilizing 2D text guided diffusion models and introducing SDS loss, a technique for distilling 2D diffusion model information to train 3D models. However, the SDS loss has two major limitations that hinder its effectiveness. Firstly, when given a text prompt, the SDS loss struggles to produce diverse content. Secondly, during training, SDS loss may cause the generated content to overfit and collapse, limiting the model's ability to learn intricate texture details. To overcome these challenges, we propose a novel approach called Noise Recalibration algorithm. By incorporating this technique, we can generate 3D content with significantly greater diversity and stunning details. Our approach offers a promising solution to the limitations of SDS loss",
    "checked": true,
    "id": "6e04206e2e4111e13d174e658aad1b58f6054516",
    "semantic_title": "diverse and stable 2d diffusion guided text to 3d generation with noise recalibration",
    "citation_count": 0,
    "authors": [
      "Xiaofeng Yang",
      "Fayao Liu",
      "Yi Xu",
      "Hanjing Su",
      "Qingyao Wu",
      "Guosheng Lin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28477": {
    "title": "Semantic Segmentation in Multiple Adverse Weather Conditions with Domain Knowledge Retention",
    "volume": "main",
    "abstract": "Semantic segmentation's performance is often compromised when applied to unlabeled adverse weather conditions. Unsupervised domain adaptation is a potential approach to enhancing the model's adaptability and robustness to adverse weather. However, existing methods encounter difficulties when sequentially adapting the model to multiple unlabeled adverse weather conditions. They struggle to acquire new knowledge while also retaining previously learned knowledge. To address these problems, we propose a semantic segmentation method for multiple adverse weather conditions that incorporates adaptive knowledge acquisition, pseudo-label blending, and weather composition replay. Our adaptive knowledge acquisition enables the model to avoid learning from extreme images that could potentially cause the model to forget. In our approach of blending pseudo-labels, we not only utilize the current model but also integrate the previously learned model into the ongoing learning process. This collaboration between the current teacher and the previous model enhances the robustness of the pseudo-labels for the current target. Our weather composition replay mechanism allows the model to continuously refine its previously learned weather information while simultaneously learning from the new target domain. Our method consistently outperforms the state-of-the-art methods, and obtains the best performance with averaged mIoU (%) of 65.7 and the lowest forgetting (%) of 3.6 against 60.1 and 11.3, on the ACDC datsets for a four-target continual multi-target domain adaptation",
    "checked": true,
    "id": "25ecfe48e1abac87324ad50dd174aa9d29350169",
    "semantic_title": "semantic segmentation in multiple adverse weather conditions with domain knowledge retention",
    "citation_count": 1,
    "authors": [
      "Xin Yang",
      "Wending Yan",
      "Yuan Yuan",
      "Michael Bi Mi",
      "Robby T. Tan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28478": {
    "title": "Hyperspectral Image Reconstruction via Combinatorial Embedding of Cross-Channel Spatio-Spectral Clues",
    "volume": "main",
    "abstract": "Existing learning-based hyperspectral reconstruction methods show limitations in fully exploiting the information among the hyperspectral bands. As such, we propose to investigate the chromatic inter-dependencies in their respective hyperspectral embedding space. These embedded features can be fully exploited by querying the inter-channel correlations in a combinatorial manner, with the unique and complementary information efficiently fused into the final prediction. We found such independent modeling and combinatorial excavation mechanisms are extremely beneficial to uncover marginal spectral features, especially in the long wavelength bands. In addition, we have proposed a spatio-spectral attention block and a spectrum-fusion attention module, which greatly facilitates the excavation and fusion of information at both semantically long-range levels and fine-grained pixel levels across all dimensions. Extensive quantitative and qualitative experiments show that our method (dubbed CESST) achieves SOTA performance. Code for this project is at: https://github.com/AlexYangxx/CESST",
    "checked": true,
    "id": "54bd7622956e804604aadb017ed6b2580039bf21",
    "semantic_title": "hyperspectral image reconstruction via combinatorial embedding of cross-channel spatio-spectral clues",
    "citation_count": 0,
    "authors": [
      "Xingxing Yang",
      "Jie Chen",
      "Zaifeng Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28479": {
    "title": "Decomposing Semantic Shifts for Composed Image Retrieval",
    "volume": "main",
    "abstract": "Composed image retrieval is a type of image retrieval task where the user provides a reference image as a starting point and specifies a text on how to shift from the starting point to the desired target image. However, most existing methods focus on the composition learning of text and reference images and oversimplify the text as a description, neglecting the inherent structure and the user's shifting intention of the texts. As a result, these methods typically take shortcuts that disregard the visual cue of the reference images. To address this issue, we reconsider the text as instructions and propose a Semantic Shift Network (SSN) that explicitly decomposes the semantic shifts into two steps: from the reference image to the visual prototype and from the visual prototype to the target image. Specifically, SSN explicitly decomposes the instructions into two components: degradation and upgradation, where the degradation is used to picture the visual prototype from the reference image, while the upgradation is used to enrich the visual prototype into the final representations to retrieve the desired target image. The experimental results show that the proposed SSN demonstrates a significant improvement of 5.42% and 1.37% on the CIRR and FashionIQ datasets, respectively, and establishes a new state-of-the-art performance. The code is available at https://github.com/starxing-yuu/SSN",
    "checked": true,
    "id": "a24b7356856f941bfcb76d52c9ab6227607a3698",
    "semantic_title": "decomposing semantic shifts for composed image retrieval",
    "citation_count": 0,
    "authors": [
      "Xingyu Yang",
      "Daqing Liu",
      "Heng Zhang",
      "Yong Luo",
      "Chaoyue Wang",
      "Jing Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28480": {
    "title": "Gaze Target Detection by Merging Human Attention and Activity Cues",
    "volume": "main",
    "abstract": "Despite achieving impressive performance, current methods for detecting gaze targets, which depend on visual saliency and spatial scene geometry, continue to face challenges when it comes to detecting gaze targets within intricate image backgrounds. One of the primary reasons for this lies in the oversight of the intricate connection between human attention and activity cues. In this study, we introduce an innovative approach that amalgamates the visual saliency detection with the body-part & object interaction both guided by the soft gaze attention. This fusion enables precise and dependable detection of gaze targets amidst intricate image backgrounds. Our approach attains state-of-the-art performance on both the Gazefollow benchmark and the GazeVideoAttn benchmark. In comparison to recent methods that rely on intricate 3D reconstruction of a single input image, our approach, which solely leverages 2D image information, still exhibits a substantial lead across all evaluation metrics, positioning it closer to human-level performance. These outcomes underscore the potent effectiveness of our proposed method in the gaze target detection task",
    "checked": true,
    "id": "725492cb88ad92c2997d9f7557cb08d371f8c97f",
    "semantic_title": "gaze target detection by merging human attention and activity cues",
    "citation_count": 0,
    "authors": [
      "Yaokun Yang",
      "Yihan Yin",
      "Feng Lu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28481": {
    "title": "PM-INR: Prior-Rich Multi-Modal Implicit Large-Scale Scene Neural Representation",
    "volume": "main",
    "abstract": "Recent advancements in implicit neural representations have contributed to high-fidelity surface reconstruction and photorealistic novel view synthesis. However, with the expansion of the scene scale, such as block or city level, existing methods will encounter challenges because traditional sampling cannot cope with the cubically growing sampling space. To alleviate the dependence on filling the sampling space, we explore using multi-modal priors to assist individual points to obtain more global semantic information and propose a priorrich multi-modal implicit neural representation network, Pm-INR, for the outdoor unbounded large-scale scene. The core of our method is multi-modal prior extraction and crossmodal prior fusion modules. The former encodes codebooks from different modality inputs and extracts valuable priors, while the latter fuses priors to maintain view consistency and preserve unique features among multi-modal priors. Finally, feature-rich cross-modal priors are injected into the sampling regions to allow each region to perceive global information without filling the sampling space. Extensive experiments have demonstrated the effectiveness and robustness of our method for outdoor unbounded large-scale scene novel view synthesis, which outperforms state-of-the-art methods in terms of PSNR, SSIM, and LPIPS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiying Yang",
      "Fukun Yin",
      "Wen Liu",
      "Jiayuan Fan",
      "Xin Chen",
      "Gang Yu",
      "Tao Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28482": {
    "title": "FontDiffuser: One-Shot Font Generation via Denoising Diffusion with Multi-Scale Content Aggregation and Style Contrastive Learning",
    "volume": "main",
    "abstract": "Automatic font generation is an imitation task, which aims to create a font library that mimics the style of reference images while preserving the content from source images. Although existing font generation methods have achieved satisfactory performance, they still struggle with complex characters and large style variations. To address these issues, we propose FontDiffuser, a diffusion-based image-to-image one-shot font generation method, which innovatively models the font imitation task as a noise-to-denoise paradigm. In our method, we introduce a Multi-scale Content Aggregation (MCA) block, which effectively combines global and local content cues across different scales, leading to enhanced preservation of intricate strokes of complex characters. Moreover, to better manage the large variations in style transfer, we propose a Style Contrastive Refinement (SCR) module, which is a novel structure for style representation learning. It utilizes a style extractor to disentangle styles from images, subsequently supervising the diffusion model via a meticulously designed style contrastive loss. Extensive experiments demonstrate FontDiffuser's state-of-the-art performance in generating diverse characters and styles. It consistently excels on complex characters and large style changes compared to previous methods. The code is available at https://github.com/yeungchenwa/FontDiffuser",
    "checked": true,
    "id": "5235c4e13498c6bca10791b21d80221f839f8440",
    "semantic_title": "fontdiffuser: one-shot font generation via denoising diffusion with multi-scale content aggregation and style contrastive learning",
    "citation_count": 4,
    "authors": [
      "Zhenhua Yang",
      "Dezhi Peng",
      "Yuxin Kong",
      "Yuyi Zhang",
      "Cong Yao",
      "Lianwen Jin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28483": {
    "title": "Full-Body Motion Reconstruction with Sparse Sensing from Graph Perspective",
    "volume": "main",
    "abstract": "Estimating 3D full-body pose from sparse sensor data is a pivotal technique employed for the reconstruction of realistic human motions in Augmented Reality and Virtual Reality. However, translating sparse sensor signals into comprehensive human motion remains a challenge since the sparsely distributed sensors in common VR systems fail to capture the motion of full human body. In this paper, we use well-designed Body Pose Graph (BPG) to represent the human body and translate the challenge into a prediction problem of graph missing nodes. Then, we propose a novel full-body motion reconstruction framework based on BPG. To establish BPG, nodes are initially endowed with features extracted from sparse sensor signals. Features from identifiable joint nodes across diverse sensors are amalgamated and processed from both temporal and spatial perspectives. Temporal dynamics are captured using the Temporal Pyramid Structure, while spatial relations in joint movements inform the spatial attributes. The resultant features serve as the foundational elements of the BPG nodes. To further refine the BPG, node features are updated through a graph neural network that incorporates edge reflecting varying joint relations. Our method's effectiveness is evidenced by the attained state-of-the-art performance, particularly in lower body motion, outperforming other baseline methods. Additionally, an ablation study validates the efficacy of each module in our proposed framework",
    "checked": true,
    "id": "0d9b42d915b213388ea56391260c262cfeac700c",
    "semantic_title": "full-body motion reconstruction with sparse sensing from graph perspective",
    "citation_count": 0,
    "authors": [
      "Feiyu Yao",
      "Zongkai Wu",
      "Li Yi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28484": {
    "title": "FoSp: Focus and Separation Network for Early Smoke Segmentation",
    "volume": "main",
    "abstract": "Early smoke segmentation (ESS) enables the accurate identification of smoke sources, facilitating the prompt extinguishing of fires and preventing large-scale gas leaks. But ESS poses greater challenges than conventional object and regular smoke segmentation due to its small scale and transparent appearance, which can result in high miss detection rate and low precision. To address these issues, a Focus and Separation Network (FoSp) is proposed. We first introduce a Focus module employing bidirectional cascade which guides low-resolution and high-resolution features towards mid-resolution to locate and determine the scope of smoke, reducing the miss detection rate. Next, we propose a Separation module that separates smoke images into a pure smoke foreground and a smoke-free background, enhancing the contrast between smoke and background fundamentally, improving segmentation precision. Finally, a Domain Fusion module is developed to integrate the distinctive features of the two modules which can balance recall and precision to achieve high F_beta. Futhermore, to promote the development of ESS, we introduce a high-quality real-world dataset called SmokeSeg, which contains more small and transparent smoke than the existing datasets. Experimental results show that our model achieves the best performance on three available smoke segmentation datasets: SYN70K (mIoU: 83.00%), SMOKE5K (F_beta: 81.6%) and SmokeSeg (F_beta: 72.05%). The code can be found at https://github.com/LujianYao/FoSp",
    "checked": true,
    "id": "7118a75302b5c4d44d7bd04f48a6bdc5813e41cb",
    "semantic_title": "fosp: focus and separation network for early smoke segmentation",
    "citation_count": 2,
    "authors": [
      "Lujian Yao",
      "Haitao Zhao",
      "Jingchao Peng",
      "Zhongze Wang",
      "Kaijie Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28485": {
    "title": "How to Evaluate the Generalization of Detection? A Benchmark for Comprehensive Open-Vocabulary Detection",
    "volume": "main",
    "abstract": "Object detection (OD) in computer vision has made significant progress in recent years, transitioning from closed-set labels to open-vocabulary detection (OVD) based on large-scale vision-language pre-training (VLP). However, current evaluation methods and datasets are limited to testing generalization over object types and referral expressions, which do not provide a systematic, fine-grained, and accurate benchmark of OVD models' abilities. In this paper, we propose a new benchmark named OVDEval, which includes 9 sub-tasks and introduces evaluations on commonsense knowledge, attribute understanding, position understanding, object relation comprehension, and more. The dataset is meticulously created to provide hard negatives that challenge models' true understanding of visual and linguistic input. Additionally, we identify a problem with the popular Average Precision (AP) metric when benchmarking models on these fine-grained label datasets and propose a new metric called Non-Maximum Suppression Average Precision (NMS-AP) to address this issue. Extensive experimental results show that existing top OVD models all fail on the new tasks except for simple object types, demonstrating the value of the proposed dataset in pinpointing the weakness of current OVD models and guiding future research. Furthermore, the proposed NMS-AP metric is verified by experiments to provide a much more truthful evaluation of OVD models, whereas traditional AP metrics yield deceptive results. Data is available at https://github.com/om-ai-lab/OVDEval",
    "checked": true,
    "id": "2b64da99b184b6dfa3543ca1bc079c954aa7b320",
    "semantic_title": "how to evaluate the generalization of detection? a benchmark for comprehensive open-vocabulary detection",
    "citation_count": 2,
    "authors": [
      "Yiyang Yao",
      "Peng Liu",
      "Tiancheng Zhao",
      "Qianqian Zhang",
      "Jiajia Liao",
      "Chunxin Fang",
      "Kyusong Lee",
      "Qing Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28486": {
    "title": "Diverse and Aligned Audio-to-Video Generation via Text-to-Video Model Adaptation",
    "volume": "main",
    "abstract": "We consider the task of generating diverse and realistic videos guided by natural audio samples from a wide variety of semantic classes. For this task, the videos are required to be aligned both globally and temporally with the input audio: globally, the input audio is semantically associated with the entire output video, and temporally, each segment of the input audio is associated with a corresponding segment of that video. We utilize an existing text-conditioned video generation model and a pre-trained audio encoder model. The proposed method is based on a lightweight adaptor network, which learns to map the audio-based representation to the input representation expected by the text-to-video generation model. As such, it also enables video generation conditioned on text, audio, and, for the first time as far as we can ascertain, on both text and audio. We validate our method extensively on three datasets demonstrating significant semantic diversity of audio-video samples and further propose a novel evaluation metric (AV-Align) to assess the alignment of generated videos with input audio samples. AV-Align is based on the detection and comparison of energy peaks in both modalities. In comparison to recent state-of-the-art approaches, our method generates videos that are better aligned with the input sound, both with respect to content and temporal axis. We also show that videos produced by our method present higher visual quality and are more diverse. Code and samples are available at: https://pages.cs.huji.ac.il/adiyoss-lab/TempoTokens/",
    "checked": true,
    "id": "5f65952c8aa110636b3cbaa9b3065993c1f782fe",
    "semantic_title": "diverse and aligned audio-to-video generation via text-to-video model adaptation",
    "citation_count": 11,
    "authors": [
      "Guy Yariv",
      "Itai Gat",
      "Sagie Benaim",
      "Lior Wolf",
      "Idan Schwartz",
      "Yossi Adi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28487": {
    "title": "AltDiffusion: A Multilingual Text-to-Image Diffusion Model",
    "volume": "main",
    "abstract": "Large Text-to-Image(T2I) diffusion models have shown a remarkable capability to produce photorealistic and diverse images based on text inputs. However, existing works only support limited language input, e.g., English, Chinese, and Japanese, leaving users beyond these languages underserved and blocking the global expansion of T2I models. Therefore, this paper presents AltDiffusion, a novel multilingual T2I diffusion model that supports eighteen different languages. Specifically, we first train a multilingual text encoder based on the knowledge distillation. Then we plug it into a pretrained English-only diffusion model and train the model with a two-stage schema to enhance the multilingual capability, including concept alignment and quality improvement stage on a large-scale multilingual dataset. Furthermore, we introduce a new benchmark, which includes Multilingual-General-18(MG-18) and Multilingual-Cultural-18(MC-18) datasets, to evaluate the capabilities of T2I diffusion models for generating high-quality images and capturing culture-specific concepts in different languages. Experimental results on both MG-18 and MC-18 demonstrate that AltDiffusion outperforms current state-of-the-art T2I models, e.g., Stable Diffusion in multilingual understanding, especially with respect to culture-specific concepts, while still having comparable capability for generating high-quality images. All source code and checkpoints could be found in https://github.com/superhero-7/AltDiffuson",
    "checked": true,
    "id": "7e9d953c92a7666e648d4741a9cb67a977a99731",
    "semantic_title": "altdiffusion: a multilingual text-to-image diffusion model",
    "citation_count": 10,
    "authors": [
      "Fulong Ye",
      "Guang Liu",
      "Xinya Wu",
      "Ledell Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28488": {
    "title": "Mutual-Modality Adversarial Attack with Semantic Perturbation",
    "volume": "main",
    "abstract": "Adversarial attacks constitute a notable threat to machine learning systems, given their potential to induce erroneous predictions and classifications. However, within real-world contexts, the essential specifics of the deployed model are frequently treated as a black box, consequently mitigating the vulnerability to such attacks. Thus, enhancing the transferability of the adversarial samples has become a crucial area of research, which heavily relies on selecting appropriate surrogate models. To address this challenge, we propose a novel approach that generates adversarial attacks in a mutual-modality optimization scheme. Our approach is accomplished by leveraging the pre-trained CLIP model. Firstly, we conduct a visual attack on the clean image that causes semantic perturbations on the aligned embedding space with the other textual modality. Then, we apply the corresponding defense on the textual modality by updating the prompts, which forces the re-matching on the perturbed embedding space. Finally, to enhance the attack transferability, we utilize the iterative training strategy on the visual attack and the textual defense, where the two processes optimize from each other. We evaluate our approach on several benchmark datasets and demonstrate that our mutual-modal attack strategy can effectively produce high-transferable attacks, which are stable regardless of the target networks. Our approach outperforms state-of-the-art attack methods and can be readily deployed as a plug-and-play solution",
    "checked": true,
    "id": "8a5a9d3557de828e6306670c8a97306b46101318",
    "semantic_title": "mutual-modality adversarial attack with semantic perturbation",
    "citation_count": 3,
    "authors": [
      "Jingwen Ye",
      "Ruonan Yu",
      "Songhua Liu",
      "Xinchao Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28489": {
    "title": "STDiff: Spatio-Temporal Diffusion for Continuous Stochastic Video Prediction",
    "volume": "main",
    "abstract": "Predicting future frames of a video is challenging because it is difficult to learn the uncertainty of the underlying factors influencing their contents. In this paper, we propose a novel video prediction model, which has infinite-dimensional latent variables over the spatio-temporal domain. Specifically, we first decompose the video motion and content information, then take a neural stochastic differential equation to predict the temporal motion information, and finally, an image diffusion model autoregressively generates the video frame by conditioning on the predicted motion feature and the previous frame. The better expressiveness and stronger stochasticity learning capability of our model lead to state-of-the-art video prediction performances. As well, our model is able to achieve temporal continuous prediction, i.e., predicting in an unsupervised way the future video frames with an arbitrarily high frame rate. Our code is available at https://github.com/XiYe20/STDiffProject",
    "checked": true,
    "id": "740c513ee582321b8b7e43246ce7e8c3ecc49a5f",
    "semantic_title": "stdiff: spatio-temporal diffusion for continuous stochastic video prediction",
    "citation_count": 0,
    "authors": [
      "Xi Ye",
      "Guillaume-Alexandre Bilodeau"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28490": {
    "title": "DiffusionEdge: Diffusion Probabilistic Model for Crisp Edge Detection",
    "volume": "main",
    "abstract": "Limited by the encoder-decoder architecture, learning-based edge detectors usually have difficulty predicting edge maps that satisfy both correctness and crispness. With the recent success of the diffusion probabilistic model (DPM), we found it is especially suitable for accurate and crisp edge detection since the denoising process is directly applied to the original image size. Therefore, we propose the first diffusion model for the task of general edge detection, which we call DiffusionEdge. To avoid expensive computational resources while retaining the final performance, we apply DPM in the latent space and enable the classic cross-entropy loss which is uncertainty-aware in pixel level to directly optimize the parameters in latent space in a distillation manner. We also adopt a decoupled architecture to speed up the denoising process and propose a corresponding adaptive Fourier filter to adjust the latent features of specific frequencies. With all the technical designs, DiffusionEdge can be stably trained with limited resources, predicting crisp and accurate edge maps with much fewer augmentation strategies. Extensive experiments on four edge detection benchmarks demonstrate the superiority of DiffusionEdge both in correctness and crispness. On the NYUDv2 dataset, compared to the second best, we increase the ODS, OIS (without post-processing) and AC by 30.2%, 28.1% and 65.1%, respectively. Code: https://github.com/GuHuangAI/DiffusionEdge",
    "checked": true,
    "id": "a47a186bd92f79f8bdbac386abfc835d9f8898f9",
    "semantic_title": "diffusionedge: diffusion probabilistic model for crisp edge detection",
    "citation_count": 3,
    "authors": [
      "Yunfan Ye",
      "Kai Xu",
      "Yuhang Huang",
      "Renjiao Yi",
      "Zhiping Cai"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28491": {
    "title": "Dynamic Feature Pruning and Consolidation for Occluded Person Re-identification",
    "volume": "main",
    "abstract": "Occluded person re-identification (ReID) is a challenging problem due to contamination from occluders. Existing approaches address the issue with prior knowledge cues, such as human body key points and semantic segmentations, which easily fail in the presence of heavy occlusion and other humans as occluders. In this paper, we propose a feature pruning and consolidation (FPC) framework to circumvent explicit human structure parsing. The framework mainly consists of a sparse encoder, a multi-view feature mathcing module, and a feature consolidation decoder. Specifically, the sparse encoder drops less important image tokens, mostly related to background noise and occluders, solely based on correlation within the class token attention. Subsequently, the matching stage relies on the preserved tokens produced by the sparse encoder to identify k-nearest neighbors in the gallery by measuring the image and patch-level combined similarity. Finally, we use the feature consolidation module to compensate pruned features using identified neighbors for recovering essential information while disregarding disturbance from noise and occlusion. Experimental results demonstrate the effectiveness of our proposed framework on occluded, partial, and holistic Re-ID datasets. In particular, our method outperforms state-of-the-art results by at least 8.6% mAP and 6.0% Rank-1 accuracy on the challenging Occluded-Duke dataset",
    "checked": true,
    "id": "def702c9566c15da7ecb7a18dc210cc60dbae2ca",
    "semantic_title": "dynamic feature pruning and consolidation for occluded person re-identification",
    "citation_count": 3,
    "authors": [
      "YuTeng Ye",
      "Hang Zhou",
      "Jiale Cai",
      "Chenxing Gao",
      "Youjia Zhang",
      "Junle Wang",
      "Qiang Hu",
      "Junqing Yu",
      "Wei Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28492": {
    "title": "Progressive Text-to-Image Diffusion with Soft Latent Direction",
    "volume": "main",
    "abstract": "In spite of the rapidly evolving landscape of text-to-image generation, the synthesis and manipulation of multiple entities while adhering to specific relational constraints pose enduring challenges. This paper introduces an innovative progressive synthesis and editing operation that systematically incorporates entities into the target image, ensuring their adherence to spatial and relational constraints at each sequential step. Our key insight stems from the observation that while a pre-trained text-to-image diffusion model adeptly handles one or two entities, it often falters when dealing with a greater number. To address this limitation, we propose harnessing the capabilities of a Large Language Model (LLM) to decompose intricate and protracted text descriptions into coherent directives adhering to stringent formats. To facilitate the execution of directives involving distinct semantic operations—namely insertion, editing, and erasing—we formulate the Stimulus, Response, and Fusion (SRF) framework. Within this framework, latent regions are gently stimulated in alignment with each operation, followed by the fusion of the responsive latent components to achieve cohesive entity manipulation. Our proposed framework yields notable advancements in object synthesis, particularly when confronted with intricate and lengthy textual inputs. Consequently, it establishes a new benchmark for text-to-image generation tasks, further elevating the field's performance standards",
    "checked": true,
    "id": "ebd4d665ddd29e67961e3041574cb1ecf9d31336",
    "semantic_title": "progressive text-to-image diffusion with soft latent direction",
    "citation_count": 3,
    "authors": [
      "YuTeng Ye",
      "Jiale Cai",
      "Hang Zhou",
      "Guanwen Li",
      "Youjia Zhang",
      "Zikai Song",
      "Chenxing Gao",
      "Junqing Yu",
      "Wei Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28493": {
    "title": "UCMCTrack: Multi-Object Tracking with Uniform Camera Motion Compensation",
    "volume": "main",
    "abstract": "Multi-object tracking (MOT) in video sequences remains a challenging task, especially in scenarios with significant camera movements. This is because targets can drift considerably on the image plane, leading to erroneous tracking outcomes. Addressing such challenges typically requires supplementary appearance cues or Camera Motion Compensation (CMC). While these strategies are effective, they also introduce a considerable computational burden, posing challenges for real-time MOT. In response to this, we introduce UCMCTrack, a novel motion model-based tracker robust to camera movements. Unlike conventional CMC that computes compensation parameters frame-by-frame, UCMCTrack consistently applies the same compensation parameters throughout a video sequence. It employs a Kalman filter on the ground plane and introduces the Mapped Mahalanobis Distance (MMD) as an alternative to the traditional Intersection over Union (IoU) distance measure. By leveraging projected probability distributions on the ground plane, our approach efficiently captures motion patterns and adeptly manages uncertainties introduced by homography projections. Remarkably, UCMCTrack, relying solely on motion cues, achieves state-of-the-art performance across a variety of challenging datasets, including MOT17, MOT20, DanceTrack and KITTI. More details and code are available at https://github.com/corfyi/UCMCTrack",
    "checked": true,
    "id": "5bd04838fbba407d8808a76485b57187e2065dd1",
    "semantic_title": "ucmctrack: multi-object tracking with uniform camera motion compensation",
    "citation_count": 9,
    "authors": [
      "Kefu Yi",
      "Kai Luo",
      "Xiaolei Luo",
      "Jiangui Huang",
      "Hao Wu",
      "Rongdong Hu",
      "Wei Hao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28494": {
    "title": "DiffRAW: Leveraging Diffusion Model to Generate DSLR-Comparable Perceptual Quality sRGB from Smartphone RAW Images",
    "volume": "main",
    "abstract": "Deriving DSLR-quality sRGB images from smartphone RAW images has become a compelling challenge due to discernible detail disparity, color mapping instability, and spatial misalignment in RAW-sRGB data pairs. We present DiffRAW, a novel method that incorporates the diffusion model for the first time in learning RAW-to-sRGB mappings. By leveraging the diffusion model, our approach effectively learns the high-quality detail distribution of DSLR images, thereby enhancing the details of output images. Simultaneously, we use the RAW image as a diffusion condition to maintain image structure information such as contours and textures. To mitigate the interference caused by the color and spatial misalignment in training data pairs, we embed a color-position preserving condition within DiffRAW, ensuring that the output images do not exhibit color biases and pixel shift issues. To accelerate the inference process of DiffRAW, we designed the Domain Transform Diffusion Method, an efficient diffusion process with its corresponding reverse process. The Domain Transform Diffusion Method can reduce the required inference steps for diffusion model-based image restoration/enhancement algorithms while enhancing the quality of the generated images. Through evaluations on the ZRR dataset, DiffRAW consistently demonstrates state-of-the-art performance across all perceptual quality metrics (e.g., LPIPS, FID, MUSIQ), while achieving comparable results in PSNR and SSIM",
    "checked": true,
    "id": "f5e8e948941cee6ebd70038e08ffdc20e3738c7f",
    "semantic_title": "diffraw: leveraging diffusion model to generate dslr-comparable perceptual quality srgb from smartphone raw images",
    "citation_count": 0,
    "authors": [
      "Mingxin Yi",
      "Kai Zhang",
      "Pei Liu",
      "Tanli Zuo",
      "Jingduo Tian"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28495": {
    "title": "Efficient Look-Up Table from Expanded Convolutional Network for Accelerating Image Super-resolution",
    "volume": "main",
    "abstract": "The look-up table (LUT) has recently shown its practicability and effectiveness in super-resolution (SR) tasks due to its low computational cost and hardware independence. However, most existing methods focus on improving the performance of SR, neglecting the demand for high-speed SR on low-computational edge devices. In this paper, we propose an efficient expanded convolution (EC) layer, which expands the output size of regular convolution to enlarge the receptive field (RF) indirectly. It can increase the size of the LUT corresponding to the network linearly with the increase of RF. Additionally, after introducing the EC, multiple LUTs are merged into one LUT, achieving faster running speed while maintaining SR performance. More specifically, we expand the coverage of the convolutional output so that the output at the current position covers the target position and its surroundings, forming an overlapping sliding window at the output end. We sum up the overlapping parts of the sliding window as the output, thereby achieving the effect of enlarging the RF size. Moreover, by expanding the numerical range of the accumulated results and rescaling them to [0,255], the method can mitigate the error caused by quantization output. Experiments indicate that the proposed method performs better than the baseline method and is faster than other LUT-based SR methods",
    "checked": true,
    "id": "f55483b40762cb8e7fecbbbf25317e2b88da7b55",
    "semantic_title": "efficient look-up table from expanded convolutional network for accelerating image super-resolution",
    "citation_count": 0,
    "authors": [
      "Kai Yin",
      "Jie Shen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28496": {
    "title": "CLIP-Gaze: Towards General Gaze Estimation via Visual-Linguistic Model",
    "volume": "main",
    "abstract": "Gaze estimation methods often experience significant performance degradation when evaluated across different domains, due to the domain gap between the testing and training data. Existing methods try to address this issue using various domain generalization approaches, but with little success because of the limited diversity of gaze datasets, such as appearance, wearable, and image quality. To overcome these limitations, we propose a novel framework called CLIP-Gaze that utilizes a pre-trained vision-language model to leverage its transferable knowledge. Our framework is the first to leverage the vision-and-language cross-modality approach for gaze estimation task. Specifically, we extract gaze-relevant feature by pushing it away from gaze-irrelevant features which can be flexibly constructed via language descriptions. To learn more suitable prompts, we propose a personalized context optimization method for text prompt tuning. Furthermore, we utilize the relationship among gaze samples to refine the distribution of gaze-relevant features, thereby improving the generalization capability of the gaze estimation model. Extensive experiments demonstrate the excellent performance of CLIP-Gaze over existing methods on four cross-domain evaluations",
    "checked": true,
    "id": "a19633e726d8fa4e3d97357eb54b7a1a4a9c806d",
    "semantic_title": "clip-gaze: towards general gaze estimation via visual-linguistic model",
    "citation_count": 1,
    "authors": [
      "Pengwei Yin",
      "Guanzhong Zeng",
      "Jingjing Wang",
      "Di Xie"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28497": {
    "title": "Point Deformable Network with Enhanced Normal Embedding for Point Cloud Analysis",
    "volume": "main",
    "abstract": "Recently MLP-based methods have shown strong performance in point cloud analysis. Simple MLP architectures are able to learn geometric features in local point groups yet fail to model long-range dependencies directly. In this paper, we propose Point Deformable Network (PDNet), a concise MLP-based network that can capture long-range relations with strong representation ability. Specifically, we put forward Point Deformable Aggregation Module (PDAM) to improve representation capability in both long-range dependency and adaptive aggregation among points. For each query point, PDAM aggregates information from deformable reference points rather than points in limited local areas. The deformable reference points are generated data-dependent, and we initialize them according to the input point positions. Additional offsets and modulation scalars are learned on the whole point features, which shift the deformable reference points to the regions of interest. We also suggest estimating the normal vector for point clouds and applying Enhanced Normal Embedding (ENE) to the geometric extractors to improve the representation ability of single-point. Extensive experiments and ablation studies on various benchmarks demonstrate the effectiveness and superiority of our PDNet",
    "checked": true,
    "id": "e2f580decc7853fcf01a8f9b662fb4952899829f",
    "semantic_title": "point deformable network with enhanced normal embedding for point cloud analysis",
    "citation_count": 1,
    "authors": [
      "Xingyilang Yin",
      "Xi Yang",
      "Liangchen Liu",
      "Nannan Wang",
      "Xinbo Gao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28498": {
    "title": "Revisiting Open-Set Panoptic Segmentation",
    "volume": "main",
    "abstract": "In this paper, we focus on the open-set panoptic segmentation (OPS) task to circumvent the data explosion problem. Different from the close-set setting, OPS targets to detect both known and unknown categories, where the latter is not annotated during training. Different from existing work that only selects a few common categories as unknown ones, we move forward to the real-world scenario by considering the various tail categories (~1k). To this end, we first build a new dataset with long-tail distribution for the OPS task. Based on this dataset, we additionally add a new class type for unknown classes and re-define the training annotations to make the OPS definition more complete and reasonable. Moreover, we analyze the influence of several significant factors in the OPS task and explore the upper bound of performance on unknown classes with different settings. Furthermore, based on the analyses, we design an effective two-phase framework for the OPS task, including thing-agnostic map generation and unknown segment mining. We further adopt semi-supervised learning to improve the OPS performance. Experimental results on different datasets validate the effectiveness of our method",
    "checked": true,
    "id": "b3b240ae2acbe593783dba8900ed8cf8ab861996",
    "semantic_title": "revisiting open-set panoptic segmentation",
    "citation_count": 0,
    "authors": [
      "Yufei Yin",
      "Hao Chen",
      "Wengang Zhou",
      "Jiajun Deng",
      "Haiming Xu",
      "Houqiang Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28499": {
    "title": "VQAttack: Transferable Adversarial Attacks on Visual Question Answering via Pre-trained Models",
    "volume": "main",
    "abstract": "Visual Question Answering (VQA) is a fundamental task in computer vision and natural language process fields. Although the \"pre-training & finetuning\" learning paradigm significantly improves the VQA performance, the adversarial robustness of such a learning paradigm has not been explored. In this paper, we delve into a new problem: using a pre-trained multimodal source model to create adversarial image-text pairs and then transferring them to attack the target VQA models. Correspondingly, we propose a novel VQATTACK model, which can iteratively generate both im- age and text perturbations with the designed modules: the large language model (LLM)-enhanced image attack and the cross-modal joint attack module. At each iteration, the LLM-enhanced image attack module first optimizes the latent representation-based loss to generate feature-level image perturbations. Then it incorporates an LLM to further enhance the image perturbations by optimizing the designed masked answer anti-recovery loss. The cross-modal joint attack module will be triggered at a specific iteration, which updates the image and text perturbations sequentially. Notably, the text perturbation updates are based on both the learned gradients in the word embedding space and word synonym-based substitution. Experimental results on two VQA datasets with five validated models demonstrate the effectiveness of the proposed VQATTACK in the transferable attack setting, compared with state-of-the-art baselines. This work reveals a significant blind spot in the \"pre-training & fine-tuning\" paradigm on VQA tasks. The source code can be found in the link https://github.com/ericyinyzy/VQAttack",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyi Yin",
      "Muchao Ye",
      "Tianrong Zhang",
      "Jiaqi Wang",
      "Han Liu",
      "Jinghui Chen",
      "Ting Wang",
      "Fenglong Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28500": {
    "title": "TF-CLIP: Learning Text-Free CLIP for Video-Based Person Re-identification",
    "volume": "main",
    "abstract": "Large-scale language-image pre-trained models (e.g., CLIP) have shown superior performances on many cross-modal retrieval tasks. However, the problem of transferring the knowledge learned from such models to video-based person re-identification (ReID) has barely been explored. In addition, there is a lack of decent text descriptions in current ReID benchmarks. To address these issues, in this work, we propose a novel one-stage text-free CLIP-based learning framework named TF-CLIP for video-based person ReID. More specifically, we extract the identity-specific sequence feature as the CLIP-Memory to replace the text feature. Meanwhile, we design a Sequence-Specific Prompt (SSP) module to update the CLIP-Memory online. To capture temporal information, we further propose a Temporal Memory Diffusion (TMD) module, which consists of two key components: Temporal Memory Construction (TMC) and Memory Diffusion (MD). Technically, TMC allows the frame-level memories in a sequence to communicate with each other, and to extract temporal information based on the relations within the sequence. MD further diffuses the temporal memories to each token in the original features to obtain more robust sequence features. Extensive experiments demonstrate that our proposed method shows much better results than other state-of-the-art methods on MARS, LS-VID and iLIDS-VID",
    "checked": true,
    "id": "7cfefc822da5f584901dac6c298e158613df43da",
    "semantic_title": "tf-clip: learning text-free clip for video-based person re-identification",
    "citation_count": 4,
    "authors": [
      "Chenyang Yu",
      "Xuehu Liu",
      "Yingquan Wang",
      "Pingping Zhang",
      "Huchuan Lu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28501": {
    "title": "MM-Point: Multi-View Information-Enhanced Multi-Modal Self-Supervised 3D Point Cloud Understanding",
    "volume": "main",
    "abstract": "In perception, multiple sensory information is integrated to map visual information from 2D views onto 3D objects, which is beneficial for understanding in 3D environments. But in terms of a single 2D view rendered from different angles, only limited partial information can be provided. The richness and value of Multi-view 2D information can provide superior self-supervised signals for 3D objects. In this paper, we propose a novel self-supervised point cloud representation learning method, MM-Point, which is driven by intra-modal and inter-modal similarity objectives. The core of MM-Point lies in the Multi-modal interaction and transmission between 3D objects and multiple 2D views at the same time. In order to more effectively simultaneously perform the consistent cross-modal objective of 2D multi-view information based on contrastive learning, we further propose Multi-MLP and Multi-level Augmentation strategies. Through carefully designed transformation strategies, we further learn Multi-level invariance in 2D Multi-views. MM-Point demonstrates state-of-the-art (SOTA) performance in various downstream tasks. For instance, it achieves a peak accuracy of 92.4% on the synthetic dataset ModelNet40, and a top accuracy of 87.8% on the real-world dataset ScanObjectNN, comparable to fully supervised methods. Additionally, we demonstrate its effectiveness in tasks such as few-shot classification, 3D part segmentation and 3D semantic segmentation",
    "checked": true,
    "id": "e89e4f900b4095200ae7a70defa6c66fb3cf161b",
    "semantic_title": "mm-point: multi-view information-enhanced multi-modal self-supervised 3d point cloud understanding",
    "citation_count": 1,
    "authors": [
      "Hai-Tao Yu",
      "Mofei Song"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28502": {
    "title": "Spatial Transform Decoupling for Oriented Object Detection",
    "volume": "main",
    "abstract": "Vision Transformers (ViTs) have achieved remarkable success in computer vision tasks. However, their potential in rotation-sensitive scenarios has not been fully explored, and this limitation may be inherently attributed to the lack of spatial invariance in the data-forwarding process. In this study, we present a novel approach, termed Spatial Transform Decoupling (STD), providing a simple-yet-effective solution for oriented object detection with ViTs. Built upon stacked ViT blocks, STD utilizes separate network branches to predict the position, size, and angle of bounding boxes, effectively harnessing the spatial transform potential of ViTs in a divide-and-conquer fashion. Moreover, by aggregating cascaded activation masks (CAMs) computed upon the regressed parameters, STD gradually enhances features within regions of interest (RoIs), which complements the self-attention mechanism. Without bells and whistles, STD achieves state-of-the-art performance on the benchmark datasets including DOTA-v1.0 (82.24% mAP) and HRSC2016 (98.55% mAP), which demonstrates the effectiveness of the proposed method. Source code is available at https://github.com/yuhongtian17/Spatial-Transform-Decoupling",
    "checked": true,
    "id": "e39cef1befcf357340effda9252d308aeb6a7235",
    "semantic_title": "spatial transform decoupling for oriented object detection",
    "citation_count": 5,
    "authors": [
      "Hongtian Yu",
      "Yunjie Tian",
      "Qixiang Ye",
      "Yunfan Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28503": {
    "title": "Step Vulnerability Guided Mean Fluctuation Adversarial Attack against Conditional Diffusion Models",
    "volume": "main",
    "abstract": "The high-quality generation results of conditional diffusion models have brought about concerns regarding privacy and copyright issues. As a possible technique for preventing the abuse of diffusion models, the adversarial attack against diffusion models has attracted academic attention recently. In this work, utilizing the phenomenon that diffusion models are highly sensitive to the mean value of the input noise, we propose the Mean Fluctuation Attack (MFA) to introduce mean fluctuations by shifting the mean values of the estimated noises during the reverse process. In addition, we reveal that the vulnerability of different reverse steps against adversarial attacks actually varies significantly. By modeling the step vulnerability and using it as guidance to sample the target steps for generating adversarial examples, the effectiveness of adversarial attacks can be substantially enhanced. Extensive experiments show that our algorithm can steadily cause the mean shift of the predicted noises so as to disrupt the entire reverse generation process and degrade the generation results significantly. We also demonstrate that the step vulnerability is intrinsic to the reverse process by verifying its effectiveness in an attack method other than MFA. Code and Supplementary is available at https://github.com/yuhongwei22/MFA",
    "checked": true,
    "id": "70512c218693310ba994bdec3254b2c3be4ed632",
    "semantic_title": "step vulnerability guided mean fluctuation adversarial attack against conditional diffusion models",
    "citation_count": 0,
    "authors": [
      "Hongwei Yu",
      "Jiansheng Chen",
      "Xinlong Ding",
      "Yudong Zhang",
      "Ting Tang",
      "Huimin Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28504": {
    "title": "PaintHuman: Towards High-Fidelity Text-to-3D Human Texturing via Denoised Score Distillation",
    "volume": "main",
    "abstract": "Recent advances in zero-shot text-to-3D human generation, which employ the human model prior (e.g., SMPL) or Score Distillation Sampling (SDS) with pre-trained text-to-image diffusion models, have been groundbreaking. However, SDS may provide inaccurate gradient directions under the weak diffusion guidance, as it tends to produce over-smoothed results and generate body textures that are inconsistent with the detailed mesh geometry. Therefore, directly leveraging existing strategies for high-fidelity text-to-3D human texturing is challenging. In this work, we propose a model called PaintHuman to addresses the challenges from two perspectives. We first propose a novel score function, Denoised Score Distillation (DSD), which directly modifies the SDS by introducing negative gradient components to iteratively correct the gradient direction and generate high-quality textures. In addition, we use the depth map as a geometric guide to ensure that the texture is semantically aligned to human mesh surfaces. To guarantee the quality of rendered results, we employ geometry-aware networks to predict surface materials and render realistic human textures. Extensive experiments, benchmarked against state-of-the-art (SoTA) methods, validate the efficacy of our approach.Project page: https://painthuman.github.io/",
    "checked": true,
    "id": "2421bfcd3886c017362e65b2d2d0f5251604a71b",
    "semantic_title": "painthuman: towards high-fidelity text-to-3d human texturing via denoised score distillation",
    "citation_count": 1,
    "authors": [
      "Jianhui Yu",
      "Hao Zhu",
      "Liming Jiang",
      "Chen Change Loy",
      "Weidong Cai",
      "Wayne Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28505": {
    "title": "CatFormer: Category-Level 6D Object Pose Estimation with Transformer",
    "volume": "main",
    "abstract": "Although there has been significant progress in category-level object pose estimation in recent years, there is still considerable room for improvement. In this paper, we propose a novel transformer-based category-level 6D pose estimation method called CatFormer to enhance the accuracy pose estimation. CatFormer comprises three main parts: a coarse deformation part, a fine deformation part, and a recurrent refinement part. In the coarse and fine deformation sections, we introduce a transformer-based deformation module that performs point cloud deformation and completion in the feature space. Additionally, after each deformation, we incorporate a transformer-based graph module to adjust fused features and establish geometric and topological relationships between points based on these features. Furthermore, we present an end-to-end recurrent refinement module that enables the prior point cloud to deform multiple times according to real scene features. We evaluate CatFormer's performance by training and testing it on CAMERA25 and REAL275 datasets. Experimental results demonstrate that CatFormer surpasses state-of-the-art methods. Moreover, we extend the usage of CatFormer to instance-level object pose estimation on the LINEMOD dataset, as well as object pose estimation in real-world scenarios. The experimental results validate the effectiveness and generalization capabilities of CatFormer. Our code and the supplemental materials are avaliable at https://github.com/BIT-robot-group/CatFormer",
    "checked": true,
    "id": "45a6153a4a69e3c8e0e11af0d8b3bd7027ad8478",
    "semantic_title": "catformer: category-level 6d object pose estimation with transformer",
    "citation_count": 1,
    "authors": [
      "Sheng Yu",
      "Di-Hua Zhai",
      "Yuanqing Xia"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28506": {
    "title": "DME: Unveiling the Bias for Better Generalized Monocular Depth Estimation",
    "volume": "main",
    "abstract": "This paper aims to design monocular depth estimation models with better generalization abilities. To this end, we have conducted quantitative analysis and discovered two important insights. First, the Simulation Correlation phenomenon, commonly seen in long-tailed classification problems, also exists in monocular depth estimation, indicating that the imbalanced depth distribution in training data may be the cause of limited generalization ability. Second, the imbalanced and long-tail distribution of depth values extends beyond the dataset scale, and also manifests within each individual image, further exacerbating the challenge of monocular depth estimation. Motivated by the above findings, we propose the Distance-aware Multi-Expert (DME) depth estimation model. Unlike prior methods that handle different depth range indiscriminately, DME adopts a divide-and-conquer philosophy where each expert is responsible for depth estimation of regions within a specific depth range. As such, the depth distribution seen by each expert is more uniform and can be more easily predicted. A pixel-level routing module is further designed and learned to stitch the prediction of all experts into the final depth map. Experiments show that DME achieves state-of-the-art performance on both NYU-Depth v2 and KITTI, and also delivers favorable zero-shot generalization capability on unseen datasets",
    "checked": true,
    "id": "2de90b72af0f825bc326ca3aee5f69a858c899e6",
    "semantic_title": "dme: unveiling the bias for better generalized monocular depth estimation",
    "citation_count": 0,
    "authors": [
      "Songsong Yu",
      "Yifan Wang",
      "Yunzhi Zhuge",
      "Lijun Wang",
      "Huchuan Lu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28507": {
    "title": "DOCTR: Disentangled Object-Centric Transformer for Point Scene Understanding",
    "volume": "main",
    "abstract": "Point scene understanding is a challenging task to process real-world scene point cloud, which aims at segmenting each object, estimating its pose, and reconstructing its mesh simultaneously. Recent state-of-the-art method first segments each object and then processes them independently with multiple stages for the different sub-tasks. This leads to a complex pipeline to optimize and makes it hard to leverage the relationship constraints between multiple objects. In this work, we propose a novel Disentangled Object-Centric TRansformer (DOCTR) that explores object-centric representation to facilitate learning with multiple objects for the multiple sub-tasks in a unified manner. Each object is represented as a query, and a Transformer decoder is adapted to iteratively optimize all the queries involving their relationship. In particular, we introduce a semantic-geometry disentangled query (SGDQ) design that enables the query features to attend separately to semantic information and geometric information relevant to the corresponding sub-tasks. A hybrid bipartite matching module is employed to well use the supervisions from all the sub-tasks during training. Qualitative and quantitative experimental results demonstrate that our method achieves state-of-the-art performance on the challenging ScanNet dataset. Code is available at https://github.com/SAITPublic/DOCTR",
    "checked": true,
    "id": "36680487f13f9c67f970ba56a8a8245429a90d58",
    "semantic_title": "doctr: disentangled object-centric transformer for point scene understanding",
    "citation_count": 0,
    "authors": [
      "Xiaoxuan Yu",
      "Hao Wang",
      "Weiming Li",
      "Qiang Wang",
      "Soonyong Cho",
      "Younghun Sung"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28508": {
    "title": "Discretization-Induced Dirichlet Posterior for Robust Uncertainty Quantification on Regression",
    "volume": "main",
    "abstract": "Uncertainty quantification is critical for deploying deep neural networks (DNNs) in real-world applications. An Auxiliary Uncertainty Estimator (AuxUE) is one of the most effective means to estimate the uncertainty of the main task prediction without modifying the main task model. To be considered robust, an AuxUE must be capable of maintaining its performance and triggering higher uncertainties while encountering Out-of-Distribution (OOD) inputs, i.e., to provide robust aleatoric and epistemic uncertainty. However, for vision regression tasks, current AuxUE designs are mainly adopted for aleatoric uncertainty estimates, and AuxUE robustness has not been explored. In this work, we propose a generalized AuxUE scheme for more robust uncertainty quantification on regression tasks. Concretely, to achieve a more robust aleatoric uncertainty estimation, different distribution assumptions are considered for heteroscedastic noise, and Laplace distribution is finally chosen to approximate the prediction error. For epistemic uncertainty, we propose a novel solution named Discretization-Induced Dirichlet pOsterior (DIDO), which models the Dirichlet posterior on the discretized prediction error. Extensive experiments on age estimation, monocular depth estimation, and super-resolution tasks show that our proposed method can provide robust uncertainty estimates in the face of noisy inputs and that it can be scalable to both image-level and pixel-wise tasks",
    "checked": true,
    "id": "9915ef6a60bcc75f41fac35e267b7ff9033c3801",
    "semantic_title": "discretization-induced dirichlet posterior for robust uncertainty quantification on regression",
    "citation_count": 1,
    "authors": [
      "Xuanlong Yu",
      "Gianni Franchi",
      "Jindong Gu",
      "Emanuel Aldea"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28509": {
    "title": "Attacks on Continual Semantic Segmentation by Perturbing Incremental Samples",
    "volume": "main",
    "abstract": "As an essential computer vision task, Continual Semantic Segmentation (CSS) has received a lot of attention. However, security issues regarding this task have not been fully studied. To bridge this gap, we study the problem of attacks in CSS in this paper. We first propose a new task, namely, attacks on incremental samples in CSS, and reveal that the attacks on incremental samples corrupt the performance of CSS in both old and new classes. Moreover, we present an adversarial sample generation method based on class shift, namely Class Shift Attack (CS-Attack), which is an offline and easy-to-implement approach for CSS. CS-Attack is able to significantly degrade the performance of models on both old and new classes without knowledge of the incremental learning approach, which undermines the original purpose of the incremental learning, i.e., learning new classes while retaining old knowledge. Experiments show that on the popular datasets Pascal VOC, ADE20k, and Cityscapes, our approach easily degrades the performance of currently popular CSS methods, which reveals the importance of security in CSS",
    "checked": true,
    "id": "f2f1d2199f620b76cf165ea79c67c59931a3de62",
    "semantic_title": "attacks on continual semantic segmentation by perturbing incremental samples",
    "citation_count": 0,
    "authors": [
      "Zhidong Yu",
      "Wei Yang",
      "Xike Xie",
      "Zhenbo Shi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28510": {
    "title": "Data-Free Hard-Label Robustness Stealing Attack",
    "volume": "main",
    "abstract": "The popularity of Machine Learning as a Service (MLaaS) has led to increased concerns about Model Stealing Attacks (MSA), which aim to craft a clone model by querying MLaaS. Currently, most research on MSA assumes that MLaaS can provide soft labels and that the attacker has a proxy dataset with a similar distribution. However, this fails to encapsulate the more practical scenario where only hard labels are returned by MLaaS and the data distribution remains elusive. Furthermore, most existing work focuses solely on stealing the model accuracy, neglecting the model robustness, while robustness is essential in security-sensitive scenarios, e.g, face-scan payment. Notably, improving model robustness often necessitates the use of expensive techniques such as adversarial training, thereby further making stealing robustness a more lucrative prospect. In response to these identified gaps, we introduce a novel Data-Free Hard-Label Robustness Stealing (DFHL-RS) attack in this paper, which enables the stealing of both model accuracy and robustness by simply querying hard labels of the target model without the help of any natural data. Comprehensive experiments demonstrate the effectiveness of our method. The clone model achieves a clean accuracy of 77.86% and a robust accuracy of 39.51% against AutoAttack, which are only 4.71% and 8.40% lower than the target model on the CIFAR-10 dataset, significantly exceeding the baselines. Our code is available at: https://github.com/LetheSec/DFHL-RS-Attack",
    "checked": true,
    "id": "23faead2015ba1e36a2c0c535f987c0b36ba8534",
    "semantic_title": "data-free hard-label robustness stealing attack",
    "citation_count": 2,
    "authors": [
      "Xiaojian Yuan",
      "Kejiang Chen",
      "Wen Huang",
      "Jie Zhang",
      "Weiming Zhang",
      "Nenghai Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28511": {
    "title": "Efficient Conditional Diffusion Model with Probability Flow Sampling for Image Super-resolution",
    "volume": "main",
    "abstract": "Image super-resolution is a fundamentally ill-posed problem because multiple valid high-resolution images exist for one low-resolution image. Super-resolution methods based on diffusion probabilistic models can deal with the ill-posed nature by learning the distribution of high-resolution images conditioned on low-resolution images, avoiding the problem of blurry images in PSNR-oriented methods. However, existing diffusion-based super-resolution methods have high time consumption with the use of iterative sampling, while the quality and consistency of generated images are less than ideal due to problems like color shifting. In this paper, we propose Efficient Conditional Diffusion Model with Probability Flow Sampling (ECDP) for image super-resolution. To reduce the time consumption, we design a continuous-time conditional diffusion model for image super-resolution, which enables the use of probability flow sampling for efficient generation. Additionally, to improve the consistency of generated images, we propose a hybrid parametrization for the denoiser network, which interpolates between the data-predicting parametrization and the noise-predicting parametrization for different noise scales. Moreover, we design an image quality loss as a complement to the score matching loss of diffusion models, further improving the consistency and quality of super-resolution. Extensive experiments on DIV2K, ImageNet, and CelebA demonstrate that our method achieves higher super-resolution quality than existing diffusion-based image super-resolution methods while having lower time consumption. Our code is available at https://github.com/Yuan-Yutao/ECDP",
    "checked": true,
    "id": "232d63c721f0de2d38a2afb5324de57f7cc8d2de",
    "semantic_title": "efficient conditional diffusion model with probability flow sampling for image super-resolution",
    "citation_count": 1,
    "authors": [
      "Yutao Yuan",
      "Chun Yuan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28512": {
    "title": "SD-MVS: Segmentation-Driven Deformation Multi-View Stereo with Spherical Refinement and EM Optimization",
    "volume": "main",
    "abstract": "In this paper, we introduce Segmentation-Driven Deformation Multi-View Stereo (SD-MVS), a method that can effectively tackle challenges in 3D reconstruction of textureless areas. We are the first to adopt the Segment Anything Model (SAM) to distinguish semantic instances in scenes and further leverage these constraints for pixelwise patch deformation on both matching cost and propagation. Concurrently, we propose a unique refinement strategy that combines spherical coordinates and gradient descent on normals and pixelwise search interval on depths, significantly improving the completeness of reconstructed 3D model. Furthermore, we adopt the Expectation-Maximization (EM) algorithm to alternately optimize the aggregate matching cost and hyperparameters, effectively mitigating the problem of parameters being excessively dependent on empirical tuning. Evaluations on the ETH3D high-resolution multi-view stereo benchmark and the Tanks and Temples dataset demonstrate that our method can achieve state-of-the-art results with less time consumption",
    "checked": true,
    "id": "6d85aa9a18216308910bf31ddd6c42a701f7e9cf",
    "semantic_title": "sd-mvs: segmentation-driven deformation multi-view stereo with spherical refinement and em optimization",
    "citation_count": 0,
    "authors": [
      "Zhenlong Yuan",
      "Jiakai Cao",
      "Zhaoxin Li",
      "Hao Jiang",
      "Zhaoqi Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28513": {
    "title": "KeDuSR: Real-World Dual-Lens Super-Resolution via Kernel-Free Matching",
    "volume": "main",
    "abstract": "Dual-lens super-resolution (SR) is a practical scenario for reference (Ref) based SR by utilizing the telephoto image (Ref) to assist the super-resolution of the low-resolution wide-angle image (LR input). Different from general RefSR, the Ref in dual-lens SR only covers the overlapped field of view (FoV) area. However, current dual-lens SR methods rarely utilize these specific characteristics and directly perform dense matching between the LR input and Ref. Due to the resolution gap between LR and Ref, the matching may miss the best-matched candidate and destroy the consistent structures in the overlapped FoV area. Different from them, we propose to first align the Ref with the center region (namely the overlapped FoV area) of the LR input by combining global warping and local warping to make the aligned Ref be sharp and consistent. Then, we formulate the aligned Ref and LR center as value-key pairs, and the corner region of the LR is formulated as queries. In this way, we propose a kernel-free matching strategy by matching between the LR-corner (query) and LR-center (key) regions, and the corresponding aligned Ref (value) can be warped to the corner region of the target. Our kernel-free matching strategy avoids the resolution gap between LR and Ref, which makes our network have better generalization ability. In addition, we construct a DuSR-Real dataset with (LR, Ref, HR) triples, where the LR and HR are well aligned. Experiments on three datasets demonstrate that our method outperforms the second-best method by a large margin. Our code and dataset are available at https://github.com/ZifanCui/KeDuSR",
    "checked": true,
    "id": "f7a5ba20dd7197190f0082d8fedd314fd404dbae",
    "semantic_title": "kedusr: real-world dual-lens super-resolution via kernel-free matching",
    "citation_count": 0,
    "authors": [
      "Huanjing Yue",
      "Zifan Cui",
      "Kun Li",
      "Jingyu Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28514": {
    "title": "SurgicalSAM: Efficient Class Promptable Surgical Instrument Segmentation",
    "volume": "main",
    "abstract": "The Segment Anything Model (SAM) is a powerful foundation model that has revolutionised image segmentation. To apply SAM to surgical instrument segmentation, a common approach is to locate precise points or boxes of instruments and then use them as prompts for SAM in a zero-shot manner. However, we observe two problems with this naive pipeline: (1) the domain gap between natural objects and surgical instruments leads to inferior generalisation of SAM; and (2) SAM relies on precise point or box locations for accurate segmentation, requiring either extensive manual guidance or a well-performing specialist detector for prompt preparation, which leads to a complex multi-stage pipeline. To address these problems, we introduce SurgicalSAM, a novel end-to-end efficient-tuning approach for SAM to effectively integrate surgical-specific information with SAM's pre-trained knowledge for improved generalisation. Specifically, we propose a lightweight prototype-based class prompt encoder for tuning, which directly generates prompt embeddings from class prototypes and eliminates the use of explicit prompts for improved robustness and a simpler pipeline. In addition, to address the low inter-class variance among surgical instrument categories, we propose contrastive prototype learning, further enhancing the discrimination of the class prototypes for more accurate class prompting. The results of extensive experiments on both EndoVis2018 and EndoVis2017 datasets demonstrate that SurgicalSAM achieves state-of-the-art performance while only requiring a small number of tunable parameters. The source code is available at https://github.com/wenxi-yue/SurgicalSAM",
    "checked": true,
    "id": "2cfbeb0c2a0c268773e3b952f022aada9cfd0538",
    "semantic_title": "surgicalsam: efficient class promptable surgical instrument segmentation",
    "citation_count": 19,
    "authors": [
      "Wenxi Yue",
      "Jing Zhang",
      "Kun Hu",
      "Yong Xia",
      "Jiebo Luo",
      "Zhiyong Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28515": {
    "title": "Unveiling Details in the Dark: Simultaneous Brightening and Zooming for Low-Light Image Enhancement",
    "volume": "main",
    "abstract": "Existing super-resolution methods exhibit limitations when applied to nighttime scenes, primarily due to their lack of adaptation to low-pair dynamic range and noise-heavy dark-light images. In response, this research introduces an innovative customized framework to simultaneously Brighten and Zoom in low-resolution images captured in low-light conditions, dubbed BrZoNet. The core method begins by feeding low-light, low-resolution images, and their corresponding ground truths into the Retinex-induced siamese decoupling network. This process yields distinct reflectance maps and illuminance maps, guided by supervision from the ground truth's decomposition maps. Subsequently, these reflectance and illuminance maps transition into an intricate super-resolution sub-network. This sub-network employs a meticulously designed cross-layer content-aware interactor - Illumination-aware Interaction Unit(IaIU), elegantly endowed with a gating mechanism. The IaIU facilitates meaningful feature interaction between illuminance and reflectance features while effectively reducing unwanted noise. An intricate super-resolution cage is also constructed to comprehensively integrate information, ultimately resulting in the generation of high-resolution images featuring intricate details. Thorough and diverse experiments validate the superiority of the proposed BrZoNet, surpassing contemporary cutting-edge technologies by proficiently augmenting brightness and intricately recovering complex details, showcasing advancements of 7.1% in PSNR, 2.4% in SSIM, and an impressive 36.8% in LPIPS metrics",
    "checked": true,
    "id": "94b1b3cf9f940d126dc504b7a6dc7e2ac1fc8fc4",
    "semantic_title": "unveiling details in the dark: simultaneous brightening and zooming for low-light image enhancement",
    "citation_count": 1,
    "authors": [
      "Ziyu Yue",
      "Jiaxin Gao",
      "Zhixun Su"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28516": {
    "title": "Weakly-Supervised Temporal Action Localization by Inferring Salient Snippet-Feature",
    "volume": "main",
    "abstract": "Weakly-supervised temporal action localization aims to locate action regions and identify action categories in untrimmed videos simultaneously by taking only video-level labels as the supervision. Pseudo label generation is a promising strategy to solve the challenging problem, but the current methods ignore the natural temporal structure of the video that can provide rich information to assist such a generation process. In this paper, we propose a novel weakly-supervised temporal action localization method by inferring salient snippet-feature. First, we design a saliency inference module that exploits the variation relationship between temporal neighbor snippets to discover salient snippet-features, which can reflect the significant dynamic change in the video. Secondly, we introduce a boundary refinement module that enhances salient snippet-features through the information interaction unit. Then, a discrimination enhancement module is introduced to enhance the discriminative nature of snippet-features. Finally, we adopt the refined snippet-features to produce high-fidelity pseudo labels, which could be used to supervise the training of the action localization network. Extensive experiments on two publicly available datasets, i.e., THUMOS14 and ActivityNet v1.3, demonstrate our proposed method achieves significant improvements compared to the state-of-the-art methods. Our source code is available at https://github.com/wuli55555/ISSF",
    "checked": true,
    "id": "c83cf73ab8482ca2ef2f9b3e9cafee6189b947bd",
    "semantic_title": "weakly-supervised temporal action localization by inferring salient snippet-feature",
    "citation_count": 2,
    "authors": [
      "Wulian Yun",
      "Mengshi Qi",
      "Chuanming Wang",
      "Huadong Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28517": {
    "title": "Behavioral Recognition of Skeletal Data Based on Targeted Dual Fusion Strategy",
    "volume": "main",
    "abstract": "The deployment of multi-stream fusion strategy on behavioral recognition from skeletal data can extract complementary features from different information streams and improve the recognition accuracy, but suffers from high model complexity and a large number of parameters. Besides, existing multi-stream methods using a fixed adjacency matrix homogenizes the model's discrimination process across diverse actions, causing reduction of the actual lift for the multi-stream model. Finally, attention mechanisms are commonly applied to the multi-dimensional features, including spatial, temporal and channel dimensions. But their attention scores are typically fused in a concatenated manner, leading to the ignorance of the interrelation between joints in complex actions. To alleviate these issues, the Front-Rear dual Fusion Graph Convolutional Network (FRF-GCN) is proposed to provide a lightweight model based on skeletal data. Targeted adjacency matrices are also designed for different front fusion streams, allowing the model to focus on actions of varying magnitudes. Simultaneously, the mechanism of Spatial-Temporal-Channel Parallel Attention (STC-P), which processes attention in parallel and places greater emphasis on useful information, is proposed to further improve model's performance. FRF-GCN demonstrates significant competitiveness compared to the current state-of-the-art methods on the NTU RGB+D, NTU RGB+D 120 and Kinetics-Skeleton 400 datasets. Our code is available at: https://github.com/sunbeam-kkt/FRF-GCN-master",
    "checked": true,
    "id": "d2b7530f6b9138e6f4664239deeb30f0b3fda46e",
    "semantic_title": "behavioral recognition of skeletal data based on targeted dual fusion strategy",
    "citation_count": 1,
    "authors": [
      "Xiao Yun",
      "Chenglong Xu",
      "Kevin Riou",
      "Kaiwen Dong",
      "Yanjing Sun",
      "Song Li",
      "Kevin Subrin",
      "Patrick Le Callet"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28518": {
    "title": "Zero-Shot Aerial Object Detection with Visual Description Regularization",
    "volume": "main",
    "abstract": "Existing object detection models are mainly trained on large-scale labeled datasets. However, annotating data for novel aerial object classes is expensive since it is time-consuming and may require expert knowledge. Thus, it is desirable to study label-efficient object detection methods on aerial images. In this work, we propose a zero-shot method for aerial object detection named visual Description Regularization, or DescReg. Concretely, we identify the weak semantic-visual correlation of the aerial objects and aim to address the challenge with prior descriptions of their visual appearance. Instead of directly encoding the descriptions into class embedding space which suffers from the representation gap problem, we propose to infuse the prior inter-class visual similarity conveyed in the descriptions into the embedding learning. The infusion process is accomplished with a newly designed similarity-aware triplet loss which incorporates structured regularization on the representation space. We conduct extensive experiments with three challenging aerial object detection datasets, including DIOR, xView, and DOTA. The results demonstrate that DescReg significantly outperforms the state-of-the-art ZSD methods with complex projection designs and generative frameworks, e.g., DescReg outperforms best reported ZSD method on DIOR by 4.5 mAP on unseen classes and 8.1 in HM. We further show the generalizability of DescReg by integrating it into generative ZSD methods as well as varying the detection architecture. Codes will be released at https://github.com/zq-zang/DescReg",
    "checked": true,
    "id": "74498a3517e1ff902c560d84ee519ed9fa8dce49",
    "semantic_title": "zero-shot aerial object detection with visual description regularization",
    "citation_count": 1,
    "authors": [
      "Zhengqing Zang",
      "Chenyu Lin",
      "Chenwei Tang",
      "Tao Wang",
      "Jiancheng Lv"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28519": {
    "title": "Controllable Mind Visual Diffusion Model",
    "volume": "main",
    "abstract": "Brain signal visualization has emerged as an active research area, serving as a critical interface between the human visual system and computer vision models. Diffusion-based methods have recently shown promise in analyzing functional magnetic resonance imaging (fMRI) data, including the reconstruction of high-quality images consistent with original visual stimuli. Nonetheless, it remains a critical challenge to effectively harness the semantic and silhouette information extracted from brain signals. In this paper, we propose a novel approach, termed as Controllable Mind Visual Diffusion Model (CMVDM). Specifically, CMVDM first extracts semantic and silhouette information from fMRI data using attribute alignment and assistant networks. Then, a control model is introduced in conjunction with a residual block to fully exploit the extracted information for image synthesis, generating high-quality images that closely resemble the original visual stimuli in both semantic content and silhouette characteristics. Through extensive experimentation, we demonstrate that CMVDM outperforms existing state-of-the-art methods both qualitatively and quantitatively. Our code is available at https://github.com/zengbohan0217/CMVDM",
    "checked": true,
    "id": "907023defd2d8c189192acd9d4c35628ac6ba868",
    "semantic_title": "controllable mind visual diffusion model",
    "citation_count": 12,
    "authors": [
      "Bohan Zeng",
      "Shanglin Li",
      "Xuhui Liu",
      "Sicheng Gao",
      "Xiaolong Jiang",
      "Xu Tang",
      "Yao Hu",
      "Jianzhuang Liu",
      "Baochang Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28520": {
    "title": "MGQFormer: Mask-Guided Query-Based Transformer for Image Manipulation Localization",
    "volume": "main",
    "abstract": "Deep learning-based models have made great progress in image tampering localization, which aims to distinguish between manipulated and authentic regions. However, these models suffer from inefficient training. This is because they use ground-truth mask labels mainly through the cross-entropy loss, which prioritizes per-pixel precision but disregards the spatial location and shape details of manipulated regions. To address this problem, we propose a Mask-Guided Query-based Transformer Framework (MGQFormer), which uses ground-truth masks to guide the learnable query token (LQT) in identifying the forged regions. Specifically, we extract feature embeddings of ground-truth masks as the guiding query token (GQT) and feed GQT and LQT into MGQFormer to estimate fake regions, respectively. Then we make MGQFormer learn the position and shape information in ground-truth mask labels by proposing a mask-guided loss to reduce the feature distance between GQT and LQT. We also observe that such mask-guided training strategy has a significant impact on the convergence speed of MGQFormer training. Extensive experiments on multiple benchmarks show that our method significantly improves over state-of-the-art methods",
    "checked": true,
    "id": "fb3277e3771d7121c07137856691f62394589029",
    "semantic_title": "mgqformer: mask-guided query-based transformer for image manipulation localization",
    "citation_count": 1,
    "authors": [
      "Kunlun Zeng",
      "Ri Cheng",
      "Weimin Tan",
      "Bo Yan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28521": {
    "title": "Weakly-Supervised Mirror Detection via Scribble Annotations",
    "volume": "main",
    "abstract": "Mirror detection is of great significance for avoiding false recognition of reflected objects in computer vision tasks. Existing mirror detection frameworks usually follow a supervised setting, which relies heavily on high quality labels and suffers from poor generalization. To resolve this, we instead propose the first weakly-supervised mirror detection framework and also provide the first scribble-based mirror dataset. Specifically, we relabel 10,158 images, most of which have a labeled pixel ratio of less than 0.01 and take only about 8 seconds to label. Considering that the mirror regions usually show great scale variation, and also irregular and occluded, thus leading to issues of incomplete or over detection, we propose a local-global feature enhancement (LGFE) module to fully capture the context and details. Moreover, it is difficult to obtain basic mirror structure using scribble annotation, and the distinction between foreground (mirror) and background (non-mirror) features is not emphasized caused by mirror reflections. Therefore, we propose a foreground-aware mask attention (FAMA), integrating mirror edges and semantic features to complete mirror regions and suppressing the influence of backgrounds. Finally, to improve the robustness of the network, we propose a prototype contrast loss (PCL) to learn more general foreground features across images. Extensive experiments show that our network outperforms relevant state-of-the-art weakly supervised methods, and even some fully supervised methods. The dataset and codes are available at https://github.com/winter-flow/WSMD",
    "checked": true,
    "id": "f3f1cdbfac9dee8facc629c13bd9f3eaed924de5",
    "semantic_title": "weakly-supervised mirror detection via scribble annotations",
    "citation_count": 0,
    "authors": [
      "Mingfeng Zha",
      "Yunqiang Pei",
      "Guoqing Wang",
      "Tianyu Li",
      "Yang Yang",
      "Wenbin Qian",
      "Heng Tao Shen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28522": {
    "title": "Towards Compact 3D Representations via Point Feature Enhancement Masked Autoencoders",
    "volume": "main",
    "abstract": "Learning 3D representation plays a critical role in masked autoencoder (MAE) based pre-training methods for point cloud, including single-modal and cross-modal based MAE. Specifically, although cross-modal MAE methods learn strong 3D representations via the auxiliary of other modal knowledge, they often suffer from heavy computational burdens and heavily rely on massive cross-modal data pairs that are often unavailable, which hinders their applications in practice. Instead, single-modal methods with solely point clouds as input are preferred in real applications due to their simplicity and efficiency. However, such methods easily suffer from limited 3D representations with global random mask input. To learn compact 3D representations, we propose a simple yet effective Point Feature Enhancement Masked Autoencoders (Point-FEMAE), which mainly consists of a global branch and a local branch to capture latent semantic features. Specifically, to learn more compact features, a share-parameter Transformer encoder is introduced to extract point features from the global and local unmasked patches obtained by global random and local block mask strategies, followed by a specific decoder to reconstruct. Meanwhile, to further enhance features in the local branch, we propose a Local Enhancement Module with local patch convolution to perceive fine-grained local context at larger scales. Our method significantly improves the pre-training efficiency compared to cross-modal alternatives, and extensive downstream experiments underscore the state-of-the-art effectiveness, particularly outperforming our baseline (Point-MAE) by 5.16%, 5.00%, and 5.04% in three variants of ScanObjectNN, respectively. Code is available at https://github.com/zyh16143998882/AAAI24-PointFEMAE",
    "checked": true,
    "id": "0425c6019c19f939b01ac5be5f544c6a895373b4",
    "semantic_title": "towards compact 3d representations via point feature enhancement masked autoencoders",
    "citation_count": 5,
    "authors": [
      "Yaohua Zha",
      "Huizhen Ji",
      "Jinmin Li",
      "Rongsheng Li",
      "Tao Dai",
      "Bin Chen",
      "Zhi Wang",
      "Shu-Tao Xia"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28523": {
    "title": "Fine-Grained Knowledge Selection and Restoration for Non-exemplar Class Incremental Learning",
    "volume": "main",
    "abstract": "Non-exemplar class incremental learning aims to learn both the new and old tasks without accessing any training data from the past. This strict restriction enlarges the difficulty of alleviating catastrophic forgetting since all techniques can only be applied to current task data. Considering this challenge, we propose a novel framework of fine-grained knowledge selection and restoration. The conventional knowledge distillation-based methods place too strict constraints on the network parameters and features to prevent forgetting, which limits the training of new tasks. To loose this constraint, we proposed a novel fine-grained selective patch-level distillation to adaptively balance plasticity and stability. Some task-agnostic patches can be used to preserve the decision boundary of the old task. While some patches containing the important foreground are favorable for learning the new task. Moreover, we employ a task-agnostic mechanism to generate more realistic prototypes of old tasks with the current task sample for reducing classifier bias for fine-grained knowledge restoration. Extensive experiments on CIFAR100, TinyImageNet and ImageNet-Subset demonstrate the effectiveness of our method. Code is available at https://github.com/scok30/vit-cil",
    "checked": true,
    "id": "9c4fcc7a1ae2f9d552551146a6d423296be9a717",
    "semantic_title": "fine-grained knowledge selection and restoration for non-exemplar class incremental learning",
    "citation_count": 1,
    "authors": [
      "Jiang-Tian Zhai",
      "Xialei Liu",
      "Lu Yu",
      "Ming-Ming Cheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28524": {
    "title": "Multi-Prompts Learning with Cross-Modal Alignment for Attribute-Based Person Re-identification",
    "volume": "main",
    "abstract": "The fine-grained attribute descriptions can significantly supplement the valuable semantic information for person image, which is vital to the success of person re-identification (ReID) task. However, current ReID algorithms typically failed to effectively leverage the rich contextual information available, primarily due to their reliance on simplistic and coarse utilization of image attributes. Recent advances in artificial intelligence generated content have made it possible to automatically generate plentiful fine-grained attribute descriptions and make full use of them. Thereby, this paper explores the potential of using the generated multiple person attributes as prompts in ReID tasks with off-the-shelf (large) models for more accurate retrieval results. To this end, we present a new framework called Multi-Prompts ReID (MP-ReID), based on prompt learning and language models, to fully dip fine attributes to assist ReID task. Specifically, MP-ReID first learns to hallucinate diverse, informative, and promptable sentences for describing the query images. This procedure includes (i) explicit prompts of which attributes a person has and furthermore (ii) implicit learnable prompts for adjusting/conditioning the criteria used towards this person identity matching. Explicit prompts are obtained by ensembling generation models, such as ChatGPT and VQA models. Moreover, an alignment module is designed to fuse multi-prompts (i.e., explicit and implicit ones) progressively and mitigate the cross-modal gap. Extensive experiments on the existing attribute-involved ReID datasets, namely, Market1501 and DukeMTMC-reID, demonstrate the effectiveness and rationality of the proposed MP-ReID solution",
    "checked": true,
    "id": "be76594a206c40e8998f7722f8e5f0304e9f17bf",
    "semantic_title": "multi-prompts learning with cross-modal alignment for attribute-based person re-identification",
    "citation_count": 2,
    "authors": [
      "Yajing Zhai",
      "Yawen Zeng",
      "Zhiyong  Huang",
      "Zheng Qin",
      "Xin Jin",
      "Da Cao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28525": {
    "title": "Mono3DVG: 3D Visual Grounding in Monocular Images",
    "volume": "main",
    "abstract": "We introduce a novel task of 3D visual grounding in monocular RGB images using language descriptions with both appearance and geometry information. Specifically, we build a large-scale dataset, Mono3DRefer, which contains 3D object targets with their corresponding geometric text descriptions, generated by ChatGPT and refined manually. To foster this task, we propose Mono3DVG-TR, an end-to-end transformer-based network, which takes advantage of both the appearance and geometry information in text embeddings for multi-modal learning and 3D object localization. Depth predictor is designed to explicitly learn geometry features. The dual text-guided adapter is proposed to refine multiscale visual and geometry features of the referred object. Based on depth-text-visual stacking attention, the decoder fuses object-level geometric cues and visual appearance into a learnable query. Comprehensive benchmarks and some insightful analyses are provided for Mono3DVG. Extensive comparisons and ablation studies show that our method significantly outperforms all baselines. The dataset and code will be released",
    "checked": true,
    "id": "abfdcab6879729544410c8bdcd2c47387749c5ad",
    "semantic_title": "mono3dvg: 3d visual grounding in monocular images",
    "citation_count": 4,
    "authors": [
      "Yang Zhan",
      "Yuan Yuan",
      "Zhitong Xiong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28526": {
    "title": "Amodal Scene Analysis via Holistic Occlusion Relation Inference and Generative Mask Completion",
    "volume": "main",
    "abstract": "Amodal scene analysis entails interpreting the occlusion relationship among scene elements and inferring the possible shapes of the invisible parts. Existing methods typically frame this task as an extended instance segmentation or a pair-wise object de-occlusion problem. In this work, we propose a new framework, which comprises a Holistic Occlusion Relation Inference (HORI) module followed by an instance-level Generative Mask Completion (GMC) module. Unlike previous approaches, which rely on mask completion results for occlusion reasoning, our HORI module directly predicts an occlusion relation matrix in a single pass. This approach is much more efficient than the pair-wise de-occlusion process and it naturally handles mutual occlusion, a common but often neglected situation. Moreover, we formulate the mask completion task as a generative process and use a diffusion-based GMC module for instance-level mask completion. This improves mask completion quality and provides multiple plausible solutions. We further introduce a large-scale amodal segmentation dataset with high-quality human annotations, including mutual occlusions. Experiments on our dataset and two public benchmarks demonstrate the advantages of our method. code public available at https://github.com/zbwxp/Amodal-AAAI",
    "checked": true,
    "id": "814bd644a60d7dda1657696ee382c5b9ab67a84d",
    "semantic_title": "amodal scene analysis via holistic occlusion relation inference and generative mask completion",
    "citation_count": 0,
    "authors": [
      "Bowen Zhang",
      "Qing Liu",
      "Jianming Zhang",
      "Yilin Wang",
      "Liyang Liu",
      "Zhe Lin",
      "Yifan Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28527": {
    "title": "High-Quality Real-Time Rendering Using Subpixel Sampling Reconstruction",
    "volume": "main",
    "abstract": "Generating high-quality, realistic rendering images for real-time applications generally requires tracing a few samples-per-pixel (spp) and using deep learning-based approaches to denoise the resulting low-spp images. Existing denoising methods necessitate a substantial time expenditure when rendering at high resolutions due to the physically-based sampling and network inference time burdens. In this paper, we propose a novel Monte Carlo sampling strategy to accelerate the sampling process and a corresponding denoiser, subpixel sampling reconstruction (SSR), to obtain high-quality images. Extensive experiments demonstrate that our method significantly outperforms previous approaches in denoising quality and reduces overall time costs, enabling real-time rendering capabilities at 2K resolution",
    "checked": true,
    "id": "ed36968b41482c2dd97676726a541e97385d65c5",
    "semantic_title": "high-quality real-time rendering using subpixel sampling reconstruction",
    "citation_count": 0,
    "authors": [
      "Boyu Zhang",
      "Hongliang Yuan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28528": {
    "title": "Weakly Supervised Few-Shot Object Detection with DETR",
    "volume": "main",
    "abstract": "In recent years, Few-shot Object Detection (FSOD) has become an increasingly important research topic in computer vision. However, existing FSOD methods require strong annotations including category labels and bounding boxes, and their performance is heavily dependent on the quality of box annotations. However, acquiring strong annotations is both expensive and time-consuming. This inspires the study on weakly supervised FSOD (WS-FSOD in short), which realizes FSOD with only image-level annotations, i.e., category labels. In this paper, we propose a new and effective weakly supervised FSOD method named WFS-DETR. By a well-designed pretraining process, WFS-DETR first acquires general object localization and integrity judgment capabilities on large-scale pretraining data. Then, it introduces object integrity into multiple-instance learning to solve the common local optimum problem by comprehensively exploiting both semantic and visual information. Finally, with simple fine-tuning, it transfers the knowledge learned from the base classes to the novel classes, which enables accurate detection of novel objects. Benefiting from this ``pretraining-refinement'' mechanism, WSF-DETR can achieve good generalization on different datasets. Extensive experiments also show that the proposed method clearly outperforms the existing counterparts in the WS-FSOD task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenbo Zhang",
      "Yinglu Zhang",
      "Lu Zhang",
      "Jiajia Zhao",
      "Jihong Guan",
      "Shuigeng Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28529": {
    "title": "S2WAT: Image Style Transfer via Hierarchical Vision Transformer Using Strips Window Attention",
    "volume": "main",
    "abstract": "Transformer's recent integration into style transfer leverages its proficiency in establishing long-range dependencies, albeit at the expense of attenuated local modeling. This paper introduces Strips Window Attention Transformer (S2WAT), a novel hierarchical vision transformer designed for style transfer. S2WAT employs attention computation in diverse window shapes to capture both short- and long-range dependencies. The merged dependencies utilize the \"Attn Merge\" strategy, which adaptively determines spatial weights based on their relevance to the target. Extensive experiments on representative datasets show the proposed method's effectiveness compared to state-of-the-art (SOTA) transformer-based and other approaches. The code and pre-trained models are available at https://github.com/AlienZhang1996/S2WAT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chiyu Zhang",
      "Xiaogang Xu",
      "Lei Wang",
      "Zaiyan Dai",
      "Jun Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28530": {
    "title": "Synergistic Multiscale Detail Refinement via Intrinsic Supervision for Underwater Image Enhancement",
    "volume": "main",
    "abstract": "Visually restoring underwater scenes primarily involves mitigating interference from underwater media. Existing methods ignore the inherent scale-related characteristics in underwater scenes. Therefore, we present the synergistic multi-scale detail refinement via intrinsic supervision (SMDR-IS) for enhancing underwater scene details, which contain multi-stages. The low-degradation stage from the original images furnishes the original stage with multi-scale details, achieved through feature propagation using the Adaptive Selective Intrinsic Supervised Feature (ASISF) module. By using intrinsic supervision, the ASISF module can precisely control and guide feature transmission across multi-degradation stages, enhancing multi-scale detail refinement and minimizing the interference from irrelevant information in the low-degradation stage. In multi-degradation encoder-decoder framework of SMDR-IS, we introduce the Bifocal Intrinsic-Context Attention Module (BICA). Based on the intrinsic supervision principles, BICA efficiently exploits multi-scale scene information in images. BICA directs higher-resolution spaces by tapping into the insights of lower-resolution ones, underscoring the pivotal role of spatial contextual relationships in underwater image restoration. Throughout training, the inclusion of a multi-degradation loss function can enhance the network, allowing it to adeptly extract information across diverse scales. When benchmarked against state-of-the-art methods, SMDR-IS consistently showcases superior performance. Our code is available at https://github.com/zhoujingchun03/SMDR-IS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dehuan Zhang",
      "Jingchun Zhou",
      "Chunle Guo",
      "Weishi Zhang",
      "Chongyi Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28531": {
    "title": "W2P: Switching from Weak Supervision to Partial Supervision for Semantic Segmentation",
    "volume": "main",
    "abstract": "Current weakly-supervised semantic segmentation (WSSS) techniques concentrate on enhancing class activation maps (CAMs) with image-level annotations. Yet, the emphasis on producing these pseudo-labels often overshadows the pivotal role of training the segmentation model itself. This paper underscores the significant influence of noisy pseudo-labels on segmentation network performance, particularly in boundary region. To address above issues, we introduce a novel paradigm: Weak to Partial Supervision (W2P). At its core, W2P categorizes the pseudo-labels from WSSS into two unique supervisions: trustworthy clean labels and uncertain noisy labels. Next, our proposed partially-supervised framework adeptly employs these clean labels to rectify the noisy ones, thereby promoting the continuous enhancement of the segmentation model. To further optimize boundary segmentation, we incorporate a noise detection mechanism that specifically preserves boundary regions while eliminating noise. During the noise refinement phase, we adopt a boundary-conscious noise correction technique to extract comprehensive boundaries from noisy areas. Furthermore, we devise a boundary generation approach that assists in predicting intricate boundary zones. Evaluations on the PASCAL VOC 2012 and MS COCO 2014 datasets confirm our method's impressive segmentation capabilities across various pseudo-labels",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangyuan Zhang",
      "Tianxiang Pan",
      "Jun-Hai Yong",
      "Bin Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28532": {
    "title": "HyperEditor: Achieving Both Authenticity and Cross-Domain Capability in Image Editing via Hypernetworks",
    "volume": "main",
    "abstract": "Editing real images authentically while also achieving cross-domain editing remains a challenge. Recent studies have focused on converting real images into latent codes and accomplishing image editing by manipulating these codes. However, merely manipulating the latent codes would constrain the edited images to the generator's image domain, hindering the attainment of diverse editing goals. In response, we propose an innovative image editing method called HyperEditor, which utilizes weight factors generated by hypernetworks to reassign the weights of the pre-trained StyleGAN2's generator. Guided by CLIP's cross-modal image-text semantic alignment, this innovative approach enables us to simultaneously accomplish authentic attribute editing and cross-domain style transfer, a capability not realized in previous methods. Additionally, we ascertain that modifying only the weights of specific layers in the generator can yield an equivalent editing result. Therefore, we introduce an adaptive layer selector, enabling our hypernetworks to autonomously identify the layers requiring output weight factors, which can further improve our hypernetworks' efficiency. Extensive experiments on abundant challenging datasets demonstrate the effectiveness of our method",
    "checked": true,
    "id": "5ea5b774898d4f4fcf1d48b42b722a4d3f0f7749",
    "semantic_title": "hypereditor: achieving both authenticity and cross-domain capability in image editing via hypernetworks",
    "citation_count": 0,
    "authors": [
      "Hai Zhang",
      "Chunwei Wu",
      "Guitao Cao",
      "Hailing Wang",
      "Wenming Cao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28533": {
    "title": "RadOcc: Learning Cross-Modality Occupancy Knowledge through Rendering Assisted Distillation",
    "volume": "main",
    "abstract": "3D occupancy prediction is an emerging task that aims to estimate the occupancy states and semantics of 3D scenes using multi-view images. However, image-based scene perception encounters significant challenges in achieving accurate prediction due to the absence of geometric priors. In this paper, we address this issue by exploring cross-modal knowledge distillation in this task, i.e., we leverage a stronger multi-modal model to guide the visual model during training. In practice, we observe that directly applying features or logits alignment, proposed and widely used in bird's-eye-view (BEV) perception, does not yield satisfactory results. To overcome this problem, we introduce RadOcc, a Rendering assisted distillation paradigm for 3D Occupancy prediction. By employing differentiable volume rendering, we generate depth and semantic maps in perspective views and propose two novel consistency criteria between the rendered outputs of teacher and student models. Specifically, the depth consistency loss aligns the termination distributions of the rendered rays, while the semantic consistency loss mimics the intra-segment similarity guided by vision foundation models (VLMs). Experimental results on the nuScenes dataset demonstrate the effectiveness of our proposed method in improving various 3D occupancy prediction approaches, e.g., our proposed methodology enhances our baseline by 2.2% in the metric of mIoU and achieves 50% in Occ3D benchmark",
    "checked": true,
    "id": "fa3b125aa1b83c3a9d099fcfcd5e823c4a21493b",
    "semantic_title": "radocc: learning cross-modality occupancy knowledge through rendering assisted distillation",
    "citation_count": 11,
    "authors": [
      "Haiming Zhang",
      "Xu Yan",
      "Dongfeng Bai",
      "Jiantao Gao",
      "Pan Wang",
      "Bingbing Liu",
      "Shuguang Cui",
      "Zhen Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28534": {
    "title": "GSDD: Generative Space Dataset Distillation for Image Super-resolution",
    "volume": "main",
    "abstract": "Single image super-resolution (SISR), especially in the real world, usually builds a large amount of LR-HR image pairs to learn representations that contain rich textural and structural information. However, relying on massive data for model training not only reduces training efficiency, but also causes heavy data storage burdens. In this paper, we attempt a pioneering study on dataset distillation (DD) for SISR problems to explore how data could be slimmed and compressed for the task. Unlike previous coreset selection methods which select a few typical examples directly from the original data, we remove the limitation that the selected data cannot be further edited, and propose to synthesize and optimize samples to preserve more task-useful representations. Concretely, by utilizing pre-trained GANs as a suitable approximation of realistic data distribution, we propose GSDD, which distills data in a latent generative space based on GAN-inversion techniques. By optimizing them to match with the practical data distribution in an informative feature space, the distilled data could then be synthesized. Experimental results demonstrate that when trained with our distilled data, GSDD can achieve comparable performance to the state-of-the-art (SOTA) SISR algorithms, while a nearly ×8 increase in training efficiency and a saving of almost 93.2% data storage space can be realized. Further experiments on challenging real-world data also demonstrate the promising generalization ability of GSDD",
    "checked": true,
    "id": "90a0d5e98387d976947b628c746619f52caf1e87",
    "semantic_title": "gsdd: generative space dataset distillation for image super-resolution",
    "citation_count": 0,
    "authors": [
      "Haiyu Zhang",
      "Shaolin Su",
      "Yu Zhu",
      "Jinqiu Sun",
      "Yanning Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28535": {
    "title": "CSL: Class-Agnostic Structure-Constrained Learning for Segmentation Including the Unseen",
    "volume": "main",
    "abstract": "Addressing Out-Of-Distribution (OOD) Segmentation and Zero-Shot Semantic Segmentation (ZS3) is challenging, necessitating segmenting unseen classes. Existing strategies adapt the class-agnostic Mask2Former (CA-M2F) tailored to specific tasks. However, these methods cater to singular tasks, demand training from scratch, and we demonstrate certain deficiencies in CA-M2F, which affect performance. We propose the Class-Agnostic Structure-Constrained Learning (CSL), a plug-in framework that can integrate with existing methods, thereby embedding structural constraints and achieving performance gain, including the unseen, specifically OOD, ZS3, and domain adaptation (DA) tasks. There are two schemes for CSL to integrate with existing methods (1) by distilling knowledge from a base teacher network, enforcing constraints across training and inference phrases, or (2) by leveraging established models to obtain per-pixel distributions without retraining, appending constraints during the inference phase. Our soft assignment and mask split methodologies enhance OOD object segmentation. Empirical evaluations demonstrate CSL's prowess in boosting the performance of existing algorithms spanning OOD segmentation, ZS3, and DA segmentation, consistently transcending the state-of-art across all three tasks",
    "checked": true,
    "id": "3180789c20103d3f9d7cf74fd11b547c83cb9995",
    "semantic_title": "csl: class-agnostic structure-constrained learning for segmentation including the unseen",
    "citation_count": 3,
    "authors": [
      "Hao Zhang",
      "Fang Li",
      "Lu Qi",
      "Ming-Hsuan Yang",
      "Narendra Ahuja"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28536": {
    "title": "A Robust Mutual-Reinforcing Framework for 3D Multi-Modal Medical Image Fusion Based on Visual-Semantic Consistency",
    "volume": "main",
    "abstract": "This work proposes a robust 3D medical image fusion framework to establish a mutual-reinforcing mechanism between visual fusion and lesion segmentation, achieving their double improvement. Specifically, we explore the consistency between vision and semantics by sharing feature fusion modules. Through the coupled optimization of the visual fusion loss and the lesion segmentation loss, visual-related and semantic-related features will be pulled into the same domain, effectively promoting accuracy improvement in a mutual-reinforcing manner. Further, we establish the robustness guarantees by constructing a two-level refinement constraint in the process of feature extraction and reconstruction. Benefiting from full consideration for common degradations in medical images, our framework can not only provide clear visual fusion results for doctor's observation, but also enhance the defense ability of lesion segmentation against these negatives. Extensive evaluations of visual fusion and lesion segmentation scenarios demonstrate the advantages of our method in terms of accuracy and robustness. Moreover, our proposed framework is generic, which can be well-compatible with existing lesion segmentation algorithms and improve their performance. The code is publicly available at https://github.com/HaoZhang1018/RMR-Fusion",
    "checked": true,
    "id": "507254ea527ebd0f5a6f5a50652365ae6ec25bd9",
    "semantic_title": "a robust mutual-reinforcing framework for 3d multi-modal medical image fusion based on visual-semantic consistency",
    "citation_count": 1,
    "authors": [
      "Hao Zhang",
      "Xuhui Zuo",
      "Huabing Zhou",
      "Tao Lu",
      "Jiayi Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28537": {
    "title": "Learning Task-Aware Language-Image Representation for Class-Incremental Object Detection",
    "volume": "main",
    "abstract": "Class-incremental object detection (CIOD) is a real-world desired capability, requiring an object detector to continuously adapt to new tasks without forgetting learned ones, with the main challenge being catastrophic forgetting. Many methods based on distillation and replay have been proposed to alleviate this problem. However, they typically learn on a pure visual backbone, neglecting the powerful representation capabilities of textual cues, which to some extent limits their performance. In this paper, we propose task-aware language-image representation to mitigate catastrophic forgetting, introducing a new paradigm for language-image-based CIOD. First of all, we demonstrate the significant advantage of language-image detectors in mitigating catastrophic forgetting. Secondly, we propose a learning task-aware language-image representation method that overcomes the existing drawback of directly utilizing the language-image detector for CIOD. More specifically, we learn the language-image representation of different tasks through an insulating approach in the training stage, while using the alignment scores produced by task-specific language-image representation in the inference stage. Through our proposed method, language-image detectors can be more practical for CIOD. We conduct extensive experiments on COCO 2017 and Pascal VOC 2007 and demonstrate that the proposed method achieves state-of-the-art results under the various CIOD settings",
    "checked": true,
    "id": "f4b6d85d131f1e7b3086cc94ab56236750a8a4b8",
    "semantic_title": "learning task-aware language-image representation for class-incremental object detection",
    "citation_count": 1,
    "authors": [
      "Hongquan Zhang",
      "Bin-Bin Gao",
      "Yi Zeng",
      "Xudong Tian",
      "Xin Tan",
      "Zhizhong Zhang",
      "Yanyun Qu",
      "Jun Liu",
      "Yuan Xie"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28538": {
    "title": "Identification of Necessary Semantic Undertakers in the Causal View for Image-Text Matching",
    "volume": "main",
    "abstract": "Image-text matching bridges vision and language, which is a fundamental task in multimodal intelligence. Its key challenge lies in how to capture visual-semantic relevance. Fine-grained semantic interactions come from fragment alignments between image regions and text words. However, not all fragments contribute to image-text relevance, and many existing methods are devoted to mining the vital ones to measure the relevance accurately. How well image and text relate depends on the degree of semantic sharing between them. Treating the degree as an effect and fragments as its possible causes, we define those indispensable causes for the generation of the degree as necessary undertakers, i.e., if any of them did not occur, the relevance would be no longer valid. In this paper, we revisit image-text matching in the causal view and uncover inherent causal properties of relevance generation. Then we propose a novel theoretical prototype for estimating the probability-of-necessity of fragments, PN_f, for the degree of semantic sharing by means of causal inference, and further design a Necessary Undertaker Identification Framework (NUIF) for image-text matching, which explicitly formalizes the fragment's contribution to image-text relevance by modeling PN_f in two ways. Extensive experiments show our method achieves state-of-the-art on benchmarks Flickr30K and MSCOCO",
    "checked": true,
    "id": "abf2dddf5f2a849e46b875ce3efdb2bab2e7f393",
    "semantic_title": "identification of necessary semantic undertakers in the causal view for image-text matching",
    "citation_count": 0,
    "authors": [
      "Huatian Zhang",
      "Lei Zhang",
      "Kun Zhang",
      "Zhendong Mao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28539": {
    "title": "HR-Pro: Point-Supervised Temporal Action Localization via Hierarchical Reliability Propagation",
    "volume": "main",
    "abstract": "Point-supervised Temporal Action Localization (PSTAL) is an emerging research direction for label-efficient learning. However, current methods mainly focus on optimizing the network either at the snippet-level or the instance-level, neglecting the inherent reliability of point annotations at both levels. In this paper, we propose a Hierarchical Reliability Propagation (HR-Pro) framework, which consists of two reliability-aware stages: Snippet-level Discrimination Learning and Instance-level Completeness Learning, both stages explore the efficient propagation of high-confidence cues in point annotations. For snippet-level learning, we introduce an online-updated memory to store reliable snippet prototypes for each class. We then employ a Reliability-aware Attention Block to capture both intra-video and inter-video dependencies of snippets, resulting in more discriminative and robust snippet representation. For instance-level learning, we propose a point-based proposal generation approach as a means of connecting snippets and instances, which produces high-confidence proposals for further optimization at the instance level. Through multi-level reliability-aware learning, we obtain more reliable confidence scores and more accurate temporal boundaries of predicted proposals. Our HR-Pro achieves state-of-the-art performance on multiple challenging benchmarks, including an impressive average mAP of 60.3% on THUMOS14. Notably, our HR-Pro largely surpasses all previous point-supervised methods, and even outperforms several competitive fully-supervised methods. Code will be available at https://github.com/pipixin321/HR-Pro",
    "checked": true,
    "id": "ea51bbc915c4372ae63b0198898cd795dc1a0121",
    "semantic_title": "hr-pro: point-supervised temporal action localization via hierarchical reliability propagation",
    "citation_count": 2,
    "authors": [
      "Huaxin Zhang",
      "Xiang Wang",
      "Xiaohao Xu",
      "Zhiwu Qing",
      "Changxin Gao",
      "Nong Sang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28540": {
    "title": "AvatarVerse: High-Quality & Stable 3D Avatar Creation from Text and Pose",
    "volume": "main",
    "abstract": "Creating expressive, diverse and high-quality 3D avatars from highly customized text descriptions and pose guidance is a challenging task, due to the intricacy of modeling and texturing in 3D that ensure details and various styles (realistic, fictional, etc). We present AvatarVerse, a stable pipeline for generating expressive high-quality 3D avatars from nothing but text descriptions and pose guidance. In specific, we introduce a 2D diffusion model conditioned on DensePose signal to establish 3D pose control of avatars through 2D images, which enhances view consistency from partially observed scenarios. It addresses the infamous Janus Problem and significantly stablizes the generation process. Moreover, we propose a progressive high-resolution 3D synthesis strategy, which obtains substantial improvement over the quality of the created 3D avatars. To this end, the proposed AvatarVerse pipeline achieves zero-shot 3D modeling of 3D avatars that are not only more expressive, but also in higher quality and fidelity than previous works. Rigorous qualitative evaluations and user studies showcase AvatarVerse's superiority in synthesizing high-fidelity 3D avatars, leading to a new standard in high-quality and stable 3D avatar creation. Our project page is: https://avatarverse3d.github.io/",
    "checked": true,
    "id": "d3fd513594cd2e4cce10b50eb7ea16760b63a2b8",
    "semantic_title": "avatarverse: high-quality & stable 3d avatar creation from text and pose",
    "citation_count": 30,
    "authors": [
      "Huichao Zhang",
      "Bowen Chen",
      "Hao Yang",
      "Liao Qu",
      "Xu Wang",
      "Li Chen",
      "Chao Long",
      "Feida Zhu",
      "Daniel Du",
      "Min  Zheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28541": {
    "title": "Improving the Adversarial Transferability of Vision Transformers with Virtual Dense Connection",
    "volume": "main",
    "abstract": "With the great achievement of vision transformers (ViTs), transformer-based approaches have become the new paradigm for solving various computer vision tasks. However, recent research shows that similar to convolutional neural networks (CNNs), ViTs are still vulnerable to adversarial attacks. To explore the shared deficiency of models with different structures, researchers begin to analyze the cross-structure adversarial transferability, which is still under-explored. Therefore, in this work, we focus on the ViT attacks to improve the cross-structure transferability between the transformer-based and convolution-based models. Previous studies fail to thoroughly investigate the influence of the components inside the ViT models on adversarial transferability, leading to inferior performance. To overcome the drawback, we launch a motivating study by linearly down-scaling the gradients of components inside the ViT models to analyze their influence on adversarial transferability. Based on the motivating study, we find that the gradient of the skip connection most influences transferability and believe that back-propagating gradients from deeper blocks can enhance transferability. Therefore, we propose the Virtual Dense Connection method (VDC). Specifically, without changing the forward pass, we first recompose the original network to add virtual dense connections. Then we back-propagate gradients of deeper Attention maps and Multi-layer Perceptron (MLP) blocks via virtual dense connections when generating adversarial samples. Extensive experiments confirm the superiority of our proposed method over the state-of-the-art baselines, with an 8.2% improvement in transferability between ViT models and a 7.2% improvement in cross-structure transferability from ViTs to CNNs",
    "checked": true,
    "id": "bb775330f90da806a0c30758ea8b2babd48076ea",
    "semantic_title": "improving the adversarial transferability of vision transformers with virtual dense connection",
    "citation_count": 1,
    "authors": [
      "Jianping Zhang",
      "Yizhan Huang",
      "Zhuoer Xu",
      "Weibin Wu",
      "Michael R. Lyu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28542": {
    "title": "Curvature-Invariant Adversarial Attacks for 3D Point Clouds",
    "volume": "main",
    "abstract": "Imperceptibility is one of the crucial requirements for adversarial examples. Previous adversarial attacks on 3D point cloud recognition suffer from noticeable outliers, resulting in low imperceptibility. We think that the drawbacks can be alleviated via taking the local curvature of the point cloud into consideration. Existing approaches introduce the local geometry distance into the attack objective function. However, their definition of the local geometry distance neglects different perceptibility of distortions along different directions. In this paper, we aim to enhance the imperceptibility of adversarial attacks on 3D point cloud recognition by better preserving the local curvature of the original 3D point clouds. To this end, we propose the Curvature-Invariant Method (CIM), which directly regularizes the back-propagated gradient during the generation of adversarial point clouds based on two assumptions. Specifically, we first decompose the back-propagated gradients into the tangent plane and the normal direction. Then we directly reduce the gradient along the large curvature direction on the tangent plane and only keep the gradient along the negative normal direction. Comprehensive experimental comparisons confirm the superiority of our approach. Notably, our strategy can achieve 7.2% and 14.5% improvements in Hausdorff distance and Gaussian curvature measurements of the imperceptibility",
    "checked": true,
    "id": "d1f7191eb1b0a0e2c9d22a7720bf3a1cf99e4f18",
    "semantic_title": "curvature-invariant adversarial attacks for 3d point clouds",
    "citation_count": 0,
    "authors": [
      "Jianping Zhang",
      "Wenwei Gu",
      "Yizhan Huang",
      "Zhihan Jiang",
      "Weibin Wu",
      "Michael R. Lyu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28543": {
    "title": "Cross-Modal Feature Distribution Calibration for Few-Shot Visual Question Answering",
    "volume": "main",
    "abstract": "Few-shot Visual Question Answering (VQA) realizes few-shot cross-modal learning, which is an emerging and challenging task in computer vision. Currently, most of the few-shot VQA methods are confined to simply extending few-shot classification methods to cross-modal tasks while ignoring the spatial distribution properties of multimodal features and cross-modal information interaction. To address this problem, we propose a novel Cross-modal feature Distribution Calibration Inference Network (CDCIN) in this paper, where a new concept named visual information entropy is proposed to realize multimodal features distribution calibration by cross-modal information interaction for more effective few-shot VQA. Visual information entropy is a statistical variable that represents the spatial distribution of visual features guided by the question, which is aligned before and after the reasoning process to mitigate redundant information and improve multi-modal features by our proposed visual information entropy calibration module. To further enhance the inference ability of cross-modal features, we additionally propose a novel pre-training method, where the reasoning sub-network of CDCIN is pretrained on the base class in a VQA classification paradigm and fine-tuned on the few-shot VQA datasets. Extensive experiments demonstrate that our proposed CDCIN achieves excellent performance on few-shot VQA and outperforms state-of-the-art methods on three widely used benchmark datasets",
    "checked": true,
    "id": "2744e028ed77d3d1f297df31dfcc8bf0c769644d",
    "semantic_title": "cross-modal feature distribution calibration for few-shot visual question answering",
    "citation_count": 0,
    "authors": [
      "Jing Zhang",
      "Xiaoqiang Liu",
      "Mingzhe Chen",
      "Zhe Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28544": {
    "title": "Robust 3D Tracking with Quality-Aware Shape Completion",
    "volume": "main",
    "abstract": "3D single object tracking remains a challenging problem due to the sparsity and incompleteness of the point clouds. Existing algorithms attempt to address the challenges in two strategies. The first strategy is to learn dense geometric features based on the captured sparse point cloud. Nevertheless, it is quite a formidable task since the learned dense geometric features are with high uncertainty for depicting the shape of the target object. The other strategy is to aggregate the sparse geometric features of multiple templates to enrich the shape information, which is a routine solution in 2D tracking. However, aggregating the coarse shape representations can hardly yield a precise shape representation. Different from 2D pixels, 3D points of different frames can be directly fused by coordinate transform, i.e., shape completion. Considering that, we propose to construct a synthetic target representation composed of dense and complete point clouds depicting the target shape precisely by shape completion for robust 3D tracking. Specifically, we design a voxelized 3D tracking framework with shape completion, in which we propose a quality-aware shape completion mechanism to alleviate the adverse effect of noisy historical predictions. It enables us to effectively construct and leverage the synthetic target representation. Besides, we also develop a voxelized relation modeling module and box refinement module to improve tracking performance. Favorable performance against state-of-the-art algorithms on three benchmarks demonstrates the effectiveness and generalization ability of our method",
    "checked": true,
    "id": "323c2e3cbadebeb0ff03270b8f155deecfab96b2",
    "semantic_title": "robust 3d tracking with quality-aware shape completion",
    "citation_count": 0,
    "authors": [
      "Jingwen Zhang",
      "Zikun Zhou",
      "Guangming Lu",
      "Jiandong Tian",
      "Wenjie Pei"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28545": {
    "title": "Neighborhood-Enhanced 3D Human Pose Estimation with Monocular LiDAR in Long-Range Outdoor Scenes",
    "volume": "main",
    "abstract": "3D human pose estimation (3HPE) in large-scale outdoor scenes using commercial LiDAR has attracted significant attention due to its potential for real-life applications. However, existing LiDAR-based methods for 3HPE primarily rely on recovering 3D human poses from individual point clouds, and the coherence cues present in the neighborhood are not sufficiently harnessed. In this work, we explore spatial and contexture coherence cues contained in the neighborhood that lead to great performance improvements in 3HPE. Specifically, firstly, we deeply investigate the 3D neighbor in the background (3BN) which serves as a spatial coherence cue for inferring reliable motion since it provides physical laws to limit motion targets. Secondly, we introduce a novel 3D scanning neighbor (3SN) generated during the data collection and 3SN implies structural edge coherence cues. We use 3SN to overcome the degradation of performance and data quality caused by the sparsity-varying properties of LiDAR point clouds. In order to effectively model the complementation between these distinct cues and build consistent temporal relationships across human motions, we propose a new transformer-based module called the CoherenceFuse module. Extensive experiments were conducted on publicly available datasets, namely LidarHuman26M, CIMI4D, SLOPER4D and Waymo Open Dataset v2.0, showcase the superiority and effectiveness of our proposed method. In particular, when compared with LidarCap on the LidarHuman26M dataset, our method demonstrates a reduction of 7.08mm in the average MPJPE metric, along with a decrease of 16.55mm in the MPJPE metric for distances exceeding 25 meters. The code and models are available at https://github.com/jingyi-zhang/Neighborhood-enhanced-LidarCap",
    "checked": true,
    "id": "139222cdca9b8e1a498181352b3579d4b305eca4",
    "semantic_title": "neighborhood-enhanced 3d human pose estimation with monocular lidar in long-range outdoor scenes",
    "citation_count": 0,
    "authors": [
      "Jingyi Zhang",
      "Qihong Mao",
      "Guosheng Hu",
      "Siqi Shen",
      "Cheng Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28546": {
    "title": "NeRF-LiDAR: Generating Realistic LiDAR Point Clouds with Neural Radiance Fields",
    "volume": "main",
    "abstract": "Labelling LiDAR point clouds for training autonomous driving is extremely expensive and difficult. LiDAR simulation aims at generating realistic LiDAR data with labels for training and verifying self-driving algorithms more efficiently. Recently, Neural Radiance Fields (NeRF) have been proposed for novel view synthesis using implicit reconstruction of 3D scenes. Inspired by this, we present NeRF-LIDAR, a novel LiDAR simulation method that leverages real-world information to generate realistic LIDAR point clouds. Different from existing LiDAR simulators, we use real images and point cloud data collected by self-driving cars to learn the 3D scene representation, point cloud generation and label rendering. We verify the effectiveness of our NeRF-LiDAR by training different 3D segmentation models on the generated LiDAR point clouds. It reveals that the trained models are able to achieve similar accuracy when compared with the same model trained on the real LiDAR data. Besides, the generated data is capable of boosting the accuracy through pre-training which helps reduce the requirements of the real labeled data. Code is available at https://github.com/fudan-zvg/NeRF-LiDAR",
    "checked": true,
    "id": "906451f8cfc2adb967f2c81e2c000c536016ceb6",
    "semantic_title": "nerf-lidar: generating realistic lidar point clouds with neural radiance fields",
    "citation_count": 20,
    "authors": [
      "Junge Zhang",
      "Feihu Zhang",
      "Shaochen Kuang",
      "Li Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28547": {
    "title": "Point Cloud Part Editing: Segmentation, Generation, Assembly, and Selection",
    "volume": "main",
    "abstract": "Ideal part editing should guarantee the diversity of edited parts, the fidelity to the remaining parts, and the quality of the results. However, previous methods do not disentangle each part completely, which means the edited parts will affect the others, resulting in poor diversity and fidelity. In addition, some methods lack constraints between parts, which need manual selections of edited results to ensure quality. Therefore, we propose a four-stage process for point cloud part editing: Segmentation, Generation, Assembly, and Selection. Based on this process, we introduce SGAS, a model for part editing that employs two strategies: feature disentanglement and constraint. By independently fitting part-level feature distributions, we realize the feature disentanglement. By explicitly modeling the transformation from object-level distribution to part-level distributions, we realize the feature constraint. Considerable experiments on different datasets demonstrate the efficiency and effectiveness of SGAS on point cloud part editing. In addition, SGAS can be pruned to realize unsupervised part-aware point cloud generation and achieves state-of-the-art results",
    "checked": true,
    "id": "20f02decdbe9c967bec7b609fb32012f6e885bfa",
    "semantic_title": "point cloud part editing: segmentation, generation, assembly, and selection",
    "citation_count": 1,
    "authors": [
      "Kaiyi Zhang",
      "Yang Chen",
      "Ximing Yang",
      "Weizhong Zhang",
      "Cheng Jin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28548": {
    "title": "CatmullRom Splines-Based Regression for Image Forgery Localization",
    "volume": "main",
    "abstract": "IFL (Image Forgery Location) helps secure digital media forensics. However, many methods suffer from false detections (i.e., FPs) and inaccurate boundaries. In this paper, we proposed the CatmullRom Splines-based Regression Network (CSR-Net), which first rethinks the IFL task from the perspective of regression to deal with this problem. Specifically speaking, we propose an adaptive CutmullRom splines fitting scheme for coarse localization of the tampered regions. Then, for false positive cases, we first develop a novel re-scoring mechanism, which aims to filter out samples that cannot have responses on both the classification branch and the instance branch. Later on, to further restrict the boundaries, we design a learnable texture extraction module, which refines and enhances the contour representation by decoupling the horizontal and vertical forgery features to extract a more robust contour representation, thus suppressing FPs. Compared to segmentation-based methods, our method is simple but effective due to the unnecessity of post-processing. Extensive experiments show the superiority of CSR-Net to existing state-of-the-art methods, not only on standard natural image datasets but also on social media datasets",
    "checked": true,
    "id": "7a24cae4b0640a34eadb561b2078cb54ec30b27e",
    "semantic_title": "catmullrom splines-based regression for image forgery localization",
    "citation_count": 1,
    "authors": [
      "Li Zhang",
      "Mingliang Xu",
      "Dong Li",
      "Jianming Du",
      "Rujing Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28549": {
    "title": "Deep Semantic Graph Transformer for Multi-View 3D Human Pose Estimation",
    "volume": "main",
    "abstract": "Most Graph Convolutional Networks based 3D human pose estimation (HPE) methods were involved in single-view 3D HPE and utilized certain spatial graphs, existing key problems such as depth ambiguity, insufficient feature representation, or limited receptive fields. To address these issues, we propose a multi-view 3D HPE framework based on deep semantic graph transformer, which adaptively learns and fuses multi-view significant semantic features of human nodes to improve 3D HPE performance. First, we propose a deep semantic graph transformer encoder to enrich spatial feature information. It deeply mines the position, spatial structure, and skeletal edge knowledge of joints and dynamically learns their correlations. Then, we build a progressive multi-view spatial-temporal feature fusion framework to mitigate joint depth uncertainty. To enhance the pose spatial representation, deep spatial semantic feature are interacted and fused across different viewpoints during monocular feature extraction. Furthermore, long-time relevant temporal dependencies are modeled and spatial-temporal information from all viewpoints is fused to intermediately supervise the depth. Extensive experiments on three 3D HPE benchmarks show that our method achieves state-of-the-art results. It can effectively enhance pose features, mitigate depth ambiguity in single-view 3D HPE, and improve 3D HPE performance without providing camera parameters. Codes and models are available at https://github.com/z0911k/SGraFormer",
    "checked": true,
    "id": "72feb57b7697a6b5f9cbfe7c15ad900ee5d0a5c7",
    "semantic_title": "deep semantic graph transformer for multi-view 3d human pose estimation",
    "citation_count": 0,
    "authors": [
      "Lijun Zhang",
      "Kangkang Zhou",
      "Feng Lu",
      "Xiang-Dong Zhou",
      "Yu Shi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28550": {
    "title": "Brush Your Text: Synthesize Any Scene Text on Images via Diffusion Model",
    "volume": "main",
    "abstract": "Recently, diffusion-based image generation methods are credited for their remarkable text-to-image generation capabilities, while still facing challenges in accurately generating multilingual scene text images. To tackle this problem, we propose Diff-Text, which is a training-free scene text generation framework for any language. Our model outputs a photo-realistic image given a text of any language along with a textual description of a scene. The model leverages rendered sketch images as priors, thus arousing the potential multilingual-generation ability of the pre-trained Stable Diffusion. Based on the observation from the influence of the cross-attention map on object placement in generated images, we propose a localized attention constraint into the cross-attention layer to address the unreasonable positioning problem of scene text. Additionally, we introduce contrastive image-level prompts to further refine the position of the textual region and achieve more accurate scene text generation. Experiments demonstrate that our method outperforms the existing method in both the accuracy of text recognition and the naturalness of foreground-background blending",
    "checked": true,
    "id": "301d7c64a085af324acd9ab729be57d3466e7ff5",
    "semantic_title": "brush your text: synthesize any scene text on images via diffusion model",
    "citation_count": 6,
    "authors": [
      "Lingjun Zhang",
      "Xinyuan Chen",
      "Yaohui Wang",
      "Yue Lu",
      "Yu Qiao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28551": {
    "title": "IRPruneDet: Efficient Infrared Small Target Detection via Wavelet Structure-Regularized Soft Channel Pruning",
    "volume": "main",
    "abstract": "Infrared Small Target Detection (IRSTD) refers to detecting faint targets in infrared images, which has achieved notable progress with the advent of deep learning. However, the drive for improved detection accuracy has led to larger, intricate models with redundant parameters, causing storage and computation inefficiencies. In this pioneering study, we introduce the concept of utilizing network pruning to enhance the efficiency of IRSTD. Due to the challenge posed by low signal-to-noise ratios and the absence of detailed semantic information in infrared images, directly applying existing pruning techniques yields suboptimal performance. To address this, we propose a novel wavelet structure-regularized soft channel pruning method, giving rise to the efficient IRPruneDet model. Our approach involves representing the weight matrix in the wavelet domain and formulating a wavelet channel pruning strategy. We incorporate wavelet regularization to induce structural sparsity without incurring extra memory usage. Moreover, we design a soft channel reconstruction method that preserves important target information against premature pruning, thereby ensuring an optimal sparse structure while maintaining overall sparsity. Through extensive experiments on two widely-used benchmarks, our IRPruneDet method surpasses established techniques in both model complexity and accuracy. Specifically, when employing U-net as the baseline network, IRPruneDet achieves a 64.13% reduction in parameters and a 51.19% decrease in FLOPS, while improving IoU from 73.31% to 75.12% and nIoU from 70.92% to 74.30%. The code is available at https://github.com/hd0013/IRPruneDet",
    "checked": true,
    "id": "1963e04effcc944e148d06386f88bf6d4472a697",
    "semantic_title": "irprunedet: efficient infrared small target detection via wavelet structure-regularized soft channel pruning",
    "citation_count": 4,
    "authors": [
      "Mingjin Zhang",
      "Handi Yang",
      "Jie Guo",
      "Yunsong Li",
      "Xinbo Gao",
      "Jing Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28552": {
    "title": "M2Doc: A Multi-Modal Fusion Approach for Document Layout Analysis",
    "volume": "main",
    "abstract": "Document layout analysis is a crucial step for intelligent document understanding. However, many existing methods primarily focus on the visual aspects and overlook the textual features of documents. Although document pre-trained models utilize multi-modal features during the pre-training phase, they tend to operate as a unimodal pipeline when it comes to layout analysis tasks. Furthermore, current multi-modal methods perform worse than unimodal detectors on complex layout analysis datasets. To address these limitations, we propose an effective and pluggable multi-modal fusion approach named M2Doc, which fuses visual and textual features for better layout detection. M2Doc contains two pluggable multi-modal fusion modules, early-fusion and late-fusion, which align and fuse visual and textual features at the pixel level and block level. Benefitting from the concision and effectiveness of M2Doc, it can be easily applied to various detectors for better layout detection, including two-stage and end-to-end object detectors. Our experimental results demonstrate significant performance improvements in detectors equipped with M2Doc on datasets such as DocLayNet (+11.3 mAP) and M6Doc (+1.9 mAP). Furthermore, through the integration of the DINO detector with M2Doc, we achieve state-of-the-art results on DocLayNet (89.0 mAP), M6Doc (69.9 mAP), and PubLayNet (95.5 mAP). The code will be publicly released at https://github.com/johnning2333/M2Doc",
    "checked": true,
    "id": "ec8794b9b16ff90eb7646fce1d32f8a3086cfc55",
    "semantic_title": "m2doc: a multi-modal fusion approach for document layout analysis",
    "citation_count": 1,
    "authors": [
      "Ning Zhang",
      "Hiuyi Cheng",
      "Jiayu Chen",
      "Zongyuan Jiang",
      "Jun Huang",
      "Yang Xue",
      "Lianwen Jin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28553": {
    "title": "Multi-View People Detection in Large Scenes via Supervised View-Wise Contribution Weighting",
    "volume": "main",
    "abstract": "Recent deep learning-based multi-view people detection (MVD) methods have shown promising results on existing datasets. However, current methods are mainly trained and evaluated on small, single scenes with a limited number of multi-view frames and fixed camera views. As a result, these methods may not be practical for detecting people in larger, more complex scenes with severe occlusions and camera calibration errors. This paper focuses on improving multi-view people detection by developing a supervised view-wise contribution weighting approach that better fuses multi-camera information under large scenes. Besides, a large synthetic dataset is adopted to enhance the model's generalization ability and enable more practical evaluation and comparison. The model's performance on new testing scenes is further improved with a simple domain adaptation technique. Experimental results demonstrate the effectiveness of our approach in achieving promising cross-scene multi-view people detection performance",
    "checked": true,
    "id": "0a17676885f19ef5d4275d3bad908b93fae8ea02",
    "semantic_title": "multi-view people detection in large scenes via supervised view-wise contribution weighting",
    "citation_count": 0,
    "authors": [
      "Qi Zhang",
      "Yunfei Gong",
      "Daijie Chen",
      "Antoni B. Chan",
      "Hui Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28554": {
    "title": "Aligning Geometric Spatial Layout in Cross-View Geo-Localization via Feature Recombination",
    "volume": "main",
    "abstract": "Cross-view geo-localization holds significant potential for various applications, but drastic differences in viewpoints and visual appearances between cross-view images make this task extremely challenging. Recent works have made notable progress in cross-view geo-localization. However, existing methods either ignore the correspondence between geometric spatial layout in cross-view images or require high costs or strict constraints to achieve such alignment. In response to these challenges, we propose a Feature Recombination Module (FRM) that explicitly establishes the geometric spatial layout correspondences between two views. Unlike existing methods, FRM aligns geometric spatial layout by directly recombining features, avoiding image preprocessing, and introducing no additional computational and parameter costs. This effectively reduces ambiguities caused by geometric misalignments between ground-level and aerial-level images. Furthermore, it is not sensitive to frameworks and applies to both CNN-based and Transformer-based architectures. Additionally, as part of the training procedure, we also introduce a novel weighted (B+1)-tuple loss (WBL) as optimization objective. Compared to the widely used weighted soft margin ranking loss, this innovative loss enhances convergence speed and final performance. Based on the two core components (FRM and WBL), we develop an end-to-end network architecture (FRGeo) to address these limitations from a different perspective. Extensive experiments show that our proposed FRGeo not only achieves state-of-the-art performance on cross-view geo-localization benchmarks, including CVUSA, CVACT, and VIGOR, but also is significantly superior or competitive in terms of computational complexity and trainable parameters. Our project homepage is at https://zqwlearning.github.io/FRGeo",
    "checked": true,
    "id": "b8add88b2d7bbf4c41389c0818afdbaafa0912e0",
    "semantic_title": "aligning geometric spatial layout in cross-view geo-localization via feature recombination",
    "citation_count": 0,
    "authors": [
      "Qingwang Zhang",
      "Yingying Zhu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28555": {
    "title": "MobileInst: Video Instance Segmentation on the Mobile",
    "volume": "main",
    "abstract": "Video instance segmentation on mobile devices is an important yet very challenging edge AI problem. It mainly suffers from (1) heavy computation and memory costs for frame-by-frame pixel-level instance perception and (2) complicated heuristics for tracking objects. To address these issues, we present MobileInst, a lightweight and mobile-friendly framework for video instance segmentation on mobile devices. Firstly, MobileInst adopts a mobile vision transformer to extract multi-level semantic features and presents an efficient query-based dual-transformer instance decoder for mask kernels and a semantic-enhanced mask decoder to generate instance segmentation per frame. Secondly, MobileInst exploits simple yet effective kernel reuse and kernel association to track objects for video instance segmentation. Further, we propose temporal query passing to enhance the tracking ability for kernels. We conduct experiments on COCO and YouTube-VIS datasets to demonstrate the superiority of MobileInst and evaluate the inference latency on one single CPU core of the Snapdragon 778G Mobile Platform, without other methods of acceleration. On the COCO dataset, MobileInst achieves 31.2 mask AP and 433 ms on the mobile CPU, which reduces the latency by 50% compared to the previous SOTA. For video instance segmentation, MobileInst achieves 35.0 AP and 30.1 AP on YouTube-VIS 2019 & 2021",
    "checked": true,
    "id": "b5cf272d4bcb230cda15259966ac0cc310b72cff",
    "semantic_title": "mobileinst: video instance segmentation on the mobile",
    "citation_count": 4,
    "authors": [
      "Renhong Zhang",
      "Tianheng Cheng",
      "Shusheng Yang",
      "Haoyi Jiang",
      "Shuai Zhang",
      "Jiancheng Lyu",
      "Xin Li",
      "Xiaowen Ying",
      "Dashan Gao",
      "Wenyu Liu",
      "Xinggang Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28556": {
    "title": "Scalable Geometric Fracture Assembly via Co-creation Space among Assemblers",
    "volume": "main",
    "abstract": "Geometric fracture assembly presents a challenging practical task in archaeology and 3D computer vision. Previous methods have focused solely on assembling fragments based on semantic information, which has limited the quantity of objects that can be effectively assembled. Therefore, there is a need to develop a scalable framework for geometric fracture assembly without relying on semantic information. To improve the effectiveness of assembling geometric fractures without semantic information, we propose a co-creation space comprising several assemblers capable of gradually and unambiguously assembling fractures. Additionally, we introduce a novel loss function, i.e., the geometric-based collision loss, to address collision issues during the fracture assembly process and enhance the results. Our framework exhibits better performance on both PartNet and Breaking Bad datasets compared to existing state-of-the-art frameworks. Extensive experiments and quantitative comparisons demonstrate the effectiveness of our proposed framework, which features linear computational complexity, enhanced abstraction, and improved generalization. Our code is publicly available at https://github.com/Ruiyuan-Zhang/CCS",
    "checked": true,
    "id": "ba868ff4ccb8f3fc812001c6bacd83a36f31172e",
    "semantic_title": "scalable geometric fracture assembly via co-creation space among assemblers",
    "citation_count": 1,
    "authors": [
      "Ruiyuan Zhang",
      "Jiaxiang Liu",
      "Zexi Li",
      "Hao Dong",
      "Jie Fu",
      "Chao Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28557": {
    "title": "S3A: Towards Realistic Zero-Shot Classification via Self Structural Semantic Alignment",
    "volume": "main",
    "abstract": "Large-scale pre-trained Vision Language Models (VLMs) have proven effective for zero-shot classification. Despite the success, most traditional VLMs-based methods are restricted by the assumption of partial source supervision or ideal target vocabularies, which rarely satisfy the open-world scenario. In this paper, we aim at a more challenging setting, Realistic Zero-Shot Classification, which assumes no annotation but instead a broad vocabulary. To address the new problem, we propose the Self Structural Semantic Alignment (S3A) framework, which extracts the structural semantic information from unlabeled data while simultaneously self-learning. Our S3A framework adopts a unique Cluster-Vote-Prompt-Realign (CVPR) algorithm, which iteratively groups unlabeled data to derive structural semantics for pseudo-supervision. Our CVPR algorithm includes iterative clustering on images, voting within each cluster to identify initial class candidates from the vocabulary, generating discriminative prompts with large language models to discern confusing candidates, and realigning images and the vocabulary as structural semantic alignment. Finally, we propose to self-train the CLIP image encoder with both individual and structural semantic alignment through a teacher-student learning strategy. Our comprehensive experiments across various generic and fine-grained benchmarks demonstrate that the S3A method substantially improves over existing VLMs-based approaches, achieving a more than 15% accuracy improvement over CLIP on average. Our codes, models, and prompts are publicly released at https://github.com/sheng-eatamath/S3A",
    "checked": false,
    "id": "437cfee2a7f7beadf09ad712f71b3265740e44a0",
    "semantic_title": "towards realistic zero-shot classification via self structural semantic alignment",
    "citation_count": 2,
    "authors": [
      "Sheng Zhang",
      "Muzammal Naseer",
      "Guangyi Chen",
      "Zhiqiang Shen",
      "Salman Khan",
      "Kun Zhang",
      "Fahad Shahbaz Khan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28558": {
    "title": "A Computation-Aware Shape Loss Function for Point Cloud Completion",
    "volume": "main",
    "abstract": "Learning-based point cloud completion tasks have shown potential in various critical tasks, such as object detection, assignment, and registration. However, accurately and efficiently quantifying the shape error between the predicted point clouds generated by networks and the ground truth remains challenging. While EMD-based loss functions excel in shape detail and perceived density distribution, their approach can only yield results with significant discrepancies from the actual EMD within a tolerable training time. To address these challenges, we first propose the initial price based on the auction algorithm, reducing the number of iterations required for the algorithm while ensuring the correctness of the assignment results. We then introduce an algorithm to compute the initial price through a successive shortest path and the Euclidean information between its nodes. Finally, we adopt a series of optimization strategies to speed up the algorithm and offer an EMD approximation scheme for point cloud problems that balances time loss and computational accuracy based on point cloud data characteristics. Our experimental results confirm that our algorithm achieves the smallest gap with the real EMD within an acceptable time range and yields the best results in end-to-end training",
    "checked": true,
    "id": "5148412ec25c26e833824f12b8644d7558a2b57c",
    "semantic_title": "a computation-aware shape loss function for point cloud completion",
    "citation_count": 0,
    "authors": [
      "Shunran Zhang",
      "Xiubo Zhang",
      "Tsz Nam Chan",
      "Shenghui Zhang",
      "Leong Hou U"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28559": {
    "title": "Vision-Language Pre-training with Object Contrastive Learning for 3D Scene Understanding",
    "volume": "main",
    "abstract": "In recent years, vision language pre-training frameworks have made significant progress in natural language processing and computer vision, achieving remarkable performance improvement on various downstream tasks. However, when extended to point cloud data, existing works mainly focus on building task-specific models, and fail to extract universal 3D vision-language embedding that generalize well. We carefully investigate three common tasks in semantic 3D scene understanding, and derive key insights into the development of a pre-training model. Motivated by these observations, we propose a vision-language pre-training framework 3DVLP (3D vision-language pre-training with object contrastive learning), which transfers flexibly on 3D vision-language downstream tasks. 3DVLP takes visual grounding as the proxy task and introduces Object-level IoU-guided Detection (OID) loss to obtain high-quality proposals in the scene. Moreover, we design Object-level Cross-Contrastive alignment (OCC) task and Object-level Self-Contrastive learning (OSC) task to align the objects with descriptions and distinguish different objects in the scene, respectively. Extensive experiments verify the excellent performance of 3DVLP on three 3D vision-language tasks, reflecting its superiority in semantic 3D scene understanding. Code is available at https://github.com/iridescentttt/3DVLP",
    "checked": true,
    "id": "86ea4aa29241149c3999301f0285d8cbb8542b11",
    "semantic_title": "vision-language pre-training with object contrastive learning for 3d scene understanding",
    "citation_count": 5,
    "authors": [
      "Taolin Zhang",
      "Sunan He",
      "Tao Dai",
      "Zhi Wang",
      "Bin Chen",
      "Shu-Tao Xia"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28560": {
    "title": "Transformer-Based Selective Super-resolution for Efficient Image Refinement",
    "volume": "main",
    "abstract": "Conventional super-resolution methods suffer from two drawbacks: substantial computational cost in upscaling an entire large image, and the introduction of extraneous or potentially detrimental information for downstream computer vision tasks during the refinement of the background. To solve these issues, we propose a novel transformer-based algorithm, Selective Super-Resolution (SSR), which partitions images into non-overlapping tiles, selects tiles of interest at various scales with a pyramid architecture, and exclusively reconstructs these selected tiles with deep features. Experimental results on three datasets demonstrate the efficiency and robust performance of our approach for super-resolution. Compared to the state-of-the-art methods, the FID score is reduced from 26.78 to 10.41 with 40% reduction in computation cost for the BDD100K dataset",
    "checked": true,
    "id": "47c88dfb0e152429a0bb4ee98781a760da960e44",
    "semantic_title": "transformer-based selective super-resolution for efficient image refinement",
    "citation_count": 0,
    "authors": [
      "Tianyi Zhang",
      "Kishore Kasichainula",
      "Yaoxin Zhuo",
      "Baoxin Li",
      "Jae-Sun Seo",
      "Yu Cao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28561": {
    "title": "Exploring Base-Class Suppression with Prior Guidance for Bias-Free One-Shot Object Detection",
    "volume": "main",
    "abstract": "One-shot object detection (OSOD) aims to detect all object instances towards the given category specified by a query image. Most existing studies in OSOD endeavor to establish effective cross-image correlation with limited query information, however, ignoring the problems of the model bias towards the base classes and the generalization degradation on the novel classes. Observing this, we propose a novel algorithm, namely Base-class Suppression with Prior Guidance (BSPG) network to achieve bias-free OSOD. Specifically, the objects of base categories can be detected by a base-class predictor and eliminated by a base-class suppression module (BcS). Moreover, a prior guidance module (PG) is designed to calculate the correlation of high-level features in a non-parametric manner, producing a class-agnostic prior map with unbiased semantic information to guide the subsequent detection process. Equipped with the proposed two modules, we endow the model with a strong discriminative ability to distinguish the target objects from distractors belonging to the base classes. Extensive experiments show that our method outperforms the previous techniques by a large margin and achieves new state-of-the-art performance under various evaluation settings",
    "checked": true,
    "id": "f2a90e6d6b469afcc41d19a821794413f3cca04c",
    "semantic_title": "exploring base-class suppression with prior guidance for bias-free one-shot object detection",
    "citation_count": 0,
    "authors": [
      "Wenwen Zhang",
      "Yun Hu",
      "Hangguan Shan",
      "Eryun Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28562": {
    "title": "HEAP: Unsupervised Object Discovery and Localization with Contrastive Grouping",
    "volume": "main",
    "abstract": "Unsupervised object discovery and localization aims to detect or segment objects in an image without any supervision. Recent efforts have demonstrated a notable potential to identify salient foreground objects by utilizing self-supervised transformer features. However, their scopes only build upon patch-level features within an image, neglecting region/image-level and cross-image relationships at a broader scale. Moreover, these methods cannot differentiate various semantics from multiple instances. To address these problems, we introduce Hierarchical mErging framework via contrAstive grouPing (HEAP). Specifically, a novel lightweight head with cross-attention mechanism is designed to adaptively group intra-image patches into semantically coherent regions based on correlation among self-supervised features. Further, to ensure the distinguishability among various regions, we introduce a region-level contrastive clustering loss to pull closer similar regions across images. Also, an image-level contrastive loss is present to push foreground and background representations apart, with which foreground objects and background are accordingly discovered. HEAP facilitates efficient hierarchical image decomposition, which contributes to more accurate object discovery while also enabling differentiation among objects of various classes. Extensive experimental results on semantic segmentation retrieval, unsupervised object discovery, and saliency detection tasks demonstrate that HEAP achieves state-of-the-art performance",
    "checked": true,
    "id": "610f893dcd2d9593e927f84ef5e84ee4a01adc72",
    "semantic_title": "heap: unsupervised object discovery and localization with contrastive grouping",
    "citation_count": 0,
    "authors": [
      "Xin Zhang",
      "Jinheng Xie",
      "Yuan Yuan",
      "Michael Bi Mi",
      "Robby T. Tan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28563": {
    "title": "Scribble Hides Class: Promoting Scribble-Based Weakly-Supervised Semantic Segmentation with Its Class Label",
    "volume": "main",
    "abstract": "Scribble-based weakly-supervised semantic segmentation using sparse scribble supervision is gaining traction as it reduces annotation costs when compared to fully annotated alternatives. Existing methods primarily generate pseudo-labels by diffusing labeled pixels to unlabeled ones with local cues for supervision. However, this diffusion process fails to exploit global semantics and class-specific cues, which are important for semantic segmentation. In this study, we propose a class-driven scribble promotion network, which utilizes both scribble annotations and pseudo-labels informed by image-level classes and global semantics for supervision. Directly adopting pseudo-labels might misguide the segmentation model, thus we design a localization rectification module to correct foreground representations in the feature space. To further combine the advantages of both supervisions, we also introduce a distance entropy loss for uncertainty reduction, which adapts per-pixel confidence weights according to the reliable region determined by the scribble and pseudo-label's boundary. Experiments on the ScribbleSup dataset with different qualities of scribble annotations outperform all the previous methods, demonstrating the superiority and robustness of our method. The code is available at https://github.com/Zxl19990529/Class-driven-Scribble-Promotion-Network",
    "checked": true,
    "id": "1e36a50ba2bb34965e1759e65d2d333627c22c08",
    "semantic_title": "scribble hides class: promoting scribble-based weakly-supervised semantic segmentation with its class label",
    "citation_count": 0,
    "authors": [
      "Xinliang Zhang",
      "Lei Zhu",
      "Hangzhou He",
      "Lujia Jin",
      "Yanye Lu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28564": {
    "title": "Negative Pre-aware for Noisy Cross-Modal Matching",
    "volume": "main",
    "abstract": "Cross-modal noise-robust learning is a challenging task since noisy correspondence is hard to recognize and rectify. Due to the cumulative and unavoidable negative impact of unresolved noise, existing methods cannot maintain a stable performance when the noise increases. In this paper, we present a novel Negative Pre-aware Cross-modal (NPC) matching solution for large visual-language model fine-tuning on noisy downstream tasks. It is featured in two aspects: (1) For noise recognition and resistance, previous methods usually directly filter out a noise subset, we propose to estimate the negative impact of each sample. It does not need additional correction mechanisms that may predict unreliable correction results, leading to self-reinforcing error. We assign a confidence weight to each sample according to its negative impact in the training process. This adaptively adjusts the contribution of each sample to avoid noisy accumulation. (2) For maintaining stable performance with increasing noise, we utilize the memorization effect of DNNs by maintaining a memory bank. Specifically, we apply GMM to select high-confident clean samples as the memory entry, where the memory entry is used to estimate the negative impact of each sample. Since clean samples are easier distinguished by GMM with increasing noise, the memory bank can still maintain high quality at a high noise ratio. Compared to the correction mechanism focusing on noise samples, memory bank-based estimation is more robust, which makes the model performance stable on noisy datasets. Extensive experiments demonstrate that our method significantly improves matching accuracy and performance stability at increasing noise ratio. Our approach also surpasses the state-of-the-art methods by a large margin. The code is available at: https://github.com/ZhangXu0963/NPC",
    "checked": true,
    "id": "d0ca72556e1cc6c5d86b34cc0449a133be3a1997",
    "semantic_title": "negative pre-aware for noisy cross-modal matching",
    "citation_count": 1,
    "authors": [
      "Xu Zhang",
      "Hao Li",
      "Mang Ye"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28565": {
    "title": "Compositional Inversion for Stable Diffusion Models",
    "volume": "main",
    "abstract": "Inversion methods, such as Textual Inversion, generate personalized images by incorporating concepts of interest provided by user images. However, existing methods often suffer from overfitting issues, where the dominant presence of inverted concepts leads to the absence of other desired concepts. It stems from the fact that during inversion, the irrelevant semantics in the user images are also encoded, forcing the inverted concepts to occupy locations far from the core distribution in the embedding space. To address this issue, we propose a method that guides the inversion process towards the core distribution for compositional embeddings. Additionally, we introduce a spatial regularization approach to balance the attention on the concepts being composed. Our method is designed as a post-training approach and can be seamlessly integrated with other inversion methods. Experimental results demonstrate the effectiveness of our proposed approach in mitigating the overfitting problem and generating more diverse and balanced compositions of concepts in the synthesized images. The source code is available at https://github.com/zhangxulu1996/Compositional-Inversion",
    "checked": true,
    "id": "a951b2e73fadcd2e78453569619980e373659fce",
    "semantic_title": "compositional inversion for stable diffusion models",
    "citation_count": 7,
    "authors": [
      "Xulu Zhang",
      "Xiao-Yong Wei",
      "Jinlin Wu",
      "Tianyi Zhang",
      "Zhaoxiang Zhang",
      "Zhen Lei",
      "Qing Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28566": {
    "title": "Cross-Modal Match for Language Conditioned 3D Object Grounding",
    "volume": "main",
    "abstract": "Language conditioned 3D object grounding aims to find the object within the 3D scene mentioned by natural language descriptions, which mainly depends on the matching between visual and natural language. Considerable improvement in grounding performance is achieved by improving the multimodal fusion mechanism or bridging the gap between detection and matching. However, several mismatches are ignored, i.e., mismatch in local visual representation and global sentence representation, and mismatch in visual space and corresponding label word space. In this paper, we propose crossmodal match for 3D grounding from mitigating these mismatches perspective. Specifically, to match local visual features with the global description sentence, we propose BEV (Bird's-eye-view) based global information embedding module. It projects multiple object proposal features into the BEV and the relations of different objects are accessed by the visual transformer which can model both positions and features with long-range dependencies. To circumvent the mismatch in feature spaces of different modalities, we propose crossmodal consistency learning. It performs cross-modal consistency constraints to convert the visual feature space into the label word feature space resulting in easier matching. Besides, we introduce label distillation loss and global distillation loss to drive these matches learning in a distillation way. We evaluate our method in mainstream evaluation settings on three datasets, and the results demonstrate the effectiveness of the proposed method",
    "checked": true,
    "id": "f31c94f43fc8265ff4e6e5fee7bdf4fd93d45fc1",
    "semantic_title": "cross-modal match for language conditioned 3d object grounding",
    "citation_count": 0,
    "authors": [
      "Yachao Zhang",
      "Runze Hu",
      "Ronghui Li",
      "Yanyun Qu",
      "Yuan Xie",
      "Xiu Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28567": {
    "title": "MotionGPT: Finetuned LLMs Are General-Purpose Motion Generators",
    "volume": "main",
    "abstract": "Generating realistic human motion from given action descriptions has experienced significant advancements because of the emerging requirement of digital humans. While recent works have achieved impressive results in generating motion directly from textual action descriptions, they often support only a single modality of the control signal, which limits their application in the real digital human industry. This paper presents a Motion General-Purpose generaTor (MotionGPT) that can use multimodal control signals, e.g., text and single-frame poses, for generating consecutive human motions by treating multimodal signals as special input tokens in large language models (LLMs). Specifically, we first quantize multimodal control signals into discrete codes and then formulate them in a unified prompt instruction to ask the LLMs to generate the motion answer. Our MotionGPT demonstrates a unified human motion generation model with multimodal control signals by tuning a mere 0.4% of LLM parameters. To the best of our knowledge, MotionGPT is the first method to generate human motion by multimodal control signals, which we hope can shed light on this new direction. Visit our webpage at https://qiqiapink.github.io/MotionGPT/",
    "checked": true,
    "id": "bc2333c9a667af90ee7ce52b911d2e04aed01526",
    "semantic_title": "motiongpt: finetuned llms are general-purpose motion generators",
    "citation_count": 46,
    "authors": [
      "Yaqi Zhang",
      "Di Huang",
      "Bin Liu",
      "Shixiang Tang",
      "Yan Lu",
      "Lu Chen",
      "Lei Bai",
      "Qi Chu",
      "Nenghai Yu",
      "Wanli Ouyang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28568": {
    "title": "Concept-Guided Prompt Learning for Generalization in Vision-Language Models",
    "volume": "main",
    "abstract": "Contrastive Language-Image Pretraining (CLIP) model has exhibited remarkable efficacy in establishing cross-modal connections between texts and images, yielding impressive performance across a broad spectrum of downstream applications through fine-tuning. However, for generalization tasks, the current fine-tuning methods for CLIP, such as CoOp and CoCoOp, demonstrate relatively low performance on some fine-grained datasets. We recognize the underlying reason is that these previous methods only projected global features into the prompt, neglecting the various visual concepts, such as colors, shapes, and sizes, which are naturally transferable across domains and play a crucial role in generalization tasks. To address this issue, in this work, we propose Concept-Guided Prompt Learning (CPL) for vision-language models. Specifically, we leverage the well-learned knowledge of CLIP to create a visual concept cache to enable conceptguided prompting. In order to refine the text features, we further develop a projector that transforms multi-level visual features into text features. We observe that this concept-guided prompt learning approach is able to achieve enhanced consistency between visual and linguistic modalities. Extensive experimental results demonstrate that our CPL method significantly improves generalization capabilities compared to the current state-of-the-art methods",
    "checked": true,
    "id": "c80c9b551f6b6b2761fe2fe048afcabcdf7e0068",
    "semantic_title": "concept-guided prompt learning for generalization in vision-language models",
    "citation_count": 4,
    "authors": [
      "Yi Zhang",
      "Ce Zhang",
      "Ke Yu",
      "Yushun Tang",
      "Zhihai He"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28569": {
    "title": "ISP-Teacher:Image Signal Process with Disentanglement Regularization for Unsupervised Domain Adaptive Dark Object Detection",
    "volume": "main",
    "abstract": "Object detection in dark conditions has always been a great challenge due to the complex formation process of low-light images. Currently, the mainstream methods usually adopt domain adaptation with Teacher-Student architecture to solve the dark object detection problem, and they imitate the dark conditions by using non-learnable data augmentation strategies on the annotated source daytime images. Note that these methods neglected to model the intrinsic imaging process, i.e. image signal processing (ISP), which is important for camera sensors to generate low-light images. To solve the above problems, in this paper, we propose a novel method named ISP-Teacher for dark object detection by exploring Teacher-Student architecture from a new perspective (i.e. self-supervised learning based ISP degradation). Specifically, we first design a day-to-night transformation module that consistent with the ISP pipeline of the camera sensors (ISP-DTM) to make the augmented images look more in line with the natural low-light images captured by cameras, and the ISP-related parameters are learned in a self-supervised manner. Moreover, to avoid the conflict between the ISP degradation and detection tasks in a shared encoder, we propose a disentanglement regularization (DR) that minimizes the absolute value of cosine similarity to disentangle two tasks and push two gradients vectors as orthogonal as possible. Extensive experiments conducted on two benchmarks show the effectiveness of our method in dark object detection. In particular, ISP-Teacher achieves an improvement of +2.4% AP and +3.3% AP over the SOTA method on BDD100k and SHIFT datasets, respectively. The code can be found at https://github.com/zhangyin1996/ISP-Teacher",
    "checked": false,
    "id": "8a4f4e3d7cd727ea4ecc1fc1d52035bfef6b459f",
    "semantic_title": "isp-teacher: image signal process with disentanglement regularization for unsupervised domain adaptive dark object detection",
    "citation_count": 0,
    "authors": [
      "Yin Zhang",
      "Yongqiang Zhang",
      "Zian Zhang",
      "Man Zhang",
      "Rui Tian",
      "Mingli Ding"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28570": {
    "title": "ArtBank: Artistic Style Transfer with Pre-trained Diffusion Model and Implicit Style Prompt Bank",
    "volume": "main",
    "abstract": "Artistic style transfer aims to repaint the content image with the learned artistic style. Existing artistic style transfer methods can be divided into two categories: small model-based approaches and pre-trained large-scale model-based approaches. Small model-based approaches can preserve the content strucuture, but fail to produce highly realistic stylized images and introduce artifacts and disharmonious patterns; Pre-trained large-scale model-based approaches can generate highly realistic stylized images but struggle with preserving the content structure. To address the above issues, we propose ArtBank, a novel artistic style transfer framework, to generate highly realistic stylized images while preserving the content structure of the content images. Specifically, to sufficiently dig out the knowledge embedded in pre-trained large-scale models, an Implicit Style Prompt Bank (ISPB), a set of trainable parameter matrices, is designed to learn and store knowledge from the collection of artworks and behave as a visual prompt to guide pre-trained large-scale models to generate highly realistic stylized images while preserving content structure. Besides, to accelerate training the above ISPB, we propose a novel Spatial-Statistical-based self-Attention Module (SSAM). The qualitative and quantitative experiments demonstrate the superiority of our proposed method over state-of-the-art artistic style transfer methods. Code is available at https://github.com/Jamie-Cheung/ArtBank",
    "checked": true,
    "id": "80f23181c5a1d425b872beac5555a454009b2ffb",
    "semantic_title": "artbank: artistic style transfer with pre-trained diffusion model and implicit style prompt bank",
    "citation_count": 0,
    "authors": [
      "Zhanjie Zhang",
      "Quanwei Zhang",
      "Wei Xing",
      "Guangyuan Li",
      "Lei Zhao",
      "Jiakai Sun",
      "Zehua Lan",
      "Junsheng Luan",
      "Yiling Huang",
      "Huaizhong Lin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28571": {
    "title": "A New Benchmark and Model for Challenging Image Manipulation Detection",
    "volume": "main",
    "abstract": "The ability to detect manipulation in multimedia data is vital in digital forensics. Existing Image Manipulation Detection (IMD) methods are mainly based on detecting anomalous features arisen from image editing or double compression artifacts. All existing IMD techniques encounter challenges when it comes to detecting small tampered regions from a large image. Moreover, compression-based IMD approaches face difficulties in cases of double compression of identical quality factors. To investigate the State-of-The-Art (SoTA) IMD methods in those challenging conditions, we introduce a new Challenging Image Manipulation Detection (CIMD) benchmark dataset, which consists of two subsets, for evaluating editing-based and compression-based IMD methods, respectively. The dataset images were manually taken and tampered with high-quality annotations. In addition, we propose a new two-branch network model based on HRNet that can better detect both the image-editing and compression artifacts in those challenging conditions. Extensive experiments on the CIMD benchmark show that our model significantly outperforms SoTA IMD methods on CIMD. The dataset is available at: https://github.com/ZhenfeiZ/CIMD",
    "checked": true,
    "id": "b09b127187c7f587f994c1f22abd4956c7791818",
    "semantic_title": "a new benchmark and model for challenging image manipulation detection",
    "citation_count": 0,
    "authors": [
      "Zhenfei Zhang",
      "Mingyang Li",
      "Ming-Ching Chang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28572": {
    "title": "TMFormer: Token Merging Transformer for Brain Tumor Segmentation with Missing Modalities",
    "volume": "main",
    "abstract": "Numerous techniques excel in brain tumor segmentation using multi-modal magnetic resonance imaging (MRI) sequences, delivering exceptional results. However, the prevalent absence of modalities in clinical scenarios hampers performance. Current approaches frequently resort to zero maps as substitutes for missing modalities, inadvertently introducing feature bias and redundant computations. To address these issues, we present the Token Merging transFormer (TMFormer) for robust brain tumor segmentation with missing modalities. TMFormer tackles these challenges by extracting and merging accessible modalities into more compact token sequences. The architecture comprises two core components: the Uni-modal Token Merging Block (UMB) and the Multi-modal Token Merging Block (MMB). The UMB enhances individual modality representation by adaptively consolidating spatially redundant tokens within and outside tumor-related regions, thereby refining token sequences for augmented representational capacity. Meanwhile, the MMB mitigates multi-modal feature fusion bias, exclusively leveraging tokens from present modalities and merging them into a unified multi-modal representation to accommodate varying modality combinations. Extensive experimental results on the BraTS 2018 and 2020 datasets demonstrate the superiority and efficacy of TMFormer compared to state-of-the-art methods when dealing with missing modalities",
    "checked": true,
    "id": "f5657a25dcb0bae96da6de52765e5ec32d887260",
    "semantic_title": "tmformer: token merging transformer for brain tumor segmentation with missing modalities",
    "citation_count": 1,
    "authors": [
      "Zheyu Zhang",
      "Gang Yang",
      "Yueyi Zhang",
      "Huanjing Yue",
      "Aiping Liu",
      "Yunwei Ou",
      "Jian Gong",
      "Xiaoyan Sun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28573": {
    "title": "FaceRSA: RSA-Aware Facial Identity Cryptography Framework",
    "volume": "main",
    "abstract": "With the flourishing of the Internet, sharing one's photos or automated processing of faces using computer vision technology has become an everyday occurrence. While enjoying the convenience, the concern for identity privacy is also emerging. Therefore, some efforts introduced the concept of ``password'' from traditional cryptography such as RSA into the face anonymization and deanonymization task to protect the facial identity without compromising the usability of the face image. However, these methods either suffer from the poor visual quality of the synthesis results or do not possess the full cryptographic properties, resulting in compromised security. In this paper, we present the first facial identity cryptography framework with full properties analogous to RSA. Our framework leverages the powerful generative capabilities of StyleGAN to achieve megapixel-level facial identity anonymization and deanonymization. Thanks to the great semantic decoupling of StyleGAN's latent space, the identity encryption and decryption process are performed in latent space by a well-designed password mapper in the manner of editing latent code. Meanwhile, the password-related information is imperceptibly hidden in the edited latent code owing to the redundant nature of the latent space. To make our cryptographic framework possesses all the properties analogous to RSA, we propose three types of loss functions: single anonymization loss, sequential anonymization loss, and associated anonymization loss. Extensive experiments and ablation analyses demonstrate the superiority of our method in terms of the quality of synthesis results, identity-irrelevant attributes preservation, deanonymization accuracy, and completeness of properties analogous to RSA",
    "checked": true,
    "id": "6aa78f095a9e50fa003136881604265d950aaeb2",
    "semantic_title": "facersa: rsa-aware facial identity cryptography framework",
    "citation_count": 0,
    "authors": [
      "Zhongyi Zhang",
      "Tianyi Wei",
      "Wenbo Zhou",
      "Hanqing Zhao",
      "Weiming Zhang",
      "Nenghai Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28574": {
    "title": "Spatial-Contextual Discrepancy Information Compensation for GAN Inversion",
    "volume": "main",
    "abstract": "Most existing GAN inversion methods either achieve accurate reconstruction but lack editability or offer strong editability at the cost of fidelity. Hence, how to balance the distortion-editability trade-off is a significant challenge for GAN inversion. To address this challenge, we introduce a novel spatial-contextual discrepancy information compensation-based GAN-inversion method (SDIC), which consists of a discrepancy information prediction network (DIPN) and a discrepancy information compensation network (DICN). SDIC follows a ``compensate-and-edit'' paradigm and successfully bridges the gap in image details between the original image and the reconstructed/edited image. On the one hand, DIPN encodes the multi-level spatial-contextual information of the original and initial reconstructed images and then predicts a spatial-contextual guided discrepancy map with two hourglass modules. In this way, a reliable discrepancy map that models the contextual relationship and captures fine-grained image details is learned. On the other hand, DICN incorporates the predicted discrepancy information into both the latent code and the GAN generator with different transformations, generating high-quality reconstructed/edited images. This effectively compensates for the loss of image details during GAN inversion. Both quantitative and qualitative experiments demonstrate that our proposed method achieves the excellent distortion-editability trade-off at a fast inference speed for both image inversion and editing tasks. Our code is available at https://github.com/ZzqLKED/SDIC",
    "checked": true,
    "id": "ffeb1657ef6bbb4ae35861445629103e8a447c2d",
    "semantic_title": "spatial-contextual discrepancy information compensation for gan inversion",
    "citation_count": 0,
    "authors": [
      "Ziqiang Zhang",
      "Yan Yan",
      "Jing-Hao Xue",
      "Hanzi Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28575": {
    "title": "Self-Distillation Regularized Connectionist Temporal Classification Loss for Text Recognition: A Simple Yet Effective Approach",
    "volume": "main",
    "abstract": "Text recognition methods are gaining rapid development. Some advanced techniques, e.g., powerful modules, language models, and un- and semi-supervised learning schemes, consecutively push the performance on public benchmarks forward. However, the problem of how to better optimize a text recognition model from the perspective of loss functions is largely overlooked. CTC-based methods, widely used in practice due to their good balance between performance and inference speed, still grapple with accuracy degradation. This is because CTC loss emphasizes the optimization of the entire sequence target while neglecting to learn individual characters. We propose a self-distillation scheme for CTC-based model to address this issue. It incorporates a framewise regularization term in CTC loss to emphasize individual supervision, and leverages the maximizing-a-posteriori of latent alignment to solve the inconsistency problem that arises in distillation between CTC-based models. We refer to the regularized CTC loss as Distillation Connectionist Temporal Classification (DCTC) loss. DCTC loss is module-free, requiring no extra parameters, longer inference lag, or additional training data or phases. Extensive experiments on public benchmarks demonstrate that DCTC can boost text recognition model accuracy by up to 2.6%, without any of these drawbacks",
    "checked": true,
    "id": "7e274d37e59611cc26e68c722fffef3b5469ffd7",
    "semantic_title": "self-distillation regularized connectionist temporal classification loss for text recognition: a simple yet effective approach",
    "citation_count": 1,
    "authors": [
      "Ziyin Zhang",
      "Ning Lu",
      "Minghui Liao",
      "Yongshuai Huang",
      "Cheng Li",
      "Min Wang",
      "Wei Peng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28576": {
    "title": "PNeRFLoc: Visual Localization with Point-Based Neural Radiance Fields",
    "volume": "main",
    "abstract": "Due to the ability to synthesize high-quality novel views, Neural Radiance Fields (NeRF) has been recently exploited to improve visual localization in a known environment. However, the existing methods mostly utilize NeRF for data augmentation to improve the regression model training, and their performances on novel viewpoints and appearances are still limited due to the lack of geometric constraints. In this paper, we propose a novel visual localization framework, i.e., PNeRFLoc, based on a unified point-based representation. On one hand, PNeRFLoc supports the initial pose estimation by matching 2D and 3D feature points as traditional structure-based methods; on the other hand, it also enables pose refinement with novel view synthesis using rendering-based optimization. Specifically, we propose a novel feature adaption module to close the gaps between the features for visual localization and neural rendering. To improve the efficacy and efficiency of neural rendering-based optimization, we also developed an efficient rendering-based framework with a warping loss function. Extensive experiments demonstrate that PNeRFLoc performs the best on the synthetic dataset when the 3D NeRF model can be well learned, and significantly outperforms all the NeRF-boosted localization methods with on-par SOTA performance on the real-world benchmark localization datasets. Project webpage: https://zju3dv.github.io/PNeRFLoc/",
    "checked": true,
    "id": "795c24e96c54d5aea0a9b52d7b1c028e67272586",
    "semantic_title": "pnerfloc: visual localization with point-based neural radiance fields",
    "citation_count": 1,
    "authors": [
      "Boming Zhao",
      "Luwei Yang",
      "Mao Mao",
      "Hujun Bao",
      "Zhaopeng Cui"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28577": {
    "title": "SimDistill: Simulated Multi-Modal Distillation for BEV 3D Object Detection",
    "volume": "main",
    "abstract": "Multi-view camera-based 3D object detection has become popular due to its low cost, but accurately inferring 3D geometry solely from camera data remains challenging and may lead to inferior performance. Although distilling precise 3D geometry knowledge from LiDAR data could help tackle this challenge, the benefits of LiDAR information could be greatly hindered by the significant modality gap between different sensory modalities. To address this issue, we propose a Simulated multi-modal Distillation (SimDistill) method by carefully crafting the model architecture and distillation strategy. Specifically, we devise multi-modal architectures for both teacher and student models, including a LiDAR-camera fusion-based teacher and a simulated fusion-based student. Owing to the ``identical'' architecture design, the student can mimic the teacher to generate multi-modal features with merely multi-view images as input, where a geometry compensation module is introduced to bridge the modality gap. Furthermore, we propose a comprehensive multi-modal distillation scheme that supports intra-modal, cross-modal, and multi-modal fusion distillation simultaneously in the Bird's-eye-view space. Incorporating them together, our SimDistill can learn better feature representations for 3D object detection while maintaining a cost-effective camera-only deployment. Extensive experiments validate the effectiveness and superiority of SimDistill over state-of-the-art methods, achieving an improvement of 4.8% mAP and 4.1% NDS over the baseline detector. The source code will be released at https://github.com/ViTAE-Transformer/SimDistill",
    "checked": true,
    "id": "cf5b9d4209d564ea2cfef3e9d2a927d1964632b1",
    "semantic_title": "simdistill: simulated multi-modal distillation for bev 3d object detection",
    "citation_count": 6,
    "authors": [
      "Haimei Zhao",
      "Qiming Zhang",
      "Shanshan Zhao",
      "Zhe Chen",
      "Jing Zhang",
      "Dacheng Tao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28578": {
    "title": "Large Occluded Human Image Completion via Image-Prior Cooperating",
    "volume": "main",
    "abstract": "The completion of large occluded human body images poses a unique challenge for general image completion methods. The complex shape variations of human bodies make it difficult to establish a consistent understanding of their structures. Furthermore, as human vision is highly sensitive to human bodies, even slight artifacts can significantly compromise image fidelity. To address these challenges, we propose a large occluded human image completion (LOHC) model based on a novel image-prior cooperative completion strategy. Our model leverages human segmentation maps as a prior, and completes the image and prior simultaneously. Compared to the widely adopted prior-then-image completion strategy for object completion, this cooperative completion process fosters more effective interaction between the prior and image information. Our model consists of two stages. The first stage is a transformer-based auto-regressive network that predicts the overall structure of the missing area by generating a coarse completed image at a lower resolution. The second stage is a convolutional network that refines the coarse images. As the coarse result may not always be accurate, we propose a Dynamic Fusion Module (DFM) to selectively fuses the useful features from the coarse image with the original input at spatial and channel levels. Through extensive experiments, we demonstrate our method's superior performance compared to state-of-the-art methods",
    "checked": true,
    "id": "05e98ecefb02efeb1688177e401a0432d695753c",
    "semantic_title": "large occluded human image completion via image-prior cooperating",
    "citation_count": 0,
    "authors": [
      "Hengrun Zhao",
      "Yu Zeng",
      "Huchuan Lu",
      "Lijun Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28579": {
    "title": "Recognizing Ultra-High-Speed Moving Objects with Bio-Inspired Spike Camera",
    "volume": "main",
    "abstract": "Bio-inspired spike camera mimics the sampling principle of primate fovea. It presents high temporal resolution and dynamic range, showing great promise in fast-moving object recognition. However, the physical limit of CMOS technology in spike cameras still hinders their capability of recognizing ultra-high-speed moving objects, e.g., extremely fast motions cause blur during the imaging process of spike cameras. This paper presents the first theoretical analysis for the causes of spiking motion blur and proposes a robust representation that addresses this issue through temporal-spatial context learning. The proposed method leverages multi-span feature aggregation to capture temporal cues and employs residual deformable convolution to model spatial correlation among neighbouring pixels. Additionally, this paper contributes an original real-captured spiking recognition dataset consisting of 12,000 ultra-high-speed (equivalent speed > 500 km/h) moving objects. Experimental results show that the proposed method achieves 73.2% accuracy in recognizing 10 classes of ultra-high-speed moving objects, outperforming all existing spike-based recognition methods. Resources will be available at https://github.com/Evin-X/UHSR",
    "checked": true,
    "id": "9a7276fbbb17596cca9a239efd8a129a92ba234a",
    "semantic_title": "recognizing ultra-high-speed moving objects with bio-inspired spike camera",
    "citation_count": 0,
    "authors": [
      "Junwei Zhao",
      "Shiliang Zhang",
      "Zhaofei Yu",
      "Tiejun Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28580": {
    "title": "Rethinking Two-Stage Referring Expression Comprehension: A Novel Grounding and Segmentation Method Modulated by Point",
    "volume": "main",
    "abstract": "As a fundamental and challenging task in the vision and language domain, Referring Expression Comprehension (REC) has shown impressive improvements recently. However, for a complex task that couples the comprehension of abstract concepts and the localization of concrete instances, one-stage approaches are bottlenecked by computing and data resources. To obtain a low-cost solution, the prevailing two-stage approaches decouple REC into localization (region proposal) and comprehension (region-expression matching) at region-level, but the solution based on isolated regions cannot sufficiently utilize the context and is usually limited by the quality of proposals. Therefore, it is necessary to rebuild an efficient two-stage solution system. In this paper, we propose a point-based two-stage framework for REC, in which the two stages are redefined as point-based cross-modal comprehension and point-based instance localization. Specifically, we reconstruct the raw bounding box and segmentation mask into center and mass scores as soft ground-truth for measuring point-level cross-modal correlations. With the soft ground-truth, REC can be approximated as a binary classification problem, which fundamentally avoids the impact of isolated regions on the optimization process. Remarkably, the consistent metrics between center and mass scores allow our system to directly optimize grounding and segmentation by utilizing the same architecture. Experiments on multiple benchmarks show the feasibility and potential of our point-based paradigm. Our code available at https://github.com/VILAN-Lab/PBREC-MT",
    "checked": true,
    "id": "e56d02ab9942aabaf61bf09a3490835e1305d06f",
    "semantic_title": "rethinking two-stage referring expression comprehension: a novel grounding and segmentation method modulated by point",
    "citation_count": 0,
    "authors": [
      "Peizhi Zhao",
      "Shiyi Zheng",
      "Wenye Zhao",
      "Dongsheng Xu",
      "Pijian Li",
      "Yi Cai",
      "Qingbao Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28581": {
    "title": "Optical Flow for Spike Camera with Hierarchical Spatial-Temporal Spike Fusion",
    "volume": "main",
    "abstract": "As an emerging neuromorphic camera with an asynchronous working mechanism, spike camera shows good potential for high-speed vision tasks. Each pixel in spike camera accumulates photons persistently and fires a spike whenever the accumulation exceeds a threshold. Such high-frequency fine-granularity photon recording facilitates the analysis and recovery of dynamic scenes with high-speed motion. This paper considers the optical flow estimation problem for spike cameras. Due to the Poisson nature of incoming photons, the occurrence of spikes is random and fluctuating, making conventional image matching inefficient. We propose a Hierarchical Spatial-Temporal (HiST) fusion module for spike representation to pursue reliable feature matching and develop a robust optical flow network, dubbed as HiST-SFlow. The HiST extracts features at multiple moments and hierarchically fuses the spatial-temporal information. We also propose an intra-moment filtering module to further extract the feature and suppress the influence of randomness in spikes. A scene loss is proposed to ensure that this hierarchical representation recovers the essential visual information in the scene. Experimental results demonstrate that the proposed method achieves state-of-the-art performance compared with the existing methods. The source codes are available at https://github.com/ruizhao26/HiST-SFlow",
    "checked": true,
    "id": "71e49d12187dcf6b0f15b29935c5de2cfeabfc78",
    "semantic_title": "optical flow for spike camera with hierarchical spatial-temporal spike fusion",
    "citation_count": 2,
    "authors": [
      "Rui Zhao",
      "Ruiqin Xiong",
      "Jian Zhang",
      "Xinfeng Zhang",
      "Zhaofei Yu",
      "Tiejun Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28582": {
    "title": "Towards Fine-Grained HBOE with Rendered Orientation Set and Laplace Smoothing",
    "volume": "main",
    "abstract": "Human body orientation estimation (HBOE) aims to estimate the orientation of a human body relative to the camera's frontal view. Despite recent advancements in this field, there still exist limitations in achieving fine-grained results. We identify certain defects and propose corresponding approaches as follows: 1). Existing datasets suffer from non-uniform angle distributions, resulting in sparse image data for certain angles. To provide comprehensive and high-quality data, we introduce RMOS (Rendered Model Orientation Set), a rendered dataset comprising 150K accurately labeled human instances with a wide range of orientations. 2). Directly using one-hot vector as labels may overlook the similarity between angle labels, leading to poor supervision. And converting the predictions from radians to degrees enlarges the regression error. To enhance supervision, we employ Laplace smoothing to vectorize the label, which contains more information. For fine-grained predictions, we adopt weighted Smooth-L1-loss to align predictions with the smoothed-label, thus providing robust supervision. 3). Previous works ignore body-part-specific information, resulting in coarse predictions. By employing local-window self-attention, our model could utilize different body part information for more precise orientation estimations. We validate the effectiveness of our method in the benchmarks with extensive experiments and show that our method outperforms state-of-the-art. Project is available at: https://github.com/Whalesong-zrs/Towards-Fine-grained-HBOE",
    "checked": true,
    "id": "ed1a94902efb5ac38c6f0a280ddc1bfa0d80a107",
    "semantic_title": "towards fine-grained hboe with rendered orientation set and laplace smoothing",
    "citation_count": 0,
    "authors": [
      "Ruisi Zhao",
      "Mingming Li",
      "Zheng Yang",
      "Binbin Lin",
      "Xiaohui Zhong",
      "Xiaobo Ren",
      "Deng Cai",
      "Boxi Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28583": {
    "title": "No Head Left Behind – Multi-Head Alignment Distillation for Transformers",
    "volume": "main",
    "abstract": "Knowledge distillation aims at reducing model size without compromising much performance. Recent work has applied it to large vision-language (VL) Transformers, and has shown that attention maps in the multi-head attention modules of vision-language Transformers contain extensive intra-modal and cross-modal co-reference relations to be distilled. The standard approach is to apply a one-to-one attention map distillation loss, i.e. the Teacher's first attention head instructs the Student's first head, the second teaches the second, and so forth, but this only works when the numbers of attention heads in the Teacher and Student are the same. To remove this constraint, we propose a new Attention Map Alignment Distillation (AMAD) method for Transformers with multi-head attention, which works for a Teacher and a Student with different numbers of attention heads. Specifically, we soft-align different heads in Teacher and Student attention maps using a cosine similarity weighting. The Teacher head contributes more to the Student heads for which it has a higher similarity weight. Each Teacher head contributes to all the Student heads by minimizing the divergence between the attention activation distributions for the soft-aligned heads. No head is left behind. This distillation approach operates like cross-attention. We experiment on distilling VL-T5 and BLIP, and apply AMAD loss on their T5, BERT, and ViT sub-modules. We show, under vision-language setting, that AMAD outperforms conventional distillation methods on VQA-2.0, COCO captioning, and Multi30K translation datasets. We further show that even without VL pre-training, the distilled VL-T5 models outperform corresponding VL pre-trained VL-T5 models that are further fine-tuned by ground-truth signals, and that fine-tuning distillation can also compensate to some degree for the absence of VL pre-training for BLIP models",
    "checked": false,
    "id": "b4c2ca16efcc514ddc869f6c6d9f55d4282d9edc",
    "semantic_title": "no head left behind - multi-head alignment distillation for transformers",
    "citation_count": 0,
    "authors": [
      "Tianyang Zhao",
      "Kunwar Yashraj Singh",
      "Srikar Appalaraju",
      "Peng Tang",
      "Vijay Mahadevan",
      "R. Manmatha",
      "Ying Nian Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28584": {
    "title": "SFC: Shared Feature Calibration in Weakly Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "Image-level weakly supervised semantic segmentation has received increasing attention due to its low annotation cost. Existing methods mainly rely on Class Activation Mapping (CAM) to obtain pseudo-labels for training semantic segmentation models. In this work, we are the first to demonstrate that long-tailed distribution in training data can cause the CAM calculated through classifier weights over-activated for head classes and under-activated for tail classes due to the shared features among head- and tail- classes. This degrades pseudo-label quality and further influences final semantic segmentation performance. To address this issue, we propose a Shared Feature Calibration (SFC) method for CAM generation. Specifically, we leverage the class prototypes which carry positive shared features and propose a Multi-Scaled Distribution-Weighted (MSDW) consistency loss for narrowing the gap between the CAMs generated through classifier weights and class prototypes during training. The MSDW loss counterbalances over-activation and under-activation by calibrating the shared features in head-/tail-class classifier weights. Experimental results show that our SFC significantly improves CAM boundaries and achieves new state-of-the-art performances. The project is available at https://github.com/Barrett-python/SFC",
    "checked": true,
    "id": "35142ac0ef855d7188549d6c259c5e336ce601bd",
    "semantic_title": "sfc: shared feature calibration in weakly supervised semantic segmentation",
    "citation_count": 4,
    "authors": [
      "Xinqiao Zhao",
      "Feilong Tang",
      "Xiaoyang Wang",
      "Jimin Xiao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28585": {
    "title": "Unifying Multi-Modal Uncertainty Modeling and Semantic Alignment for Text-to-Image Person Re-identification",
    "volume": "main",
    "abstract": "Text-to-Image person re-identification (TI-ReID) aims to retrieve the images of target identity according to the given textual description. The existing methods in TI-ReID focus on aligning the visual and textual modalities through contrastive feature alignment or reconstructive masked language modeling (MLM). However, these methods parameterize the image/text instances as deterministic embeddings and do not explicitly consider the inherent uncertainty in pedestrian images and their textual descriptions, leading to limited image-text relationship expression and semantic alignment. To address the above problem, in this paper, we propose a novel method that unifies multi-modal uncertainty modeling and semantic alignment for TI-ReID. Specifically, we model the image and textual feature vectors of pedestrian as Gaussian distributions, where the multi-granularity uncertainty of the distribution is estimated by incorporating batch-level and identity-level feature variances for each modality. The multi-modal uncertainty modeling acts as a feature augmentation and provides richer image-text semantic relationship. Then we present a bi-directional cross-modal circle loss to more effectively align the probabilistic features between image and text in a self-paced manner. To further promote more comprehensive image-text semantic alignment, we design a task that complements the masked language modeling, focusing on the cross-modality semantic recovery of global masked token after cross-modal interaction. Extensive experiments conducted on three TI-ReID datasets highlight the effectiveness and superiority of our method over state-of-the-arts",
    "checked": true,
    "id": "7f7eb538b68948c1ed9256d4dffc5bc6f2a0857e",
    "semantic_title": "unifying multi-modal uncertainty modeling and semantic alignment for text-to-image person re-identification",
    "citation_count": 0,
    "authors": [
      "Zhiwei Zhao",
      "Bin Liu",
      "Yan Lu",
      "Qi Chu",
      "Nenghai Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28586": {
    "title": "Mining Gaze for Contrastive Learning toward Computer-Assisted Diagnosis",
    "volume": "main",
    "abstract": "Obtaining large-scale radiology reports can be difficult for medical images due to ethical concerns, limiting the effectiveness of contrastive pre-training in the medical image domain and underscoring the need for alternative methods. In this paper, we propose eye-tracking as an alternative to text reports, as it allows for the passive collection of gaze signals without ethical issues. By tracking the gaze of radiologists as they read and diagnose medical images, we can understand their visual attention and clinical reasoning. When a radiologist has similar gazes for two medical images, it may indicate semantic similarity for diagnosis, and these images should be treated as positive pairs when pre-training a computer-assisted diagnosis (CAD) network through contrastive learning. Accordingly, we introduce the Medical contrastive Gaze Image Pre-training (McGIP) as a plug-and-play module for contrastive learning frameworks. McGIP uses radiologist gaze to guide contrastive pre-training. We evaluate our method using two representative types of medical images and two common types of gaze data. The experimental results demonstrate the practicality of McGIP, indicating its high potential for various clinical scenarios and applications",
    "checked": true,
    "id": "df1851fc667ef02a36084aefa67c6e49e2f856ad",
    "semantic_title": "mining gaze for contrastive learning toward computer-assisted diagnosis",
    "citation_count": 2,
    "authors": [
      "Zihao Zhao",
      "Sheng Wang",
      "Qian Wang",
      "Dinggang Shen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28587": {
    "title": "Quad Bayer Joint Demosaicing and Denoising Based on Dual Encoder Network with Joint Residual Learning",
    "volume": "main",
    "abstract": "The recent imaging technology Quad Bayer CFA brings better imaging PSNR and higher visual quality compared to traditional Bayer CFA, but also serious challenges for demosaicing and denoising during the ISP pipeline. In this paper, we propose a novel dual encoder network, namely DRNet, to achieve joint demosaicing and denoising for Quad Bayer CFA. The dual encoders are carefully designed in that one is mainly constructed by a joint residual block to jointly estimate the residuals for demosaicing and denoising separately. In contrast, the other one is started with a pixel modulation block which is specially designed to match the characteristics of Quad Bayer pattern for better feature extraction. We demonstrate the effectiveness of each proposed component through detailed ablation investigations. The comparison results on public benchmarks illustrate that our DRNet achieves an apparent performance gain~(0.38dB to the 2nd best) from the state-of-the-art method and balances performance and efficiency well. The experiments on real-world images show that the proposed method could enhance the reconstruction quality from the native ISP algorithm",
    "checked": true,
    "id": "b21f014ed14846f152d153bf9167b46b935e615e",
    "semantic_title": "quad bayer joint demosaicing and denoising based on dual encoder network with joint residual learning",
    "citation_count": 1,
    "authors": [
      "Bolun Zheng",
      "Haoran Li",
      "Quan Chen",
      "Tingyu Wang",
      "Xiaofei Zhou",
      "Zhenghui Hu",
      "Chenggang Yan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28588": {
    "title": "End-to-End RGB-D Image Compression via Exploiting Channel-Modality Redundancy",
    "volume": "main",
    "abstract": "As a kind of 3D data, RGB-D images have been extensively used in object tracking, 3D reconstruction, remote sensing mapping, and other tasks. In the realm of computer vision, the significance of RGB-D images is progressively growing. However, the existing learning-based image compression methods usually process RGB images and depth images separately, which cannot entirely exploit the redundant information between the modalities, limiting the further improvement of the Rate-Distortion performance. With the goal of overcoming the defect, in this paper, we propose a learning-based dual-branch RGB-D image compression framework. Compared with traditional RGB domain compression scheme, a YUV domain compression scheme is presented for spatial redundancy removal. In addition, Intra-Modality Attention (IMA) and Cross-Modality Attention (CMA) are introduced for modal redundancy removal. For the sake of benefiting from cross-modal prior information, Context Prediction Module (CPM) and Context Fusion Module (CFM) are raised in the conditional entropy model which makes the context probability prediction more accurate. The experimental results demonstrate our method outperforms existing image compression methods in two RGB-D image datasets. Compared with BPG, our proposed framework can achieve up to 15% bit rate saving for RGB images",
    "checked": true,
    "id": "7a76677c8ee2ace9eeaf2f948514067afaf82e48",
    "semantic_title": "end-to-end rgb-d image compression via exploiting channel-modality redundancy",
    "citation_count": 1,
    "authors": [
      "Huiming Zheng",
      "Wei Gao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28589": {
    "title": "Any-Size-Diffusion: Toward Efficient Text-Driven Synthesis for Any-Size HD Images",
    "volume": "main",
    "abstract": "Stable diffusion, a generative model used in text-to-image synthesis, frequently encounters resolution-induced composition problems when generating images of varying sizes. This issue primarily stems from the model being trained on pairs of single-scale images and their corresponding text descriptions. Moreover, direct training on images of unlimited sizes is unfeasible, as it would require an immense number of text-image pairs and entail substantial computational expenses. To overcome these challenges, we propose a two-stage pipeline named Any-Size-Diffusion (ASD), designed to efficiently generate well-composed HD images of any size, while minimizing the need for high-memory GPU resources. Specifically, the initial stage, dubbed Any Ratio Adaptability Diffusion (ARAD), leverages a selected set of images with a restricted range of ratios to optimize the text-conditional diffusion model, thereby improving its ability to adjust composition to accommodate diverse image sizes. To support the creation of images at any desired size, we further introduce a technique called Fast Seamless Tiled Diffusion (FSTD) at the subsequent stage. This method allows for the rapid enlargement of the ASD output to any high-resolution size, avoiding seaming artifacts or memory overloads. Experimental results on the LAION-COCO and MM-CelebA-HQ benchmarks demonstrate that ASD can produce well-structured images of arbitrary sizes, cutting down the inference time by 2X compared to the traditional tiled algorithm. The source code is available at https://github.com/ProAirVerse/Any-Size-Diffusion",
    "checked": true,
    "id": "7d7d2b9137080248c9881fb85a100bb5ba30f2ac",
    "semantic_title": "any-size-diffusion: toward efficient text-driven synthesis for any-size hd images",
    "citation_count": 17,
    "authors": [
      "Qingping Zheng",
      "Yuanfan Guo",
      "Jiankang Deng",
      "Jianhua Han",
      "Ying Li",
      "Songcen Xu",
      "Hang Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28590": {
    "title": "Spatio-Temporal Fusion for Human Action Recognition via Joint Trajectory Graph",
    "volume": "main",
    "abstract": "Graph Convolutional Networks (GCNs) and Transformers have been widely applied to skeleton-based human action recognition, with each offering unique advantages in capturing spatial relationships and long-range dependencies. However, for most GCN methods, the construction of topological structures relies solely on the spatial information of human joints, limiting their ability to directly capture richer spatio-temporal dependencies. Additionally, the self-attention modules of many Transformer methods lack topological structure information, restricting the robustness and generalization of the models. To address these issues, we propose a Joint Trajectory Graph (JTG) that integrates spatio-temporal information into a uniform graph structure. We also present a Joint Trajectory GraphFormer (JT-GraphFormer), which directly captures the spatio-temporal relationships among all joint trajectories for human action recognition. To better integrate topological information into spatio-temporal relationships, we introduce a Spatio-Temporal Dijkstra Attention (STDA) mechanism to calculate relationship scores for all the joints in JTG. Furthermore, we incorporate the Koopman operator into the classification stage to enhance the model's representation ability and classification performance. Experiments demonstrate that JT-GraphFormer achieves outstanding performance in human action recognition tasks, outperforming state-of-the-art methods on the NTU RGB+D, NTU RGB+D 120, and N-UCLA datasets",
    "checked": true,
    "id": "88373535e01f54e1be5376bc9f99f487a0894245",
    "semantic_title": "spatio-temporal fusion for human action recognition via joint trajectory graph",
    "citation_count": 0,
    "authors": [
      "Yaolin Zheng",
      "Hongbo Huang",
      "Xiuying Wang",
      "Xiaoxu Yan",
      "Longfei Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28591": {
    "title": "ODTrack: Online Dense Temporal Token Learning for Visual Tracking",
    "volume": "main",
    "abstract": "Online contextual reasoning and association across consecutive video frames are critical to perceive instances in visual tracking. However, most current top-performing trackers persistently lean on sparse temporal relationships between reference and search frames via an offline mode. Consequently, they can only interact independently within each image-pair and establish limited temporal correlations. To alleviate the above problem, we propose a simple, flexible and effective video-level tracking pipeline, named ODTrack, which densely associates the contextual relationships of video frames in an online token propagation manner. ODTrack receives video frames of arbitrary length to capture the spatio-temporal trajectory relationships of an instance, and compresses the discrimination features (localization information) of a target into a token sequence to achieve frame-to-frame association. This new solution brings the following benefits: 1) the purified token sequences can serve as prompts for the inference in the next video frame, whereby past information is leveraged to guide future inference; 2) the complex online update strategies are effectively avoided by the iterative propagation of token sequences, and thus we can achieve more efficient model representation and computation. ODTrack achieves a new SOTA performance on seven benchmarks, while running at real-time speed. Code and models are available at https://github.com/GXNU-ZhongLab/ODTrack",
    "checked": true,
    "id": "dbcf027c9d19973fbe635caa56ee31cb53fb4574",
    "semantic_title": "odtrack: online dense temporal token learning for visual tracking",
    "citation_count": 5,
    "authors": [
      "Yaozong Zheng",
      "Bineng Zhong",
      "Qihua Liang",
      "Zhiyi Mo",
      "Shengping Zhang",
      "Xianxian Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28592": {
    "title": "PVALane: Prior-Guided 3D Lane Detection with View-Agnostic Feature Alignment",
    "volume": "main",
    "abstract": "Monocular 3D lane detection is essential for a reliable autonomous driving system and has recently been rapidly developing. Existing popular methods mainly employ a predefined 3D anchor for lane detection based on front-viewed (FV) space, aiming to mitigate the effects of view transformations. However, the perspective geometric distortion between FV and 3D space in this FV-based approach introduces extremely dense anchor designs, which ultimately leads to confusing lane representations. In this paper, we introduce a novel prior-guided perspective on lane detection and propose an end-to-end framework named PVALane, which utilizes 2D prior knowledge to achieve precise and efficient 3D lane detection. Since 2D lane predictions can provide strong priors for lane existence, PVALane exploits FV features to generate sparse prior anchors with potential lanes in 2D space. These dynamic prior anchors help PVALane to achieve distinct lane representations and effectively improve the precision of PVALane due to the reduced lane search space. Additionally, by leveraging these prior anchors and representing lanes in both FV and bird-eye-viewed (BEV) spaces, we effectively align and merge semantic and geometric information from FV and BEV features. Extensive experiments conducted on the OpenLane and ONCE-3DLanes datasets demonstrate the superior performance of our method compared to existing state-of-the-art approaches and exhibit excellent robustness",
    "checked": true,
    "id": "28b7b7a38bd476fc786520129b7f10632f579d58",
    "semantic_title": "pvalane: prior-guided 3d lane detection with view-agnostic feature alignment",
    "citation_count": 1,
    "authors": [
      "Zewen Zheng",
      "Xuemin Zhang",
      "Yongqiang Mou",
      "Xiang Gao",
      "Chengxin Li",
      "Guoheng Huang",
      "Chi-Man Pun",
      "Xiaochen Yuan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28593": {
    "title": "SpFormer: Spatio-Temporal Modeling for Scanpaths with Transformer",
    "volume": "main",
    "abstract": "Saccadic scanpath, a data representation of human visual behavior, has received broad interest in multiple domains. Scanpath is a complex eye-tracking data modality that includes the sequences of fixation positions and fixation duration, coupled with image information. However, previous methods usually face the spatial misalignment problem of fixation features and loss of critical temporal data (including temporal correlation and fixation duration). In this study, we propose a Transformer-based scanpath model, SpFormer, to alleviate these problems. First, we propose a fixation-centric paradigm to extract the aligned spatial fixation features and tokenize the scanpaths. Then, according to the visual working memory mechanism, we design a local meta attention to reduce the semantic redundancy of fixations and guide the model to focus on the meta scanpath. Finally, we progressively integrate the duration information and fuse it with the fixation features to solve the problem of ambiguous location with the Transformer block increasing. We conduct extensive experiments on four databases under three tasks. The SpFormer establishes new state-of-the-art results in distinct settings, verifying its flexibility and versatility in practical applications. The code can be obtained from https://github.com/wenqizhong/SpFormer",
    "checked": true,
    "id": "658a6315ca47d7d5a4625ce429f29f60b3db171c",
    "semantic_title": "spformer: spatio-temporal modeling for scanpaths with transformer",
    "citation_count": 0,
    "authors": [
      "Wenqi Zhong",
      "Linzhi Yu",
      "Chen Xia",
      "Junwei Han",
      "Dingwen Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28594": {
    "title": "ExpCLIP: Bridging Text and Facial Expressions via Semantic Alignment",
    "volume": "main",
    "abstract": "The objective of stylized speech-driven facial animation is to create animations that encapsulate specific emotional expressions. Existing methods often depend on pre-established emotional labels or facial expression templates, which may limit the necessary flexibility for accurately conveying user intent. In this research, we introduce a technique that enables the control of arbitrary styles by leveraging natural language as emotion prompts. This technique presents benefits in terms of both flexibility and user-friendliness. To realize this objective, we initially construct a Text-Expression Alignment Dataset (TEAD), wherein each facial expression is paired with several prompt-like descriptions. We propose an innovative automatic annotation method, supported by CahtGPT, to expedite the dataset construction, thereby eliminating the substantial expense of manual annotation. Following this, we utilize TEAD to train a CLIP-based model, termed ExpCLIP, which encodes text and facial expressions into semantically aligned style embeddings. The embeddings are subsequently integrated into the facial animation generator to yield expressive and controllable facial animations. Given the limited diversity of facial emotions in existing speech-driven facial animation training data, we further introduce an effective Expression Prompt Augmentation (EPA) mechanism to enable the animation generator to support unprecedented richness in style control. Comprehensive experiments illustrate that our method accomplishes expressive facial animation generation and offers enhanced flexibility in effectively conveying the desired style",
    "checked": true,
    "id": "68159fc73e3b9d289d0b72137b1e3357d50ce2fe",
    "semantic_title": "expclip: bridging text and facial expressions via semantic alignment",
    "citation_count": 3,
    "authors": [
      "Yicheng Zhong",
      "Huawei Wei",
      "Peiji Yang",
      "Zhisheng Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28595": {
    "title": "Learning Image Demoiréing from Unpaired Real Data",
    "volume": "main",
    "abstract": "This paper focuses on addressing the issue of image demoiréing. Unlike the large volume of existing studies that rely on learning from paired real data, we attempt to learn a demoiréing model from unpaired real data, i.e., moiré images associated with irrelevant clean images. The proposed method, referred to as Unpaired Demoiréing(UnDeM), synthesizes pseudo moiré images from unpaired datasets, generating pairs with clean images for training demoiréing models. To achieve this, we divide real moiré images into patches and group them in compliance with their moiré complexity. We introduce a novel moiré generation framework to synthesize moiré images with diverse moiré features, resembling real moiré patches, and details akin to real moiré-free images. Additionally, we introduce an adaptive denoise method to eliminate the low-quality pseudo moiré images that adversely impact the learning of demoiréing models. We conduct extensive experiments on the commonly-used FHDMi and UHDM datasets. Results manifest that our UnDeM performs better than existing methods when using existing demoiréing models such as MBCNN and ESDNet-L. Code: https://github.com/zysxmu/UnDeM",
    "checked": false,
    "id": "59965b159c569ea208f80a786063cd12a28f5e9a",
    "semantic_title": "learning image demoireing from unpaired real data",
    "citation_count": 0,
    "authors": [
      "Yunshan Zhong",
      "Yuyao Zhou",
      "Yuxin Zhang",
      "Fei Chao",
      "Rongrong Ji"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28596": {
    "title": "Lifting by Image – Leveraging Image Cues for Accurate 3D Human Pose Estimation",
    "volume": "main",
    "abstract": "The \"lifting from 2D pose\" method has been the dominant approach to 3D Human Pose Estimation (3DHPE) due to the powerful visual analysis ability of 2D pose estimators. Widely known, there exists a depth ambiguity problem when estimating solely from 2D pose, where one 2D pose can be mapped to multiple 3D poses. Intuitively, the rich semantic and texture information in images can contribute to a more accurate \"lifting\" procedure. Yet, existing research encounters two primary challenges. Firstly, the distribution of image data in 3D motion capture datasets is too narrow because of the laboratorial environment, which leads to poor generalization ability of methods trained with image information. Secondly, effective strategies for leveraging image information are lacking. In this paper, we give new insight into the cause of poor generalization problems and the effectiveness of image features. Based on that, we propose an advanced framework. Specifically, the framework consists of two stages. First, we enable the keypoints to query and select the beneficial features from all image patches. To reduce the keypoints attention to inconsequential background features, we design a novel Pose-guided Transformer Layer, which adaptively limits the updates to unimportant image patches. Then, through a designed Adaptive Feature Selection Module, we prune less significant image patches from the feature map. In the second stage, we allow the keypoints to further emphasize the retained critical image features. This progressive learning approach prevents further training on insignificant image features. Experimental results show that our model achieves state-of-the-art performance on both the Human3.6M dataset and the MPI-INF-3DHP dataset",
    "checked": false,
    "id": "5c4b5ecef0a174260a09b8b5b2f27a3e9c870932",
    "semantic_title": "lifting by image - leveraging image cues for accurate 3d human pose estimation",
    "citation_count": 2,
    "authors": [
      "Feng Zhou",
      "Jianqin Yin",
      "Peiyang Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28597": {
    "title": "NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models",
    "volume": "main",
    "abstract": "Trained with an unprecedented scale of data, large language models (LLMs) like ChatGPT and GPT-4 exhibit the emergence of significant reasoning abilities from model scaling. Such a trend underscored the potential of training LLMs with unlimited language data, advancing the development of a universal embodied agent. In this work, we introduce the NavGPT, a purely LLM-based instruction-following navigation agent, to reveal the reasoning capability of GPT models in complex embodied scenes by performing zero-shot sequential action prediction for vision-and-language navigation (VLN). At each step, NavGPT takes the textual descriptions of visual observations, navigation history, and future explorable directions as inputs to reason the agent's current status, and makes the decision to approach the target. Through comprehensive experiments, we demonstrate NavGPT can explicitly perform high-level planning for navigation, including decomposing instruction into sub-goals, integrating commonsense knowledge relevant to navigation task resolution, identifying landmarks from observed scenes, tracking navigation progress, and adapting to exceptions with plan adjustment. Furthermore, we show that LLMs is capable of generating high-quality navigational instructions from observations and actions along a path, as well as drawing accurate top-down metric trajectory given the agent's navigation history. Despite the performance of using NavGPT to zero-shot R2R tasks still falling short of trained models, we suggest adapting multi-modality inputs for LLMs to use as visual navigation agents and applying the explicit reasoning of LLMs to benefit learning-based models. Code is available at: https://github.com/GengzeZhou/NavGPT",
    "checked": true,
    "id": "8199c9d55dd998f69f703e0ad250ca0697e3ad27",
    "semantic_title": "navgpt: explicit reasoning in vision-and-language navigation with large language models",
    "citation_count": 49,
    "authors": [
      "Gengze Zhou",
      "Yicong Hong",
      "Qi Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28598": {
    "title": "Novel Class Discovery in Chest X-rays via Paired Images and Text",
    "volume": "main",
    "abstract": "Novel class discover(NCD) aims to identify new classes undefined during model training phase with the help of knowledge of known classes. Many methods have been proposed and notably boosted performance of NCD in natural images. However, there has been no work done in discovering new classes based on medical images and disease categories, which is crucial for understanding and diagnosing specific diseases. Moreover, most of the existing methods only utilize information from image modality and use labels as the only supervisory information. In this paper, we propose a multi-modal novel class discovery method based on paired images and text, inspired by the low classification accuracy of chest X-ray images and the relatively higher accuracy of the paired text. Specifically, we first pretrain the image encoder and text encoder with multi-modal contrastive learning on the entire dataset and then we generate pseudo-labels separately on the image branch and text branch. We utilize intra-modal consistency to assess the quality of pseudo-labels and adjust the weights of the pseudo-labels from both branches to generate the ultimate pseudo-labels for training. Experiments on eight subset splits of MIMIC-CXR-JPG dataset show that our method improves the clustering performance of unlabeled classes by about 10% on average compared to state-of-the-art methods. Code is available at: https://github.com/zzzzzzzzjy/MMNCD-main",
    "checked": true,
    "id": "42cc1dadeb50663ec186eabdffd02bf89fb4fa7f",
    "semantic_title": "novel class discovery in chest x-rays via paired images and text",
    "citation_count": 0,
    "authors": [
      "Jiaying Zhou",
      "Yang Liu",
      "Qingchao Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28599": {
    "title": "AMSP-UOD: When Vortex Convolution and Stochastic Perturbation Meet Underwater Object Detection",
    "volume": "main",
    "abstract": "In this paper, we present a novel Amplitude-Modulated Stochastic Perturbation and Vortex Convolutional Network, AMSP-UOD, designed for underwater object detection. AMSP-UOD specifically addresses the impact of non-ideal imaging factors on detection accuracy in complex underwater environments. To mitigate the influence of noise on object detection performance, we propose AMSP Vortex Convolution (AMSP-VConv) to disrupt the noise distribution, enhance feature extraction capabilities, effectively reduce parameters, and improve network robustness. We design the Feature Association Decoupling Cross Stage Partial (FAD-CSP) module, which strengthens the association of long and short range features, improving the network performance in complex underwater environments. Additionally, our sophisticated post-processing method, based on non-maximum suppression with aspect-ratio similarity thresholds, optimizes detection in dense scenes, such as waterweed and schools of fish, improving object detection accuracy. Extensive experiments on the URPC and RUOD datasets demonstrate that our method outperforms existing state-of-the-art methods in terms of accuracy and noise immunity. AMSP-UOD proposes an innovative solution with the potential for real-world applications. Our code is available at https://github.com/zhoujingchun03/AMSP-UOD",
    "checked": true,
    "id": "52f6d071a65babb1b6b84f206abb4c2ffb733130",
    "semantic_title": "amsp-uod: when vortex convolution and stochastic perturbation meet underwater object detection",
    "citation_count": 1,
    "authors": [
      "Jingchun Zhou",
      "Zongxin He",
      "Kin-Man Lam",
      "Yudong Wang",
      "Weishi Zhang",
      "Chunle Guo",
      "Chongyi Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28600": {
    "title": "SOGDet: Semantic-Occupancy Guided Multi-View 3D Object Detection",
    "volume": "main",
    "abstract": "In the field of autonomous driving, accurate and comprehensive perception of the 3D environment is crucial. Bird's Eye View (BEV) based methods have emerged as a promising solution for 3D object detection using multi-view images as input. However, existing 3D object detection methods often ignore the physical context in the environment, such as sidewalk and vegetation, resulting in sub-optimal performance. In this paper, we propose a novel approach called SOGDet (Semantic-Occupancy Guided Multi-view 3D Object Detection), that leverages a 3D semantic-occupancy branch to improve the accuracy of 3D object detection. In particular, the physical context modeled by semantic occupancy helps the detector to perceive the scenes in a more holistic view. Our SOGDet is flexible to use and can be seamlessly integrated with most existing BEV-based methods. To evaluate its effectiveness, we apply this approach to several state-of-the-art baselines and conduct extensive experiments on the exclusive nuScenes dataset. Our results show that SOGDet consistently enhance the performance of three baseline methods in terms of nuScenes Detection Score (NDS) and mean Average Precision (mAP). This indicates that the combination of 3D object detection and 3D semantic occupancy leads to a more comprehensive perception of the 3D environment, thereby aiding build more robust autonomous driving systems. The codes are available at: https://github.com/zhouqiu/SOGDet",
    "checked": true,
    "id": "cbcdb85838ae0d6511c2f7c3e22e84134d308e39",
    "semantic_title": "sogdet: semantic-occupancy guided multi-view 3d object detection",
    "citation_count": 3,
    "authors": [
      "Qiu Zhou",
      "Jinming Cao",
      "Hanchao Leng",
      "Yifang Yin",
      "Yu Kun",
      "Roger Zimmermann"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28601": {
    "title": "Test-Time Adaptation via Style and Structure Guidance for Histological Image Registration",
    "volume": "main",
    "abstract": "Image registration plays a crucial role in histological image analysis, encompassing tasks like multi-modality fusion and disease grading. Traditional registration methods optimize objective functions for each image pair, yielding reliable accuracy but demanding heavy inference burdens. Recently, learning-based registration methods utilize networks to learn the optimization process during training and apply a one-step forward process during testing. While these methods offer promising registration performance with reduced inference time, they remain sensitive to appearance variances and local structure changes commonly encountered in histological image registration scenarios. In this paper, for the first time, we propose a novel test-time adaptation method for histological image registration, aiming to improve the generalization ability of learning-based methods. Specifically, we design two operations, style guidance and shape guidance, for the test-time adaptation process. The former leverages style representations encoded by feature statistics to address the issue of appearance variances, while the latter incorporates shape representations encoded by HOG features to improve registration accuracy in regions with structural changes. Furthermore, we consider the continuity of the model during the test-time adaptation process. Different from the previous methods initialized by a given trained model, we introduce a smoothing strategy to leverage historical models for better generalization. We conduct experiments with several representative learning-based backbones on the public histological dataset, demonstrating the superior registration performance of our test-time adaptation method",
    "checked": true,
    "id": "3e260ae334a13586aff6a55e8ccdde59ada7d2b8",
    "semantic_title": "test-time adaptation via style and structure guidance for histological image registration",
    "citation_count": 0,
    "authors": [
      "Shenglong Zhou",
      "Zhiwei Xiong",
      "Feng Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28602": {
    "title": "Reducing Spatial Fitting Error in Distillation of Denoising Diffusion Models",
    "volume": "main",
    "abstract": "Denoising Diffusion models have exhibited remarkable capabilities in image generation. However, generating high-quality samples requires a large number of iterations. Knowledge distillation for diffusion models is an effective method to address this limitation with a shortened sampling process but causes degraded generative quality. Based on our analysis with bias-variance decomposition and experimental observations, we attribute the degradation to the spatial fitting error occurring in the training of both the teacher and student model in the distillation. Accordingly, we propose Spatial Fitting-Error Reduction Distillation model (SFERD). SFERD utilizes attention guidance from the teacher model and a designed semantic gradient predictor to reduce the student's fitting error. Empirically, our proposed model facilitates high-quality sample generation in a few function evaluations. We achieve an FID of 5.31 on CIFAR-10 and 9.39 on ImageNet 64x64 with only one step, outperforming existing diffusion methods. Our study provides a new perspective on diffusion distillation by highlighting the intrinsic denoising ability of models",
    "checked": true,
    "id": "84c5c593315322c3ba9d2a3fffd312f56bd9ee43",
    "semantic_title": "reducing spatial fitting error in distillation of denoising diffusion models",
    "citation_count": 0,
    "authors": [
      "Shengzhe Zhou",
      "Zejian Li",
      "Shengyuan Zhang",
      "Lefan Hou",
      "Changyuan Yang",
      "Guang Yang",
      "Zhiyuan Yang",
      "Lingyun Sun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28603": {
    "title": "SAMFlow: Eliminating Any Fragmentation in Optical Flow with Segment Anything Model",
    "volume": "main",
    "abstract": "Optical Flow Estimation aims to find the 2D dense motion field between two frames. Due to the limitation of model structures and training datasets, existing methods often rely too much on local clues and ignore the integrity of objects, resulting in fragmented motion estimation. Through theoretical analysis, we find the pre-trained large vision models are helpful in optical flow estimation, and we notice that the recently famous Segment Anything Model (SAM) demonstrates a strong ability to segment complete objects, which is suitable for solving the fragmentation problem. We thus propose a solution to embed the frozen SAM image encoder into FlowFormer to enhance object perception. To address the challenge of in-depth utilizing SAM in non-segmentation tasks like optical flow estimation, we propose an Optical Flow Task-Specific Adaption scheme, including a Context Fusion Module to fuse the SAM encoder with the optical flow context encoder, and a Context Adaption Module to adapt the SAM features for optical flow task with Learned Task-Specific Embedding. Our proposed SAMFlow model reaches 0.86/2.10 clean/final EPE and 3.55/12.32 EPE/F1-all on Sintel and KITTI-15 training set, surpassing Flowformer by 8.5%/9.9% and 13.2%/16.3%. Furthermore, our model achieves state-of-the-art performance on the Sintel and KITTI-15 benchmarks, ranking #1 among all two-frame methods on Sintel clean pass",
    "checked": true,
    "id": "fe37211c32ee4ae6a8fb47ac81e8c24147b9ec99",
    "semantic_title": "samflow: eliminating any fragmentation in optical flow with segment anything model",
    "citation_count": 4,
    "authors": [
      "Shili Zhou",
      "Ruian He",
      "Weimin Tan",
      "Bo Yan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28604": {
    "title": "Efficient Lightweight Image Denoising with Triple Attention Transformer",
    "volume": "main",
    "abstract": "Transformer has shown outstanding performance on image denoising, but the existing Transformer methods for image denoising are with large model sizes and high computational complexity, which is unfriendly to resource-constrained devices. In this paper, we propose a Lightweight Image Denoising Transformer method (LIDFormer) based on Triple Multi-Dconv Head Transposed Attention (TMDTA) to boost computational efficiency. LIDFormer first implements Discrete Wavelet Transform (DWT), which transforms the input image into a low-frequency space, greatly reducing the computational complexity of image denoising. However, the low-frequency image lacks fine-feature information, which degrades the denoising performance. To handle this problem, we introduce the Complementary Periodic Feature Reusing (CPFR) scheme for aggregating the shallow-layer features and the deep-layer features. Furthermore, TMDTA is proposed to integrate global context along three dimensions, thereby enhancing the ability of global feature representation. Note that our method can be applied as a pipeline for both convolutional neural networks and Transformers. Extensive experiments on several benchmarks demonstrate that the proposed LIDFormer achieves a better trade-off between high performance and low computational complexity on real-world image denoising tasks",
    "checked": true,
    "id": "623911840891e92211ff339a84490bd7c031948f",
    "semantic_title": "efficient lightweight image denoising with triple attention transformer",
    "citation_count": 1,
    "authors": [
      "Yubo Zhou",
      "Jin Lin",
      "Fangchen Ye",
      "Yanyun Qu",
      "Yuan Xie"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28605": {
    "title": "Intentional Evolutionary Learning for Untrimmed Videos with Long Tail Distribution",
    "volume": "main",
    "abstract": "Human intention understanding in untrimmed videos aims to watch a natural video and predict what the person's intention is. Currently, exploration of predicting human intentions in untrimmed videos is far from enough. On the one hand, untrimmed videos with mixed actions and backgrounds have a significant long-tail distribution with concept drift characteristics. On the other hand, most methods can only perceive instantaneous intentions, but cannot determine the evolution of intentions. To solve the above challenges, we propose a loss based on Instance Confidence and Class Accuracy (ICCA), which aims to alleviate the prediction bias caused by the long-tail distribution with concept drift characteristics in video streams. In addition, we propose an intention-oriented evolutionary learning method to determine the intention evolution pattern (from what action to what action) and the time of evolution (when the action evolves). We conducted extensive experiments on two untrimmed video datasets (THUMOS14 and ActivityNET v1.3), and our method has achieved excellent results compared to SOTA methods. The code and supplementary materials are available at https://github.com/Jennifer123www/UntrimmedVideo",
    "checked": true,
    "id": "1c683a7ca01c66756d3d667ba6b7a2e92692f696",
    "semantic_title": "intentional evolutionary learning for untrimmed videos with long tail distribution",
    "citation_count": 0,
    "authors": [
      "Yuxi Zhou",
      "Xiujie Wang",
      "Jianhua Zhang",
      "Jiajia Wang",
      "Jie Yu",
      "Hao Zhou",
      "Yi Gao",
      "Shengyong Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28606": {
    "title": "SasWOT: Real-Time Semantic Segmentation Architecture Search WithOut Training",
    "volume": "main",
    "abstract": "In this paper, we present SasWOT, the first training-free Semantic segmentation Architecture Search (SAS) framework via an auto-discovery proxy. Semantic segmentation is widely used in many real-time applications. For fast inference and memory efficiency, Previous SAS seeks the optimal segmenter by differentiable or RL Search. However, the significant computational costs of these training-based SAS limit their practical usage. To improve the search efficiency, we explore the training-free route but empirically observe that the existing zero-cost proxies designed on the classification task are sub-optimal on the segmentation benchmark. To address this challenge, we develop a customized proxy search framework for SAS tasks to augment its predictive capabilities. Specifically, we design the proxy search space based on the some observations: (1) different inputs of segmenter statistics can be well combined; (2) some basic operators can effectively improve the correlation. Thus, we build computational graphs with multiple statistics as inputs and different advanced basis arithmetic as the primary operations to represent candidate proxies. Then, we employ an evolutionary algorithm to crossover and mutate the superior candidates in the population based on correlation evaluation. Finally, based on the searched proxy, we perform the segmenter search without candidate training. In this way, SasWOT not only enables automated proxy optimization for SAS tasks but also achieves significant search acceleration before the retrain stage. Extensive experiments on Cityscapes and CamVid datasets demonstrate that SasWOT achieves superior trade-off between accuracy and speed over several state-of-the-art techniques. More remarkably, on Cityscapes dataset, SasWOT achieves the performance of 71.3% mIoU with the speed of 162 FPS",
    "checked": true,
    "id": "4cbcd2d7d89878502a28a72ecaf6391265c40ba9",
    "semantic_title": "saswot: real-time semantic segmentation architecture search without training",
    "citation_count": 4,
    "authors": [
      "Chendi Zhu",
      "Lujun Li",
      "Yuli Wu",
      "Zhengxing Sun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28607": {
    "title": "Enhance Sketch Recognition's Explainability via Semantic Component-Level Parsing",
    "volume": "main",
    "abstract": "Free-hand sketches are appealing for humans as a universal tool to depict the visual world. Humans can recognize varied sketches of a category easily by identifying the concurrence and layout of the intrinsic semantic components of the category, since humans draw free-hand sketches based a common consensus that which types of semantic components constitute each sketch category. For example, an airplane should at least have a fuselage and wings. Based on this analysis, a semantic component-level memory module is constructed and embedded in the proposed structured sketch recognition network in this paper. The memory keys representing semantic components of each sketch category can be self-learned and enhance the recognition network's explainability. Our proposed networks can deal with different situations of sketch recognition, i.e., with or without semantic components labels of strokes. Experiments on the SPG and SketchIME datasets demonstrate the memory module's flexibility and the recognition network's explainability. The code and data are available at https://github.com/GuangmingZhu/SketchESC",
    "checked": true,
    "id": "3a194c1caeaeffe0abc1f6337eb8f9b21100e95a",
    "semantic_title": "enhance sketch recognition's explainability via semantic component-level parsing",
    "citation_count": 0,
    "authors": [
      "Guangming Zhu",
      "Siyuan Wang",
      "Tianci Wu",
      "Liang Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28608": {
    "title": "Learning Discriminative Noise Guidance for Image Forgery Detection and Localization",
    "volume": "main",
    "abstract": "This study introduces a new method for detecting and localizing image forgery by focusing on manipulation traces within the noise domain. We posit that nearly invisible noise in RGB images carries tampering traces, useful for distinguishing and locating forgeries. However, the advancement of tampering technology complicates the direct application of noise for forgery detection, as the noise inconsistency between forged and authentic regions is not fully exploited. To tackle this, we develop a two-step discriminative noise-guided approach to explicitly enhance the representation and use of noise inconsistencies, thereby fully exploiting noise information to improve the accuracy and robustness of forgery detection. Specifically, we first enhance the noise discriminability of forged regions compared to authentic ones using a de-noising network and a statistics-based constraint. Then, we merge a model-driven guided filtering mechanism with a data-driven attention mechanism to create a learnable and differentiable noise-guided filter. This sophisticated filter allows us to maintain the edges of forged regions learned from the noise. Comprehensive experiments on multiple datasets demonstrate that our method can reliably detect and localize forgeries, surpassing existing state-of-the-art methods",
    "checked": true,
    "id": "bcd886bed631db5a58dc81911d602b49c3bcfd54",
    "semantic_title": "learning discriminative noise guidance for image forgery detection and localization",
    "citation_count": 1,
    "authors": [
      "Jiaying Zhu",
      "Dong Li",
      "Xueyang Fu",
      "Gang Yang",
      "Jie Huang",
      "Aiping Liu",
      "Zheng-Jun Zha"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28609": {
    "title": "Video Frame Prediction from a Single Image and Events",
    "volume": "main",
    "abstract": "Recently, the task of Video Frame Prediction (VFP), which predicts future video frames from previous ones through extrapolation, has made remarkable progress. However, the performance of existing VFP methods is still far from satisfactory due to the fixed framerate video used: 1) they have difficulties in handling complex dynamic scenes; 2) they cannot predict future frames with flexible prediction time intervals. The event cameras can record the intensity changes asynchronously with a very high temporal resolution, which provides rich dynamic information about the observed scenes. In this paper, we propose to predict video frames from a single image and the following events, which can not only handle complex dynamic scenes but also predict future frames with flexible prediction time intervals. First, we introduce a symmetrical cross-modal attention augmentation module to enhance the complementary information between images and events. Second, we propose to jointly achieve optical flow estimation and frame generation by combining the motion information of events and the semantic information of the image, then inpainting the holes produced by forward warping to obtain an ideal prediction frame. Based on these, we propose a lightweight pyramidal coarse-to-fine model that can predict a 720P frame within 25 ms. Extensive experiments show that our proposed model significantly outperforms the state-of-the-art frame-based and event-based VFP methods and has the fastest runtime. Code is available at https://npucvr.github.io/VFPSIE/",
    "checked": true,
    "id": "b09e59bed6bc6649f6ac825ff37e10ec06c86931",
    "semantic_title": "video frame prediction from a single image and events",
    "citation_count": 0,
    "authors": [
      "Juanjuan Zhu",
      "Zhexiong Wan",
      "Yuchao Dai"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28610": {
    "title": "Finding Visual Saliency in Continuous Spike Stream",
    "volume": "main",
    "abstract": "As a bio-inspired vision sensor, the spike camera emulates the operational principles of the fovea, a compact retinal region, by employing spike discharges to encode the accumulation of per-pixel luminance intensity. Leveraging its high temporal resolution and bio-inspired neuromorphic design, the spike camera holds significant promise for advancing computer vision applications. Saliency detection mimic the behavior of human beings and capture the most salient region from the scenes. In this paper, we investigate the visual saliency in the continuous spike stream for the first time. To effectively process the binary spike stream, we propose a Recurrent Spiking Transformer (RST) framework, which is based on a full spiking neural network. Our framework enables the extraction of spatio-temporal features from the continuous spatio-temporal spike stream while maintaining low power consumption. To facilitate the training and validation of our proposed model, we build a comprehensive real-world spike-based visual saliency dataset, enriched with numerous light conditions. Extensive experiments demonstrate the superior performance of our Recurrent Spiking Transformer framework in comparison to other spike neural network-based methods. Our framework exhibits a substantial margin of improvement in capturing and highlighting visual saliency in the spike stream, which not only provides a new perspective for spike-based saliency segmentation but also shows a new paradigm for full SNN-based transformer models. The code and dataset are available at https://github.com/BIT-Vision/SVS",
    "checked": true,
    "id": "15244c806812b55b545a60e9b92beb3dfb86dfb2",
    "semantic_title": "finding visual saliency in continuous spike stream",
    "citation_count": 0,
    "authors": [
      "Lin Zhu",
      "Xianzhang Chen",
      "Xiao Wang",
      "Hua Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28611": {
    "title": "SEER: Backdoor Detection for Vision-Language Models through Searching Target Text and Image Trigger Jointly",
    "volume": "main",
    "abstract": "This paper proposes SEER, a novel backdoor detection algorithm for vision-language models, addressing the gap in the literature on multi-modal backdoor detection. While backdoor detection in single-modal models has been well studied, the investigation of such defenses in multi-modal models remains limited. Existing backdoor defense mechanisms cannot be directly applied to multi-modal settings due to their increased complexity and search space explosion. In this paper, we propose to detect backdoors in vision-language models by jointly searching image triggers and malicious target texts in feature space shared by vision and language modalities. Our extensive experiments demonstrate that SEER can achieve over 92% detection rate on backdoor detection in vision-language models in various settings without accessing training data or knowledge of downstream tasks",
    "checked": true,
    "id": "0a46ffdaa4fdaa2c0a83e6efe8b6181bd98d5876",
    "semantic_title": "seer: backdoor detection for vision-language models through searching target text and image trigger jointly",
    "citation_count": 0,
    "authors": [
      "Liuwan Zhu",
      "Rui Ning",
      "Jiang Li",
      "Chunsheng Xin",
      "Hongyi Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28612": {
    "title": "Text Image Inpainting via Global Structure-Guided Diffusion Models",
    "volume": "main",
    "abstract": "Real-world text can be damaged by corrosion issues caused by environmental or human factors, which hinder the preservation of the complete styles of texts, e.g., texture and structure. These corrosion issues, such as graffiti signs and incomplete signatures, bring difficulties in understanding the texts, thereby posing significant challenges to downstream applications, e.g., scene text recognition and signature identification. Notably, current inpainting techniques often fail to adequately address this problem and have difficulties restoring accurate text images along with reasonable and consistent styles. Formulating this as an open problem of text image inpainting, this paper aims to build a benchmark to facilitate its study. In doing so, we establish two specific text inpainting datasets which contain scene text images and handwritten text images, respectively. Each of them includes images revamped by real-life and synthetic datasets, featuring pairs of original images, corrupted images, and other assistant information. On top of the datasets, we further develop a novel neural framework, Global Structure-guided Diffusion Model (GSDM), as a potential solution. Leveraging the global structure of the text as a prior, the proposed GSDM develops an efficient diffusion model to recover clean texts. The efficacy of our approach is demonstrated by thorough empirical study, including a substantial boost in both recognition accuracy and image quality. These findings not only highlight the effectiveness of our method but also underscore its potential to enhance the broader field of text image understanding and processing. Code and datasets are available at: https://github.com/blackprotoss/GSDM",
    "checked": true,
    "id": "ca8004a5e14cae464375f00f3f55895a30efdb8c",
    "semantic_title": "text image inpainting via global structure-guided diffusion models",
    "citation_count": 1,
    "authors": [
      "Shipeng Zhu",
      "Pengfei Fang",
      "Chenjie Zhu",
      "Zuoyan Zhao",
      "Qiang Xu",
      "Hui Xue"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28613": {
    "title": "Rethinking Mesh Watermark: Towards Highly Robust and Adaptable Deep 3D Mesh Watermarking",
    "volume": "main",
    "abstract": "The goal of 3D mesh watermarking is to embed the message in 3D meshes that can withstand various attacks imperceptibly and reconstruct the message accurately from watermarked meshes. The watermarking algorithm is supposed to withstand multiple attacks, and the complexity should not grow significantly with the mesh size. Unfortunately, previous methods are less robust against attacks and lack of adaptability. In this paper, we propose a robust and adaptable deep 3D mesh watermarking Deep3DMark that leverages attention-based convolutions in watermarking tasks to embed binary messages in vertex distributions without texture assistance. Furthermore, our Deep3DMark exploits the property that simplified meshes inherit similar relations from the original ones, where the relation is the offset vector directed from one vertex to its neighbor. By doing so, our method can be trained on simplified meshes but remains effective on large size meshes (size adaptable) and unseen categories of meshes (geometry adaptable). Extensive experiments demonstrate our method remains efficient and effective even if the mesh size is 190× increased. Under mesh attacks, Deep3DMark achieves 10%∼50% higher accuracy than traditional methods, and 2× higher SNR and 8% higher accuracy than previous DNN-based methods",
    "checked": true,
    "id": "a3f3acebc894e9e1b4d74c88c0282a67c71a81fe",
    "semantic_title": "rethinking mesh watermark: towards highly robust and adaptable deep 3d mesh watermarking",
    "citation_count": 1,
    "authors": [
      "Xingyu Zhu",
      "Guanhui Ye",
      "Xiapu Luo",
      "Xuetao Wei"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28614": {
    "title": "Boosting Few-Shot Learning via Attentive Feature Regularization",
    "volume": "main",
    "abstract": "Few-shot learning (FSL) based on manifold regularization aims to improve the recognition capacity of novel objects with limited training samples by mixing two samples from different categories with a blending factor. However, this mixing operation weakens the feature representation due to the linear interpolation and the overlooking of the importance of specific channels. To solve these issues, this paper proposes attentive feature regularization (AFR) which aims to improve the feature representativeness and discriminability. In our approach, we first calculate the relations between different categories of semantic labels to pick out the related features used for regularization. Then, we design two attention-based calculations at both the instance and channel levels. These calculations enable the regularization procedure to focus on two crucial aspects: the feature complementarity through adaptive interpolation in related categories and the emphasis on specific feature channels. Finally, we combine these regularization strategies to significantly improve the classifier performance. Empirical studies on several popular FSL benchmarks demonstrate the effectiveness of AFR, which improves the recognition accuracy of novel categories without the need to retrain any feature extractor, especially in the 1-shot setting. Furthermore, the proposed AFR can seamlessly integrate into other FSL methods to improve classification performance",
    "checked": true,
    "id": "458a126146f552b0488c0a7e20841960053aff42",
    "semantic_title": "boosting few-shot learning via attentive feature regularization",
    "citation_count": 1,
    "authors": [
      "Xingyu Zhu",
      "Shuo Wang",
      "Jinda Lu",
      "Yanbin Hao",
      "Haifeng Liu",
      "Xiangnan He"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28615": {
    "title": "Memory-Efficient Prompt Tuning for Incremental Histopathology Classification",
    "volume": "main",
    "abstract": "Recent studies have made remarkable progress in histopathology classification. Based on current successes, contemporary works proposed to further upgrade the model towards a more generalizable and robust direction through incrementally learning from the sequentially delivered domains. Unlike previous parameter isolation based approaches that usually demand massive computation resources during model updating, we present a memory-efficient prompt tuning framework to cultivate model generalization potential in economical memory cost. For each incoming domain, we reuse the existing parameters of the initial classification model and attach lightweight trainable prompts into it for customized tuning. Considering the domain heterogeneity, we perform decoupled prompt tuning, where we adopt a domain-specific prompt for each domain to independently investigate its distinctive characteristics, and one domain-invariant prompt shared across all domains to continually explore the common content embedding throughout time. All domain-specific prompts will be appended to the prompt bank and isolated from further changes to prevent forgetting the distinctive features of early-seen domains. While the domain-invariant prompt will be passed on and iteratively evolve by style-augmented prompt refining to improve model generalization capability over time. In specific, we construct a graph with existing prompts and build a style-augmented graph attention network to guide the domain-invariant prompt exploring the overlapped latent embedding among all delivered domains for more domain-generic representations. We have extensively evaluated our framework with two histopathology tasks, i.e., breast cancer metastasis classification and epithelium-stroma tissue classification, where our approach yielded superior performance and memory efficiency over the competing methods",
    "checked": true,
    "id": "0b30b627ab5c5779fe713b4902741c4a841818a8",
    "semantic_title": "memory-efficient prompt tuning for incremental histopathology classification",
    "citation_count": 0,
    "authors": [
      "Yu Zhu",
      "Kang Li",
      "Lequan Yu",
      "Pheng Ann Heng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28616": {
    "title": "SPGroup3D: Superpoint Grouping Network for Indoor 3D Object Detection",
    "volume": "main",
    "abstract": "Current 3D object detection methods for indoor scenes mainly follow the voting-and-grouping strategy to generate proposals. However, most methods utilize instance-agnostic groupings, such as ball query, leading to inconsistent semantic information and inaccurate regression of the proposals. To this end, we propose a novel superpoint grouping network for indoor anchor-free one-stage 3D object detection. Specifically, we first adopt an unsupervised manner to partition raw point clouds into superpoints, areas with semantic consistency and spatial similarity. Then, we design a geometry-aware voting module that adapts to the centerness in anchor-free detection by constraining the spatial relationship between superpoints and object centers. Next, we present a superpoint-based grouping module to explore the consistent representation within proposals. This module includes a superpoint attention layer to learn feature interaction between neighboring superpoints, and a superpoint-voxel fusion layer to propagate the superpoint-level information to the voxel level. Finally, we employ effective multiple matching to capitalize on the dynamic receptive fields of proposals based on superpoints during the training. Experimental results demonstrate our method achieves state-of-the-art performance on ScanNet V2, SUN RGB-D, and S3DIS datasets in the indoor one-stage 3D object detection. Source code is available at https://github.com/zyrant/SPGroup3D",
    "checked": true,
    "id": "b61d77bf52440ff0bebf325ffd8f4f91ada02662",
    "semantic_title": "spgroup3d: superpoint grouping network for indoor 3d object detection",
    "citation_count": 0,
    "authors": [
      "Yun Zhu",
      "Le Hui",
      "Yaqi Shen",
      "Jin Xie"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28617": {
    "title": "SEIT: Structural Enhancement for Unsupervised Image Translation in Frequency Domain",
    "volume": "main",
    "abstract": "For the task of unsupervised image translation, transforming the image style while preserving its original structure remains challenging. In this paper, we propose an unsupervised image translation method with structural enhancement in frequency domain named SEIT. Specifically, a frequency dynamic adaptive (FDA) module is designed for image style transformation that can well transfer the image style while maintaining its overall structure by decoupling the image content and style in frequency domain. Moreover, a wavelet-based structure enhancement (WSE) module is proposed to improve the intermediate translation results by matching the high-frequency information, thus enriching the structural details. Furthermore, a multi-scale network architecture is designed to extract the domain-specific information using image-independent encoders for both the source and target domains. The extensive experimental results well demonstrate the effectiveness of the proposed method",
    "checked": true,
    "id": "cb8279b5c1e60111b3a70634998b7afefffdbd45",
    "semantic_title": "seit: structural enhancement for unsupervised image translation in frequency domain",
    "citation_count": 1,
    "authors": [
      "Zhifeng Zhu",
      "Yaochen Li",
      "Yifan Li",
      "Jinhuo Yang",
      "Peijun Chen",
      "Yuehu Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28618": {
    "title": "A Pre-convolved Representation for Plug-and-Play Neural Illumination Fields",
    "volume": "main",
    "abstract": "Recent advances in implicit neural representation have demonstrated the ability to recover detailed geometry and material from multi-view images. However, the use of simplified lighting models such as environment maps to represent non-distant illumination, or using a network to fit indirect light modeling without a solid basis, can lead to an undesirable decomposition between lighting and material. To address this, we propose a fully differentiable framework named Neural Illumination Fields (NeIF) that uses radiance fields as a lighting model to handle complex lighting in a physically based way. Together with integral lobe encoding for roughness-adaptive specular lobe and leveraging the pre-convolved background for accurate decomposition, the proposed method represents a significant step towards integrating physically based rendering into the NeRF representation. The experiments demonstrate the superior performance of novel-view rendering compared to previous works, and the capability to re-render objects under arbitrary NeRF-style environments opens up exciting possibilities for bridging the gap between virtual and real-world scenes",
    "checked": true,
    "id": "5a6775e12471a3db6ca87cc74553a0fcffa09332",
    "semantic_title": "a pre-convolved representation for plug-and-play neural illumination fields",
    "citation_count": 0,
    "authors": [
      "Yiyu Zhuang",
      "Qi Zhang",
      "Xuan Wang",
      "Hao Zhu",
      "Ying Feng",
      "Xiaoyu Li",
      "Ying Shan",
      "Xun Cao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28619": {
    "title": "IPRemover: A Generative Model Inversion Attack against Deep Neural Network Fingerprinting and Watermarking",
    "volume": "main",
    "abstract": "Training Deep Neural Networks (DNNs) can be expensive when data is difficult to obtain or labeling them requires significant domain expertise. Hence, it is crucial that the Intellectual Property (IP) of DNNs trained on valuable data be protected against IP infringement. DNN fingerprinting and watermarking are two lines of work in DNN IP protection. Recently proposed DNN fingerprinting techniques are able to detect IP infringement while preserving model performance by relying on the key assumption that the decision boundaries of independently trained models are intrinsically different from one another. In contrast, DNN watermarking embeds a watermark in a model and verifies IP infringement if an identical or similar watermark is extracted from a suspect model. The techniques deployed in fingerprinting and watermarking vary significantly because their underlying mechanisms are different. From an adversary's perspective, a successful IP removal attack should defeat both fingerprinting and watermarking. However, to the best of our knowledge, there is no work on such attacks in the literature yet. In this paper, we fill this gap by presenting an IP removal attack that can defeat both fingerprinting and watermarking. We consider the challenging data-free scenario whereby all data is inverted from the victim model. Under this setting, a stolen model only depends on the victim model. Experimental results demonstrate the success of our attack in defeating state-of-the-art DNN fingerprinting and watermarking techniques. This work reveals a novel attack surface that exploits generative model inversion attacks to bypass DNN IP defenses. This threat must be addressed by future defenses for reliable IP protection",
    "checked": true,
    "id": "3101c96bb6464120716af4e005522423e7c8cd66",
    "semantic_title": "ipremover: a generative model inversion attack against deep neural network fingerprinting and watermarking",
    "citation_count": 0,
    "authors": [
      "Wei Zong",
      "Yang-Wai Chow",
      "Willy Susilo",
      "Joonsang  Baek",
      "Jongkil Kim",
      "Seyit Camtepe"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28620": {
    "title": "DiffBEV: Conditional Diffusion Model for Bird's Eye View Perception",
    "volume": "main",
    "abstract": "BEV perception is of great importance in the field of autonomous driving, serving as the cornerstone of planning, controlling, and motion prediction. The quality of the BEV feature highly affects the performance of BEV perception. However, taking the noises in camera parameters and LiDAR scans into consideration, we usually obtain BEV representation with harmful noises. Diffusion models naturally have the ability to denoise noisy samples to the ideal data, which motivates us to utilize the diffusion model to get a better BEV representation. In this work, we propose an end-to-end framework, named DiffBEV, to exploit the potential of diffusion model to generate a more comprehensive BEV representation. To the best of our knowledge, we are the first to apply diffusion model to BEV perception. In practice, we design three types of conditions to guide the training of the diffusion model which denoises the coarse samples and refines the semantic feature in a progressive way. What's more, a cross-attention module is leveraged to fuse the context of BEV feature and the semantic content of conditional diffusion model. DiffBEV achieves a 25.9% mIoU on the nuScenes dataset, which is 6.2% higher than the best-performing existing approach. Quantitative and qualitative results on multiple benchmarks demonstrate the effectiveness of DiffBEV in BEV semantic segmentation and 3D object detection tasks",
    "checked": true,
    "id": "dc378359d3f3a5348bf799f1570e4a9dd7eb8bfd",
    "semantic_title": "diffbev: conditional diffusion model for bird's eye view perception",
    "citation_count": 10,
    "authors": [
      "Jiayu Zou",
      "Kun Tian",
      "Zheng Zhu",
      "Yun Ye",
      "Xingang Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28621": {
    "title": "Cross-Covariate Gait Recognition: A Benchmark",
    "volume": "main",
    "abstract": "Gait datasets are essential for gait research. However, this paper observes that present benchmarks, whether conventional constrained or emerging real-world datasets, fall short regarding covariate diversity. To bridge this gap, we undertake an arduous 20-month effort to collect a cross-covariate gait recognition (CCGR) dataset. The CCGR dataset has 970 subjects and about 1.6 million sequences; almost every subject has 33 views and 53 different covariates. Compared to existing datasets, CCGR has both population and individual-level diversity. In addition, the views and covariates are well labeled, enabling the analysis of the effects of different factors. CCGR provides multiple types of gait data, including RGB, parsing, silhouette, and pose, offering researchers a comprehensive resource for exploration. In order to delve deeper into addressing cross-covariate gait recognition, we propose parsing-based gait recognition (ParsingGait) by utilizing the newly proposed parsing data. We have conducted extensive experiments. Our main results show: 1) Cross-covariate emerges as a pivotal challenge for practical applications of gait recognition. 2) ParsingGait demonstrates remarkable potential for further advancement. 3) Alarmingly, existing SOTA methods achieve less than 43% accuracy on the CCGR, highlighting the urgency of exploring cross-covariate gait recognition. Link: https://github.com/ShinanZou/CCGR",
    "checked": true,
    "id": "450d8d35d3508af3c92b339064292df6c5312f04",
    "semantic_title": "cross-covariate gait recognition: a benchmark",
    "citation_count": 1,
    "authors": [
      "Shinan Zou",
      "Chao Fan",
      "Jianbo Xiong",
      "Chuanfu Shen",
      "Shiqi Yu",
      "Jin Tang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28622": {
    "title": "Towards Efficient Diffusion-Based Image Editing with Instant Attention Masks",
    "volume": "main",
    "abstract": "Diffusion-based Image Editing (DIE) is an emerging research hot-spot, which often applies a semantic mask to control the target area for diffusion-based editing. However, most existing solutions obtain these masks via manual operations or off-line processing, greatly reducing their efficiency. In this paper, we propose a novel and efficient image editing method for Text-to-Image (T2I) diffusion models, termed Instant Diffusion Editing (InstDiffEdit). In particular, InstDiffEdit aims to employ the cross-modal attention ability of existing diffusion models to achieve instant mask guidance during the diffusion steps. To reduce the noise of attention maps and realize the full automatics, we equip InstDiffEdit with a training-free refinement scheme to adaptively aggregate the attention distributions for the automatic yet accurate mask generation. Meanwhile, to supplement the existing evaluations of DIE, we propose a new benchmark called Editing-Mask to examine the mask accuracy and local editing ability of existing methods. To validate InstDiffEdit, we also conduct extensive experiments on ImageNet and Imagen, and compare it with a bunch of the SOTA methods. The experimental results show that InstDiffEdit not only outperforms the SOTA methods in both image quality and editing results, but also has a much faster inference speed, i.e., +5 to +6 times. Our code available at https://anonymous.4open.science/r/InstDiffEdit-C306",
    "checked": true,
    "id": "c50facdc9daa8234fc9fd37f588ed2db97ce0995",
    "semantic_title": "towards efficient diffusion-based image editing with instant attention masks",
    "citation_count": 2,
    "authors": [
      "Siyu Zou",
      "Jiji Tang",
      "Yiyi Zhou",
      "Jing He",
      "Chaoyi Zhao",
      "Rongsheng Zhang",
      "Zhipeng Hu",
      "Xiaoshuai Sun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28623": {
    "title": "VQCNIR: Clearer Night Image Restoration with Vector-Quantized Codebook",
    "volume": "main",
    "abstract": "Night photography often struggles with challenges like low light and blurring, stemming from dark environments and prolonged exposures. Current methods either disregard priors and directly fitting end-to-end networks, leading to inconsistent illumination, or rely on unreliable handcrafted priors to constrain the network, thereby bringing the greater error to the final result. We believe in the strength of data-driven high-quality priors and strive to offer a reliable and consistent prior, circumventing the restrictions of manual priors. In this paper, we propose Clearer Night Image Restoration with Vector-Quantized Codebook (VQCNIR) to achieve remarkable and consistent restoration outcomes on real-world and synthetic benchmarks. To ensure the faithful restoration of details and illumination, we propose the incorporation of two essential modules: the Adaptive Illumination Enhancement Module (AIEM) and the Deformable Bi-directional Cross-Attention (DBCA) module. The AIEM leverages the inter-channel correlation of features to dynamically maintain illumination consistency between degraded features and high-quality codebook features. Meanwhile, the DBCA module effectively integrates texture and structural information through bi-directional cross-attention and deformable convolution, resulting in enhanced fine-grained detail and structural fidelity across parallel decoders. Extensive experiments validate the remarkable benefits of VQCNIR in enhancing image quality under low-light conditions, showcasing its state-of-the-art performance on both synthetic and real-world datasets. The code is available at https://github.com/AlexZou14/VQCNIR",
    "checked": true,
    "id": "9b41a463b12cf1126806ee65ee95b30ed3d01b0d",
    "semantic_title": "vqcnir: clearer night image restoration with vector-quantized codebook",
    "citation_count": 1,
    "authors": [
      "Wenbin Zou",
      "Hongxia Gao",
      "Tian Ye",
      "Liang Chen",
      "Weipeng Yang",
      "Shasha Huang",
      "Hongsheng Chen",
      "Sixiang Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28624": {
    "title": "Enhancing Neural Radiance Fields with Adaptive Multi-Exposure Fusion: A Bilevel Optimization Approach for Novel View Synthesis",
    "volume": "main",
    "abstract": "Neural Radiance Fields (NeRF) have made significant strides in the modeling and rendering of 3D scenes. However, due to the complexity of luminance information, existing NeRF methods often struggle to produce satisfactory renderings when dealing with high and low exposure images. To address this issue, we propose an innovative approach capable of effectively modeling and rendering images under multiple exposure conditions. Our method adaptively learns the characteristics of images under different exposure conditions through an unsupervised evaluator-simulator structure for HDR (High Dynamic Range) fusion. This approach enhances NeRF's comprehension and handling of light variations, leading to the generation of images with appropriate brightness. Simultaneously, we present a bilevel optimization method tailored for novel view synthesis, aiming to harmonize the luminance information of input images while preserving their structural and content consistency. This approach facilitates the concurrent optimization of multi-exposure correction and novel view synthesis, in an unsupervised manner. Through comprehensive experiments conducted on the LOM and LOL datasets, our approach surpasses existing methods, markedly enhancing the task of novel view synthesis for multi-exposure environments and attaining state-of-the-art results. The source code can be found at https://github.com/Archer-204/AME-NeRF",
    "checked": true,
    "id": "6724e07e3a5837c5cc9add00d842c09de7857dc0",
    "semantic_title": "enhancing neural radiance fields with adaptive multi-exposure fusion: a bilevel optimization approach for novel view synthesis",
    "citation_count": 0,
    "authors": [
      "Yang Zou",
      "Xingyuan Li",
      "Zhiying Jiang",
      "Jinyuan Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28625": {
    "title": "Improved MLP Point Cloud Processing with High-Dimensional Positional Encoding",
    "volume": "main",
    "abstract": "Multi-Layer Perceptron (MLP) models are the bedrock of contemporary point cloud processing. However, their complex network architectures obscure the source of their strength. We first develop an \"abstraction and refinement\" (ABS-REF) view for the neural modeling of point clouds. This view elucidates that whereas the early models focused on the ABS stage, the more recent techniques devise sophisticated REF stages to attain performance advantage in point cloud processing. We then borrow the concept of \"positional encoding\" from transformer literature, and propose a High-dimensional Positional Encoding (HPE) module, which can be readily deployed to MLP based architectures. We leverage our module to develop a suite of HPENet, which are MLP networks that follow ABS-REF paradigm, albeit with a sophisticated HPE based REF stage. The developed technique is extensively evaluated for 3D object classification, object part segmentation, semantic segmentation and object detection. We establish new state-of-the-art results of 87.6 mAcc on ScanObjectNN for object classification, and 85.5 class mIoU on ShapeNetPart for object part segmentation, and 72.7 and 78.7 mIoU on Area-5 and 6-fold experiments with S3DIS for semantic segmentation. The source code for this work is available at https://github.com/zouyanmei/HPENet",
    "checked": true,
    "id": "9c3c71ef2162302a5db3f24b4e3b1eaa90b504c9",
    "semantic_title": "improved mlp point cloud processing with high-dimensional positional encoding",
    "citation_count": 0,
    "authors": [
      "Yanmei Zou",
      "Hongshan Yu",
      "Zhengeng Yang",
      "Zechuan Li",
      "Naveed Akhtar"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28626": {
    "title": "Sparse3D: Distilling Multiview-Consistent Diffusion for Object Reconstruction from Sparse Views",
    "volume": "main",
    "abstract": "Reconstructing 3D objects from extremely sparse views is a long-standing and challenging problem. While recent techniques employ image diffusion models for generating plausible images at novel viewpoints or for distilling pre-trained diffusion priors into 3D representations using score distillation sampling (SDS), these methods often struggle to simultaneously achieve high-quality, consistent, and detailed results for both novel-view synthesis (NVS) and geometry. In this work, we present Sparse3D, a novel 3D reconstruction method tailored for sparse view inputs. Our approach distills robust priors from a multiview-consistent diffusion model to refine a neural radiance field. Specifically, we employ a controller that harnesses epipolar features from input views, guiding a pre-trained diffusion model, such as Stable Diffusion, to produce novel-view images that maintain 3D consistency with the input. By tapping into 2D priors from powerful image diffusion models, our integrated model consistently delivers high-quality results, even when faced with open-world objects. To address the blurriness introduced by conventional SDS, we introduce the category-score distillation sampling (C-SDS) to enhance detail. We conduct experiments on CO3DV2 which is a multi-view dataset of real-world objects. Both quantitative and qualitative evaluations demonstrate that our approach outperforms previous state-of-the-art works on the metrics regarding NVS and geometry reconstruction",
    "checked": true,
    "id": "8aa2bddbea68bcdcf08f2f0ffb3ec829e27bddd8",
    "semantic_title": "sparse3d: distilling multiview-consistent diffusion for object reconstruction from sparse views",
    "citation_count": 16,
    "authors": [
      "Zixin Zou",
      "Weihao Cheng",
      "Yan-Pei Cao",
      "Shi-Sheng Huang",
      "Ying Shan",
      "Song-Hai  Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28627": {
    "title": "CEDFlow: Latent Contour Enhancement for Dark Optical Flow Estimation",
    "volume": "main",
    "abstract": "Accurately computing optical flow in low-contrast and noisy dark images is challenging, especially when contour information is degraded or difficult to extract. This paper proposes CEDFlow, a latent space contour enhancement for estimating optical flow in dark environments. By leveraging spatial frequency feature decomposition, CEDFlow effectively encodes local and global motion features. Importantly, we introduce the 2nd-order Gaussian difference operation to select salient contour features in the latent space precisely. It is specifically designed for large-scale contour components essential in dark optical flow estimation. Experimental results on the FCDN and VBOF datasets demonstrate that CEDFlow outperforms state-of-the-art methods in terms of the EPE index and produces more accurate and robust flow estimation. Our code is available at: https://github.com/xautstuzfy",
    "checked": true,
    "id": "2ced19bfec23d6a0f2ba799893793bde14f70e9e",
    "semantic_title": "cedflow: latent contour enhancement for dark optical flow estimation",
    "citation_count": 0,
    "authors": [
      "Fengyuan Zuo",
      "Zhaolin Xiao",
      "Haiyan Jin",
      "Haonan Su"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28628": {
    "title": "Parameterization of (Partial) Maximum Satisfiability above Matching in a Variable-Clause Graph",
    "volume": "main",
    "abstract": "In the paper, we study the Maximum Satisfiability and the Partial Maximum Satisfiability problems. Using Gallai–Edmonds decomposition, we significantly improve the upper bound for the Maximum Satisfiability problem parameterized above maximum matching in the variable-clause graph. Our algorithm operates with a runtime of O*(2.83^k'), a substantial improvement compared to the previous approach requiring O*(4^k' ), where k' denotes the relevant parameter. Moreover, this result immediately implies O*(1.14977^m) and O*(1.27895^m) time algorithms for the (n, 3)-MaxSAT and (n, 4)-MaxSAT where m is the overall number of clauses. These upper bounds improve prior-known upper bounds equal to O*(1.1554^m) and O*(1.2872^m). We also adapt the algorithm so that it can handle instances of Partial Maximum Satisfiability without losing performance in some cases. Note that this is somewhat surprising, as the existence of even one hard clause can significantly increase the hardness of a problem",
    "checked": true,
    "id": "068fbda3a44012d80df847d3c39636e62fd399a8",
    "semantic_title": "parameterization of (partial) maximum satisfiability above matching in a variable-clause graph",
    "citation_count": 0,
    "authors": [
      "Vasily Alferov",
      "Ivan Bliznets",
      "Kirill Brilliantov"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28629": {
    "title": "Approximation Scheme for Weighted Metric Clustering via Sherali-Adams",
    "volume": "main",
    "abstract": "Motivated by applications to classification problems on metric data, we study Weighted Metric Clustering problem: given a metric d over n points and a k x k symmetric matrix A with non-negative entries, the goal is to find a k-partition of these points into clusters C1,...,Ck, while minimizing the sum of A[i,j] * d(u,v) over all pairs of clusters Ci and Cj and all pairs of points u from Ci and v from Cj. Specific choices of A lead to Weighted Metric Clustering capturing well-studied graph partitioning problems in metric spaces, such as Min-Uncut, Min-k-Sum, Min-k-Cut, and more. Our main result is that Weighted Metric Clustering admits a polynomial-time approximation scheme (PTAS). Our algorithm handles all the above problems using the Sherali-Adams linear programming relaxation. This subsumes several prior works, unifies many of the techniques for various metric clustering objectives, and yields a PTAS for several new problems, including metric clustering on manifolds and a new family of hierarchical clustering objectives. Our experiments on the hierarchical clustering objective show that it better captures the ground-truth structural information compared to the popular Dasgupta's objective",
    "checked": true,
    "id": "50e87897258c94388800b669c5092272f05e2641",
    "semantic_title": "approximation scheme for weighted metric clustering via sherali-adams",
    "citation_count": 0,
    "authors": [
      "Dmitrii Avdiukhin",
      "Vaggos Chatziafratis",
      "Konstantin Makarychev",
      "Grigory Yaroslavtsev"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28630": {
    "title": "Neural Time-Reversed Generalized Riccati Equation",
    "volume": "main",
    "abstract": "Optimal control deals with optimization problems in which variables steer a dynamical system, and its outcome contributes to the objective function. Two classical approaches to solving these problems are Dynamic Programming and the Pontryagin Maximum Principle. In both approaches, Hamiltonian equations offer an interpretation of optimality through auxiliary variables known as costates. However, Hamiltonian equations are rarely used due to their reliance on forward-backward algorithms across the entire temporal domain. This paper introduces a novel neural-based approach to optimal control. Neural networks are employed not only for implementing state dynamics but also for estimating costate variables. The parameters of the latter network are determined at each time step using a newly introduced local policy referred to as the time-reversed generalized Riccati equation. This policy is inspired by a result discussed in the Linear Quadratic (LQ) problem, which we conjecture stabilizes state dynamics. We support this conjecture by discussing experimental results from a range of optimal control case studies",
    "checked": true,
    "id": "ee9d48ae6549a4014a37c1c13dcb59bbcc0db1da",
    "semantic_title": "neural time-reversed generalized riccati equation",
    "citation_count": 0,
    "authors": [
      "Alessandro Betti",
      "Michele Casoni",
      "Marco Gori",
      "Simone Marullo",
      "Stefano Melacci",
      "Matteo Tiezzi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28631": {
    "title": "Runtime vs. Extracted Proof Size: An Exponential Gap for CDCL on QBFs",
    "volume": "main",
    "abstract": "Conflict-driven clause learning (CDCL) is the dominating algorithmic paradigm for SAT solving and hugely successful in practice. In its lifted version QCDCL, it is one of the main approaches for solving quantified Boolean formulas (QBF). In both SAT and QBF, proofs can be efficiently extracted from runs of (Q)CDCL solvers. While for CDCL, it is known that the proof size in the underlying proof system propositional resolution matches the CDCL runtime up to a polynomial factor, we show that in QBF there is an exponential gap between QCDCL runtime and the size of the extracted proofs in QBF resolution systems. We demonstrate that this is not just a gap between QCDCL runtime and the size of any QBF resolution proof, but even the extracted proofs are exponentially smaller for some instances. Hence searching for a small proof via QCDCL (even with non-deterministic decision policies) will provably incur an exponential overhead for some instances",
    "checked": true,
    "id": "b352fe651cf9971eca51cd9ab6949ec00fabe47f",
    "semantic_title": "runtime vs. extracted proof size: an exponential gap for cdcl on qbfs",
    "citation_count": 0,
    "authors": [
      "Olaf Beyersdorff",
      "Benjamin Böhm",
      "Meena Mahajan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28632": {
    "title": "Testing Self-Reducible Samplers",
    "volume": "main",
    "abstract": "Samplers are the backbone of the implementations of any randomized algorithm. Unfortunately, obtaining an efficient algorithm to test the correctness of samplers is very hard to find. Recently, in a series of works, testers like Barbarik, Teq, Flash for testing of some particular kinds of samplers, like CNF-samplers and Horn-samplers, were obtained. However, their techniques have a significant limitation because one can not expect to use their methods to test for other samplers, such as perfect matching samplers or samplers for sampling linear extensions in posets. In this paper, we present a new testing algorithm that works for such samplers and can estimate the distance of a new sampler from a known sampler (say, the uniform sampler). Testing the identity of distributions is the heart of testing the correctness of samplers. This paper's main technical contribution is developing a new distance estimation algorithm for distributions over high-dimensional cubes using the recently proposed subcube conditioning sampling model. Given subcube conditioning access to an unknown distribution P, and a known distribution Q defined over an n-dimensional Boolean hypercube, our algorithm CubeProbeEst estimates the variation distance between P and Q within additive error using subcube conditional samples from P. Following the testing-via-learning paradigm, we also get a tester that distinguishes between the cases when P and Q are close or far in variation distance with high probability using subcube conditional samples. This estimation algorithm CubeProbeEst in the subcube conditioning sampling model helps us to design the first tester for self-reducible samplers. The correctness of the tester is formally proved. Moreover, we implement CubeProbeEst to test the quality of three samplers for sampling linear extensions in posets",
    "checked": true,
    "id": "d456d1d873638d1cda6e4ea1ed74fe77f2e5189d",
    "semantic_title": "testing self-reducible samplers",
    "citation_count": 0,
    "authors": [
      "Rishiraj Bhattacharyya",
      "Sourav Chakraborty",
      "Yash Pote",
      "Uddalok Sarkar",
      "Sayantan Sen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28633": {
    "title": "Using Symmetries to Lift Satisfiability Checking",
    "volume": "main",
    "abstract": "We analyze how symmetries can be used to compress structures (also known as interpretations) onto a smaller domain without loss of information. This analysis suggests the possibility to solve satisfiability problems in the compressed domain for better performance. Thus, we propose a 2-step novel method: (i) the sentence to be satisfied is automatically translated into an equisatisfiable sentence over a ``lifted'' vocabulary that allows domain compression; (ii) satisfiability of the lifted sentence is checked by growing the (initially unknown) compressed domain until a satisfying structure is found. The key issue is to ensure that this satisfying structure can always be expanded into an uncompressed structure that satisfies the original sentence to be satisfied. We present an adequate translation for sentences in typed first-order logic extended with aggregates. Our experimental evaluation shows large speedups for generative configuration problems. The method also has applications in the verification of software operating on complex data structures. Our results justify further research in automatic translation of sentences for symmetry reduction",
    "checked": true,
    "id": "54f938f63d17c9aa19b60cfafbdd3b019e028cd6",
    "semantic_title": "using symmetries to lift satisfiability checking",
    "citation_count": 0,
    "authors": [
      "Pierre Carbonnelle",
      "Gottfried Schenner",
      "Maurice Bruynooghe",
      "Bart Bogaerts",
      "Marc Denecker"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28634": {
    "title": "Robust Beamforming for Downlink Multi-Cell Systems: A Bilevel Optimization Perspective",
    "volume": "main",
    "abstract": "Utilization of inter-base station cooperation for information processing has shown great potential in enhancing the overall quality of communication services (QoS) in wireless communication networks. Nevertheless, such cooperations require the knowledge of channel state information (CSI) at base stations (BSs), which is assumed to be perfectly known. However, CSI errors are inevitable in practice which necessitates beamforming technique that can achieve robust performance in the presence of channel estimation errors. Existing approaches relax the robust beamforming design problems into semidefinite programming (SDP), which can only achieve a solution that is far from being optimal. To this end, this paper views robust beamforming design problems from a bilevel optimization perspective. In particular, we focus on maximizing the worst-case weighted sum-rate (WSR) in the downlink multi-cell multi-user multiple-input single-output (MISO) system considering bounded CSI errors. We first reformulate this problem into a bilevel optimization problem and then develop an efficient algorithm based on the cutting plane method. A distributed optimization algorithm has also been developed to facilitate the parallel processing in practical settings. Numerical results are provided to confirm the effectiveness of the proposed algorithm in terms of performance and complexity, particularly in the presence of CSI uncertainties",
    "checked": true,
    "id": "43d9eea33ae6e18e004f28a6c026ebf29523c86c",
    "semantic_title": "robust beamforming for downlink multi-cell systems: a bilevel optimization perspective",
    "citation_count": 0,
    "authors": [
      "Xingdi Chen",
      "Yu Xiong",
      "Kai Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28635": {
    "title": "Hardness of Random Reordered Encodings of Parity for Resolution and CDCL",
    "volume": "main",
    "abstract": "Parity reasoning is challenging for Conflict-Driven Clause Learning (CDCL) SAT solvers. This has been observed even for simple formulas encoding two contradictory parity constraints with different variable orders (Chew and Heule 2020). We provide an analytical explanation for their hardness by showing that they require exponential resolution refutations with high probability when the variable order is chosen at random. We obtain this result by proving that these formulas, which are known to be Tseitin formulas, have Tseitin graphs of linear treewidth with high probability. Since such Tseitin formulas require exponential resolution refutations, our result follows. We generalize this argument to a new class of formulas that capture a basic form of parity reasoning involving a sum of two random parity constraints with random orders. Even when the variable order for the sum is chosen favorably, these formulas remain hard for resolution. In contrast, we prove that they have short DRAT refutations. We show experimentally that the running time of CDCL SAT solvers on both classes of formulas grows exponentially with their treewidth",
    "checked": true,
    "id": "f43f1a00352c4b40ef11aa0d33b7e8ed05ec2095",
    "semantic_title": "hardness of random reordered encodings of parity for resolution and cdcl",
    "citation_count": 0,
    "authors": [
      "Leroy Chew",
      "Alexis de Colnet",
      "Friedrich Slivovsky",
      "Stefan Szeider"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28636": {
    "title": "Percentile Risk-Constrained Budget Pacing for Guaranteed Display Advertising in Online Optimization",
    "volume": "main",
    "abstract": "Guaranteed display (GD) advertising is a critical component of advertising since it provides publishers with stable revenue and enables advertisers to target specific audiences with guaranteed impressions. However, smooth pacing control for online ad delivery presents a challenge due to significant budget disparities, user arrival distribution drift, and dynamic change between supply and demand. This paper presents robust risk-constrained pacing (RCPacing) that utilizes Lagrangian dual multipliers to fine-tune probabilistic throttling through monotonic mapping functions within the percentile space of impression performance distribution. RCPacing combines distribution drift resilience and compatibility with guaranteed allocation mechanism, enabling us to provide near-optimal online services. We also show that RCPacing achieves O(sqrt(T)) dynamic regret where T is the length of the horizon. RCPacing's effectiveness is validated through offline evaluations and online A/B testing conducted on Taobao brand advertising platform",
    "checked": true,
    "id": "fa2f03e350b66acf3dcdc206dd2d1d1a1dc84a50",
    "semantic_title": "percentile risk-constrained budget pacing for guaranteed display advertising in online optimization",
    "citation_count": 0,
    "authors": [
      "Liang Dai",
      "Kejie Lyu",
      "Chengcheng Zhang",
      "Guangming Zhao",
      "Zhonglin Zu",
      "Liang Wang",
      "Bo Zheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28637": {
    "title": "Unifying Decision and Function Queries in Stochastic Boolean Satisfiability",
    "volume": "main",
    "abstract": "Stochastic Boolean satisfiability (SSAT) is a natural formalism for optimization under uncertainty. Its decision version implicitly imposes a final threshold quantification on an SSAT formula. However, the single threshold quantification restricts the expressive power of SSAT. In this work, we enrich SSAT with an additional threshold quantifier, resulting in a new formalism SSAT(θ). The increased expressiveness allows SSAT(θ), which remains in the PSPACE complexity class, to subsume and encode the languages in the counting hierarchy. An SSAT(θ) solver, ClauSSat(θ), is developed. Experiments show the applicability of the solver in uniquely solving complex SSAT(θ) instances of parameter synthesis and SSAT extension",
    "checked": true,
    "id": "f12a49e5843cc38bd376761a71ee4c362a9739a1",
    "semantic_title": "unifying decision and function queries in stochastic boolean satisfiability",
    "citation_count": 0,
    "authors": [
      "Yu-Wei Fan",
      "Jie-Hong R. Jiang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28638": {
    "title": "Parallel Empirical Evaluations: Resilience despite Concurrency",
    "volume": "main",
    "abstract": "Computational evaluations are crucial in modern problem-solving when we surpass theoretical algorithms or bounds. These experiments frequently take much work, and the sheer amount of needed resources makes it impossible to execute them on a single personal computer or laptop. Cluster schedulers allow for automatizing these tasks and scale to many computers. But, when we evaluate implementations of combinatorial algorithms, we depend on stable runtime results. Common approaches either limit parallelism or suffer from unstable runtime measurements due to interference among jobs on modern hardware. The former is inefficient and not sustainable. The latter results in unreplicable experiments. In this work, we address this issue and offer an acceptable balance between efficiency, software, hardware complexity, reliability, and replicability. We investigate effects towards replicability stability and illustrate how to efficiently use widely employed cluster resources for parallel evaluations. Furthermore, we present solutions which mitigate issues that emerge from the concurrent execution of benchmark jobs. Our experimental evaluation shows that – despite parallel execution – our approach reduces the runtime instability on the majority of instances to one second",
    "checked": true,
    "id": "1666d9cb4817d04c71f25a6c09bddc579e301a0c",
    "semantic_title": "parallel empirical evaluations: resilience despite concurrency",
    "citation_count": 0,
    "authors": [
      "Johannes K. Fichte",
      "Tobias Geibinger",
      "Markus Hecher",
      "Matthias Schlögel"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28639": {
    "title": "Locally Rainbow Paths",
    "volume": "main",
    "abstract": "We introduce the algorithmic problem of finding a locally rainbow path of length l connecting two distinguished vertices s and t in a vertex-colored directed graph. Herein, a path is locally rainbow if between any two visits of equally colored vertices, the path traverses consecutively at leaset r differently colored vertices. This problem generalizes the well-known problem of finding a rainbow path. It finds natural applications whenever there are different types of resources that must be protected from overuse, such as crop sequence optimization or production process scheduling. We show that the problem is computationally intractable even if r=2 or if one looks for a locally rainbow among the shortest paths. On the positive side, if one looks for a path that takes only a short detour (i.e., it is slightly longer than the shortest path) and if r is small, the problem can be solved efficiently. Indeed, the running time of the respective algorithm is near-optimal unless the ETH fails",
    "checked": true,
    "id": "2d78244cc8f34e793fabbbd1a63585032a1584ab",
    "semantic_title": "locally rainbow paths",
    "citation_count": 0,
    "authors": [
      "Till Fluschnik",
      "Leon Kellerhals",
      "Malte Renken"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28640": {
    "title": "Approximate Integer Solution Counts over Linear Arithmetic Constraints",
    "volume": "main",
    "abstract": "Counting integer solutions of linear constraints has found interesting applications in various fields. It is equivalent to the problem of counting lattice points inside a polytope. However, state-of-the-art algorithms for this problem become too slow for even a modest number of variables. In this paper, we propose a new framework to approximate the lattice counts inside a polytope with a new random-walk sampling method. The counts computed by our approach has been proved approximately bounded by a (epsilon, delta)-bound. Experiments on extensive benchmarks show that our algorithm could solve polytopes with dozens of dimensions, which significantly outperforms state-of-the-art counters",
    "checked": true,
    "id": "292d3555c8d6f5f4905bd341a87a2d6b84e42f2b",
    "semantic_title": "approximate integer solution counts over linear arithmetic constraints",
    "citation_count": 0,
    "authors": [
      "Cunjing Ge"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28641": {
    "title": "Composing Biases by Using CP to Decompose Minimal Functional Dependencies for Acquiring Complex Formulae",
    "volume": "main",
    "abstract": "Given a table with a minimal set of input columns that functionally determines an output column, we introduce a method that tries to gradually decompose the corresponding minimal functional dependency (mfd) to acquire a formula expressing the output column in terms of the input columns. A first key element of the method is to create sub-problems that are easier to solve than the original formula acquisition problem, either because it learns formulae with fewer inputs parameters, or as it focuses on formulae of a particular class, such as Boolean formulae; as a result, the acquired formulae can mix different learning biases such as polynomials, conditionals or Boolean expressions. A second key feature of the method is that it can be applied recursively to find formulae that combine polynomial, conditional or Boolean sub-terms in a nested manner. The method was tested on data for eight families of combinatorial objects; new conjectures were found that were previously unattainable. The method often creates conjectures that combine several formulae into one with a limited number of automatically found Boolean terms",
    "checked": true,
    "id": "791ce30234d74956641449f84fceb33216aabc1d",
    "semantic_title": "composing biases by using cp to decompose minimal functional dependencies for acquiring complex formulae",
    "citation_count": 1,
    "authors": [
      "Ramiz Gindullin",
      "Nicolas Beldiceanu",
      "Jovial Cheukam-Ngouonou",
      "Rémi Douence",
      "Claude-Guy Quimper"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28642": {
    "title": "End-to-End Verification for Subgraph Solving",
    "volume": "main",
    "abstract": "Modern subgraph-finding algorithm implementations consist of thousands of lines of highly optimized code, and this complexity raises questions about their trustworthiness. Recently, some state-of-the-art subgraph solvers have been enhanced to output machine-verifiable proofs that their results are correct. While this significantly improves reliability, it is not a fully satisfactory solution, since end-users have to trust both the proof checking algorithms and the translation of the high-level graph problem into a low-level 0-1 integer linear program (ILP) used for the proofs. In this work, we present the first formally verified toolchain capable of full end-to-end verification for subgraph solving, which closes both of these trust gaps. We have built encoder frontends for various graph problems together with a 0-1 ILP (a.k.a. pseudo-Boolean) proof checker, all implemented and formally verified in the CakeML ecosystem. This toolchain is flexible and extensible, and we use it to build verified proof checkers for both decision and optimization graph problems, namely, subgraph isomorphism, maximum clique, and maximum common (connected) induced subgraph. Our experimental evaluation shows that end-to-end formal verification is now feasible for a wide range of hard graph problems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stephan Gocht",
      "Ciaran McCreesh",
      "Magnus O. Myreen",
      "Jakob Nordström",
      "Andy Oertel",
      "Yong Kiam Tan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28643": {
    "title": "SAT-Based Techniques for Lexicographically Smallest Finite Models",
    "volume": "main",
    "abstract": "This paper proposes SAT-based techniques to calculate a specific normal form of a given finite mathematical structure (model). The normal form is obtained by permuting the domain elements so that the representation of the structure is lexicographically smallest possible. Such a normal form is of interest to mathematicians as it enables easy cataloging of algebraic structures. In particular, two structures are isomorphic precisely when their normal forms are the same. This form is also natural to inspect as mathematicians have been using it routinely for many decades. We develop a novel approach where a SAT solver is used in a black-box fashion to compute the smallest representative. The approach constructs the representative gradually and searches the space of possible isomorphisms, requiring a small number of variables. However, the approach may lead to a large number of SAT calls and therefore we devise propagation techniques to reduce this number. The paper focuses on finite structures with a single binary operation (encompassing groups, semigroups, etc.). However, the approach is generalizable to arbitrary finite structures. We provide an implementation of the proposed algorithm and evaluate it on a variety of algebraic structures",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mikoláš Janota",
      "Choiwah Chow",
      "João Araújo",
      "Michael Codish",
      "Petr Vojtěchovský"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28644": {
    "title": "Theoretical and Empirical Analysis of Cost-Function Merging for Implicit Hitting Set WCSP Solving",
    "volume": "main",
    "abstract": "The Implicit Hitting Set (HS) approach has shown very effective for MaxSAT solving. However, only preliminary promising results have been obtained for the very similar Weighted CSP framework. In this paper we contribute towards both a better theoretical understanding of the HS approach and a more effective HS-based solvers for WCSP. First, we bound the minimum number of iterations of HS thanks to what we call distinguished cores. Then, we show a source of inefficiency by introducing two simple problems where HS is unfeasible. Next, we propose two reformulation methods that merge cost-functions to overcome the problem. We provide a theoretical analysis that quantifies the magnitude of the improvement of each method with respect to the number of iterations of the algorithm. In particular, we show that the reformulations can bring an exponential number of iterations down to a constant number in our working examples. Finally, we complement our theoretical analysis with two sets of experiments. First, we show that our results are aligned with real executions. Second, and most importantly, we conduct experiments on typical benchmark problems and show that cost-function merging may be heuristically applied and it may accelerate HS algorithms by several orders of magnitude. In some cases, it even outperforms state-of-the-art solvers",
    "checked": true,
    "id": "05b885051d07107051723090b149e001bfa37026",
    "semantic_title": "theoretical and empirical analysis of cost-function merging for implicit hitting set wcsp solving",
    "citation_count": 0,
    "authors": [
      "Javier Larrosa",
      "Conrado Martínez",
      "Emma Rollon"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28645": {
    "title": "Automatic Core-Guided Reformulation via Constraint Explanation and Condition Learning",
    "volume": "main",
    "abstract": "SAT and propagation solvers often underperform for optimisation models whose objective sums many single-variable terms. MaxSAT solvers avoid this by detecting and exploiting cores: subsets of these terms that cannot collectively take their lower bounds. Previous work has shown manual analysis of cores can help define model reformulations likely to speed up solving for many model instances. This paper presents a method to automate this process. For each selected core the method identifies the instance constraints that caused it; infers the model constraints and parameters that explain how these instance constraints were formed; and learns the conditions that made those model constraint instances generate cores, while others did not. It then uses this information to reformulate the objective. The empirical evaluation shows this method can produce useful reformulations. Importantly, the method can be useful in many other situations that require explaining a set of constraints",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kevin Leo",
      "Grame Gange",
      "Maria Garcia de la Banda",
      "Mark Wallace"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28646": {
    "title": "Learning to Pivot as a Smart Expert",
    "volume": "main",
    "abstract": "Linear programming has been practically solved mainly by simplex and interior point methods. Compared with the weakly polynomial complexity obtained by the interior point methods, the existence of strongly polynomial bounds for the length of the pivot path generated by the simplex methods remains a mystery. In this paper, we propose two novel pivot experts that leverage both global and local information of the linear programming instances for the primal simplex method and show their excellent performance numerically. The experts can be regarded as a benchmark to evaluate the performance of classical pivot rules, although they are hard to directly implement. To tackle this challenge, we employ a graph convolutional neural network model, trained via imitation learning, to mimic the behavior of the pivot expert. Our pivot rule, learned empirically, displays a significant advantage over conventional methods in various linear programming problems, as demonstrated through a series of rigorous experiments",
    "checked": true,
    "id": "24841f22baf64db20a9b1c40105ea13fdf9bb8fc",
    "semantic_title": "learning to pivot as a smart expert",
    "citation_count": 2,
    "authors": [
      "Tianhao Liu",
      "Shanwen Pu",
      "Dongdong Ge",
      "Yinyu Ye"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28647": {
    "title": "Using Clustering to Strengthen Decision Diagram Bounds for Discrete Optimization",
    "volume": "main",
    "abstract": "Offering a generic approach to obtaining both upper and lower bounds, decision diagrams (DDs) are becoming an increasingly important tool for solving discrete optimization problems. In particular, they provide a powerful and often complementary alternative to other well-known generic bounding mechanisms such as the LP relaxation. A standard approach to employ DDs for discrete optimization is to formulate the problem as a Dynamic Program and use that formulation to compile a DD top-down in a layer-by-layer fashion. To limit the size of the resulting DD and to obtain bounds, one typically imposes a maximum width for each layer which is then enforced by either merging nodes (resulting in a so-called relaxed DD that provides a dual bound) or by dropping nodes (resulting in a so-called restricted DD that provides a primal bound). The quality of the DD bounds obtained from this top-down compilation process heavily depends on the heuristics used for the selection of the nodes to merge or drop. While it is sometimes possible to engineer problem-specific heuristics for this selection problem, the most generic approach relies on sorting the layer's nodes based on objective function information. In this paper, we propose a generic and problem-agnostic approach that relies on clustering nodes based on the state information associated with each node. In a set of computational experiments with different knapsack and scheduling problems, we show that our approach generally outperforms the classical generic approach, and often achieves drastically better bounds both with respect to the size of the DD and the time used for compiling the DD",
    "checked": true,
    "id": "23787b66370abe5c4ccfabcff22f031d477127cb",
    "semantic_title": "using clustering to strengthen decision diagram bounds for discrete optimization",
    "citation_count": 0,
    "authors": [
      "Mohsen Nafar",
      "Michael Römer"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28648": {
    "title": "On Partial Optimal Transport: Revising the Infeasibility of Sinkhorn and Efficient Gradient Methods",
    "volume": "main",
    "abstract": "This paper studies the Partial Optimal Transport (POT) problem between two unbalanced measures with at most n supports and its applications in various AI tasks such as color transfer or domain adaptation. There is hence a need for fast approximations of POT with increasingly large problem sizes in arising applications. We first theoretically and experimentally investigate the infeasibility of the state-of-the-art Sinkhorn algorithm for POT, which consequently degrades its qualitative performance in real world applications like point-cloud registration. To this end, we propose a novel rounding algorithm for POT, and then provide a feasible Sinkhorn procedure with a revised computation complexity of O(n^2/epsilon^4). Our rounding algorithm also permits the development of two first-order methods to approximate the POT problem. The first algorithm, Adaptive Primal-Dual Accelerated Gradient Descent (APDAGD), finds an epsilon-approximate solution to the POT problem in O(n^2.5/epsilon). The second method, Dual Extrapolation, achieves the computation complexity of O(n^2/epsilon), thereby being the best in the literature. We further demonstrate the flexibility of POT compared to standard OT as well as the practicality of our algorithms on real applications where two marginal distributions are unbalanced",
    "checked": true,
    "id": "ef9c7373b852dbd47c7d1e32897aa66773f61e45",
    "semantic_title": "on partial optimal transport: revising the infeasibility of sinkhorn and efficient gradient methods",
    "citation_count": 2,
    "authors": [
      "Anh Duc Nguyen",
      "Tuan Dung Nguyen",
      "Quang Minh Nguyen",
      "Hoang H. Nguyen",
      "Lam M. Nguyen",
      "Kim-Chuan Toh"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28649": {
    "title": "An Eager Satisfiability Modulo Theories Solver for Algebraic Datatypes",
    "volume": "main",
    "abstract": "Algebraic data types (ADTs) are a construct classically found in functional programming languages that capture data structures like enumerated types, lists, and trees. In recent years, interest in ADTs has increased. For example, popular programming languages, like Python, have added support for ADTs. Automated reasoning about ADTs can be done using satisfiability modulo theories (SMT) solving, an extension of the Boolean satisfiability problem with first-order logic and associated background theories. Unfortunately, SMT solvers that support ADTs do not scale as state-of-the-art approaches all use variations of the same lazy approach. In this paper, we present an SMT solver that takes a fundamentally different approach, an eager approach. Specifically, our solver reduces ADT queries to a simpler logical theory, uninterpreted functions (UF), and then uses an existing solver on the reduced query. We prove the soundness and completeness of our approach and demonstrate that it outperforms the state of the art on existing benchmarks, as well as a new, more challenging benchmark set from the planning domain",
    "checked": true,
    "id": "5f17d706ea03a1a76cc45611517cb37dcc7df9fd",
    "semantic_title": "an eager satisfiability modulo theories solver for algebraic datatypes",
    "citation_count": 0,
    "authors": [
      "Amar Shah",
      "Federico Mora",
      "Sanjit A.  Seshia"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28650": {
    "title": "An Approximate Skolem Function Counter",
    "volume": "main",
    "abstract": "One approach to probabilistic inference involves counting the number of models of a given Boolean formula. Here, we are interested in inferences involving higher-order objects, i.e., functions. We study the following task: Given a Boolean specification between a set of inputs and outputs, count the number of functions of inputs such that the specification is met. Such functions are called Skolem functions. We are motivated by the recent development of scalable approaches to Boolean function synthesis. This stands in relation to our problem analogously to the relationship between Boolean satisfiability and the model counting problem. Yet, counting Skolem functions poses considerable new challenges. From the complexity-theoretic standpoint, counting Skolem functions is not only #P-hard; it is quite unlikely to have an FPRAS (Fully Polynomial Randomized Approximation Scheme) as the problem of synthesizing a Skolem function remains challenging, even given access to an NP oracle. The primary contribution of this work is the first algorithm, SkolemFC, that computes the number of Skolem functions. SkolemFC relies on technical connections between counting functions and propositional model counting: our algorithm makes a linear number of calls to an approximate model counter and computes an estimate of the number of Skolem functions with theoretical guarantees. Our prototype displays impressive scalability, handling benchmarks comparably to state-of-the-art Skolem function synthesis engines, even though counting all such functions ostensibly poses a greater challenge than synthesizing a single function",
    "checked": true,
    "id": "80ed48571aad8ac2217fe3124b7aa3b674d28ce8",
    "semantic_title": "an approximate skolem function counter",
    "citation_count": 0,
    "authors": [
      "Arijit Shaw",
      "Brendan Juba",
      "Kuldeep S. Meel"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28651": {
    "title": "Optimizing ADMM and Over-Relaxed ADMM Parameters for Linear Quadratic Problems",
    "volume": "main",
    "abstract": "The Alternating Direction Method of Multipliers (ADMM) has gained significant attention across a broad spectrum of machine learning applications. Incorporating the over-relaxation technique shows potential for enhancing the convergence rate of ADMM. However, determining optimal algorithmic parameters, including both the associated penalty and relaxation parameters, often relies on empirical approaches tailored to specific problem domains and contextual scenarios. Incorrect parameter selection can significantly hinder ADMM's convergence rate. To address this challenge, in this paper we first propose a general approach to optimize the value of penalty parameter, followed by a novel closed-form formula to compute the optimal relaxation parameter in the context of linear quadratic problems (LQPs). We then experimentally validate our parameter selection methods through random instantiations and diverse imaging applications, encompassing diffeomorphic image registration, image deblurring, and MRI reconstruction",
    "checked": true,
    "id": "790316d2d2d1fe795368bcaf692bbfab30c5199a",
    "semantic_title": "optimizing admm and over-relaxed admm parameters for linear quadratic problems",
    "citation_count": 0,
    "authors": [
      "Jintao Song",
      "Wenqi Lu",
      "Yunwen Lei",
      "Yuchao Tang",
      "Zhenkuan Pan",
      "Jinming Duan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28652": {
    "title": "Disjoint Partial Enumeration without Blocking Clauses",
    "volume": "main",
    "abstract": "A basic algorithm for enumerating disjoint propositional models (disjoint AllSAT) is based on adding blocking clauses incrementally, ruling out previously found models. On the one hand, blocking clauses have the potential to reduce the number of generated models exponentially, as they can handle partial models. On the other hand, the introduction of a large number of blocking clauses affects memory consumption and drastically slows down unit propagation. We propose a new approach that allows for enumerating disjoint partial models with no need for blocking clauses by integrating: Conflict-Driven Clause-Learning (CDCL), Chronological Backtracking (CB), and methods for shrinking models (Implicant Shrinking). Experiments clearly show the benefits of our novel approach",
    "checked": true,
    "id": "955e2620f0a300e54a37550b53bbc5497edaed17",
    "semantic_title": "disjoint partial enumeration without blocking clauses",
    "citation_count": 2,
    "authors": [
      "Giuseppe Spallitta",
      "Roberto Sebastiani",
      "Armin Biere"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28653": {
    "title": "SAT-Based Algorithms for Regular Graph Pattern Matching",
    "volume": "main",
    "abstract": "Graph matching is a fundamental problem in pattern recognition, with many applications such as software analysis and computational biology. One well-known type of graph matching problem is graph isomorphism, which consists of deciding if two graphs are identical. Despite its usefulness, the properties that one may check using graph isomorphism are rather limited, since it only allows strict equality checks between two graphs. For example, it does not allow one to check complex structural properties such as if the target graph is an arbitrary length sequence followed by an arbitrary size loop. We propose a generalization of graph isomorphism that allows one to check such properties through a declarative specification. This specification is given in the form of a Regular Graph Pattern (ReGaP), a special type of graph, inspired by regular expressions, that may contain wildcard nodes that represent arbitrary structures such as variable-sized sequences or subgraphs. We propose a SAT-based algorithm for checking if a target graph matches a given ReGaP. We also propose a preprocessing technique for improving the performance of the algorithm and evaluate it through an extensive experimental evaluation on benchmarks from the CodeSearchNet dataset",
    "checked": true,
    "id": "763a47fb0b0b24b546455715bdc7a90c68d668cc",
    "semantic_title": "sat-based algorithms for regular graph pattern matching",
    "citation_count": 0,
    "authors": [
      "Miguel Terra-Neves",
      "José Amaral",
      "Alexandre Lemos",
      "Rui Quintino",
      "Pedro Resende",
      "Antonio Alegria"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28654": {
    "title": "CEGAR-Based Approach for Solving Combinatorial Optimization Modulo Quantified Linear Arithmetics Problems",
    "volume": "main",
    "abstract": "Bioinformatics has always been a prolific domain for generating complex satisfiability and optimization problems. For instance, the synthesis of multi-scale models of biological networks has recently been associated with the resolution of optimization problems mixing Boolean logic and universally quantified linear constraints (OPT+qLP), which can be benchmarked on real-world models. In this paper, we introduce a Counter-Example-Guided Abstraction Refinement (CEGAR) to solve such problems efficiently. Our CEGAR exploits monotone properties inherent to linear optimization in order to generalize counter-examples of Boolean relaxations. We implemented our approach by extending Answer Set Programming (ASP) solver Clingo with a quantified linear constraints propagator. Our prototype enables exploiting independence of sub-formulas to further exploit the generalization of counter-examples. We evaluate the impact of refinement and partitioning on two sets of OPT+qLP problems inspired by system biology. Additionally, we conducted a comparison with the state-of-the-art ASP solver Clingo[lpx] that handles non-quantified linear constraints, showing the advantage of our CEGAR approach for solving large problems",
    "checked": true,
    "id": "d4d6e96b908edd617399d13062303ab53ffae19b",
    "semantic_title": "cegar-based approach for solving combinatorial optimization modulo quantified linear arithmetics problems",
    "citation_count": 0,
    "authors": [
      "Kerian Thuillier",
      "Anne Siegel",
      "Loïc Paulevé"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28655": {
    "title": "Learning to Learn in Interactive Constraint Acquisition",
    "volume": "main",
    "abstract": "Constraint Programming (CP) has been successfully used to model and solve complex combinatorial problems. However, modeling is often not trivial and requires expertise, which is a bottleneck to wider adoption. In Constraint Acquisition (CA), the goal is to assist the user by automatically learning the model. In (inter)active CA, this is done by interactively posting queries to the user, e.g. does this partial solution satisfy your (unspecified) constraints or not. While interactive CA methods learn the constraints, the learning is related to symbolic concept learning, as the goal is to learn an exact representation. However, a large number of queries is required to learn the model, which is a major limitation. In this paper, we aim to alleviate this limitation by tightening the connection of CA and Machine Learning (ML), by, for the first time in interactive CA, exploiting statistical ML methods. We propose to use probabilistic classification models to guide interactive CA queries to the most promising parts. We discuss how to train classifiers to predict whether a candidate expression from the bias is a constraint of the problem or not, using both relation-based and scope-based features. We then show how the predictions can be used in all layers of interactive CA: the query generation, the scope finding, and the lowest-level constraint finding. We experimentally evaluate our proposed methods using different classifiers and show that our methods greatly outperform the state of the art, decreasing the number of queries needed to converge by up to 72%",
    "checked": true,
    "id": "159961ab621abf656d824f01249c724c1ac68d35",
    "semantic_title": "learning to learn in interactive constraint acquisition",
    "citation_count": 1,
    "authors": [
      "Dimosthenis Tsouros",
      "Senne Berden",
      "Tias Guns"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28656": {
    "title": "GSO-Net: Grid Surface Optimization via Learning Geometric Constraints",
    "volume": "main",
    "abstract": "In the context of surface representations, we find a natural structural similarity between grid surface and image data. Motivated by this inspiration, we propose a novel approach: encoding grid surfaces as geometric images and using image processing methods to address surface optimization-related problems. As a result, we have created the first dataset for grid surface optimization and devised a learning-based grid surface optimization network specifically tailored to geometric images, addressing the surface optimization problem through a data-driven learning of geometric constraints paradigm. We conduct extensive experiments on developable surface optimization, surface flattening, and surface denoising tasks using the designed network and datasets. The results demonstrate that our proposed method not only addresses the surface optimization problem better than traditional numerical optimization methods, especially for complex surfaces, but also boosts the optimization speed by multiple orders of magnitude. This pioneering study successfully applies deep learning methods to the field of surface optimization and provides a new solution paradigm for similar tasks, which will provide inspiration and guidance for future developments in the field of discrete surface optimization. The code and dataset are available at https://github.com/chaoyunwang/GSO-Net",
    "checked": true,
    "id": "ec769eb08841b6d6dd0abe802df6a56ce9316e8a",
    "semantic_title": "gso-net: grid surface optimization via learning geometric constraints",
    "citation_count": 0,
    "authors": [
      "Chaoyun Wang",
      "Jingmin Xin",
      "Nanning Zheng",
      "Caigui Jiang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28657": {
    "title": "Encoding Constraints as Binary Constraint Networks Satisfying BTP",
    "volume": "main",
    "abstract": "Recently, the Binary Constraint Tree (BCT), a tree structured Binary Constraint Network (BCN), has been shown to be more succinct than various ad-hoc constraints. In this paper, we investigate the modelling power of a well-known tractable hybrid class generalizing BCT, i.e. the class of BCNs satisfying Broken Triangle Property (BTP) called BTP Networks (BTPNs). We show that the consistency checker of BTPN can be computed by polysize monotone circuit, thus, some global constraints cannot be encoded as polysize BTPN, such as the AllDifferent and Linear constraints. Then our study reveals that BTPN is strictly more succinct than the DNNF constraint and all 14 ad-hoc constraints discussed in (Wang and Yap 2023), such as the context-free grammar, BCT and smart table constraints. Furthermore, we also show that BTPN is as powerful as DNNF in terms of computing various operations and queries. In addition, we prove that it is NP-hard to determine the minimum sized BTPN encoding a constraint",
    "checked": true,
    "id": "e6de273b15ddd3f9e8dbb97d4c3277f1bfd32e84",
    "semantic_title": "encoding constraints as binary constraint networks satisfying btp",
    "citation_count": 0,
    "authors": [
      "Ruiwei Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28658": {
    "title": "What Are the Rules? Discovering Constraints from Data",
    "volume": "main",
    "abstract": "Constraint programming and AI planning are powerful tools for solving assignment, optimization, and scheduling problems. They require, however, the rarely available combination of domain knowledge and mathematical modeling expertise. Learning constraints from exemplary solutions can close this gap and alleviate the effort of modeling. Existing approaches either require extensive user interaction, need exemplary invalid solutions that must be generated by experts at great expense, or show high noise-sensitivity. We aim to find constraints from potentially noisy solutions, without the need of user interaction. To this end, we formalize the problem in terms of the Minimum Description Length (MDL) principle, by which we select the model with the best lossless compression of the data. Solving the problem involves model counting, which is #P-hard to approximate. We therefore propose the greedy URPILS algorithm to find high-quality constraints in practice. Extensive experiments on constraint programming and AI planning benchmark data show URPILS not only finds more accurate and succinct constraints, but also is more robust to noise, and has lower sample complexity than the state of the art",
    "checked": true,
    "id": "47baf43c3e7c20d4700b70e0373f140b89c7d1bd",
    "semantic_title": "what are the rules? discovering constraints from data",
    "citation_count": 0,
    "authors": [
      "Boris Wiegand",
      "Dietrich  Klakow",
      "Jilles Vreeken"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28659": {
    "title": "SAT-Based Tree Decomposition with Iterative Cascading Policy Selection",
    "volume": "main",
    "abstract": "Solvers for propositional satisfiability (SAT) effectively tackle hard optimization problems. However, translating to SAT can cause a significant size increase, restricting its use to smaller instances. To mitigate this, frameworks using multiple local SAT calls for gradually improving a heuristic solution have been proposed. The performance of such algorithmic frameworks heavily relies on critical parameters, including the size of selected local instances and the time allocated per SAT call. This paper examines the automated configuration of the treewidth SAT-based local improvement method (TW-SLIM) framework, which uses multiple SAT calls for computing tree decompositions of small width, a fundamental problem in combinatorial optimization. We explore various TW-SLIM configuration methods, including offline learning and real-time adjustments, significantly outperforming default settings in multi-SAT scenarios with changing problems. Building upon insights gained from offline training and real-time configurations for TW-SLIM, we propose the iterative cascading policy—a novel hybrid technique that uniquely combines both. The iterative cascading policy employs a pool of 30 configurations obtained through clustering-based offline methods, deploying them in dynamic cascades across multiple rounds. In each round, the 30 configurations are tested according to the cascading ordering, and the best tree decomposition is retained for further improvement, with the option to adjust the following ordering of cascades. This iterative approach significantly enhances the performance of TW-SLIM beyond baseline results, even within varying global timeouts. This highlights the effectiveness of the proposed iterative cascading policy in enhancing the efficiency and efficacy of complex algorithmic frameworks like TW-SLIM",
    "checked": true,
    "id": "a1ccd722ff0e7e019a6518b546104b7eed5a5704",
    "semantic_title": "sat-based tree decomposition with iterative cascading policy selection",
    "citation_count": 0,
    "authors": [
      "Hai Xia",
      "Stefan Szeider"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28660": {
    "title": "Engineering an Exact Pseudo-Boolean Model Counter",
    "volume": "main",
    "abstract": "Model counting, a fundamental task in computer science, involves determining the number of satisfying assignments to a Boolean formula, typically represented in conjunctive normal form (CNF). While model counting for CNF formulas has received extensive attention with a broad range of applications, the study of model counting for Pseudo-Boolean (PB) formulas has been relatively overlooked. Pseudo-Boolean formulas, being more succinct than propositional Boolean formulas, offer greater flexibility in representing real-world problems. Consequently, there is a crucial need to investigate efficient techniques for model counting for PB formulas. In this work, we propose the first exact Pseudo-Boolean model counter, PBCount , that relies on knowledge compilation approach via algebraic decision diagrams. Our extensive empirical evaluation shows that PBCount can compute counts for 1513 instances while the current state-of-the-art approach could only handle 1013 instances. Our work opens up several avenues for future work in the context of model counting for PB formulas, such as the development of preprocessing techniques and exploration of approaches other than knowledge compilation",
    "checked": true,
    "id": "9fca00510720045c7662a2c573237b127c147837",
    "semantic_title": "engineering an exact pseudo-boolean model counter",
    "citation_count": 0,
    "authors": [
      "Suwei Yang",
      "Kuldeep S. Meel"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28661": {
    "title": "A Reinforcement-Learning-Based Multiple-Column Selection Strategy for Column Generation",
    "volume": "main",
    "abstract": "Column generation (CG) is one of the most successful approaches for solving large-scale linear programming (LP) problems. Given an LP with a prohibitively large number of variables (i.e., columns), the idea of CG is to explicitly consider only a subset of columns and iteratively add potential columns to improve the objective value. While adding the column with the most negative reduced cost can guarantee the convergence of CG, it has been shown that adding multiple columns per iteration rather than a single column can lead to faster convergence. However, it remains a challenge to design a multiple-column selection strategy to select the most promising columns from a large number of candidate columns. In this paper, we propose a novel reinforcement-learning-based (RL) multiple-column selection strategy. To the best of our knowledge, it is the first RL-based multiple-column selection strategy for CG. The effectiveness of our approach is evaluated on two sets of problems: the cutting stock problem and the graph coloring problem. Compared to several widely used single-column and multiple-column selection strategies, our RL-based multiple-column selection strategy leads to faster convergence and achieves remarkable reductions in the number of CG iterations and runtime",
    "checked": true,
    "id": "791a5ec8b3b0db96054fd9364d71eab20f36dfbf",
    "semantic_title": "a reinforcement-learning-based multiple-column selection strategy for column generation",
    "citation_count": 1,
    "authors": [
      "Haofeng Yuan",
      "Lichang Fang",
      "Shiji Song"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28662": {
    "title": "Large-Scale Non-convex Stochastic Constrained Distributionally Robust Optimization",
    "volume": "main",
    "abstract": "Distributionally robust optimization (DRO) is a powerful framework for training robust models against data distribution shifts. This paper focuses on constrained DRO, which has an explicit characterization of the robustness level. Existing studies on constrained DRO mostly focus on convex loss function, and exclude the practical and challenging case with non-convex loss function, e.g., neural network. This paper develops a stochastic algorithm and its performance analysis for non-convex constrained DRO. The computational complexity of our stochastic algorithm at each iteration is independent of the overall dataset size, and thus is suitable for large-scale applications. We focus on the general Cressie-Read family divergence defined uncertainty set which includes chi^2-divergences as a special case. We prove that our algorithm finds an epsilon-stationary point with an improved computational complexity than existing methods. Our method also applies to the smoothed conditional value at risk (CVaR) DRO",
    "checked": true,
    "id": "bcca82f18d2dc514fd95d5d9815bbb094d3880ae",
    "semantic_title": "large-scale non-convex stochastic constrained distributionally robust optimization",
    "citation_count": 1,
    "authors": [
      "Qi Zhang",
      "Yi Zhou",
      "Ashley Prater-Bennette",
      "Lixin Shen",
      "Shaofeng Zou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28663": {
    "title": "Multimodal Graph Neural Architecture Search under Distribution Shifts",
    "volume": "main",
    "abstract": "Multimodal graph neural architecture search (MGNAS) has shown great success for automatically designing the optimal multimodal graph neural network (MGNN) architecture by leveraging multimodal representation, crossmodal information and graph structure in one unified framework. However, existing MGNAS fails to handle distribution shifts that naturally exist in multimodal graph data, since the searched architectures inevitably capture spurious statistical correlations under distribution shifts. To solve this problem, we propose a novel Out-of-distribution Generalized Multimodal Graph Neural Architecture Search (OMG-NAS) method which optimizes the MGNN architecture with respect to its performance on decorrelated OOD data. Specifically, we propose a multimodal graph representation decorrelation strategy, which encourages the searched MGNN model to output representations that eliminate spurious correlations through iteratively optimizing the feature weights and controller. In addition, we propose a global sample weight estimator that facilitates the sharing of optimal sample weights learned from existing architectures. This design promotes the effective estimation of the sample weights for candidate MGNN architectures to generate decorrelated multimodal graph representations, concentrating more on the truly predictive relations between invariant features and ground-truth labels. Extensive experiments on real-world multimodal graph datasets demonstrate the superiority of our proposed method over SOTA baselines",
    "checked": true,
    "id": "0b18bb8858a6e437b007cadea507a09946061383",
    "semantic_title": "multimodal graph neural architecture search under distribution shifts",
    "citation_count": 2,
    "authors": [
      "Jie Cai",
      "Xin Wang",
      "Haoyang Li",
      "Ziwei Zhang",
      "Wenwu Zhu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28664": {
    "title": "Make Lossy Compression Meaningful for Low-Light Images",
    "volume": "main",
    "abstract": "Low-light images frequently occur due to unavoidable environmental influences or technical limitations, such as insufficient lighting or limited exposure time. To achieve better visibility for visual perception, low-light image enhancement is usually adopted. Besides, lossy image compression is vital for meeting the requirements of storage and transmission in computer vision applications. To touch the above two practical demands, current solutions can be categorized into two sequential manners: ``Compress before Enhance (CbE)'' or ``Enhance before Compress (EbC)''. However, both of them are not suitable since: (1) Error accumulation in the individual models plagues sequential solutions. Especially, once low-light images are compressed by existing general lossy image compression approaches, useful information (e.g., texture details) would be lost resulting in a dramatic performance decrease in low-light image enhancement. (2) Due to the intermediate process, the sequential solution introduces an additional burden resulting in low efficiency. We propose a novel joint solution to simultaneously achieve a high compression rate and good enhancement performance for low-light images with much lower computational cost and fewer model parameters. We design an end-to-end trainable architecture, which includes the main enhancement branch and the signal-to-noise ratio (SNR) aware branch. Experimental results show that our proposed joint solution achieves a significant improvement over different combinations of existing state-of-the-art sequential ``Compress before Enhance'' or ``Enhance before Compress'' solutions for low-light images, which would make lossy low-light image compression more meaningful. The project is publicly available at: https://github.com/CaiShilv/Joint-IC-LL",
    "checked": true,
    "id": "00ae861b1a207ac7741b165824809657fec0134f",
    "semantic_title": "make lossy compression meaningful for low-light images",
    "citation_count": 0,
    "authors": [
      "Shilv Cai",
      "Liqun Chen",
      "Sheng Zhong",
      "Luxin Yan",
      "Jiahuan Zhou",
      "Xu Zou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28665": {
    "title": "RR-PU: A Synergistic Two-Stage Positive and Unlabeled Learning Framework for Robust Tax Evasion Detection",
    "volume": "main",
    "abstract": "Tax evasion, an unlawful practice in which taxpayers deliberately conceal information to avoid paying tax liabilities, poses significant challenges for tax authorities. Effective tax evasion detection is critical for assisting tax authorities in mitigating tax revenue loss. Recently, machine-learning-based methods, particularly those employing positive and unlabeled (PU) learning, have been adopted for tax evasion detection, achieving notable success. However, these methods exhibit two major practical limitations. First, their success heavily relies on the strong assumption that the label frequency (the fraction of identified taxpayers among tax evaders) is known in advance. Second, although some methods attempt to estimate label frequency using approaches like Mixture Proportion Estimation (MPE) without making any assumptions, they subsequently construct a classifier based on the error-prone label frequency obtained from the previous estimation. This two-stage approach may not be optimal, as it neglects error accumulation in classifier training resulting from the estimation bias in the first stage. To address these limitations, we propose a novel PU learning-based tax evasion detection framework called RR-PU, which can revise the bias in a two-stage synergistic manner. Specifically, RR-PU refines the label frequency initialization by leveraging a regrouping technique to fortify the MPE perspective. Subsequently, we integrate a trainable slack variable to fine-tune the initial label frequency, concurrently optimizing this variable and the classifier to eliminate latent bias in the initial stage. Experimental results on three real-world tax datasets demonstrate that RR-PU outperforms state-of-the-art methods in tax evasion detection tasks",
    "checked": true,
    "id": "3e358a74c84a276cea66b45a89d7b395e63495c8",
    "semantic_title": "rr-pu: a synergistic two-stage positive and unlabeled learning framework for robust tax evasion detection",
    "citation_count": 0,
    "authors": [
      "Shuzhi Cao",
      "Jianfei Ruan",
      "Bo Dong",
      "Bin Shi",
      "Qinghua Zheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28666": {
    "title": "Hierarchical and Incremental Structural Entropy Minimization for Unsupervised Social Event Detection",
    "volume": "main",
    "abstract": "As a trending approach for social event detection, graph neural network (GNN)-based methods enable a fusion of natural language semantics and the complex social network structural information, thus showing SOTA performance. However, GNN-based methods can miss useful message correlations. Moreover, they require manual labeling for training and predetermining the number of events for prediction. In this work, we address social event detection via graph structural entropy (SE) minimization. While keeping the merits of the GNN-based methods, the proposed framework, HISEvent, constructs more informative message graphs, is unsupervised, and does not require the number of events given a priori. Specifically, we incrementally explore the graph neighborhoods using 1-dimensional (1D) SE minimization to supplement the existing message graph with edges between semantically related messages. We then detect events from the message graph by hierarchically minimizing 2-dimensional (2D) SE. Our proposed 1D and 2D SE minimization algorithms are customized for social event detection and effectively tackle the efficiency problem of the existing SE minimization algorithms. Extensive experiments show that HISEvent consistently outperforms GNN-based methods and achieves the new SOTA for social event detection under both closed- and open-set settings while being efficient and robust",
    "checked": true,
    "id": "f8258069425323de1df423ff50abf97c12e22289",
    "semantic_title": "hierarchical and incremental structural entropy minimization for unsupervised social event detection",
    "citation_count": 6,
    "authors": [
      "Yuwei Cao",
      "Hao Peng",
      "Zhengtao Yu",
      "Philip S. Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28667": {
    "title": "Distributional Off-Policy Evaluation for Slate Recommendations",
    "volume": "main",
    "abstract": "Recommendation strategies are typically evaluated by using previously logged data, employing off-policy evaluation methods to estimate their expected performance. However, for strategies that present users with slates of multiple items, the resulting combinatorial action space renders many of these methods impractical. Prior work has developed estimators that leverage the structure in slates to estimate the expected off-policy performance, but the estimation of the entire performance distribution remains elusive. Estimating the complete distribution allows for a more comprehensive evaluation of recommendation strategies, particularly along the axes of risk and fairness that employ metrics computable from the distribution. In this paper, we propose an estimator for the complete off-policy performance distribution for slates and establish conditions under which the estimator is unbiased and consistent. This builds upon prior work on off-policy evaluation for slates and off-policy distribution estimation in reinforcement learning. We validate the efficacy of our method empirically on synthetic data as well as on a slate recommendation simulator constructed from real-world data (MovieLens-20M). Our results show a significant reduction in estimation variance and improved sample efficiency over prior work across a range of slate structures",
    "checked": true,
    "id": "72e68991204523d5cb89eb49a499c63a4a35120d",
    "semantic_title": "distributional off-policy evaluation for slate recommendations",
    "citation_count": 0,
    "authors": [
      "Shreyas Chaudhari",
      "David Arbour",
      "Georgios Theocharous",
      "Nikos Vlassis"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28668": {
    "title": "Uncertainty-Aware Yield Prediction with Multimodal Molecular Features",
    "volume": "main",
    "abstract": "Predicting chemical reaction yields is pivotal for efficient chemical synthesis, an area that focuses on the creation of novel compounds for diverse uses. Yield prediction demands accurate representations of reactions for forecasting practical transformation rates. Yet, the uncertainty issues broadcasting in real-world situations prohibit current models to excel in this task owing to the high sensitivity of yield activities and the uncertainty in yield measurements. Existing models often utilize single-modal feature representations, such as molecular fingerprints, SMILES sequences, or molecular graphs, which is not sufficient to capture the complex interactions and dynamic behavior of molecules in reactions. In this paper, we present an advanced Uncertainty-Aware Multimodal model (UAM) to tackle these challenges. Our approach seamlessly integrates data sources from multiple modalities by encompassing sequence representations, molecular graphs, and expert-defined chemical reaction features for a comprehensive representation of reactions. Additionally, we address both the model and data-based uncertainty, refining the model's predictive capability. Extensive experiments on three datasets, including two high throughput experiment (HTE) datasets and one chemist-constructed Amide coupling reaction dataset, demonstrate that UAM outperforms the state-of-the-art methods. The code and used datasets are available at https://github.com/jychen229/Multimodal-reaction-yield-prediction",
    "checked": true,
    "id": "fedccf6c70a7d075753e68deeb4d03f936c7e824",
    "semantic_title": "uncertainty-aware yield prediction with multimodal molecular features",
    "citation_count": 0,
    "authors": [
      "Jiayuan Chen",
      "Kehan  Guo",
      "Zhen Liu",
      "Olexandr Isayev",
      "Xiangliang Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28669": {
    "title": "Sparse Enhanced Network: An Adversarial Generation Method for Robust Augmentation in Sequential Recommendation",
    "volume": "main",
    "abstract": "Sequential Recommendation plays a significant role in daily recommendation systems, such as e-commerce platforms like Amazon and Taobao. However, even with the advent of large models, these platforms often face sparse issues in the historical browsing records of individual users due to new users joining or the introduction of new products. As a result, existing sequence recommendation algorithms may not perform well. To address this, sequence-based data augmentation methods have garnered attention. Existing sequence enhancement methods typically rely on augmenting existing data, employing techniques like cropping, masking prediction, random reordering, and random replacement of the original sequence. While these methods have shown improvements, they often overlook the exploration of the deep embedding space of the sequence. To tackle these challenges, we propose a Sparse Enhanced Network (SparseEnNet), which is a robust adversarial generation method. SparseEnNet aims to fully explore the hidden space in sequence recommendation, generating more robust enhanced items. Additionally, we adopt an adversarial generation method, allowing the model to differentiate between data augmentation categories and achieve better prediction performance for the next item in the sequence. Experiments have demonstrated that our method achieves a remarkable 4-14% improvement over existing methods when evaluated on the real-world datasets. (https://github.com/junyachen/SparseEnNet)",
    "checked": true,
    "id": "546d5cfcbbc84c094b17994bb1d5f5c61bf36c7b",
    "semantic_title": "sparse enhanced network: an adversarial generation method for robust augmentation in sequential recommendation",
    "citation_count": 0,
    "authors": [
      "Junyang Chen",
      "Guoxuan Zou",
      "Pan Zhou",
      "Wu Yirui",
      "Zhenghan Chen",
      "Houcheng Su",
      "Huan Wang",
      "Zhiguo Gong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28670": {
    "title": "Signed Graph Neural Ordinary Differential Equation for Modeling Continuous-Time Dynamics",
    "volume": "main",
    "abstract": "Modeling continuous-time dynamics constitutes a foundational challenge, and uncovering inter-component correlations within complex systems holds promise for enhancing the efficacy of dynamic modeling. The prevailing approach of integrating graph neural networks with ordinary differential equations has demonstrated promising performance. However, they disregard the crucial signed information potential on graphs, impeding their capacity to accurately capture real-world phenomena and leading to subpar outcomes. In response, we introduce a novel approach: a signed graph neural ordinary differential equation, adeptly addressing the limitations of miscapturing signed information. Our proposed solution boasts both flexibility and efficiency. To substantiate its effectiveness, we seamlessly integrate our devised strategies into three preeminent graph-based dynamic modeling frameworks: graph neural ordinary differential equations, graph neural controlled differential equations, and graph recurrent neural networks. Rigorous assessments encompass three intricate dynamic scenarios from physics and biology, as well as scrutiny across four authentic real-world traffic datasets. Remarkably outperforming the trio of baselines, empirical results underscore the substantial performance enhancements facilitated by our proposed approach. Our code can be found at https://github.com/beautyonce/SGODE",
    "checked": true,
    "id": "45ece02ea82f79c4c678c8de8e95cd0dfbe5a37d",
    "semantic_title": "signed graph neural ordinary differential equation for modeling continuous-time dynamics",
    "citation_count": 0,
    "authors": [
      "Lanlan Chen",
      "Kai Wu",
      "Jian Lou",
      "Jing Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28671": {
    "title": "Deep Structural Knowledge Exploitation and Synergy for Estimating Node Importance Value on Heterogeneous Information Networks",
    "volume": "main",
    "abstract": "The classic problem of node importance estimation has been conventionally studied with homogeneous network topology analysis. To deal with practical network heterogeneity, a few recent methods employ graph neural models to automatically learn diverse sources of information. However, the major concern revolves around that their fully adaptive learning process may lead to insufficient information exploration, thereby formulating the problem as the isolated node value prediction with underperformance and less interpretability. In this work, we propose a novel learning framework namely SKES. Different from previous automatic learning designs, SKES exploits heterogeneous structural knowledge to enrich the informativeness of node representations. Then based on a sufficiently uninformative reference, SKES estimates the importance value for any input node, by quantifying its informativeness disparity against the reference. This establishes an interpretable node importance computation paradigm. Furthermore, SKES dives deep into the understanding that \"nodes with similar characteristics are prone to have similar importance values\" whilst guaranteeing that such informativeness disparity between any different nodes is orderly reflected by the embedding distance of their associated latent features. Extensive experiments on three widely-evaluated benchmarks demonstrate the performance superiority of SKES over several recent competing methods",
    "checked": true,
    "id": "31d86bad9fe4e2cbe1ea20c3e30cbfbe5a601bee",
    "semantic_title": "deep structural knowledge exploitation and synergy for estimating node importance value on heterogeneous information networks",
    "citation_count": 0,
    "authors": [
      "Yankai Chen",
      "Yixiang Fang",
      "Qiongyan Wang",
      "Xin Cao",
      "Irwin King"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28672": {
    "title": "KGTS: Contrastive Trajectory Similarity Learning over Prompt Knowledge Graph Embedding",
    "volume": "main",
    "abstract": "Trajectory similarity computation serves as a fundamental functionality of various spatial information applications. Although existing deep learning similarity computation methods offer better efficiency and accuracy than non-learning solutions, they are still immature in trajectory embedding and suffer from poor generality and heavy preprocessing for training. Targeting these limitations, we propose a novel framework named KGTS based on knowledge graph grid embedding, prompt trajectory embedding, and unsupervised contrastive learning for improved trajectory similarity computation. Specifically, we first embed map grids with a GRot embedding method to vigorously grasp the neighbouring relations of grids. Then, a prompt trajectory embedding network incorporates the resulting grid embedding and extracts trajectory structure and point order information. It is trained by unsupervised contrastive learning, which not only alleviates the heavy preprocessing burden but also provides exceptional generality with creatively designed strategies for positive sample generation. The prompt trajectory embedding adopts a customized prompt paradigm to mitigate the gap between the grid embedding and the trajectory embedding. Extensive experiments on two real-world trajectory datasets demonstrate the superior performance of KGTS over state-of-the-art methods",
    "checked": true,
    "id": "8bd3e0c1b6a68a1068da83003335ac01f1af8dcf",
    "semantic_title": "kgts: contrastive trajectory similarity learning over prompt knowledge graph embedding",
    "citation_count": 0,
    "authors": [
      "Zhen Chen",
      "Dalin Zhang",
      "Shanshan Feng",
      "Kaixuan Chen",
      "Lisi Chen",
      "Peng Han",
      "Shuo Shang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28673": {
    "title": "Learning to Reweight for Generalizable Graph Neural Network",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs) show promising results for graph tasks. However, existing GNNs' generalization ability will degrade when there exist distribution shifts between testing and training graph data. The fundamental reason for the severe degeneration is that most GNNs are designed based on the I.I.D hypothesis. In such a setting, GNNs tend to exploit subtle statistical correlations existing in the training set for predictions, even though it is a spurious correlation. In this paper, we study the problem of the generalization ability of GNNs on Out-Of-Distribution (OOD) settings. To solve this problem, we propose the Learning to Reweight for Generalizable Graph Neural Network (L2R-GNN) to enhance the generalization ability for achieving satisfactory performance on unseen testing graphs that have different distributions with training graphs. We propose a novel nonlinear graph decorrelation method, which can substantially improve the out-of-distribution generalization ability and compares favorably to previous methods in restraining the over-reduced sample size. The variables of graph representation are clustered based on the stability of their correlations, and graph decorrelation method learns weights to remove correlations between the variables of different clusters rather than any two variables. Besides, we introduce an effective stochastic algorithm based on bi-level optimization for the L2R-GNN framework, which enables simultaneously learning the optimal weights and GNN parameters, and avoids the over-fitting issue. Experiments show that L2R-GNN greatly outperforms baselines on various graph prediction benchmarks under distribution shifts",
    "checked": true,
    "id": "d7d13917a6b134300a12089fb4c88bed470b35bf",
    "semantic_title": "learning to reweight for generalizable graph neural network",
    "citation_count": 3,
    "authors": [
      "Zhengyu Chen",
      "Teng  Xiao",
      "Kun Kuang",
      "Zheqi Lv",
      "Min Zhang",
      "Jinluan Yang",
      "Chengqiang Lu",
      "Hongxia Yang",
      "Fei Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28674": {
    "title": "Effective Comparative Prototype Hashing for Unsupervised Domain Adaptation",
    "volume": "main",
    "abstract": "Unsupervised domain adaptive hashing is a highly promising research direction within the field of retrieval. It aims to transfer valuable insights from the source domain to the target domain while maintaining high storage and retrieval efficiency. Despite its potential, this field remains relatively unexplored. Previous methods usually lead to unsatisfactory retrieval performance, as they frequently directly apply slightly modified domain adaptation algorithms to hash learning framework, or pursue domain alignment within the Hamming space characterized by limited semantic information. In this paper, we propose a simple yet effective approach named Comparative Prototype Hashing (CPH) for unsupervised domain adaptive image retrieval. We establish a domain-shared unit hypersphere space through prototype contrastive learning and then obtain the Hamming hypersphere space via mapping from the shared hypersphere. This strategy achieves a cohesive synergy between learning uniformly distributed and category conflict-averse feature representations, eliminating domain discrepancies, and facilitating hash code learning. Moreover, by leveraging dual-domain information to supervise the entire hashing model training process, we can generate hash codes that retain inter-sample similarity relationships within both domains. Experimental results validate that our CPH significantly outperforms the state-of-the-art counterparts across multiple cross-domain and single-domain retrieval tasks. Notably, on Office-Home and Office-31 datasets, CPH achieves an average performance improvement of 19.29% and 13.85% on cross-domain retrieval tasks compared to the second-best results, respectively. The source codes of our method are available at: https://github.com/christinecui/CPH",
    "checked": true,
    "id": "374fc4a5083a9ab6ce221bb6b19b0ec2a279262b",
    "semantic_title": "effective comparative prototype hashing for unsupervised domain adaptation",
    "citation_count": 1,
    "authors": [
      "Hui Cui",
      "Lihai Zhao",
      "Fengling Li",
      "Lei Zhu",
      "Xiaohui Han",
      "Jingjing Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28675": {
    "title": "Modeling Knowledge Graphs with Composite Reasoning",
    "volume": "main",
    "abstract": "The ability to combine multiple pieces of existing knowledge to infer new knowledge is both crucial and challenging. In this paper, we explore how facts of various entities are combined in the context of knowledge graph completion (KGC). We use composite reasoning to unify the views from different KGC models, including translational models, tensor factorization (TF)-based models, instance-based learning models, and KGC regularizers. Moreover, our comprehensive examination of composite reasoning revealed an unexpected phenomenon: certain TF-based models learn embeddings with erroneous composite reasoning, which ultimately violates their fundamental collaborative filtering assumption and reduces their effects. This motivates us to reduce their composition error. Empirical evaluations demonstrate that mitigating the composition risk not only enhances the performance of TF-based models across all tested settings, but also surpass or is competitive with the state-of-the-art performance on two out of four benchmarks",
    "checked": true,
    "id": "807d5519984d4a7bdc6482263ff6131c4e638000",
    "semantic_title": "modeling knowledge graphs with composite reasoning",
    "citation_count": 0,
    "authors": [
      "Wanyun Cui",
      "Linqiu Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28676": {
    "title": "Discovering Sequential Patterns with Predictable Inter-event Delays",
    "volume": "main",
    "abstract": "Summarizing sequential data with serial episodes allows non-trivial insight into the data generating process. Existing methods penalize gaps in pattern occurrences equally, regardless of where in the pattern these occur. This results in a strong bias against patterns with long inter-event delays, and in addition that regularity in terms of delays is not rewarded or discovered---even though both aspects provide key insight. In this paper we tackle both these problems by explicitly modeling inter-event delay distributions. That is, we are not only interested in discovering the patterns, but also in describing how many times steps typically occur between their individual events. We formalize the problem in terms of the Minimum Description Length principle, by which we say the best set of patterns is the one that compresses the data best. The resulting optimization problem does not lend itself to exact optimization, and hence we propose Hopper to heuristically mine high quality patterns. Extensive experiments show that Hopper efficiently recovers the ground truth, discovers meaningful patterns from real-world data, and outperforms existing methods in discovering long-delay patterns",
    "checked": true,
    "id": "4d88f30e3330de6872d9badf563ae05bf55ce34f",
    "semantic_title": "discovering sequential patterns with predictable inter-event delays",
    "citation_count": 1,
    "authors": [
      "Joscha Cüppers",
      "Paul Krieger",
      "Jilles Vreeken"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28677": {
    "title": "Unveiling Implicit Deceptive Patterns in Multi-Modal Fake News via Neuro-Symbolic Reasoning",
    "volume": "main",
    "abstract": "In the current Internet landscape, the rampant spread of fake news, particularly in the form of multi-modal content, poses a great social threat. While automatic multi-modal fake news detection methods have shown promising results, the lack of explainability remains a significant challenge. Existing approaches provide superficial explainability by displaying learned important components or views from well-trained networks, but they often fail to uncover the implicit deceptive patterns that reveal how fake news is fabricated. To address this limitation, we begin by predefining three typical deceptive patterns, namely image manipulation, cross-modal inconsistency, and image repurposing, which shed light on the mechanisms underlying fake news fabrication. Then, we propose a novel Neuro-Symbolic Latent Model called NSLM, that not only derives accurate judgments on the veracity of news but also uncovers the implicit deceptive patterns as explanations. Specifically, the existence of each deceptive pattern is expressed as a two-valued learnable latent variable, which is acquired through amortized variational inference and weak supervision based on symbolic logic rules. Additionally, we devise pseudo-siamese networks to capture distinct deceptive patterns effectively. Experimental results on two real-world datasets demonstrate that our NSLM achieves the best performance in fake news detection while providing insightful explanations of deceptive patterns",
    "checked": true,
    "id": "7b3a430caf62d8c63457c586b39f078a3bca2bd7",
    "semantic_title": "unveiling implicit deceptive patterns in multi-modal fake news via neuro-symbolic reasoning",
    "citation_count": 0,
    "authors": [
      "Yiqi Dong",
      "Dongxiao He",
      "Xiaobao Wang",
      "Youzhu Jin",
      "Meng Ge",
      "Carl Yang",
      "Di Jin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28678": {
    "title": "Enhancing Job Recommendation through LLM-Based Generative Adversarial Networks",
    "volume": "main",
    "abstract": "Recommending suitable jobs to users is a critical task in online recruitment platforms. While existing job recommendation methods encounter challenges such as the low quality of users' resumes, which hampers their accuracy and practical effectiveness.With the rapid development of large language models (LLMs), utilizing the rich external knowledge encapsulated within them, as well as their powerful reasoning capabilities, is a promising way to complete users' resumes for more accurate recommendations. However, directly leveraging LLMs to enhance recommendation results is not a one-size-fits-all solution, as LLMs may suffer from fabricated generation and few-shot problems, which degrade the quality of resume completion. In this paper, we propose a novel LLM-based approach for job recommendation. To alleviate the limitation of fabricated generation for LLMs, we extract accurate and valuable information beyond users' self-description, which helps the LLMs better profile users for resume completion. Specifically, we not only extract users' explicit properties (e.g., skills, interests) from their self-description but also infer users' implicit characteristics from their behaviors for more accurate and meaningful resume completion. Nevertheless, some users still suffer from few-shot problems, which arise due to scarce interaction records, leading to limited guidance for high-quality resume generation. To address this issue, we propose aligning unpaired low-quality with high-quality generated resumes by Generative Adversarial Networks (GANs), which can refine the resume representations for better recommendation results. Extensive experiments on three large real-world recruitment datasets demonstrate the effectiveness of our proposed method",
    "checked": true,
    "id": "e82348d329bedb4545ac04d405cce07a2a4ede9b",
    "semantic_title": "enhancing job recommendation through llm-based generative adversarial networks",
    "citation_count": 11,
    "authors": [
      "Yingpeng Du",
      "Di Luo",
      "Rui Yan",
      "Xiaopei Wang",
      "Hongzhi Liu",
      "Hengshu Zhu",
      "Yang Song",
      "Jie Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28679": {
    "title": "Structural Entropy Based Graph Structure Learning for Node Classification",
    "volume": "main",
    "abstract": "As one of the most common tasks in graph data analysis, node classification is frequently solved by using graph structure learning (GSL) techniques to optimize graph structures and learn suitable graph neural networks. Most of the existing GSL methods focus on fusing different structural features (basic views) extracted from the graph, but very little graph semantics, like hierarchical communities, has been incorporated. Thus, they might be insufficient when dealing with the graphs containing noises from real-world complex systems. To address this issue, we propose a novel and effective GSL framework for node classification based on the structural information theory. Specifically, we first prove that an encoding tree with the minimal structural entropy could contain sufficient information for node classification and eliminate redundant noise via the graph's hierarchical abstraction. Then, we provide an efficient algorithm for constructing the encoding tree to enhance the basic views. Combining the community influence deduced from the encoding tree and the prediction confidence of each view, we further fuse the enhanced views to generate the optimal structure. Finally, we conduct extensive experiments on a variety of datasets. The results demonstrate that our method outperforms the state-of-the-art competitors on effectiveness and robustness",
    "checked": true,
    "id": "17c6592e9285f9b402c6b620ff2de0c814a8c994",
    "semantic_title": "structural entropy based graph structure learning for node classification",
    "citation_count": 1,
    "authors": [
      "Liang Duan",
      "Xiang Chen",
      "Wenjie Liu",
      "Daliang Liu",
      "Kun Yue",
      "Angsheng Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28680": {
    "title": "Progressive Distillation Based on Masked Generation Feature Method for Knowledge Graph Completion",
    "volume": "main",
    "abstract": "In recent years, knowledge graph completion (KGC) models based on pre-trained language model (PLM) have shown promising results. However, the large number of parameters and high computational cost of PLM models pose challenges for their application in downstream tasks. This paper proposes a progressive distillation method based on masked generation features for KGC task, aiming to significantly reduce the complexity of pre-trained models. Specifically, we perform pre-distillation on PLM to obtain high-quality teacher models, and compress the PLM network to obtain multi-grade student models. However, traditional feature distillation suffers from the limitation of having a single representation of information in teacher models. To solve this problem, we propose masked generation of teacher-student features, which contain richer representation information. Furthermore, there is a significant gap in representation ability between teacher and student. Therefore, we design a progressive distillation method to distill student models at each grade level, enabling efficient knowledge transfer from teachers to students. The experimental results demonstrate that the model in the pre-distillation stage surpasses the existing state-of-the-art methods. Furthermore, in the progressive distillation stage, the model significantly reduces the model parameters while maintaining a certain level of performance. Specifically, the model parameters of the lower-grade student model are reduced by 56.7\\% compared to the baseline",
    "checked": true,
    "id": "ec7f99f81d31ce4c1526c3ac3c86f453b6fcadfb",
    "semantic_title": "progressive distillation based on masked generation feature method for knowledge graph completion",
    "citation_count": 0,
    "authors": [
      "Cunhang Fan",
      "Yujie Chen",
      "Jun Xue",
      "Yonghui Kong",
      "Jianhua Tao",
      "Zhao Lv"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28681": {
    "title": "StockMixer: A Simple Yet Strong MLP-Based Architecture for Stock Price Forecasting",
    "volume": "main",
    "abstract": "Stock price forecasting is a fundamental yet challenging task in quantitative investment. Various researchers have developed a combination of neural network models (e.g., RNNs, GNNs, Transformers) for capturing complex indicator, temporal and stock correlations of the stock data.While complex architectures are highly expressive, they are often difficult to optimize and the performances are often compromised by the limited stock data. In this paper, we propose a simple MLP-based architecture named StockMixer which is easy to optimize and enjoys strong predictive performance. StockMixer performs indicator mixing, followed by time mixing, and finally stock mixing. Unlike the standard MLP-based mixing, we devise the time mixing to exchange multi-scale time patch information and realize the stock mixing by exploiting stock-to-market and market-to-stock influences explicitly. Extensive experiments on real stock benchmarks demonstrate our proposed StockMixer outperforms various state-of-the-art forecasting methods with a notable margin while reducing memory usage and runtime cost.Code is available at https://github.com/SJTU-Quant/StockMixer",
    "checked": true,
    "id": "fc6f8d04e7f6a50665a4ad12f320a4853088b355",
    "semantic_title": "stockmixer: a simple yet strong mlp-based architecture for stock price forecasting",
    "citation_count": 0,
    "authors": [
      "Jinyong Fan",
      "Yanyan Shen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28682": {
    "title": "Dense Projection for Anomaly Detection",
    "volume": "main",
    "abstract": "This work presents a novel method called dense projection for unsupervised anomaly detection (DPAD). The main idea is maximizing the local density of (normal) training data and then determining whether a test data is anomalous or not by evaluating its density. Specifically, DPAD uses a deep neural network to learn locally dense representations of normal data. Since density estimation is computationally expensive, we minimize the local distances of the representations in an iteratively reweighting manner, where the weights are updated adaptively and the parameters are regularized to avoid model collapse (all representations collapse to a single point). Compared with many state-of-the-art methods of anomaly detection, our DPAD does not rely on any assumption about the distribution or spatial structure of the normal data and representations. Moreover, we provide theoretical guarantees for the effectiveness of DPAD. The experiments show that our method DPAD is effective not only in traditional one-class classification problems but also in scenarios with complex normal data composed of multiple classes",
    "checked": true,
    "id": "d19fc50cd16dad8a1a67580b8d65f9393e7dd0ca",
    "semantic_title": "dense projection for anomaly detection",
    "citation_count": 1,
    "authors": [
      "Dazhi Fu",
      "Zhao Zhang",
      "Jicong Fan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28683": {
    "title": "Knowledge-Enhanced Historical Document Segmentation and Recognition",
    "volume": "main",
    "abstract": "Optical Character Recognition (OCR) of historical document images remains a challenging task because of the distorted input images, extensive number of uncommon characters, and the scarcity of labeled data, which impedes modern deep learning-based OCR techniques from achieving good recognition accuracy. Meanwhile, there exists a substantial amount of expert knowledge that can be utilized in this task. However, such knowledge is usually complicated and could only be accurately expressed with formal languages such as first-order logic (FOL), which is difficult to be directly integrated into deep learning models. This paper proposes KESAR, a novel Knowledge-Enhanced Document Segmentation And Recognition method for historical document images based on the Abductive Learning (ABL) framework. The segmentation and recognition models are enhanced by incorporating background knowledge for character extraction and prediction, followed by an efficient joint optimization of both models. We validate the effectiveness of KESAR on historical document datasets. The experimental results demonstrate that our method can simultaneously utilize knowledge-driven reasoning and data-driven learning, which outperforms the current state-of-the-art methods",
    "checked": true,
    "id": "96d2b9bc85ef8cbd441edb5eaca704f65c2d57ab",
    "semantic_title": "knowledge-enhanced historical document segmentation and recognition",
    "citation_count": 0,
    "authors": [
      "En-Hao Gao",
      "Yu-Xuan Huang",
      "Wen-Chao Hu",
      "Xin-Hao Zhu",
      "Wang-Zhou Dai"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28684": {
    "title": "Zero-1-to-3: Domain-Level Zero-Shot Cognitive Diagnosis via One Batch of Early-Bird Students towards Three Diagnostic Objectives",
    "volume": "main",
    "abstract": "Cognitive diagnosis seeks to estimate the cognitive states of students by exploring their logged practice quiz data. It plays a pivotal role in personalized learning guidance within intelligent education systems. In this paper, we focus on an important, practical, yet often underexplored task: domain-level zero-shot cognitive diagnosis (DZCD), which arises due to the absence of student practice logs in newly launched domains. Recent cross-domain diagnostic models have been demonstrated to be a promising strategy for DZCD. These methods primarily focus on how to transfer student states across domains. However, they might inadvertently incorporate non-transferable information into student representations, thereby limiting the efficacy of knowledge transfer. To tackle this, we propose Zero-1-to-3, a domain-level zero-shot cognitive diagnosis framework via one batch of early-bird students towards three diagnostic objectives. Our approach initiates with pre-training a diagnosis model with dual regularizers, which decouples student states into domain-shared and domain-specific parts. The shared cognitive signals can be transferred to the target domain, enriching the cognitive priors for the new domain, which ensures the cognitive state propagation objective. Subsequently, we devise a strategy to generate simulated practice logs for cold-start students through analyzing the behavioral patterns from early-bird students, fulfilling the domain-adaption goal. Consequently, we refine the cognitive states of cold-start students as diagnostic outcomes via virtual data, aligning with the diagnosis-oriented goal. Finally, extensive experiments on six real-world datasets highlight the efficacy of our model for DZCD and its practical application in question recommendation. The code is publicly available at https://github.com/bigdata-ustc/Zero-1-to-3",
    "checked": true,
    "id": "bda125a71f67d9ad68a9884e0f38eff346b2ab35",
    "semantic_title": "zero-1-to-3: domain-level zero-shot cognitive diagnosis via one batch of early-bird students towards three diagnostic objectives",
    "citation_count": 2,
    "authors": [
      "Weibo Gao",
      "Qi Liu",
      "Hao Wang",
      "Linan Yue",
      "Haoyang Bi",
      "Yin Gu",
      "Fangzhou Yao",
      "Zheng Zhang",
      "Xin Li",
      "Yuanjing He"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28685": {
    "title": "Your Career Path Matters in Person-Job Fit",
    "volume": "main",
    "abstract": "We are again confronted with one of the most vexing aspects of the advancement of technology: automation and AI technology cause the devaluation of human labor, resulting in unemployment. With this background, automatic person-job fit systems are promising solutions to promote the employment rate. The purpose of person-job fit is to calculate a matching score between the job seeker's resume and the job posting, determining whether the job seeker is suitable for the position. In this paper, we propose a new approach to person-job fit that characterizes the hidden preference derived from the job seeker's career path. We categorize and utilize three types of preferences in the career path: consistency, likeness, and continuity. We prove that understanding the career path enables us to provide more appropriate career suggestions to job seekers. To demonstrate the practical value of our proposed model, we conduct extensive experiments on real-world data extracted from an online recruitment platform and then present detailed cases to show how the career path matter in person-job fit",
    "checked": true,
    "id": "ff9fe0b41ffe787315655fd3c81ea9ebae6ec9f2",
    "semantic_title": "your career path matters in person-job fit",
    "citation_count": 0,
    "authors": [
      "Zhuocheng Gong",
      "Yang Song",
      "Tao Zhang",
      "Ji-Rong Wen",
      "Dongyan Zhao",
      "Rui Yan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28686": {
    "title": "Efficient Representation Learning of Satellite Image Time Series and Their Fusion for Spatiotemporal Applications",
    "volume": "main",
    "abstract": "Satellite data bolstered by their increasing accessibility is leading to many endeavors of automated monitoring of the earth's surface for various applications. Such applications demand high spatial resolution images at a temporal resolution of a few days which entails the challenge of processing a huge volume of image time series data. To overcome this computing bottleneck, we present PatchNet, a bespoke adaptation of beam search and attention mechanism. PatchNet is an automated patch selection neural network that requires only a partial spatial traversal of an image time series and yet achieves impressive results. Satellite systems face a trade-off between spatial and temporal resolutions due to budget/technical constraints e.g., Landsat-8/9 or Sentinel-2 have high spatial resolution whereas, MODIS has high temporal resolution. To deal with the limitation of coarse temporal resolution, we propose FuSITSNet, a twofold feature-based generic fusion model with multimodal learning in a contrastive setting. It produces a learned representation after fusion of two satellite image time series leveraging finer spatial resolution of Landsat and finer temporal resolution of MODIS. The patch alignment module of FuSITSNet aligns the PatchNet processed patches of Landsat-8 with the corresponding MODIS regions to incorporate its finer resolution temporal features. The untraversed patches are handled by the cross-modality attention which highlights additional hot spot features from the two modalities. We conduct extensive experiments on more than 2000 counties of US for crop yield, snow cover, and solar energy prediction and show that even one-fourth spatial processing of image time series produces state-of-the-art results. FuSITSNet outperforms the predictions of single modality and data obtained using existing generative fusion models and allows for monitoring of dynamic phenomena using freely accessible images, thereby unlocking new opportunities",
    "checked": true,
    "id": "5037a52d974e67a742775fc36b0ea32fe4651d77",
    "semantic_title": "efficient representation learning of satellite image time series and their fusion for spatiotemporal applications",
    "citation_count": 0,
    "authors": [
      "Poonam Goyal",
      "Arshveer Kaur",
      "Arvind Ram",
      "Navneet Goyal"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28687": {
    "title": "Rethinking Reverse Distillation for Multi-Modal Anomaly Detection",
    "volume": "main",
    "abstract": "In recent years, there has been significant progress in employing color images for anomaly detection in industrial scenarios, but it is insufficient for identifying anomalies that are invisible in RGB images alone. As a supplement, introducing extra modalities such as depth and surface normal maps can be helpful to detect these anomalies. To this end, we present a novel Multi-Modal Reverse Distillation (MMRD) paradigm that consists of a frozen multi-modal teacher encoder to generate distillation targets and a learnable student decoder targeting to restore multi-modal representations from the teacher. Specifically, the teacher extracts complementary visual features from different modalities via a siamese architecture and then parameter-freely fuses these information from multiple levels as the targets of distillation. For the student, it learns modality-related priors from the teacher representations of normal training data and performs interaction between them to form multi-modal representations for target reconstruction. Extensive experiments show that our MMRD outperforms recent state-of-the-art methods on both anomaly detection and localization on MVTec-3D AD and Eyecandies benchmarks. Codes will be available upon acceptance",
    "checked": true,
    "id": "28c86f0e18f8842a6316d32726f0b10c044af558",
    "semantic_title": "rethinking reverse distillation for multi-modal anomaly detection",
    "citation_count": 1,
    "authors": [
      "Zhihao Gu",
      "Jiangning Zhang",
      "Liang Liu",
      "Xu Chen",
      "Jinlong Peng",
      "Zhenye Gan",
      "Guannan Jiang",
      "Annan Shu",
      "Yabiao Wang",
      "Lizhuang Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28688": {
    "title": "LGMRec: Local and Global Graph Learning for Multimodal Recommendation",
    "volume": "main",
    "abstract": "The multimodal recommendation has gradually become the infrastructure of online media platforms, enabling them to provide personalized service to users through a joint modeling of user historical behaviors (e.g., purchases, clicks) and item various modalities (e.g., visual and textual). The majority of existing studies typically focus on utilizing modal features or modal-related graph structure to learn user local interests. Nevertheless, these approaches encounter two limitations: (1) Shared updates of user ID embeddings result in the consequential coupling between collaboration and multimodal signals; (2) Lack of exploration into robust global user interests to alleviate the sparse interaction problems faced by local interest modeling. To address these issues, we propose a novel Local and Global Graph Learning-guided Multimodal Recommender (LGMRec), which jointly models local and global user interests. Specifically, we present a local graph embedding module to independently learn collaborative-related and modality-related embeddings of users and items with local topological relations. Moreover, a global hypergraph embedding module is designed to capture global user and item embeddings by modeling insightful global dependency relations. The global embeddings acquired within the hypergraph embedding space can then be combined with two decoupled local embeddings to improve the accuracy and robustness of recommendations. Extensive experiments conducted on three benchmark datasets demonstrate the superiority of our LGMRec over various state-of-the-art recommendation baselines, showcasing its effectiveness in modeling both local and global user interests",
    "checked": true,
    "id": "1c89c27fc84aed68d281477128009a118b078149",
    "semantic_title": "lgmrec: local and global graph learning for multimodal recommendation",
    "citation_count": 3,
    "authors": [
      "Zhiqiang Guo",
      "Jianjun Li",
      "Guohui Li",
      "Chaoyang Wang",
      "Si Shi",
      "Bin Ruan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28689": {
    "title": "Intra- and Inter-group Optimal Transport for User-Oriented Fairness in Recommender Systems",
    "volume": "main",
    "abstract": "Recommender systems are typically biased toward a small group of users, leading to severe unfairness in recommendation performance, i.e., User-Oriented Fairness (UOF) issue. Existing research on UOF exhibits notable limitations in two phases of recommendation models. In the training phase, current methods fail to tackle the root cause of the UOF issue, which lies in the unfair training process between advantaged and disadvantaged users. In the evaluation phase, the current UOF metric lacks the ability to comprehensively evaluate varying cases of unfairness. In this paper, we aim to address the aforementioned limitations and ensure recommendation models treat user groups of varying activity levels equally. In the training phase, we propose a novel Intra- and Inter-GrOup Optimal Transport framework (II-GOOT) to alleviate the data sparsity problem for disadvantaged users and narrow the training gap between advantaged and disadvantaged users. In the evaluation phase, we introduce a novel metric called ?-UOF, which enables the identification and assessment of various cases of UOF. This helps prevent recommendation models from leading to unfavorable fairness outcomes, where both advantaged and disadvantaged users experience subpar recommendation performance. We conduct extensive experiments on three real-world datasets based on four backbone recommendation models to prove the effectiveness of ?-UOF and the efficiency of our proposed II-GOOT",
    "checked": true,
    "id": "8291c9ba9969edde4d069f27a3050047f751a6ba",
    "semantic_title": "intra- and inter-group optimal transport for user-oriented fairness in recommender systems",
    "citation_count": 0,
    "authors": [
      "Zhongxuan Han",
      "Chaochao Chen",
      "Xiaolin Zheng",
      "Meng Li",
      "Weiming Liu",
      "Binhui Yao",
      "Yuyuan Li",
      "Jianwei Yin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28690": {
    "title": "A Diffusion-Based Framework for Multi-Class Anomaly Detection",
    "volume": "main",
    "abstract": "Reconstruction-based approaches have achieved remarkable outcomes in anomaly detection. The exceptional image reconstruction capabilities of recently popular diffusion models have sparked research efforts to utilize them for enhanced reconstruction of anomalous images. Nonetheless, these methods might face challenges related to the preservation of image categories and pixel-wise structural integrity in the more practical multi-class setting. To solve the above problems, we propose a Difusion-based Anomaly Detection (DiAD) framework for multi-class anomaly detection, which consists of a pixel-space autoencoder, a latent-space Semantic-Guided (SG) network with a connection to the stable diffusion's denoising network, and a feature-space pre-trained feature extractor. Firstly, The SG network is proposed for reconstructing anomalous regions while preserving the original image's semantic information. Secondly, we introduce Spatial-aware Feature Fusion (SFF) block to maximize reconstruction accuracy when dealing with extensively reconstructed areas. Thirdly, the input and reconstructed images are processed by a pre-trained feature extractor to generate anomaly maps based on features extracted at different scales. Experiments on MVTec-AD and VisA datasets demonstrate the effectiveness of our approach which surpasses the state-of-the-art methods, e.g., achieving 96.8/52.6 and 97.2/99.0 (AUROC/AP) for localization and detection respectively on multi-class MVTec-AD dataset. Code will be available at https://lewandofskee.github.io/projects/diad",
    "checked": true,
    "id": "b60ceba95cf7fc2040da44aad3aa4424d2dba0d3",
    "semantic_title": "a diffusion-based framework for multi-class anomaly detection",
    "citation_count": 6,
    "authors": [
      "Haoyang He",
      "Jiangning Zhang",
      "Hongxu Chen",
      "Xuhai Chen",
      "Zhishan Li",
      "Xu Chen",
      "Yabiao Wang",
      "Chengjie Wang",
      "Lei Xie"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28691": {
    "title": "ADA-GAD: Anomaly-Denoised Autoencoders for Graph Anomaly Detection",
    "volume": "main",
    "abstract": "Graph anomaly detection is crucial for identifying nodes that deviate from regular behavior within graphs, benefiting various domains such as fraud detection and social network. Although existing reconstruction-based methods have achieved considerable success, they may face the Anomaly Overfitting and Homophily Trap problems caused by the abnormal patterns in the graph, breaking the assumption that normal nodes are often better reconstructed than abnormal ones. Our observations indicate that models trained on graphs with fewer anomalies exhibit higher detection performance. Based on this insight, we introduce a novel two-stage framework called Anomaly-Denoised Autoencoders for Graph Anomaly Detection (ADA-GAD). In the first stage, we design a learning-free anomaly-denoised augmentation method to generate graphs with reduced anomaly levels. We pretrain graph autoencoders on these augmented graphs at multiple levels, which enables the graph autoencoders to capture normal patterns. In the next stage, the decoders are retrained for detection on the original graph, benefiting from the multi-level representations learned in the previous stage. Meanwhile, we propose the node anomaly distribution regularization to further alleviate Anomaly Overfitting. We validate the effectiveness of our approach through extensive experiments on both synthetic and real-world datasets",
    "checked": true,
    "id": "694ccf3e5ebeef403f085318623925b516ca676a",
    "semantic_title": "ada-gad: anomaly-denoised autoencoders for graph anomaly detection",
    "citation_count": 1,
    "authors": [
      "Junwei He",
      "Qianqian Xu",
      "Yangbangyan Jiang",
      "Zitai Wang",
      "Qingming Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28692": {
    "title": "ViSTec: Video Modeling for Sports Technique Recognition and Tactical Analysis",
    "volume": "main",
    "abstract": "The immense popularity of racket sports has fueled substantial demand in tactical analysis with broadcast videos. However, existing manual methods require laborious annotation, and recent attempts leveraging video perception models are limited to low-level annotations like ball trajectories, overlooking tactics that necessitate an understanding of stroke techniques. State-of-the-art action segmentation models also struggle with technique recognition due to frequent occlusions and motion-induced blurring in racket sports videos. To address these challenges, We propose ViSTec, a Video-based Sports Technique recognition model inspired by human cognition that synergizes sparse visual data with rich contextual insights. Our approach integrates a graph to explicitly model strategic knowledge in stroke sequences and enhance technique recognition with contextual inductive bias. A two-stage action perception model is jointly trained to align with the contextual knowledge in the graph. Experiments demonstrate that our method outperforms existing models by a significant margin. Case studies with experts from the Chinese national table tennis team validate our model's capacity to automate analysis for technical actions and tactical strategies. More details are available at: https://ViSTec2024.github.io/",
    "checked": true,
    "id": "2f8d236b0a04150d7e9c607e56e1734c48fab5b8",
    "semantic_title": "vistec: video modeling for sports technique recognition and tactical analysis",
    "citation_count": 0,
    "authors": [
      "Yuchen He",
      "Zeqing Yuan",
      "Yihong Wu",
      "Liqi Cheng",
      "Dazhen Deng",
      "Yingcai Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28693": {
    "title": "Label Attentive Distillation for GNN-Based Graph Classification",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs) have emerged as a powerful tool for modeling graph-structured data, exhibiting remarkable potential in applications such as social networks, recommendation systems, and molecular structures. However, the conventional GNNs perform node-level feature aggregation from neighbors without considering graph-label information, which leads to the misaligned embedding problem that may cause a detrimental effect on graph-level tasks such as graph classification. In this paper, we propose a novel label-attentive distillation method called LAD-GNN for graph representation learning to solve this problem. It alternatively trains a teacher model and a student GNN with a distillation-based approach. In the teacher model, a label-attentive encoder is proposed to encode the label information fusing with the node features to generate ideal embedding. In the student model, the ideal embedding is used as intermediate supervision to urge the student GNN to learn class-friendly node embedding to facilitate graph-level tasks. Generally, LAD-GNN is an enhanced GNN training approach that can be incorporated with arbitrary GNN backbone to improve performance without significant increase of computational cost. Extensive experiments with 7 GNN backbones based on 10 benchmark datasets show that LAD-GNN improves the SOTA GNNs in graph classification accuracy. The source codes of LAD-GNN are publicly available on https://github.com/XiaobinHong/LAD-GNN",
    "checked": true,
    "id": "66f189f73a26426caa79b9317486ede50b4d57ff",
    "semantic_title": "label attentive distillation for gnn-based graph classification",
    "citation_count": 0,
    "authors": [
      "Xiaobin Hong",
      "Wenzhong Li",
      "Chaoqun Wang",
      "Mingkai Lin",
      "Sanglu Lu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28694": {
    "title": "DAG-Aware Variational Autoencoder for Social Propagation Graph Generation",
    "volume": "main",
    "abstract": "Propagation models in social networks are critical, with extensive applications across various fields and downstream tasks. However, existing propagation models are often oversimplified, scenario-specific, and lack real-world user social attributes. These limitations detaching from real-world analysis lead to inaccurate representations of the propagation process in social networks. To address these issues, we propose a User Features Attention-based DAG-Aware Variational Autoencoder (DAVA) for propagation graph generation. First, nearly 1 million pieces of user attributes data are collected. Then DAVA can integrate the analysis of propagation graph topology and corresponding user attributes as prior knowledge. By leveraging a lightweight attention-based framework and a sliding window mechanism based on BFS permutations weighted by user influence, DAVA significantly enhances the ability to generate realistic, large-scale propagation data, yielding graph scales ten times greater than those produced by existing SOTA methods. Every module of DAVA has flexibility and extension that allows for easy substitution to suit other generation tasks. Additionally, we provide a comprehensive evaluation of DAVA, one focus is the effectiveness of generated data in improving the performance of downstream tasks. During the generation process, we discover the Credibility Erosion Effect by modifying the generation rules, revealing a social phenomenon in social network propagation",
    "checked": true,
    "id": "de80d4056d7958c1c11dd1a17779487dad12ca21",
    "semantic_title": "dag-aware variational autoencoder for social propagation graph generation",
    "citation_count": 0,
    "authors": [
      "Dongpeng Hou",
      "Chao Gao",
      "Xuelong Li",
      "Zhen Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28695": {
    "title": "Social-Aware Group Display Configuration in VR Conference",
    "volume": "main",
    "abstract": "Virtual Reality (VR) has emerged due to advancements in hardware and computer graphics. During the pandemic, conferences and exhibitions leveraging VR have gained attention. However, large-scale VR conferences, face a significant problem not yet studied in the literature -- displaying too many irrelevant users on the screen which may negatively impact the user experience. To address this issue, we formulate a new research problem, Social-Aware VR Conference Group Display Configuration (SVGD). Accordingly, we design the Social Utility-Aware VR Conference Group Formation (SVC) algorithm, which is a 2-approximation algorithm to SVGD. SVC iteratively selects either the P-Configuration or S-Configuration based on their effective ratios. This ensures that in each iteration, SVC identifies and chooses the solution with the highest current effectiveness. Experiments on real metaverse datasets show that the proposed SVC outperforms 11 baselines by 75% in terms of solution quality",
    "checked": true,
    "id": "737ab089c1c5a4282bffa5ba86d377daf3d3e6db",
    "semantic_title": "social-aware group display configuration in vr conference",
    "citation_count": 0,
    "authors": [
      "Bay-Yuan Hsu",
      "Chih-Ya Shen",
      "Hao Shan Yuan",
      "Wang-Chien  Lee",
      "De-Nian Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28696": {
    "title": "AnomalyDiffusion: Few-Shot Anomaly Image Generation with Diffusion Model",
    "volume": "main",
    "abstract": "Anomaly inspection plays an important role in industrial manufacture. Existing anomaly inspection methods are limited in their performance due to insufficient anomaly data. Although anomaly generation methods have been proposed to augment the anomaly data, they either suffer from poor generation authenticity or inaccurate alignment between the generated anomalies and masks. To address the above problems, we propose AnomalyDiffusion, a novel diffusion-based few-shot anomaly generation model, which utilizes the strong prior information of latent diffusion model learned from large-scale dataset to enhance the generation authenticity under few-shot training data. Firstly, we propose Spatial Anomaly Embedding, which consists of a learnable anomaly embedding and a spatial embedding encoded from an anomaly mask, disentangling the anomaly information into anomaly appearance and location information. Moreover, to improve the alignment between the generated anomalies and the anomaly masks, we introduce a novel Adaptive Attention Re-weighting Mechanism. Based on the disparities between the generated anomaly image and normal sample, it dynamically guides the model to focus more on the areas with less noticeable generated anomalies, enabling generation of accurately-matched anomalous image-mask pairs. Extensive experiments demonstrate that our model significantly outperforms the state-of-the-art methods in generation authenticity and diversity, and effectively improves the performance of downstream anomaly inspection tasks. The code and data are available in https://github.com/sjtuplayer/anomalydiffusion",
    "checked": true,
    "id": "edbda62d15b423526ea0b798aeb9c1150877569b",
    "semantic_title": "anomalydiffusion: few-shot anomaly image generation with diffusion model",
    "citation_count": 15,
    "authors": [
      "Teng Hu",
      "Jiangning Zhang",
      "Ran Yi",
      "Yuzhen Du",
      "Xu Chen",
      "Liang Liu",
      "Yabiao Wang",
      "Chengjie Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28697": {
    "title": "Learning Time Slot Preferences via Mobility Tree for Next POI Recommendation",
    "volume": "main",
    "abstract": "Next Point-of-Interests (POIs) recommendation task aims to provide a dynamic ranking of POIs based on users' current check-in trajectories. The recommendation performance of this task is contingent upon a comprehensive understanding of users' personalized behavioral patterns through Location-based Social Networks (LBSNs) data. While prior studies have adeptly captured sequential patterns and transitional relationships within users' check-in trajectories, a noticeable gap persists in devising a mechanism for discerning specialized behavioral patterns during distinct time slots, such as noon, afternoon, or evening. In this paper, we introduce an innovative data structure termed the ``Mobility Tree'', tailored for hierarchically describing users' check-in records. The Mobility Tree encompasses multi-granularity time slot nodes to learn user preferences across varying temporal periods. Meanwhile, we propose the Mobility Tree Network (MTNet), a multitask framework for personalized preference learning based on Mobility Trees. We develop a four-step node interaction operation to propagate feature information from the leaf nodes to the root node. Additionally, we adopt a multitask training strategy to push the model towards learning a robust representation. The comprehensive experimental results demonstrate the superiority of MTNet over eleven state-of-the-art next POI recommendation models across three real-world LBSN datasets, substantiating the efficacy of time slot preference learning facilitated by Mobility Tree",
    "checked": true,
    "id": "aa89cc8888d0f7334dc0e0bfa6422dc4643be05e",
    "semantic_title": "learning time slot preferences via mobility tree for next poi recommendation",
    "citation_count": 0,
    "authors": [
      "Tianhao Huang",
      "Xuan Pan",
      "Xiangrui Cai",
      "Ying Zhang",
      "Xiaojie Yuan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28698": {
    "title": "ReGCL: Rethinking Message Passing in Graph Contrastive Learning",
    "volume": "main",
    "abstract": "Graph contrastive learning (GCL) has demonstrated remarkable efficacy in graph representation learning. However, previous studies have overlooked the inherent conflict that arises when employing graph neural networks (GNNs) as encoders for node-level contrastive learning. This conflict pertains to the partial incongruity between the feature aggregation mechanism of graph neural networks and the embedding distinction characteristic of contrastive learning. Theoretically, to investigate the location and extent of the conflict, we analyze the participation of message-passing from the gradient perspective of InfoNCE loss. Different from contrastive learning in other domains, the conflict in GCL arises due to the presence of certain samples that contribute to both the gradients of positive and negative simultaneously under the manner of message passing, which are opposite optimization directions. To further address the conflict issue, we propose a practical framework called ReGCL, which utilizes theoretical findings of GCL gradients to effectively improve graph contrastive learning. Specifically, two gradient-based strategies are devised in terms of both message passing and loss function to mitigate the conflict. Firstly, a gradient-guided structure learning method is proposed in order to acquire a structure that is adapted to contrastive learning principles. Secondly, a gradient-weighted InfoNCE loss function is designed to reduce the impact of false negative samples with high probabilities, specifically from the standpoint of the graph encoder. Extensive experiments demonstrate the superiority of the proposed method in comparison to state-of-the-art baselines across various node classification benchmarks",
    "checked": true,
    "id": "edfbdb6c8370c744f4616556cfa9f8f4d7d2804b",
    "semantic_title": "regcl: rethinking message passing in graph contrastive learning",
    "citation_count": 0,
    "authors": [
      "Cheng Ji",
      "Zixuan Huang",
      "Qingyun Sun",
      "Hao Peng",
      "Xingcheng Fu",
      "Qian Li",
      "Jianxin Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28699": {
    "title": "D3: A Methodological Exploration of Domain Division, Modeling, and Balance in Multi-Domain Recommendations",
    "volume": "main",
    "abstract": "To enhance the efficacy of multi-scenario services in industrial recommendation systems, the emergence of multi-domain recommendation has become prominent, which entails simultaneous modeling of all domains through a unified model, effectively capturing commonalities and differences among them. However, current methods rely on manual domain partitioning, which overlook the intricate domain relationships and the heterogeneity of different domains during joint optimization, hindering the integration of domain commonalities and differences. To address these challenges, this paper proposes a universal and flexible framework D3 aimed at optimizing the multi-domain recommendation pipeline from three key aspects. Firstly, an attention-based domain adaptation module is introduced to automatically identify and incorporate domain-sensitive features during training. Secondly, we propose a fusion gate module that enables the seamless integration of commonalities and diversities among domains, allowing for implicit characterization of intricate domain relationships. Lastly, we tackle the issue of joint optimization by deriving loss weights from two complementary viewpoints: domain complexity and domain specificity, alleviating inconsistencies among different domains during the training phase. Experiments on three public datasets demonstrate the effectiveness and superiority of our proposed framework. In addition, D3 has been implemented on a real-life, high-traffic internet platform catering to millions of users daily",
    "checked": true,
    "id": "56503e479a8514dafaad423410286d3526bd67c8",
    "semantic_title": "d3: a methodological exploration of domain division, modeling, and balance in multi-domain recommendations",
    "citation_count": 1,
    "authors": [
      "Pengyue Jia",
      "Yichao Wang",
      "Shanru Lin",
      "Xiaopeng Li",
      "Xiangyu Zhao",
      "Huifeng Guo",
      "Ruiming Tang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28700": {
    "title": "Graph Invariant Learning with Subgraph Co-mixup for Out-of-Distribution Generalization",
    "volume": "main",
    "abstract": "Graph neural networks (GNNs) have been demonstrated to perform well in graph representation learning, but always lacking in generalization capability when tackling out-of-distribution (OOD) data. Graph invariant learning methods, backed by the invariance principle among defined multiple environments, have shown effectiveness in dealing with this issue. However, existing methods heavily rely on well-predefined or accurately generated environment partitions, which are hard to be obtained in practice, leading to sub-optimal OOD generalization performances. In this paper, we propose a novel graph invariant learning method based on invariant and variant patterns co-mixup strategy, which is capable of jointly generating mixed multiple environments and capturing invariant patterns from the mixed graph data. Specifically, we first adopt a subgraph extractor to identify invariant subgraphs. Subsequently, we design one novel co-mixup strategy, i.e., jointly conducting environment mixup and invariant mixup. For the environment mixup, we mix the variant environment-related subgraphs so as to generate sufficiently diverse multiple environments, which is important to guarantee the quality of the graph invariant learning. For the invariant mixup, we mix the invariant subgraphs, further encouraging to capture invariant patterns behind graphs while getting rid of spurious correlations for OOD generalization. We demonstrate that the proposed environment mixup and invariant mixup can mutually promote each other. Extensive experiments on both synthetic and real-world datasets demonstrate that our method significantly outperforms state-of-the-art under various distribution shifts",
    "checked": true,
    "id": "42692eb4668728d71a8dd0b6bd45e0b279054283",
    "semantic_title": "graph invariant learning with subgraph co-mixup for out-of-distribution generalization",
    "citation_count": 5,
    "authors": [
      "Tianrui Jia",
      "Haoyang Li",
      "Cheng Yang",
      "Tao Tao",
      "Chuan Shi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28701": {
    "title": "Enhancing Multi-Scale Diffusion Prediction via Sequential Hypergraphs and Adversarial Learning",
    "volume": "main",
    "abstract": "Information diffusion prediction plays a crucial role in understanding the propagation of information in social networks, encompassing both macroscopic and microscopic prediction tasks. Macroscopic prediction estimates the overall impact of information diffusion, while microscopic prediction focuses on identifying the next user to be influenced. While prior research often concentrates on one of these aspects, a few tackle both concurrently. These two tasks provide complementary insights into the diffusion process at different levels, revealing common traits and unique attributes. The exploration of leveraging common features across these tasks to enhance information prediction remains an underexplored avenue. In this paper, we propose an intuitive and effective model that addresses both macroscopic and microscopic prediction tasks. Our approach considers the interactions and dynamics among cascades at the macro level and incorporates the social homophily of users in social networks at the micro level. Additionally, we introduce adversarial training and orthogonality constraints to ensure the integrity of shared features. Experimental results on four datasets demonstrate that our model significantly outperforms state-of-the-art methods",
    "checked": true,
    "id": "a1e23cc5b8d054d01a30a67dc7c33cd0fd49df5b",
    "semantic_title": "enhancing multi-scale diffusion prediction via sequential hypergraphs and adversarial learning",
    "citation_count": 0,
    "authors": [
      "Pengfei Jiao",
      "Hongqian Chen",
      "Qing Bao",
      "Wang Zhang",
      "Huaming Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28702": {
    "title": "Multi-Domain Recommendation to Attract Users via Domain Preference Modeling",
    "volume": "main",
    "abstract": "Recently, web platforms are operating various service domains simultaneously. Targeting a platform that operates multiple service domains, we introduce a new task, Multi-Domain Recommendation to Attract Users (MDRAU), which recommends items from multiple ``unseen'' domains with which each user has not interacted yet, by using knowledge from the user's ``seen'' domains. In this paper, we point out two challenges of MDRAU task. First, there are numerous possible combinations of mappings from seen to unseen domains because users have usually interacted with a different subset of service domains. Second, a user might have different preference for each of the target unseen domains, which requires recommendations to reflect users' preference on domains as well as items. To tackle these challenges, we propose DRIP framework that models users' preference at two levels (i.e., domain and item) and learns various seen-unseen domain mappings in a unified way with masked domain modeling. Our extensive experiments demonstrate the effectiveness of DRIP in MDRAU task and its ability to capture users' domain-level preferences",
    "checked": true,
    "id": "4fd714b70648e39eb2946e45d2e22a9b8e82764f",
    "semantic_title": "multi-domain recommendation to attract users via domain preference modeling",
    "citation_count": 1,
    "authors": [
      "Hyunjun Ju",
      "SeongKu Kang",
      "Dongha Lee",
      "Junyoung Hwang",
      "Sanghwan Jang",
      "Hwanjo Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28703": {
    "title": "Few Shot Part Segmentation Reveals Compositional Logic for Industrial Anomaly Detection",
    "volume": "main",
    "abstract": "Logical anomalies (LA) refer to data violating underlying logical constraints e.g., the quantity, arrangement, or composition of components within an image. Detecting accurately such anomalies requires models to reason about various component types through segmentation. However, curation of pixel-level annotations for semantic segmentation is both time-consuming and expensive. Although there are some prior few-shot or unsupervised co-part segmentation algorithms, they often fail on images with industrial object. These images have components with similar textures and shapes, and a precise differentiation proves challenging. In this study, we introduce a novel component segmentation model for LA detection that leverages a few labeled samples and unlabeled images sharing logical constraints. To ensure consistent segmentation across unlabeled images, we employ a histogram matching loss in conjunction with an entropy loss. As segmentation predictions play a crucial role, we propose to enhance both local and global sample validity detection by capturing key aspects from visual semantics via three memory banks: class histograms, component composition embeddings and patch-level representations. For effective LA detection, we propose an adaptive scaling strategy to standardize anomaly scores from different memory banks in inference. Extensive experiments on the public benchmark MVTec LOCO AD reveal our method achieves 98.1% AUROC in LA detection vs. 89.6% from competing methods",
    "checked": true,
    "id": "0feee73010309fde08e0ec2a334e122aac4a51c0",
    "semantic_title": "few shot part segmentation reveals compositional logic for industrial anomaly detection",
    "citation_count": 2,
    "authors": [
      "Soopil Kim",
      "Sion An",
      "Philip Chikontwe",
      "Myeongkyun Kang",
      "Ehsan Adeli",
      "Kilian M. Pohl",
      "Sang Hyun Park"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28704": {
    "title": "VITA: ‘Carefully Chosen and Weighted Less' Is Better in Medication Recommendation",
    "volume": "main",
    "abstract": "We address the medication recommendation problem, which aims to recommend effective medications for a patient's current visit by utilizing information (e.g., diagnoses and procedures) given at the patient's current and past visits. While there exist a number of recommender systems designed for this problem, we point out that they are challenged in accurately capturing the relation (spec., the degree of relevance) between the current and each of the past visits for the patient when obtaining her current health status, which is the basis for recommending medications. To address this limitation, we propose a novel medication recommendation framework, named VITA, based on the following two novel ideas: (1) relevant-Visit selectIon; (2) Target-aware Attention. Through extensive experiments using real-world datasets, we demonstrate the superiority of VITA (spec., up to 5.67% higher accuracy, in terms of Jaccard, than the best competitor) and the effectiveness of its two core ideas. The code is available at https://github.com/jhheo0123/VITA",
    "checked": false,
    "id": "e718c2b9268b4ab7c76cc3cabe2dca28cab5d022",
    "semantic_title": "vita: 'carefully chosen and weighted less' is better in medication recommendation",
    "citation_count": 1,
    "authors": [
      "Taeri Kim",
      "Jiho Heo",
      "Hongil Kim",
      "Kijung Shin",
      "Sang-Wook Kim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28705": {
    "title": "Optimal Quasi-clique: Hardness, Equivalence with Densest-k-Subgraph, and Quasi-partitioned Community Mining",
    "volume": "main",
    "abstract": "Dense subgraph discovery (DSD) is a key primitive in graph mining that typically deals with extracting cliques and near-cliques. In this paper, we revisit the optimal quasi-clique (OQC) formulation for DSD and establish that it is NP--hard. In addition, we reveal the hitherto unknown property that OQC can be used to explore the entire spectrum of densest subgraphs of all distinct sizes by appropriately varying a single hyperparameter, thereby forging an intimate link with the classic densest-k-subgraph problem (DkS). We corroborate these findings on real-world graphs by applying the simple greedy algorithm for OQC with improved hyperparameter tuning, to quickly generate high-quality approximations of the size-density frontier. Our findings indicate that OQC not only extracts high quality (near)-cliques, but also large and loosely-connected subgraphs that exhibit well defined local community structure. The latter discovery is particularly intriguing, since OQC is not explicitly geared towards community detection",
    "checked": true,
    "id": "707da1c7e1746cec80f7ec3c418a53f9b717c479",
    "semantic_title": "optimal quasi-clique: hardness, equivalence with densest-k-subgraph, and quasi-partitioned community mining",
    "citation_count": 1,
    "authors": [
      "Aritra Konar",
      "Nicholas D. Sidiropoulos"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28706": {
    "title": "Learning Persistent Community Structures in Dynamic Networks via Topological Data Analysis",
    "volume": "main",
    "abstract": "Dynamic community detection methods often lack effective mechanisms to ensure temporal consistency, hindering the analysis of network evolution. In this paper, we propose a novel deep graph clustering framework with temporal consistency regularization on inter-community structures, inspired by the concept of minimal network topological changes within short intervals. Specifically, to address the representation collapse problem, we first introduce MFC, a matrix factorization-based deep graph clustering algorithm that preserves node embedding. Based on static clustering results, we construct probabilistic community networks and compute their persistence homology, a robust topological measure, to assess structural similarity between them. Moreover, a novel neural network regularization TopoReg is introduced to ensure the preservation of topological similarity between inter-community structures over time intervals. Our approach enhances temporal consistency and clustering accuracy on real-world datasets with both fixed and varying numbers of communities. It is also a pioneer application of TDA in temporally persistent community detection, offering an insightful contribution to field of network analysis. Code and data are available at the public git repository: https://github.com/kundtx/MFC-TopoReg",
    "checked": true,
    "id": "0b64660223a8dd709177104965b79da055b4624f",
    "semantic_title": "learning persistent community structures in dynamic networks via topological data analysis",
    "citation_count": 0,
    "authors": [
      "Dexu  Kong",
      "Anping Zhang",
      "Yang Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28707": {
    "title": "Spatio-Temporal Pivotal Graph Neural Networks for Traffic Flow Forecasting",
    "volume": "main",
    "abstract": "Traffic flow forecasting is a classical spatio-temporal data mining problem with many real-world applications. Recently, various methods based on Graph Neural Networks (GNN) have been proposed for the problem and achieved impressive prediction performance. However, we argue that the majority of existing methods disregarding the importance of certain nodes (referred to as pivotal nodes) that naturally exhibit extensive connections with multiple other nodes. Predicting on pivotal nodes poses a challenge due to their complex spatio-temporal dependencies compared to other nodes. In this paper, we propose a novel GNN-based method called Spatio-Temporal Pivotal Graph Neural Networks (STPGNN) to address the above limitation. We introduce a pivotal node identification module for identifying pivotal nodes. We propose a novel pivotal graph convolution module, enabling precise capture of spatio-temporal dependencies centered around pivotal nodes. Moreover, we propose a parallel framework capable of extracting spatio-temporal traffic features on both pivotal and non-pivotal nodes. Experiments on seven real-world traffic datasets verify our proposed method's effectiveness and efficiency compared to state-of-the-art baselines",
    "checked": true,
    "id": "e30f3e3ccb2c647a2caeaa362ac8d4b5ef67727f",
    "semantic_title": "spatio-temporal pivotal graph neural networks for traffic flow forecasting",
    "citation_count": 1,
    "authors": [
      "Weiyang Kong",
      "Ziyu Guo",
      "Yubao Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28708": {
    "title": "Knowledge-Aware Explainable Reciprocal Recommendation",
    "volume": "main",
    "abstract": "Reciprocal recommender systems (RRS) have been widely used in online platforms such as online dating and recruitment. They can simultaneously fulfill the needs of both parties involved in the recommendation process. Due to the inherent nature of the task, interaction data is relatively sparse compared to other recommendation tasks. Existing works mainly address this issue through content-based recommendation methods. However, these methods often implicitly model textual information from a unified perspective, making it challenging to capture the distinct intentions held by each party, which further leads to limited performance and the lack of interpretability. In this paper, we propose a Knowledge-Aware Explainable Reciprocal Recommender System (KAERR), which models metapaths between two parties independently, considering their respective perspectives and requirements. Various metapaths are fused using an attention-based mechanism, where the attention weights unveil dual-perspective preferences and provide recommendation explanations for both parties. Extensive experiments on two real-world datasets from diverse scenarios demonstrate that the proposed model outperforms state-of-the-art baselines, while also delivering compelling reasons for recommendations to both parties",
    "checked": true,
    "id": "58d2af76a4072659d4956652d5fe791abc6c719f",
    "semantic_title": "knowledge-aware explainable reciprocal recommendation",
    "citation_count": 0,
    "authors": [
      "Kai-Huang Lai",
      "Zhe-Rui Yang",
      "Pei-Yuan Lai",
      "Chang-Dong Wang",
      "Mohsen  Guizani ",
      "Min Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28709": {
    "title": "Adaptive Hardness Negative Sampling for Collaborative Filtering",
    "volume": "main",
    "abstract": "Negative sampling is essential for implicit collaborative filtering to provide proper negative training signals so as to achieve desirable performance. We experimentally unveil a common limitation of all existing negative sampling methods that they can only select negative samples of a fixed hardness level, leading to the false positive problem (FPP) and false negative problem (FNP). We then propose a new paradigm called adaptive hardness negative sampling (AHNS) and discuss its three key criteria. By adaptively selecting negative samples with appropriate hardnesses during the training process, AHNS can well mitigate the impacts of FPP and FNP. Next, we present a concrete instantiation of AHNS called AHNS_{p<0}, and theoretically demonstrate that AHNS_{p<0} can fit the three criteria of AHNS well and achieve a larger lower bound of normalized discounted cumulative gain. Besides, we note that existing negative sampling methods can be regarded as more relaxed cases of AHNS. Finally, we conduct comprehensive experiments, and the results show that AHNS_{p<0} can consistently and substantially outperform several state-of-the-art competitors on multiple datasets",
    "checked": true,
    "id": "6eb29f6b204b22b0a1eac2a108edf37a04036102",
    "semantic_title": "adaptive hardness negative sampling for collaborative filtering",
    "citation_count": 3,
    "authors": [
      "Riwei Lai",
      "Rui Chen",
      "Qilong Han",
      "Chi Zhang",
      "Li Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28710": {
    "title": "MDFL: Multi-Domain Diffusion-Driven Feature Learning",
    "volume": "main",
    "abstract": "High-dimensional images, known for their rich semantic information, are widely applied in remote sensing and other fields. The spatial information in these images reflects the object's texture features, while the spectral information reveals the potential spectral representations across different bands. Currently, the understanding of high-dimensional images remains limited to a single-domain perspective with performance degradation. Motivated by the masking texture effect observed in the human visual system, we present a multi-domain diffusion-driven feature learning network (MDFL) , a scheme to redefine the effective information domain that the model really focuses on. This method employs diffusion-based posterior sampling to explicitly consider joint information interactions between the high-dimensional manifold structures in the spectral, spatial, and frequency domains, thereby eliminating the influence of masking texture effects in visual models. Additionally, we introduce a feature reuse mechanism to gather deep and raw features of high-dimensional data. We demonstrate that MDFL significantly improves the feature extraction performance of high-dimensional data, thereby providing a powerful aid for revealing the intrinsic patterns and structures of such data. The experimental results on three multi-modal remote sensing datasets show that MDFL reaches an average overall accuracy of 98.25%, outperforming various state-of-the-art baseline schemes. Code available at https://github.com/LDXDU/MDFL-AAAI-24",
    "checked": true,
    "id": "f9e55769bda7d7b7f70dd67f41324962211ca7de",
    "semantic_title": "mdfl: multi-domain diffusion-driven feature learning",
    "citation_count": 2,
    "authors": [
      "Daixun Li",
      "Weiying Xie",
      "Jiaqing Zhang",
      "Yunsong Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28711": {
    "title": "CoreRec: A Counterfactual Correlation Inference for Next Set Recommendation",
    "volume": "main",
    "abstract": "Next set recommendation aims to predict the items that are likely to be bought in the next purchase. Central to this endeavor is the task of capturing intra-set and cross-set correlations among items. However, the modeling of cross-set correlations poses challenges due to specific issues. Primarily, these correlations are often implicit, and the prevailing approach of establishing an indiscriminate link across the entire set of objects neglects factors like purchase frequency and correlations between purchased items. Such hastily formed connections across sets introduce substantial noise. Additionally, the preeminence of high-frequency items in numerous sets could potentially overshadow and distort correlation modeling with respect to low-frequency items. Thus, we devoted to mitigating misleading inter-set correlations. With a fresh perspective rooted in causality, we delve into the question of whether correlations between a particular item and items from other sets should be relied upon for item representation learning and set prediction. Technically, we introduce the Counterfactual Correlation Inference framework for next set recommendation, denoted as CoreRec. This framework establishes a counterfactual scenario in which the recommendation model impedes cross-set correlations to generate intervened predictions. By contrasting these intervened predictions with the original ones, we gauge the causal impact of inter-set neighbors on set prediction—essentially assessing whether they contribute to spurious correlations. During testing, we introduce a post-trained switch module that selects between set-aware item representations derived from either the original or the counterfactual scenarios. To validate our approach, we extensively experiment using three real-world datasets, affirming both the effectiveness of CoreRec and the cogency of our analytical approach",
    "checked": true,
    "id": "e4d9d5aa83e23f7d44a3dc57994f919f01d045e0",
    "semantic_title": "corerec: a counterfactual correlation inference for next set recommendation",
    "citation_count": 0,
    "authors": [
      "Kexin Li",
      "Chengjiang Long",
      "Shengyu Zhang",
      "Xudong Tang",
      "Zhichao Zhai",
      "Kun Kuang",
      "Jun Xiao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28712": {
    "title": "Ada-Retrieval: An Adaptive Multi-Round Retrieval Paradigm for Sequential Recommendations",
    "volume": "main",
    "abstract": "Retrieval models aim at selecting a small set of item candidates which match the preference of a given user. They play a vital role in large-scale recommender systems since subsequent models such as rankers highly depend on the quality of item candidates. However, most existing retrieval models employ a single-round inference paradigm, which may not adequately capture the dynamic nature of user preferences and stuck in one area in the item space. In this paper, we propose Ada-Retrieval, an adaptive multi-round retrieval paradigm for recommender systems that iteratively refines user representations to better capture potential candidates in the full item space. Ada-Retrieval comprises two key modules: the item representation adapter and the user representation adapter, designed to inject context information into items' and users' representations. The framework maintains a model-agnostic design, allowing seamless integration with various backbone models such as RNNs or Transformers. We perform experiments on three widely used public datasets, incorporating five powerful sequential recommenders as backbone models. Our results demonstrate that Ada-Retrieval significantly enhances the performance of various base models, with consistent improvements observed across different datasets. Our code and data are publicly available at: https://github.com/ll0ruc/Ada-Retrieval",
    "checked": true,
    "id": "c8be020ecdd38823b076e6deb60a5b4f7c3deaf0",
    "semantic_title": "ada-retrieval: an adaptive multi-round retrieval paradigm for sequential recommendations",
    "citation_count": 0,
    "authors": [
      "Lei Li",
      "Jianxun Lian",
      "Xiao Zhou",
      "Xing Xie"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28713": {
    "title": "CONSIDER: Commonalities and Specialties Driven Multilingual Code Retrieval Framework",
    "volume": "main",
    "abstract": "Multilingual code retrieval aims to find code snippets relevant to a user's query from a multilingual codebase, which plays a crucial role in software development and expands their application scenarios compared to classical monolingual code retrieval. Despite the performance improvements achieved by previous studies, two crucial problems are overlooked in the multilingual scenario. First, certain programming languages face data scarcity in specific domains, resulting in limited representation capabilities within those domains. Second, different programming languages can be used interchangeably within the same domain, making it challenging for multilingual models to accurately identify the intended programming language of a user's query. To address these issues, we propose the CommONalities and SpecIalties Driven Multilingual CodE Retrieval Framework (CONSIDER), which includes two modules. The first module enhances the representation of various programming languages by modeling pairwise and global commonalities among them. The second module introduces a novel contrastive learning negative sampling algorithm that leverages language confusion to automatically extract specific language features. Through our experiments, we confirm the significant benefits of our model in real-world multilingual code retrieval scenarios in various aspects. Furthermore, an evaluation demonstrates the effectiveness of our proposed CONSIDER framework in monolingual scenarios as well. Our source code is available at https://github.com/smsquirrel/consider",
    "checked": true,
    "id": "3089c2ed18f40d2a3a27397d6358f9afdbfe20b3",
    "semantic_title": "consider: commonalities and specialties driven multilingual code retrieval framework",
    "citation_count": 0,
    "authors": [
      "Rui Li",
      "Liyang He",
      "Qi Liu",
      "Yuze Zhao",
      "Zheng Zhang",
      "Zhenya Huang",
      "Yu Su",
      "Shijin Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28714": {
    "title": "UniGen: A Unified Generative Framework for Retrieval and Question Answering with Large Language Models",
    "volume": "main",
    "abstract": "Generative information retrieval, encompassing two major tasks of Generative Document Retrieval (GDR) and Grounded Answer Generation (GAR), has gained significant attention in natural language processing. Existing methods for GDR and GAR rely on separate retrieval and reader modules, which hinder simultaneous optimization. To overcome this, we present UniGen, a Unified Generative framework for retrieval and question answering that integrates both tasks into a single generative model leveraging the capabilities of large language models. UniGen employs a shared encoder and two distinct decoders for generative retrieval and question answering. To facilitate the learning of both tasks, we introduce connectors, generated by large language models, to bridge the gaps between query inputs and generation targets, as well as between document identifiers and answers. Furthermore, we propose an iterative enhancement strategy that leverages generated answers and retrieved documents to iteratively improve both tasks. Through extensive experiments on the MS MARCO and NQ datasets, we demonstrate the effectiveness of UniGen, showcasing its superior performance in both retrieval and question answering tasks",
    "checked": true,
    "id": "b51481f3d6082f3536991128ef9091a262eb6b5e",
    "semantic_title": "unigen: a unified generative framework for retrieval and question answering with large language models",
    "citation_count": 1,
    "authors": [
      "Xiaoxi Li",
      "Yujia Zhou",
      "Zhicheng Dou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28715": {
    "title": "MESED: A Multi-Modal Entity Set Expansion Dataset with Fine-Grained Semantic Classes and Hard Negative Entities",
    "volume": "main",
    "abstract": "The Entity Set Expansion (ESE) task aims to expand a handful of seed entities with new entities belonging to the same semantic class. Conventional ESE methods are based on mono-modality (i.e., literal modality), which struggle to deal with complex entities in the real world such as (1) Negative entities with fine-grained semantic differences. (2) Synonymous entities. (3) Polysemous entities. (4) Long-tailed entities. These challenges prompt us to propose novel Multi-modal Entity Set Expansion (MESE), where models integrate information from multiple modalities to represent entities. Intuitively, the benefits of multi-modal information for ESE are threefold: (1) Different modalities can provide complementary information. (2) Multi-modal information provides a unified signal via common visual properties for the same semantic class or entity. (3) Multi-modal information offers robust alignment signals for synonymous entities. To assess model performance in MESE, we constructed the MESED dataset which is the first multi-modal dataset for ESE with large-scale and elaborate manual calibration. A powerful multi-modal model MultiExpan is proposed which is pre-trained on four multimodal pre-training tasks. The extensive experiments and analyses on MESED demonstrate the high quality of the dataset and the effectiveness of our MultiExpan, as well as pointing the direction for future research. The benchmark and code are public at https://github.com/THUKElab/MESED",
    "checked": true,
    "id": "648a7a41ebeffc9f3248ee5d80356213e6002b10",
    "semantic_title": "mesed: a multi-modal entity set expansion dataset with fine-grained semantic classes and hard negative entities",
    "citation_count": 4,
    "authors": [
      "Yangning Li",
      "Tingwei Lu",
      "Hai-Tao Zheng",
      "Yinghui Li",
      "Shulin Huang",
      "Tianyu Yu",
      "Jun Yuan",
      "Rui Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28716": {
    "title": "A Generalized Neural Diffusion Framework on Graphs",
    "volume": "main",
    "abstract": "Recent studies reveal the connection between GNNs and the diffusion process, which motivates many diffusion based GNNs to be proposed. However, since these two mechanisms are closely related, one fundamental question naturally arises: Is there a general diffusion framework that can formally unify these GNNs? The answer to this question can not only deepen our understanding of the learning process of GNNs, but also may open a new door to design a broad new class of GNNs. In this paper, we propose a general diffusion equation framework with the fidelity term, which formally establishes the relationship between the diffusion process with more GNNs. Meanwhile, with this framework, we identify one characteristic of graph diffusion networks, i.e., the current neural diffusion process only corresponds to the first-order diffusion equation. However, by an experimental investigation, we show that the labels of high-order neighbors actually appear monophily property, which induces the similarity based on labels among high-order neighbors without requiring the similarity among first-order neighbors. This discovery motives to design a new high-order neighbor-aware diffusion equation, and derive a new type of graph diffusion network (HiD-Net) based on the framework. With the high-order diffusion equation, HiD-Net is more robust against attacks and works on both homophily and heterophily graphs. We not only theoretically analyze the relation between HiD-Net with high-order random walk, but also provide a theoretical convergence guarantee. Extensive experimental results well demonstrate the effectiveness of HiD-Net over state-of-the-art graph diffusion networks",
    "checked": true,
    "id": "60057b812d3170c9f7ad15fcecbc8b55672f89ae",
    "semantic_title": "a generalized neural diffusion framework on graphs",
    "citation_count": 7,
    "authors": [
      "Yibo Li",
      "Xiao Wang",
      "Hongrui Liu",
      "Chuan Shi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28717": {
    "title": "Learning to Rank in Generative Retrieval",
    "volume": "main",
    "abstract": "Generative retrieval stands out as a promising new paradigm in text retrieval that aims to generate identifier strings of relevant passages as the retrieval target. This generative paradigm taps into powerful generative language models, distinct from traditional sparse or dense retrieval methods. However, only learning to generate is insufficient for generative retrieval. Generative retrieval learns to generate identifiers of relevant passages as an intermediate goal and then converts predicted identifiers into the final passage rank list. The disconnect between the learning objective of autoregressive models and the desired passage ranking target leads to a learning gap. To bridge this gap, we propose a learning-to-rank framework for generative retrieval, dubbed LTRGR. LTRGR enables generative retrieval to learn to rank passages directly, optimizing the autoregressive model toward the final passage ranking target via a rank loss. This framework only requires an additional learning-to-rank training phase to enhance current generative retrieval systems and does not add any burden to the inference stage. We conducted experiments on three public benchmarks, and the results demonstrate that LTRGR achieves state-of-the-art performance among generative retrieval methods. The code and checkpoints are released at https://github.com/liyongqi67/LTRGR",
    "checked": true,
    "id": "70bc71200aabbab55bf0a3c749e324645c642940",
    "semantic_title": "learning to rank in generative retrieval",
    "citation_count": 17,
    "authors": [
      "Yongqi Li",
      "Nan Yang",
      "Liang Wang",
      "Furu Wei",
      "Wenjie Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28718": {
    "title": "Urban Region Embedding via Multi-View Contrastive Prediction",
    "volume": "main",
    "abstract": "Recently, learning urban region representations utilizing multi-modal data (information views) has become increasingly popular, for deep understanding of the distributions of various socioeconomic features in cities. However, previous methods usually blend multi-view information in a posteriors stage, falling short in learning coherent and consistent representations across different views. In this paper, we form a new pipeline to learn consistent representations across varying views, and propose the multi-view Contrastive Prediction model for urban Region embedding (ReCP), which leverages the multiple information views from point-of-interest (POI) and human mobility data. Specifically, ReCP comprises two major modules, namely an intra-view learning module utilizing contrastive learning and feature reconstruction to capture the unique information from each single view, and inter-view learning module that perceives the consistency between the two views using a contrastive prediction learning scheme. We conduct thorough experiments on two downstream tasks to assess the proposed model, i.e., land use clustering and region popularity prediction. The experimental results demonstrate that our model outperforms state-of-the-art baseline methods significantly in urban region representation learning",
    "checked": true,
    "id": "4cb3faceba42e148e49728c322a4559f3d4e07fa",
    "semantic_title": "urban region embedding via multi-view contrastive prediction",
    "citation_count": 1,
    "authors": [
      "Zechen Li",
      "Weiming Huang",
      "Kai Zhao",
      "Min Yang",
      "Yongshun Gong",
      "Meng Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28719": {
    "title": "Hawkes-Enhanced Spatial-Temporal Hypergraph Contrastive Learning Based on Criminal Correlations",
    "volume": "main",
    "abstract": "Crime prediction is a crucial yet challenging task within urban computing, which benefits public safety and resource optimization. Over the years, various models have been proposed, and spatial-temporal hypergraph learning models have recently shown outstanding performances. However, three correlations underlying crime are ignored, thus hindering the performance of previous models. Specifically, there are two spatial correlations and one temporal correlation, i.e., (1) co-occurrence of different types of crimes (type spatial correlation), (2) the closer to the crime center, the more dangerous it is around the neighborhood area (neighbor spatial correlation), and (3) the closer between two timestamps, the more relevant events are (hawkes temporal correlation). To this end, we propose Hawkes-enhanced Spatial-Temporal Hypergraph Contrastive Learning framework (HCL), which mines the aforementioned correlations via two specific strategies. Concretely, contrastive learning strategies are designed for two spatial correlations, and hawkes process modeling is adopted for temporal correlations. Extensive experiments demonstrate the promising capacities of HCL from four aspects, i.e., superiority, transferability, effectiveness, and sensitivity",
    "checked": true,
    "id": "24695cbd2c3c9e5e4de7eca7fd627e56a3d39402",
    "semantic_title": "hawkes-enhanced spatial-temporal hypergraph contrastive learning based on criminal correlations",
    "citation_count": 1,
    "authors": [
      "Ke Liang",
      "Sihang Zhou",
      "Meng Liu",
      "Yue Liu",
      "Wenxuan Tu",
      "Yi Zhang",
      "Liming Fang",
      "Zhe Liu",
      "Xinwang Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28720": {
    "title": "A Comprehensive Augmentation Framework for Anomaly Detection",
    "volume": "main",
    "abstract": "Data augmentation methods are commonly integrated into the training of anomaly detection models. Previous approaches have primarily focused on replicating real-world anomalies or enhancing diversity, without considering that the standard of anomaly varies across different classes, potentially leading to a biased training distribution. This paper analyzes crucial traits of simulated anomalies that contribute to the training of reconstructive networks and condenses them into several methods, thus creating a comprehensive framework by selectively utilizing appropriate combinations. Furthermore, we integrate this framework with a reconstruction-based approach and concurrently propose a split training strategy that alleviates the overfitting issue while avoiding introducing interference to the reconstruction process. The evaluations conducted on the MVTec anomaly detection dataset demonstrate that our method outperforms the previous state-of-the-art approach, particularly in terms of object classes. We also generate a simulated dataset comprising anomalies with diverse characteristics, and experimental results demonstrate that our approach exhibits promising potential for generalizing effectively to various unseen anomalies encountered in real-world scenarios",
    "checked": true,
    "id": "232c027d224e6095fa6a7106f11588a2fa0a7861",
    "semantic_title": "a comprehensive augmentation framework for anomaly detection",
    "citation_count": 0,
    "authors": [
      "Jiang Lin",
      "Yaping Yan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28721": {
    "title": "Temporally and Distributionally Robust Optimization for Cold-Start Recommendation",
    "volume": "main",
    "abstract": "Collaborative Filtering (CF) recommender models highly depend on user-item interactions to learn CF representations, thus falling short of recommending cold-start items. To address this issue, prior studies mainly introduce item features (e.g., thumbnails) for cold-start item recommendation. They learn a feature extractor on warm-start items to align feature representations with interactions, and then leverage the feature extractor to extract the feature representations of cold-start items for interaction prediction. Unfortunately, the features of cold-start items, especially the popular ones, tend to diverge from those of warm-start ones due to temporal feature shifts, preventing the feature extractor from accurately learning feature representations of cold-start items. To alleviate the impact of temporal feature shifts, we consider using Distributionally Robust Optimization (DRO) to enhance the generation ability of the feature extractor. Nonetheless, existing DRO methods face an inconsistency issue: the worse-case warm-start items emphasized during DRO training might not align well with the cold-start item distribution. To capture the temporal feature shifts and combat this inconsistency issue, we propose a novel temporal DRO with new optimization objectives, namely, 1) to integrate a worst-case factor to improve the worst-case performance, and 2) to devise a shifting factor to capture the shifting trend of item features and enhance the optimization of the potentially popular groups in cold-start items. Substantial experiments on three real-world datasets validate the superiority of our temporal DRO in enhancing the generalization ability of cold-start recommender models",
    "checked": true,
    "id": "5f01fc9f0f410b8c4d47f0d13f6351f2826204ae",
    "semantic_title": "temporally and distributionally robust optimization for cold-start recommendation",
    "citation_count": 1,
    "authors": [
      "Xinyu Lin",
      "Wenjie Wang",
      "Jujia Zhao",
      "Yongqi Li",
      "Fuli Feng",
      "Tat-Seng Chua"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28722": {
    "title": "Towards Continual Knowledge Graph Embedding via Incremental Distillation",
    "volume": "main",
    "abstract": "Traditional knowledge graph embedding (KGE) methods typically require preserving the entire knowledge graph (KG) with significant training costs when new knowledge emerges. To address this issue, the continual knowledge graph embedding (CKGE) task has been proposed to train the KGE model by learning emerging knowledge efficiently while simultaneously preserving decent old knowledge. However, the explicit graph structure in KGs, which is critical for the above goal, has been heavily ignored by existing CKGE methods. On the one hand, existing methods usually learn new triples in a random order, destroying the inner structure of new KGs. On the other hand, old triples are preserved with equal priority, failing to alleviate catastrophic forgetting effectively. In this paper, we propose a competitive method for CKGE based on incremental distillation (IncDE), which considers the full use of the explicit graph structure in KGs. First, to optimize the learning order, we introduce a hierarchical strategy, ranking new triples for layer-by-layer learning. By employing the inter- and intra-hierarchical orders together, new triples are grouped into layers based on the graph structure features. Secondly, to preserve the old knowledge effectively, we devise a novel incremental distillation mechanism, which facilitates the seamless transfer of entity representations from the previous layer to the next one, promoting old knowledge preservation. Finally, we adopt a two-stage training paradigm to avoid the over-corruption of old knowledge influenced by under-trained new knowledge. Experimental results demonstrate the superiority of IncDE over state-of-the-art baselines. Notably, the incremental distillation mechanism contributes to improvements of 0.2%-6.5% in the mean reciprocal rank (MRR) score. More exploratory experiments validate the effectiveness of IncDE in proficiently learning new knowledge while preserving old knowledge across all time steps",
    "checked": true,
    "id": "f42d060fb530a11daecd90695211c01a5c264f8d",
    "semantic_title": "towards continual knowledge graph embedding via incremental distillation",
    "citation_count": 4,
    "authors": [
      "Jiajun Liu",
      "Wenjun Ke",
      "Peng Wang",
      "Ziyu Shang",
      "Jinhua Gao",
      "Guozheng Li",
      "Ke Ji",
      "Yanhe Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28723": {
    "title": "Graph Disentangled Contrastive Learning with Personalized Transfer for Cross-Domain Recommendation",
    "volume": "main",
    "abstract": "Cross-Domain Recommendation (CDR) has been proven to effectively alleviate the data sparsity problem in Recommender System (RS). Recent CDR methods often disentangle user features into domain-invariant and domain-specific features for efficient cross-domain knowledge transfer. Despite showcasing robust performance, three crucial aspects remain unexplored for existing disentangled CDR approaches: i) The significance nuances of the interaction behaviors are ignored in generating disentangled features; ii) The user features are disentangled irrelevant to the individual items to be recommended; iii) The general knowledge transfer overlooks the user's personality when interacting with diverse items. To this end, we propose a Graph Disentangled Contrastive framework for CDR (GDCCDR) with personalized transfer by meta-networks. An adaptive parameter-free filter is proposed to gauge the significance of diverse interactions, thereby facilitating more refined disentangled representations. In sight of the success of Contrastive Learning (CL) in RS, we propose two CL-based constraints for item-aware disentanglement. Proximate CL ensures the coherence of domain-invariant features between domains, while eliminatory CL strives to disentangle features within each domains using mutual information between users and items. Finally, for domain-invariant features, we adopt meta-networks to achieve personalized transfer. Experimental results on four real-world datasets demonstrate the superiority of GDCCDR over state-of-the-art methods",
    "checked": true,
    "id": "705d6778eaa1f6cf3bd935350e9ee68d92d5d070",
    "semantic_title": "graph disentangled contrastive learning with personalized transfer for cross-domain recommendation",
    "citation_count": 3,
    "authors": [
      "Jing Liu",
      "Lele Sun",
      "Weizhi Nie",
      "Peiguang Jing",
      "Yuting Su"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28724": {
    "title": "Multimodal Event Causality Reasoning with Scene Graph Enhanced Interaction Network",
    "volume": "main",
    "abstract": "Multimodal event causality reasoning aims to recognize the causal relations based on the given events and accompanying image pairs, requiring the model to have a comprehensive grasp of visual and textual information. However, existing studies fail to effectively model the relations of the objects within the image and capture the object interactions across the image pair, resulting in an insufficient understanding of visual information by the model. To address these issues, we propose a Scene Graph Enhanced Interaction Network (SEIN) in this paper, which can leverage the interactions of the generated scene graph for multimodal event causality reasoning. Specifically, the proposed method adopts a graph convolutional network to model the objects and their relations derived from the scene graph structure, empowering the model to exploit the rich structural and semantic information in the image adequately. To capture the object interactions between the two images, we design an optimal transport-based alignment strategy to match the objects across the images, which could help the model recognize changes in visual information and facilitate causality reasoning. In addition, we introduce a cross-modal fusion module to combine textual and visual features for causality prediction. Experimental results indicate that the proposed SEIN outperforms state-of-the-art methods on the Vis-Causal dataset",
    "checked": true,
    "id": "6afbc23b48d53c3285f638a91726d0fd73885349",
    "semantic_title": "multimodal event causality reasoning with scene graph enhanced interaction network",
    "citation_count": 0,
    "authors": [
      "Jintao Liu",
      "Kaiwen Wei",
      "Chenglong Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28725": {
    "title": "AT4CTR: Auxiliary Match Tasks for Enhancing Click-Through Rate Prediction",
    "volume": "main",
    "abstract": "Click-through rate (CTR) prediction is a vital task in industrial recommendation systems. Most existing methods focus on the network architecture design of the CTR model for better accuracy and suffer from the data sparsity problem. Especially in industrial recommendation systems, the widely applied negative sample down-sampling technique due to resource limitation worsens the problem, resulting in a decline in performance. In this paper, we propose Auxiliary Match Tasks for enhancing Click-Through Rate (AT4CTR) prediction accuracy by alleviating the data sparsity problem. Specifically, we design two match tasks inspired by collaborative filtering to enhance the relevance modeling between user and item. As the \"click\" action is a strong signal which indicates the user's preference towards the item directly, we make the first match task aim at pulling closer the representation between the user and the item regarding the positive samples. Since the user's past click behaviors can also be treated as the user him/herself, we apply the next item prediction as the second match task. For both the match tasks, we choose the InfoNCE as their loss function. The two match tasks can provide meaningful training signals to speed up the model's convergence and alleviate the data sparsity. We conduct extensive experiments on one public dataset and one large-scale industrial recommendation dataset. The result demonstrates the effectiveness of the proposed auxiliary match tasks. AT4CTR has been deployed in the real industrial advertising system and has gained remarkable revenue",
    "checked": true,
    "id": "82cf8bf5e5e5b3f455fd761ff007fd9d21792068",
    "semantic_title": "at4ctr: auxiliary match tasks for enhancing click-through rate prediction",
    "citation_count": 2,
    "authors": [
      "Qi Liu",
      "Xuyang Hou",
      "Defu Lian",
      "Zhe Wang",
      "Haoran Jin",
      "Jia Cheng",
      "Jun Lei"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28726": {
    "title": "Online Conversion Rate Prediction via Multi-Interval Screening and Synthesizing under Delayed Feedback",
    "volume": "main",
    "abstract": "Due to the widespread adoption of the cost-per-action(CPA) display strategy that demands a real-time conversion rate prediction(CVR), delayed feedback is becoming one of the major challenges in online advertising. As the true labels of a significant quantity of samples are only available after long delays, the observed training data are usually biased, harming the performance of models. Recent studies show integrating models with varying waiting windows to observe true labels is beneficial, but the aggregation framework remains far from reaching a consensus. In this work, we propose the Multi-Interval Screening and Synthesizing model (MISS for short) for online CVR prediction. We first design a multi-interval screening model with various output heads to produce accurate and distinctive estimates. Then a light-weight synthesizing model with an assembled training pipeline is applied to thoroughly exploit the knowledge and relationship among heads, obtaining reliable predictions. Extensive experiments on two real-world advertising datasets validate the effectiveness of our model",
    "checked": true,
    "id": "c4786675bf2f89fcc51f4e2af1e5c35f633a028c",
    "semantic_title": "online conversion rate prediction via multi-interval screening and synthesizing under delayed feedback",
    "citation_count": 0,
    "authors": [
      "Qiming Liu",
      "Xiang Ao",
      "Yuyao Guo",
      "Qing He"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28727": {
    "title": "KG-TREAT: Pre-training for Treatment Effect Estimation by Synergizing Patient Data with Knowledge Graphs",
    "volume": "main",
    "abstract": "Treatment effect estimation (TEE) is the task of determining the impact of various treatments on patient outcomes. Current TEE methods fall short due to reliance on limited labeled data and challenges posed by sparse and high-dimensional observational patient data. To address the challenges, we introduce a novel pre-training and fine-tuning framework, KG-TREAT, which synergizes large-scale observational patient data with biomedical knowledge graphs (KGs) to enhance TEE. Unlike previous approaches, KG-TREAT constructs dual-focus KGs and integrates a deep bi-level attention synergy method for in-depth information fusion, enabling distinct encoding of treatment-covariate and outcome-covariate relationships. KG-TREAT also incorporates two pre-training tasks to ensure a thorough grounding and contextualization of patient data and KGs. Evaluation on four downstream TEE tasks shows KG-TREAT's superiority over existing methods, with an average improvement of 7% in Area under the ROC Curve (AUC) and 9% in Influence Function-based Precision of Estimating Heterogeneous Effects (IF-PEHE). The effectiveness of our estimated treatment effects is further affirmed by alignment with established randomized clinical trial findings",
    "checked": true,
    "id": "923399d193031b9e2b6c2bc2c4f0bbb55f42b7d2",
    "semantic_title": "kg-treat: pre-training for treatment effect estimation by synergizing patient data with knowledge graphs",
    "citation_count": 0,
    "authors": [
      "Ruoqi Liu",
      "Lingfei Wu",
      "Ping Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28728": {
    "title": "Learning Accurate and Bidirectional Transformation via Dynamic Embedding Transportation for Cross-Domain Recommendation",
    "volume": "main",
    "abstract": "With the rapid development of Internet and Web techniques, Cross-Domain Recommendation (CDR) models have been widely explored for resolving the data-sparsity and cold-start problem. Meanwhile, most CDR models should utilize explicit domain-shareable information (e.g., overlapped users or items) for knowledge transfer across domains. However, this assumption may not be always satisfied since users and items are always non-overlapped in real practice. The performance of many previous works will be severely impaired when these domain-shareable information are not available. To address the aforementioned issues, we propose the Joint Preference Exploration and Dynamic Embedding Transportation model (JPEDET) in this paper which is a novel framework for solving the CDR problem when users and items are non-overlapped. JPEDET includes two main modules, i.e., joint preference exploration module and dynamic embedding transportation module. The joint preference exploration module aims to fuse rating and review information for modelling user preferences. The dynamic embedding transportation module is set to share knowledge via neural ordinary equations for dual transformation across domains. Moreover, we innovatively propose the dynamic transport flow equipped with linear interpolation guidance on barycentric Wasserstein path for achieving accurate and bidirectional transformation. Our empirical study on Amazon datasets demonstrates that JPEDET significantly outperforms the state-of-the-art models under the CDR setting",
    "checked": true,
    "id": "9ef46b1a8041dede250bf9fd58d7c5c34a2f10ab",
    "semantic_title": "learning accurate and bidirectional transformation via dynamic embedding transportation for cross-domain recommendation",
    "citation_count": 0,
    "authors": [
      "Weiming Liu",
      "Chaochao Chen",
      "Xinting Liao",
      "Mengling Hu",
      "Yanchao Tan",
      "Fan Wang",
      "Xiaolin Zheng",
      "Yew Soon Ong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28729": {
    "title": "Knowledge Graph Error Detection with Contrastive Confidence Adaption",
    "volume": "main",
    "abstract": "Knowledge graphs (KGs) often contain various errors. Previous works on detecting errors in KGs mainly rely on triplet embedding from graph structure. We conduct an empirical study and find that these works struggle to discriminate noise from semantically-similar correct triplets. In this paper, we propose a KG error detection model CCA to integrate both textual and graph structural information from triplet reconstruction for better distinguishing semantics. We design interactive contrastive learning to capture the differences between textual and structural patterns. Furthermore, we construct realistic datasets with semantically-similar noise and adversarial noise. Experimental results demonstrate that CCA outperforms state-of-the-art baselines, especially on semantically-similar noise and adversarial noise",
    "checked": true,
    "id": "56d2c2f6801108f56cfea7292c9905650e62a511",
    "semantic_title": "knowledge graph error detection with contrastive confidence adaption",
    "citation_count": 0,
    "authors": [
      "Xiangyu Liu",
      "Yang Liu",
      "Wei Hu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28730": {
    "title": "Perturbation-Invariant Adversarial Training for Neural Ranking Models: Improving the Effectiveness-Robustness Trade-Off",
    "volume": "main",
    "abstract": "Neural ranking models (NRMs) have shown great success in information retrieval (IR). But their predictions can easily be manipulated using adversarial examples, which are crafted by adding imperceptible perturbations to legitimate documents. This vulnerability raises significant concerns about their reliability and hinders the widespread deployment of NRMs. By incorporating adversarial examples into training data, adversarial training has become the de facto defense approach to adversarial attacks against NRMs. However, this defense mechanism is subject to a trade-off between effectiveness and adversarial robustness. In this study, we establish theoretical guarantees regarding the effectiveness-robustness trade-off in NRMs. We decompose the robust ranking error into two components, i.e., a natural ranking error for effectiveness evaluation and a boundary ranking error for assessing adversarial robustness. Then, we define the perturbation invariance of a ranking model and prove it to be a differentiable upper bound on the boundary ranking error for attainable computation. Informed by our theoretical analysis, we design a novel perturbation-invariant adversarial training (PIAT) method for ranking models to achieve a better effectiveness-robustness trade-off. We design a regularized surrogate loss, in which one term encourages the effectiveness to be maximized while the regularization term encourages the output to be smooth, so as to improve adversarial robustness. Experimental results on several ranking models demonstrate the superiority of PITA compared to existing adversarial defenses",
    "checked": true,
    "id": "f59412d7ef927bd3c37534cac551b47f5770f11c",
    "semantic_title": "perturbation-invariant adversarial training for neural ranking models: improving the effectiveness-robustness trade-off",
    "citation_count": 3,
    "authors": [
      "Yu-An Liu",
      "Ruqing Zhang",
      "Mingkun Zhang",
      "Wei Chen",
      "Maarten de Rijke",
      "Jiafeng Guo",
      "Xueqi Cheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28731": {
    "title": "Full Bayesian Significance Testing for Neural Networks",
    "volume": "main",
    "abstract": "Significance testing aims to determine whether a proposition about the population distribution is the truth or not given observations. However, traditional significance testing often needs to derive the distribution of the testing statistic, failing to deal with complex nonlinear relationships. In this paper, we propose to conduct Full Bayesian Significance Testing for neural networks, called nFBST, to overcome the limitation in relationship characterization of traditional approaches. A Bayesian neural network is utilized to fit the nonlinear and multi-dimensional relationships with small errors and avoid hard theoretical derivation by computing the evidence value. Besides, nFBST can test not only global significance but also local and instance-wise significance, which previous testing methods don't focus on. Moreover, nFBST is a general framework that can be extended based on the measures selected, such as Grad-nFBST, LRP-nFBST, DeepLIFT-nFBST, LIME-nFBST. A range of experiments on both simulated and real data are conducted to show the advantages of our method",
    "checked": true,
    "id": "173f86ea5a3753a2d44217b879dd6634c6e118d1",
    "semantic_title": "full bayesian significance testing for neural networks",
    "citation_count": 1,
    "authors": [
      "Zehua Liu",
      "Zimeng Li",
      "Jingyuan Wang",
      "Yue He"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28732": {
    "title": "KGDM: A Diffusion Model to Capture Multiple Relation Semantics for Knowledge Graph Embedding",
    "volume": "main",
    "abstract": "Knowledge graph embedding (KGE) is an efficient and scalable method for knowledge graph completion. However, most existing KGE methods suffer from the challenge of multiple relation semantics, which often degrades their performance. This is because most KGE methods learn fixed continuous vectors for entities (relations) and make deterministic entity predictions to complete the knowledge graph, which hardly captures multiple relation semantics. To tackle this issue, previous works try to learn complex probabilistic embeddings instead of fixed embeddings but suffer from heavy computational complexity. In contrast, this paper proposes a simple yet efficient framework namely the Knowledge Graph Diffusion Model (KGDM) to capture the multiple relation semantics in prediction. Its key idea is to cast the problem of entity prediction into conditional entity generation. Specifically, KGDM estimates the probabilistic distribution of target entities in prediction through Denoising Diffusion Probabilistic Models (DDPM). To bridge the gap between continuous diffusion models and discrete KGs, two learnable embedding functions are defined to map entities and relation to continuous vectors. To consider connectivity patterns of KGs, a Conditional Entity Denoiser model is introduced to generate target entities conditioned on given entities and relations. Extensive experiments demonstrate that KGDM significantly outperforms existing state-of-the-art methods in three benchmark datasets",
    "checked": true,
    "id": "822ad7c33316202a2511d300c6b8a263b758ad1a",
    "semantic_title": "kgdm: a diffusion model to capture multiple relation semantics for knowledge graph embedding",
    "citation_count": 1,
    "authors": [
      "Xiao Long",
      "Liansheng Zhuang",
      "Aodi Li",
      "Jiuchang Wei",
      "Houqiang Li",
      "Shafei Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28733": {
    "title": "Deep Hierarchical Video Compression",
    "volume": "main",
    "abstract": "Recently, probabilistic predictive coding that directly models the conditional distribution of latent features across successive frames for temporal redundancy removal has yielded promising results. Existing methods using a single-scale Variational AutoEncoder (VAE) must devise complex networks for conditional probability estimation in latent space, neglecting multiscale characteristics of video frames. Instead, this work proposes hierarchical probabilistic predictive coding, for which hierarchal VAEs are carefully designed to characterize multiscale latent features as a family of flexible priors and posteriors to predict the probabilities of future frames. Under such a hierarchical structure, lightweight networks are sufficient for prediction. The proposed method outperforms representative learned video compression models on common testing videos and demonstrates computational friendliness with much less memory footprint and faster encoding/decoding. Extensive experiments on adaptation to temporal patterns also indicate the better generalization of our hierarchical predictive mechanism. Furthermore, our solution is the first to enable progressive decoding that is favored in networked video applications with packet loss",
    "checked": true,
    "id": "88e455e6245f4e8ffefa065bbe1d7a0d98a4f384",
    "semantic_title": "deep hierarchical video compression",
    "citation_count": 0,
    "authors": [
      "Ming Lu",
      "Zhihao Duan",
      "Fengqing Zhu",
      "Zhan Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28734": {
    "title": "Spectral-Based Graph Neural Networks for Complementary Item Recommendation",
    "volume": "main",
    "abstract": "Modeling complementary relationships greatly helps recommender systems to accurately and promptly recommend the subsequent items when one item is purchased. Unlike traditional similar relationships, items with complementary relationships may be purchased successively (such as iPhone and Airpods Pro), and they not only share relevance but also exhibit dissimilarity. Since the two attributes are opposites, modeling complementary relationships is challenging. Previous attempts to exploit these relationships have either ignored or oversimplified the dissimilarity attribute, resulting in ineffective modeling and an inability to balance the two attributes. Since Graph Neural Networks (GNNs) can capture the relevance and dissimilarity between nodes in the spectral domain, we can leverage spectral-based GNNs to effectively understand and model complementary relationships. In this study, we present a novel approach called Spectral-based Complementary Graph Neural Networks (SComGNN) that utilizes the spectral properties of complementary item graphs. We make the first observation that complementary relationships consist of low-frequency and mid-frequency components, corresponding to the relevance and dissimilarity attributes, respectively. Based on this spectral observation, we design spectral graph convolutional networks with low-pass and mid-pass filters to capture the low-frequency and mid-frequency components. Additionally, we propose a two-stage attention mechanism to adaptively integrate and balance the two attributes. Experimental results on four e-commerce datasets demonstrate the effectiveness of our model, with SComGNN significantly outperforming existing baseline models",
    "checked": true,
    "id": "bf06398ba68ba3ee2c2109ffd8be783f8889c762",
    "semantic_title": "spectral-based graph neural networks for complementary item recommendation",
    "citation_count": 1,
    "authors": [
      "Haitong Luo",
      "Xuying Meng",
      "Suhang Wang",
      "Hanyun Cao",
      "Weiyao Zhang",
      "Yequan Wang",
      "Yujun Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28735": {
    "title": "Enhancing Cognitive Diagnosis Using Un-interacted Exercises: A Collaboration-Aware Mixed Sampling Approach",
    "volume": "main",
    "abstract": "Cognitive diagnosis is a crucial task in computer-aided education, aimed at evaluating students' proficiency levels across various knowledge concepts through exercises. Current models, however, primarily rely on students' answered exercises, neglecting the complex and rich information contained in un-interacted exercises. While recent research has attempted to leverage the data within un-interacted exercises linked to interacted knowledge concepts, aiming to address the long-tail issue, these studies fail to fully explore the informative, un-interacted exercises related to broader knowledge concepts. This oversight results in diminished performance when these models are applied to comprehensive datasets. In response to this gap, we present the Collaborative-aware Mixed Exercise Sampling (CMES) framework, which can effectively exploit the information present in un-interacted exercises linked to un-interacted knowledge concepts. Specifically, we introduce a novel universal sampling module where the training samples comprise not merely raw data slices, but enhanced samples generated by combining weight-enhanced attention mixture techniques. Given the necessity of real response labels in cognitive diagnosis, we also propose a ranking-based pseudo feedback module to regulate students' responses on generated exercises. The versatility of the CMES framework bolsters existing models and improves their adaptability. Finally, we demonstrate the effectiveness and interpretability of our framework through comprehensive experiments on real-world datasets",
    "checked": true,
    "id": "cdcc79e80b993dbaba2f0c6b05347158aeb3a699",
    "semantic_title": "enhancing cognitive diagnosis using un-interacted exercises: a collaboration-aware mixed sampling approach",
    "citation_count": 1,
    "authors": [
      "Haiping Ma",
      "Changqian Wang",
      "Hengshu Zhu",
      "Shangshang Yang",
      "Xiaoming Zhang",
      "Xingyi Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28736": {
    "title": "Plug-In Diffusion Model for Sequential Recommendation",
    "volume": "main",
    "abstract": "Pioneering efforts have verified the effectiveness of the diffusion models in exploring the informative uncertainty for recommendation. Considering the difference between recommendation and image synthesis tasks, existing methods have undertaken tailored refinements to the diffusion and reverse process. However, these approaches typically use the highest-score item in corpus for user interest prediction, leading to the ignorance of the user's generalized preference contained within other items, thereby remaining constrained by the data sparsity issue. To address this issue, this paper presents a novel Plug-in Diffusion Model for Recommendation (PDRec) framework, which employs the diffusion model as a flexible plugin to jointly take full advantage of the diffusion-generating user preferences on all items. Specifically, PDRec first infers the users' dynamic preferences on all items via a time-interval diffusion model and proposes a Historical Behavior Reweighting (HBR) mechanism to identify the high-quality behaviors and suppress noisy behaviors. In addition to the observed items, PDRec proposes a Diffusion-based Positive Augmentation (DPA) strategy to leverage the top-ranked unobserved items as the potential positive samples, bringing in informative and diverse soft signals to alleviate data sparsity. To alleviate the false negative sampling issue, PDRec employs Noise-free Negative Sampling (NNS) to select stable negative samples for ensuring effective model optimization. Extensive experiments and analyses on four datasets have verified the superiority of the proposed PDRec over the state-of-the-art baselines and showcased the universality of PDRec as a flexible plugin for commonly-used sequential encoders in different recommendation scenarios. The code is available in https://github.com/hulkima/PDRec",
    "checked": true,
    "id": "1f6fddbb12110398b76666c771712ab49e6277c4",
    "semantic_title": "plug-in diffusion model for sequential recommendation",
    "citation_count": 5,
    "authors": [
      "Haokai Ma",
      "Ruobing Xie",
      "Lei Meng",
      "Xin Chen",
      "Xu Zhang",
      "Leyu Lin",
      "Zhanhui Kang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28737": {
    "title": "Tail-STEAK: Improve Friend Recommendation for Tail Users via Self-Training Enhanced Knowledge Distillation",
    "volume": "main",
    "abstract": "Graph neural networks (GNNs) are commonly employed in collaborative friend recommendation systems. Nevertheless, recent studies reveal a notable performance gap, particularly for users with limited connections, commonly known as tail users, in contrast to their counterparts with abundant connections (head users). Uniformly treating head and tail users poses two challenges for tail user preference learning: (C1) Label Sparsity, as tail users typically possess limited labels; and (C2) Neighborhood Sparsity, where tail users exhibit sparse observable friendships, leading to distinct preference distributions and performance degradation compared to head users. In response to these challenges, we introduce Tail-STEAK, an innovative framework that combines self-training with enhanced knowledge distillation for tail user representation learning. To address(C1), we present Tail-STEAK-base, a two-stage self-training framework. In the first stage, only head users and their accurate connections are utilized for training, while pseudo links are generated for tail users in the second stage. To tackle (C2), we propose two data augmentation-based self-knowledge distillation pretext tasks. These tasks are seamlessly integrated into different stages of Tail-STEAK-base, culminating in the comprehensive Tail-STEAK framework. Extensive experiments, conducted on state-of-the-art GNN-based friend recommendation models, substantiate the efficacy of Tail-STEAK in significantly improving tail user performance. Our code and data are publicly available at https://github.com/antman9914/Tail-STEAK",
    "checked": true,
    "id": "d71af52bcc04fdebd1327f294987ef456241e5da",
    "semantic_title": "tail-steak: improve friend recommendation for tail users via self-training enhanced knowledge distillation",
    "citation_count": 0,
    "authors": [
      "Yijun Ma",
      "Chaozhuo Li",
      "Xiao Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28738": {
    "title": "Graph Contrastive Invariant Learning from the Causal Perspective",
    "volume": "main",
    "abstract": "Graph contrastive learning (GCL), learning the node representation by contrasting two augmented graphs in a self-supervised way, has attracted considerable attention. GCL is usually believed to learn the invariant representation. However, does this understanding always hold in practice? In this paper, we first study GCL from the perspective of causality. By analyzing GCL with the structural causal model (SCM), we discover that traditional GCL may not well learn the invariant representations due to the non-causal information contained in the graph. How can we fix it and encourage the current GCL to learn better invariant representations? The SCM offers two requirements and motives us to propose a novel GCL method. Particularly, we introduce the spectral graph augmentation to simulate the intervention upon non-causal factors. Then we design the invariance objective and independence objective to better capture the causal factors. Specifically, (i) the invariance objective encourages the encoder to capture the invariant information contained in causal variables, and (ii) the independence objective aims to reduce the influence of confounders on the causal variables. Experimental results demonstrate the effectiveness of our approach on node classification tasks",
    "checked": true,
    "id": "4c19e9d2f0da6e28f53617f355fb58f53b2f3aca",
    "semantic_title": "graph contrastive invariant learning from the causal perspective",
    "citation_count": 1,
    "authors": [
      "Yanhu Mo",
      "Xiao Wang",
      "Shaohua Fan",
      "Chuan Shi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28739": {
    "title": "HGE: Embedding Temporal Knowledge Graphs in a Product Space of Heterogeneous Geometric Subspaces",
    "volume": "main",
    "abstract": "Temporal knowledge graphs represent temporal facts (s,p,o,?) relating a subject s and an object o via a relation label p at time ?, where ? could be a time point or time interval. Temporal knowledge graphs may exhibit static temporal patterns at distinct points in time and dynamic temporal patterns between different timestamps. In order to learn a rich set of static and dynamic temporal patterns and apply them for inference, several embedding approaches have been suggested in the literature. However, as most of them resort to single underlying embedding spaces, their capability to model all kinds of temporal patterns was severely limited by having to adhere to the geometric property of their one embedding space. We lift this limitation by an embedding approach that maps temporal facts into a product space of several heterogeneous geometric subspaces with distinct geometric properties, i.e.\\ Complex, Dual, and Split-complex spaces. In addition, we propose a temporal-geometric attention mechanism to integrate information from different geometric subspaces conveniently according to the captured relational and temporal information. Experimental results on standard temporal benchmark datasets favorably evaluate our approach against state-of-the-art models",
    "checked": true,
    "id": "61c520685f2bd1e66c8d66e4c9342074850f0014",
    "semantic_title": "hge: embedding temporal knowledge graphs in a product space of heterogeneous geometric subspaces",
    "citation_count": 1,
    "authors": [
      "Jiaxin Pan",
      "Mojtaba Nayyeri",
      "Yinan Li",
      "Steffen Staab"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28740": {
    "title": "Cross-Domain Contrastive Learning for Time Series Clustering",
    "volume": "main",
    "abstract": "Most deep learning-based time series clustering models concentrate on data representation in a separate process from clustering. This leads to that clustering loss cannot guide feature extraction. Moreover, most methods solely analyze data from the temporal domain, disregarding the potential within the frequency domain. To address these challenges, we introduce a novel end-to-end Cross-Domain Contrastive learning model for time series Clustering (CDCC). Firstly, it integrates the clustering process and feature extraction using contrastive constraints at both cluster-level and instance-level. Secondly, the data is encoded simultaneously in both temporal and frequency domains, leveraging contrastive learning to enhance within-domain representation. Thirdly, cross-domain constraints are proposed to align the latent representations and category distribution across domains. With the above strategies, CDCC not only achieves end-to-end output but also effectively integrates frequency domains. Extensive experiments and visualization analysis are conducted on 40 time series datasets from UCR, demonstrating the superior performance of the proposed model",
    "checked": true,
    "id": "f97766ea14aa26b89a7981eb7f63567c413e3c82",
    "semantic_title": "cross-domain contrastive learning for time series clustering",
    "citation_count": 0,
    "authors": [
      "Furong Peng",
      "Jiachen Luo",
      "Xuan Lu",
      "Sheng Wang",
      "Feijiang Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28741": {
    "title": "Refining Latent Homophilic Structures over Heterophilic Graphs for Robust Graph Convolution Networks",
    "volume": "main",
    "abstract": "Graph convolution networks (GCNs) are extensively utilized in various graph tasks to mine knowledge from spatial data. Our study marks the pioneering attempt to quantitatively investigate the GCN robustness over omnipresent heterophilic graphs for node classification. We uncover that the predominant vulnerability is caused by the structural out-of-distribution (OOD) issue. This finding motivates us to present a novel method that aims to harden GCNs by automatically learning Latent Homophilic Structures over heterophilic graphs. We term such a methodology as LHS. To elaborate, our initial step involves learning a latent structure by employing a novel self-expressive technique based on multi-node interactions. Subsequently, the structure is refined using a pairwisely constrained dual-view contrastive learning approach. We iteratively perform the above procedure, enabling a GCN model to aggregate information in a homophilic way on heterophilic graphs. Armed with such an adaptable structure, we can properly mitigate the structural OOD threats over heterophilic graphs. Experiments on various benchmarks show the effectiveness of the proposed LHS approach for robust GCNs",
    "checked": true,
    "id": "69f21e177c23924b1f839945913766b015587708",
    "semantic_title": "refining latent homophilic structures over heterophilic graphs for robust graph convolution networks",
    "citation_count": 0,
    "authors": [
      "Chenyang Qiu",
      "Guoshun Nan",
      "Tianyu Xiong",
      "Wendi Deng",
      "Di Wang",
      "Zhiyang Teng",
      "Lijuan Sun",
      "Qimei Cui",
      "Xiaofeng Tao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28742": {
    "title": "Link Prediction in Multilayer Networks via Cross-Network Embedding",
    "volume": "main",
    "abstract": "Link prediction is a fundamental task in network analysis, with the objective of predicting missing or potential links. While existing studies have mainly concentrated on single networks, it is worth noting that numerous real-world networks exhibit interconnectedness. For example, individuals often register on various social media platforms to access diverse services, such as chatting, tweeting, blogging, and rating movies. These platforms share a subset of users and are termed multilayer networks. The interlayer links in such networks hold valuable information that provides more comprehensive insights into the network structure. To effectively exploit this complementary information and enhance link prediction in the target network, we propose a novel cross-network embedding method. This method aims to represent different networks in a shared latent space, preserving proximity within single networks as well as consistency across multilayer networks. Specifically, nodes can aggregate messages from aligned nodes in other layers. Extensive experiments conducted on real-world datasets demonstrate the superior performance of our proposed method for link prediction in multilayer networks",
    "checked": true,
    "id": "03377f8307e2aac4bca3b0bb51c9481c2f696830",
    "semantic_title": "link prediction in multilayer networks via cross-network embedding",
    "citation_count": 0,
    "authors": [
      "Guojing Ren",
      "Xiao Ding",
      "Xiao-Ke Xu",
      "Hai-Feng Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28743": {
    "title": "Towards Diverse Perspective Learning with Selection over Multiple Temporal Poolings",
    "volume": "main",
    "abstract": "In Time Series Classification (TSC), temporal pooling methods that consider sequential information have been proposed. However, we found that each temporal pooling has a distinct mechanism, and can perform better or worse depending on time series data. We term this fixed pooling mechanism a single perspective of temporal poolings. In this paper, we propose a novel temporal pooling method with diverse perspective learning: Selection over Multiple Temporal Poolings (SoM-TP). SoM-TP dynamically selects the optimal temporal pooling among multiple methods for each data by attention. The dynamic pooling selection is motivated by the ensemble concept of Multiple Choice Learning (MCL), which selects the best among multiple outputs. The pooling selection by SoM-TP's attention enables a non-iterative pooling ensemble within a single classifier. Additionally, we define a perspective loss and Diverse Perspective Learning Network (DPLN). The loss works as a regularizer to reflect all the pooling perspectives from DPLN. Our perspective analysis using Layer-wise Relevance Propagation (LRP) reveals the limitation of a single perspective and ultimately demonstrates diverse perspective learning of SoM-TP. We also show that SoM-TP outperforms CNN models based on other temporal poolings and state-of-the-art models in TSC with extensive UCR/UEA repositories",
    "checked": true,
    "id": "8005cad12a23394f9a031677604f40c62e3836a9",
    "semantic_title": "towards diverse perspective learning with selection over multiple temporal poolings",
    "citation_count": 0,
    "authors": [
      "Jihyeon Seong",
      "Jungmin Kim",
      "Jaesik Choi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28744": {
    "title": "LAFA: Multimodal Knowledge Graph Completion with Link Aware Fusion and Aggregation",
    "volume": "main",
    "abstract": "Recently, an enormous amount of research has emerged on multimodal knowledge graph completion (MKGC), which seeks to extract knowledge from multimodal data and predict the most plausible missing facts to complete a given multimodal knowledge graph (MKG). However, existing MKGC approaches largely ignore that visual information may introduce noise and lead to uncertainty when adding them to the traditional KG embeddings due to the contribution of each associated image to entity is different in diverse link scenarios. Moreover, treating each triple independently when learning entity embeddings leads to local structural and the whole graph information missing. To address these challenges, we propose a novel link aware fusion and aggregation based multimodal knowledge graph completion model named LAFA, which is composed of link aware fusion module and link aware aggregation module. The link aware fusion module alleviates noise of irrelevant visual information by calculating the importance between an entity and its associated images in different link scenarios, and fuses the visual and structural embeddings according to the importance through our proposed modality embedding fusion mechanism. The link aware aggregation module assigns neighbor structural information to a given central entity by calculating the importance between the entity and its neighbors, and aggregating the fused embeddings through linear combination according to the importance. Extensive experiments on standard datasets validate that LAFA can obtain state-of-the-art performance",
    "checked": true,
    "id": "b64a245dcd4388962d6569e4fdc05a9d7cc47ac3",
    "semantic_title": "lafa: multimodal knowledge graph completion with link aware fusion and aggregation",
    "citation_count": 1,
    "authors": [
      "Bin Shang",
      "Yinliang Zhao",
      "Jun Liu",
      "Di Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28745": {
    "title": "Mixed Geometry Message and Trainable Convolutional Attention Network for Knowledge Graph Completion",
    "volume": "main",
    "abstract": "Knowledge graph completion (KGC) aims to study the embedding representation to solve the incompleteness of knowledge graphs (KGs). Recently, graph convolutional networks (GCNs) and graph attention networks (GATs) have been widely used in KGC tasks by capturing neighbor information of entities. However, Both GCNs and GATs based KGC models have their limitations, and the best method is to analyze the neighbors of each entity (pre-validating), while this process is prohibitively expensive. Furthermore, the representation quality of the embeddings can affect the aggregation of neighbor information (message passing). To address the above limitations, we propose a novel knowledge graph completion model with mixed geometry message and trainable convolutional attention network named MGTCA. Concretely, the mixed geometry message function generates rich neighbor message by integrating spatially information in the hyperbolic space, hypersphere space and Euclidean space jointly. To complete the autonomous switching of graph neural networks (GNNs) and eliminate the necessity of pre-validating the local structure of KGs, a trainable convolutional attention network is proposed by comprising three types of GNNs in one trainable formulation. Furthermore, a mixed geometry scoring function is proposed, which calculates scores of triples by novel prediction function and similarity function based on different geometric spaces. Extensive experiments on three standard datasets confirm the effectiveness of our innovations, and the performance of MGTCA is significantly improved compared to the state-of-the-art approaches",
    "checked": true,
    "id": "eb14b24b329a6cc80747644616e15491ef49596f",
    "semantic_title": "mixed geometry message and trainable convolutional attention network for knowledge graph completion",
    "citation_count": 1,
    "authors": [
      "Bin Shang",
      "Yinliang Zhao",
      "Jun Liu",
      "Di Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28746": {
    "title": "ResDiff: Combining CNN and Diffusion Model for Image Super-resolution",
    "volume": "main",
    "abstract": "Adapting the Diffusion Probabilistic Model (DPM) for direct image super-resolution is wasteful, given that a simple Convolutional Neural Network (CNN) can recover the main low-frequency content. Therefore, we present ResDiff, a novel Diffusion Probabilistic Model based on Residual structure for Single Image Super-Resolution (SISR). ResDiff utilizes a combination of a CNN, which restores primary low-frequency components, and a DPM, which predicts the residual between the ground-truth image and the CNN predicted image. In contrast to the common diffusion-based methods that directly use LR space to guide the noise towards HR space, ResDiff utilizes the CNN's initial prediction to direct the noise towards the residual space between HR space and CNN-predicted space, which not only accelerates the generation process but also acquires superior sample quality. Additionally, a frequency-domain-based loss function for CNN is introduced to facilitate its restoration, and a frequency-domain guided diffusion is designed for DPM on behalf of predicting high-frequency details. The extensive experiments on multiple benchmark datasets demonstrate that ResDiff outperforms previous diffusion based methods in terms of shorter model convergence time, superior generation quality, and more diverse samples",
    "checked": true,
    "id": "1b8452be196d943241c76c170e93ea1d49aa07ea",
    "semantic_title": "resdiff: combining cnn and diffusion model for image super-resolution",
    "citation_count": 26,
    "authors": [
      "Shuyao Shang",
      "Zhengyang Shan",
      "Guangxing Liu",
      "LunQian Wang",
      "XingHua Wang",
      "Zekai Zhang",
      "Jinglin Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28747": {
    "title": "An Attentive Inductive Bias for Sequential Recommendation beyond the Self-Attention",
    "volume": "main",
    "abstract": "Sequential recommendation (SR) models based on Transformers have achieved remarkable successes. The self-attention mechanism of Transformers for computer vision and natural language processing suffers from the oversmoothing problem, i.e., hidden representations becoming similar to tokens. In the SR domain, we, for the first time, show that the same problem occurs. We present pioneering investigations that reveal the low-pass filtering nature of self-attention in the SR, which causes oversmoothing. To this end, we propose a novel method called Beyond Self-Attention for Sequential Recommendation (BSARec), which leverages the Fourier transform to i) inject an inductive bias by considering fine-grained sequential patterns and ii) integrate low and high-frequency information to mitigate oversmoothing. Our discovery shows significant advancements in the SR domain and is expected to bridge the gap for existing Transformer-based SR models. We test our proposed approach through extensive experiments on 6 benchmark datasets. The experimental results demonstrate that our model outperforms 7 baseline methods in terms of recommendation performance. Our code is available at https://github.com/yehjin-shin/BSARec",
    "checked": true,
    "id": "3cba6280473873e14f0bc19c63011ad1f4baf812",
    "semantic_title": "an attentive inductive bias for sequential recommendation beyond the self-attention",
    "citation_count": 4,
    "authors": [
      "Yehjin Shin",
      "Jeongwhan Choi",
      "Hyowon Wi",
      "Noseong Park"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28748": {
    "title": "A Diffusion-Based Pre-training Framework for Crystal Property Prediction",
    "volume": "main",
    "abstract": "Many significant problems involving crystal property prediction from 3D structures have limited labeled data due to expensive and time-consuming physical simulations or lab experiments. To overcome this challenge, we propose a pretrain-finetune framework for the crystal property prediction task named CrysDiff based on diffusion models. In the pre-training phase, CrysDiff learns the latent marginal distribution of crystal structures via the reconstruction task. Subsequently, CrysDiff can be fine-tuned under the guidance of the new sparse labeled data, fitting the conditional distribution of the target property given the crystal structures. To better model the crystal geometry, CrysDiff notably captures the full symmetric properties of the crystals, including the invariance of reflection, rotation, and periodic translation. Extensive experiments demonstrate that CrysDiff can significantly improve the performance of the downstream crystal property prediction task on multiple target properties, outperforming all the SOTA pre-training models for crystals with good margins on the popular JARVIS-DFT dataset",
    "checked": true,
    "id": "d945f29704f90139aab6da2122bf4cab7fa57bb4",
    "semantic_title": "a diffusion-based pre-training framework for crystal property prediction",
    "citation_count": 1,
    "authors": [
      "Zixing Song",
      "Ziqiao Meng",
      "Irwin King"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28749": {
    "title": "STEM: Unleashing the Power of Embeddings for Multi-Task Recommendation",
    "volume": "main",
    "abstract": "Multi-task learning (MTL) has gained significant popularity in recommender systems as it enables simultaneous optimization of multiple objectives. A key challenge in MTL is negative transfer, but existing studies explored negative transfer on all samples, overlooking the inherent complexities within them. We split the samples according to the relative amount of positive feedback among tasks. Surprisingly, negative transfer still occurs in existing MTL methods on samples that receive comparable feedback across tasks. Existing work commonly employs a shared-embedding paradigm, limiting the ability of modeling diverse user preferences on different tasks. In this paper, we introduce a novel Shared and Task-specific EMbeddings (STEM) paradigm that aims to incorporate both shared and task-specific embeddings to effectively capture task-specific user preferences. Under this paradigm, we propose a simple model STEM-Net, which is equipped with an All Forward Task-specific Backward gating network to facilitate the learning of task-specific embeddings and direct knowledge transfer across tasks. Remarkably, STEM-Net demonstrates exceptional performance on comparable samples, achieving positive transfer. Comprehensive evaluation on three public MTL recommendation datasets demonstrates that STEM-Net outperforms state-of-the-art models by a substantial margin. Our code is released at https://github.com/LiangcaiSu/STEM",
    "checked": true,
    "id": "2f06c35c0ddaafd8a80690141b7044968a434448",
    "semantic_title": "stem: unleashing the power of embeddings for multi-task recommendation",
    "citation_count": 8,
    "authors": [
      "Liangcai Su",
      "Junwei Pan",
      "Ximei Wang",
      "Xi Xiao",
      "Shijie Quan",
      "Xihua Chen",
      "Jie Jiang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28750": {
    "title": "Anchoring Path for Inductive Relation Prediction in Knowledge Graphs",
    "volume": "main",
    "abstract": "Aiming to accurately predict missing edges representing relations between entities, which are pervasive in real-world Knowledge Graphs (KGs), relation prediction plays a critical role in enhancing the comprehensiveness and utility of KGs. Recent research focuses on path-based methods due to their inductive and explainable properties. However, these methods face a great challenge when lots of reasoning paths do not form Closed Paths (CPs) in the KG. To address this challenge, we propose Anchoring Path Sentence Transformer (APST) by introducing Anchoring Paths (APs) to alleviate the reliance of CPs. Specifically, we develop a search-based description retrieval method to enrich entity descriptions and an assessment mechanism to evaluate the rationality of APs. APST takes both APs and CPs as the inputs of a unified Sentence Transformer architecture, enabling comprehensive predictions and high-quality explanations. We evaluate APST on three public datasets and achieve state-of-the-art (SOTA) performance in 30 of 36 transductive, inductive, and few-shot experimental settings",
    "checked": true,
    "id": "69436ae73d7587baad8e8096d846d54345d58312",
    "semantic_title": "anchoring path for inductive relation prediction in knowledge graphs",
    "citation_count": 0,
    "authors": [
      "Zhixiang Su",
      "Di Wang",
      "Chunyan Miao",
      "Lizhen Cui"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28751": {
    "title": "MAPTree: Beating \"Optimal\" Decision Trees with Bayesian Decision Trees",
    "volume": "main",
    "abstract": "Decision trees remain one of the most popular machine learning models today, largely due to their out-of-the-box performance and interpretability. In this work, we present a Bayesian approach to decision tree induction via maximum a posteriori inference of a posterior distribution over trees. We first demonstrate a connection between maximum a posteriori inference of decision trees and AND/OR search. Using this connection, we propose an AND/OR search algorithm, dubbed MAPTree, which is able to recover the maximum a posteriori tree. Lastly, we demonstrate the empirical performance of the maximum a posteriori tree both on synthetic data and in real world settings. On 16 real world datasets, MAPTree either outperforms baselines or demonstrates comparable performance but with much smaller trees. On a synthetic dataset, MAPTree also demonstrates greater robustness to noise and better generalization than existing approaches. Finally, MAPTree recovers the maxiumum a posteriori tree faster than existing sampling approaches and, in contrast with those algorithms, is able to provide a certificate of optimality. The code for our experiments is available at https://github.com/ThrunGroup/maptree",
    "checked": true,
    "id": "8c5cd913466fafda651a8a6b372f8cb91b12198f",
    "semantic_title": "maptree: beating \"optimal\" decision trees with bayesian decision trees",
    "citation_count": 0,
    "authors": [
      "Colin Sullivan",
      "Mo Tiwari",
      "Sebastian Thrun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28752": {
    "title": "CREAD: A Classification-Restoration Framework with Error Adaptive Discretization for Watch Time Prediction in Video Recommender Systems",
    "volume": "main",
    "abstract": "The watch time is a significant indicator of user satisfaction in video recommender systems. However, the prediction of watch time as a target variable is often hindered by its highly imbalanced distribution with a scarcity of observations for larger target values and over-populated samples for small values. State-of-the-art watch time prediction models discretize the continuous watch time into a set of buckets in order to consider the distribution of watch time. However, it is highly uninvestigated how these discrete buckets should be created from the continuous watch time distribution, and existing discretization approaches suffer from either a large learning error or a large restoration error. To address this challenge, we propose a Classification-Restoration framework with Error-Adaptive-Discretization (CREAD) to accurately predict the watch time. The proposed framework contains a discretization module, a classification module, and a restoration module. It predicts the watch time through multiple classification problems. The discretization process is a key contribution of the CREAD framework. We theoretically analyze the impacts of the discretization on the learning error and the restoration error, and then propose the error-adaptive discretization (EAD) technique to better balance the two errors, which achieves better performance over traditional discretization approaches. We conduct detailed offline evaluations on a public dataset and an industrial dataset, both showing performance gains through the proposed approach. Moreover, We have fully launched our framework to an online video platform, which resulted in a significant increase in users' video watch time by 0.29% through A/B testing. These results highlight the effectiveness of the CREAD framework in watch time prediction in video recommender systems",
    "checked": true,
    "id": "53e6792ce18afa3037cc439502ae5f3802681aed",
    "semantic_title": "cread: a classification-restoration framework with error adaptive discretization for watch time prediction in video recommender systems",
    "citation_count": 0,
    "authors": [
      "Jie Sun",
      "Zhaoying Ding",
      "Xiaoshuang Chen",
      "Qi Chen",
      "Yincheng Wang",
      "Kaiqiao Zhan",
      "Ben Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28753": {
    "title": "ModWaveMLP: MLP-Based Mode Decomposition and Wavelet Denoising Model to Defeat Complex Structures in Traffic Forecasting",
    "volume": "main",
    "abstract": "Traffic prediction is the core issue of Intelligent Transportation Systems. Recently, researchers have tended to use complex structures, such as transformer-based structures, for tasks such as traffic prediction. Notably, traffic data is simpler to process compared to text and images, which raises questions about the necessity of these structures. Additionally, when handling traffic data, researchers tend to manually design the model structure based on the data features, which makes the structure of traffic prediction redundant and the model generalizability limited. To address the above, we introduce the ‘ModWaveMLP'—A multilayer perceptron (MLP) based model designed according to mode decomposition and wavelet noise reduction information learning concepts. The model is based on simple MLP structure, which achieves the separation and prediction of different traffic modes and does not depend on additional features introduced such as the topology of the traffic network. By performing experiments on real-world datasets METR-LA and PEMS-BAY, our model achieves SOTA, outperforms GNN and transformer-based models, and outperforms those that introduce additional feature data with better generalizability, and we further demonstrate the effectiveness of the various parts of the model through ablation experiments. This offers new insights to subsequent researchers involved in traffic model design. The code is available at: https://github.com/Kqingzheng/ModWaveMLP",
    "checked": true,
    "id": "dfa0e7e38dd3ceffe21d54d5beacc173a30bbe7b",
    "semantic_title": "modwavemlp: mlp-based mode decomposition and wavelet denoising model to defeat complex structures in traffic forecasting",
    "citation_count": 0,
    "authors": [
      "Ke Sun",
      "Pei Liu",
      "Pengfei Li",
      "Zhifang  Liao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28754": {
    "title": "Motif-Aware Riemannian Graph Neural Network with Generative-Contrastive Learning",
    "volume": "main",
    "abstract": "Graphs are typical non-Euclidean data of complex structures. In recent years, Riemannian graph representation learning has emerged as an exciting alternative to Euclidean ones. However, Riemannian methods are still in an early stage: most of them present a single curvature (radius) regardless of structural complexity, suffer from numerical instability due to the exponential/logarithmic map, and lack the ability to capture motif regularity. In light of the issues above, we propose the problem of Motif-aware Riemannian Graph Representation Learning, seeking a numerically stable encoder to capture motif regularity in a diverse-curvature manifold without labels. To this end, we present a novel Motif-aware Riemannian model with Generative-Contrastive learning (MotifRGC), which conducts a minmax game in Riemannian manifold in a self-supervised manner. First, we propose a new type of Riemannian GCN (D-GCN), in which we construct a diverse-curvature manifold by a product layer with the diversified factor, and replace the exponential/logarithmic map by a stable kernel layer. Second, we introduce a motif-aware Riemannian generative-contrastive learning to capture motif regularity in the constructed manifold and learn motif-aware node representation without external labels. Empirical results show the superiority of MofitRGC",
    "checked": true,
    "id": "2d0623c2d05be3ecc88cf0852aa769c89d0aaa68",
    "semantic_title": "motif-aware riemannian graph neural network with generative-contrastive learning",
    "citation_count": 4,
    "authors": [
      "Li Sun",
      "Zhenhao Huang",
      "Zixi Wang",
      "Feiyang Wang",
      "Hao Peng",
      "Philip S. Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28755": {
    "title": "Fine-Tuning Graph Neural Networks by Preserving Graph Generative Patterns",
    "volume": "main",
    "abstract": "Recently, the paradigm of pre-training and fine-tuning graph neural networks has been intensively studied and applied in a wide range of graph mining tasks. Its success is generally attributed to the structural consistency between pre-training and downstream datasets, which, however, does not hold in many real-world scenarios. Existing works have shown that the structural divergence between pre-training and downstream graphs significantly limits the transferability when using the vanilla fine-tuning strategy. This divergence leads to model overfitting on pre-training graphs and causes difficulties in capturing the structural properties of the downstream graphs. In this paper, we identify the fundamental cause of structural divergence as the discrepancy of generative patterns between the pre-training and downstream graphs. Furthermore, we propose G-Tuning to preserve the generative patterns of downstream graphs. Given a downstream graph G, the core idea is to tune the pre-trained GNN so that it can reconstruct the generative patterns of G, the graphon W. However, the exact reconstruction of a graphon is known to be computationally expensive. To overcome this challenge, we provide a theoretical analysis that establishes the existence of a set of alternative graphons called graphon bases for any given graphon. By utilizing a linear combination of these graphon bases, we can efficiently approximate W. This theoretical finding forms the basis of our model, as it enables effective learning of the graphon bases and their associated coefficients. Compared with existing algorithms, G-Tuning demonstrates consistent performance improvement in 7 in-domain and 7 out-of-domain transfer learning experiments",
    "checked": true,
    "id": "5714916191ead5a581c6bb0258bb380e005f5d24",
    "semantic_title": "fine-tuning graph neural networks by preserving graph generative patterns",
    "citation_count": 2,
    "authors": [
      "Yifei Sun",
      "Qi Zhu",
      "Yang Yang",
      "Chunping Wang",
      "Tianyu Fan",
      "Jiajun Zhu",
      "Lei Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28756": {
    "title": "Finding Interpretable Class-Specific Patterns through Efficient Neural Search",
    "volume": "main",
    "abstract": "Discovering patterns in data that best describe the differences between classes allows to hypothesize and reason about class-specific mechanisms. In molecular biology, for example, these bear the promise of advancing the understanding of cellular processes differing between tissues or diseases, which could lead to novel treatments. To be useful in practice, methods that tackle the problem of finding such differential patterns have to be readily interpretable by domain experts, and scalable to the extremely high-dimensional data. In this work, we propose a novel, inherently interpretable binary neural network architecture Diffnaps that extracts differential patterns from data. Diffnaps is scalable to hundreds of thousands of features and robust to noise, thus overcoming the limitations of current state-of-the-art methods in large-scale applications such as in biology. We show on synthetic and real world data, including three biological applications, that unlike its competitors, Diffnaps consistently yields accurate, succinct, and interpretable class descriptions",
    "checked": true,
    "id": "a49d2bf65b928f5cb1ad2ae8a645d5ccb36da042",
    "semantic_title": "finding interpretable class-specific patterns through efficient neural search",
    "citation_count": 1,
    "authors": [
      "Nils Philipp Walter",
      "Jonas Fischer",
      "Jilles Vreeken"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28757": {
    "title": "End-to-End Learning of LTLf Formulae by Faithful LTLf Encoding",
    "volume": "main",
    "abstract": "It is important to automatically discover the underlying tree-structured formulae from large amounts of data. In this paper, we examine learning linear temporal logic on finite traces (LTLf) formulae, which is a tree structure syntactically and characterizes temporal properties semantically. Its core challenge is to bridge the gap between the concise tree-structured syntax and the complex LTLf semantics. Besides, the learning quality is endangered by explosion of the search space and wrong search bias guided by imperfect data. We tackle these challenges by proposing an LTLf encoding method to parameterize a neural network so that the neural computation is able to simulate the inference of LTLf formulae. We first identify faithful LTLf encoding, a subclass of LTLf encoding, which has a one-to-one correspondence to LTLf formulae. Faithful encoding guarantees that the learned parameter assignment of the neural network can directly be interpreted to an LTLf formula. With such an encoding method, we then propose an end-to-end approach, TLTLf, to learn LTLf formulae through neural networks parameterized by our LTLf encoding method. Experimental results demonstrate that our approach achieves state-of-the-art performance with up to 7% improvement in accuracy, highlighting the benefits of introducing the faithful LTLf encoding",
    "checked": true,
    "id": "2332bc247ddc79576314b38973ba81344bfea940",
    "semantic_title": "end-to-end learning of ltlf formulae by faithful ltlf encoding",
    "citation_count": 1,
    "authors": [
      "Hai Wan",
      "Pingjia Liang",
      "Jianfeng Du",
      "Weilin Luo",
      "Rongzhen Ye",
      "Bo Peng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28758": {
    "title": "Contributing Dimension Structure of Deep Feature for Coreset Selection",
    "volume": "main",
    "abstract": "Coreset selection seeks to choose a subset of crucial training samples for efficient learning. It has gained traction in deep learning, particularly with the surge in training dataset sizes. Sample selection hinges on two main aspects: a sample's representation in enhancing performance and the role of sample diversity in averting overfitting. Existing methods typically measure both the representation and diversity of data based on similarity metrics, such as L2-norm. They have capably tackled representation via distribution matching guided by the similarities of features, gradients, or other information between data. However, the results of effectively diverse sample selection are mired in sub-optimality. This is because the similarity metrics usually simply aggregate dimension similarities without acknowledging disparities among the dimensions that significantly contribute to the final similarity. As a result, they fall short of adequately capturing diversity. To address this, we propose a feature-based diversity constraint, compelling the chosen subset to exhibit maximum diversity. Our key lies in the introduction of a novel Contributing Dimension Structure (CDS) metric. Different from similarity metrics that measure the overall similarity of high-dimensional features, our CDS metric considers not only the reduction of redundancy in feature dimensions, but also the difference between dimensions that contribute significantly to the final similarity. We reveal that existing methods tend to favor samples with similar CDS, leading to a reduced variety of CDS types within the coreset and subsequently hindering model performance. In response, we enhance the performance of five classical selection methods by integrating the CDS constraint. Our experiments on three datasets demonstrate the general effectiveness of the proposed method in boosting existing methods",
    "checked": true,
    "id": "958c3ddf6e3d21231e941cfda607be03b7399b20",
    "semantic_title": "contributing dimension structure of deep feature for coreset selection",
    "citation_count": 0,
    "authors": [
      "Zhijing Wan",
      "Zhixiang Wang",
      "Yuran Wang",
      "Zheng Wang",
      "Hongyuan Zhu",
      "Shin'ichi Satoh"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28759": {
    "title": "Towards Dynamic Spatial-Temporal Graph Learning: A Decoupled Perspective",
    "volume": "main",
    "abstract": "With the progress of urban transportation systems, a significant amount of high-quality traffic data is continuously collected through streaming manners, which has propelled the prosperity of the field of spatial-temporal graph prediction. In this paper, rather than solely focusing on designing powerful models for static graphs, we shift our focus to spatial-temporal graph prediction in the dynamic scenario, which involves a continuously expanding and evolving underlying graph. To address inherent challenges, a decoupled learning framework (DLF) is proposed in this paper, which consists of a spatial-temporal graph learning network (DSTG) with a specialized decoupling training strategy. Incorporating inductive biases of time-series structures, DSTG can interpret time dependencies into latent trend and seasonal terms. To enable prompt adaptation to the evolving distribution of the dynamic graph, our decoupling training strategy is devised to iteratively update these two types of patterns. Specifically, for learning seasonal patterns, we conduct thorough training for the model using a long time series (e.g., three months of data). To enhance the learning ability of the model, we also introduce the masked auto-encoding mechanism. During this period, we frequently update trend patterns to expand new information from dynamic graphs. Considering both effectiveness and efficiency, we develop a subnet sampling strategy to select a few representative nodes for fine-tuning the weights of the model. These sampled nodes cover unseen patterns and previously learned patterns. Experiments on dynamic spatial-temporal graph datasets further demonstrate the competitive performance, superior efficiency, and strong scalability of the proposed framework",
    "checked": true,
    "id": "e3bb7012edfaac357311eca07515d193b4cf26bb",
    "semantic_title": "towards dynamic spatial-temporal graph learning: a decoupled perspective",
    "citation_count": 1,
    "authors": [
      "Binwu Wang",
      "Pengkun Wang",
      "Yudong Zhang",
      "Xu Wang",
      "Zhengyang Zhou",
      "Lei Bai",
      "Yang Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28760": {
    "title": "EnMatch: Matchmaking for Better Player Engagement via Neural Combinatorial Optimization",
    "volume": "main",
    "abstract": "Matchmaking is a core task in e-sports and online games, as it contributes to player engagement and further influences the game's lifecycle. Previous methods focus on creating fair games at all times. They divide players into different tiers based on skill levels and only select players from the same tier for each game. Though this strategy can ensure fair matchmaking, it is not always good for player engagement. In this paper, we propose a novel Engagement-oriented Matchmaking (EnMatch) framework to ensure fair games and simultaneously enhance player engagement. Two main issues need to be addressed. First, it is unclear how to measure the impact of different team compositions and confrontations on player engagement during the game considering the variety of player characteristics. Second, such a detailed consideration on every single player during matchmaking will result in an NP-hard combinatorial optimization problem with non-linear objectives. In light of these challenges, we turn to real-world data analysis to reveal engagement-related factors. The resulting insights guide the development of engagement modeling, enabling the estimation of quantified engagement before a match is completed. To handle the combinatorial optimization problem, we formulate the problem into a reinforcement learning framework, in which a neural combinatorial optimization problem is built and solved. The performance of EnMatch is finally demonstrated through the comparison with other state-of-the-art methods based on several real-world datasets and online deployments on two games",
    "checked": true,
    "id": "d752293be11e0d6ffb3534cc95704023705eb511",
    "semantic_title": "enmatch: matchmaking for better player engagement via neural combinatorial optimization",
    "citation_count": 0,
    "authors": [
      "Kai Wang",
      "Haoyu Liu",
      "Zhipeng Hu",
      "Xiaochuan Feng",
      "Minghao Zhao",
      "Shiwei Zhao",
      "Runze Wu",
      "Xudong Shen",
      "Tangjie Lv",
      "Changjie Fan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28761": {
    "title": "Review-Enhanced Hierarchical Contrastive Learning for Recommendation",
    "volume": "main",
    "abstract": "Designed to establish potential relations and distill high-order representations, graph-based recommendation systems continue to reveal promising results by jointly modeling ratings and reviews. However, existing studies capture simple review relations, failing to (1) completely explore hidden connections between users (or items), (2) filter out redundant information derived from reviews, and (3) model the behavioral association between rating and review interactions. To address these challenges, we propose a review-enhanced hierarchical contrastive learning, namely ReHCL. First, ReHCL constructs topic and semantic graphs to fully mine review relations from different views. Moreover, a cross-view graph contrastive learning is used to achieve enhancement of node representations and extract useful review knowledge. Meanwhile, we design a neighbor-based positive sampling to capture the graph-structured similarity between topic and semantic views, further performing efficient contrast and reducing redundant noise. Next, we propose a cross-modal contrastive learning to match the rating and review representations, by exploring the association between ratings and reviews. Lastly, these two contrastive learning modes form a hierarchical contrastive learning task, which is applied to enhance the final recommendation task. Extensive experiments verify the superiority of ReHCL compared with state-of-the-arts",
    "checked": true,
    "id": "b606ba6048368a68f786180833368b05a88fb8ff",
    "semantic_title": "review-enhanced hierarchical contrastive learning for recommendation",
    "citation_count": 0,
    "authors": [
      "Ke Wang",
      "Yanmin Zhu",
      "Tianzi Zang",
      "Chunyang Wang",
      "Mengyuan Jing"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28762": {
    "title": "Pseudo-Label Calibration Semi-supervised Multi-Modal Entity Alignment",
    "volume": "main",
    "abstract": "Multi-modal entity alignment (MMEA) aims to identify equivalent entities between two multi-modal knowledge graphs for integration. Unfortunately, prior arts have attempted to improve the interaction and fusion of multi-modal information, which have overlooked the influence of modal-specific noise and the usage of labeled and unlabeled data in semi-supervised settings. In this work, we introduce a Pseudo-label Calibration Multi-modal Entity Alignment (PCMEA) in a semi-supervised way. Specifically, in order to generate holistic entity representations, we first devise various embedding modules and attention mechanisms to extract visual, structural, relational, and attribute features. Different from the prior direct fusion methods, we next propose to exploit mutual information maximization to filter the modal-specific noise and to augment modal-invariant commonality. Then, we combine pseudo-label calibration with momentum-based contrastive learning to make full use of the labeled and unlabeled data, which improves the quality of pseudo-label and pulls aligned entities closer. Finally, extensive experiments on two MMEA datasets demonstrate the effectiveness of our PCMEA, which yields state-of-the-art performance",
    "checked": true,
    "id": "48b6fcffadfdd6076a30371f138a36ec101cae28",
    "semantic_title": "pseudo-label calibration semi-supervised multi-modal entity alignment",
    "citation_count": 0,
    "authors": [
      "Luyao Wang",
      "Pengnian Qi",
      "Xigang Bao",
      "Chunlai Zhou",
      "Biao Qin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28763": {
    "title": "Preference Aware Dual Contrastive Learning for Item Cold-Start Recommendation",
    "volume": "main",
    "abstract": "Existing cold-start recommendation methods often adopt item-level alignment strategies to align the content feature and the collaborative feature of warm items for model training, however, cold items in the test stage have no historical interactions with users to obtain the collaborative feature. These existing models ignore the aforementioned condition of cold items in the training stage, resulting in the performance limitation. In this paper, we propose a preference aware dual contrastive learning based recommendation model (PAD-CLRec), where the user preference is explored to take into account the condition of cold items for feature alignment. Here, the user preference is obtained by aggregating a group of collaborative feature of the warm items in the user's purchase records. Then, a group-level alignment between the user preference and the item's content feature can be realized via a proposed preference aware contrastive function for enhancing cold-item recommendation. In addition, a joint objective function is introduced to achieve a better trade-off between the recommendation performance of warm items and cold items from both item-level and group-level perspectives, yielding better overall recommendation performance. Extensive experiments are conducted to demonstrate the effectiveness of the proposed method, and the results show the superiority of our method, as compared with the state-of-the-arts",
    "checked": true,
    "id": "6684a1717bc1a2338b7a2d73bb738a4b024a595a",
    "semantic_title": "preference aware dual contrastive learning for item cold-start recommendation",
    "citation_count": 0,
    "authors": [
      "Wenbo Wang",
      "Bingquan Liu",
      "Lili Shan",
      "Chengjie Sun",
      "Ben Chen",
      "Jian Guan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28764": {
    "title": "Deciphering Compatibility Relationships with Textual Descriptions via Extraction and Explanation",
    "volume": "main",
    "abstract": "Understanding and accurately explaining compatibility relationships between fashion items is a challenging problem in the burgeoning domain of AI-driven outfit recommendations. Present models, while making strides in this area, still occasionally fall short, offering explanations that can be elementary and repetitive. This work aims to address these shortcomings by introducing the Pair Fashion Explanation (PFE) dataset, a unique resource that has been curated to illuminate these compatibility relationships. Furthermore, we propose an innovative two stage pipeline model that leverages this dataset. This fine-tuning allows the model to generate explanations that convey the compatibility relationships between items. Our experiments showcase the model's potential in crafting descriptions that are knowledgeable, aligned with ground-truth matching correlations, and that produce understandable and informative descriptions, as assessed by both automatic metrics and human evaluation. Our code and data are released at https://github.com/wangyu-ustc/PairFashionExplanation",
    "checked": true,
    "id": "0208b95cc3b677113c14335b899effe0fcad90a9",
    "semantic_title": "deciphering compatibility relationships with textual descriptions via extraction and explanation",
    "citation_count": 1,
    "authors": [
      "Yu Wang",
      "Zexue He",
      "Zhankui He",
      "Hao Xu",
      "Julian McAuley"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28765": {
    "title": "Open-Set Graph Domain Adaptation via Separate Domain Alignment",
    "volume": "main",
    "abstract": "Domain adaptation has become an attractive learning paradigm, as it can leverage source domains with rich labels to deal with classification tasks in an unlabeled target domain. A few recent studies develop domain adaptation approaches for graph-structured data. In the case of node classification task, current domain adaptation methods only focus on the closed-set setting, where source and target domains share the same label space. A more practical assumption is that the target domain may contain new classes that are not included in the source domain. Therefore, in this paper, we introduce a novel and challenging problem for graphs, i.e., open-set domain adaptive node classification, and propose a new approach to solve it. Specifically, we develop an algorithm for efficient knowledge transfer from a labeled source graph to an unlabeled target graph under a separate domain alignment (SDA) strategy, in order to learn discriminative feature representations for the target graph. Our goal is to not only correctly classify target nodes into the known classes, but also classify unseen types of nodes into an unknown class. Experimental results on real-world datasets show that our method outperforms existing methods on graph domain adaptation",
    "checked": true,
    "id": "fe51d5f2a9d1f0aa89b993464c7941f68812fb66",
    "semantic_title": "open-set graph domain adaptation via separate domain alignment",
    "citation_count": 0,
    "authors": [
      "Yu Wang",
      "Ronghang Zhu",
      "Pengsheng Ji",
      "Sheng Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28766": {
    "title": "G^2SAM: Graph-Based Global Semantic Awareness Method for Multimodal Sarcasm Detection",
    "volume": "main",
    "abstract": "Multimodal sarcasm detection, aiming to detect the ironic sentiment within multimodal social data, has gained substantial popularity in both the natural language processing and computer vision communities. Recently, graph-based studies by drawing sentimental relations to detect multimodal sarcasm have made notable advancements. However, they have neglected exploiting graph-based global semantic congruity from existing instances to facilitate the prediction, which ultimately hinders the model's performance. In this paper, we introduce a new inference paradigm that leverages global graph-based semantic awareness to handle this task. Firstly, we construct fine-grained multimodal graphs for each instance and integrate them into semantic space to draw graph-based relations. During inference, we leverage global semantic congruity to retrieve k-nearest neighbor instances in semantic space as references for voting on the final prediction. To enhance the semantic correlation of representation in semantic space, we also introduce label-aware graph contrastive learning to further improve the performance. Experimental results demonstrate that our model achieves state-of-the-art (SOTA) performance in multimodal sarcasm detection. The code will be available at https://github.com/upccpu/G2SAM",
    "checked": true,
    "id": "f1505b0153099e10e10109bca69bbe1f73467209",
    "semantic_title": "g^2sam: graph-based global semantic awareness method for multimodal sarcasm detection",
    "citation_count": 1,
    "authors": [
      "Yiwei Wei",
      "Shaozu Yuan",
      "Hengyang Zhou",
      "Longbiao Wang",
      "Zhiling Yan",
      "Ruosong Yang",
      "Meng Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28767": {
    "title": "Poincaré Differential Privacy for Hierarchy-Aware Graph Embedding",
    "volume": "main",
    "abstract": "Hierarchy is an important and commonly observed topological property in real-world graphs that indicate the relationships between supervisors and subordinates or the organizational behavior of human groups. As hierarchy is introduced as a new inductive bias into the Graph Neural Networks (GNNs) in various tasks, it implies latent topological relations for attackers to improve their inference attack performance, leading to serious privacy leakage issues. In addition, existing privacy-preserving frameworks suffer from reduced protection ability in hierarchical propagation due to the deficiency of adaptive upper-bound estimation of the hierarchical perturbation boundary. It is of great urgency to effectively leverage the hierarchical property of data while satisfying privacy guarantees. To solve the problem, we propose the Poincar\\'e Differential Privacy framework, named PoinDP, to protect the hierarchy-aware graph embedding based on hyperbolic geometry. Specifically, PoinDP first learns the hierarchy weights for each entity based on the Poincar\\'e model in hyperbolic space. Then, the Personalized Hierarchy-aware Sensitivity is designed to measure the sensitivity of the hierarchical structure and adaptively allocate the privacy protection strength. Besides, Hyperbolic Gaussian Mechanism (HGM) is proposed to extend the Gaussian mechanism in Euclidean space to hyperbolic space to realize random perturbations that satisfy differential privacy under the hyperbolic space metric. Extensive experiment results on five real-world datasets demonstrate the proposed PoinDP's advantages of effective privacy protection while maintaining good performance on the node classification task",
    "checked": true,
    "id": "54efa5dd888cffc491431f833d6255fd6863e6bb",
    "semantic_title": "poincaré differential privacy for hierarchy-aware graph embedding",
    "citation_count": 2,
    "authors": [
      "Yuecen Wei",
      "Haonan Yuan",
      "Xingcheng Fu",
      "Qingyun Sun",
      "Hao Peng",
      "Xianxian Li",
      "Chunming Hu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28768": {
    "title": "Pairwise-Label-Based Deep Incremental Hashing with Simultaneous Code Expansion",
    "volume": "main",
    "abstract": "Deep incremental hashing has become a subject of considerable interest due to its capability to learn hash codes in an incremental manner, eliminating the need to generate codes for classes that have already been learned. However, accommodating more classes requires longer hash codes, and regenerating database codes becomes inevitable when code expansion is required. In this paper, we present a unified deep hash framework that can simultaneously learn new classes and increase hash code capacity. Specifically, we design a triple-channel asymmetric framework to optimize a new CNN model with a target code length and a code projection matrix. This enables us to directly generate hash codes for new images, and efficiently generate expanded hash codes for original database images from the old ones with the learned projection matrix. Meanwhile, we propose a pairwise-label-based incremental similarity-preserving loss to optimize the new CNN model, which can incrementally preserve new similarities while maintaining the old ones. Additionally, we design a double-end quantization loss to reduce the quantization error from new and original query images. As a result, our method efficiently embeds both new and original similarities into the expanded hash codes, while keeping the original database codes unchanged. We conduct extensive experiments on three widely-used image retrieval benchmarks, demonstrating that our method can significantly reduce the time required to expand existing database codes, while maintaining state-of-the-art retrieval performance",
    "checked": true,
    "id": "cf1a20cb2c93415c8a4da1cebb898430603c6870",
    "semantic_title": "pairwise-label-based deep incremental hashing with simultaneous code expansion",
    "citation_count": 0,
    "authors": [
      "Dayan Wu",
      "Qinghang Su",
      "Bo Li",
      "Weiping Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28769": {
    "title": "Exploring Large Language Model for Graph Data Understanding in Online Job Recommendations",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have revolutionized natural language processing tasks, demonstrating their exceptional capabilities in various domains. However, their potential for graph semantic mining in job recommendations remains largely unexplored. This paper focuses on unveiling the capability of large language models in understanding behavior graphs and leveraging this understanding to enhance recommendations in online recruitment, including promoting out-of-distribution (OOD) applications. We present a novel framework that harnesses the rich contextual information and semantic representations provided by large language models to analyze behavior graphs and uncover underlying patterns and relationships. Specifically, we propose a meta-path prompt constructor that aids LLM recommender in grasping the semantics of behavior graphs for the first time and design a corresponding path augmentation module to alleviate the prompt bias introduced by path-based sequence input. By facilitating this capability, our framework enables personalized and accurate job recommendations for individual users. We evaluate the effectiveness of our approach on comprehensive real-world datasets and demonstrate its ability to improve the relevance and quality of recommended results. This research not only sheds light on the untapped potential of large language models but also provides valuable insights for developing advanced recommendation systems in the recruitment market. The findings contribute to the growing field of natural language processing and offer practical implications for enhancing job search experiences",
    "checked": true,
    "id": "09826f769cef899388909d9f4cfaa335429c41a4",
    "semantic_title": "exploring large language model for graph data understanding in online job recommendations",
    "citation_count": 32,
    "authors": [
      "Likang Wu",
      "Zhaopeng Qiu",
      "Zhi Zheng",
      "Hengshu Zhu",
      "Enhong Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28770": {
    "title": "CI-STHPAN: Pre-trained Attention Network for Stock Selection with Channel-Independent Spatio-Temporal Hypergraph",
    "volume": "main",
    "abstract": "Quantitative stock selection is one of the most challenging FinTech tasks due to the non-stationary dynamics and complex market dependencies. Existing studies rely on channel mixing methods, exacerbating the issue of distribution shift in financial time series. Additionally, complex model structures they build make it difficult to handle very long sequences. Furthermore, most of them are based on predefined stock relationships thus making it difficult to capture the dynamic and highly volatile stock markets. To address the above issues, in this paper, we propose Channel-Independent based Spatio-Temporal Hypergraph Pre-trained Attention Networks (CI-STHPAN), a two-stage framework for stock selection, involving Transformer and HGAT based stock time series self-supervised pre-training and stock-ranking based downstream task fine-tuning. We calculate the similarity of stock time series of different channel in dynamic intervals based on Dynamic Time Warping (DTW), and further construct channel-independent stock dynamic hypergraph based on the similarity. Experiments with NASDAQ and NYSE markets data over five years show that our framework outperforms SOTA approaches in terms of investment return ratio (IRR) and Sharpe ratio (SR). Additionally, we find that even without introducing graph information, self-supervised learning based on the vanilla Transformer Encoder also surpasses SOTA results. Notable improvements are gained on the NYSE market. It is mainly attributed to the improvement of fine-tuning approach on Information Coefficient (IC) and Information Ratio based IC (ICIR), indicating that the fine-tuning method enhances the accuracy and stability of the model prediction",
    "checked": true,
    "id": "88e6bae24ae33dc1b381213dfd105c1a21b31e6d",
    "semantic_title": "ci-sthpan: pre-trained attention network for stock selection with channel-independent spatio-temporal hypergraph",
    "citation_count": 0,
    "authors": [
      "Hongjie Xia",
      "Huijie Ao",
      "Long Li",
      "Yu Liu",
      "Sen Liu",
      "Guangnan Ye",
      "Hongfeng Chai"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28771": {
    "title": "Feature Distribution Matching by Optimal Transport for Effective and Robust Coreset Selection",
    "volume": "main",
    "abstract": "Training neural networks with good generalization requires large computational costs in many deep learning methods due to large-scale datasets and over-parameterized models. Despite the emergence of a number of coreset selection methods to reduce the computational costs, the problem of coreset distribution bias, i.e., the skewed distribution between the coreset and the entire dataset, has not been well studied. In this paper, we find that the closer the feature distribution of the coreset is to that of the entire dataset, the better the generalization performance of the coreset, particularly under extreme pruning. This motivates us to propose a simple yet effective method for coreset selection to alleviate the distribution bias between the coreset and the entire dataset, called feature distribution matching (FDMat). Unlike gradient-based methods, which selects samples with larger gradient values or approximates gradient values of the entire dataset, FDMat aims to select coreset that is closest to feature distribution of the entire dataset. Specifically, FDMat transfers coreset selection as an optimal transport problem from the coreset to the entire dataset in feature embedding spaces. Moreover, our method shows strong robustness due to the removal of samples far from the distribution, especially for the entire dataset containing noisy and class-imbalanced samples. Extensive experiments on multiple benchmarks show that FDMat can improve the performance of coreset selection than existing coreset methods. The code is available at https://github.com/successhaha/FDMat",
    "checked": true,
    "id": "c693b14ba2ce0d08a71eac534767678c79b74666",
    "semantic_title": "feature distribution matching by optimal transport for effective and robust coreset selection",
    "citation_count": 0,
    "authors": [
      "Weiwei Xiao",
      "Yongyong Chen",
      "Qiben Shan",
      "Yaowei Wang",
      "Jingyong Su"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28772": {
    "title": "NestE: Modeling Nested Relational Structures for Knowledge Graph Reasoning",
    "volume": "main",
    "abstract": "Reasoning with knowledge graphs (KGs) has primarily focused on triple-shaped facts. Recent advancements have been explored to enhance the semantics of these facts by incorporating more potent representations, such as hyper-relational facts. However, these approaches are limited to atomic facts, which describe a single piece of information. This paper extends beyond atomic facts and delves into nested facts, represented by quoted triples where subjects and objects are triples themselves (e.g., ((BarackObama, holds_position, President), succeed_by, (DonaldTrump, holds_position, President))). These nested facts enable the expression of complex semantics like situations over time and logical patterns} over entities and relations. In response, we introduce NestE, a novel KG embedding approach that captures the semantics of both atomic and nested factual knowledge. NestE represents each atomic fact as a 1*3 matrix, and each nested relation is modeled as a 3*3 matrix that rotates the 1*3 atomic fact matrix through matrix multiplication. Each element of the matrix is represented as a complex number in the generalized 4D hypercomplex space, including (spherical) quaternions, hyperbolic quaternions, and split-quaternions. Through thorough analysis, we demonstrate the embedding's efficacy in capturing diverse logical patterns over nested facts, surpassing the confines of first-order logic-like expressions. Our experimental results showcase NestE's significant performance gains over current baselines in triple prediction and conditional link prediction. The code and pre-trained models are open available at https://github.com/xiongbo010/NestE",
    "checked": true,
    "id": "7e19b3fababc61088f378481b219d19589319858",
    "semantic_title": "neste: modeling nested relational structures for knowledge graph reasoning",
    "citation_count": 0,
    "authors": [
      "Bo Xiong",
      "Mojtaba Nayyeri",
      "Linhao Luo",
      "Zihao Wang",
      "Shirui Pan",
      "Steffen Staab"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28773": {
    "title": "Revisiting Graph-Based Fraud Detection in Sight of Heterophily and Spectrum",
    "volume": "main",
    "abstract": "Graph-based fraud detection (GFD) can be regarded as a challenging semi-supervised node binary classification task. In recent years, Graph Neural Networks (GNN) have been widely applied to GFD, characterizing the anomalous possibility of a node by aggregating neighbor information. However, fraud graphs are inherently heterophilic, thus most of GNNs perform poorly due to their assumption of homophily. In addition, due to the existence of heterophily and class imbalance problem, the existing models do not fully utilize the precious node label information. To address the above issues, this paper proposes a semi-supervised GNN-based fraud detector SEC-GFD. This detector includes a hybrid filtering module and a local environmental constraint module, the two modules are utilized to solve heterophily and label utilization problem respectively. The first module starts from the perspective of the spectral domain, and solves the heterophily problem to a certain extent. Specifically, it divides the spectrum into various mixed-frequency bands based on the correlation between spectrum energy distribution and heterophily. Then in order to make full use of the node label information, a local environmental constraint module is adaptively designed. The comprehensive experimental results on four real-world fraud detection datasets denote that SEC-GFD outperforms other competitive graph-based fraud detectors. We release our code at https://github.com/Sunxkissed/SEC-GFD",
    "checked": true,
    "id": "04012d052ac9d2c7ba2258f8c119749fca69828a",
    "semantic_title": "revisiting graph-based fraud detection in sight of heterophily and spectrum",
    "citation_count": 3,
    "authors": [
      "Fan Xu",
      "Nan Wang",
      "Hao Wu",
      "Xuezhi Wen",
      "Xibin Zhao",
      "Hai Wan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28774": {
    "title": "Empowering Dual-Level Graph Self-Supervised Pretraining with Motif Discovery",
    "volume": "main",
    "abstract": "While self-supervised graph pretraining techniques have shown promising results in various domains, their application still experiences challenges of limited topology learning, human knowledge dependency, and incompetent multi-level interactions. To address these issues, we propose a novel solution, Dual-level Graph self-supervised Pretraining with Motif discovery (DGPM), which introduces a unique dual-level pretraining structure that orchestrates node-level and subgraph-level pretext tasks. Unlike prior approaches, DGPM autonomously uncovers significant graph motifs through an edge pooling module, aligning learned motif similarities with graph kernel-based similarities. A cross-matching task enables sophisticated node-motif interactions and novel representation learning. Extensive experiments on 15 datasets validate DGPM's effectiveness and generalizability, outperforming state-of-the-art methods in unsupervised representation learning and transfer learning settings. The autonomously discovered motifs demonstrate the potential of DGPM to enhance robustness and interpretability",
    "checked": true,
    "id": "3460757b8aa8b8dfaa2d751a375a800e9e5bb234",
    "semantic_title": "empowering dual-level graph self-supervised pretraining with motif discovery",
    "citation_count": 1,
    "authors": [
      "Pengwei Yan",
      "Kaisong Song",
      "Zhuoren Jiang",
      "Yangyang Kang",
      "Tianqianjin Lin",
      "Changlong Sun",
      "Xiaozhong Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28775": {
    "title": "Hypergraph Joint Representation Learning for Hypervertices and Hyperedges via Cross Expansion",
    "volume": "main",
    "abstract": "Hypergraph captures high-order information in structured data and obtains much attention in machine learning and data mining. Existing approaches mainly learn representations for hypervertices by transforming a hypergraph to a standard graph, or learn representations for hypervertices and hyperedges in separate spaces. In this paper, we propose a hypergraph expansion method to transform a hypergraph to a standard graph while preserving high-order information. Different from previous hypergraph expansion approaches like clique expansion and star expansion, we transform both hypervertices and hyperedges in the hypergraph to vertices in the expanded graph, and construct connections between hypervertices or hyperedges, so that richer relationships can be used in graph learning. Based on the expanded graph, we propose a learning model to embed hypervertices and hyperedges in a joint representation space. Compared with the method of learning separate spaces for hypervertices and hyperedges, our method is able to capture common knowledge involved in hypervertices and hyperedges, and also improve the data efficiency and computational efficiency. To better leverage structure information, we minimize the graph reconstruction loss to preserve the structure information in the model. We perform experiments on both hypervertex classification and hyperedge classification tasks to demonstrate the effectiveness of our proposed method",
    "checked": true,
    "id": "f8a4a67cdbb9b6f9c9cabbb655e0c290242ca854",
    "semantic_title": "hypergraph joint representation learning for hypervertices and hyperedges via cross expansion",
    "citation_count": 0,
    "authors": [
      "Yuguang Yan",
      "Yuanlin Chen",
      "Shibo Wang",
      "Hanrui Wu",
      "Ruichu Cai"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28776": {
    "title": "FairSIN: Achieving Fairness in Graph Neural Networks through Sensitive Information Neutralization",
    "volume": "main",
    "abstract": "Despite the remarkable success of graph neural networks (GNNs) in modeling graph-structured data, like other machine learning models, GNNs are also susceptible to making biased predictions based on sensitive attributes, such as race and gender. For fairness consideration, recent state-of-the-art (SOTA) methods propose to filter out sensitive information from inputs or representations, e.g., edge dropping or feature masking. However, we argue that such filtering-based strategies may also filter out some non-sensitive feature information, leading to a sub-optimal trade-off between predictive performance and fairness. To address this issue, we unveil an innovative neutralization-based paradigm, where additional Fairness-facilitating Features (F3) are incorporated into node features or representations before message passing. The F3 are expected to statistically neutralize the sensitive bias in node representations and provide additional nonsensitive information. We also provide theoretical explanations for our rationale, concluding that F3 can be realized by emphasizing the features of each node's heterogeneous neighbors (neighbors with different sensitive attributes). We name our method as FairSIN, and present three implementation variants from both data-centric and model-centric perspectives. Experimental results on five benchmark datasets with three different GNN backbones show that FairSIN significantly improves fairness metrics while maintaining high prediction accuracies. Codes and appendix can be found at https://github.com/BUPT-GAMMA/FariSIN",
    "checked": true,
    "id": "d54fb8f5d5b53f0bef0ff0286b3f2da5225c282f",
    "semantic_title": "fairsin: achieving fairness in graph neural networks through sensitive information neutralization",
    "citation_count": 1,
    "authors": [
      "Cheng Yang",
      "Jixi Liu",
      "Yunhe Yan",
      "Chuan Shi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28777": {
    "title": "Fine-Tuning Large Language Model Based Explainable Recommendation with Explainable Quality Reward",
    "volume": "main",
    "abstract": "Large language model-based explainable recommendation (LLM-based ER) systems can provide remarkable human-like explanations and have widely received attention from researchers. However, the original LLM-based ER systems face three low-quality problems in their generated explanations, i.e., lack of personalization, inconsistency, and questionable explanation data. To address these problems, we propose a novel LLM-based ER model denoted as LLM2ER to serve as a backbone and devise two innovative explainable quality reward models for fine-tuning such a backbone in a reinforcement learning paradigm, ultimately yielding a fine-tuned model denoted as LLM2ER-EQR, which can provide high-quality explanations. LLM2ER-EQR can generate personalized, informative, and consistent high-quality explanations learned from questionable-quality explanation datasets. Extensive experiments conducted on three real-world datasets demonstrate that our model can generate fluent, diverse, informative, and highly personalized explanations",
    "checked": true,
    "id": "bff611796cfc60a7312400cd037789d6afdb3a45",
    "semantic_title": "fine-tuning large language model based explainable recommendation with explainable quality reward",
    "citation_count": 0,
    "authors": [
      "Mengyuan Yang",
      "Mengying Zhu",
      "Yan Wang",
      "Linxun Chen",
      "Yilei Zhao",
      "Xiuyuan Wang",
      "Bing Han",
      "Xiaolin Zheng",
      "Jianwei Yin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28778": {
    "title": "Graph Neural Networks with Soft Association between Topology and Attribute",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs) have shown great performance in learning representations for graph-structured data. However, recent studies have found that the interference between topology and attribute can lead to distorted node representations. Most GNNs are designed based on homophily assumptions, thus they cannot be applied to graphs with heterophily. This research critically analyzes the propagation principles of various GNNs and the corresponding challenges from an optimization perspective. A novel GNN called Graph Neural Networks with Soft Association between Topology and Attribute (GNN-SATA) is proposed. Different embeddings are utilized to gain insights into attributes and structures while establishing their interconnections through soft association. Further as integral components of the soft association, a Graph Pruning Module (GPM) and Graph Augmentation Module (GAM) are developed. These modules dynamically remove or add edges to the adjacency relationships to make the model better fit with graphs with homophily or heterophily. Experimental results on homophilic and heterophilic graph datasets convincingly demonstrate that the proposed GNN-SATA effectively captures more accurate adjacency relationships and outperforms state-of-the-art approaches. Especially on the heterophilic graph dataset Squirrel, GNN-SATA achieves a 2.81% improvement in accuracy, utilizing merely 27.19% of the original number of adjacency relationships. Our code is released at https://github.com/wwwfadecom/GNN-SATA",
    "checked": true,
    "id": "819d9ef75975c78c5ce12e54af93737f4b698f55",
    "semantic_title": "graph neural networks with soft association between topology and attribute",
    "citation_count": 0,
    "authors": [
      "Yachao Yang",
      "Yanfeng Sun",
      "Shaofan Wang",
      "Jipeng Guo",
      "Junbin Gao",
      "Fujiao Ju",
      "Baocai  Yin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28779": {
    "title": "TriSampler: A Better Negative Sampling Principle for Dense Retrieval",
    "volume": "main",
    "abstract": "Negative sampling stands as a pivotal technique in dense retrieval, essential for training effective retrieval models and significantly impacting retrieval performance. While existing negative sampling methods have made commendable progress by leveraging hard negatives, a comprehensive guiding principle for constructing negative candidates and designing negative sampling distributions is still lacking. To bridge this gap, we embark on a theoretical analysis of negative sampling in dense retrieval. This exploration culminates in the unveiling of the quasi-triangular principle, a novel framework that elucidates the triangular-like interplay between query, positive document, and negative document. Fueled by this guiding principle, we introduce TriSampler, a straightforward yet highly effective negative sampling method. The keypoint of TriSampler lies in its ability to selectively sample more informative negatives within a prescribed constrained region. Experimental evaluation show that TriSampler consistently attains superior retrieval performance across a diverse of representative retrieval models",
    "checked": true,
    "id": "6aa7fb4db6dcfba612976efa9bceac4696b21699",
    "semantic_title": "trisampler: a better negative sampling principle for dense retrieval",
    "citation_count": 1,
    "authors": [
      "Zhen Yang",
      "Zhou Shao",
      "Yuxiao Dong",
      "Jie Tang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28780": {
    "title": "Parallel Ranking of Ads and Creatives in Real-Time Advertising Systems",
    "volume": "main",
    "abstract": "Creativity is the heart and soul of advertising services. Effective creatives can create a win-win scenario: advertisers each target users and achieve marketing objectives more effectively, users more quickly find products of interest, and platforms generate more advertising revenue. With the advent of AI-Generated Content, advertisers now can produce vast amounts of creative content at a minimal cost. The current challenge lies in how advertising systems can select the most pertinent creative in real-time for each user personally. Existing methods typically perform serial ranking of ads or creatives, limiting the creative module in terms of both effectiveness and efficiency. In this paper, we propose for the first time a novel architecture for online parallel estimation of ads and creatives ranking, as well as the corresponding offline joint optimization model. The online architecture enables sophisticated personalized creative modeling while reducing overall latency. The offline joint model for CTR estimation allows mutual awareness and collaborative optimization between ads and creatives. Additionally, we optimize the offline evaluation metrics for the implicit feedback sorting task involved in ad creative ranking. We conduct extensive experiments to compare ours with two state-of-the-art approaches. The results demonstrate the effectiveness of our approach in both offline evaluations and real-world advertising platforms online in terms of response time, CTR, and CPM",
    "checked": true,
    "id": "4f742b14521a47c59e1f4094115b7ae859abc690",
    "semantic_title": "parallel ranking of ads and creatives in real-time advertising systems",
    "citation_count": 1,
    "authors": [
      "Zhiguang Yang",
      "Liufang Sang",
      "Haoran Wang",
      "Wenlong Chen",
      "Lu Wang",
      "Jie He",
      "Changping Peng",
      "Zhangang Lin",
      "Chun Gan",
      "Jingping Shao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28781": {
    "title": "WaveNet: Tackling Non-stationary Graph Signals via Graph Spectral Wavelets",
    "volume": "main",
    "abstract": "In the existing spectral GNNs, polynomial-based methods occupy the mainstream in designing a filter through the Laplacian matrix. However, polynomial combinations factored by the Laplacian matrix naturally have limitations in message passing (e.g., over-smoothing). Furthermore, most existing spectral GNNs are based on polynomial bases, which struggle to capture the high-frequency parts of the graph spectral signal. Additionally, we also find that even increasing the polynomial order does not change this situation, which means polynomial-based models have a natural deficiency when facing high-frequency signals. To tackle these problems, we propose WaveNet, which aims to effectively capture the high-frequency part of the graph spectral signal from the perspective of wavelet bases through reconstructing the message propagation matrix. We utilize Multi-Resolution Analysis (MRA) to model this question, and our proposed method can reconstruct arbitrary filters theoretically. We also conduct node classification experiments on real-world graph benchmarks and achieve superior performance on most datasets. Our code is available at https://github.com/Bufordyang/WaveNet",
    "checked": true,
    "id": "46e2a0011ee8308d0338fdcdf69b7cc960a606c4",
    "semantic_title": "wavenet: tackling non-stationary graph signals via graph spectral wavelets",
    "citation_count": 0,
    "authors": [
      "Zhirui Yang",
      "Yulan Hu",
      "Sheng Ouyang",
      "Jingyu Liu",
      "Shuqiang Wang",
      "Xibo Ma",
      "Wenhan Wang",
      "Hanjing Su",
      "Yong Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28782": {
    "title": "RRL: Recommendation Reverse Learning",
    "volume": "main",
    "abstract": "As societies become increasingly aware of data privacy, regulations require that private information about users must be removed from both database and ML models, which is more colloquially called `the right to be forgotten`. Such privacy problems of recommendation systems, which hold large amounts of private data, are drawing increasing attention. Recent research suggests dividing the preference data into multiple shards and training submodels with these shards and forgetting users' personal preference data by retraining the submodels of marked shards. Despite the computational efficiency development compared with retraining from scratch, the overall recommendation performance deteriorates after dividing the shards because the collaborative information contained in the training data is broken. In this paper, we aim to propose a forgetting framework for recommendation models that neither separate the training data nor jeopardizes the recommendation performance, named Recommendation Reverse Learning (RRL). Given the trained recommendation model and marked preference data, we devise Reverse BPR Objective (RBPR Objective) to fine-tune the recommendation model to force it to forget the marked data. Nevertheless, as the recommendation model encode the complex collaborative information among users, we propose to utilize Fisher Information Matrix (FIM) to estimate the influence of reverse learning on other users' collaborative information and guide the updates of representations. We conduct experiments on two representative recommendation models and three public benchmark datasets to verify the efficiency of RRL. To verify the forgetting completeness, we use RRL to make the recommendation model poisoned by shilling attacks forget malicious users",
    "checked": true,
    "id": "b7e40983b8de20f58021f760c648d56518b10816",
    "semantic_title": "rrl: recommendation reverse learning",
    "citation_count": 1,
    "authors": [
      "Xiaoyu You",
      "Jianwei Xu",
      "Mi Zhang",
      "Zechen Gao",
      "Min Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28783": {
    "title": "UNEX-RL: Reinforcing Long-Term Rewards in Multi-Stage Recommender Systems with UNidirectional EXecution",
    "volume": "main",
    "abstract": "In recent years, there has been a growing interest in utilizing reinforcement learning (RL) to optimize long-term rewards in recommender systems. Since industrial recommender systems are typically designed as multi-stage systems, RL methods with a single agent face challenges when optimizing multiple stages simultaneously. The reason is that different stages have different observation spaces, and thus cannot be modeled by a single agent. To address this issue, we propose a novel UNidirectional-EXecution-based multi-agent Reinforcement Learning (UNEX-RL) framework to reinforce the long-term rewards in multi-stage recommender systems. We show that the unidirectional execution is a key feature of multi-stage recommender systems, bringing new challenges to the applications of multi-agent reinforcement learning (MARL), namely the observation dependency and the cascading effect. To tackle these challenges, we provide a cascading information chain (CIC) method to separate the independent observations from action-dependent observations and use CIC to train UNEX-RL effectively. We also discuss practical variance reduction techniques for UNEX-RL. Finally, we show the effectiveness of UNEX-RL on both public datasets and an online recommender system with over 100 million users. Specifically, UNEX-RL reveals a 0.558% increase in users' usage time compared with single-agent RL algorithms in online A/B experiments, highlighting the effectiveness of UNEX-RL in industrial recommender systems",
    "checked": true,
    "id": "f3ae31f6d3e6a72ccd3437e6b23255686fd4e193",
    "semantic_title": "unex-rl: reinforcing long-term rewards in multi-stage recommender systems with unidirectional execution",
    "citation_count": 1,
    "authors": [
      "Gengrui Zhang",
      "Yao Wang",
      "Xiaoshuang Chen",
      "Hongyi Qian",
      "Kaiqiao Zhan",
      "Ben Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28784": {
    "title": "M3D: Dataset Condensation by Minimizing Maximum Mean Discrepancy",
    "volume": "main",
    "abstract": "Training state-of-the-art (SOTA) deep models often requires extensive data, resulting in substantial training and storage costs. To address these challenges, dataset condensation has been developed to learn a small synthetic set that preserves essential information from the original large-scale dataset. Nowadays, optimization-oriented methods have been the primary method in the field of dataset condensation for achieving SOTA results. However, the bi-level optimization process hinders the practical application of such methods to realistic and larger datasets. To enhance condensation efficiency, previous works proposed Distribution-Matching (DM) as an alternative, which significantly reduces the condensation cost. Nonetheless, current DM-based methods still yield less comparable results to SOTA optimization-oriented methods. In this paper, we argue that existing DM-based methods overlook the higher-order alignment of the distributions, which may lead to sub-optimal matching results. Inspired by this, we present a novel DM-based method named M3D for dataset condensation by Minimizing the Maximum Mean Discrepancy between feature representations of the synthetic and real images. By embedding their distributions in a reproducing kernel Hilbert space, we align all orders of moments of the distributions of real and synthetic images, resulting in a more generalized condensed set. Notably, our method even surpasses the SOTA optimization-oriented method IDC on the high-resolution ImageNet dataset. Extensive analysis is conducted to verify the effectiveness of the proposed method. Source codes are available at https://github.com/Hansong-Zhang/M3D",
    "checked": true,
    "id": "afe07fcab9d6e3dad9dae614417d3f76c1bd010c",
    "semantic_title": "m3d: dataset condensation by minimizing maximum mean discrepancy",
    "citation_count": 6,
    "authors": [
      "Hansong Zhang",
      "Shikun Li",
      "Pengju Wang",
      "Dan Zeng",
      "Shiming Ge"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28785": {
    "title": "DiG-In-GNN: Discriminative Feature Guided GNN-Based Fraud Detector against Inconsistencies in Multi-Relation Fraud Graph",
    "volume": "main",
    "abstract": "Fraud detection on multi-relation graphs aims to identify fraudsters in graphs. Graph Neural Network (GNN) models leverage graph structures to pass messages from neighbors to the target nodes, thereby enriching the representations of those target nodes. However, feature and structural inconsistency in the graph, owing to fraudsters' camouflage behaviors, diminish the suspiciousness of fraud nodes which hinders the effectiveness of GNN-based models. In this work, we propose DiG-In-GNN, Discriminative Feature Guided GNN against Inconsistency, to dig into graphs for fraudsters. Specifically, we use multi-scale contrastive learning from the perspective of the neighborhood subgraph where the target node is located to generate guidance nodes to cope with the feature inconsistency. Then, guided by the guidance nodes, we conduct fine-grained neighbor selection through reinforcement learning for each neighbor node to precisely filter nodes that can enhance the message passing and therefore alleviate structural inconsistency. Finally, the two modules are integrated together to obtain discriminable representations of the nodes. Experiments on three fraud detection datasets demonstrate the superiority of the proposed method DiG-In-GNN, which obtains up to 20.73% improvement over previous state-of-the-art methods. Our code can be found at https://github.com/GraphBerry/DiG-In-GNN",
    "checked": true,
    "id": "e6c845b387016a69bf1d88ab65ec162566bd28ab",
    "semantic_title": "dig-in-gnn: discriminative feature guided gnn-based fraud detector against inconsistencies in multi-relation fraud graph",
    "citation_count": 0,
    "authors": [
      "Jinghui Zhang",
      "Zhengjia Xu",
      "Dingyang Lv",
      "Zhan Shi",
      "Dian Shen",
      "Jiahui Jin",
      "Fang Dong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28786": {
    "title": "Dual-View Whitening on Pre-trained Text Embeddings for Sequential Recommendation",
    "volume": "main",
    "abstract": "Recent advances in sequential recommendation models have demonstrated the efficacy of integrating pre-trained text embeddings with item ID embeddings to achieve superior performance. However, our study takes a unique perspective by exclusively focusing on the untapped potential of text embeddings, obviating the need for ID embeddings. We begin by implementing a pre-processing strategy known as whitening, which effectively transforms the anisotropic semantic space of pre-trained text embeddings into an isotropic Gaussian distribution. Comprehensive experiments reveal that applying whitening to pre-trained text embeddings in sequential recommendation models significantly enhances performance. Yet, a full whitening operation might break the potential manifold of items with similar text semantics. To retain the original semantics while benefiting from the isotropy of the whitened text features, we propose a Dual-view Whitening method for Sequential Recommendation (DWSRec), which leverages both fully whitened and relaxed whitened item representations as dual views for effective recommendations. We further examine the advantages of our approach through both empirical and theoretical analyses. Experiments on three public benchmark datasets show that DWSRec outperforms state-of-the-art methods for sequential recommendation",
    "checked": true,
    "id": "c72038405dcf499ce63c4ac6d1d2a60d86164ff8",
    "semantic_title": "dual-view whitening on pre-trained text embeddings for sequential recommendation",
    "citation_count": 1,
    "authors": [
      "Lingzi Zhang",
      "Xin Zhou",
      "Zhiwei Zeng",
      "Zhiqi Shen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28787": {
    "title": "CAMEL: Capturing Metaphorical Alignment with Context Disentangling for Multimodal Emotion Recognition",
    "volume": "main",
    "abstract": "Understanding the emotional polarity of multimodal content with metaphorical characteristics, such as memes, poses a significant challenge in Multimodal Emotion Recognition (MER). Previous MER researches have overlooked the phenomenon of metaphorical alignment in multimedia content, which involves non-literal associations between concepts to convey implicit emotional tones. Metaphor-agnostic MER methods may be misinformed by the isolated unimodal emotions, which are distinct from the real emotions blended in multimodal metaphors. Moreover, contextual semantics can further affect the emotions associated with similar metaphors, leading to the challenge of maintaining contextual compatibility. To address the issue of metaphorical alignment in MER, we propose to leverage a conditional generative approach for capturing metaphorical analogies. Our approach formulates schematic prompts and corresponding references based on theoretical foundations, which allows the model to better grasp metaphorical nuances. In order to maintain contextual sensitivity, we incorporate a disentangled contrastive matching mechanism, which undergoes curricular adjustment to regulate its intensity during the learning process. The automatic and human evaluation experiments on two benchmarks prove that, our model provides considerable and stable improvements in recognizing multimodal emotion with metaphor attributes",
    "checked": true,
    "id": "c48905b12e61e16a0b92bec9ce659e41e66dc1cc",
    "semantic_title": "camel: capturing metaphorical alignment with context disentangling for multimodal emotion recognition",
    "citation_count": 0,
    "authors": [
      "Linhao Zhang",
      "Li Jin",
      "Guangluan Xu",
      "Xiaoyu Li",
      "Cai Xu",
      "Kaiwen Wei",
      "Nayu Liu",
      "Haonan Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28788": {
    "title": "ROG_PL: Robust Open-Set Graph Learning via Region-Based Prototype Learning",
    "volume": "main",
    "abstract": "Open-set graph learning is a practical task that aims to classify the known class nodes and to identify unknown class samples as unknowns. Conventional node classification methods usually perform unsatisfactorily in open-set scenarios due to the complex data they encounter, such as out-of-distribution (OOD) data and in-distribution (IND) noise. OOD data are samples that do not belong to any known classes. They are outliers if they occur in training (OOD noise), and open-set samples if they occur in testing. IND noise are training samples which are assigned incorrect labels. The existence of IND noise and OOD noise is prevalent, which usually cause the ambiguity problem, including the intra-class variety problem and the inter-class confusion problem. Thus, to explore robust open-set learning methods is necessary and difficult, and it becomes even more difficult for non-IID graph data. To this end, we propose a unified framework named ROG_PL to achieve robust open-set learning on complex noisy graph data, by introducing prototype learning. In specific, ROG_PL consists of two modules, i.e., denoising via label propagation and open-set prototype learning via regions. The first module corrects noisy labels through similarity-based label propagation and removes low-confidence samples, to solve the intra-class variety problem caused by noise. The second module learns open-set prototypes for each known class via non-overlapped regions and remains both interior and border prototypes to remedy the inter-class confusion problem. The two modules are iteratively updated under the constraints of classification loss and prototype diversity loss. To the best of our knowledge, the proposed ROG_PL is the first robust open-set node classification method for graph data with complex noise. Experimental evaluations of ROG_PL on several benchmark graph datasets demonstrate that it has good performance",
    "checked": false,
    "id": "6cc4639b6892679021396a8bfde693964ee64a76",
    "semantic_title": "rogpl: robust open-set graph learning via region-based prototype learning",
    "citation_count": 1,
    "authors": [
      "Qin Zhang",
      "Xiaowei Li",
      "Jiexin Lu",
      "Liping Qiu",
      "Shirui Pan",
      "Xiaojun Chen",
      "Junyang Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28789": {
    "title": "Temporal Graph Contrastive Learning for Sequential Recommendation",
    "volume": "main",
    "abstract": "Sequential recommendation is a crucial task in understanding users' evolving interests and predicting their future behaviors. While existing approaches on sequence or graph modeling to learn interaction sequences of users have shown promising performance, how to effectively exploit temporal information and deal with the uncertainty noise in evolving user behaviors is still quite challenging. To this end, in this paper, we propose a Temporal Graph Contrastive Learning method for Sequential Recommendation (TGCL4SR) which leverages not only local interaction sequences but also global temporal graphs to comprehend item correlations and analyze user behaviors from a temporal perspective. Specifically, we first devise a Temporal Item Transition Graph (TITG) to fully leverage global interactions to understand item correlations, and augment this graph by dual transformations based on neighbor sampling and time disturbance. Accordingly, we design a Temporal item Transition graph Convolutional network (TiTConv) to capture temporal item transition patterns in TITG. Then, a novel Temporal Graph Contrastive Learning (TGCL) mechanism is designed to enhance the uniformity of representations between augmented graphs from identical sequences. For local interaction sequences, we design a temporal sequence encoder to incorporate time interval embeddings into the architecture of Transformer. At the training stage, we take maximum mean discrepancy and TGCL losses as auxiliary objectives. Extensive experiments on several real-world datasets show the effectiveness of TGCL4SR against state-of-the-art baselines of sequential recommendation",
    "checked": true,
    "id": "d7c6cabbd51b9dabee34aa442cb3e2ba6b76958f",
    "semantic_title": "temporal graph contrastive learning for sequential recommendation",
    "citation_count": 2,
    "authors": [
      "Shengzhe Zhang",
      "Liyi Chen",
      "Chao Wang",
      "Shuangli Li",
      "Hui Xiong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28790": {
    "title": "Influential Exemplar Replay for Incremental Learning in Recommender Systems",
    "volume": "main",
    "abstract": "Personalized recommender systems have found widespread applications for effective information filtering. Conventional models engage in knowledge mining within the static setting to reconstruct singular historical data. Nonetheless, the dynamics of real-world environments are in a constant state of flux, rendering acquired model knowledge inadequate for accommodating emergent trends and thus leading to notable recommendation performance decline. Given the typically prohibitive cost of exhaustive model retraining, it has emerged to study incremental learning for recommender systems with ever-growing data. In this paper, we propose an effective model-agnostic framework, namely INFluential Exemplar Replay (INFER). INFER facilitates recommender models in retaining the earlier assimilated knowledge, e.g., users' enduring preferences, while concurrently accommodating evolving trends manifested in users' new interaction behaviors. We commence with a vanilla implementation that centers on identifying the most representative data samples for effective consolidation of early knowledge. Subsequently, we propose an advanced solution, namely INFERONCE, to optimize the computational overhead associated with the vanilla implementation. Extensive experiments on four prototypical backbone models, two classic recommendation tasks, and four widely used benchmarks consistently demonstrate the effectiveness of our method as well as its compatibility for extending to several incremental recommender models",
    "checked": true,
    "id": "ed9396c702528e45f6fbe0f1d098c094a4b7885a",
    "semantic_title": "influential exemplar replay for incremental learning in recommender systems",
    "citation_count": 0,
    "authors": [
      "Xinni Zhang",
      "Yankai Chen",
      "Chenhao Ma",
      "Yixiang Fang",
      "Irwin King"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28791": {
    "title": "Another Way to the Top: Exploit Contextual Clustering in Learned Image Coding",
    "volume": "main",
    "abstract": "While convolution and self-attention are extensively used in learned image compression (LIC) for transform coding, this paper proposes an alternative called Contextual Clustering based LIC (CLIC) which primarily relies on clustering operations and local attention for correlation characterization and compact representation of an image. As seen, CLIC expands the receptive field into the entire image for intra-cluster feature aggregation. Afterward, features are reordered to their original spatial positions to pass through the local attention units for inter-cluster embedding. Additionally, we introduce the Guided Post-Quantization Filtering (GuidedPQF) into CLIC, effectively mitigating the propagation and accumulation of quantization errors at the initial decoding stage. Extensive experiments demonstrate the superior performance of CLIC over state-of-the-art works: when optimized using MSE, it outperforms VVC by about 10% BD-Rate in three widely-used benchmark datasets; when optimized using MS-SSIM, it saves more than 50% BD-Rate over VVC. Our CLIC offers a new way to generate compact representations for image compression, which also provides a novel direction along the line of LIC development",
    "checked": true,
    "id": "1549acfea2a07cb433bbd6ee216deddf31241e67",
    "semantic_title": "another way to the top: exploit contextual clustering in learned image coding",
    "citation_count": 2,
    "authors": [
      "Yichi Zhang",
      "Zhihao Duan",
      "Ming Lu",
      "Dandan Ding",
      "Fengqing Zhu",
      "Zhan Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28792": {
    "title": "Multi-Domain Deep Learning from a Multi-View Perspective for Cross-Border E-commerce Search",
    "volume": "main",
    "abstract": "Building click-through rate (CTR) and conversion rate (CVR) prediction models for cross-border e-commerce search requires modeling the correlations among multi-domains. Existing multi-domain methods would suffer severely from poor scalability and low efficiency when number of domains increases. To this end, we propose a Domain-Aware Multi-view mOdel (DAMO), which is domain-number-invariant, to effectively leverage cross-domain relations from a multi-view perspective. Specifically, instead of working in the original feature space defined by different domains, DAMO maps everything to a new low-rank multi-view space. To achieve this, DAMO firstly extracts multi-domain features in an explicit feature-interactive manner. These features are parsed to a multi-view extractor to obtain view-invariant and view-specific features. Then a multi-view predictor inputs these two sets of features and outputs view-based predictions. To enforce view-awareness in the predictor, we further propose a lightweight view-attention estimator to dynamically learn the optimal view-specific weights w.r.t. a view-guided loss. Extensive experiments on public and industrial datasets show that compared with state-of-the-art models, our DAMO achieves better performance with lower storage and computational costs. In addition, deploying DAMO to a large-scale cross-border e-commence platform leads to 1.21%, 1.76%, and 1.66% improvements over the existing CGC-based model in the online AB-testing experiment in terms of CTR, CVR, and Gross Merchandises Value, respectively",
    "checked": true,
    "id": "f40307db4d361d9ade5bc251fb3def1916b850e3",
    "semantic_title": "multi-domain deep learning from a multi-view perspective for cross-border e-commerce search",
    "citation_count": 0,
    "authors": [
      "Yiqian Zhang",
      "Yinfu Feng",
      "Wen-Ji Zhou",
      "Yunan Ye",
      "Min Tan",
      "Rong Xiao",
      "Haihong Tang",
      "Jiajun Ding",
      "Jun Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28793": {
    "title": "Spatial-Temporal Interplay in Human Mobility: A Hierarchical Reinforcement Learning Approach with Hypergraph Representation",
    "volume": "main",
    "abstract": "In the realm of human mobility, the decision-making process for selecting the next-visit location is intricately influenced by a trade-off between spatial and temporal constraints, which are reflective of individual needs and preferences. This trade-off, however, varies across individuals, making the modeling of these spatial-temporal dynamics a formidable challenge. To address the problem, in this work, we introduce the \"Spatial-temporal Induced Hierarchical Reinforcement Learning\" (STI-HRL) framework, for capturing the interplay between spatial and temporal factors in human mobility decision-making. Specifically, STI-HRL employs a two-tiered decision-making process: the low-level focuses on disentangling spatial and temporal preferences using dedicated agents, while the high-level integrates these considerations to finalize the decision. To complement the hierarchical decision setting, we construct a hypergraph to organize historical data, encapsulating the multi-aspect semantics of human mobility. We propose a cross-channel hypergraph embedding module to learn the representations as the states to facilitate the decision-making cycle. Our extensive experiments on two real-world datasets validate the superiority of STI-HRL over state-of-the-art methods in predicting users' next visits across various performance metrics",
    "checked": true,
    "id": "12dc83d753c7e766c728ce2a08cdaf065152f54d",
    "semantic_title": "spatial-temporal interplay in human mobility: a hierarchical reinforcement learning approach with hypergraph representation",
    "citation_count": 2,
    "authors": [
      "Zhaofan Zhang",
      "Yanan Xiao",
      "Lu Jiang",
      "Dingqi Yang",
      "Minghao Yin",
      "Pengyang Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28794": {
    "title": "FacetCRS: Multi-Faceted Preference Learning for Pricking Filter Bubbles in Conversational Recommender System",
    "volume": "main",
    "abstract": "The filter bubble is a notorious issue in Recommender Systems (RSs), which describes the phenomenon whereby users are exposed to a limited and narrow range of information or content that reinforces their existing dominant preferences and beliefs. This results in a lack of exposure to diverse and varied content. Many existing works have predominantly examined filter bubbles in static or relatively-static recommendation settings. However, filter bubbles will be continuously intensified over time due to the feedback loop between the user and the system in the real-world online recommendation. To address these issues, we propose a novel paradigm, Multi-Facet Preference Learning for Pricking Filter Bubbles in Conversational Recommender System (FacetCRS), which aims to burst filter bubbles in the conversational recommender system (CRS) through timely user-item interactions via natural language conversations. By considering diverse user preferences and intentions, FacetCRS automatically model user preference into multi-facets, including entity-, word-, context-, and review-facet, to capture diverse and dynamic user preferences to prick filter bubbles in the CRS. It is an end-to-end CRS framework to adaptively learn representations of various levels of preference facet and diverse types of external knowledge. Extensive experiments on two publicly available benchmark datasets demonstrate that our proposed method achieves state-of-the-art performance in mitigating filter bubbles and enhancing recommendation quality in CRS",
    "checked": true,
    "id": "b8618735cab030d2b62a2d332814e0bc96a9ca8a",
    "semantic_title": "facetcrs: multi-faceted preference learning for pricking filter bubbles in conversational recommender system",
    "citation_count": 0,
    "authors": [
      "Yongsen Zheng",
      "Ziliang Chen",
      "Jinghui Qin",
      "Liang Lin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28795": {
    "title": "GMP-AR: Granularity Message Passing and Adaptive Reconciliation for Temporal Hierarchy Forecasting",
    "volume": "main",
    "abstract": "Time series forecasts of different temporal granularity are widely used in real-world applications, e.g., sales prediction in days and weeks for making different inventory plans. However, these tasks are usually solved separately without ensuring coherence, which is crucial for aligning downstream decisions. Previous works mainly focus on ensuring coherence with some straightforward methods, e.g., aggregation from the forecasts of fine granularity to the coarse ones, and allocation from the coarse granularity to the fine ones. These methods merely take the temporal hierarchical structure to maintain coherence without improving the forecasting accuracy. In this paper, we propose a novel granularity message-passing mechanism (GMP) that leverages temporal hierarchy information to improve forecasting performance and also utilizes an adaptive reconciliation (AR) strategy to maintain coherence without performance loss. Furthermore, we introduce an optimization module to achieve task-based targets while adhering to more real-world constraints. Experiments on real-world datasets demonstrate that our framework (GMP-AR) achieves superior performances on temporal hierarchical forecasting tasks compared to state-of-the-art methods. In addition, our framework has been successfully applied to a real-world task of payment traffic management in Alipay by integrating with the task-based optimization module",
    "checked": true,
    "id": "eda45cd6529a34d19acc4f469fc5c2a02a59939e",
    "semantic_title": "gmp-ar: granularity message passing and adaptive reconciliation for temporal hierarchy forecasting",
    "citation_count": 0,
    "authors": [
      "Fan Zhou",
      "Chen Pan",
      "Lintao Ma",
      "Yu Liu",
      "Siqiao Xue",
      "James Zhang",
      "Jun Zhou",
      "Hongyuan Mei",
      "Weitao Lin",
      "Zi Zhuang",
      "Wenxin Ning",
      "Yunhua Hu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28796": {
    "title": "Explainable Origin-Destination Crowd Flow Interpolation via Variational Multi-Modal Recurrent Graph Auto-Encoder",
    "volume": "main",
    "abstract": "Origin-destination (OD) crowd flow, if more accurately inferred at a fine-grained level, has the potential to enhance the efficacy of various urban applications. While in practice for mining OD crowd flow with effect, the problem of spatially interpolating OD crowd flow occurs since the ineluctable missing values. This problem is further complicated by the inherent scarcity and noise nature of OD crowd flow data. In this paper, we propose an uncertainty-aware interpolative and explainable framework, namely UApex, for realizing reliable and trustworthy OD crowd flow interpolation. Specifically, we first design a Variational Multi-modal Recurrent Graph Auto-Encoder (VMR-GAE) for uncertainty-aware OD crowd flow interpolation. A key idea here is to formulate the problem as semi-supervised learning on directed graphs. Next, to mitigate the data scarcity, we incorporate a distribution alignment mechanism that can introduce supplementary modals into variational inference. Then, a dedicated decoder with a Poisson prior is proposed for OD crowd flow interpolation. Moreover, to make VMR-GAE more trustworthy, we develop an efficient and uncertainty-aware explainer that can provide explanations from the spatiotemporal topology perspective via the Shapley value. Extensive experiments on two real-world datasets validate that VMR-GAE outperforms the state-of-the-art baselines. Also, an exploratory empirical study shows that the proposed explainer can generate meaningful spatiotemporal explanations",
    "checked": true,
    "id": "16e9a10ac7a83d80cb6538d9fbefece51b0a6657",
    "semantic_title": "explainable origin-destination crowd flow interpolation via variational multi-modal recurrent graph auto-encoder",
    "citation_count": 0,
    "authors": [
      "Qiang Zhou",
      "Xinjiang Lu",
      "Jingjing Gu",
      "Zhe Zheng",
      "Bo Jin",
      "Jingbo Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28797": {
    "title": "An Efficient Subgraph-Inferring Framework for Large-Scale Heterogeneous Graphs",
    "volume": "main",
    "abstract": "Heterogeneous Graph Neural Networks (HGNNs) play a vital role in advancing the field of graph representation learning by addressing the complexities arising from diverse data types and interconnected relationships in real-world scenarios. However, traditional HGNNs face challenges when applied to large-scale graphs due to the necessity of training or inferring on the entire graph. As the size of the heterogeneous graphs increases, the time and memory overhead required by these models escalates rapidly, even reaching unacceptable levels. To address this issue, in this paper, we present a novel framework named (SubInfer), which conducts training and inferring on subgraphs instead of the entire graphs, hence efficiently handling large-scale heterogeneous graphs. The proposed framework comprises three main steps: 1) partitioning the heterogeneous graph from multiple perspectives to preserve various semantic information, 2) completing the subgraphs to improve the convergence speed of subgraph training and the performance of subgraph inference, and 3) training and inferring the HGNN model on distributed clusters to further reduce the time overhead. The framework is applicable to the vast majority of HGNN models. Experiments on five benchmark datasets demonstrate that SubInfer effectively optimizes the training and inference phase, delivering comparable performance to traditional HGNN models while significantly reducing time and memory overhead",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Zhou",
      "Hong Huang",
      "Ruize Shi",
      "Kehan Yin",
      "Hai Jin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28894": {
    "title": "Optimal Makespan in a Minute Timespan! A Scalable Multi-Robot Goal Assignment Algorithm for Minimizing Mission Time",
    "volume": "main",
    "abstract": "We study a variant of the multi-robot goal assignment problem where a unique goal to each robot needs to be assigned while minimizing the largest cost of movement among the robots, called makespan. A significant step in solving this problem is to find the cost associated with the robot-goal pairs, which requires solving a complex path planning problem. We present OM, a scalable optimal algorithm that solves the multi-robot goal assignment problem by computing the paths for a significantly less number of robot-goal pairs compared to the state-of-the-art algorithms, leading to a computationally superior mechanism to solve the problem. We extensively evaluate our algorithm for hundreds of robots on randomly generated and standard workspaces. Our experimental results demonstrate that the proposed algorithm achieves a noticeable speedup over two state-of-the-art baseline algorithms",
    "checked": true,
    "id": "c378f751f88bf11c1896e439f828ef6d6ed08dd4",
    "semantic_title": "optimal makespan in a minute timespan! a scalable multi-robot goal assignment algorithm for minimizing mission time",
    "citation_count": 0,
    "authors": [
      "Aakash  ",
      "Indranil Saha"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28895": {
    "title": "On Computing Makespan-Optimal Solutions for Generalized Sliding-Tile Puzzles",
    "volume": "main",
    "abstract": "In the 15-puzzle game, 15 labeled square tiles are reconfigured on a 4 × 4 board through an escort, wherein each (time) step, a single tile neighboring it may slide into it, leaving the space previously occupied by the tile as the new escort. We study a generalized sliding-tile puzzle (GSTP) in which (1) there are 1+ escorts and (2) multiple tiles can move synchronously in a single time step. Compared with popular discrete multi-agent/robot motion models, GSTP provides a more accurate model for a broad array of high-utility applications, including warehouse automation and autonomous garage parking, but is less studied due to the more involved tile interactions. In this work, we analyze optimal GSTP solution structures, establishing that computing makespan optimal solutions for GSTP is NP-complete and developing polynomial time algorithms yielding makespans approximating the minimum with expected/high probability constant factors, assuming randomized start and goal configurations",
    "checked": true,
    "id": "482fadaa976a36cc79d369808a98faf779a3f4fd",
    "semantic_title": "on computing makespan-optimal solutions for generalized sliding-tile puzzles",
    "citation_count": 1,
    "authors": [
      "Marcus Gozon",
      "Jingjin Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28896": {
    "title": "Interactive Visual Task Learning for Robots",
    "volume": "main",
    "abstract": "We present a framework for robots to learn novel visual concepts and tasks via in-situ linguistic interactions with human users. Previous approaches have either used large pre-trained visual models to infer novel objects zero-shot, or added novel concepts along with their attributes and representations to a concept hierarchy. We extend the approaches that focus on learning visual concept hierarchies by enabling them to learn novel concepts and solve unseen robotics tasks with them. To enable a visual concept learner to solve robotics tasks one-shot, we developed two distinct techniques. Firstly, we propose a novel approach, Hi-Viscont(HIerarchical VISual CONcept learner for Task), which augments information of a novel concept to its parent nodes within a concept hierarchy. This information propagation allows all concepts in a hierarchy to update as novel concepts are taught in a continual learning setting. Secondly, we represent a visual task as a scene graph with language annotations, allowing us to create novel permutations of a demonstrated task zero-shot in-situ. We present two sets of results. Firstly, we compare Hi-Viscont with the baseline model (FALCON) on visual question answering(VQA) in three domains. While being comparable to the baseline model on leaf level concepts, Hi-Viscont achieves an improvement of over 9% on non-leaf concepts on average. Secondly, we conduct a human-subjects experiment where users teach our robot visual tasks in-situ. We compare our model's performance against the baseline FALCON model. Our framework achieves 33% improvements in success rate metric, and 19% improvements in the object level accuracy compared to the baseline model. With both of these results we demonstrate the ability of our model to learn tasks and concepts in a continual learning setting on the robot",
    "checked": true,
    "id": "b8452a3f0317a5e0fb1027d19d87846c71b9d8b7",
    "semantic_title": "interactive visual task learning for robots",
    "citation_count": 0,
    "authors": [
      "Weiwei Gu",
      "Anant Sah",
      "Nakul Gopalan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28897": {
    "title": "DexFuncGrasp: A Robotic Dexterous Functional Grasp Dataset Constructed from a Cost-Effective Real-Simulation Annotation System",
    "volume": "main",
    "abstract": "Robot grasp dataset is the basis of designing the robot's grasp generation model. Compared with the building grasp dataset for Low-DOF grippers, it is harder for High-DOF dexterous robot hand. Most current datasets meet the needs of generating stable grasps, but they are not suitable for dexterous hands to complete human-like functional grasp, such as grasp the handle of a cup or pressing the button of a flashlight, so as to enable robots to complete subsequent functional manipulation action autonomously, and there is no dataset with functional grasp pose annotations at present. This paper develops a unique Cost-Effective Real-Simulation Annotation System by leveraging natural hand's actions. The system is able to capture a functional grasp of a dexterous hand in a simulated environment assisted by human demonstration in real world. By using this system, dexterous grasp data can be collected efficiently as well as cost-effective. Finally, we construct the first dexterous functional grasp dataset with rich pose annotations. A Functional Grasp Synthesis Model is also provided to validate the effectiveness of the proposed system and dataset. Our project page is: https://hjlllll.github.io/DFG/",
    "checked": true,
    "id": "27957676bc0623bda8343660ad6a4d00404739a5",
    "semantic_title": "dexfuncgrasp: a robotic dexterous functional grasp dataset constructed from a cost-effective real-simulation annotation system",
    "citation_count": 1,
    "authors": [
      "Jinglue Hang",
      "Xiangbo Lin",
      "Tianqiang Zhu",
      "Xuanheng Li",
      "Rina Wu",
      "Xiaohong Ma",
      "Yi Sun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28898": {
    "title": "LINGO-Space: Language-Conditioned Incremental Grounding for Space",
    "volume": "main",
    "abstract": "We aim to solve the problem of spatially localizing composite instructions referring to space: space grounding. Compared to current instance grounding, space grounding is challenging due to the ill-posedness of identifying locations referred to by discrete expressions and the compositional ambiguity of referring expressions. Therefore, we propose a novel probabilistic space-grounding methodology (LINGO-Space) that accurately identifies a probabilistic distribution of space being referred to and incrementally updates it, given subsequent referring expressions leveraging configurable polar distributions. Our evaluations show that the estimation using polar distributions enables a robot to ground locations successfully through 20 table-top manipulation benchmark tests. We also show that updating the distribution helps the grounding method accurately narrow the referring space. We finally demonstrate the robustness of the space grounding with simulated manipulation and real quadruped robot navigation tasks. Code and videos are available at https://lingo-space.github.io",
    "checked": true,
    "id": "6b0ed1851827966074f425c43e80c9078dd1ea70",
    "semantic_title": "lingo-space: language-conditioned incremental grounding for space",
    "citation_count": 2,
    "authors": [
      "Dohyun Kim",
      "Nayoung Oh",
      "Deokmin Hwang",
      "Daehyung Park"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28899": {
    "title": "CTO-SLAM: Contour Tracking for Object-Level Robust 4D SLAM",
    "volume": "main",
    "abstract": "The demand for 4D ( 3D+time ) SLAM system is increasingly urgent, especially for decision-making and scene understanding. However, most of the existing simultaneous localization and mapping ( SLAM ) systems primarily assume static environments. They fail to represent dynamic scenarios due to the challenge of establishing robust long-term spatiotemporal associations in dynamic object tracking. We address this limitation and propose CTO-SLAM, a monocular and RGB-D object-level 4D SLAM system to track moving objects and estimate their motion simultaneously. In this paper, we propose contour tracking, which introduces contour features to enhance the keypoint representation of dynamic objects and coupled with pixel tracking to achieve long-term robust object tracking. Based on contour tracking, we propose a novel sampling-based object pose initialization algorithm and the following adapted bundle adjustment ( BA ) optimization algorithm to estimate dynamic object poses with high accuracy. The CTO-SLAM system is verified on both KITTI and VKITTI datasets. The experimental results demonstrate that our system effectively addresses cumulative errors in long-term spatiotemporal association and hence obtains substantial improvements over the state-of-the-art systems. The source code is available at https://github.com/realXiaohan/CTO-SLAM",
    "checked": true,
    "id": "baf0c218bc654868cd6fba2708b76ee608d4c294",
    "semantic_title": "cto-slam: contour tracking for object-level robust 4d slam",
    "citation_count": 0,
    "authors": [
      "Xiaohan Li",
      "Dong Liu",
      "Jun Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28900": {
    "title": "BAT: Behavior-Aware Human-Like Trajectory Prediction for Autonomous Driving",
    "volume": "main",
    "abstract": "The ability to accurately predict the trajectory of surrounding vehicles is a critical hurdle to overcome on the journey to fully autonomous vehicles. To address this challenge, we pioneer a novel behavior-aware trajectory prediction model (BAT) that incorporates insights and findings from traffic psychology, human behavior, and decision-making. Our model consists of behavior-aware, interaction-aware, priority-aware, and position-aware modules that perceive and understand the underlying interactions and account for uncertainty and variability in prediction, enabling higher-level learning and flexibility without rigid categorization of driving behavior. Importantly, this approach eliminates the need for manual labeling in the training process and addresses the challenges of non-continuous behavior labeling and the selection of appropriate time windows. We evaluate BAT's performance across the Next Generation Simulation (NGSIM), Highway Drone (HighD), Roundabout Drone (RounD), and Macao Connected Autonomous Driving (MoCAD) datasets, showcasing its superiority over prevailing state-of-the-art (SOTA) benchmarks in terms of prediction accuracy and efficiency. Remarkably, even when trained on reduced portions of the training data (25%), our model outperforms most of the baselines, demonstrating its robustness and efficiency in predicting vehicle trajectories, and the potential to reduce the amount of data required to train autonomous vehicles, especially in corner cases. In conclusion, the behavior-aware model represents a significant advancement in the development of autonomous vehicles capable of predicting trajectories with the same level of proficiency as human drivers. The project page is available on our GitHub",
    "checked": true,
    "id": "1b91176947d4564023a8a07a8975327089ba9148",
    "semantic_title": "bat: behavior-aware human-like trajectory prediction for autonomous driving",
    "citation_count": 11,
    "authors": [
      "Haicheng Liao",
      "Zhenning Li",
      "Huanming Shen",
      "Wenxuan Zeng",
      "Dongping Liao",
      "Guofa Li",
      "Chengzhong Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28901": {
    "title": "Deep Homography Estimation for Visual Place Recognition",
    "volume": "main",
    "abstract": "Visual place recognition (VPR) is a fundamental task for many applications such as robot localization and augmented reality. Recently, the hierarchical VPR methods have received considerable attention due to the trade-off between accuracy and efficiency. They usually first use global features to retrieve the candidate images, then verify the spatial consistency of matched local features for re-ranking. However, the latter typically relies on the RANSAC algorithm for fitting homography, which is time-consuming and non-differentiable. This makes existing methods compromise to train the network only in global feature extraction. Here, we propose a transformer-based deep homography estimation (DHE) network that takes the dense feature map extracted by a backbone network as input and fits homography for fast and learnable geometric verification. Moreover, we design a re-projection error of inliers loss to train the DHE network without additional homography labels, which can also be jointly trained with the backbone network to help it extract the features that are more suitable for local matching. Extensive experiments on benchmark datasets show that our method can outperform several state-of-the-art methods. And it is more than one order of magnitude faster than the mainstream hierarchical VPR methods using RANSAC. The code is released at https://github.com/Lu-Feng/DHE-VPR",
    "checked": true,
    "id": "5b3bdb627ea0f7beb0da0d614686ecce46f30f73",
    "semantic_title": "deep homography estimation for visual place recognition",
    "citation_count": 1,
    "authors": [
      "Feng Lu",
      "Shuting Dong",
      "Lijun Zhang",
      "Bingxi Liu",
      "Xiangyuan Lan",
      "Dongmei Jiang",
      "Chun Yuan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28902": {
    "title": "Task Planning for Object Rearrangement in Multi-Room Environments",
    "volume": "main",
    "abstract": "Object rearrangement in a multi-room setup should produce a reasonable plan that reduces the agent's overall travel and the number of steps. Recent state-of-the-art methods fail to produce such plans because they rely on explicit exploration for discovering unseen objects due to partial observability and a heuristic planner to sequence the actions for rearrangement. This paper proposes a novel task planner to efficiently plan a sequence of actions to discover unseen objects and rearrange misplaced objects within an untidy house to achieve a desired tidy state. The proposed method introduces several innovative techniques, including (i) a method for discovering unseen objects using commonsense knowledge from large language models, (ii) a collision resolution and buffer prediction method based on Cross-Entropy Method to handle blocked goal and swap cases, (iii) a directed spatial graph-based state space for scalability, and (iv) deep reinforcement learning (RL) for producing an efficient plan to simultaneously discover unseen objects and rearrange the visible misplaced ones to minimize the overall traversal. The paper also presents new metrics and a benchmark dataset called MoPOR to evaluate the effectiveness of the rearrangement planning in a multi-room setting. The experimental results demonstrate that the proposed method effectively addresses the multi-room rearrangement problem",
    "checked": true,
    "id": "0ef5cb867a83017d38ac454901190396e880d3b5",
    "semantic_title": "task planning for object rearrangement in multi-room environments",
    "citation_count": 0,
    "authors": [
      "Karan Mirakhor",
      "Sourav Ghosh",
      "Dipanjan Das",
      "Brojeshwar Bhowmick"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28903": {
    "title": "Hierarchical Planning and Learning for Robots in Stochastic Settings Using Zero-Shot Option Invention",
    "volume": "main",
    "abstract": "This paper addresses the problem of inventing and using hierarchical representations for stochastic robot-planning problems. Rather than using hand-coded state or action representations as input, it presents new methods for learning how to create a high-level action representation for long-horizon, sparse reward robot planning problems in stochastic settings with unknown dynamics. After training, this system yields a robot-specific but environment independent planning system. Given new problem instances in unseen stochastic environments, it first creates zero-shot options (without any experience on the new environment) with dense pseudo-rewards and then uses them to solve the input problem in a hierarchical planning and refinement process. Theoretical results identify sufficient conditions for completeness of the presented approach. Extensive empirical analysis shows that even in settings that go beyond these sufficient conditions, this approach convincingly outperforms baselines by 2x in terms of solution time with orders of magnitude improvement in solution quality",
    "checked": true,
    "id": "909f291e698dfe0ee409ee74619e1e8bf17158c2",
    "semantic_title": "hierarchical planning and learning for robots in stochastic settings using zero-shot option invention",
    "citation_count": 1,
    "authors": [
      "Naman Shah",
      "Siddharth Srivastava"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28904": {
    "title": "MorphVAE: Advancing Morphological Design of Voxel-Based Soft Robots with Variational Autoencoders",
    "volume": "main",
    "abstract": "Soft robot design is an intricate field with unique challenges due to its complex and vast search space. In the past literature, evolutionary computation algorithms, including novel probabilistic generative models (PGMs), have shown potential in this realm. However, these methods are sample inefficient and predominantly focus on rigid robots in locomotion tasks, which limit their performance and application in robot design automation. In this work, we propose MorphVAE, an innovative PGM that incorporates a multi-task training scheme and a meticulously crafted sampling technique termed ``continuous natural selection'', aimed at bolstering sample efficiency. This method empowers us to gain insights from assessed samples across diverse tasks and temporal evolutionary stages, while simultaneously maintaining a delicate balance between optimization efficiency and biodiversity. Through extensive experiments in various locomotion and manipulation tasks, we substantiate the efficiency of MorphVAE in generating high-performing and diverse designs, surpassing the performance of competitive baselines",
    "checked": true,
    "id": "d266056a29dc5e8952260abf0586371e451158f6",
    "semantic_title": "morphvae: advancing morphological design of voxel-based soft robots with variational autoencoders",
    "citation_count": 0,
    "authors": [
      "Junru Song",
      "Yang Yang",
      "Wei Peng",
      "Weien Zhou",
      "Feifei Wang",
      "Wen Yao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28905": {
    "title": "DistilVPR: Cross-Modal Knowledge Distillation for Visual Place Recognition",
    "volume": "main",
    "abstract": "The utilization of multi-modal sensor data in visual place recognition (VPR) has demonstrated enhanced performance compared to single-modal counterparts. Nonetheless, integrating additional sensors comes with elevated costs and may not be feasible for systems that demand lightweight operation, thereby impacting the practical deployment of VPR. To address this issue, we resort to knowledge distillation, which empowers single-modal students to learn from cross-modal teachers without introducing additional sensors during inference. Despite the notable advancements achieved by current distillation approaches, the exploration of feature relationships remains an under-explored area. In order to tackle the challenge of cross-modal distillation in VPR, we present DistilVPR, a novel distillation pipeline for VPR. We propose leveraging feature relationships from multiple agents, including self-agents and cross-agents for teacher and student neural networks. Furthermore, we integrate various manifolds, characterized by different space curvatures for exploring feature relationships. This approach enhances the diversity of feature relationships, including Euclidean, spherical, and hyperbolic relationship modules, thereby enhancing the overall representational capacity. The experiments demonstrate that our proposed pipeline achieves state-of-the-art performance compared to other distillation baselines. We also conduct necessary ablation studies to show design effectiveness. The code is released at: https://github.com/sijieaaa/DistilVPR",
    "checked": true,
    "id": "0af83577159dfee22c59d66cf5734ff53f6700c7",
    "semantic_title": "distilvpr: cross-modal knowledge distillation for visual place recognition",
    "citation_count": 0,
    "authors": [
      "Sijie Wang",
      "Rui She",
      "Qiyu Kang",
      "Xingchao Jian",
      "Kai Zhao",
      "Yang Song",
      "Wee Peng Tay"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28906": {
    "title": "Angle Robustness Unmanned Aerial Vehicle Navigation in GNSS-Denied Scenarios",
    "volume": "main",
    "abstract": "Due to the inability to receive signals from the Global Navigation Satellite System (GNSS) in extreme conditions, achieving accurate and robust navigation for Unmanned Aerial Vehicles (UAVs) is a challenging task. Recently emerged, vision-based navigation has been a promising and feasible alternative to GNSS-based navigation. However, existing vision-based techniques are inadequate in addressing flight deviation caused by environmental disturbances and inaccurate position predictions in practical settings. In this paper, we present a novel angle robustness navigation paradigm to deal with flight deviation in point-to-point navigation tasks. Additionally, we propose a model that includes the Adaptive Feature Enhance Module, Cross-knowledge Attention-guided Module and Robust Task-oriented Head Module to accurately predict direction angles for high-precision navigation. To evaluate the vision-based navigation methods, we collect a new dataset termed as UAV_AR368. Furthermore, we design the Simulation Flight Testing Instrument (SFTI) using Google Earth to simulate different flight environments, thereby reducing the expenses associated with real flight testing. Experiment results demonstrate that the proposed model outperforms the state-of-the-art by achieving improvements of 26.0% and 45.6% in the success rate of arrival under ideal and disturbed circumstances, respectively",
    "checked": true,
    "id": "6574f29b8ad631434b39c74de3fd6758ec618d7d",
    "semantic_title": "angle robustness unmanned aerial vehicle navigation in gnss-denied scenarios",
    "citation_count": 0,
    "authors": [
      "Yuxin Wang",
      "Zunlei Feng",
      "Haofei Zhang",
      "Yang Gao",
      "Jie Lei",
      "Li Sun",
      "Mingli Song"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28907": {
    "title": "Learning from Ambiguous Demonstrations with Self-Explanation Guided Reinforcement Learning",
    "volume": "main",
    "abstract": "Our work aims at efficiently leveraging ambiguous demonstrations for the training of a reinforcement learning (RL) agent. An ambiguous demonstration can usually be interpreted in multiple ways, which severely hinders the RL agent from learning stably and efficiently. Since an optimal demonstration may also suffer from being ambiguous, previous works that combine RL and learning from demonstration (RLfD works) may not work well. Inspired by how humans handle such situations, we propose to use self-explanation (an agent generates explanations for itself) to recognize valuable high-level relational features as an interpretation of why a successful trajectory is successful. This way, the agent can leverage the explained important relations as guidance for its RL learning. Our main contribution is to propose the Self-Explanation for RL from Demonstrations (SERLfD) framework, which can overcome the limitations of existing RLfD works. Our experimental results show that an RLfD model can be improved by using our SERLfD framework in terms of training stability and performance. To foster further research in self-explanation-guided robot learning, we have made our demonstrations and code publicly accessible at https://github.com/YantianZha/SERLfD. For a deeper understanding of our work, interested readers can refer to our arXiv version at https://arxiv.org/pdf/2110.05286.pdf, including an accompanying appendix",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yantian Zha",
      "Lin Guan",
      "Subbarao Kambhampati"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28908": {
    "title": "Multi-Constellation-Inspired Single-Shot Global LiDAR Localization",
    "volume": "main",
    "abstract": "Global localization is a challenging task for intelligent robots, as its accuracy directly contributes to the performance of downstream navigation and planning tasks. However, existing literature focus more on the place retrieval and the success rate of localization, with limited attention given to the metrics of position estimation. In this paper, a single-shot global LiDAR localization method is proposed with the ultimate goal of achieving high position accuracy, inspired by the positioning approach of multi-constellation localization systems. Initially, we perform coarse localization using global descriptors and select observation points along with their corresponding coordinates based on the obtained coarse localization results. Coordinates can be acquired from a pre-built map, GNSS, or other devices. Then, a lightweight LiDAR odometry method is designed to estimate the distance between the retrieved data and the observation points. Ultimately, the localization problem is transformed into an optimization problem of solving a system of multiple sphere equations. The experimental results on the KITTI dataset and the self-collected dataset demonstrate that our method achieves an average localization error (including errors in the z-axis) of 0.89 meters. In addition, it achieves retrieval efficiency of 0.357 s per frame on the former dataset and 0.214 s per frame on the latter one. Code and data are available at https://github.com/jlurobot/multi-constellation-localization",
    "checked": true,
    "id": "243df2b32698cf1f58fe54a142cd19e9c4495c3d",
    "semantic_title": "multi-constellation-inspired single-shot global lidar localization",
    "citation_count": 1,
    "authors": [
      "Tongzhou Zhang",
      "Gang Wang",
      "Yu Chen",
      "Hai Zhang",
      "Jue Hu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28909": {
    "title": "DeepPointMap: Advancing LiDAR SLAM with Unified Neural Descriptors",
    "volume": "main",
    "abstract": "Point clouds have shown significant potential in various domains, including Simultaneous Localization and Mapping (SLAM). However, existing approaches either rely on dense point clouds to achieve high localization accuracy or use generalized descriptors to reduce map size. Unfortunately, these two aspects seem to conflict with each other. To address this limitation, we propose an unified architecture, DeepPointMap, achieving excellent preference on both aspects. We utilize neural network to extract highly representative and sparse neural descriptors from point clouds, enabling memory-efficient map representation and accurate multi-scale localization tasks (e.g., odometry and loop-closure). Moreover, we showcase the versatility of our framework by extending it to more challenging multi-agent collaborative SLAM. The promising results obtained in these scenarios further emphasize the effectiveness and potential of our approach",
    "checked": true,
    "id": "c104e54563a80487743a738902238b49b277093e",
    "semantic_title": "deeppointmap: advancing lidar slam with unified neural descriptors",
    "citation_count": 0,
    "authors": [
      "Xiaze Zhang",
      "Ziheng Ding",
      "Qi Jing",
      "Yuejie Zhang",
      "Wenchao Ding",
      "Rui Feng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28798": {
    "title": "Analytically Tractable Models for Decision Making under Present Bias",
    "volume": "main",
    "abstract": "Time-inconsistency is a characteristic of human behavior in which people plan for long-term benefits but take actions that differ from the plan due to conflicts with short-term benefits. Such time-inconsistent behavior is believed to be caused by present bias, a tendency to overestimate immediate rewards and underestimate future rewards. It is essential in behavioral economics to investigate the relationship between present bias and time-inconsistency. In this paper, we propose a model for analyzing agent behavior with present bias in tasks to make progress toward a goal over a specific period. Unlike previous models, the state sequence of the agent can be described analytically in our model. Based on this property, we analyze three crucial problems related to agents under present bias: task abandonment, optimal goal setting, and optimal reward scheduling. Extensive analysis reveals how present bias affects the condition under which task abandonment occurs and optimal intervention strategies. Our findings are meaningful for preventing task abandonment and intervening through incentives in the real world",
    "checked": true,
    "id": "fd5e4bcb0e5dc9441d0f9b25a94364fc94bdb628",
    "semantic_title": "analytically tractable models for decision making under present bias",
    "citation_count": 0,
    "authors": [
      "Yasunori Akagi",
      "Naoki Marumo",
      "Takeshi Kurashima"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28799": {
    "title": "Optimistic Policy Gradient in Multi-Player Markov Games with a Single Controller: Convergence beyond the Minty Property",
    "volume": "main",
    "abstract": "Policy gradient methods enjoy strong practical performance in numerous tasks in reinforcement learning. Their theoretical understanding in multiagent settings, however, remains limited, especially beyond two-player competitive and potential Markov games. In this paper, we develop a new framework to characterize optimistic policy gradient methods in multi-player Markov games with a single controller. Specifically, under the further assumption that the game exhibits an equilibrium collapse, in that the marginals of coarse correlated equilibria (CCE) induce Nash equilibria (NE), we show convergence to stationary epsilon-NE in O(1/epsilon^2) iterations, where O suppresses polynomial factors in the natural parameters of the game. Such an equilibrium collapse is well-known to manifest itself in two-player zero-sum Markov games, but also occurs even in a class of multi-player Markov games with separable interactions, as established by recent work. As a result, we bypass known complexity barriers for computing stationary NE when either of our assumptions fails. Our approach relies on a natural generalization of the classical Minty property that we introduce, which we anticipate to have further applications beyond Markov games",
    "checked": true,
    "id": "a910452882e5da9202fe2d7247e0d5008d0a7d25",
    "semantic_title": "optimistic policy gradient in multi-player markov games with a single controller: convergence beyond the minty property",
    "citation_count": 2,
    "authors": [
      "Ioannis Anagnostides",
      "Ioannis Panageas",
      "Gabriele Farina",
      "Tuomas Sandholm"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28800": {
    "title": "Improved Metric Distortion via Threshold Approvals",
    "volume": "main",
    "abstract": "We consider a social choice setting in which agents and alternatives are represented by points in a metric space, and the cost of an agent for an alternative is the distance between the corresponding points in the space. The goal is to choose a single alternative to (approximately) minimize the social cost (cost of all agents) or the maximum cost of any agent, when only limited information about the preferences of the agents is given. Previous work has shown that the best possible distortion one can hope to achieve is 3 when access to the ordinal preferences of the agents is given, even when the distances between alternatives in the metric space are known. We improve upon this bound of 3 by designing deterministic mechanisms that exploit a bit of cardinal information. We show that it is possible to achieve distortion 1+sqrt(2) by using the ordinal preferences of the agents, the distances between alternatives, and a threshold approval set per agent that contains all alternatives for whom her cost is within an appropriately chosen factor of her cost for her most-preferred alternative. We show that this bound is the best possible for any deterministic mechanism in general metric spaces, and also provide improved bounds for the fundamental case of a line metric",
    "checked": true,
    "id": "c881dce069766c7dbd3d9c7fb8565d0ee23dc781",
    "semantic_title": "improved metric distortion via threshold approvals",
    "citation_count": 4,
    "authors": [
      "Elliot Anshelevich",
      "Aris Filos-Ratsikas",
      "Christopher Jerrett",
      "Alexandros A. Voudouris"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28801": {
    "title": "Fair Lotteries for Participatory Budgeting",
    "volume": "main",
    "abstract": "In pursuit of participatory budgeting (PB) outcomes with broader fairness guarantees, we initiate the study of lotteries over discrete PB outcomes. As the projects have heterogeneous costs, the amount spent may not be equal ex ante and ex post. To address this, we develop a technique to bound the amount by which the ex-post spend differs from the ex-ante spend---the property is termed budget balanced up to one project (BB1). With respect to fairness, we take a best-of-both-worlds perspective, seeking outcomes that are both ex-ante and ex-post fair. Towards this goal, we initiate a study of ex-ante fairness properties in PB, including Individual Fair Share (IFS), Unanimous Fair Share (UFS) and their stronger variants, as well as Group Fair Share (GFS). We show several incompatibility results between these ex-ante fairness notions and existing ex-post concepts based on justified representation. One of our main contributions is a randomized algorithm which simultaneously satisfies ex-ante Strong UFS, ex-post full justified representation (FJR) and ex-post BB1 for PB with binary utilities",
    "checked": true,
    "id": "e23e8fe972388c4e00163cb2ca99293ab9eaee40",
    "semantic_title": "fair lotteries for participatory budgeting",
    "citation_count": 2,
    "authors": [
      "Haris Aziz",
      "Xinhang Lu",
      "Mashbat Suzuki",
      "Jeremy Vollen",
      "Toby Walsh"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28802": {
    "title": "Envy-Free House Allocation under Uncertain Preferences",
    "volume": "main",
    "abstract": "Envy-freeness is one of the most important fairness concerns when allocating items. We study envy-free house allocation when agents have uncertain preferences over items and consider several well-studied preference uncertainty models. The central problem that we focus on is computing an allocation that has the highest probability of being envy-free. We show that each model leads to a distinct set of algorithmic and complexity results, including detailed results on (in-)approximability. En route, we consider two related problems of checking whether there exists an allocation that is possibly or necessarily envy-free. We give a complete picture of the computational complexity of these two problems for all the uncertainty models we consider",
    "checked": true,
    "id": "85bb437337b0ddbca2f68bcb4b5019fcb2cca5c8",
    "semantic_title": "envy-free house allocation under uncertain preferences",
    "citation_count": 1,
    "authors": [
      "Haris Aziz",
      "Isaiah Iliffe",
      "Bo Li",
      "Angus Ritossa",
      "Ankang Sun",
      "Mashbat Suzuki"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28803": {
    "title": "Content Filtering with Inattentive Information Consumers",
    "volume": "main",
    "abstract": "We develop a model of content filtering as a game between the filter and the content consumer, where the latter incurs information costs for examining the content. Motivating examples include censoring misinformation, spam/phish filtering, and recommender systems acting on a stream of content. When the attacker is exogenous, we show that improving the filter's quality is weakly Pareto improving, but has no impact on equilibrium payoffs until the filter becomes sufficiently accurate. Further, if the filter does not internalize the consumer's information costs, its lack of commitment power may render it useless and lead to inefficient outcomes. When the attacker is also strategic, improvements in filter quality may decrease equilibrium payoffs",
    "checked": true,
    "id": "1115ebb1984bf1055da4898d3d5d915d4e2b2811",
    "semantic_title": "content filtering with inattentive information consumers",
    "citation_count": 0,
    "authors": [
      "Ian Ball",
      "James Bono",
      "Justin Grana",
      "Nicole Immorlica",
      "Brendan Lucier",
      "Aleksandrs Slivkins"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28804": {
    "title": "Nearly Equitable Allocations beyond Additivity and Monotonicity",
    "volume": "main",
    "abstract": "Equitability (EQ) in fair division requires that items be allocated such that all agents value the bundle they receive equally. With indivisible items, an equitable allocation may not exist, and hence we instead consider a meaningful analog, EQx, that requires equitability up to any item. EQx allocations exist for monotone, additive valuations. However, if (1) the agents' valuations are not additive or (2) the set of indivisible items includes both goods and chores (positively and negatively valued items), then prior to the current work it was not known whether EQx allocations exist or not. We study both the existence and efficient computation of EQx allocations. (1) For monotone valuations (not necessarily additive), we show that EQx allocations always exist. Also, for the large class of weakly well-layered valuations, EQx allocations can be found in polynomial time. Further, we prove that approximately EQx allocations can be computed efficiently under general monotone valuations. (2) For non-monotone valuations, we show that an EQx allocation may not exist, even for two agents with additive valuations. Under some special cases, however, we show existence and efficient computability of EQx allocations. This includes the case of two agents with additive valuations where each item is either a good or a chore, and there are no mixed items",
    "checked": true,
    "id": "a03d7e7137a59f4daf970e411811e70c202c41fc",
    "semantic_title": "nearly equitable allocations beyond additivity and monotonicity",
    "citation_count": 0,
    "authors": [
      "Siddharth Barman",
      "Umang Bhaskar",
      "Yeshwant Pandit",
      "Soumyajit Pyne"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28805": {
    "title": "Principal-Agent Reward Shaping in MDPs",
    "volume": "main",
    "abstract": "Principal-agent problems arise when one party acts on behalf of another, leading to conflicts of interest. The economic literature has extensively studied principal-agent problems, and recent work has extended this to more complex scenarios such as Markov Decision Processes (MDPs). In this paper, we further explore this line of research by investigating how reward shaping under budget constraints can improve the principal's utility. We study a two-player Stackelberg game where the principal and the agent have different reward functions, and the agent chooses an MDP policy for both players. The principal offers an additional reward to the agent, and the agent picks their policy selfishly to maximize their reward, which is the sum of the original and the offered reward. Our results establish the NP-hardness of the problem and offer polynomial approximation algorithms for two classes of instances: Stochastic trees and deterministic decision processes with a finite horizon",
    "checked": true,
    "id": "ad700dc8798e4a7a0e0d464d1077c4cafd6aae69",
    "semantic_title": "principal-agent reward shaping in mdps",
    "citation_count": 5,
    "authors": [
      "Omer Ben-Porat",
      "Yishay Mansour",
      "Michal Moshkovitz",
      "Boaz Taitler"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28806": {
    "title": "Enhancing the Efficiency of Altruism and Taxes in Affine Congestion Games through Signalling",
    "volume": "main",
    "abstract": "We address the problem of improving the worst-case efficiency of pure Nash equilibria (aka, the price of anarchy) in affine congestion games, through a novel use of signalling. We assume that, for each player in the game, a most preferred strategy is publicly signalled. This can be done either distributedly by the players themselves, or be the outcome of some centralized algorithm. We apply this signalling scheme to two well-studied scenarios: games with partially altruistic players and games with resource taxation. We show a significant improvement in the price of anarchy of these games, whenever the aggregate signalled strategy profile is a good approximation of the game social optimum",
    "checked": true,
    "id": "b1029c6a4bb6ba4f0b01c8ddb3dc3868d2a56ed9",
    "semantic_title": "enhancing the efficiency of altruism and taxes in affine congestion games through signalling",
    "citation_count": 0,
    "authors": [
      "Vittorio Bilò",
      "Cosimo Vinci"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28807": {
    "title": "Approval-Based Committee Voting in Practice: A Case Study of (over-)Representation in the Polkadot Blockchain",
    "volume": "main",
    "abstract": "We provide the first large-scale data collection of real-world approval-based committee elections. These elections have been conducted on the Polkadot blockchain as part of their Nominated Proof-of-Stake mechanism and contain around one thousand candidates and tens of thousands of (weighted) voters each. We conduct an in-depth study of application-relevant questions, including a quantitative and qualitative analysis of the outcomes returned by different voting rules. Besides considering proportionality measures that are standard in the multiwinner voting literature, we pay particular attention to less-studied measures of overrepresentation, as these are closely related to the security of the Polkadot network. We also analyze how different design decisions such as the committee size affect the examined measures",
    "checked": true,
    "id": "2ff21a0671d5dd4f9aabfc56e760d58670864e6d",
    "semantic_title": "approval-based committee voting in practice: a case study of (over-)representation in the polkadot blockchain",
    "citation_count": 0,
    "authors": [
      "Niclas Boehmer",
      "Markus Brill",
      "Alfonso Cevallos",
      "Jonas Gehrlein",
      "Luis Sánchez-Fernández",
      "Ulrike Schmidt-Kraepelin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28808": {
    "title": "Completing Priceable Committees: Utilitarian and Representation Guarantees for Proportional Multiwinner Voting",
    "volume": "main",
    "abstract": "When selecting committees based on preferences of voters, a variety of different criteria can be considered. Two natural objectives are maximizing the utilitarian welfare (the sum of voters' utilities) and coverage (the number of represented voters) of the selected committee. Previous work has studied the impact on utilitarian welfare and coverage when requiring the committee to satisfy minimal requirements such as justified representation or weak proportionality. In this paper, we consider the impact of imposing much more demanding proportionality axioms. We identify a class of voting rules that achieve strong guarantees on utilitarian welfare and coverage when combined with appropriate completions. This class is defined via a weakening of priceability and contains prominent rules such as the Method of Equal Shares. We show that committees selected by these rules (i) can be completed to achieve optimal coverage and (ii) can be completed to achieve an asymptotically optimal approximation to the utilitarian welfare if they additionally satisfy EJR+. Answering an open question of Elkind et al. (2022), we use the Greedy Justified Candidate Rule to obtain the best possible utilitarian guarantee subject to proportionality. We also consider completion methods suggested in the participatory budgeting literature and other objectives besides welfare and coverage",
    "checked": true,
    "id": "186be6dae5fc4cbe6434e6a8532e181be3b0cc10",
    "semantic_title": "completing priceable committees: utilitarian and representation guarantees for proportional multiwinner voting",
    "citation_count": 2,
    "authors": [
      "Markus Brill",
      "Jannik Peters"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28809": {
    "title": "Stability in Online Coalition Formation",
    "volume": "main",
    "abstract": "Coalition formation is concerned with the question of how to partition a set of agents into disjoint coalitions according to their preferences. Deviating from most of the previous work, we consider an online variant of the problem, where agents arrive in sequence and whenever an agent arrives, they have to be assigned to a coalition immediately and irrevocably. The scarce existing literature on online coalition formation has focused on the objective of maximizing social welfare, a demanding requirement, even in the offline setting. Instead, we seek to achieve stable coalition structures in an online setting, and focus on stability concepts based on deviations by single agents. We present a comprehensive picture in additively separable hedonic games, leading to dichotomies, where positive results are obtained by deterministic algorithms and negative results even hold for randomized algorithms",
    "checked": true,
    "id": "40eb1f9e0dd2e8b65f9181371ce9d5b556d5f383",
    "semantic_title": "stability in online coalition formation",
    "citation_count": 2,
    "authors": [
      "Martin Bullinger",
      "René Romen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28810": {
    "title": "Participation Incentives in Approval-Based Committee Elections",
    "volume": "main",
    "abstract": "In approval-based committee (ABC) voting, the goal is to choose a subset of predefined size of the candidates based on the voters' approval preferences over the candidates. While this problem has attracted significant attention in recent years, the incentives for voters to participate in an election for a given ABC voting rule have been neglected so far. This paper is thus the first to explicitly study this property, typically called participation, for ABC voting rules. In particular, we show that all ABC scoring rules even satisfy group participation, whereas most sequential rules severely fail participation. We furthermore explore several escape routes to the impossibility for sequential ABC voting rules: we prove for many sequential rules that (i) they satisfy participation on laminar profiles, (ii) voters who approve none of the elected candidates cannot benefit by abstaining, and (iii) it is NP-hard for a voter to decide whether she benefits from abstaining",
    "checked": true,
    "id": "aa79cff462ccf1406a74a5e37ae725139d7a7bb2",
    "semantic_title": "participation incentives in approval-based committee elections",
    "citation_count": 1,
    "authors": [
      "Martin Bullinger",
      "Chris Dong",
      "Patrick Lederer",
      "Clara Mehler"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28811": {
    "title": "Low-Distortion Clustering with Ordinal and Limited Cardinal Information",
    "volume": "main",
    "abstract": "Motivated by recent work in computational social choice, we extend the metric distortion framework to clustering problems. Given a set of n agents located in an underlying metric space, our goal is to partition them into k clusters, optimizing some social cost objective. The metric space is defined by a distance function d between the agent locations. Information about d is available only implicitly via n rankings, through which each agent ranks all other agents in terms of their distance from her. Still, even though no cardinal information (i.e., the exact distance values) is available, we would like to evaluate clustering algorithms in terms of social cost objectives that are defined using d. This is done using the notion of distortion, which measures how far from optimality a clustering can be, taking into account all underlying metrics that are consistent with the ordinal information available. Unfortunately, the most important clustering objectives (e.g., those used in the well-known k-median and k-center problems) do not admit algorithms with finite distortion. To sidestep this disappointing fact, we follow two alternative approaches: We first explore whether resource augmentation can be beneficial. We consider algorithms that use more than k clusters but compare their social cost to that of the optimal k-clusterings. We show that using exponentially (in terms of k) many clusters, we can get low (constant or logarithmic) distortion for the k-center and k-median objectives. Interestingly, such an exponential blowup is shown to be necessary. More importantly, we explore whether limited cardinal information can be used to obtain better results. Somewhat surprisingly, for k-median and k-center, we show that a number of queries that is polynomial in k and only logarithmic in n (i.e., only sublinear in the number of agents for the most relevant scenarios in practice) is enough to get constant distortion",
    "checked": true,
    "id": "72a8222968e83e02da0a5823f4f9135725d26d70",
    "semantic_title": "low-distortion clustering with ordinal and limited cardinal information",
    "citation_count": 3,
    "authors": [
      "Jakob Burkhardt",
      "Ioannis Caragiannis",
      "Karl Fehrs",
      "Matteo Russo",
      "Chris Schwiegelshohn",
      "Sudarshan Shyam"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28812": {
    "title": "Efficient Learning in Polyhedral Games via Best-Response Oracles",
    "volume": "main",
    "abstract": "We study online learning and equilibrium computation in games with polyhedral decision sets, a property shared by normal-form games (NFGs) and extensive-form games (EFGs), when the learning agent is restricted to utilizing a best-response oracle. We show how to achieve constant regret in zero-sum games and O(T^0.25) regret in general-sum games while using only O(log t) best-response queries at a given iteration t, thus improving over the best prior result, which required O(T) queries per iteration. Moreover, our framework yields the first last-iterate convergence guarantees for self-play with best-response oracles in zero-sum games. This convergence occurs at a linear rate, though with a condition-number dependence. We go on to show a O(T^(-0.5)) best-iterate convergence rate without such a dependence. Our results build on linear-rate convergence results for variants of the Frank-Wolfe (FW) algorithm for strongly convex and smooth minimization problems over polyhedral domains. These FW results depend on a condition number of the polytope, known as facial distance. In order to enable application to settings such as EFGs, we show two broad new results: 1) the facial distance for polytopes in standard form is at least γ/k where γ is the minimum value of a nonzero coordinate of a vertex of the polytope and k≤n is the number of tight inequality constraints in the optimal face, and 2) the facial distance for polytopes of the form Ax=b, Cx≤d, x≥0 where x∈R^n, C≥0 is a nonzero integral matrix, and d≥0, is at least 1/(c√n), where c is the infinity norm of C. This yields the first such results for several problems such as sequence-form polytopes, flow polytopes, and matching polytopes",
    "checked": false,
    "id": "7ed9dcc3b3884032d52d12cd219b2790f682b703",
    "semantic_title": "efficient learning in polyhedral games via best response oracles",
    "citation_count": 1,
    "authors": [
      "Darshan Chakrabarti",
      "Gabriele Farina",
      "Christian Kroer"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28813": {
    "title": "Proportional Aggregation of Preferences for Sequential Decision Making",
    "volume": "main",
    "abstract": "We study the problem of fair sequential decision making given voter preferences. In each round, a decision rule must choose a decision from a set of alternatives where each voter reports which of these alternatives they approve. Instead of going with the most popular choice in each round, we aim for proportional representation, using axioms inspired by the multi-winner voting literature. The axioms require that every group of α% of the voters, if it agrees in every round (i.e., approves a common alternative), then those voters must approve at least α% of the decisions. A stronger version of the axioms requires that every group of α% of the voters that agrees in a β fraction of rounds must approve β⋅α% of the decisions. We show that three attractive voting rules satisfy axioms of this style. One of them (Sequential Phragmén) makes its decisions online, and the other two satisfy strengthened versions of the axioms but make decisions semi-online (Method of Equal Shares) or fully offline (Proportional Approval Voting). We present empirical results for these rules based on synthetic data and U.S. political elections. We also run experiments using the moral machine dataset about ethical dilemmas. We train preference models on user responses from different countries and let the models cast votes. We find that aggregating these votes using our rules leads to a more equal utility distribution across demographics than making decisions using a single global preference model",
    "checked": true,
    "id": "07bc3859970cdca573c3f0105541a123e6951253",
    "semantic_title": "proportional aggregation of preferences for sequential decision making",
    "citation_count": 2,
    "authors": [
      "Nikhil Chandak",
      "Shashwat Goel",
      "Dominik Peters"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28814": {
    "title": "How to Make Knockout Tournaments More Popular?",
    "volume": "main",
    "abstract": "Given a mapping from a set of players to the leaves of a complete binary tree (called a seeding), a knockout tournament is conducted as follows: every round, every two players with a common parent compete against each other, and the winner is promoted to the common parent; then, the leaves are deleted. When only one player remains, it is declared the winner. This is a popular competition format in sports, elections, and decision-making. Over the past decade, it has been studied intensively from both theoretical and practical points of view. Most frequently, the objective is to seed the tournament in a way that ``assists'' (or even guarantees) some particular player to win the competition. We introduce a new objective, which is very sensible from the perspective of the directors of the competition: maximize the profit or popularity of the tournament. Specifically, we associate a ``score'' with every possible match, and aim to seed the tournament to maximize the sum of the scores of the matches that take place. We focus on the case where we assume a total order on the players' strengths, and provide a wide spectrum of results on the computational complexity of the problem",
    "checked": true,
    "id": "f27dc59e42e7ddc681bed404952e471cbba8e429",
    "semantic_title": "how to make knockout tournaments more popular?",
    "citation_count": 1,
    "authors": [
      "Juhi Chaudhary",
      "Hendrik Molter",
      "Meirav Zehavi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28815": {
    "title": "1/2-Approximate MMS Allocation for Separable Piecewise Linear Concave Valuations",
    "volume": "main",
    "abstract": "We study fair distribution of a collection of m indivisible goods among a group of n agents, using the widely recognized fairness principles of Maximin Share (MMS) and Any Price Share (APS). These principles have undergone thorough investigation within the context of additive valuations. We explore these notions for valuations that extend beyond additivity. First, we study approximate MMS under the separable (piecewise-linear) concave (SPLC) valuations, an important class generalizing additive, where the best known factor was 1/3-MMS. We show that 1/2-MMS allocation exists and can be computed in polynomial time, significantly improving the state-of-the-art. We note that SPLC valuations introduce an elevated level of intricacy in contrast to additive. For instance, the MMS value of an agent can be as high as her value for the entire set of items. We use a relax-and-round paradigm that goes through competitive equilibrium and LP relaxation. Our result extends to give (symmetric) 1/2-APS, a stronger guarantee than MMS. APS is a stronger notion that generalizes MMS by allowing agents with arbitrary entitlements. We study the approximation of APS under submodular valuation functions. We design and analyze a simple greedy algorithm using concave extensions of submodular functions. We prove that the algorithm gives a 1/3-APS allocation which matches the best-known factor. Concave extensions are hard to compute in polynomial time and are, therefore, generally not used in approximation algorithms. Our approach shows a way to utilize it within analysis (while bypassing its computation), and hence might be of independent interest",
    "checked": false,
    "id": "0d4523b7683cee6ef5877ef3e5766acc63fc1be6",
    "semantic_title": "1/2 approximate mms allocation for separable piecewise linear concave valuations",
    "citation_count": 1,
    "authors": [
      "Chandra Chekuri",
      "Pooja Kulkarni ",
      "Rucha Kulkarni",
      "Ruta Mehta"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28816": {
    "title": "Dynamic Budget Throttling in Repeated Second-Price Auctions",
    "volume": "main",
    "abstract": "In today's online advertising markets, a crucial requirement for an advertiser is to control her total expenditure within a time horizon under some budget. Among various budget control methods, throttling has emerged as a popular choice, managing an advertiser's total expenditure by selecting only a subset of auctions to participate in. This paper provides a theoretical panorama of a single advertiser's dynamic budget throttling process in repeated second-price auctions. We first establish a lower bound on the regret and an upper bound on the asymptotic competitive ratio for any throttling algorithm, respectively, when the advertiser's values are stochastic and adversarial. Regarding the algorithmic side, we propose the OGD-CB algorithm, which guarantees a near-optimal expected regret with stochastic values. On the other hand, when values are adversarial, we prove that this algorithm also reaches the upper bound on the asymptotic competitive ratio. We further compare throttling with pacing, another widely adopted budget control method, in repeated second-price auctions. In the stochastic case, we demonstrate that pacing is generally superior to throttling for the advertiser, supporting the well-known result that pacing is asymptotically optimal in this scenario. However, in the adversarial case, we give an exciting result indicating that throttling is also an asymptotically optimal dynamic bidding strategy. Our results bridge the gaps in theoretical research of throttling in repeated auctions and comprehensively reveal the ability of this popular budget-smoothing strategy",
    "checked": true,
    "id": "ca6caed9431ab697c03c56563669fb30c060c0c2",
    "semantic_title": "dynamic budget throttling in repeated second-price auctions",
    "citation_count": 6,
    "authors": [
      "Zhaohua Chen",
      "Chang Wang",
      "Qian Wang",
      "Yuqi Pan",
      "Zhuming Shi",
      "Zheng Cai",
      "Yukun Ren",
      "Zhihua Zhu",
      "Xiaotie Deng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28817": {
    "title": "The Complexity of Computing Robust Mediated Equilibria in Ordinal Games",
    "volume": "main",
    "abstract": "Usually, to apply game-theoretic methods, we must specify utilities precisely, and we run the risk that the solutions we compute are not robust to errors in this specification. Ordinal games provide an attractive alternative: they require specifying only which outcomes are preferred to which other ones. Unfortunately, they provide little guidance for how to play unless there are pure Nash equilibria; evaluating mixed strategies appears to fundamentally require cardinal utilities. In this paper, we observe that we can in fact make good use of mixed strategies in ordinal games if we consider settings that allow for folk theorems. These allow us to find equilibria that are robust, in the sense that they remain equilibria no matter which cardinal utilities are the correct ones -- as long as they are consistent with the specified ordinal preferences. We analyze this concept and study the computational complexity of finding such equilibria in a range of settings",
    "checked": true,
    "id": "206cb559ffad3ee42eff88dfea3d41ac004f4e5e",
    "semantic_title": "the complexity of computing robust mediated equilibria in ordinal games",
    "citation_count": 1,
    "authors": [
      "Vincent Conitzer"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28818": {
    "title": "Learning Discrete-Time Major-Minor Mean Field Games",
    "volume": "main",
    "abstract": "Recent techniques based on Mean Field Games (MFGs) allow the scalable analysis of multi-player games with many similar, rational agents. However, standard MFGs remain limited to homogeneous players that weakly influence each other, and cannot model major players that strongly influence other players, severely limiting the class of problems that can be handled. We propose a novel discrete time version of major-minor MFGs (M3FGs), along with a learning algorithm based on fictitious play and partitioning the probability simplex. Importantly, M3FGs generalize MFGs with common noise and can handle not only random exogeneous environment states but also major players. A key challenge is that the mean field is stochastic and not deterministic as in standard MFGs. Our theoretical investigation verifies both the M3FG model and its algorithmic solution, showing firstly the well-posedness of the M3FG model starting from a finite game of interest, and secondly convergence and approximation guarantees of the fictitious play algorithm. Then, we empirically verify the obtained theoretical results, ablating some of the theoretical assumptions made, and show successful equilibrium learning in three example problems. Overall, we establish a learning framework for a novel and broad class of tractable games",
    "checked": true,
    "id": "feb3b502a15c3300554a150511d6d2ef4685247a",
    "semantic_title": "learning discrete-time major-minor mean field games",
    "citation_count": 2,
    "authors": [
      "Kai Cui",
      "Gökçe Dayanıklı",
      "Mathieu Laurière",
      "Matthieu Geist",
      "Olivier Pietquin",
      "Heinz Koeppl"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28819": {
    "title": "Automated Design of Affine Maximizer Mechanisms in Dynamic Settings",
    "volume": "main",
    "abstract": "Dynamic mechanism design is a challenging extension to ordinary mechanism design in which the mechanism designer must make a sequence of decisions over time in the face of possibly untruthful reports of participating agents. Optimizing dynamic mechanisms for welfare is relatively well understood. However, there has been less work on optimizing for other goals (e.g., revenue), and without restrictive assumptions on valuations, it is remarkably challenging to characterize good mechanisms. Instead, we turn to automated mechanism design to find mechanisms with good performance in specific problem instances. We extend the class of affine maximizer mechanisms to MDPs where agents may untruthfully report their rewards. This extension results in a challenging bilevel optimization problem in which the upper problem involves choosing optimal mechanism parameters, and the lower problem involves solving the resulting MDP. Our approach can find truthful dynamic mechanisms that achieve strong performance on goals other than welfare, and can be applied to essentially any problem setting---without restrictions on valuations---for which RL can learn optimal policies",
    "checked": true,
    "id": "6adad06ce0e0872a9ee49badde3a442104f6a623",
    "semantic_title": "automated design of affine maximizer mechanisms in dynamic settings",
    "citation_count": 3,
    "authors": [
      "Michael Curry",
      "Vinzenz Thoma",
      "Darshan Chakrabarti",
      "Stephen McAleer",
      "Christian Kroer",
      "Tuomas Sandholm",
      "Niao He",
      "Sven Seuken"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28820": {
    "title": "How to Evaluate Behavioral Models",
    "volume": "main",
    "abstract": "Researchers building behavioral models, such as behavioral game theorists, use experimental data to evaluate predictive models of human behavior. However, there is little agreement about which loss function should be used in evaluations, with error rate, negative log-likelihood, cross-entropy, Brier score, and squared L2 error all being common choices. We attempt to offer a principled answer to the question of which loss functions should be used for this task, formalizing axioms that we argue loss functions should satisfy. We construct a family of loss functions, which we dub ``diagonal bounded Bregman divergences'', that satisfy all of these axioms. These rule out many loss functions used in practice, but notably include squared L2 error; we thus recommend its use for evaluating behavioral models",
    "checked": true,
    "id": "36888fa51d8ca7e79accec54239de13403a2fe7f",
    "semantic_title": "how to evaluate behavioral models",
    "citation_count": 0,
    "authors": [
      "Greg d'Eon",
      "Sophie Greenwood",
      "Kevin Leyton-Brown",
      "James R. Wright"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28821": {
    "title": "Independence of Irrelevant Alternatives under the Lens of Pairwise Distortion",
    "volume": "main",
    "abstract": "We give a quantitative analysis of the independence of irrelevant alternatives (IIA) axiom. IIA says that the society's preference between x and y should depend only on individual preferences between x and y: we show that, in several contexts, if the individuals express their preferences about additional (``irrelevant'') alternatives, this information helps to estimate better which of x and y has higher social welfare. Our contribution is threefold: (1) we provide a new tool to measure the impact of IIA on social welfare (pairwise distortion), based on the well-established notion of voting distortion, (2) we study the average impact of IIA in both general and metric settings, with experiments on synthetic and real data and (3) we study the worst-case impact of IIA in the 1D-Euclidean metric space",
    "checked": true,
    "id": "7add0656242fcf4840feea2de7e6dd108089549a",
    "semantic_title": "independence of irrelevant alternatives under the lens of pairwise distortion",
    "citation_count": 0,
    "authors": [
      "Théo Delemazure",
      "Jérôme Lang",
      "Grzegorz Pierczyński"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28822": {
    "title": "The Complexity of Fair Division of Indivisible Items with Externalities",
    "volume": "main",
    "abstract": "We study the computational complexity of fairly allocating a set of indivisible items under externalities. In this recently-proposed setting, in addition to the utility the agent gets from their bundle, they also receive utility from items allocated to other agents. We focus on the extended definitions of envy-freeness up to one item (EF1) and of envy-freeness up to any item (EFX), and we provide the landscape of their complexity for several different scenarios. We prove that it is NP-complete to decide whether there exists an EFX allocation, even when there are only three agents, or even when there are only six different values for the items. We complement these negative results by showing that when both the number of agents and the number of different values for items are bounded by a parameter the problem becomes fixed-parameter tractable. Furthermore, we prove that two-valued and binary-valued instances are equivalent and that EFX and EF1 allocations coincide for this class of instances. Finally, motivated from real-life scenarios, we focus on a class of structured valuation functions, which we term agent/item-correlated. We prove their equivalence to the \"standard\" setting without externalities. Therefore, all previous results for EF1 and EFX apply immediately for these valuations",
    "checked": true,
    "id": "cf6c05185620526e0a75bf6e4271945c3f61292e",
    "semantic_title": "the complexity of fair division of indivisible items with externalities",
    "citation_count": 1,
    "authors": [
      "Argyrios Deligkas",
      "Eduard Eiben",
      "Viktoriia Korchemna",
      "Šimon Schierreich"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28823": {
    "title": "Competition among Pairwise Lottery Contests",
    "volume": "main",
    "abstract": "We investigate a two-stage competitive model involving multiple contests. In this model, each contest designer chooses two participants from a pool of candidate contestants and determines the biases. Contestants strategically distribute their efforts across various contests within their budget. We first show the existence of a pure strategy Nash equilibrium (PNE) for the contestants, and propose a fully polynomial-time approximation scheme to compute an approximate PNE. In the scenario where designers simultaneously decide the participants and biases, the subgame perfect equilibrium (SPE) may not exist. Nonetheless, when designers' decisions are made in two substages, the existence of SPE is established. In the scenario where designers can hold multiple contests, we show that the SPE always exists under mild conditions and can be computed efficiently",
    "checked": true,
    "id": "25e144483acbcb1bb6282351ecbab3db95985218",
    "semantic_title": "competition among pairwise lottery contests",
    "citation_count": 0,
    "authors": [
      "Xiaotie Deng",
      "Hangxin Gan",
      "Ningyuan Li",
      "Weian Li",
      "Qi Qi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28824": {
    "title": "Refined Characterizations of Approval-Based Committee Scoring Rules",
    "volume": "main",
    "abstract": "In approval-based committee (ABC) elections, the goal is to select a fixed-size subset of the candidates, a so-called committee, based on the voters' approval ballots over the candidates. One of the most popular classes of ABC voting rules are ABC scoring rules, for which voters give points to each committee and the committees with maximal total points are chosen. While the set of ABC scoring rules has recently been characterized in a model where the output is a ranking of all committees, no full characterization of these rules exists in the standard model where a set of winning committees is returned. We address this issue by characterizing two important subclasses of ABC scoring rules in the standard ABC election model, thereby both extending the result for ABC ranking rules to the standard setting and refining it to subclasses. In more detail, by relying on a consistency axiom for variable electorates, we characterize (i) the prominent class of Thiele rules and (ii) a new class of ABC voting rules called ballot size weighted approval voting. Based on these theorems, we also infer characterizations of three well-known ABC voting rules, namely multi-winner approval voting, proportional approval voting, and satisfaction approval voting",
    "checked": true,
    "id": "432601e1091238fd5db74025439c778980c04009",
    "semantic_title": "refined characterizations of approval-based committee scoring rules",
    "citation_count": 3,
    "authors": [
      "Chris Dong",
      "Patrick Lederer"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28825": {
    "title": "Implications of Distance over Redistricting Maps: Central and Outlier Maps",
    "volume": "main",
    "abstract": "In representative democracy, a redistricting map is chosen to partition an electorate into districts which each elects a representative. A valid redistricting map must satisfy a collection of constraints such as being compact, contiguous, and of almost-equal population. However, these constraints are loose enough to enable an enormous ensemble of valid redistricting maps. This enables a partisan legislature to gerrymander by choosing a map which unfairly favors it. In this paper, we introduce an interpretable and tractable distance measure over redistricting maps which does not use election results and study its implications over the ensemble of redistricting maps. Specifically, we define a central map which may be considered \"most typical\" and give a rigorous justification for it by showing that it mirrors the Kemeny ranking in a scenario where we have a committee voting over a collection of redistricting maps to be drawn. We include runnning time and sample complexity analysis for our algorithms, including some negative results which hold using any algorithm. We further study outlier detection based on this distance measure and show that our framework can detect some gerrymandered maps. More precisely, we show some maps that are widely considered to be gerrymandered that lie very far away from our central maps in comparison to a large ensemble of valid redistricting maps. Since our distance measure does not rely on election results, this gives a significant advantage in gerrymandering detection which is lacking in all previous methods",
    "checked": true,
    "id": "e565ae3b1b9809e8bbdcb4b17f4a8056fa7e5e19",
    "semantic_title": "implications of distance over redistricting maps: central and outlier maps",
    "citation_count": 0,
    "authors": [
      "Seyed A. Esmaeili",
      "Darshan Chakrabarti",
      "Hayley Grape",
      "Brian Brubach"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28826": {
    "title": "On Optimal Tradeoffs between EFX and Nash Welfare",
    "volume": "main",
    "abstract": "A major problem in fair division is how to allocate a set of indivisible resources among agents fairly and efficiently. The goal of this work is to characterize the tradeoffs between two well-studied measures of fairness and efficiency --- envy freeness up to any item (EFX) for fairness, and Nash welfare for efficiency --- by saying, for given constants α and β, whether there exists an α-EFX allocation that guarantees a β-fraction of the maximum Nash welfare (β-MNW). For additive valuations, we show that for any α ∈ [0,1], there exists a partial allocation that is α-EFX and 1/(α+1)-MNW. This tradeoff turns out to be tight (for every α) as demonstrated by an impossibility result that we give. We also show that for α ∈ [0, φ-1 ≃ 0.618] these partial allocations can be turned into complete allocations where all items are assigned. Furthermore, for any α ∈ [0, 1/2], we show that the tight tradeoff of α-EFX and 1/(α+1)-MNW with complete allocations holds for the more general setting of subadditive valuations. Our results improve upon the current state of the art, for both additive and subadditive valuations, and match the best-known approximations of EFX under complete allocations, regardless of Nash welfare guarantees. Notably, our constructions for additive valuations also provide EF1 and constant approximations for maximin share guarantees",
    "checked": true,
    "id": "326370f538a47eee1377ba0f77d4d02f8bf65e45",
    "semantic_title": "on optimal tradeoffs between efx and nash welfare",
    "citation_count": 8,
    "authors": [
      "Michal Feldman",
      "Simon Mauras",
      "Tomasz Ponitka"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28827": {
    "title": "Manipulation-Robust Selection of Citizens' Assemblies",
    "volume": "main",
    "abstract": "Among the recent work on designing algorithms for selecting citizens' assembly participants, one key property of these algorithms has not yet been studied: their manipulability. Strategic manipulation is a concern because these algorithms must satisfy representation constraints according to volunteers' self-reported features; misreporting these features could thereby increase a volunteer's chance of being selected, decrease someone else's chance, and/or increase the expected number of seats given to their group. Strikingly, we show that Leximin — an algorithm that is widely used for its fairness — is highly manipulable in this way. We then introduce a new class of selection algorithms that use Lp norms as objective functions. We show that the manipulability of the Lp-based algorithm decreases in O(1/n^(1-1/p)) as the number of volunteers n grows, approaching the optimal rate of O(1/n) as p approaches infinity. These theoretical results are confirmed via experiments in eight real-world datasets",
    "checked": true,
    "id": "2712158437260fadd25260715ae9965ebfd4445e",
    "semantic_title": "manipulation-robust selection of citizens' assemblies",
    "citation_count": 4,
    "authors": [
      "Bailey Flanigan",
      "Jennifer Liang",
      "Ariel D. Procaccia",
      "Sven Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28828": {
    "title": "Project-Fair and Truthful Mechanisms for Budget Aggregation",
    "volume": "main",
    "abstract": "We study the budget aggregation problem in which a set of strategic voters must split a finite divisible resource (such as money or time) among a set of competing projects. Our goal is twofold: We seek truthful mechanisms that provide fairness guarantees to the projects. For the first objective, we focus on the class of moving phantom mechanisms, which are -- to this day -- essentially the only known truthful mechanisms in this setting. For project fairness, we consider the mean division as a fair baseline, and bound the maximum difference between the funding received by any project and this baseline. We propose a novel and simple moving phantom mechanism that provides optimal project fairness guarantees. As a corollary of our results, we show that our new mechanism minimizes the L1 distance to the mean for three projects and gives the first non-trivial bounds on this quantity for more than three projects",
    "checked": true,
    "id": "a8c125f4b6ba8419a4c36a397150571f3666736f",
    "semantic_title": "project-fair and truthful mechanisms for budget aggregation",
    "citation_count": 3,
    "authors": [
      "Rupert Freeman",
      "Ulrike Schmidt-Kraepelin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28829": {
    "title": "Maxileximin Envy Allocations and Connected Goods",
    "volume": "main",
    "abstract": "Fair allocation of indivisible goods presents intriguing challenges from both a social choice perspective and an algorithmic standpoint. Due to the indivisibility of goods, it is common for one agent to envy the bundle of goods assigned to another agent and, indeed, envy-free solutions do not exist in general. In line with the classical game-theoretic concept of Nucleolus in coalitional games, we propose that a fair allocation should minimize the agents' dissatisfaction profile in a lexicographic manner, where the dissatisfaction of an agent is defined as her maximum envy towards other agents. Therefore, we seek allocations that minimize the maximum envy. In cases where multiple solutions have an equal maximum value, we minimize the second-worst value, and so on. Additionally, as is customary in fair division problems, we also consider an efficiency requirement: among the allocations with the best agents' dissatisfaction profile, we prioritize those that maximize the sum of agents' utilities, known as maximum social welfare. Such allocations, referred to as maxileximin allocations, always exist. In this study, we analyze the computational properties of maxileximin allocations in the context of fair allocation problems with constraints. Specifically, we focus on the Connected Fair Division problem, where goods correspond to the nodes of a graph, and a bundle of goods is allowed if the subgraph formed by those goods is connected. We demonstrate that the problem is F∆P2 -complete, even for instances with simple graphical structures such as path and star graphs. However, we identify islands of tractability for instances with more intricate graphs, such as those having bounded treewidth, provided that the number of agents is bounded by a fixed number and utility functions use small values",
    "checked": true,
    "id": "77ab7970adcf16ef03d8b661df955b2e0d3234c7",
    "semantic_title": "maxileximin envy allocations and connected goods",
    "citation_count": 0,
    "authors": [
      "Gianluigi Greco",
      "Francesco Scarcello"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28830": {
    "title": "Information Design for Congestion Games with Unknown Demand",
    "volume": "main",
    "abstract": "We study a novel approach to information design in the standard traffic model of network congestion games. It captures the natural condition that the demand is unknown to the users of the network. A principal (e.g., a mobility service) commits to a signaling strategy, observes the realized demand and sends a (public) signal to agents (i.e., users of the network). Based on the induced belief about the demand, the users then form an equilibrium. We consider the algorithmic goal of the principal: Compute a signaling scheme that minimizes the expected total cost of the induced equilibrium. We concentrate on single-commodity networks and affine cost functions, for which we obtain the following results. First, we devise a fully polynomial-time approximation scheme (FPTAS) for the case that the demand can only take two values. It relies on several structural properties of the cost of the induced equilibrium as a function of the updated belief about the distribution of demands. We show that this function is piecewise linear for any number of demands, and monotonic for two demands. Second, we give a complete characterization of the graph structures for which it is optimal to fully reveal the information about the realized demand. This signaling scheme turns out to be optimal for all cost functions and probability distributions over demands if and only if the graph is series-parallel. Third, we propose an algorithm that computes the optimal signaling scheme for any number of demands whose time complexity is polynomial in the number of supports that occur in a Wardrop equilibrium for some demand. Finally, we conduct a computational study that tests this algorithm on real-world instances",
    "checked": true,
    "id": "65a484cb7893da64ad01ee2e8e68b181e0b03992",
    "semantic_title": "information design for congestion games with unknown demand",
    "citation_count": 0,
    "authors": [
      "Svenja M. Griesbach",
      "Martin Hoefer",
      "Max Klimm",
      "Tim Koglin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28831": {
    "title": "Zero-Sum Games between Mean-Field Teams: Reachability-Based Analysis under Mean-Field Sharing",
    "volume": "main",
    "abstract": "This work studies the behaviors of two large-population teams competing in a discrete environment. The team-level interactions are modeled as a zero-sum game while the agent dynamics within each team is formulated as a collaborative mean-field team problem. Drawing inspiration from the mean-field literature, we first approximate the large-population team game with its infinite-population limit. Subsequently, we construct a fictitious centralized system and transform the infinite-population game to an equivalent zero-sum game between two coordinators. Via a novel reachability analysis, we study the optimality of coordination strategies, which induce decentralized strategies under the original information structure. The optimality of the resulting strategies is established in the original finite-population game, and the theoretical guarantees are verified by numerical examples",
    "checked": true,
    "id": "bc1688b4f3e6c4e89603d8205c2d7875e0279ce4",
    "semantic_title": "zero-sum games between mean-field teams: reachability-based analysis under mean-field sharing",
    "citation_count": 2,
    "authors": [
      "Yue Guan",
      "Mohammad Afshari",
      "Panagiotis Tsiotras"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28832": {
    "title": "Worst-Case VCG Redistribution Mechanism Design Based on the Lottery Ticket Hypothesis",
    "volume": "main",
    "abstract": "We study worst-case VCG redistribution mechanism design for the public project problem. The mechanism design task comes down to designing a payment function that maximizes the worst-case allocative efficiency ratio. We use a multilayer perceptron (MLP) with ReLU activation to model the payment function and use mixed integer programming (MIP) to solve for the worst-case type profiles that maximally violate the mechanism design constraints. We collect these worst-case type profiles and use them as training samples to train toward better worst-case mechanisms. In practice, we require a tiny neural network structure for the above approach to scale. The Lottery Ticket Hypothesis states that a large network is likely to contain a \"winning ticket\" -- a much smaller subnetwork that \"won the initialization lottery\", which makes its training particularly effective. Motivated by this hypothesis, we train a large network and prune it into a tiny subnetwork. We run MIP-based worst-case training on the drawn subnetwork and evaluate the resulting mechanism's worst-case performance. If the subnetwork does not achieve good worst-case performance, then we record the type profiles that cause the current draw to be bad. To draw again, we restore the large network to its initial weights and prune using recorded type profiles from earlier draws, therefore avoiding drawing the same ticket twice. We expect to eventually encounter a tiny subnetwork that leads to effective training for our worst-case mechanism design task. Lastly, a by-product of multiple ticket draws is an ensemble of mechanisms with different worst cases, which improves the worst-case performance further. Using our approach, we find previously unknown optimal mechanisms for up to 5 agents. Our results confirm the tightness of existing theoretical upper bounds. For up to 20 agents, we derive significantly improved worst-case mechanisms, surpassing a long list of existing manual results",
    "checked": true,
    "id": "a0d5d2947759cb14f9ba8d8e711423c274e86842",
    "semantic_title": "worst-case vcg redistribution mechanism design based on the lottery ticket hypothesis",
    "citation_count": 2,
    "authors": [
      "Mingyu Guo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28833": {
    "title": "An Exercise in Tournament Design: When Some Matches Must Be Scheduled",
    "volume": "main",
    "abstract": "Single-elimination (SE) tournaments are a popular format used in competitive environments and decision making. Algorithms for SE tournament manipulation have been an active topic of research in recent years. In this paper, we initiate the algorithmic study of a novel variant of SE tournament manipulation that aims to model the fact that certain matchups are highly desired in a sporting context, incentivizing an organizer to manipulate the bracket to make such matchups take place. We obtain both hardness and tractability results. We show that while the problem of computing a bracket enforcing a given set of matches in an SE tournament is NP-hard, there are natural restrictions that lead to polynomial-time solvability. In particular, we show polynomial-time solvability if there is a linear ordering on the ability of players with only a constant number of exceptions where a player with lower ability beats a player with higher ability",
    "checked": true,
    "id": "f3a7dbf97b36eebd7cd0ab3a14cb910e17f60c43",
    "semantic_title": "an exercise in tournament design: when some matches must be scheduled",
    "citation_count": 0,
    "authors": [
      "Sushmita Gupta",
      "Ramanujan Sridharan",
      "Peter Strulo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28834": {
    "title": "Regret Analysis of Repeated Delegated Choice",
    "volume": "main",
    "abstract": "We present a study on a repeated delegated choice problem, which is the first to consider an online learning variant of Kleinberg and Kleinberg, EC'18. In this model, a principal interacts repeatedly with an agent who possesses an exogenous set of solutions to search for efficient ones. Each solution can yield varying utility for both the principal and the agent, and the agent may propose a solution to maximize its own utility in a selfish manner. To mitigate this behavior, the principal announces an eligible set which screens out a certain set of solutions. The principal, however, does not have any information on the distribution of solutions nor the number of solutions in advance. Therefore, the principal dynamically announces various eligible sets to efficiently learn the distribution. The principal's objective is to minimize cumulative regret compared to the optimal eligible set in hindsight. We explore two dimensions of the problem setup, whether the agent behaves myopically or strategizes across the rounds, and whether the solutions yield deterministic or stochastic utility. We obtain sublinear regret upper bounds in various regimes, and derive corresponding lower bounds which implies the tightness of the results. Overall, we bridge a well-known problem in economics to the evolving area of online learning, and present a comprehensive study in this problem",
    "checked": true,
    "id": "0daf7a304fea3ef99eedb586be7795b7d4ea3c58",
    "semantic_title": "regret analysis of repeated delegated choice",
    "citation_count": 1,
    "authors": [
      "Mohammad Hajiaghayi",
      "Mohammad Mahdavi",
      "Keivan Rezaei",
      "Suho Shin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28835": {
    "title": "Cost Minimization for Equilibrium Transition",
    "volume": "main",
    "abstract": "In this paper, we delve into the problem of using monetary incentives to encourage players to shift from an initial Nash equilibrium to a more favorable one within a game. Our main focus revolves around computing the minimum reward required to facilitate this equilibrium transition. The game involves a single row player who possesses m strategies and k column players, each endowed with n strategies. Our findings reveal that determining whether the minimum reward is zero is NP-complete, and computing the minimum reward becomes APX-hard. Nonetheless, we bring some positive news, as this problem can be efficiently handled if either k or n is a fixed constant. Furthermore, we have devised an approximation algorithm with an additive error that runs in polynomial time. Lastly, we explore a specific case wherein the utility functions exhibit single-peaked characteristics, and we successfully demonstrate that the optimal reward can be computed in polynomial time",
    "checked": true,
    "id": "abd369f20796ab908503003776fddec5da7dd254",
    "semantic_title": "cost minimization for equilibrium transition",
    "citation_count": 2,
    "authors": [
      "Haoqiang Huang",
      "Zihe Wang",
      "Zhide Wei",
      "Jie Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28836": {
    "title": "Reachability of Fair Allocations via Sequential Exchanges",
    "volume": "main",
    "abstract": "In the allocation of indivisible goods, a prominent fairness notion is envy-freeness up to one good (EF1). We initiate the study of reachability problems in fair division by investigating the problem of whether one EF1 allocation can be reached from another EF1 allocation via a sequence of exchanges such that every intermediate allocation is also EF1. We show that two EF1 allocations may not be reachable from each other even in the case of two agents, and deciding their reachability is PSPACE-complete in general. On the other hand, we prove that reachability is guaranteed for two agents with identical or binary utilities as well as for any number of agents with identical binary utilities. We also examine the complexity of deciding whether there is an EF1 exchange sequence that is optimal in the number of exchanges required",
    "checked": true,
    "id": "59e5e714991463eace13b5f07dc67630de057259",
    "semantic_title": "reachability of fair allocations via sequential exchanges",
    "citation_count": 0,
    "authors": [
      "Ayumi Igarashi",
      "Naoyuki Kamiyama",
      "Warut Suksompong",
      "Sheung Man Yuen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28837": {
    "title": "Repeated Fair Allocation of Indivisible Items",
    "volume": "main",
    "abstract": "The problem of fairly allocating a set of indivisible items is a well-known challenge in the field of (computational) social choice. In this scenario, there is a fundamental incompatibility between notions of fairness (such as envy-freeness and proportionality) and economic efficiency (such as Pareto-optimality). However, in the real world, items are not always allocated once and for all, but often repeatedly. For example, the items may be recurring chores to distribute in a household. Motivated by this, we initiate the study of the repeated fair division of indivisible goods and chores, and propose a formal model for this scenario. In this paper, we show that, if the number of repetitions is a multiple of the number of agents, there always exists a sequence of allocations that is proportional and Pareto-optimal. On the other hand, irrespective of the number of repetitions, an envy-free and Pareto-optimal sequence of allocations may not exist. For the case of two agents, we show that if the number of repetitions is even, it is always possible to find a sequence of allocations that is overall envy-free and Pareto-optimal. We then prove even stronger fairness guarantees, showing that every allocation in such a sequence satisfies some relaxation of envy-freeness. Finally, in case that the number of repetitions can be chosen freely, we show that envy-free and Pareto-optimal allocations are achievable for any number of agents",
    "checked": true,
    "id": "301eb0bf497bbc9d9ec0ba1c4e0c23a3ac6d3d90",
    "semantic_title": "repeated fair allocation of indivisible items",
    "citation_count": 3,
    "authors": [
      "Ayumi Igarashi",
      "Martin Lackner",
      "Oliviero Nardi",
      "Arianna Novaro"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28838": {
    "title": "Spatial Voting with Incomplete Voter Information",
    "volume": "main",
    "abstract": "We consider spatial voting where candidates are located in the Euclidean d-dimensional space, and each voter ranks candidates based on their distance from the voter's ideal point. We explore the case where information about the location of voters' ideal points is incomplete: for each dimension, we are given an interval of possible values. We study the computational complexity of finding the possible and necessary winners for positional scoring rules. Our results show that we retain tractable cases of the classic model where voters have partial-order preferences. Moreover, we show that there are positional scoring rules under which the possible-winner problem is intractable for partial orders, but tractable in the one-dimensional spatial setting. We also consider approval voting in this setting. We show that for up to two dimensions, the necessary-winner problem is tractable, while the possible-winner problem is hard for any number of dimensions",
    "checked": true,
    "id": "7122046f4791de981fccfc891302810da1f7506b",
    "semantic_title": "spatial voting with incomplete voter information",
    "citation_count": 1,
    "authors": [
      "Aviram Imber",
      "Jonas Israel",
      "Markus Brill",
      "Hadas Shachnai",
      "Benny Kimelfeld"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28839": {
    "title": "Maximizing Nash Social Welfare under Two-Sided Preferences",
    "volume": "main",
    "abstract": "The maximum Nash social welfare (NSW)---which maximizes the geometric mean of agents' utilities---is a fundamental solution concept with remarkable fairness and efficiency guarantees. The computational aspects of NSW have been extensively studied for *one-sided* preferences where a set of agents have preferences over a set of resources. Our work deviates from this trend and studies NSW maximization for *two-sided* preferences, wherein a set of workers and firms, each having a cardinal valuation function, are matched with each other. We provide a systematic study of the computational complexity of maximizing NSW for many-to-one matchings under two-sided preferences. Our main negative result is that maximizing NSW is NP-hard even in a highly restricted setting where each firm has capacity 2, all valuations are in the range {0,1,2}, and each agent positively values at most three other agents. In search of positive results, we develop approximation algorithms as well as parameterized algorithms in terms of natural parameters such as the number of workers, the number of firms, and the firms' capacities. We also provide algorithms for restricted domains such as symmetric binary valuations and bounded degree instances",
    "checked": true,
    "id": "41590e2d7710136b51ea213a67ebbe7180ccfbc1",
    "semantic_title": "maximizing nash social welfare under two-sided preferences",
    "citation_count": 2,
    "authors": [
      "Pallavi Jain",
      "Rohit Vaish"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28840": {
    "title": "Optimal Mechanism in a Dynamic Stochastic Knapsack Environment",
    "volume": "main",
    "abstract": "This study introduces an optimal mechanism in a dynamic stochastic knapsack environment. The model features a single seller who has a fixed quantity of a perfectly divisible item. Impatient buyers with a piece-wise linear utility function arrive randomly and they report the two-dimensional private information: marginal value and demanded quantity. We derive a revenue-maximizing dynamic mechanism in a finite discrete time framework that satisfies incentive compatibility, individual rationality, and feasibility conditions. This is achieved by characterizing buyers' utility and utilizing the Bellman equation. Moreover, we establish the essential penalty scheme for incentive compatibility, as well as the allocation and payment policies. Lastly, we propose algorithms to approximate the optimal policy, based on the Monte Carlo simulation-based regression method and reinforcement learning",
    "checked": true,
    "id": "f4f3d785126008a98c025212da0ec22385cbeab7",
    "semantic_title": "optimal mechanism in a dynamic stochastic knapsack environment",
    "citation_count": 0,
    "authors": [
      "Jihyeok Jung",
      "Chan-Oi Song",
      "Deok-Joo Lee",
      "Kiho Yoon"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28841": {
    "title": "Proportional Representation in Metric Spaces and Low-Distortion Committee Selection",
    "volume": "main",
    "abstract": "We introduce a novel definition for a small set R of k points being \"representative\" of a larger set in a metric space. Given a set V (e.g., documents or voters) to represent, and a set C of possible representatives, our criterion requires that for any subset S comprising a theta fraction of V, the average distance of S to their best theta*k points in R should not be more than a factor gamma compared to their average distance to the best theta*k points among all of C. This definition is a strengthening of proportional fairness and core fairness, but - different from those notions - requires that large cohesive clusters be represented proportionally to their size. Since there are instances for which - unless gamma is polynomially large - no solutions exist, we study this notion in a resource augmentation framework, implicitly stating the constraints for a set R of size k as though its size were only k/alpha, for alpha > 1. Furthermore, motivated by the application to elections, we mostly focus on the \"ordinal\" model, where the algorithm does not learn the actual distances; instead, it learns only for each point v in V and each candidate pairs c, c' which of c, c' is closer to v. Our main result is that the Expanding Approvals Rule (EAR) of Aziz and Lee is (alpha, gamma) representative with gamma <= 1 + 6.71 * (alpha)/(alpha-1). Our results lead to three notable byproducts. First, we show that the EAR achieves constant proportional fairness in the ordinal model, giving the first positive result on metric proportional fairness with ordinal information. Second, we show that for the core fairness objective, the EAR achieves the same asymptotic tradeoff between resource augmentation and approximation as the recent results of Li et al., which used full knowledge of the metric. Finally, our results imply a very simple single-winner voting rule with metric distortion at most 44",
    "checked": true,
    "id": "b639f5e3f1509abcae5fc2e56ff39a97f5db959c",
    "semantic_title": "proportional representation in metric spaces and low-distortion committee selection",
    "citation_count": 6,
    "authors": [
      "Yusuf Kalayci",
      "David Kempe",
      "Vikram Kher"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28842": {
    "title": "Towards Optimal Subsidy Bounds for Envy-Freeable Allocations",
    "volume": "main",
    "abstract": "We study the fair division of indivisible items with subsidies among n agents, where the absolute marginal valuation of each item is at most one. Under monotone valuations (where each item is a good), it is known that a maximum subsidy of 2(n-1) and a total subsidy of 2(n-1)² are sufficient to guarantee the existence of an envy-freeable allocation. In this paper, we improve upon these bounds, even in a wider model. Namely, we show that, given an EF1 allocation, we can compute in polynomial time an envy-free allocation with a subsidy of at most n-1 per agent and a total subsidy of at most n(n-1)/2. Moreover, we present further improved bounds for monotone valuations",
    "checked": true,
    "id": "bf5f419c2fa1c8adda285b2be89bace7738f7ad2",
    "semantic_title": "towards optimal subsidy bounds for envy-freeable allocations",
    "citation_count": 2,
    "authors": [
      "Yasushi Kawase",
      "Kazuhisa Makino",
      "Hanna Sumita",
      "Akihisa Tamura",
      "Makoto Yokoo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28843": {
    "title": "Strategyproof Mechanisms for Group-Fair Obnoxious Facility Location Problems",
    "volume": "main",
    "abstract": "We study the group-fair obnoxious facility location problems from the mechanism design perspective where agents belong to different groups and have private location preferences on the undesirable locations of the facility. Our main goal is to design strategyproof mechanisms that elicit the true location preferences from the agents and determine a facility location that approximately optimizes several group-fair objectives. We first consider the maximum total and average group cost (group-fair) objectives. For these objectives, we propose deterministic mechanisms that achieve 3-approximation ratios and provide matching lower bounds. We then provide the characterization of 2-candidate strategyproof randomized mechanisms. Leveraging the characterization, we design randomized mechanisms with improved approximation ratios of 2 for both objectives. We also provide randomized lower bounds of 5/4 for both objectives. Moreover, we investigate intergroup and intragroup fairness (IIF) objectives, addressing fairness between groups and within each group. We present a mechanism that achieves a 4-approximation for the IIF objectives and provide tight lower bounds",
    "checked": true,
    "id": "d15b810a0f7e7aca05969b586d7e47af2b3b46b0",
    "semantic_title": "strategyproof mechanisms for group-fair obnoxious facility location problems",
    "citation_count": 0,
    "authors": [
      "Jiaqian Li",
      "Minming Li",
      "Hau Chan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28844": {
    "title": "Opponent-Model Search in Games with Incomplete Information",
    "volume": "main",
    "abstract": "Games with incomplete information are games that model situations where players do not have common knowledge about the game they play, e.g. card games such as poker or bridge. Opponent models can be of crucial importance for decision-making in such games. We propose algorithms for computing optimal and/or robust strategies in games with incomplete information, given various types of knowledge about opponent models. As an application, we describe a framework for reasoning about an opponent's reasoning in such games, where opponent models arise naturally",
    "checked": true,
    "id": "c9a4c7572b319fb91707b09fa393592b8910c3ef",
    "semantic_title": "opponent-model search in games with incomplete information",
    "citation_count": 0,
    "authors": [
      "Junkang Li",
      "Bruno Zanuttini",
      "Véronique Ventos"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28845": {
    "title": "Double Auction on Diffusion Network",
    "volume": "main",
    "abstract": "Mechanism design on social networks has attracted extensive attention recently. The goal is to design mechanisms to incentivize participants to invite more participants via their social networks, and the challenge is that the participants are competitors. Various mechanisms have been proposed for single-/multiple-unit auctions, but it has been shown that it is challenging to design such mechanisms for more complex settings. We move this forward to investigate a double auction on a network where each trader (a buyer or a seller) can link to other buyers and sellers. Incentiving invitation is more difficult than in multi-unit one-sided auctions, because there are two different roles and a buyer (seller) seems happy to invite a seller (buyer), but again the invited seller (buyer) may invite another buyer (seller) to compete with the original buyer (seller). To combat this, we propose a solution called dynamic trade reduction (DTR), which also guarantees a non-negative revenue for the market owner. Interestingly, our solution is also applicable to the multi-unit one-sided auction when there is only one seller linking to only buyers on the network. We believe that the principle of our solution has the potential to be extended to design the multi-item one-sided auction",
    "checked": true,
    "id": "e060af5500a33e6c9457909fed1557d8c5565979",
    "semantic_title": "double auction on diffusion network",
    "citation_count": 0,
    "authors": [
      "Miao Li",
      "Yuhan Cao",
      "Dengji Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28846": {
    "title": "Pay to (Not) Play: Monetizing Impatience in Mobile Games",
    "volume": "main",
    "abstract": "Mobile gaming is a rapidly growing and incredibly profitable sector; having grown seven-fold over the past 10 years, it now grosses over $100 billion annually. This growth was due in large part to a shift in monetization strategies: rather than charging players an upfront cost (\"pay-to-play\"), games often request optional microtransactions throughout gameplay (\"free-to-play\"). We focus on a common scenario in which games include wait times---gating either items or game progression---that players can pay to skip. Game designers typically say that they optimize for player happiness rather than revenue; however, prices for skips are typically set at levels that few players are willing to pay, leading to low purchase rates. Under a traditional analysis, it would seem that game designers fail at their stated goal if few players buy what they are selling. We argue that an alternate model can better explain this dynamic: players value tasks more highly as they are perceived to be more difficult. While skips can increase players' utilities by providing instant gratification, pricing skips too cheaply can lower players' utilities by decreasing the perceived amount of work needed to complete a task. We show that high revenue, high player utility, and low purchase rates can all coexist under this model, particularly under a realistic distribution of players having few buyers but a few big-spending \"whales.\" We also investigate how a game designer should optimize prices under our model. An appendix of the paper with proofs, more comprehensive results and visualizations can be found at https://arxiv.org/abs/2312.10205",
    "checked": true,
    "id": "1089d45db53b81741eefc7219a8cffd5ed31fd8d",
    "semantic_title": "pay to (not) play: monetizing impatience in mobile games",
    "citation_count": 1,
    "authors": [
      "Taylor Lundy",
      "Narun Raman",
      "Hu Fu",
      "Kevin Leyton-Brown"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28847": {
    "title": "Weighted Envy-Freeness for Submodular Valuations",
    "volume": "main",
    "abstract": "We investigate the fair allocation of indivisible goods to agents with possibly different entitlements represented by weights. Previous work has shown that guarantees for additive valuations with existing envy-based notions cannot be extended to the case where agents have matroid-rank (i.e., binary submodular) valuations. We propose two families of envy-based notions for matroid-rank and general submodular valuations, one based on the idea of transferability and the other on marginal values. We show that our notions can be satisfied via generalizations of rules such as picking sequences and maximum weighted Nash welfare. In addition, we introduce welfare measures based on harmonic numbers, and show that variants of maximum weighted harmonic welfare offer stronger fairness guarantees than maximum weighted Nash welfare under matroid-rank valuations",
    "checked": true,
    "id": "416f8066d577703d3ffbbafe6b9edf5e9efd87f5",
    "semantic_title": "weighted envy-freeness for submodular valuations",
    "citation_count": 9,
    "authors": [
      "Luisa Montanari",
      "Ulrike Schmidt-Kraepelin",
      "Warut Suksompong",
      "Nicholas Teh"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28848": {
    "title": "Computing Nash Equilibria in Potential Games with Private Uncoupled Constraints",
    "volume": "main",
    "abstract": "We consider the problem of computing Nash equilibria in potential games where each player's strategy set is subject to private uncoupled constraints. This scenario is frequently encountered in real-world applications like road network congestion games where individual drivers adhere to personal budget and fuel limitations. Despite the plethora of algorithms that efficiently compute Nash equilibria (NE) in potential games, the domain of constrained potential games remains largely unexplored. We introduce an algorithm that leverages the Lagrangian formulation of NE. The algorithm is implemented independently by each player and runs in polynomial time with respect to the approximation error, the sum of the size of the action-spaces, and the game's inherit parameters",
    "checked": true,
    "id": "c79ad31b809a431fdc3249ad5cdb4a4a344b9272",
    "semantic_title": "computing nash equilibria in potential games with private uncoupled constraints",
    "citation_count": 0,
    "authors": [
      "Nikolas Patris",
      "Stelios Stavroulakis",
      "Fivos Kalogiannis",
      "Rose Zhang",
      "Ioannis Panageas"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28849": {
    "title": "Peer Neighborhood Mechanisms: A Framework for Mechanism Generalization",
    "volume": "main",
    "abstract": "Peer prediction incentive mechanisms for crowdsourcing are generally limited to eliciting samples from categorical distributions. Prior work on extending peer prediction to arbitrary distributions has largely relied on assumptions on the structures of the distributions or known properties of the data providers. We introduce a novel class of incentive mechanisms that extend peer prediction mechanisms to arbitrary distributions by replacing the notion of an exact match with a concept of neighborhood matching. We present conditions on the belief updates of the data providers that guarantee incentive-compatibility for rational data providers, and admit a broad class of possible reasonable updates",
    "checked": true,
    "id": "681f8762d026080412115048d36b56a8577603fe",
    "semantic_title": "peer neighborhood mechanisms: a framework for mechanism generalization",
    "citation_count": 1,
    "authors": [
      "Adam Richardson",
      "Boi Faltings"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28850": {
    "title": "Machine Learning-Powered Combinatorial Clock Auction",
    "volume": "main",
    "abstract": "We study the design of iterative combinatorial auctions (ICAs). The main challenge in this domain is that the bundle space grows exponentially in the number of items. To address this, several papers have recently proposed machine learning (ML)-based preference elicitation algorithms that aim to elicit only the most important information from bidders. However, from a practical point of view, the main shortcoming of this prior work is that those designs elicit bidders' preferences via value queries (i.e., \"What is your value for the bundle {A, B}?''). In most real-world ICA domains, value queries are considered impractical, since they impose an unrealistically high cognitive burden on bidders, which is why they are not used in practice. In this paper, we address this shortcoming by designing an ML-powered combinatorial clock auction that elicits information from the bidders only via demand queries (i.e., \"At prices p, what is your most preferred bundle of items?''). We make two key technical contributions: First, we present a novel method for training an ML model on demand queries. Second, based on those trained ML models, we introduce an efficient method for determining the demand query with the highest clearing potential, for which we also provide a theoretical foundation. We experimentally evaluate our ML-based demand query mechanism in several spectrum auction domains and compare it against the most established real-world ICA: the combinatorial clock auction (CCA). Our mechanism significantly outperforms the CCA in terms of efficiency in all domains, it achieves higher efficiency in a significantly reduced number of rounds, and, using linear prices, it exhibits vastly higher clearing potential. Thus, with this paper we bridge the gap between research and practice and propose the first practical ML-powered ICA",
    "checked": true,
    "id": "cb28d77c3fcd16c419b1384a0f29d25f570c2e81",
    "semantic_title": "machine learning-powered combinatorial clock auction",
    "citation_count": 0,
    "authors": [
      "Ermis Nikiforos Soumalias",
      "Jakob Weissteiner",
      "Jakob Heiss",
      "Sven Seuken"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28851": {
    "title": "Almost Envy-Free Allocations of Indivisible Goods or Chores with Entitlements",
    "volume": "main",
    "abstract": "We here address the problem of fairly allocating indivisible goods or chores to n agents with weights that define their entitlement to the set of indivisible resources. Stemming from well-studied fairness concepts such as envy-freeness up to one good (EF1) and envy-freeness up to any good (EFX) for agents with equal entitlements, we present, in this study, the first set of impossibility results alongside algorithmic guarantees for fairness among agents with unequal entitlements. Within this paper, we expand the concept of envy-freeness up to any good or chore to the weighted context (WEFX and XWEF respectively), demonstrating that these allocations are not guaranteed to exist for two or three agents. Despite these negative results, we develop a WEFX procedure for two agents with integer weights, and furthermore, we devise an approximate WEFX procedure for two agents with normalized weights. We further present a polynomial-time algorithm that guarantees a weighted envy-free allocation up to one chore (1WEF) for any number of agents with additive cost functions. Our work underscores the heightened complexity of the weighted fair division problem when compared to its unweighted counterpart",
    "checked": true,
    "id": "b1a1905439c9958389f68aa22d982d65dc56e196",
    "semantic_title": "almost envy-free allocations of indivisible goods or chores with entitlements",
    "citation_count": 3,
    "authors": [
      "Max Springer",
      "MohammadTaghi Hajiaghayi",
      "Hadi Yami"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28852": {
    "title": "The Moderating Effect of Instant Runoff Voting",
    "volume": "main",
    "abstract": "Instant runoff voting (IRV) has recently gained popularity as an alternative to plurality voting for political elections, with advocates claiming a range of advantages, including that it produces more moderate winners than plurality and could thus help address polarization. However, there is little theoretical backing for this claim, with existing evidence focused on case studies and simulations. In this work, we prove that IRV has a moderating effect relative to plurality voting in a precise sense, developed in a 1-dimensional Euclidean model of voter preferences. We develop a theory of exclusion zones, derived from properties of the voter distribution, which serve to show how moderate and extreme candidates interact during IRV vote tabulation. The theory allows us to prove that if voters are symmetrically distributed and not too concentrated at the extremes, IRV cannot elect an extreme candidate over a moderate. In contrast, we show plurality can and validate our results computationally. Our methods provide new frameworks for the analysis of voting systems, deriving exact winner distributions geometrically and establishing a connection between plurality voting and stick-breaking processes",
    "checked": true,
    "id": "66519cc6db750a518afd9876f57fa4d68f2bf2b1",
    "semantic_title": "the moderating effect of instant runoff voting",
    "citation_count": 0,
    "authors": [
      "Kiran Tomlinson",
      "Johan Ugander",
      "Jon Kleinberg"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28853": {
    "title": "Unravelling Expressive Delegations: Complexity and Normative Analysis",
    "volume": "main",
    "abstract": "We consider binary group decision-making under a rich model of liquid democracy: agents submit ranked delegation options, where each option may be a function of multiple agents' votes; e.g., \"I vote yes if a majority of my friends vote yes.\" Such ballots are unravelled into a profile of direct votes by selecting one entry from each ballot so as not to introduce cyclic dependencies. We study delegation via monotonic Boolean functions, and two unravelling procedures: MinSum, which minimises the sum of the ranks of the chosen entries, and its egalitarian counterpart, MinMax. We provide complete computational dichotomies: MinSum is hard to compute (and approximate) as soon as any non-trivial functions are permitted, and polynomial otherwise; for MinMax the easiness results extend to arbitrary-arity logical ORs and ANDs taken in isolation, but not beyond. For the classic model of delegating to individual agents, we give asymptotically near-tight algorithms for carrying out the two procedures and efficient algorithms for finding optimal unravellings with the highest vote count for a given alternative. These algorithms inspire novel tie-breaking rules for the setup of voting to change a status quo. We then introduce a new axiom, which can be viewed as a variant of the participation axiom, and use algorithmic techniques developed earlier in the paper to show that it is satisfied by MinSum and a lexicographic refinement of MinMax (but not MinMax itself)",
    "checked": true,
    "id": "8fceb1ec58d2e63e4f64b75ad2f278e2d670bd01",
    "semantic_title": "unravelling expressive delegations: complexity and normative analysis",
    "citation_count": 1,
    "authors": [
      "Giannis Tyrovolas",
      "Andrei Constantinescu",
      "Edith Elkind"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28854": {
    "title": "Predicting Real-World Penny Auction Durations by Integrating Game Theory and Machine Learning",
    "volume": "main",
    "abstract": "Game theory and machine learning are two widely used techniques for predicting the outcomes of strategic interactions among humans. However, the game theory-based approach often relies on strong rationality and informational assumptions, while the machine learning-based approach typically requires the testing data to come from the same distribution as the training data. Our work studies how to integrate the two techniques to address these weaknesses. We focus on the interactions among real bidders in penny auctions, and develop a three-stage framework to predict the distributions of auction durations, which indicate the numbers of bids and auctioneer revenues. Specifically, we first leverage a pre-trained neural network to encode the descriptions of products in auctions into embeddings. Second, we apply game theory models to make preliminary predictions of auction durations. In particular, we tackle the challenge of accurately inferring parameters in game theory models. Third, we develop a Multi-Branch Mixture Density Network to learn the mapping from product embeddings and game-theoretic predictions to the distributions of actual auction durations. Experiments on real-world penny auction data demonstrate that our framework outperforms both game theory-based and machine learning-based prediction approaches",
    "checked": true,
    "id": "a41239044aee77a50ace7d852bd2eb3ac1b12e8e",
    "semantic_title": "predicting real-world penny auction durations by integrating game theory and machine learning",
    "citation_count": 0,
    "authors": [
      "Yujia Wang",
      "Haoran Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28855": {
    "title": "Simultaneous Optimization of Bid Shading and Internal Auction for Demand-Side Platforms",
    "volume": "main",
    "abstract": "Online advertising has been one of the most important sources for industry's growth, where the demand-side platforms (DSP) play an important role via bidding to the ad exchanges on behalf of their advertiser clients. Since more and more ad exchanges have shifted from second to first price auctions, it is challenging for DSPs to adjust bidding strategy in the volatile environment. Recent studies on bid shading in first-price auctions may have limited performance due to relatively strong hypotheses about winning probability distribution. Moreover, these studies do not consider the incentive of advertiser clients, which can be crucial for a reliable advertising platform. In this work, we consider both the optimization of bid shading technique and the design of internal auction which is ex-post incentive compatible (IC) for the management of a DSP. Firstly, we prove that the joint design of bid shading and ex-post IC auction can be reduced to choosing one monotone bid function for each advertiser without loss of optimality. Then we propose a parameterized neural network to implement the monotone bid functions. With well-designed surrogate loss, the objective can be optimized in an end-to-end manner. Finally, our experimental results demonstrate the effectiveness and superiority of our algorithm",
    "checked": true,
    "id": "2c214d88d3d8b523dee02983f368a3db4dc276f4",
    "semantic_title": "simultaneous optimization of bid shading and internal auction for demand-side platforms",
    "citation_count": 0,
    "authors": [
      "Yadong Xu",
      "Bonan Ni",
      "Weiran Shen",
      "Xun Wang",
      "Zichen Wang",
      "Yinsong Xue",
      "Pingzhong Tang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28856": {
    "title": "Learning Coalition Structures with Games",
    "volume": "main",
    "abstract": "Coalitions naturally exist in many real-world systems involving multiple decision makers such as ridesharing, security, and online ad auctions, but the coalition structure among the agents is often unknown. We propose and study an important yet previously overseen problem -- Coalition Structure Learning (CSL), where we aim to carefully design a series of games for the agents and infer the underlying coalition structure by observing their interactions in those games. We establish a lower bound on the sample complexity -- defined as the number of games needed to learn the structure -- of any algorithms for CSL and propose the Iterative Grouping (IG) algorithm for designing normal-form games to achieve the lower bound. We show that IG can be extended to other succinct games such as congestion games and graphical games. Moreover, we solve CSL in a more restrictive and practical setting: auctions. We show a variant of IG to solve CSL in the auction setting even if we cannot design the bidder valuations. Finally, we conduct experiments to evaluate IG in the auction setting and the results align with our theoretical analysis",
    "checked": true,
    "id": "1aa7ab971ec5f3ac08c889637946233ad2d4d31d",
    "semantic_title": "learning coalition structures with games",
    "citation_count": 0,
    "authors": [
      "Yixuan Even Xu",
      "Chun Kai Ling",
      "Fei Fang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28857": {
    "title": "Non-excludable Bilateral Trade between Groups",
    "volume": "main",
    "abstract": "Bilateral trade is one of the most natural and important forms of economic interaction: A seller has a single, indivisible item for sale, and a buyer is potentially interested. The two parties typically have different, privately known valuations for the item, and ideally, they would like to trade if the buyer values the item more than the seller. The celebrated impossibility result by Myerson and Satterthwaite shows that any mechanism for this setting must violate at least one important desideratum. In this paper, we investigate a richer paradigm of bilateral trade, with many self-interested buyers and sellers on both sides of a single trade who cannot be excluded from the trade. We show that this allows for more positive results. In fact, we establish a dichotomy in the possibility of trading efficiently. If in expectation, the buyers value the item more, we can achieve efficiency in the limit. If this is not the case, then efficiency cannot be achieved in general. En route, we characterize trading mechanisms that encourage truth-telling, which may be of independent interest. We also evaluate our trading mechanisms experimentally, and the experiments align with our theoretical results",
    "checked": true,
    "id": "eae4fdf757d5735ae963b0b8e662b136d43f8112",
    "semantic_title": "non-excludable bilateral trade between groups",
    "citation_count": 0,
    "authors": [
      "Yixuan Even Xu",
      "Hanrui Zhang",
      "Vincent Conitzer"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28858": {
    "title": "Greedy-Based Online Fair Allocation with Adversarial Input: Enabling Best-of-Many-Worlds Guarantees",
    "volume": "main",
    "abstract": "We study an online allocation problem with sequentially arriving items and adversarially chosen agent values, with the goal of balancing fairness and efficiency. Our goal is to study the performance of algorithms that achieve strong guarantees under other input models such as stochastic inputs, in order to achieve robust guarantees against a variety of inputs. To that end, we study the PACE (Pacing According to Current Estimated utility) algorithm, an existing algorithm designed for stochastic input. We show that in the equal-budgets case, PACE is equivalent to an integral greedy algorithm. We go on to show that with natural restrictions on the adversarial input model, both the greedy allocation and PACE have asymptotically bounded multiplicative envy as well as competitive ratio for Nash welfare, with the multiplicative factors either constant or with optimal order dependence on the number of agents. This completes a \"best-of-many-worlds\" guarantee for PACE, since past work showed that PACE achieves guarantees for stationary and stochastic-but-non-stationary input models",
    "checked": true,
    "id": "4fcb5c47f7a372848535708f74daa49b984f2dec",
    "semantic_title": "greedy-based online fair allocation with adversarial input: enabling best-of-many-worlds guarantees",
    "citation_count": 1,
    "authors": [
      "Zongjun Yang",
      "Luofeng Liao",
      "Christian Kroer"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28859": {
    "title": "On the Outcome Equivalence of Extensive-Form and Behavioral Correlated Equilibria",
    "volume": "main",
    "abstract": "We investigate two notions of correlated equilibrium for extensive-form games: the extensive-form correlated equilibrium (EFCE) and the behavioral correlated equilibrium (BCE). We show that the two are outcome-equivalent, in the sense that every outcome distribution achievable under one notion is achievable under the other. Our result implies, to our knowledge, the first polynomial-time algorithm for computing a BCE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brian Hu Zhang",
      "Tuomas Sandholm"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28860": {
    "title": "Eliciting Honest Information from Authors Using Sequential Review",
    "volume": "main",
    "abstract": "In the setting of conference peer review, the conference aims to accept high-quality papers and reject low-quality papers based on noisy review scores. A recent work proposes the isotonic mechanism, which can elicit the ranking of paper qualities from an author with multiple submissions to help improve the conference's decisions. However, the isotonic mechanism relies on the assumption that the author's utility is both an increasing and a convex function with respect to the review score, which is often violated in realistic settings (e.g.~when authors aim to maximize the number of accepted papers). In this paper, we propose a sequential review mechanism that can truthfully elicit the ranking information from authors while only assuming the agent's utility is increasing with respect to the true quality of her accepted papers. The key idea is to review the papers of an author in a sequence based on the provided ranking and conditioning the review of the next paper on the review scores of the previous papers. Advantages of the sequential review mechanism include: 1) eliciting truthful ranking information in a more realistic setting than prior work; 2) reducing the reviewing workload and increasing the average quality of papers being reviewed; 3) incentivizing authors to write fewer papers of higher quality",
    "checked": true,
    "id": "6cce9883d9c60735d2f361aa88d9b627dfa85840",
    "semantic_title": "eliciting honest information from authors using sequential review",
    "citation_count": 0,
    "authors": [
      "Yichi Zhang",
      "Grant Schoenebeck",
      "Weijie Su"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28861": {
    "title": "Fair Allocation of Items in Multiple Regions",
    "volume": "main",
    "abstract": "We initiate the study of fair allocation with the set of divisible or indivisible items distributed in multiple regions. The key requirement is that each agent can only obtain items from one region. In this work, we consider two kinds of fairness concepts: envy-based notions including envy-freeness (EF) and envy-freeness up to one/any item (EF1/EFX), and share-based notions including proportionality (PROP) and proportionality up to one/any item (PROP1/PROPX). On the negative side, we show NP-hardness and inapproximability results about the aforementioned fairness notions. On the positive side, we propose several algorithms to compute the partial allocations that satisfy envy-based notions and allocations that approximate the above fairness notions",
    "checked": true,
    "id": "4e07c5ed2974efdfefb800d995777e066e6544f5",
    "semantic_title": "fair allocation of items in multiple regions",
    "citation_count": 0,
    "authors": [
      "Houyu Zhou",
      "Tianze Wei",
      "Biaoshuai Tao",
      "Minming Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28862": {
    "title": "Altruism in Facility Location Problems",
    "volume": "main",
    "abstract": "We study the facility location problems (FLPs) with altruistic agents who act to benefit others in their affiliated groups. Our aim is to design mechanisms that elicit true locations from the agents in different overlapping groups and place a facility to serve agents to approximately optimize a given objective based on agents' costs to the facility. Existing studies of FLPs consider myopic agents who aim to minimize their own costs to the facility. We mainly consider altruistic agents with well-motivated group costs that are defined over costs incurred by all agents in their groups. Accordingly, we define Pareto strategyproofness to account for altruistic agents and their multiple group memberships with incomparable group costs. We consider mechanisms satisfying this strategyproofness under various combinations of the planner's objectives and agents' group costs. For each of these settings, we provide upper and lower bounds of approximation ratios of the mechanisms satisfying Pareto strategyproofness",
    "checked": true,
    "id": "9111f49f9db435dea200dd1a8534ad11a13e9434",
    "semantic_title": "altruism in facility location problems",
    "citation_count": 1,
    "authors": [
      "Houyu Zhou",
      "Hau Chan",
      "Minming Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28863": {
    "title": "Explaining Reinforcement Learning Agents through Counterfactual Action Outcomes",
    "volume": "main",
    "abstract": "Explainable reinforcement learning (XRL) methods aim to help elucidate agent policies and decision-making processes. The majority of XRL approaches focus on local explanations, seeking to shed light on the reasons an agent acts the way it does at a specific world state. While such explanations are both useful and necessary, they typically do not portray the outcomes of the agent's selected choice of action. In this work, we propose ``COViz'', a new local explanation method that visually compares the outcome of an agent's chosen action to a counterfactual one. In contrast to most local explanations that provide state-limited observations of the agent's motivation, our method depicts alternative trajectories the agent could have taken from the given state and their outcomes. We evaluated the usefulness of COViz in supporting people's understanding of agents' preferences and compare it with reward decomposition, a local explanation method that describes an agent's expected utility for different actions by decomposing it into meaningful reward types. Furthermore, we examine the complementary benefits of integrating both methods. Our results show that such integration significantly improved participants' performance",
    "checked": true,
    "id": "66766682c5669964ec6405e0acd9ca228024c5ab",
    "semantic_title": "explaining reinforcement learning agents through counterfactual action outcomes",
    "citation_count": 2,
    "authors": [
      "Yotam Amitai",
      "Yael Septon",
      "Ofra Amir"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28864": {
    "title": "Data-Driven Knowledge-Aware Inference of Private Information in Continuous Double Auctions",
    "volume": "main",
    "abstract": "Inferring the private information of humans from their strategic behavioral data is crucial and challenging. The main approach is first obtaining human behavior functions (which map public information and human private information to behavior), enabling subsequent inference of private information from observed behavior. Most existing studies rely on strong equilibrium assumptions to obtain behavior functions. Our work focuses on continuous double auctions, where multiple traders with heterogeneous rationalities and beliefs dynamically trade commodities and deriving equilibria is generally intractable. We develop a knowledge-aware machine learning-based framework to infer each trader's private cost vectors for producing different units of its commodity. Our key idea is to learn behavior functions by incorporating the statistical knowledge about private costs given the observed trader asking behavior across the population. Specifically, we first use a neural network to characterize each trader's behavior function. Second, we leverage the statistical knowledge to derive the posterior distribution of each trader's private costs given its observed asks. Third, through designing a novel loss function, we utilize the knowledge-based posterior distributions to guide the learning of the neural network. We conduct extensive experiments on a large experimental dataset, and demonstrate the superior performance of our framework over baselines in inferring the private information of humans",
    "checked": true,
    "id": "81b3841e3fd57127b6656b0dc839d3c5fa300604",
    "semantic_title": "data-driven knowledge-aware inference of private information in continuous double auctions",
    "citation_count": 0,
    "authors": [
      "Lvye Cui",
      "Haoran Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28865": {
    "title": "Procedural Level Generation with Diffusion Models from a Single Example",
    "volume": "main",
    "abstract": "Level generation is a central focus of Procedural Content Generation (PCG), yet deep learning-based approaches are limited by scarce training data, i.e., human-designed levels. Despite being a dominant framework, Generative Adversarial Networks (GANs) exhibit a substantial quality gap between generated and human-authored levels, alongside rising training costs, particularly with increasing token complexity. In this paper, we introduce a diffusion-based generative model that learns from just one example. Our approach involves two core components: 1) an efficient yet expressive level representation, and 2) a latent denoising network with constrained receptive fields. To start with, our method utilizes token semantic labels, similar to word embeddings, to provide dense representations. This strategy not only surpasses one-hot encoding in representing larger game levels but also improves stability and accelerates convergence in latent diffusion. In addition, we adapt the denoising network architecture to confine the receptive field to localized patches of the data, aiming to facilitate single-example learning. Extensive experiments demonstrate that our model is capable of generating stylistically congruent samples of arbitrary sizes compared to manually designed levels. It suits a wide range of level structures with fewer artifacts than GAN-based approaches. The source code is available at https://github.com/shiqi-dai/diffusioncraft",
    "checked": true,
    "id": "8994311e4e80d9981ca4236132a6796bbbfe73ae",
    "semantic_title": "procedural level generation with diffusion models from a single example",
    "citation_count": 1,
    "authors": [
      "Shiqi Dai",
      "Xuanyu Zhu",
      "Naiqi Li",
      "Tao Dai",
      "Zhi Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28866": {
    "title": "When Are Two Lists Better than One?: Benefits and Harms in Joint Decision-Making",
    "volume": "main",
    "abstract": "Historically, much of machine learning research has focused on the performance of the algorithm alone, but recently more attention has been focused on optimizing joint human-algorithm performance. Here, we analyze a specific type of human-algorithm collaboration where the algorithm has access to a set of n items, and presents a subset of size k to the human, who selects a final item from among those k. This scenario could model content recommendation, route planning, or any type of labeling task. Because both the human and algorithm have imperfect, noisy information about the true ordering of items, the key question is: which value of k maximizes the probability that the best item will be ultimately selected? For k=1, performance is optimized by the algorithm acting alone, and for k=n it is optimized by the human acting alone. Surprisingly, we show that for multiple of noise models, it is optimal to set k in [2, n-1] - that is, there are strict benefits to collaborating, even when the human and algorithm have equal accuracy separately. We demonstrate this theoretically for the Mallows model and experimentally for the Random Utilities models of noisy permutations. However, we show this pattern is *reversed* when the human is anchored on the algorithm's presented ordering - the joint system always has strictly worse performance. We extend these results to the case where the human and algorithm differ in their accuracy levels, showing that there always exist regimes where a more accurate agent would strictly benefit from collaborating with a less accurate one, but these regimes are asymmetric between the human and the algorithm's accuracy",
    "checked": true,
    "id": "0000f2eb98405f7d4044761c1707a199ac6189bc",
    "semantic_title": "when are two lists better than one?: benefits and harms in joint decision-making",
    "citation_count": 2,
    "authors": [
      "Kate Donahue",
      "Sreenivas Gollapudi",
      "Kostas Kollias"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28867": {
    "title": "A Local-Ascending-Global Learning Strategy for Brain-Computer Interface",
    "volume": "main",
    "abstract": "Neuroscience research indicates that the interaction among different functional regions of the brain plays a crucial role in driving various cognitive tasks. Existing studies have primarily focused on constructing either local or global functional connectivity maps within the brain, often lacking an adaptive approach to fuse functional brain regions and explore latent relationships between localization during different cognitive tasks. This paper introduces a novel approach called the Local-Ascending-Global Learning Strategy (LAG) to uncover higher-level latent topological patterns among functional brain regions. The strategy initiates from the local connectivity of individual brain functional regions and develops a K-Level Self-Adaptive Ascending Network (SALK) to dynamically capture strong connectivity patterns among brain regions during different cognitive tasks. Through the step-by-step fusion of brain regions, this approach captures higher-level latent patterns, shedding light on the progressively adaptive fusion of various brain functional regions under different cognitive tasks. Notably, this study represents the first exploration of higher-level latent patterns through progressively adaptive fusion of diverse brain functional regions under different cognitive tasks. The proposed LAG strategy is validated using datasets related to fatigue (SEED-VIG), emotion (SEED-IV), and motor imagery (BCI_C_IV_2a). The results demonstrate the generalizability of LAG, achieving satisfactory outcomes in independent-subject experiments across all three datasets. This suggests that LAG effectively characterizes higher-level latent patterns associated with different cognitive tasks, presenting a novel approach to understanding brain patterns in varying cognitive contexts",
    "checked": true,
    "id": "88d20c1aca0199e0fa049826faa03852ee6ad913",
    "semantic_title": "a local-ascending-global learning strategy for brain-computer interface",
    "citation_count": 0,
    "authors": [
      "Dongrui Gao",
      "Haokai Zhang",
      "Pengrui Li",
      "Tian Tang",
      "Shihong Liu",
      "Zhihong Zhou",
      "Shaofei Ying",
      "Ye Zhu",
      "Yongqing Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28868": {
    "title": "Working Memory Capacity of ChatGPT: An Empirical Study",
    "volume": "main",
    "abstract": "Working memory is a critical aspect of both human intelligence and artificial intelligence, serving as a workspace for the temporary storage and manipulation of information. In this paper, we systematically assess the working memory capacity of ChatGPT, a large language model developed by OpenAI, by examining its performance in verbal and spatial n-back tasks under various conditions. Our experiments reveal that ChatGPT has a working memory capacity limit strikingly similar to that of humans. Furthermore, we investigate the impact of different instruction strategies on ChatGPT's performance and observe that the fundamental patterns of a capacity limit persist. From our empirical findings, we propose that n-back tasks may serve as tools for benchmarking the working memory capacity of large language models and hold potential for informing future efforts aimed at enhancing AI working memory",
    "checked": true,
    "id": "237c4abc5922aa4cf5775fd88e22ff2e8c98809b",
    "semantic_title": "working memory capacity of chatgpt: an empirical study",
    "citation_count": 4,
    "authors": [
      "Dongyu Gong",
      "Xingchen Wan",
      "Dingmin Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28869": {
    "title": "Count What You Want: Exemplar Identification and Few-Shot Counting of Human Actions in the Wild",
    "volume": "main",
    "abstract": "This paper addresses the task of counting human actions of interest using sensor data from wearable devices. We propose a novel exemplar-based framework, allowing users to provide exemplars of the actions they want to count by vocalizing predefined sounds ``one'', ``two'', and ``three''. Our method first localizes temporal positions of these utterances from the audio sequence. These positions serve as the basis for identifying exemplars representing the action class of interest. A similarity map is then computed between the exemplars and the entire sensor data sequence, which is further fed into a density estimation module to generate a sequence of estimated density values. Summing these density values provides the final count. To develop and evaluate our approach, we introduce a diverse and realistic dataset consisting of real-world data from 37 subjects and 50 action categories, encompassing both sensor and audio data. The experiments on this dataset demonstrate the viability of the proposed method in counting instances of actions from new classes and subjects that were not part of the training data. On average, the discrepancy between the predicted count and the ground truth value is 7.47, significantly lower than the errors of the frequency-based and transformer-based methods. Our project, code and dataset can be found at https://github.com/cvlab-stonybrook/ExRAC",
    "checked": true,
    "id": "a5ae2262bf1988e063935842ddc109360f6be3e9",
    "semantic_title": "count what you want: exemplar identification and few-shot counting of human actions in the wild",
    "citation_count": 0,
    "authors": [
      "Yifeng Huang",
      "Duc Duy Nguyen",
      "Lam Nguyen",
      "Cuong Pham",
      "Minh Hoai"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28870": {
    "title": "Learning Optimal Advantage from Preferences and Mistaking It for Reward",
    "volume": "main",
    "abstract": "We consider algorithms for learning reward functions from human preferences over pairs of trajectory segments, as used in reinforcement learning from human feedback (RLHF). Most recent work assumes that human preferences are generated based only upon the reward accrued within those segments, or their partial return. Recent work casts doubt on the validity of this assumption, proposing an alternative preference model based upon regret. We investigate the consequences of assuming preferences are based upon partial return when they actually arise from regret. We argue that the learned function is an approximation of the optimal advantage function, not a reward function. We find that if a specific pitfall is addressed, this incorrect assumption is not particularly harmful, resulting in a highly shaped reward function. Nonetheless, this incorrect usage of the approximation of the optimal advantage function is less desirable than the appropriate and simpler approach of greedy maximization of it. From the perspective of the regret preference model, we also provide a clearer interpretation of fine tuning contemporary large language models with RLHF. This paper overall provides insight regarding why learning under the partial return preference model tends to work so well in practice, despite it conforming poorly to how humans give preferences",
    "checked": true,
    "id": "e571f9b823dcb31580161a37342a73dd93ebbe52",
    "semantic_title": "learning optimal advantage from preferences and mistaking it for reward",
    "citation_count": 7,
    "authors": [
      "W. Bradley Knox",
      "Stephane Hatgis-Kessell",
      "Sigurdur Orn Adalgeirsson",
      "Serena Booth",
      "Anca Dragan",
      "Peter Stone",
      "Scott Niekum"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28871": {
    "title": "A Unified Self-Distillation Framework for Multimodal Sentiment Analysis with Uncertain Missing Modalities",
    "volume": "main",
    "abstract": "Multimodal Sentiment Analysis (MSA) has attracted widespread research attention recently. Most MSA studies are based on the assumption of modality completeness. However, many inevitable factors in real-world scenarios lead to uncertain missing modalities, which invalidate the fixed multimodal fusion approaches. To this end, we propose a Unified multimodal Missing modality self-Distillation Framework (UMDF) to handle the problem of uncertain missing modalities in MSA. Specifically, a unified self-distillation mechanism in UMDF drives a single network to automatically learn robust inherent representations from the consistent distribution of multimodal data. Moreover, we present a multi-grained crossmodal interaction module to deeply mine the complementary semantics among modalities through coarse- and fine-grained crossmodal attention. Eventually, a dynamic feature integration module is introduced to enhance the beneficial semantics in incomplete modalities while filtering the redundant information therein to obtain a refined and robust multimodal representation. Comprehensive experiments on three datasets demonstrate that our framework significantly improves MSA performance under both uncertain missing-modality and complete-modality testing conditions",
    "checked": true,
    "id": "8be8ff81ceab34a53fe2a388415adde675ab8963",
    "semantic_title": "a unified self-distillation framework for multimodal sentiment analysis with uncertain missing modalities",
    "citation_count": 5,
    "authors": [
      "Mingcheng Li",
      "Dingkang Yang",
      "Yuxuan Lei",
      "Shunli Wang",
      "Shuaibing Wang",
      "Liuzhen Su",
      "Kun Yang",
      "Yuzheng Wang",
      "Mingyang Sun",
      "Lihua Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28872": {
    "title": "Decoding AI's Nudge: A Unified Framework to Predict Human Behavior in AI-Assisted Decision Making",
    "volume": "main",
    "abstract": "With the rapid development of AI-based decision aids, different forms of AI assistance have been increasingly integrated into the human decision making processes. To best support humans in decision making, it is essential to quantitatively understand how diverse forms of AI assistance influence humans' decision making behavior. To this end, much of the current research focuses on the end-to-end prediction of human behavior using ``black-box'' models, often lacking interpretations of the nuanced ways in which AI assistance impacts the human decision making process. Meanwhile, methods that prioritize the interpretability of human behavior predictions are often tailored for one specific form of AI assistance, making adaptations to other forms of assistance difficult. In this paper, we propose a computational framework that can provide an interpretable characterization of the influence of different forms of AI assistance on decision makers in AI-assisted decision making. By conceptualizing AI assistance as the ``nudge'' in human decision making processes, our approach centers around modelling how different forms of AI assistance modify humans' strategy in weighing different information in making their decisions. Evaluations on behavior data collected from real human decision makers show that the proposed framework outperforms various baselines in accurately predicting human behavior in AI-assisted decision making. Based on the proposed framework, we further provide insights into how individuals with different cognitive styles are nudged by AI assistance differently",
    "checked": true,
    "id": "e3eb02c13cb5f52f95e3ef5f116b251d282f127e",
    "semantic_title": "decoding ai's nudge: a unified framework to predict human behavior in ai-assisted decision making",
    "citation_count": 3,
    "authors": [
      "Zhuoyan Li",
      "Zhuoran Lu",
      "Ming Yin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28873": {
    "title": "GigaHumanDet: Exploring Full-Body Detection on Gigapixel-Level Images",
    "volume": "main",
    "abstract": "Performing person detection in super-high-resolution images has been a challenging task. For such a task, modern detectors, which usually encode a box using center and width/height, struggle with accuracy due to two factors: 1) Human characteristic: people come in various postures and the center with high freedom is difficult to capture robust visual pattern; 2) Image characteristic: due to vast scale diversity of input (gigapixel-level), distance regression (for width and height) is hard to pinpoint, especially for a person, with substantial scale, who is near the camera. To address these challenges, we propose GigaHumanDet, an innovative solution aimed at further enhancing detection accuracy for gigapixel-level images. GigaHumanDet employs the corner modeling method to avoid the potential issues of a high degree of freedom in center pinpointing. To better distinguish similar-looking persons and enforce instance consistency of corner pairs, an instance-guided learning approach is designed to capture discriminative individual semantics. Further, we devise reliable shape-aware bodyness equipped with a multi-precision strategy as the human corner matching guidance to be appropriately adapted to the single-view large scene. Experimental results on PANDA and STCrowd datasets show the superiority and strong applicability of our design. Notably, our model achieves 82.4% in term of AP, outperforming current state-of-the-arts by more than 10%",
    "checked": true,
    "id": "e9ea5ec07b7016ff277f66203cfd2824dcb68c0e",
    "semantic_title": "gigahumandet: exploring full-body detection on gigapixel-level images",
    "citation_count": 0,
    "authors": [
      "Chenglong Liu",
      "Haoran Wei",
      "Jinze Yang",
      "Jintao Liu",
      "Wenxi Li",
      "Yuchen Guo",
      "Lu Fang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28874": {
    "title": "Hypergraph-Guided Disentangled Spectrum Transformer Networks for Near-Infrared Facial Expression Recognition",
    "volume": "main",
    "abstract": "With the strong robusticity on illumination variations, near-infrared (NIR) can be an effective and essential complement to visible (VIS) facial expression recognition in low lighting or complete darkness conditions. However, facial expression recognition (FER) from NIR images presents a more challenging problem than traditional FER due to the limitations imposed by the data scale and the difficulty of extracting discriminative features from incomplete visible lighting contents. In this paper, we give the first attempt at deep NIR facial expression recognition and propose a novel method called near-infrared facial expression transformer (NFER-Former). Specifically, to make full use of the abundant label information in the field of VIS, we introduce a Self-Attention Orthogonal Decomposition mechanism that disentangles the expression information and spectrum information from the input image, so that the expression features can be extracted without the interference of spectrum variation. We also propose a Hypergraph-Guided Feature Embedding method that models some key facial behaviors and learns the structure of the complex correlations between them, thereby alleviating the interference of inter-class similarity. Additionally, we construct a large NIR-VIS Facial Expression dataset that includes 360 subjects to better validate the efficiency of NFER-Former. Extensive experiments and ablation studies show that NFER-Former significantly improves the performance of NIR FER and achieves state-of-the-art results on the only two available NIR FER datasets, Oulu-CASIA and Large-HFE",
    "checked": true,
    "id": "bdcbee8cceb84afa8d1f62b632483df315ec8acc",
    "semantic_title": "hypergraph-guided disentangled spectrum transformer networks for near-infrared facial expression recognition",
    "citation_count": 0,
    "authors": [
      "Bingjun Luo",
      "Haowen Wang",
      "Jinpeng Wang",
      "Junjie Zhu",
      "Xibin Zhao",
      "Yue Gao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28875": {
    "title": "Goal Alignment: Re-analyzing Value Alignment Problems Using Human-Aware AI",
    "volume": "main",
    "abstract": "While the question of misspecified objectives has gotten much attention in recent years, most works in this area primarily focus on the challenges related to the complexity of the objective specification mechanism (for example, the use of reward functions). However, the complexity of the objective specification mechanism is just one of many reasons why the user may have misspecified their objective. A foundational cause for misspecification that is being overlooked by these works is the inherent asymmetry in human expectations about the agent's behavior and the behavior generated by the agent for the specified objective. To address this, we propose a novel formulation for the objective misspecification problem that builds on the human-aware planning literature, which was originally introduced to support explanation and explicable behavioral generation. Additionally, we propose a first-of-its-kind interactive algorithm that is capable of using information generated under incorrect beliefs about the agent to determine the true underlying goal of the user",
    "checked": true,
    "id": "8961e4c9fcda8ffcf364aacb067e7e1f1968118f",
    "semantic_title": "goal alignment: re-analyzing value alignment problems using human-aware ai",
    "citation_count": 4,
    "authors": [
      "Malek Mechergui",
      "Sarath Sreedharan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28876": {
    "title": "Efficient Online Crowdsourcing with Complex Annotations",
    "volume": "main",
    "abstract": "Crowdsourcing platforms use various truth discovery algorithms to aggregate annotations from multiple labelers. In an online setting, however, the main challenge is to decide whether to ask for more annotations for each item to efficiently trade off cost (i.e., the number of annotations) for quality of the aggregated annotations. In this paper, we propose a novel approach for general complex annotation (such as bounding boxes and taxonomy paths), that works in an online crowdsourcing setting. We prove that the expected average similarity of a labeler is linear in their accuracy conditional on the reported label. This enables us to infer reported label accuracy in a broad range of scenarios. We conduct extensive evaluations on real-world crowdsourcing data from Meta and show the effectiveness of our proposed online algorithms in improving the cost-quality trade-off",
    "checked": true,
    "id": "d73508c03796892ced1489aa3a50dddb2c729ce0",
    "semantic_title": "efficient online crowdsourcing with complex annotations",
    "citation_count": 0,
    "authors": [
      "Reshef Meir",
      "Viet-An Nguyen",
      "Xu Chen",
      "Jagdish Ramakrishnan",
      "Udi Weinsberg"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28877": {
    "title": "Can You Rely on Synthetic Labellers in Preference-Based Reinforcement Learning? It's Complicated",
    "volume": "main",
    "abstract": "Preference-based Reinforcement Learning (PbRL) enables non-experts to train Reinforcement Learning models using preference feedback. However, the effort required to collect preference labels from real humans means that PbRL research primarily relies on synthetic labellers. We validate the most common synthetic labelling strategy by comparing against labels collected from a crowd of humans on three Deep Mind Control (DMC) suite tasks: stand, walk, and run. We find that: (1) the synthetic labels are a good proxy for real humans under some circumstances, (2) strong preference label agreement between human and synthetic labels is not necessary for similar policy performance, (3) policy performance is higher at the start of training from human feedback and is higher at the end of training from synthetic feedback, and (4) training on only examples with high levels of inter-annotator agreement does not meaningfully improve policy performance. Our results justify the use of synthetic labellers to develop and ablate PbRL methods, and provide insight into how human labelling changes over the course of policy training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Katherine Metcalf",
      "Miguel Sarabia",
      "Masha Fedzechkina",
      "Barry-John Theobald"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28878": {
    "title": "When to Show a Suggestion? Integrating Human Feedback in AI-Assisted Programming",
    "volume": "main",
    "abstract": "AI powered code-recommendation systems, such as Copilot and CodeWhisperer, provide code suggestions inside a programmer's environment (e.g., an IDE) with the aim of improving productivity. We pursue mechanisms for leveraging signals about programmers' acceptance and rejection of code suggestions to guide recommendations. We harness data drawn from interactions with GitHub Copilot, a system used by millions of programmers, to develop interventions that can save time for programmers. We introduce a utility-theoretic framework to drive decisions about suggestions to display versus withhold. The approach, conditional suggestion display from human feedback (CDHF), relies on a cascade of models that provide the likelihood that recommended code will be accepted. These likelihoods are used to selectively hide suggestions, reducing both latency and programmer verification time. Using data from 535 programmers, we perform a retrospective evaluation of CDHF and show that we can avoid displaying a significant fraction of suggestions that would have been rejected. We further demonstrate the importance of incorporating the programmer's latent unobserved state in decisions about when to display suggestions through an ablation study. Finally, we showcase how using suggestion acceptance as a reward signal for guiding the display of suggestions can lead to suggestions of reduced quality, indicating an unexpected pitfall",
    "checked": true,
    "id": "9beb5c2c17aa059b7c018a18eb88d183563e75f0",
    "semantic_title": "when to show a suggestion? integrating human feedback in ai-assisted programming",
    "citation_count": 12,
    "authors": [
      "Hussein Mozannar",
      "Gagan Bansal",
      "Adam Fourney",
      "Eric Horvitz"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28879": {
    "title": "Improving Transferability for Cross-Domain Trajectory Prediction via Neural Stochastic Differential Equation",
    "volume": "main",
    "abstract": "Multi-agent trajectory prediction is crucial for various practical applications, spurring the construction of many large-scale trajectory datasets, including vehicles and pedestrians. However, discrepancies exist among datasets due to external factors and data acquisition strategies. External factors include geographical differences and driving styles, while data acquisition strategies include data acquisition rate, history/prediction length, and detector/tracker error. Consequently, the proficient performance of models trained on large-scale datasets has limited transferability on other small-size datasets, bounding the utilization of existing large-scale datasets. To address this limitation, we propose a method based on continuous and stochastic representations of Neural Stochastic Differential Equations (NSDE) for alleviating discrepancies due to data acquisition strategy. We utilize the benefits of continuous representation for handling arbitrary time steps and the use of stochastic representation for handling detector/tracker errors. Additionally, we propose a dataset-specific diffusion network and its training framework to handle dataset-specific detection/tracking errors. The effectiveness of our method is validated against state-of-the-art trajectory prediction models on the popular benchmark datasets: nuScenes, Argoverse, Lyft, INTERACTION, and Waymo Open Motion Dataset (WOMD). Improvement in performance gain on various source and target dataset configurations shows the generalized competence of our approach in addressing cross-dataset discrepancies",
    "checked": true,
    "id": "cc300c698f2bdb4a79a0cdd919a416012536e5f1",
    "semantic_title": "improving transferability for cross-domain trajectory prediction via neural stochastic differential equation",
    "citation_count": 4,
    "authors": [
      "Daehee Park",
      "Jaewoo Jeong",
      "Kuk-Jin Yoon"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28880": {
    "title": "Exploring Domain Incremental Video Highlights Detection with the LiveFood Benchmark",
    "volume": "main",
    "abstract": "Video highlights detection (VHD) is an active research field in computer vision, aiming to locate the most user-appealing clips given raw video inputs. However, most VHD methods are based on the closed world assumption, i.e., a fixed number of highlight categories is defined in advance and all training data are available beforehand. Consequently, existing methods have poor scalability with respect to increasing highlight domains and training data. To address above issues, we propose a novel video highlights detection method named Global Prototype Encoding (GPE) to learn incrementally for adapting to new domains via parameterized prototypes. To facilitate this new research direction, we collect a finely annotated dataset termed LiveFood, including over 5,100 live gourmet videos that consist of four domains: ingredients, cooking, presentation, and eating. To the best of our knowledge, this is the first work to explore video highlights detection in the incremental learning setting, opening up new land to apply VHD for practical scenarios where both the concerned highlight domains and training data increase over time. We demonstrate the effectiveness of GPE through extensive experiments. Notably, GPE surpasses popular domain incremental learning methods on LiveFood, achieving significant mAP improvements on all domains. Concerning the classic datasets, GPE also yields comparable performance as previous arts. The code is available at: https://github.com/ForeverPs/IncrementalVHD_GPE",
    "checked": true,
    "id": "e922b0f7ede1bfa6d7efaef823fdf12f37fdb70c",
    "semantic_title": "exploring domain incremental video highlights detection with the livefood benchmark",
    "citation_count": 0,
    "authors": [
      "Sen Pei",
      "Shixiong Xu",
      "Xiaojie Jin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28881": {
    "title": "Sample-Constrained Black Box Optimization for Audio Personalization",
    "volume": "main",
    "abstract": "We consider the problem of personalizing audio to maximize user experience. Briefly, we aim to find a filter h*, which applied to any music or speech, will maximize the user's satisfaction. This is a black-box optimization problem since the user's satisfaction function is unknown. Substantive work has been done on this topic where the key idea is to play audio samples to the user, each shaped by a different filter hi, and query the user for their satisfaction scores f(hi). A family of \"surrogate\" functions is then designed to fit these scores and the optimization method gradually refines these functions to arrive at the filter ˆh* that maximizes satisfaction. In certain applications, we observe that a second type of querying is possible where users can tell us the individual elements h*[j] of the optimal filter h*. Consider an analogy from cooking where the goal is to cook a recipe that maximizes user satisfaction. A user can be asked to score various cooked recipes (e.g., tofu fried rice) or to score individual ingredients (say, salt, sugar, rice, chicken, etc.). Given a budget of B queries, where a query can be of either type, our goal is to find the recipe that will maximize this user's satisfaction. Our proposal builds on Sparse Gaussian Process Regression (GPR) and shows how a hybrid approach can outperform any one type of querying. Our results are validated through simulations and real world experiments, where volunteers gave feedback on music/speech audio and were able to achieve high satisfaction levels. We believe this idea of hybrid querying opens new problems in black-box optimization and solutions can benefit other applications beyond audio personalization",
    "checked": true,
    "id": "a1fd6568e672e8f235223d18cf8082415c84e3a1",
    "semantic_title": "sample-constrained black box optimization for audio personalization",
    "citation_count": 0,
    "authors": [
      "Rajalaxmi Rajagopalan",
      "Yu-Lin Wei",
      "Romit Roy Choudhury"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28882": {
    "title": "Intelligent Calibration for Bias Reduction in Sentiment Corpora Annotation Process",
    "volume": "main",
    "abstract": "This paper focuses in the inherent anchoring bias present in sequential reviews-sentiment corpora annotation processes. It proposes employing a limited subset of meticulously chosen reviews at the outset of the process, as a means of calibration, effectively mitigating the phenomenon. Through extensive experimentation we validate the phenomenon of sentiment bias in the annotation process and show that its magnitude can be influenced by pre-calibration. Furthermore, we show that the choice of the calibration set matters, hence the need for effective guidelines for choosing the reviews to be included in it. A comparison of annotators performance with the proposed calibration to annotation processes that do not use calibration or use a randomly-picked calibration set, reveals that indeed the calibration set picked is highly effective---it manages to substantially reduce the average absolute error compared to the other cases. Furthermore, the proposed selection guidelines are found to be highly robust in picking an effective calibration set also for domains different than the one based on which these rules were extracted",
    "checked": true,
    "id": "e521f72d764f64f8c0ac7af050812f6833d4d755",
    "semantic_title": "intelligent calibration for bias reduction in sentiment corpora annotation process",
    "citation_count": 0,
    "authors": [
      "Idan Toker",
      "David Sarne",
      "Jonathan Schler"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28883": {
    "title": "TransGOP: Transformer-Based Gaze Object Prediction",
    "volume": "main",
    "abstract": "Gaze object prediction aims to predict the location and category of the object that is watched by a human. Previous gaze object prediction works use CNN-based object detectors to predict the object's location. However, we find that Transformer-based object detectors can predict more accurate object location for dense objects in retail scenarios. Moreover, the long-distance modeling capability of the Transformer can help to build relationships between the human head and the gaze object, which is important for the GOP task. To this end, this paper introduces Transformer into the fields of gaze object prediction and proposes an end-to-end Transformer-based gaze object prediction method named TransGOP. Specifically, TransGOP uses an off-the-shelf Transformer-based object detector to detect the location of objects and designs a Transformer-based gaze autoencoder in the gaze regressor to establish long-distance gaze relationships. Moreover, to improve gaze heatmap regression, we propose an object-to-gaze cross-attention mechanism to let the queries of the gaze autoencoder learn the global-memory position knowledge from the object detector. Finally, to make the whole framework end-to-end trained, we propose a Gaze Box loss to jointly optimize the object detector and gaze regressor by enhancing the gaze heatmap energy in the box of the gaze object. Extensive experiments on the GOO-Synth and GOO-Real datasets demonstrate that our TransGOP achieves state-of-the-art performance on all tracks, i.e., object detection, gaze estimation, and gaze object prediction. Our code will be available at https://github.com/chenxi-Guo/TransGOP.git",
    "checked": true,
    "id": "66cf7f356ac2796427cc08932dd2f0cde8c1ba74",
    "semantic_title": "transgop: transformer-based gaze object prediction",
    "citation_count": 2,
    "authors": [
      "Binglu Wang",
      "Chenxi Guo",
      "Yang Jin",
      "Haisheng Xia",
      "Nian Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28884": {
    "title": "Visual Redundancy Removal for Composite Images: A Benchmark Dataset and a Multi-Visual-Effects Driven Incremental Method",
    "volume": "main",
    "abstract": "Composite images (CIs) typically combine various elements from different scenes, views, and styles, which are a very important information carrier in the era of mixed media such as virtual reality, mixed reality, metaverse, etc. However, the complexity of CI content presents a significant challenge for subsequent visual perception modeling and compression. In addition, the lack of benchmark CI databases also hinders the use of recent advanced data-driven methods. To address these challenges, we first establish one of the earliest visual redundancy prediction (VRP) databases for CIs. Moreover, we propose a multi-visual effect (MVE)-driven incremental learning method that combines the strengths of hand-crafted and data-driven approaches to achieve more accurate VRP modeling. Specifically, we design special incremental rules to learn the visual knowledge flow of MVE. To effectively capture the associated features of MVE, we further develop a three-stage incremental learning approach for VRP based on an encoder-decoder network. Extensive experimental results validate the superiority of the proposed method in terms of subjective, objective, and compression experiments",
    "checked": true,
    "id": "89a560852eebff0b0c6f187ed2d5e31811dca6c9",
    "semantic_title": "visual redundancy removal for composite images: a benchmark dataset and a multi-visual-effects driven incremental method",
    "citation_count": 0,
    "authors": [
      "Miaohui Wang",
      "Rong Zhang",
      "Lirong Huang",
      "Yanshan Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28885": {
    "title": "TexFit: Text-Driven Fashion Image Editing with Diffusion Models",
    "volume": "main",
    "abstract": "Fashion image editing aims to edit an input image to obtain richer or distinct visual clothing matching effects. Existing global fashion image editing methods are difficult to achieve rich outfit combination effects while local fashion image editing is more in line with the needs of diverse and personalized outfit matching. The local editing techniques typically depend on text and auxiliary modalities (e.g., human poses, human keypoints, garment sketches, etc.) for image manipulation, where the auxiliary modalities essentially assist in locating the editing region. Since these auxiliary modalities usually involve additional efforts in practical application scenarios, text-driven fashion image editing shows high flexibility. In this paper, we propose TexFit, a Text-driven Fashion image Editing method using diffusion models, which performs the local image editing only with the easily accessible text. Our approach employs a text-based editing region location module to predict precise editing region in the fashion image. Then, we take the predicted region as the generation condition of diffusion models together with the text prompt to achieve precise local editing of fashion images while keeping the rest part intact. In addition, previous fashion datasets usually focus on global description, lacking local descriptive information that can guide the precise local editing. Therefore, we develop a new DFMM-Spotlight dataset by using region extraction and attribute combination strategies. It focuses locally on clothes and accessories, enabling local editing with text input. Experimental results on the DFMM-Spotlight dataset demonstrate the effectiveness of our model. Code and Datasets are available at https://texfit.github.io/",
    "checked": true,
    "id": "25d1c0a2fe7a19dcc6a57d5ccf4501d2643c5006",
    "semantic_title": "texfit: text-driven fashion image editing with diffusion models",
    "citation_count": 0,
    "authors": [
      "Tongxin Wang",
      "Mang Ye"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28886": {
    "title": "Rating-Based Reinforcement Learning",
    "volume": "main",
    "abstract": "This paper develops a novel rating-based reinforcement learning approach that uses human ratings to obtain human guidance in reinforcement learning. Different from the existing preference-based and ranking-based reinforcement learning paradigms, based on human relative preferences over sample pairs, the proposed rating-based reinforcement learning approach is based on human evaluation of individual trajectories without relative comparisons between sample pairs. The rating-based reinforcement learning approach builds on a new prediction model for human ratings and a novel multi-class loss function. We conduct several experimental studies based on synthetic ratings and real human ratings to evaluate the effectiveness and benefits of the new rating-based reinforcement learning approach",
    "checked": true,
    "id": "8e0e795463a2007497c9c257f9de337c53f1c4b9",
    "semantic_title": "rating-based reinforcement learning",
    "citation_count": 3,
    "authors": [
      "Devin White",
      "Mingkang Wu",
      "Ellen Novoseller",
      "Vernon J. Lawhern",
      "Nicholas Waytowich",
      "Yongcan Cao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28887": {
    "title": "MKG-FENN: A Multimodal Knowledge Graph Fused End-to-End Neural Network for Accurate Drug–Drug Interaction Prediction",
    "volume": "main",
    "abstract": "Taking incompatible multiple drugs together may cause adverse interactions and side effects on the body. Accurate prediction of drug-drug interaction (DDI) events is essential for avoiding this issue. Recently, various artificial intelligence-based approaches have been proposed for predicting DDI events. However, DDI events are associated with complex relationships and mechanisms among drugs, targets, enzymes, transporters, molecular structures, etc. Existing approaches either partially or loosely consider these relationships and mechanisms by a non-end-to-end learning framework, resulting in sub-optimal feature extractions and fusions for prediction. Different from them, this paper proposes a Multimodal Knowledge Graph Fused End-to-end Neural Network (MKGFENN) that consists of two main parts: multimodal knowledge graph (MKG) and fused end-to-end neural network (FENN). First, MKG is constructed by comprehensively exploiting DDI events-associated relationships and mechanisms from four knowledge graphs of drugs-chemical entities, drug-substructures, drugs-drugs, and molecular structures. Correspondingly, a four channels graph neural network is designed to extract high-order and semantic features from MKG. Second, FENN designs a multi-layer perceptron to fuse the extracted features by end-to-end learning. With such designs, the feature extractions and fusions of DDI events are guaranteed to be comprehensive and optimal for prediction. Through extensive experiments on real drug datasets, we demonstrate that MKG-FENN exhibits high accuracy and significantly outperforms state-of-the-art models in predicting DDI events. The source code and supplementary file of this article are available on: https://github.com/wudi1989/MKG-FENN",
    "checked": false,
    "id": "c10d94a827d8c1ef8a22e199a1ca3ccffbce01fc",
    "semantic_title": "mkg-fenn: a multimodal knowledge graph fused end-to-end neural network for accurate drug-drug interaction prediction",
    "citation_count": 0,
    "authors": [
      "Di Wu",
      "Wu Sun",
      "Yi He",
      "Zhong Chen",
      "Xin Luo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28888": {
    "title": "Spatial-Related Sensors Matters: 3D Human Motion Reconstruction Assisted with Textual Semantics",
    "volume": "main",
    "abstract": "Leveraging wearable devices for motion reconstruction has emerged as an economical and viable technique. Certain methodologies employ sparse Inertial Measurement Units (IMUs) on the human body and harness data-driven strategies to model human poses. However, the reconstruction of motion based solely on sparse IMU data is inherently fraught with ambiguity, a consequence of numerous identical IMU readings corresponding to different poses. In this paper, we explore the spatial importance of sparse sensors, supervised by text that describes specific actions. Specifically, uncertainty is introduced to derive weighted features for each IMU. We also design a Hierarchical Temporal Transformer (HTT) and apply contrastive learning to achieve precise temporal and feature alignment of sensor data with textual semantics. Experimental results demonstrate our proposed approach achieves significant improvements in multiple metrics compared to existing methods. Notably, with textual supervision, our method not only differentiates between ambiguous actions such as sitting and standing but also produces more precise and natural motion",
    "checked": true,
    "id": "99ac970928a04f35333401f691fd38084e985957",
    "semantic_title": "spatial-related sensors matters: 3d human motion reconstruction assisted with textual semantics",
    "citation_count": 1,
    "authors": [
      "Xueyuan Yang",
      "Chao Yao",
      "Xiaojuan Ban"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28889": {
    "title": "Scalable Motion Style Transfer with Constrained Diffusion Generation",
    "volume": "main",
    "abstract": "Current training of motion style transfer systems relies on consistency losses across style domains to preserve contents, hindering its scalable application to a large number of domains and private data. Recent image transfer works show the potential of independent training on each domain by leveraging implicit bridging between diffusion models, with the content preservation, however, limited to simple data patterns. We address this by imposing biased sampling in backward diffusion while maintaining the domain independence in the training stage. We construct the bias from the source domain keyframes and apply them as the gradient of content constraints, yielding a framework with keyframe manifold constraint gradients (KMCGs). Our validation demonstrates the success of training separate models to transfer between as many as ten dance motion styles. Comprehensive experiments find a significant improvement in preserving motion contents in comparison to baseline and ablative diffusion-based style transfer models. In addition, we perform a human study for a subjective assessment of the quality of generated dance motions. The results validate the competitiveness of KMCGs",
    "checked": true,
    "id": "c29b667eba2e1be8b2b197d240714d13d2b9f73f",
    "semantic_title": "scalable motion style transfer with constrained diffusion generation",
    "citation_count": 0,
    "authors": [
      "Wenjie Yin",
      "Yi Yu",
      "Hang Yin",
      "Danica Kragic",
      "Mårten Björkman"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28890": {
    "title": "‘Why Didn't You Allocate This Task to Them?' Negotiation-Aware Task Allocation and Contrastive Explanation Generation",
    "volume": "main",
    "abstract": "In this work, we design an Artificially Intelligent Task Allocator (AITA) that proposes a task allocation for a team of humans. A key property of this allocation is that when an agent with imperfect knowledge (about their teammate's costs and/or the team's performance metric) contests the allocation with a counterfactual, a contrastive explanation can always be provided to showcase why the proposed allocation is better than the proposed counterfactual. For this, we consider a negotiation process that produces a negotiation-aware task allocation and, when contested, leverages a negotiation tree to provide a contrastive explanation. With human subject studies, we show that the proposed allocation indeed appears fair to a majority of participants and, when not, the explanations generated are judged as convincing and easy to comprehend",
    "checked": false,
    "id": "83682de8d4a7abdfdc21f4296fff3ad4a47ccec4",
    "semantic_title": "why didn't you allocate this task to them?' negotiation-aware task allocation and contrastive explanation generation",
    "citation_count": 5,
    "authors": [
      "Zahra Zahedi",
      "Sailik Sengupta",
      "Subbarao Kambhampati"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28891": {
    "title": "Beyond Mimicking Under-Represented Emotions: Deep Data Augmentation with Emotional Subspace Constraints for EEG-Based Emotion Recognition",
    "volume": "main",
    "abstract": "In recent years, using Electroencephalography (EEG) to recognize emotions has garnered considerable attention. Despite advancements, limited EEG data restricts its potential. Thus, Generative Adversarial Networks (GANs) are proposed to mimic the observed distributions and generate EEG data. However, for imbalanced datasets, GANs struggle to produce reliable augmentations for under-represented minority emotions by merely mimicking them. Thus, we introduce Emotional Subspace Constrained Generative Adversarial Networks (ESC-GAN) as an alternative to existing frameworks. We first propose the EEG editing paradigm, editing reference EEG signals from well-represented to under-represented emotional subspaces. Then, we introduce diversity-aware and boundary-aware losses to constrain the augmented subspace. Here, the diversity-aware loss encourages a diverse emotional subspace by enlarging the sample difference, while boundary-aware loss constrains the augmented subspace near the decision boundary where recognition models can be vulnerable. Experiments show ESC-GAN boosts emotion recognition performance on benchmark datasets, DEAP, AMIGOS, and SEED, while protecting against potential adversarial attacks. Finally, the proposed method opens new avenues for editing EEG signals under emotional subspace constraints, facilitating unbiased and secure EEG data augmentation",
    "checked": true,
    "id": "4f22cee12f060cc838c64120fa4e2082ed3120fd",
    "semantic_title": "beyond mimicking under-represented emotions: deep data augmentation with emotional subspace constraints for eeg-based emotion recognition",
    "citation_count": 1,
    "authors": [
      "Zhi Zhang",
      "Shenghua Zhong",
      "Yan Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28892": {
    "title": "MetaRLEC: Meta-Reinforcement Learning for Discovery of Brain Effective Connectivity",
    "volume": "main",
    "abstract": "In recent years, the discovery of brain effective connectivity (EC) networks through computational analysis of functional magnetic resonance imaging (fMRI) data has gained prominence in neuroscience and neuroimaging. However, owing to the influence of diverse factors during data collection and processing, fMRI data typically exhibits high noise and limited sample characteristics, consequently leading to suboptimal performance of current methods. In this paper, we propose a novel brain effective connectivity discovery method based on meta-reinforcement learning, called MetaRLEC. The method mainly consists of three modules: actor, critic, and meta-critic. MetaRLEC first employs an encoder-decoder framework: the encoder utilizing a Transformer, converts noisy fMRI data into a state embedding; the decoder employing bidirectional LSTM, discovers brain region dependencies from the state and generates actions (EC networks). Then a critic network evaluates these actions, incentivizing the actor to learn higher-reward actions amidst the high-noise setting. Finally, a meta-critic framework facilitates online learning of historical state-action pairs, integrating an action-value neural network and supplementary training losses to enhance the model's adaptability to small-sample fMRI data. We conduct comprehensive experiments on both simulated and real-world data to demonstrate the efficacy of our proposed method",
    "checked": true,
    "id": "1bf0f04a8cb9f6c39dddff4ed206591b617b2912",
    "semantic_title": "metarlec: meta-reinforcement learning for discovery of brain effective connectivity",
    "citation_count": 1,
    "authors": [
      "Zuozhen Zhang",
      "Junzhong Ji",
      "Jinduo Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28893": {
    "title": "DanceMVP: Self-Supervised Learning for Multi-Task Primitive-Based Dance Performance Assessment via Transformer Text Prompting",
    "volume": "main",
    "abstract": "Dance is generally considered to be complex for most people as it requires coordination of numerous body motions and accurate responses to the musical content and rhythm. Studies on automatic dance performance assessment could help people improve their sensorimotor skills and promote research in many fields, including human motion analysis and motion generation. Recent papers on dance performance assessment usually evaluate simple dance motions with a single task - estimating final performance scores. In this paper, we propose DanceMVP: multi-task dance performance assessment via text prompting that solves three related tasks - (i) dance vocabulary recognition, (ii) dance performance scoring and (iii) dance rhythm evaluation. In the pre-training phase, we contrastively learn the primitive-based features of complex dance motion and music using the InfoNCE loss. For the downstream task, we propose a transformer-based text prompter to perform multi-task evaluations for the three proposed assessment tasks. Also, we build a multimodal dance-music dataset named ImperialDance. The novelty of our ImperialDance is that it contains dance motions for diverse expertise levels and a significant amount of repeating dance sequences for the same choreography to keep track of the dance performance progression. Qualitative results show that our pre-trained feature representation could cluster dance pieces for different dance genres, choreographies, expertise levels and primitives, which generalizes well on both ours and other dance-music datasets. The downstream experiments demonstrate the robustness and improvement of our method over several ablations and baselines across all three tasks, as well as monitoring the users' dance level progression",
    "checked": true,
    "id": "5ddc35132844e1906c2096cc2efa754cc44e4b4b",
    "semantic_title": "dancemvp: self-supervised learning for multi-task primitive-based dance performance assessment via transformer text prompting",
    "citation_count": 1,
    "authors": [
      "Yun Zhong",
      "Yiannis Demiris"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28910": {
    "title": "Complexity of Credulous and Skeptical Acceptance in Epistemic Argumentation Framework",
    "volume": "main",
    "abstract": "Dung's Argumentation Framework (AF) has been extended in several directions. Among the numerous proposed extensions, three of them seem to be of particular interest and have correlations between them. These extensions are: constrained AF (CAF), where AF is augmented with (strong) constraints; epistemic AF (EAF), where AF is augmented with epistemic constraints; and incomplete AF (iAF), where arguments and attacks can be uncertain. While the complexity and expressiveness of CAF and iAF have been studied, that of EAF has not been explored so far. In this paper we investigate the complexity and expressivity of EAF. To this end, we first introduce the Labeled CAF (LCAF), a variation of CAF where constraints are defined over the alphabet of labeled arguments. Then, we investigate the complexity of credulous and skeptical reasoning and show that: i) EAF is more expressive than iAF (under preferred semantics), ii) although LCAF is a restriction of EAF where modal operators are not allowed, these frameworks have the same complexity, iii) the results for LCAF close a gap in the characterization of the complexity of CAF. Interestingly, even though EAF has the same complexity as LCAF, it allows modeling domain knowledge in a more natural and easy-to-understand way",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gianvincenzo Alfano",
      "Sergio Greco",
      "Francesco Parisi",
      "Irina Trubitsyna"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28911": {
    "title": "Approximation Algorithms for Preference Aggregation Using CP-Nets",
    "volume": "main",
    "abstract": "This paper studies the design and analysis of approximation algorithms for aggregating preferences over combinatorial domains, represented using Conditional Preference Networks (CP-nets). Its focus is on aggregating preferences over so-called swaps, for which optimal solutions in general are already known to be of exponential size. We first analyze a trivial 2-approximation algorithm that simply outputs the best of the given input preferences, and establish a structural condition under which the approximation ratio of this algorithm is improved to 4/3. We then propose a polynomial-time approximation algorithm whose outputs are provably no worse than those of the trivial algorithm, but often substantially better. A family of problem instances is presented for which our improved algorithm produces optimal solutions, while, for any ε, the trivial algorithm cannot attain a (2- ε)-approximation. These results may lead to the first polynomial-time approximation algorithm that solves the CP-net aggregation problem for swaps with an approximation ratio substantially better than 2",
    "checked": true,
    "id": "771ef620d9789f013b00328e5098f88d7c949b32",
    "semantic_title": "approximation algorithms for preference aggregation using cp-nets",
    "citation_count": 1,
    "authors": [
      "Abu Mohammad Hammad Ali",
      "Boting Yang",
      "Sandra Zilles"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28912": {
    "title": "What Does a Query Answer Tell You? Informativeness of Query Answers for Knowledge Bases",
    "volume": "main",
    "abstract": "Query answering for Knowledge Bases (KBs) amounts to extracting information from the various models of a KB, and presenting the user with an object that represents such information. In the vast majority of cases, this object consists of those tuples of constants that satisfy the query expression either in every model (certain answers) or in some model (possible answers). However, similarly to the case of incomplete databases, both these forms of answers are a lossy representation of all the knowledge inferable from the query and the queried KB. In this paper, we illustrate a formal framework to characterize the information that query answers for KBs are able to represent. As a first application of the framework, we study the informativeness of current query answering approaches, including the recently introduced partial answers. We then define a novel notion of answers, allowing repetition of variables across answer tuples. We show that these answers are capable of representing a meaningful form of information, and we also study their data complexity properties",
    "checked": true,
    "id": "07f26c55b93b5d33ff0a41ebed08140c6f054ada",
    "semantic_title": "what does a query answer tell you? informativeness of query answers for knowledge bases",
    "citation_count": 0,
    "authors": [
      "Luca Andolfi",
      "Gianluca Cima",
      "Marco Console",
      "Maurizio Lenzerini"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28913": {
    "title": "Defeasible Normative Reasoning: A Proof-Theoretic Integration of Logical Argumentation",
    "volume": "main",
    "abstract": "We present a novel computational approach to resolving conflicts among norms by nonmonotonic normative reasoning (in constrained I/O logics). Our approach extends standard sequent-based proof systems and makes them more adequate to nonmonotonic reasoning by adding to the sequents annotations that keep track of what is known about the defeasible status of the derived sequents. This makes transparent the reasons according to which norms should be applicable or inapplicable, and accordingly the sequents that make use of such norms are accepted or retracted. We also show that this proof theoretic method has tight links to the semantics of formal argumentation frameworks. The outcome of this paper is thus a threefold characterization result that relates, in the context of nonmonotonic normative reasoning, three traditional ingredients of AI-based reasoning methods: maximally consistent sets of premises (in constrained I/O logics), derived sequents (which are accepted in corresponding annotated sequent calculi), and logical arguments (that belong to the grounded extensions of the induced logical argumentation frameworks)",
    "checked": true,
    "id": "0e070b867affaa1c44fb535156d9d459425714f0",
    "semantic_title": "defeasible normative reasoning: a proof-theoretic integration of logical argumentation",
    "citation_count": 0,
    "authors": [
      "Ofer Arieli",
      "Kees van Berkel",
      "Christian Straßer"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28914": {
    "title": "Computing the Why-Provenance for Datalog Queries via SAT Solvers",
    "volume": "main",
    "abstract": "Explaining an answer to a Datalog query is an essential task towards Explainable AI, especially nowadays where Datalog plays a critical role in the development of ontology-based applications. A well-established approach for explaining a query answer is the so-called why-provenance, which essentially collects all the subsets of the input database that can be used to obtain that answer via some derivation process, typically represented as a proof tree. It is well known, however, that computing the why-provenance for Datalog queries is computationally expensive, and thus, very few attempts can be found in the literature. The goal of this work is to demonstrate how off-the-shelf SAT solvers can be exploited towards an efficient computation of the why-provenance for Datalog queries. Interestingly, our SAT-based approach allows us to build the why-provenance in an incremental fashion, that is, one explanation at a time, which is much more useful in a practical context than the one-shot computation of the whole set of explanations as done by existing approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marco Calautti",
      "Ester Livshits",
      "Andreas Pieris",
      "Markus Schneider"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28915": {
    "title": "Generalisation through Negation and Predicate Invention",
    "volume": "main",
    "abstract": "The ability to generalise from a small number of examples is a fundamental challenge in machine learning. To tackle this challenge, we introduce an inductive logic programming (ILP) approach that combines negation and predicate invention. Combining these two features allows an ILP system to generalise better by learning rules with universally quantified body-only variables. We implement our idea in NOPI, which can learn normal logic programs with predicate invention, including Datalog programs with stratified negation. Our experimental results on multiple domains show that our approach can improve predictive accuracies and learning times",
    "checked": true,
    "id": "0bdee9a7928b93acf31c59bdea790801a4557ed4",
    "semantic_title": "generalisation through negation and predicate invention",
    "citation_count": 2,
    "authors": [
      "David M. Cerna",
      "Andrew Cropper"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28916": {
    "title": "Learning Small Decision Trees for Data of Low Rank-Width",
    "volume": "main",
    "abstract": "We consider the NP-hard problem of finding a smallest decision tree representing a classification instance in terms of a partially defined Boolean function. Small decision trees are desirable to provide an interpretable model for the given data. We show that the problem is fixed-parameter tractable when parameterized by the rank-width of the incidence graph of the given classification instance. Our algorithm proceeds by dynamic programming using an NLC decomposition obtained from a rank-width decomposition. The key to the algorithm is a succinct representation of partial solutions. This allows us to limit the space and time requirements for each dynamic programming step in terms of the parameter",
    "checked": true,
    "id": "f15c511e37214a1302a6a72a1eb13ad28ece5305",
    "semantic_title": "learning small decision trees for data of low rank-width",
    "citation_count": 0,
    "authors": [
      "Konrad K. Dabrowski",
      "Eduard Eiben",
      "Sebastian Ordyniak",
      "Giacomo Paesani",
      "Stefan Szeider"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28917": {
    "title": "Stable Model Semantics for Description Logic Terminologies",
    "volume": "main",
    "abstract": "This paper studies a stable model semantics for Description Logic (DL) knowledge bases (KBs) and for (possibly cyclic) terminologies, ultimately showing that terminologies under the proposed semantics can be equipped with effective reasoning algorithms. The semantics is derived using Quantified Equilibrium Logic, and---in contrast to the usual semantics of DLs based on classical logic---supports default negation and allows to combine the open-world and the closed-world assumptions in a natural way. Towards understanding the computational properties of this and related formalisms, we show a strong undecidability result that applies not only to KBs under the stable model semantics, but also to the more basic setting of minimal model reasoning. Specifically, we show that concept satisfiability in minimal models of an ALCIO KB is undecidable. We then turn our attention to (possibly cyclic) DL terminologies, where ontological axioms are limited to definitions of concept names in terms of complex concepts. This restriction still yields a very rich setting. We show that standard reasoning problems, like concept satisfiability and subsumption, are ExpTime-complete for terminologies expressed in ALCI under the stable model semantics",
    "checked": true,
    "id": "275be806c72d7132d7c88321dd9a6436a5c0dcc1",
    "semantic_title": "stable model semantics for description logic terminologies",
    "citation_count": 0,
    "authors": [
      "Federica Di Stefano",
      "Mantas Šimkus"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28918": {
    "title": "Redefining ABA+ Semantics via Abstract Set-to-Set Attacks",
    "volume": "main",
    "abstract": "Assumption-based argumentation (ABA) is a powerful defeasible reasoning formalism which is based on the interplay of assumptions, their contraries, and inference rules. ABA with preferences (ABA+) generalizes the basic model by allowing qualitative comparison between assumptions. The integration of preferences however comes with a cost. In ABA+, the evaluation under two central and well-established semantics---grounded and complete semantics---is not guaranteed to yield an outcome. Moreover, while ABA frameworks without preferences allow for a graph-based representation in Dung-style frameworks, an according instantiation for general ABA+ frameworks has not been established so far. In this work, we tackle both issues: First, we develop a novel abstract argumentation formalism based on set-to-set attacks. We show that our so-called Hyper Argumentation Frameworks (HYPAFs) capture ABA+. Second, we propose relaxed variants of complete and grounded semantics for HYPAFs that yield an extension for all frameworks by design, while still faithfully generalizing the established semantics of Dung-style Argumentation Frameworks. We exploit the newly established correspondence between ABA+ and HYPAFs to obtain variants for grounded and complete ABA+ semantics that are guaranteed to yield an outcome. Finally, we discuss basic properties and provide a complexity analysis. Along the way, we settle the computational complexity of several ABA+ semantics",
    "checked": true,
    "id": "88cc2e5d8b817ada6dd8dd3136ff1bf87a9a8169",
    "semantic_title": "redefining aba+ semantics via abstract set-to-set attacks",
    "citation_count": 0,
    "authors": [
      "Yannis Dimopoulos ",
      "Wolfgang Dvorak",
      "Matthias König",
      "Anna Rapberger",
      "Markus Ulbricht",
      "Stefan Woltran"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28919": {
    "title": "Towards Epistemic-Doxastic Planning with Observation and Revision",
    "volume": "main",
    "abstract": "Epistemic planning is useful in situations where multiple agents have different knowledge and beliefs about the world, such as in robot-human interaction. One aspect that has been largely neglected in the literature is planning with observations in the presence of false beliefs. This is a particularly challenging problem because it requires belief revision. We introduce a simple specification language for reasoning about actions with knowledge and belief. We demonstrate our approach on well-known false-belief tasks such as the Sally-Anne Task and compare it to other action languages. Our logic leads to an epistemic planning formalism that is expressive enough to model second-order false-belief tasks, yet has the same computational complexity as classical planning",
    "checked": true,
    "id": "3596046d6562a6a9ad19792dd232a669484587a3",
    "semantic_title": "towards epistemic-doxastic planning with observation and revision",
    "citation_count": 0,
    "authors": [
      "Thorsten Engesser",
      "Andreas Herzig",
      "Elise Perrotin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28920": {
    "title": "Dynamic Tangled Derivative Logic of Metric Spaces",
    "volume": "main",
    "abstract": "Dynamical systems are abstract models of interaction between space and time. They are often used in fields such as physics and engineering to understand complex processes, but due to their general nature, they have found applications for studying computational processes, interaction in multi-agent systems, machine learning algorithms and other computer science related phenomena. In the vast majority of applications, a dynamical system consists of the action of a continuous `transition function' on a metric space. In this work, we consider decidable formal systems for reasoning about such structures. Spatial logics can be traced back to the 1940's, but our work follows a more dynamic turn that these logics have taken due to two recent developments: the study of the topological mu-calculus, and the the integration of linear temporal logic with logics based on the Cantor derivative. In this paper, we combine dynamic topological logics based on the Cantor derivative and the `next point in time' operators with an expressively complete fixed point operator to produce a combination of the topological mu-calculus with linear temporal logic. We show that the resulting logics are decidable and have a natural axiomatisation. Moreover, we prove that these logics are complete for interpretations on the Cantor space, the rational numbers, and subspaces thereof",
    "checked": true,
    "id": "07dba583bf51fe81e5d7905df3ac3dcdc3fe9906",
    "semantic_title": "dynamic tangled derivative logic of metric spaces",
    "citation_count": 0,
    "authors": [
      "David Fernández-Duque",
      "Yoàv Montacute"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28921": {
    "title": "Submodel Enumeration for CTL Is Hard",
    "volume": "main",
    "abstract": "Expressing system specifications using Computation Tree Logic (CTL) formulas, formalising programs using Kripke structures, and then model checking the system is an established workflow in program verification and has wide applications in AI. In this paper, we consider the task of model enumeration, which asks for a uniform stream of output systems that satisfy the given specification. We show that, given a CTL formula and a system (potentially falsified by the formula), enumerating satisfying submodels is always hard for CTL--regardless of which subset of CTL-operators is considered. As a silver lining on the horizon, we present fragments via restrictions on the allowed Boolean functions that still allow for fast enumeration",
    "checked": true,
    "id": "88b0600cb3bb89e39b90ec9e4d15c3802e562c72",
    "semantic_title": "submodel enumeration for ctl is hard",
    "citation_count": 0,
    "authors": [
      "Nicolas Fröhlich",
      "Arne Meier"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28922": {
    "title": "Linear-Time Verification of Data-Aware Processes Modulo Theories via Covers and Automata",
    "volume": "main",
    "abstract": "The need to model and analyse dynamic systems operating over complex data is ubiquitous in AI and neighboring areas, in particular business process management. Analysing such data-aware systems is a notoriously difficult problem, as they are intrinsically infinite-state. Existing approaches work for specific datatypes, and/or limit themselves to the verification of safety properties. In this paper, we lift both such limitations, studying for the first time linear-time verification for so-called data-aware processes modulo theories (DMTs), from the foundational and practical point of view. The DMT model is very general, as it supports processes operating over variables that can store arbitrary types of data, ranging over infinite domains and equipped with domain-specific predicates. Specifically, we provide four contributions. First, we devise a semi-decision procedure for linear-time verification of DMTs, which works for a very large class of datatypes obeying to mild model-theoretic assumptions. The procedure relies on a unique combination of automata-theoretic and cover computation techniques to respectively deal with linear-time properties and datatypes. Second, we identify an abstract, semantic property that guarantees the existence of a faithful finite-state abstraction of the original system, and show that our method becomes a decision procedure in this case. Third, we identify concrete, checkable classes of systems that satisfy this property, generalising several results in the literature. Finally, we present an implementation and an experimental evaluation over a benchmark of real-world data-aware business processes",
    "checked": false,
    "id": "440ad7ec1f177cc19b48f5a2718fdaa9120c0294",
    "semantic_title": "linear-time verification of data-aware processes modulo theories via covers and automata (extended version)",
    "citation_count": 0,
    "authors": [
      "Alessandro Gianola",
      "Marco Montali",
      "Sarah Winkler"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28923": {
    "title": "On the Structural Hardness of Answer Set Programming: Can Structure Efficiently Confine the Power of Disjunctions?",
    "volume": "main",
    "abstract": "Answer Set Programming (ASP) is a generic problem modeling and solving framework with a strong focus on knowledge representation and a rapid growth of industrial applications. So far, the study of complexity resulted in characterizing hardness and determining their sources, fine-grained insights in the form of dichotomy-style results, as well as detailed parameterized complexity landscapes. Unfortunately, for the well-known parameter treewidth disjunctive programs require double-exponential runtime under reasonable complexity assumptions. This quickly becomes out of reach. We deal with the classification of structural parameters for disjunctive ASP on the program's rule structure (incidence graph). First, we provide a polynomial kernel to obtain single-exponential runtime in terms of vertex cover size, despite subset-minimization being not represented in the program's structure. Then we turn our attention to strictly better structural parameters between vertex cover size and treewidth. Here, we provide double-exponential lower bounds for the most prominent parameters in that range: treedepth, feedback vertex size, and cliquewidth. Based on this, we argue that unfortunately our options beyond vertex cover size are limited. Our results provide an in-depth hardness study, relying on a novel reduction from normal to disjunctive programs, trading the increase of complexity for an exponential parameter compression",
    "checked": true,
    "id": "093cce24307b75586f09d5fa084622fcf2e6b084",
    "semantic_title": "on the structural hardness of answer set programming: can structure efficiently confine the power of disjunctions?",
    "citation_count": 0,
    "authors": [
      "Markus Hecher",
      "Rafael Kiesel"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28924": {
    "title": "Knowledge Enhanced Representation Learning for Drug Discovery",
    "volume": "main",
    "abstract": "Recent research on predicting the binding affinity between drug molecules and proteins use representations learned, through unsupervised learning techniques, from large databases of molecule SMILES and protein sequences. While these representations have significantly enhanced the predictions, they are usually based on a limited set of modalities, and they do not exploit available knowledge about existing relations among molecules and proteins. Our study reveals that enhanced representations, derived from multimodal knowledge graphs describing relations among molecules and proteins, lead to state-of-the-art results in well-established benchmarks (first place in the leaderboard for Therapeutics Data Commons benchmark ``Drug-Target Interaction Domain Generalization Benchmark\", with an improvement of 8 points with respect to previous best result). Moreover, our results significantly surpass those achieved in standard benchmarks by using conventional pre-trained representations that rely only on sequence or SMILES data. We release our multimodal knowledge graphs, integrating data from seven public data sources, and which contain over 30 million triples. Pretrained models from our proposed graphs and benchmark task source code are also released",
    "checked": true,
    "id": "7ba985b1d3bd5d2bc4aedd77bd74edc87741042c",
    "semantic_title": "knowledge enhanced representation learning for drug discovery",
    "citation_count": 0,
    "authors": [
      "Thanh Lam Hoang",
      "Marco Luca Sbodio",
      "Marcos Martinez Galindo",
      "Mykhaylo Zayats",
      "Raul Fernandez-Diaz",
      "Victor Valls",
      "Gabriele Picco",
      "Cesar Berrospi",
      "Vanessa Lopez"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28925": {
    "title": "Learning MDL Logic Programs from Noisy Data",
    "volume": "main",
    "abstract": "Many inductive logic programming approaches struggle to learn programs from noisy data. To overcome this limitation, we introduce an approach that learns minimal description length programs from noisy data, including recursive programs. Our experiments on several domains, including drug design, game playing, and program synthesis, show that our approach can outperform existing approaches in terms of predictive accuracies and scale to moderate amounts of noise",
    "checked": true,
    "id": "fd2d8639345cb67f9ff3720bb53122239cc72e60",
    "semantic_title": "learning mdl logic programs from noisy data",
    "citation_count": 2,
    "authors": [
      "Céline Hocquette",
      "Andreas Niskanen",
      "Matti Järvisalo",
      "Andrew Cropper"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28926": {
    "title": "A Compiler for Weak Decomposable Negation Normal Form",
    "volume": "main",
    "abstract": "This paper integrates weak decomposable negation normal form (wDNNF) circuits, introduced by Akshay et al. in 2018, into the knowledge compilation map. This circuit type generalises decomposable negation normal form (DNNF) circuits in such a way that they allow a restricted form of sharing variables among the inputs of a conjunction node. We show that wDNNF circuits have the same properties as DNNF circuits regarding the queries and transformations presented in the knowledge compilation map, whilst being strictly more succinct than DNNF circuits (that is, they can represent Boolean functions compactly). We also present and evaluate a knowledge compiler, called Bella, for converting CNF formulae into wDNNF circuits. Our experiments demonstrate that wDNNF circuits are suitable for configuration instances",
    "checked": true,
    "id": "702a2a15c0c504f273c5ad4d9a9a1ea4ca8aa827",
    "semantic_title": "a compiler for weak decomposable negation normal form",
    "citation_count": 0,
    "authors": [
      "Petr Illner",
      "Petr Kučera"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28927": {
    "title": "Exact ASP Counting with Compact Encodings",
    "volume": "main",
    "abstract": "Answer Set Programming (ASP) has emerged as a promising paradigm in knowledge representation and automated reason- ing owing to its ability to model hard combinatorial problems from diverse domains in a natural way. Building on advances in propositional SAT solving, the past two decades have wit- nessed the emergence of well-engineered systems for solv- ing the answer set satisfiability problem, i.e., finding mod- els or answer sets for a given answer set program. In re- cent years, there has been growing interest in problems be- yond satisfiability, such as model counting, in the context of ASP. Akin to the early days of propositional model count- ing, state-of-the-art exact answer set counters do not scale well beyond small instances. Exact ASP counters struggle with handling larger input formulas. The primary contribu- tion of this paper is a new ASP counting framework, called sharpASP, which counts answer sets avoiding larger input formulas. This relies on an alternative way of defining answer sets that allows lifting of key techniques developed in the con- text of propositional model counting. Our extensive empirical analysis over 1470 benchmarks demonstrates significant per- formance gain over current state-of-the-art exact answer set counters. Specifically, by using sharpASP, we were able to solve 1062 benchmarks with PAR2 score of 3082 whereas using prior state-of-the-art, we could only solve 895 bench- marks with PAR2 score of 4205, all other experimental con- ditions being the same",
    "checked": true,
    "id": "0eae051b66aebf848bf4731bef0411d34ef75a59",
    "semantic_title": "exact asp counting with compact encodings",
    "citation_count": 1,
    "authors": [
      "Mohimenul Kabir",
      "Supratik Chakraborty",
      "Kuldeep S. Meel"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28928": {
    "title": "Minimal Macro-Based Rewritings of Formal Languages: Theory and Applications in Ontology Engineering (and Beyond)",
    "volume": "main",
    "abstract": "In this paper, we introduce the problem of rewriting finite formal languages using syntactic macros such that the rewriting is minimal in size. We present polynomial-time algorithms to solve variants of this problem and show their correctness. To demonstrate the practical relevance of the proposed problems and the feasibility and effectiveness of our algorithms in practice, we apply these to biomedical ontologies authored in OWL. We find that such rewritings can significantly reduce the size of ontologies by capturing repeated expressions with macros. This approach not only offers valuable assistance in enhancing ontology quality and comprehension but can also be seen as a general methodology for evaluating features of rewriting systems (including syntactic macros, templates, or other forms of rewriting rules), which can be analyzed in terms of their influence on computational problems",
    "checked": true,
    "id": "45b46b7cef6625ade02e16c7cc038ffdf095b544",
    "semantic_title": "minimal macro-based rewritings of formal languages: theory and applications in ontology engineering (and beyond)",
    "citation_count": 0,
    "authors": [
      "Christian Kindermann",
      "Anne-Marie George",
      "Bijan Parsia",
      "Uli Sattler"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28929": {
    "title": "On the Expressivity of Recurrent Neural Cascades",
    "volume": "main",
    "abstract": "Recurrent Neural Cascades (RNCs) are the recurrent neural networks with no cyclic dependencies among recurrent neurons. This class of recurrent networks has received a lot of attention in practice. Besides training methods for a fixed architecture such as backpropagation, the cascade architecture naturally allows for constructive learning methods, where recurrent nodes are added incrementally one at a time, often yielding smaller networks. Furthermore, acyclicity amounts to a structural prior that even for the same number of neurons yields a more favourable sample complexity compared to a fully-connected architecture. A central question is whether the advantages of the cascade architecture come at the cost of a reduced expressivity. We provide new insights into this question. We show that the regular languages captured by RNCs with sign and tanh activation with positive recurrent weights are the star-free regular languages. In order to establish our results we developed a novel framework where capabilities of RNCs are assessed by analysing which semigroups and groups a single neuron is able to implement. A notable implication of our framework is that RNCs can achieve the expressivity of all regular languages by introducing neurons that can implement groups",
    "checked": true,
    "id": "21034467d4e1137802f610948c8ad26597400143",
    "semantic_title": "on the expressivity of recurrent neural cascades",
    "citation_count": 0,
    "authors": [
      "Nadezda Alexandrovna Knorozova",
      "Alessandro Ronca"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28930": {
    "title": "Efficient Axiomatization of OWL 2 EL Ontologies from Data by Means of Formal Concept Analysis",
    "volume": "main",
    "abstract": "We present an FCA-based axiomatization method that produces a complete OWL 2 EL TBox (the terminological part of an OWL 2 EL ontology) from a graph dataset in at most exponential time. We describe technical details that allow for efficient implementation as well as variations that dispense with the computation of extremely large axioms, thereby rendering the approach applicable albeit some completeness is lost. Moreover, we evaluate the prototype on real-world datasets",
    "checked": true,
    "id": "9ee40e27becf195e695e276aaf208f2b644087c3",
    "semantic_title": "efficient axiomatization of owl 2 el ontologies from data by means of formal concept analysis",
    "citation_count": 1,
    "authors": [
      "Francesco Kriegel"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28931": {
    "title": "BAIT: Benchmarking (Embedding) Architectures for Interactive Theorem-Proving",
    "volume": "main",
    "abstract": "Artificial Intelligence for Theorem Proving (AITP) has given rise to a plethora of benchmarks and methodologies, particularly in Interactive Theorem Proving (ITP). Research in the area is fragmented, with a diverse set of approaches being spread across several ITP systems. This presents a significant challenge to the comparison of methods, which are often complex and difficult to replicate. Addressing this, we present BAIT, a framework for the fair and streamlined comparison of learning approaches in ITP. We demonstrate BAIT's capabilities with an in-depth comparison, across several ITP benchmarks, of state-of-the-art architectures applied to the problem of formula embedding. We find that Structure Aware Transformers perform particularly well, improving on techniques associated with the original problem sets. BAIT also allows us to assess the end-to-end proving performance of systems built on interactive environments. This unified perspective reveals a novel end-to-end system that improves on prior work. We also provide a qualitative analysis, illustrating that improved performance is associated with more semantically-aware embeddings. By streamlining the implementation and comparison of Machine Learning algorithms in the ITP context, we anticipate BAIT will be a springboard for future research",
    "checked": true,
    "id": "926c790d37d0989790906eceb824bf4fcee770ff",
    "semantic_title": "bait: benchmarking (embedding) architectures for interactive theorem-proving",
    "citation_count": 0,
    "authors": [
      "Sean Lamont",
      "Michael Norrish",
      "Amir Dezfouli",
      "Christian Walder",
      "Paul Montague"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28932": {
    "title": "INFORMEDQX: Informed Conflict Detection for Over-Constrained Problems",
    "volume": "main",
    "abstract": "Conflict detection is relevant in various application scenarios, ranging from interactive decision-making to the diagnosis of faulty knowledge bases. Conflicts can be regarded as sets of constraints that cause an inconsistency. In many scenarios (e.g., constraint-based configuration), conflicts are repeatedly determined for the same or similar sets of constraints. This misses out on the valuable opportunity for leveraging knowledge reuse and related potential performance improvements, which are extremely important, specifically interactive constraint-based applications. In this paper, we show how to integrate knowledge reuse concepts into non-instructive conflict detection. We introduce the InformedQX algorithm, which is a reuse-aware variant of QuickXPlain. The results of a related performance analysis with the Linux-2.6.3.33 configuration knowledge base show significant improvements in terms of runtime performance compared to QuickXPlain",
    "checked": true,
    "id": "c259280ce493f897927bce75850235a155dcc579",
    "semantic_title": "informedqx: informed conflict detection for over-constrained problems",
    "citation_count": 0,
    "authors": [
      "Viet-Man Le",
      "Alexander Felfernig",
      "Thi Ngoc Trang Tran",
      "Mathias Uta"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28933": {
    "title": "Abstraction of Situation Calculus Concurrent Game Structures",
    "volume": "main",
    "abstract": "We present a general framework for abstracting agent behavior in multi-agent synchronous games in the situation calculus, which provides a first-order representation of the state and allows us to model how plays depend on the data and objects involved. We represent such games as action theories of a special form called situation calculus synchronous game structures (SCSGSs), in which we have a single action \"tick\" whose effects depend on the combination of moves selected by the players. In our framework, one specifies both an abstract SCSGS and a concrete SCSGS, as well as a refinement mapping that specifies how each abstract move is implemented by a Golog program defined over the concrete SCSGS. We define notions of sound and complete abstraction with respect to a mapping over such SCSGS. To express strategic properties on the abstract and concrete games we adopt a first-order variant of alternating-time mu-calculus mu-ATL-FO. We show that we can exploit abstraction in verifying mu-ATL-FO properties of SCSGSs under the assumption that agents can always execute abstract moves to completion even if not fully controlling their outcomes",
    "checked": true,
    "id": "f515b122337d1c4f5f4c1a307c8952527c245bf2",
    "semantic_title": "abstraction of situation calculus concurrent game structures",
    "citation_count": 0,
    "authors": [
      "Yves Lesperance",
      "Giuseppe De Giacomo",
      "Maryam Rostamigiv",
      "Shakil M. Khan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28934": {
    "title": "Relational Programming with Foundational Models",
    "volume": "main",
    "abstract": "Foundation models have vast potential to enable diverse AI applications. The powerful yet incomplete nature of these models has spurred a wide range of mechanisms to augment them with capabilities such as in-context learning, information retrieval, and code interpreting. We propose Vieira, a declarative framework that unifies these mechanisms in a general solution for programming with foundation models. Vieira follows a probabilistic relational paradigm and treats foundation models as stateless functions with relational inputs and outputs. It supports neuro-symbolic applications by enabling the seamless combination of such models with logic programs, as well as complex, multi-modal applications by streamlining the composition of diverse sub-models. We implement Vieira by extending the Scallop compiler with a foreign interface that supports foundation models as plugins. We implement plugins for 12 foundation models including GPT, CLIP, and SAM. We evaluate Vieira on 9 challenging tasks that span language, vision, and structured and vector databases. Our evaluation shows that programs in Vieira are concise, can incorporate modern foundation models, and have comparable or better accuracy than competitive baselines",
    "checked": true,
    "id": "a7257249e2279ae67b932cff72ff10b5c2c9062e",
    "semantic_title": "relational programming with foundational models",
    "citation_count": 0,
    "authors": [
      "Ziyang Li",
      "Jiani Huang",
      "Jason Liu",
      "Felix Zhu",
      "Eric Zhao",
      "William Dodds",
      "Neelay Velingker",
      "Rajeev Alur",
      "Mayur Naik"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28935": {
    "title": "MINES: Message Intercommunication for Inductive Relation Reasoning over Neighbor-Enhanced Subgraphs",
    "volume": "main",
    "abstract": "GraIL and its variants have shown their promising capacities for inductive relation reasoning on knowledge graphs. However, the uni-directional message-passing mechanism hinders such models from exploiting hidden mutual relations between entities in directed graphs. Besides, the enclosing subgraph extraction in most GraIL-based models restricts the model from extracting enough discriminative information for reasoning. Consequently, the expressive ability of these models is limited. To address the problems, we propose a novel GraIL-based framework, termed MINES, by introducing a Message Intercommunication mechanism on the Neighbor-Enhanced Subgraph. Concretely, the message intercommunication mechanism is designed to capture the omitted hidden mutual information. It introduces bi-directed information interactions between connected entities by inserting an undirected/bi-directed GCN layer between uni-directed RGCN layers. Moreover, inspired by the success of involving more neighbors in other graph-based tasks, we extend the neighborhood area beyond the enclosing subgraph to enhance the information collection for inductive relation reasoning. Extensive experiments prove the promising capacity of the proposed MINES from various aspects, especially for the superiority, effectiveness, and transfer ability",
    "checked": true,
    "id": "276d530243519a477800a0202c5628185462c917",
    "semantic_title": "mines: message intercommunication for inductive relation reasoning over neighbor-enhanced subgraphs",
    "citation_count": 0,
    "authors": [
      "Ke Liang",
      "Lingyuan Meng",
      "Sihang Zhou",
      "Wenxuan Tu",
      "Siwei Wang",
      "Yue Liu",
      "Meng Liu",
      "Long Zhao",
      "Xiangjun Dong",
      "Xinwang Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28936": {
    "title": "Auditable Algorithms for Approximate Model Counting",
    "volume": "main",
    "abstract": "The problem of model counting, i.e., counting satisfying assignments of a Boolean formula, is a fundamental problem in computer science, with diverse applications. Given #P-hardness of the problem, many algorithms have been developed over the years to provide an approximate model count. Recently, building on the practical success of SAT-solvers used as NP oracles, the focus has shifted from theory to practical implementations of such algorithms. This has brought to focus new challenges. In this paper, we consider one such challenge – that of auditable deterministic approximate model counters wherein a counter should also generate a certificate, which allows a user (often with limited computational power) to independently audit whether the count returned by an invocation of the algorithm is indeed within the promised bounds. We start by examining a celebrated approximate model counting algorithm due to Stockmeyer that uses polynomially many calls to a \\Sigma^2_P oracle, and show that it can be audited via a \\Pi^2_P formula on (n^2 log^2 n) variables, where n is the number of variables in the original formula. Since n is often large (10's to 100's of thousands) for typical instances, we ask if the count of variables in the certificate formula can be reduced – a critical question towards potential implementation. We show that this improvement in certification can be achieved with a tradeoff in the counting algorithm's complexity. Specifically, we develop new deterministic approximate model counting algorithms that invoke a \\Sigma^3_P oracle, but can be certified using a \\Pi^2_P formula on fewer variables: our final algorithm uses just (n log n) variables. Our study demonstrates that one can simplify certificate checking significantly if we allow the counting algorithm to access a slightly more powerful oracle. We believe this shows for the first time how the audit complexity can be traded for the complexity of approximate counting",
    "checked": true,
    "id": "d41f55207216b5fba244ef6a33ad81f815b765ce",
    "semantic_title": "auditable algorithms for approximate model counting",
    "citation_count": 1,
    "authors": [
      "Kuldeep S. Meel",
      "Supratik Chakraborty",
      "S. Akshay"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28937": {
    "title": "A General Theoretical Framework for Learning Smallest Interpretable Models",
    "volume": "main",
    "abstract": "We develop a general algorithmic framework that allows us to obtain fixed-parameter tractability for computing smallest symbolic models that represent given data. Our framework applies to all ML model types that admit a certain extension property. By showing this extension property for decision trees, decision sets, decision lists, and binary decision diagrams, we obtain that minimizing these fundamental model types is fixed-parameter tractable. Our framework even applies to ensembles, which combine individual models by majority decision",
    "checked": true,
    "id": "fa04fba2f8a2a009360c5cef075f7ce43747f1ba",
    "semantic_title": "a general theoretical framework for learning smallest interpretable models",
    "citation_count": 0,
    "authors": [
      "Sebastian Ordyniak",
      "Giacomo Paesani",
      "Mateusz Rychlicki",
      "Stefan Szeider"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28938": {
    "title": "Reinforcement Learning and Data-Generation for Syntax-Guided Synthesis",
    "volume": "main",
    "abstract": "Program synthesis is the task of automatically generating code based on a specification. In Syntax-Guided Synthesis (SyGuS) this specification is a combination of a syntactic template and a logical formula, and the result is guaranteed to satisfy both. We present a reinforcement-learning guided algorithm for SyGuS which uses Monte-Carlo Tree Search (MCTS) to search the space of candidate solutions. Our algorithm learns policy and value functions which, combined with the upper confidence bound for trees, allow it to balance exploration and exploitation. A common challenge in applying machine learning approaches to syntax-guided synthesis is the scarcity of training data. To address this, we present a method for automatically generating training data for SyGuS based on anti-unification of existing first-order satisfiability problems, which we use to train our MCTS policy. We implement and evaluate this setup and demonstrate that learned policy and value improve the synthesis performance over a baseline by over 26 percentage points in the training and testing sets. Our tool outperforms state-of-the-art tool cvc5 on the training set and performs comparably in terms of the total number of problems solved on the testing set (solving 23% of the benchmarks on which cvc5 fails). We make our data set publicly available, to enable further application of machine learning methods to the SyGuS problem",
    "checked": true,
    "id": "efcc35a24c5f3da7bf56b8ce390e1b5a7d77508f",
    "semantic_title": "reinforcement learning and data-generation for syntax-guided synthesis",
    "citation_count": 1,
    "authors": [
      "Julian Parsert",
      "Elizabeth Polgreen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28939": {
    "title": "Adaptive Reactive Synthesis for LTL and LTLf Modulo Theories",
    "volume": "main",
    "abstract": "Reactive synthesis is the process of generate correct con- trollers from temporal logic specifications. Typically, synthesis is restricted to Boolean specifications in LTL. Recently, a Boolean abstraction technique allows to translate LTLT specifications that contain literals in theories into equi-realizable LTL specifications, but no full synthesis procedure exists yet. In synthesis modulo theories, the system receives valuations of environment variables (from a first-order theory T ) and outputs valuations of system variables from T . In this paper, we address how to syntheize a full controller using a combination of the static Boolean controller obtained from the Booleanized LTL specification together with on-the-fly queries to a solver that produces models of satisfiable existential T formulae. This is the first synthesis method for LTL modulo theories. Additionally, our method can produce adaptive responses which increases explainability and can improve runtime properties like performance. Our approach is applicable to both LTL modulo theories and LTLf modulo theories",
    "checked": true,
    "id": "be322a9c02834d81d6e85efcd2f938dcfdee7751",
    "semantic_title": "adaptive reactive synthesis for ltl and ltlf modulo theories",
    "citation_count": 4,
    "authors": [
      "Andoni Rodríguez",
      "César Sánchez"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28940": {
    "title": "A Unified View on Forgetting and Strong Equivalence Notions in Answer Set Programming",
    "volume": "main",
    "abstract": "Answer Set Programming (ASP) is a prominent rule-based language for knowledge representation and reasoning with roots in logic programming and non-monotonic reasoning. The aim to capture the essence of removing (ir)relevant details in ASP programs led to the investigation of different notions, from strong persistence (SP) forgetting, to faithful abstractions, and, recently, strong simplifications, where the latter two can be seen as relaxed and strengthened notions of forgetting, respectively. Although it was observed that these notions are related, especially given that they have characterizations through the semantics for strong equivalence, it remained unclear whether they can be brought together. In this work, we bridge this gap by introducing a novel relativized equivalence notion, which is a relaxation of the recent simplification notion, that is able to capture all related notions from the literature. We provide the necessary and sufficient conditions for relativized simplifiability, which shows that the challenging part is for when the context programs do not contain all the atoms to remove. We then introduce an operator that combines projection and a relaxation of SP-forgetting to obtain the relativized simplifications. We furthermore provide complexity results that complete the overall picture",
    "checked": true,
    "id": "64404c1f25eb2e3c31b01714766f4c51210c8610",
    "semantic_title": "a unified view on forgetting and strong equivalence notions in answer set programming",
    "citation_count": 0,
    "authors": [
      "Zeynep G. Saribatur",
      "Stefan Woltran"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28941": {
    "title": "BeliefFlow: A Framework for Logic-Based Belief Diffusion via Iterated Belief Change",
    "volume": "main",
    "abstract": "This paper presents BeliefFlow, a novel framework for representing how logical beliefs spread among interacting agents within a network. In a Belief Flow Network (BFN), agents communicate asynchronously. The agents' beliefs are represented using epistemic states, which encompass their current beliefs and conditional beliefs guiding future changes. When communication occurs between two connected agents, the receiving agent changes its epistemic state using an improvement operator, a well-known type of rational iterated belief change operator that generalizes belief revision operators. We show that BFNs satisfy appealing properties, leading to two significant outcomes. First, in any BFN with strong network connectivity, the beliefs of all agents converge towards a global consensus. Second, within any BFN, we show that it is possible to compute an optimal strategy for influencing the global beliefs. This strategy, which involves controlling the beliefs of a least number of agents through bribery, can be identified from the topology of the network and can be computed in polynomial time",
    "checked": true,
    "id": "26afafa7ff347343c0048fb806e802482fd95203",
    "semantic_title": "beliefflow: a framework for logic-based belief diffusion via iterated belief change",
    "citation_count": 0,
    "authors": [
      "Nicolas Schwind",
      "Katsumi Inoue",
      "Sébastien Konieczny",
      "Pierre Marquis"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28942": {
    "title": "NegVSR: Augmenting Negatives for Generalized Noise Modeling in Real-world Video Super-Resolution",
    "volume": "main",
    "abstract": "The capability of video super-resolution (VSR) to synthesize high-resolution (HR) video from ideal datasets has been demonstrated in many works. However, applying the VSR model to real-world video with unknown and complex degradation remains a challenging task. First, existing degradation metrics in most VSR methods are not able to effectively simulate real-world noise and blur. On the contrary, simple combinations of classical degradation are used for real-world noise modeling, which led to the VSR model often being violated by out-of-distribution noise. Second, many SR models focus on noise simulation and transfer. Nevertheless, the sampled noise is monotonous and limited. To address the aforementioned problems, we propose a Negatives augmentation strategy for generalized noise modeling in Video Super-Resolution (NegVSR) task. Specifically, we first propose sequential noise generation toward real-world data to extract practical noise sequences. Then, the degeneration domain is widely expanded by negative augmentation to build up various yet challenging real-world noise sets. We further propose the augmented negative guidance loss to learn robust features among augmented negatives effectively. Extensive experiments on real-world datasets (e.g., VideoLQ and FLIR) show that our method outperforms state-of-the-art methods with clear margins, especially in visual quality. Project page is available at: https://negvsr.github.io/",
    "checked": true,
    "id": "8b9c3f626fe3dba91aadbc10dfc028dca665ea3c",
    "semantic_title": "negvsr: augmenting negatives for generalized noise modeling in real-world video super-resolution",
    "citation_count": 1,
    "authors": [
      "Yexing Song",
      "Meilin Wang",
      "Zhijing Yang",
      "Xiaoyu Xian",
      "Yukai Shi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28943": {
    "title": "Scalable Enumeration of Trap Spaces in Boolean Networks via Answer Set Programming",
    "volume": "main",
    "abstract": "Boolean Networks (BNs) are widely used as a modeling formalism in several domains, notably systems biology and computer science. A fundamental problem in BN analysis is the enumeration of trap spaces, which are hypercubes in the state space that cannot be escaped once entered. Several methods have been proposed for enumerating trap spaces, however they often suffer from scalability and efficiency issues, particularly for large and complex models. To our knowledge, the most efficient and recent methods for the trap space enumeration all rely on Answer Set Programming (ASP), which has been widely applied to the analysis of BNs. Motivated by these considerations, our work proposes a new method for enumerating trap spaces in BNs using ASP. We evaluate the method on a mix of 250+ real-world and 400+ randomly generated BNs, showing that it enables analysis of models beyond the capabilities of existing tools (namely pyboolnet, mpbn, trappist, and trapmvn)",
    "checked": true,
    "id": "09ce280e2deda5d894c330319bcd4333c4ada55e",
    "semantic_title": "scalable enumeration of trap spaces in boolean networks via answer set programming",
    "citation_count": 1,
    "authors": [
      "Giang Trinh",
      "Belaid Benhamou",
      "Samuel Pastva",
      "Sylvain Soliman"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28944": {
    "title": "Non-flat ABA Is an Instance of Bipolar Argumentation",
    "volume": "main",
    "abstract": "Assumption-based Argumentation (ABA) is a well-known structured argumentation formalism, whereby arguments and attacks between them are drawn from rules, defeasible assumptions and their contraries. A common restriction imposed on ABA frameworks (ABAFs) is that they are flat, i.e. each of the defeasible assumptions can only be assumed, but not derived. While it is known that flat ABAFs can be translated into abstract argumentation frameworks (AFs) as proposed by Dung, no translation exists from general, possibly non-flat ABAFs into any kind of abstract argumentation formalism. In this paper, we close this gap and show that bipolar AFs (BAFs) can instantiate general ABAFs. To this end we develop suitable, novel BAF semantics which borrow from the notion of deductive support. We investigate basic properties of our BAFs, including computational complexity, and prove the desired relation to ABAFs under several semantics",
    "checked": true,
    "id": "64274713fdd5c8162d9e3d3960456e07908cd6c9",
    "semantic_title": "non-flat aba is an instance of bipolar argumentation",
    "citation_count": 2,
    "authors": [
      "Markus Ulbricht",
      "Nico Potyka",
      "Anna Rapberger",
      "Francesca Toni"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28945": {
    "title": "Bilateral Gradual Semantics for Weighted Argumentation",
    "volume": "main",
    "abstract": "Abstract argumentation is a reasoning model for evaluating arguments. Recently, gradual semantics has received considerable attention in weighted argumentation, which assigns an acceptability degree to each argument as its strength. In this paper, we aim to enhance gradual semantics by non-reciprocally incorporating the notion of rejectability degree. Such a setting offers a bilateral perspective on argument strength, enabling more comprehensive argument evaluations in practical situations. To this end, we first provide a set of principles for our semantics, taking both the acceptability and rejectability degrees into account, and propose three novel semantics conforming to the above principles. These semantics are defined as the limits of iterative sequences that always converge in any given weighted argumentation system, making them preferable for real-world applications",
    "checked": true,
    "id": "8afdd877478a11cf05c54278d5a535c12b589d41",
    "semantic_title": "bilateral gradual semantics for weighted argumentation",
    "citation_count": 1,
    "authors": [
      "Zongshun Wang",
      "Yuping Shen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28946": {
    "title": "Decomposing Constraint Networks for Calculating c-Representations",
    "volume": "main",
    "abstract": "It is well-known from probability theory that network-based methods like Bayesian networks constitute remarkable frameworks for efficient probabilistic reasoning. In this paper, we focus on qualitative default reasoning based on Spohn's ranking functions for which network-based methods have not yet been studied satisfactorily. With constraint networks, we develop a framework for iterative calculations of c-representations, a family of ranking models of conditional belief bases which show outstanding properties from a commonsense and formal point of view, that are characterized by assigning possible worlds a degree of implausibility via penalizing the falsification of conditionals. Constraint networks unveil the dependencies among these penalty points (and hence among the conditionals) and make it possible to compute the penalty points locally on so-called safe sub-bases. As an application of our framework, we show that skeptical c-inferences can be drawn locally from safe sub-bases without losing validity",
    "checked": true,
    "id": "f48bc10541c39bf3ce6940e8ee9c6d4c7df76386",
    "semantic_title": "decomposing constraint networks for calculating c-representations",
    "citation_count": 0,
    "authors": [
      "Marco Wilhelm",
      "Gabriele Kern-Isberner"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28947": {
    "title": "Optimised Storage for Datalog Reasoning",
    "volume": "main",
    "abstract": "Materialisation facilitates Datalog reasoning by precomputing all consequences of the facts and the rules so that queries can be directly answered over the materialised facts. However, storing all materialised facts may be infeasible in practice, especially when the rules are complex and the given set of facts is large. We observe that for certain combinations of rules, there exist data structures that compactly represent the reasoning result and can be efficiently queried when necessary. In this paper, we present a general framework that allows for the integration of such optimised storage schemes with standard materialisation algorithms. Moreover, we devise optimised storage schemes targeting at transitive rules and union rules, two types of (combination of) rules that commonly occur in practice. Our experimental evaluation shows that our approach significantly improves memory consumption, sometimes by orders of magnitude, while remaining competitive in terms of query answering time",
    "checked": true,
    "id": "f1f1d8361c0a7df7037e5907dc5195dbe372fca8",
    "semantic_title": "optimised storage for datalog reasoning",
    "citation_count": 0,
    "authors": [
      "Xinyue Zhang",
      "Pan Hu",
      "Yavor Nenov",
      "Ian Horrocks"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28948": {
    "title": "Prot2Text: Multimodal Protein's Function Generation with GNNs and Transformers",
    "volume": "main",
    "abstract": "In recent years, significant progress has been made in the field of protein function prediction with the development of various machine-learning approaches. However, most existing methods formulate the task as a multi-classification problem, i.e. assigning predefined labels to proteins. In this work, we propose a novel approach, Prot2Text, which predicts a protein's function in a free text style, moving beyond the conventional binary or categorical classifications. By combining Graph Neural Networks(GNNs) and Large Language Models(LLMs), in an encoder-decoder framework, our model effectively integrates diverse data types including protein sequence, structure, and textual annotation and description. This multimodal approach allows for a holistic representation of proteins' functions, enabling the generation of detailed and accurate functional descriptions. To evaluate our model, we extracted a multimodal protein dataset from SwissProt, and demonstrate empirically the effectiveness of Prot2Text. These results highlight the transformative impact of multimodal models, specifically the fusion of GNNs and LLMs, empowering researchers with powerful tools for more accurate function prediction of existing as well as first-to-see proteins",
    "checked": true,
    "id": "de7e5fee8cf03bd485b1104d3e40e8ab45d76c0a",
    "semantic_title": "prot2text: multimodal protein's function generation with gnns and transformers",
    "citation_count": 18,
    "authors": [
      "Hadi Abdine",
      "Michail Chatzianastasis",
      "Costas Bouyioukos",
      "Michalis Vazirgiannis"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28949": {
    "title": "Unsupervised Neighborhood Propagation Kernel Layers for Semi-supervised Node Classification",
    "volume": "main",
    "abstract": "We present a deep Graph Convolutional Kernel Machine (GCKM) for semi-supervised node classification in graphs. The method is built of two main types of blocks: (i) We introduce unsupervised kernel machine layers propagating the node features in a one-hop neighborhood, using implicit node feature mappings. (ii) We specify a semi-supervised classification kernel machine through the lens of the Fenchel-Young inequality. We derive an effective initialization scheme and efficient end-to-end training algorithm in the dual variables for the full architecture. The main idea underlying GCKM is that, because of the unsupervised core, the final model can achieve higher performance in semi-supervised node classification when few labels are available for training. Experimental results demonstrate the effectiveness of the proposed framework",
    "checked": true,
    "id": "757cb2d5ddd785cafe6e801683b33724d4d35826",
    "semantic_title": "unsupervised neighborhood propagation kernel layers for semi-supervised node classification",
    "citation_count": 3,
    "authors": [
      "Sonny Achten",
      "Francesco Tonin",
      "Panagiotis Patrinos",
      "Johan A.K. Suykens"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28950": {
    "title": "No Prejudice! Fair Federated Graph Neural Networks for Personalized Recommendation",
    "volume": "main",
    "abstract": "Ensuring fairness in Recommendation Systems (RSs) across demographic groups is critical due to the increased integration of RSs in applications such as personalized healthcare, finance, and e-commerce. Graph-based RSs play a crucial role in capturing intricate higher-order interactions among entities. However, integrating these graph models into the Federated Learning (FL) paradigm with fairness constraints poses formidable challenges as this requires access to the entire interaction graph and sensitive user information (such as gender, age, etc.) at the central server. This paper addresses the pervasive issue of inherent bias within RSs for different demographic groups without compromising the privacy of sensitive user attributes in FL environment with the graph-based model. To address the group bias, we propose F2PGNN (Fair Federated Personalized Graph Neural Network), a novel framework that leverages the power of Personalized Graph Neural Network (GNN) coupled with fairness considerations. Additionally, we use differential privacy techniques to fortify privacy protection. Experimental evaluation on three publicly available datasets showcases the efficacy of F2PGNN in mitigating group unfairness by 47% ∼ 99% compared to the state-of-the-art while preserving privacy and maintaining the utility. The results validate the significance of our framework in achieving equitable and personalized recommendations using GNN within the FL landscape. Source code is at: https://github.com/nimeshagrawal/F2PGNN-AAAI24",
    "checked": true,
    "id": "4d74f9322af338b874ed1d29cdc2c3b43fe4c568",
    "semantic_title": "no prejudice! fair federated graph neural networks for personalized recommendation",
    "citation_count": 2,
    "authors": [
      "Nimesh Agrawal",
      "Anuj Kumar Sirohi",
      "Sandeep Kumar",
      "Jayadeva"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28951": {
    "title": "Pareto Front-Diverse Batch Multi-Objective Bayesian Optimization",
    "volume": "main",
    "abstract": "We consider the problem of multi-objective optimization (MOO) of expensive black-box functions with the goal of discovering high-quality and diverse Pareto fronts where we are allowed to evaluate a batch of inputs. This problem arises in many real-world applications including penicillin production where diversity of solutions is critical. We solve this problem in the framework of Bayesian optimization (BO) and propose a novel approach referred to as Pareto front-Diverse Batch Multi-Objective BO (PDBO). PDBO tackles two important challenges: 1) How to automatically select the best acquisition function in each BO iteration, and 2) How to select a diverse batch of inputs by considering multiple objectives. We propose principled solutions to address these two challenges. First, PDBO employs a multi-armed bandit approach to select one acquisition function from a given library. We solve a cheap MOO problem by assigning the selected acquisition function for each expensive objective function to obtain a candidate set of inputs for evaluation. Second, it utilizes Determinantal Point Processes (DPPs) to choose a Pareto-front-diverse batch of inputs for evaluation from the candidate set obtained from the first step. The key parameters for the methods behind these two steps are updated after each round of function evaluations. Experiments on multiple MOO benchmarks demonstrate that PDBO outperforms prior methods in terms of both the quality and diversity of Pareto solutions",
    "checked": true,
    "id": "017435814af0e52375306012018535a3b286cf85",
    "semantic_title": "pareto front-diverse batch multi-objective bayesian optimization",
    "citation_count": 0,
    "authors": [
      "Alaleh Ahmadianshalchi",
      "Syrine Belakaria",
      "Janardhan Rao Doppa"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28952": {
    "title": "SimCS: Simulation for Domain Incremental Online Continual Segmentation",
    "volume": "main",
    "abstract": "Continual Learning is a step towards lifelong intelligence where models continuously learn from recently collected data without forgetting previous knowledge. Existing continual learning approaches mostly focus on image classification in the class-incremental setup with clear task boundaries and unlimited computational budget. This work explores the problem of Online Domain-Incremental Continual Segmentation (ODICS), where the model is continually trained over batches of densely labeled images from different domains, with limited computation and no information about the task boundaries. ODICS arises in many practical applications. In autonomous driving, this may correspond to the realistic scenario of training a segmentation model over time on a sequence of cities. We analyze several existing continual learning methods and show that they perform poorly in this setting despite working well in class-incremental segmentation. We propose SimCS, a parameter-free method complementary to existing ones that uses simulated data to regularize continual learning. Experiments show that SimCS provides consistent improvements when combined with different CL methods",
    "checked": true,
    "id": "a400ea218429d6df1df55dac2ee88cf78f659629",
    "semantic_title": "simcs: simulation for domain incremental online continual segmentation",
    "citation_count": 3,
    "authors": [
      "Motasem Alfarra",
      "Zhipeng Cai",
      "Adel Bibi",
      "Bernard Ghanem",
      "Matthias Müller"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28953": {
    "title": "Sample Efficient Reinforcement Learning with Partial Dynamics Knowledge",
    "volume": "main",
    "abstract": "The problem of sample complexity of online reinforcement learning is often studied in the literature without taking into account any partial knowledge about the system dynamics that could potentially accelerate the learning process. In this paper, we study the sample complexity of online Q-learning methods when some prior knowledge about the dynamics is available or can be learned efficiently. We focus on systems that evolve according to an additive disturbance model of the form S_{h+1} = ƒ(S_h, A_h) + W_h, where ƒ represents the underlying system dynamics, and W_h are unknown disturbances independent of states and actions. In the setting of finite episodic Markov decision processes with S states, A actions, and episode length H, we present an optimistic Q-learning algorithm that achieves Õ(Poly(H)√T) regret under perfect knowledge of ƒ, where T is the total number of interactions with the system. This is in contrast to the typical Õ(Poly(H)√SAT) regret for existing Q-learning methods. Further, if only a noisy estimate ƒ_hat of ƒ is available, our method can learn an approximately optimal policy in a number of samples that is independent of the cardinalities of state and action spaces. The sub-optimality gap depends on the approximation error ƒ_hat − ƒ, as well as the Lipschitz constant of the corresponding optimal value function. Our approach does not require modeling of the transition probabilities and enjoys the same memory complexity as model-free methods",
    "checked": true,
    "id": "8364ae9b3f7bd8533af069b854498b7e389176b8",
    "semantic_title": "sample efficient reinforcement learning with partial dynamics knowledge",
    "citation_count": 0,
    "authors": [
      "Meshal Alharbi",
      "Mardavij Roozbehani",
      "Munther Dahleh"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28954": {
    "title": "Understanding and Improving Optimization in Predictive Coding Networks",
    "volume": "main",
    "abstract": "Backpropagation (BP), the standard learning algorithm for artificial neural networks, is often considered biologically implausible. In contrast, the standard learning algorithm for predictive coding (PC) models in neuroscience, known as the inference learning algorithm (IL), is a promising, bio-plausible alternative. However, several challenges and questions hinder IL's application to real-world problems. For example, IL is computationally demanding, and without memory-intensive optimizers like Adam, IL may converge to poor local minima. Moreover, although IL can reduce loss more quickly than BP, the reasons for these speedups or their robustness remains unclear. In this paper, we tackle these challenges by 1) altering the standard implementation of PC circuits to substantially reduce computation, 2) developing a novel optimizer that improves the convergence of IL without increasing memory usage, and 3) establishing theoretical results that help elucidate the conditions under which IL is sensitive to second and higher-order information",
    "checked": true,
    "id": "a2ca7d4a1364c2912939248865b526995eac9a67",
    "semantic_title": "understanding and improving optimization in predictive coding networks",
    "citation_count": 4,
    "authors": [
      "Nicholas Alonso",
      "Jeffrey Krichmar",
      "Emre Neftci"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28955": {
    "title": "Limited Memory Online Gradient Descent for Kernelized Pairwise Learning with Dynamic Averaging",
    "volume": "main",
    "abstract": "Pairwise learning, an important domain within machine learning, addresses loss functions defined on pairs of training examples, including those in metric learning and AUC maximization. Acknowledging the quadratic growth in computation complexity accompanying pairwise loss as the sample size grows, researchers have turned to online gradient descent (OGD) methods for enhanced scalability. Recently, an OGD algorithm emerged, employing gradient computation involving prior and most recent examples, a step that effectively reduces algorithmic complexity to O(T), with T being the number of received examples. This approach, however, confines itself to linear models while assuming the independence of example arrivals. We introduce a lightweight OGD algorithm that does not require the independence of examples and generalizes to kernel pairwise learning. Our algorithm builds the gradient based on a random example and a moving average representing the past data, which results in a sub-linear regret bound with a complexity of O(T). Furthermore, through the integration of O(√T logT) random Fourier features, the complexity of kernel calculations is effectively minimized. Several experiments with real-world datasets show that the proposed technique outperforms kernel and linear algorithms in offline and online scenarios",
    "checked": true,
    "id": "7e62417c144a59928ad55e275e7c99cd608de11a",
    "semantic_title": "limited memory online gradient descent for kernelized pairwise learning with dynamic averaging",
    "citation_count": 0,
    "authors": [
      "Hilal AlQuabeh",
      "William de Vazelhes",
      "Bin Gu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28956": {
    "title": "Faithful Model Explanations through Energy-Constrained Conformal Counterfactuals",
    "volume": "main",
    "abstract": "Counterfactual explanations offer an intuitive and straightforward way to explain black-box models and offer algorithmic recourse to individuals. To address the need for plausible explanations, existing work has primarily relied on surrogate models to learn how the input data is distributed. This effectively reallocates the task of learning realistic explanations for the data from the model itself to the surrogate. Consequently, the generated explanations may seem plausible to humans but need not necessarily describe the behaviour of the black-box model faithfully. We formalise this notion of faithfulness through the introduction of a tailored evaluation metric and propose a novel algorithmic framework for generating Energy-Constrained Conformal Counterfactuals that are only as plausible as the model permits. Through extensive empirical studies, we demonstrate that ECCCo reconciles the need for faithfulness and plausibility. In particular, we show that for models with gradient access, it is possible to achieve state-of-the-art performance without the need for surrogate models. To do so, our framework relies solely on properties defining the black-box model itself by leveraging recent advances in energy-based modelling and conformal prediction. To our knowledge, this is the first venture in this direction for generating faithful counterfactual explanations. Thus, we anticipate that ECCCo can serve as a baseline for future research. We believe that our work opens avenues for researchers and practitioners seeking tools to better distinguish trustworthy from unreliable models",
    "checked": true,
    "id": "1c56c092cfd0807164515af9b5bb0843bc38e96c",
    "semantic_title": "faithful model explanations through energy-constrained conformal counterfactuals",
    "citation_count": 1,
    "authors": [
      "Patrick Altmeyer",
      "Mojtaba Farmanbar",
      "Arie van Deursen",
      "Cynthia C. S. Liem"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28957": {
    "title": "Optimal Transport with Tempered Exponential Measures",
    "volume": "main",
    "abstract": "In the field of optimal transport, two prominent subfields face each other: (i) unregularized optimal transport, ``a-la-Kantorovich'', which leads to extremely sparse plans but with algorithms that scale poorly, and (ii) entropic-regularized optimal transport, ``a-la-Sinkhorn-Cuturi'', which gets near-linear approximation algorithms but leads to maximally un-sparse plans. In this paper, we show that an extension of the latter to tempered exponential measures, a generalization of exponential families with indirect measure normalization, gets to a very convenient middle ground, with both very fast approximation algorithms and sparsity, which is under control up to sparsity patterns. In addition, our formulation fits naturally in the unbalanced optimal transport problem setting",
    "checked": true,
    "id": "3ca4f3f849eb555db784aafa7d0339a1ef2e00f8",
    "semantic_title": "optimal transport with tempered exponential measures",
    "citation_count": 3,
    "authors": [
      "Ehsan Amid",
      "Frank Nielsen",
      "Richard Nock",
      "Manfred K. Warmuth"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28958": {
    "title": "Elijah: Eliminating Backdoors Injected in Diffusion Models via Distribution Shift",
    "volume": "main",
    "abstract": "Diffusion models (DM) have become state-of-the-art generative models because of their capability of generating high-quality images from noises without adversarial training. However, they are vulnerable to backdoor attacks as reported by recent studies. When a data input (e.g., some Gaussian noise) is stamped with a trigger (e.g., a white patch), the backdoored model always generates the target image (e.g., an improper photo). However, effective defense strategies to mitigate backdoors from DMs are underexplored. To bridge this gap, we propose the first backdoor detection and removal framework for DMs. We evaluate our framework Elijah on over hundreds of DMs of 3 types including DDPM, NCSN and LDM, with 13 samplers against 3 existing backdoor attacks. Extensive experiments show that our approach can have close to 100% detection accuracy and reduce the backdoor effects to close to zero without significantly sacrificing the model utility",
    "checked": true,
    "id": "180f8781ea06316d8abef13a6cf1bfe0fef9093c",
    "semantic_title": "elijah: eliminating backdoors injected in diffusion models via distribution shift",
    "citation_count": 7,
    "authors": [
      "Shengwei An",
      "Sheng-Yen Chou",
      "Kaiyuan Zhang",
      "Qiuling Xu",
      "Guanhong Tao",
      "Guangyu Shen",
      "Siyuan Cheng",
      "Shiqing Ma",
      "Pin-Yu Chen",
      "Tsung-Yi Ho",
      "Xiangyu Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28959": {
    "title": "Transfer and Alignment Network for Generalized Category Discovery",
    "volume": "main",
    "abstract": "Generalized Category Discovery (GCD) is a crucial real-world task that aims to recognize both known and novel categories from an unlabeled dataset by leveraging another labeled dataset with only known categories. Despite the improved performance on known categories, current methods perform poorly on novel categories. We attribute the poor performance to two reasons: biased knowledge transfer between labeled and unlabeled data and noisy representation learning on the unlabeled data. The former leads to unreliable estimation of learning targets for novel categories and the latter hinders models from learning discriminative features. To mitigate these two issues, we propose a Transfer and Alignment Network (TAN), which incorporates two knowledge transfer mechanisms to calibrate the biased knowledge and two feature alignment mechanisms to learn discriminative features. Specifically, we model different categories with prototypes and transfer the prototypes in labeled data to correct model bias towards known categories. On the one hand, we pull instances with known categories in unlabeled data closer to these prototypes to form more compact clusters and avoid boundary overlap between known and novel categories. On the other hand, we use these prototypes to calibrate noisy prototypes estimated from unlabeled data based on category similarities, which allows for more accurate estimation of prototypes for novel categories that can be used as reliable learning targets later. After knowledge transfer, we further propose two feature alignment mechanisms to acquire both instance- and category-level knowledge from unlabeled data by aligning instance features with both augmented features and the calibrated prototypes, which can boost model performance on both known and novel categories with less noise. Experiments on three benchmark datasets show that our model outperforms SOTA methods, especially on novel categories. Theoretical analysis is provided for an in-depth understanding of our model in general. Our code and data are available at https://github.com/Lackel/TAN",
    "checked": true,
    "id": "be817cd931182d516569768be888bd3631c13d4a",
    "semantic_title": "transfer and alignment network for generalized category discovery",
    "citation_count": 3,
    "authors": [
      "Wenbin An",
      "Feng Tian",
      "Wenkai Shi",
      "Yan Chen",
      "Yaqiang Wu",
      "Qianying Wang",
      "Ping Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28960": {
    "title": "Fluctuation-Based Adaptive Structured Pruning for Large Language Models",
    "volume": "main",
    "abstract": "Network Pruning is a promising way to address the huge computing resource demands of the deployment and inference of Large Language Models (LLMs). Retraining-free is important for LLMs' pruning methods. However, almost all of the existing retraining-free pruning approaches for LLMs focus on unstructured pruning, which requires specific hardware support for acceleration. In this paper, we propose a novel retraining-free structured pruning framework for LLMs, named FLAP (FLuctuation-based Adaptive Structured Pruning). It is hardware-friendly by effectively reducing storage and enhancing inference speed. For effective structured pruning of LLMs, we highlight three critical elements that demand the utmost attention: formulating structured importance metrics, adaptively searching the global compressed model, and implementing compensation mechanisms to mitigate performance loss. First, FLAP determines whether the output feature map is easily recoverable when a column of weight is removed, based on the fluctuation pruning metric. Then it standardizes the importance scores to adaptively determine the global compressed model structure. At last, FLAP adds additional bias terms to recover the output feature maps using the baseline values. We thoroughly evaluate our approach on a variety of language benchmarks. Without any retraining, our method significantly outperforms the state-of-the-art methods, including LLM-Pruner and the extension of Wanda in structured pruning. The code is released at https://github.com/CASIA-IVA-Lab/FLAP",
    "checked": true,
    "id": "9ea8001d6eb52d14134ceede7f88b1d8fa8db41f",
    "semantic_title": "fluctuation-based adaptive structured pruning for large language models",
    "citation_count": 7,
    "authors": [
      "Yongqi An",
      "Xu Zhao",
      "Tao Yu",
      "Ming Tang",
      "Jinqiao Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28961": {
    "title": "Active Learning Guided by Efficient Surrogate Learners",
    "volume": "main",
    "abstract": "Re-training a deep learning model each time a single data point receives a new label is impractical due to the inherent complexity of the training process. Consequently, existing active learning (AL) algorithms tend to adopt a batch-based approach where, during each AL iteration, a set of data points is collectively chosen for annotation. However, this strategy frequently leads to redundant sampling, ultimately eroding the efficacy of the labeling procedure. In this paper, we introduce a new AL algorithm that harnesses the power of a Gaussian process surrogate in conjunction with the neural network principal learner. Our proposed model adeptly updates the surrogate learner for every new data instance, enabling it to emulate and capitalize on the continuous learning dynamics of the neural network without necessitating a complete retraining of the principal model for each individual label. Experiments on four benchmark datasets demonstrate that this approach yields significant enhancements, either rivaling or aligning with the performance of state-of-the-art techniques",
    "checked": true,
    "id": "2bc1707d8edb8748af096dfbbf111c353811ac3b",
    "semantic_title": "active learning guided by efficient surrogate learners",
    "citation_count": 0,
    "authors": [
      "Yunpyo An",
      "Suyeong Park",
      "Kwang In Kim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28962": {
    "title": "Formal Logic Enabled Personalized Federated Learning through Property Inference",
    "volume": "main",
    "abstract": "Recent advancements in federated learning (FL) have greatly facilitated the development of decentralized collaborative applications, particularly in the domain of Artificial Intelligence of Things (AIoT). However, a critical aspect missing from the current research landscape is the ability to enable data-driven client models with symbolic reasoning capabilities. Specifically, the inherent heterogeneity of participating client devices poses a significant challenge, as each client exhibits unique logic reasoning properties. Failing to consider these device-specific specifications can result in critical properties being missed in the client predictions, leading to suboptimal performance. In this work, we propose a new training paradigm that leverages temporal logic reasoning to address this issue. Our approach involves enhancing the training process by incorporating mechanically generated logic expressions for each FL client. Additionally, we introduce the concept of aggregation clusters and develop a partitioning algorithm to effectively group clients based on the alignment of their temporal reasoning properties. We evaluate the proposed method on two tasks: a real-world traffic volume prediction task consisting of sensory data from fifteen states and a smart city multi-task prediction utilizing synthetic data. The evaluation results exhibit clear improvements, with performance accuracy improved by up to 54% across all sequential prediction models",
    "checked": true,
    "id": "c4139d08cab3804063be6e7bd72a5b6572ef8e96",
    "semantic_title": "formal logic enabled personalized federated learning through property inference",
    "citation_count": 0,
    "authors": [
      "Ziyan An",
      "Taylor T. Johnson",
      "Meiyi Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28963": {
    "title": "Generating Universal Adversarial Perturbations for Quantum Classifiers",
    "volume": "main",
    "abstract": "Quantum Machine Learning (QML) has emerged as a promising field of research, aiming to leverage the capabilities of quantum computing to enhance existing machine learning methodologies. Recent studies have revealed that, like their classical counterparts, QML models based on Parametrized Quantum Circuits (PQCs) are also vulnerable to adversarial attacks. Moreover, the existence of Universal Adversarial Perturbations (UAPs) in the quantum domain has been demonstrated theoretically in the context of quantum classifiers. In this work, we introduce QuGAP: a novel framework for generating UAPs for quantum classifiers. We conceptualize the notion of additive UAPs for PQC-based classifiers and theoretically demonstrate their existence. We then utilize generative models (QuGAP-A) to craft additive UAPs and experimentally show that quantum classifiers are susceptible to such attacks. Moreover, we formulate a new method for generating unitary UAPs (QuGAP-U) using quantum generative models and a novel loss function based on fidelity constraints. We evaluate the performance of the proposed framework and show that our method achieves state-of-the-art misclassification rates, while maintaining high fidelity between legitimate and adversarial samples",
    "checked": true,
    "id": "e1a68264a3bd5a88b693dac33aedb07061211e02",
    "semantic_title": "generating universal adversarial perturbations for quantum classifiers",
    "citation_count": 0,
    "authors": [
      "Gautham Anil",
      "Vishnu Vinod",
      "Apurva Narayan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28964": {
    "title": "Enhancing Training of Spiking Neural Network with Stochastic Latency",
    "volume": "main",
    "abstract": "Spiking neural networks (SNNs) have garnered significant attention for their low power consumption when deployed on neuromorphic hardware that operates in orders of magnitude lower power than general-purpose hardware. Direct training methods for SNNs come with an inherent latency for which the SNNs are optimized, and in general, the higher the latency, the better the predictive powers of the models, but at the same time, the higher the energy consumption during training and inference. Furthermore, an SNN model optimized for one particular latency does not necessarily perform well in lower latencies, which becomes relevant in scenarios where it is necessary to switch to a lower latency because of the depletion of onboard energy or other operational requirements. In this work, we propose Stochastic Latency Training (SLT), a direct training method for SNNs that optimizes the model for the given latency but simultaneously offers a minimum reduction of predictive accuracy when shifted to lower inference latencies. We provide heuristics for our approach with partial theoretical justification and experimental evidence showing the state-of-the-art performance of our models on datasets such as CIFAR-10, DVS-CIFAR-10, CIFAR-100, and DVS-Gesture. Our code is available at https://github.com/srinuvaasu/SLT",
    "checked": true,
    "id": "9f74b1c637cf7ed89d425b57bb1ca100f29bc3a7",
    "semantic_title": "enhancing training of spiking neural network with stochastic latency",
    "citation_count": 0,
    "authors": [
      "Srinivas Anumasa",
      "Bhaskar Mukhoty",
      "Velibor Bojkovic",
      "Giulia De Masi",
      "Huan Xiong",
      "Bin Gu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28965": {
    "title": "Task-Agnostic Privacy-Preserving Representation Learning for Federated Learning against Attribute Inference Attacks",
    "volume": "main",
    "abstract": "Federated learning (FL) has been widely studied recently due to its property to collaboratively train data from different devices without sharing the raw data. Nevertheless, recent studies show that an adversary can still be possible to infer private information about devices' data, e.g., sensitive attributes such as income, race, and sexual orientation. To mitigate the attribute inference attacks, various existing privacy-preserving FL methods can be adopted/adapted. However, all these existing methods have key limitations: they need to know the FL task in advance, or have intolerable computational overheads or utility losses, or do not have provable privacy guarantees. We address these issues and design a task-agnostic privacy-preserving presentation learning method for FL (TAPPFL) against attribute inference attacks. TAPPFL is formulated via information theory. Specifically, TAPPFL has two mutual information goals, where one goal learns task-agnostic data representations that contain the least information about the private attribute in each device's data, and the other goal ensures the learnt data representations include as much information as possible about the device data to maintain FL utility. We also derive privacy guarantees of TAPPFL against worst-case attribute inference attacks, as well as the inherent tradeoff between utility preservation and privacy protection. Extensive results on multiple datasets and applications validate the effectiveness of TAPPFL to protect data privacy, maintain the FL utility, and be efficient as well. Experimental results also show that TAPPFL outperforms the existing defenses",
    "checked": true,
    "id": "fc777ee2a6a9d31d8967830d18fccf26841f4279",
    "semantic_title": "task-agnostic privacy-preserving representation learning for federated learning against attribute inference attacks",
    "citation_count": 1,
    "authors": [
      "Caridad Arroyo Arevalo",
      "Sayedeh Leila Noorbakhsh",
      "Yun Dong",
      "Yuan Hong",
      "Binghui Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28966": {
    "title": "Neural Network Approximators for Marginal MAP in Probabilistic Circuits",
    "volume": "main",
    "abstract": "Probabilistic circuits (PCs) such as sum-product networks efficiently represent large multi-variate probability distributions. They are preferred in practice over other probabilistic representations, such as Bayesian and Markov networks, because PCs can solve marginal inference (MAR) tasks in time that scales linearly in the size of the network. Unfortunately, the most probable explanation (MPE) task and its generalization, the marginal maximum-a-posteriori (MMAP) inference task remain NP-hard in these models. Inspired by the recent work on using neural networks for generating near-optimal solutions to optimization problems such as integer linear programming, we propose an approach that uses neural networks to approximate MMAP inference in PCs. The key idea in our approach is to approximate the cost of an assignment to the query variables using a continuous multilinear function and then use the latter as a loss function. The two main benefits of our new method are that it is self-supervised, and after the neural network is learned, it requires only linear time to output a solution. We evaluate our new approach on several benchmark datasets and show that it outperforms three competing linear time approximations: max-product inference, max-marginal inference, and sequential estimation, which are used in practice to solve MMAP tasks in PCs",
    "checked": true,
    "id": "34a0bac103d826c537975994a90c04260d42de03",
    "semantic_title": "neural network approximators for marginal map in probabilistic circuits",
    "citation_count": 0,
    "authors": [
      "Shivvrat Arya",
      "Tahrima Rahman",
      "Vibhav Gogate"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28967": {
    "title": "Generator Assisted Mixture of Experts for Feature Acquisition in Batch",
    "volume": "main",
    "abstract": "Given a set of observations, feature acquisition is about finding the subset of unobserved features which would enhance accuracy. Such problems has been explored in a sequential setting in prior work. Here, the model receives feedback from every new feature acquireed and chooses to explore more features or to predict. However, sequential acquisition is not feasible in some settings where time is of essence. We consider the problem of feature acquisition in batch, where the subset of features to be queried in batch is chosen based on the currently observed features, and then acquired as a batch, followed by prediction. We solve this problem using several technical innovations. First, we use a feature generator to draw a subset of the synthetic features for some examples, which reduces the cost of oracle queries. Second, to make the feature acquisition problem tractable for the large heterogeneous observed features, we partition the data into buckets, by borrowing tools from locality sensitive hashing and then train a mixture of experts model. Third, we design a tractable lower bound of the original objective. We use a greedy algorithm combined with model training to solve the underlying problem. Experiments with four datasets shows that our approach outperforms these methods in terms of trade off between accuracy and feature acquisition cost",
    "checked": true,
    "id": "2da61e8b66d62e4dd3c34b904a00f2afc6baa5f1",
    "semantic_title": "generator assisted mixture of experts for feature acquisition in batch",
    "citation_count": 0,
    "authors": [
      "Vedang Asgaonkar",
      "Aditya Jain ",
      "Abir De"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28968": {
    "title": "Taming Binarized Neural Networks and Mixed-Integer Programs",
    "volume": "main",
    "abstract": "There has been a great deal of recent interest in binarized neural networks, especially because of their explainability. At the same time, automatic differentiation algorithms such as backpropagation fail for binarized neural networks, which limits their applicability. We show that binarized neural networks admit a tame representation by reformulating the problem of training binarized neural networks as a subadditive dual of a mixed-integer program, which we show to have nice properties. This makes it possible to use the framework of Bolte et al. for implicit differentiation, which offers the possibility for practical implementation of backpropagation in the context of binarized neural networks. This approach could also be used for a broader class of mixed-integer programs, beyond the training of binarized neural networks, as encountered in symbolic approaches to AI and beyond",
    "checked": true,
    "id": "31b3e8be2efd06b1aefece6ce5965296b9f01966",
    "semantic_title": "taming binarized neural networks and mixed-integer programs",
    "citation_count": 3,
    "authors": [
      "Johannes Aspman",
      "Georgios Korpas",
      "Jakub Marecek"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28969": {
    "title": "Contextual Pandora's Box",
    "volume": "main",
    "abstract": "Pandora's Box is a fundamental stochastic optimization problem, where the decision-maker must find a good alternative, while minimizing the search cost of exploring the value of each alternative. In the original formulation, it is assumed that accurate distributions are given for the values of all the alternatives, while recent work studies the online variant of Pandora's Box where the distributions are originally unknown. In this work, we study Pandora's Box in the online setting, while incorporating context. At each round, we are presented with a number of alternatives each having a context, an exploration cost and an unknown value drawn from an unknown distribution that may change at every round. Our main result is a no-regret algorithm that performs comparably well against the optimal algorithm which knows all prior distributions exactly. Our algorithm works even in the bandit setting where the algorithm never learns the values of the alternatives that were not explored. The key technique that enables our result is a novel modification of the realizability condition in contextual bandits that connects a context to a sufficient statistic of each alternative's distribution (its reservation value) rather than its mean",
    "checked": true,
    "id": "5b441dad22ec780d60e479824190a7916f726bce",
    "semantic_title": "contextual pandora's box",
    "citation_count": 3,
    "authors": [
      "Alexia Atsidakou",
      "Constantine Caramanis",
      "Evangelia Gergatsouli",
      "Orestis Papadigenopoulos",
      "Christos Tzamos"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28970": {
    "title": "Contextual Pre-planning on Reward Machine Abstractions for Enhanced Transfer in Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "Recent studies show that deep reinforcement learning (DRL) agents tend to overfit to the task on which they were trained and fail to adapt to minor environment changes. To expedite learning when transferring to unseen tasks, we propose a novel approach to representing the current task using reward machines (RMs), state machine abstractions that induce subtasks based on the current task's rewards and dynamics. Our method provides agents with symbolic representations of optimal transitions from their current abstract state and rewards them for achieving these transitions. These representations are shared across tasks, allowing agents to exploit knowledge of previously encountered symbols and transitions, thus enhancing transfer. Empirical results show that our representations improve sample efficiency and few-shot transfer in a variety of domains",
    "checked": true,
    "id": "1633f4662d6ccae0bb5df3ee9ef8c3f20ed9e3af",
    "semantic_title": "contextual pre-planning on reward machine abstractions for enhanced transfer in deep reinforcement learning",
    "citation_count": 0,
    "authors": [
      "Guy Azran",
      "Mohamad H. Danesh",
      "Stefano V. Albrecht",
      "Sarah Keren"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28971": {
    "title": "FairTrade: Achieving Pareto-Optimal Trade-Offs between Balanced Accuracy and Fairness in Federated Learning",
    "volume": "main",
    "abstract": "As Federated Learning (FL) gains prominence in distributed machine learning applications, achieving fairness without compromising predictive performance becomes paramount. The data being gathered from distributed clients in an FL environment often leads to class imbalance. In such scenarios, balanced accuracy rather than accuracy is the true representation of model performance. However, most state-of-the-art fair FL methods report accuracy as the measure of performance, which can lead to misguided interpretations of the model's effectiveness to mitigate discrimination. To the best of our knowledge, this work presents the first attempt towards achieving Pareto-optimal trade-offs between balanced accuracy and fairness in a federated environment (FairTrade). By utilizing multi-objective optimization, the framework negotiates the intricate balance between model's balanced accuracy and fairness. The framework's agnostic design adeptly accommodates both statistical and causal fairness notions, ensuring its adaptability across diverse FL contexts. We provide empirical evidence of our framework's efficacy through extensive experiments on five real-world datasets and comparisons with six baselines. The empirical results underscore the potential of our framework in improving the trade-off between fairness and balanced accuracy in FL applications",
    "checked": true,
    "id": "3423175b85be002563e8e6e3758adb27bf476527",
    "semantic_title": "fairtrade: achieving pareto-optimal trade-offs between balanced accuracy and fairness in federated learning",
    "citation_count": 0,
    "authors": [
      "Maryam Badar",
      "Sandipan Sikdar",
      "Wolfgang Nejdl",
      "Marco Fisichella"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28972": {
    "title": "Robustness-Guided Image Synthesis for Data-Free Quantization",
    "volume": "main",
    "abstract": "Quantization has emerged as a promising direction for model compression. Recently, data-free quantization has been widely studied as a promising method to avoid privacy concerns, which synthesizes images as an alternative to real training data. Existing methods use classification loss to ensure the reliability of the synthesized images. Unfortunately, even if these images are well-classified by the pre-trained model, they still suffer from low semantics and homogenization issues. Intuitively, these low-semantic images are sensitive to perturbations, and the pre-trained model tends to have inconsistent output when the generator synthesizes an image with low semantics. To this end, we propose Robustness-Guided Image Synthesis (RIS), a simple but effective method to enrich the semantics of synthetic images and improve image diversity, further boosting the performance of data-free compression tasks. Concretely, we first introduce perturbations on input and model weight, then define the inconsistency metrics at feature and prediction levels before and after perturbations. On the basis of inconsistency on two levels, we design a robustness optimization objective to eliminate low-semantic images. Moreover, we also make our approach diversity-aware by forcing the generator to synthesize images with small correlations. With RIS, we achieve state-of-the-art performance for various settings on data-free quantization and can be extended to other data-free compression tasks",
    "checked": true,
    "id": "b2385ec964b26cb560e3cc7bfb8f4471285b5749",
    "semantic_title": "robustness-guided image synthesis for data-free quantization",
    "citation_count": 0,
    "authors": [
      "Jianhong Bai",
      "Yuchen Yang",
      "Huanpeng Chu",
      "Hualiang Wang",
      "Zuozhu Liu",
      "Ruizhe Chen",
      "Xiaoxuan He",
      "Lianrui Mu",
      "Chengfei Cai",
      "Haoji Hu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28973": {
    "title": "Regret Analysis of Policy Gradient Algorithm for Infinite Horizon Average Reward Markov Decision Processes",
    "volume": "main",
    "abstract": "In this paper, we consider an infinite horizon average reward Markov Decision Process (MDP). Distinguishing itself from existing works within this context, our approach harnesses the power of the general policy gradient-based algorithm, liberating it from the constraints of assuming a linear MDP structure. We propose a vanilla policy gradient-based algorithm and show its global convergence property. We then prove that the proposed algorithm has O(T^3/4) regret. Remarkably, this paper marks a pioneering effort by presenting the first exploration into regret bound computation for the general parameterized policy gradient algorithm in the context of average reward scenarios",
    "checked": true,
    "id": "e1a2364caeddf6f1acfa43769c6b183f1a4739ad",
    "semantic_title": "regret analysis of policy gradient algorithm for infinite horizon average reward markov decision processes",
    "citation_count": 6,
    "authors": [
      "Qinbo Bai",
      "Washim Uddin Mondal",
      "Vaneet Aggarwal"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28974": {
    "title": "Combating Data Imbalances in Federated Semi-supervised Learning with Dual Regulators",
    "volume": "main",
    "abstract": "Federated learning has become a popular method to learn from decentralized heterogeneous data. Federated semi-supervised learning (FSSL) emerges to train models from a small fraction of labeled data due to label scarcity on decentralized clients. Existing FSSL methods assume independent and identically distributed (IID) labeled data across clients and consistent class distribution between labeled and unlabeled data within a client. This work studies a more practical and challenging scenario of FSSL, where data distribution is different not only across clients but also within a client between labeled and unlabeled data. To address this challenge, we propose a novel FSSL framework with dual regulators, FedDure. FedDure lifts the previous assumption with a coarse-grained regulator (C-reg) and a fine-grained regulator (F-reg): C-reg regularizes the updating of the local model by tracking the learning effect on labeled data distribution; F-reg learns an adaptive weighting scheme tailored for unlabeled instances in each client. We further formulate the client model training as bi-level optimization that adaptively optimizes the model in the client with two regulators. Theoretically, we show the convergence guarantee of the dual regulators. Empirically, we demonstrate that FedDure is superior to the existing methods across a wide range of settings, notably by more than 11% on CIFAR-10 and CINIC-10 datasets",
    "checked": true,
    "id": "282ba076448daff03f0b1ab007796b0c20a2b7b8",
    "semantic_title": "combating data imbalances in federated semi-supervised learning with dual regulators",
    "citation_count": 2,
    "authors": [
      "Sikai Bai",
      "Shuaicheng Li",
      "Weiming Zhuang",
      "Jie Zhang",
      "Kunlin Yang",
      "Jun Hou",
      "Shuai Yi",
      "Shuai Zhang",
      "Junyu Gao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28975": {
    "title": "SpikingBERT: Distilling BERT to Train Spiking Language Models Using Implicit Differentiation",
    "volume": "main",
    "abstract": "Large language Models (LLMs), though growing exceedingly powerful, comprises of orders of magnitude less neurons and synapses than the human brain. However, it requires significantly more power/energy to operate. In this work, we propose a novel bio-inspired spiking language model (LM) which aims to reduce the computational cost of conventional LMs by drawing motivation from the synaptic information flow in the brain. In this paper, we demonstrate a framework that leverages the average spiking rate of neurons at equilibrium to train a neuromorphic spiking LM using implicit differentiation technique, thereby overcoming the non-differentiability problem of spiking neural network (SNN) based algorithms without using any type of surrogate gradient. The steady-state convergence of the spiking neurons also allows us to design a spiking attention mechanism, which is critical in developing a scalable spiking LM. Moreover, the convergence of average spiking rate of neurons at equilibrium is utilized to develop a novel ANN-SNN knowledge distillation based technique wherein we use a pre-trained BERT model as \"teacher\" to train our \"student\" spiking architecture. While the primary architecture proposed in this paper is motivated by BERT, the technique can be potentially extended to different kinds of LLMs. Our work is the first one to demonstrate the performance of an operational spiking LM architecture on multiple different tasks in the GLUE benchmark. Our implementation source code is available at https://github.com/NeuroCompLab-psu/SpikingBERT",
    "checked": true,
    "id": "cd1c724ad7a01186711add767cea2811af720dd1",
    "semantic_title": "spikingbert: distilling bert to train spiking language models using implicit differentiation",
    "citation_count": 20,
    "authors": [
      "Malyaban  Bal",
      "Abhronil Sengupta"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28976": {
    "title": "Disentangled Partial Label Learning",
    "volume": "main",
    "abstract": "Partial label learning (PLL) induces a multi-class classifier from training examples each associated with a set of candidate labels, among which only one is valid. The formation of real-world data typically arises from heterogeneous entanglement of series latent explanatory factors, which are considered intrinsic properties for discriminating between different patterns. Though learning disentangled representation is expected to facilitate label disambiguation for partial-label (PL) examples, few existing works were dedicated to addressing this issue. In this paper, we make the first attempt towards disentangled PLL and propose a novel approach named TERIAL, which makes predictions according to derived disentangled representation of instances and label embeddings. The TERIAL approach formulates the PL examples as an undirected bipartite graph where instances are only connected with their candidate labels, and employs a tailored neighborhood routing mechanism to yield disentangled representation of nodes in the graph. Specifically, the proposed routing mechanism progressively infers the explanatory factors that contribute to the edge between adjacent nodes and augments the representation of the central node with factor-aware embedding information propagated from specific neighbors simultaneously via iteratively analyzing the promising subspace clusters formed by the node and its neighbors. The estimated labeling confidence matrix is also introduced to accommodate unreliable links owing to the inherent ambiguity of PLL. Moreover, we theoretically prove that the neighborhood routing mechanism will converge to the point estimate that maximizes the marginal likelihood of observed PL training examples. Comprehensive experiments over various datasets demonstrate that our approach outperforms the state-of-the-art counterparts",
    "checked": true,
    "id": "2d8e8a644ffdc1a5f6fef7e9154f5c0f9f87015a",
    "semantic_title": "disentangled partial label learning",
    "citation_count": 0,
    "authors": [
      "Wei-Xuan Bao",
      "Yong Rui",
      "Min-Ling Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28977": {
    "title": "Efficient Target Propagation by Deriving Analytical Solution",
    "volume": "main",
    "abstract": "Exploring biologically plausible algorithms as alternatives to error backpropagation (BP) is a challenging research topic in artificial intelligence. It also provides insights into the brain's learning methods. Recently, when combined with well-designed feedback loss functions such as Local Difference Reconstruction Loss (LDRL) and through hierarchical training of feedback pathway synaptic weights, Target Propagation (TP) has achieved performance comparable to BP in image classification tasks. However, with an increase in the number of network layers, the tuning and training cost of feedback weights escalates. Drawing inspiration from the work of Ernoult et al., we propose a training method that seeks the optimal solution for feedback weights. This method enhances the efficiency of feedback training by analytically minimizing feedback loss, allowing the feedback layer to skip certain local training iterations. More specifically, we introduce the Jacobian matching loss (JML) for feedback training. We also proactively implement layers designed to derive analytical solutions that minimize JML. Through experiments, we have validated the effectiveness of this approach. Using the CIFAR-10 dataset, our method showcases accuracy levels comparable to state-of-the-art TP methods. Furthermore, we have explored its effectiveness in more intricate network architectures",
    "checked": true,
    "id": "3816b6b3710e622f0cd2851ea2b2006b3f7a4e0f",
    "semantic_title": "efficient target propagation by deriving analytical solution",
    "citation_count": 0,
    "authors": [
      "Yanhao Bao",
      "Tatsukichi Shibuya",
      "Ikuro Sato",
      "Rei Kawakami",
      "Nakamasa Inoue"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28978": {
    "title": "Strong Baselines for Parameter-Efficient Few-Shot Fine-Tuning",
    "volume": "main",
    "abstract": "Few-shot classification (FSC) entails learning novel classes given only a few examples per class after a pre-training (or meta-training) phase on a set of base classes. Recent works have shown that simply fine-tuning a pre-trained Vision Transformer (ViT) on new test classes is a strong approach for FSC. Fine-tuning ViTs, however, is expensive in time, compute and storage. This has motivated the design of parameter efficient fine-tuning (PEFT) methods which fine-tune only a fraction of the Transformer's parameters. While these methods have shown promise, inconsistencies in experimental conditions make it difficult to disentangle their advantage from other experimental factors including the feature extractor architecture, pre-trained initialization and fine-tuning algorithm, amongst others. In our paper, we conduct a large-scale, experimentally consistent, empirical analysis to study PEFTs for few-shot image classification. Through a battery of over 1.8k controlled experiments on large-scale few-shot benchmarks including Meta-Dataset and ORBIT, we uncover novel insights on PEFTs that cast light on their efficacy in fine-tuning ViTs for few-shot classification. Through our controlled empirical study, we have two main findings: (i) Fine-tuning just the LayerNorm parameters (which we call LN-Tune) during few-shot adaptation is an extremely strong baseline across ViTs pre-trained with both self-supervised and supervised objectives, (ii) For self-supervised ViTs, we find that simply learning a set of scaling parameters for each attention matrix (which we call Attn-Scale) along with a domain-residual adapter (DRA) module leads to state-of-the-art performance (while being ~9x more parameter-efficient) on Meta-Dataset. Our empirical findings set strong baselines and call for rethinking the current design of PEFT methods for FSC",
    "checked": false,
    "id": "78e74cb54786c0f01ea5ffd230d0da91efae15c3",
    "semantic_title": "strong baselines for parameter efficient few-shot fine-tuning",
    "citation_count": 14,
    "authors": [
      "Samyadeep Basu",
      "Shell Hu",
      "Daniela Massiceti",
      "Soheil Feizi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28979": {
    "title": "TREE-G: Decision Trees Contesting Graph Neural Networks",
    "volume": "main",
    "abstract": "When dealing with tabular data, models based on decision trees are a popular choice due to their high accuracy on these data types, their ease of application, and explainability properties. However, when it comes to graph-structured data, it is not clear how to apply them effectively, in a way that in- corporates the topological information with the tabular data available on the vertices of the graph. To address this challenge, we introduce TREE-G. TREE-G modifies standard decision trees, by introducing a novel split function that is specialized for graph data. Not only does this split function incorporate the node features and the topological information, but it also uses a novel pointer mechanism that allows split nodes to use information computed in previous splits. Therefore, the split function adapts to the predictive task and the graph at hand. We analyze the theoretical properties of TREE-G and demonstrate its benefits empirically on multiple graph and vertex prediction benchmarks. In these experiments, TREE-G consistently outperforms other tree-based models and often outperforms other graph-learning algorithms such as Graph Neural Networks (GNNs) and Graph Kernels, sometimes by large margins. Moreover, TREE-Gs models and their predic tions can be explained and visualized",
    "checked": true,
    "id": "a2676e45784e8cc4b092e75d13a9f46b08b7e6b4",
    "semantic_title": "tree-g: decision trees contesting graph neural networks",
    "citation_count": 1,
    "authors": [
      "Maya Bechler-Speicher",
      "Amir Globerson",
      "Ran Gilad-Bachrach"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28980": {
    "title": "Scores for Learning Discrete Causal Graphs with Unobserved Confounders",
    "volume": "main",
    "abstract": "Structural learning is arguably one of the most challenging and pervasive tasks found throughout the data sciences. There exists a growing literature that studies structural learning in non-parametric settings where conditional independence constraints are taken to define the equivalence class. In the presence of unobserved confounders, it is understood that non-conditional independence constraints are imposed over the observational distribution, including certain equalities and inequalities between functionals of the joint distribution. In this paper, we develop structural learning methods that leverage additional constraints beyond conditional independences. Specifically, we first introduce a score for arbitrary graphs combining Watanabe's asymptotic expansion of the marginal likelihood and new bounds over the cardinality of the exogenous variables. Second, we show that the new score has desirable properties in terms of expressiveness and computability. In terms of expressiveness, we prove that the score captures distinct constraints imprinted in the data, including Verma's and inequalities'. In terms of computability, we show properties of score equivalence and decomposability, which allows, in principle, to break the problem of structural learning in smaller and more manageable pieces. Third, we implement this score using an MCMC sampling algorithm and test its properties in several simulation scenarios",
    "checked": true,
    "id": "71d850ae260cfdc36b452405d2ec8b6e9df2fcdb",
    "semantic_title": "scores for learning discrete causal graphs with unobserved confounders",
    "citation_count": 4,
    "authors": [
      "Alexis Bellot",
      "Junzhe Zhang",
      "Elias Bareinboim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28981": {
    "title": "Simplicity Bias in Overparameterized Machine Learning",
    "volume": "main",
    "abstract": "A thorough theoretical understanding of the surprising generalization ability of deep networks (and other overparameterized models) is still lacking. Here we demonstrate that simplicity bias is a major phenomenon to be reckoned with in overparameterized machine learning. In addition to explaining the outcome of simplicity bias, we also study its source: following concrete rigorous examples, we argue that (i) simplicity bias can explain generalization in overparameterized learning models such as neural networks; (ii) simplicity bias and excellent generalization are optimizer-independent, as our example shows, and although the optimizer affects training, it is not the driving force behind simplicity bias; (iii) simplicity bias in pre-training models, and subsequent posteriors, is universal and stems from the subtle fact that uniformly-at-random constructed priors are not uniformly-at-random sampled ; and (iv) in neural network models, the biasing mechanism in wide (and shallow) networks is different from the biasing mechanism in deep (and narrow) networks",
    "checked": true,
    "id": "092b45273b6097ceda1c6faeb7d366a83989a090",
    "semantic_title": "simplicity bias in overparameterized machine learning",
    "citation_count": 0,
    "authors": [
      "Yakir Berchenko"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28982": {
    "title": "Maximizing the Success Probability of Policy Allocations in Online Systems",
    "volume": "main",
    "abstract": "The effectiveness of advertising in e-commerce largely depends on the ability of merchants to bid on and win impressions for their targeted users. The bidding procedure is highly complex due to various factors such as market competition, user behavior, and the diverse objectives of advertisers. In this paper we consider the problem at the level of user timelines instead of individual bid requests, manipulating full policies (i.e. pre-defined bidding strategies) and not bid values. In order to optimally allocate policies to users, typical multiple treatments allocation methods solve knapsack-like problems which aim at maximizing an expected value under constraints. In the specific context of online advertising, we argue that optimizing for the probability of success is a more suited objective than expected value maximization, and we introduce the SuccessProbaMax algorithm that aims at finding the policy allocation which is the most likely to outperform a fixed reference policy. Finally, we conduct comprehensive experiments both on synthetic and real-world data to evaluate its performance. The results demonstrate that our proposed algorithm outperforms conventional expected-value maximization algorithms in terms of success rate",
    "checked": true,
    "id": "0dfaaaa791f52b0ed9b99a42b36b1090b818b54c",
    "semantic_title": "maximizing the success probability of policy allocations in online systems",
    "citation_count": 1,
    "authors": [
      "Artem Betlei",
      "Mariia Vladimirova",
      "Mehdi Sebbar",
      "Nicolas Urien",
      "Thibaud Rahier",
      "Benjamin Heymann"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28983": {
    "title": "DGCLUSTER: A Neural Framework for Attributed Graph Clustering via Modularity Maximization",
    "volume": "main",
    "abstract": "Graph clustering is a fundamental and challenging task in the field of graph mining where the objective is to group the nodes into clusters taking into consideration the topology of the graph. It has several applications in diverse domains spanning social network analysis, recommender systems, computer vision, and bioinformatics. In this work, we propose a novel method, DGCluster, which primarily optimizes the modularity objective using graph neural networks and scales linearly with the graph size. Our method does not require the number of clusters to be specified as a part of the input and can also leverage the availability of auxiliary node level information. We extensively test DGCluster on several real-world datasets of varying sizes, across multiple popular cluster quality metrics. Our approach consistently outperforms the state-of-the-art methods, demonstrating significant performance gains in almost all settings",
    "checked": true,
    "id": "1a3c5888f0dcb3c1aacf81d870c94326121871f1",
    "semantic_title": "dgcluster: a neural framework for attributed graph clustering via modularity maximization",
    "citation_count": 2,
    "authors": [
      "Aritra Bhowmick",
      "Mert Kosan",
      "Zexi Huang",
      "Ambuj Singh",
      "Sourav Medya"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28984": {
    "title": "MEPSI: An MDL-Based Ensemble Pruning Approach with Structural Information",
    "volume": "main",
    "abstract": "Ensemble pruning that combines a subset of individual learners generated in parallel to make predictions is an important topic in ensemble learning. Past decades have developed a lot of pruning algorithms that focus on the external behavior of learners on samples, which may lead to over-fitting. In this paper, we conjecture that the generalization performance of an ensemble is not only related to its external behavior on samples but also dependent on the internal structure of individual learners. We propose the general MEPSI approach based on Kolmogorov complexity and the Minimum Description Length (MDL) principle, which formulates the ensemble pruning task as the two-objective optimization problem that comprises the empirical error and structural information among individual learners. We also provide a concrete implementation of MEPSI on decision trees. The theoretical results provide generalization bounds for both the general MEPSI approach and tree-based implementation. The comparative experiments conducted on multiple real-world data sets demonstrate the effectiveness of our proposed method",
    "checked": true,
    "id": "366c5e60bb0862197c4ff36cdbaf39a13fbe24ea",
    "semantic_title": "mepsi: an mdl-based ensemble pruning approach with structural information",
    "citation_count": 0,
    "authors": [
      "Xiao-Dong Bi",
      "Shao-Qun Zhang",
      "Yuan Jiang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28985": {
    "title": "Constraint Latent Space Matters: An Anti-anomalous Waveform Transformation Solution from Photoplethysmography to Arterial Blood Pressure",
    "volume": "main",
    "abstract": "Arterial blood pressure (ABP) holds substantial promise for proactive cardiovascular health management. Notwithstanding its potential, the invasive nature of ABP measurements confines their utility primarily to clinical environments, limiting their applicability for continuous monitoring beyond medical facilities. The conversion of photoplethysmography (PPG) signals into ABP equivalents has garnered significant attention due to its potential in revolutionizing cardiovascular disease management. Recent strides in PPG-to-ABP prediction encompass the integration of generative and discriminative models. Despite these advances, the efficacy of these models is curtailed by the latent space shift predicament, stemming from alterations in PPG data distribution across disparate hardware and individuals, potentially leading to distorted ABP waveforms. To tackle this problem, we present an innovative solution named the Latent Space Constraint Transformer (LSCT), leveraging a quantized codebook to yield robust latent spaces by employing multiple discretizing bases. To facilitate improved reconstruction, the Correlation-boosted Attention Module (CAM) is introduced to systematically query pertinent bases on a global scale. Furthermore, to enhance expressive capacity, we propose the Multi-Spectrum Enhancement Knowledge (MSEK), which fosters local information flow within the channels of latent code and provides additional embedding for reconstruction. Through comprehensive experimentation on both publicly available datasets and a private downstream task dataset, the proposed approach demonstrates noteworthy performance enhancements compared to existing methods. Extensive ablation studies further substantiate the effectiveness of each introduced module",
    "checked": true,
    "id": "7ec98f9f22bf5d3bafdd03ff18f9db100e4df8bb",
    "semantic_title": "constraint latent space matters: an anti-anomalous waveform transformation solution from photoplethysmography to arterial blood pressure",
    "citation_count": 0,
    "authors": [
      "Cheng Bian",
      "Xiaoyu Li",
      "Qi Bi",
      "Guangpu Zhu",
      "Jiegeng Lyu",
      "Weile Zhang",
      "Yelei Li",
      "Zijing Zeng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28986": {
    "title": "Axiomatic Aggregations of Abductive Explanations",
    "volume": "main",
    "abstract": "The recent criticisms of the robustness of post hoc model approximation explanation methods (like LIME and SHAP) have led to the rise of model-precise abductive explanations. For each data point, abductive explanations provide a minimal subset of features that are sufficient to generate the outcome. While theoretically sound and rigorous, abductive explanations suffer from a major issue --- there can be several valid abductive explanations for the same data point. In such cases, providing a single abductive explanation can be insufficient; on the other hand, providing all valid abductive explanations can be incomprehensible due to their size. In this work, we solve this issue by aggregating the many possible abductive explanations into feature importance scores. We propose three aggregation methods: two based on power indices from cooperative game theory and a third based on a well-known measure of causal strength. We characterize these three methods axiomatically, showing that each of them uniquely satisfies a set of desirable properties. We also evaluate them on multiple datasets and show that these explanations are robust to the attacks that fool SHAP and LIME",
    "checked": true,
    "id": "50695ac1a8bedf90596c6b4a1876df6f9711d658",
    "semantic_title": "axiomatic aggregations of abductive explanations",
    "citation_count": 3,
    "authors": [
      "Gagan Biradar",
      "Yacine Izza",
      "Elita Lobo",
      "Vignesh Viswanathan",
      "Yair Zick"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28987": {
    "title": "MIND: Multi-Task Incremental Network Distillation",
    "volume": "main",
    "abstract": "The recent surge of pervasive devices that generate dynamic data streams has underscored the necessity for learning systems to adapt continually to data distributional shifts. To tackle this challenge, the research community has put forth a spectrum of methodologies, including the demanding pursuit of class-incremental learning without replay data. In this study, we present MIND, a parameter isolation method that aims to significantly enhance the performance of replay-free solutions and achieve state-of-the-art results on several widely studied datasets. Our approach introduces two main contributions: two alternative distillation procedures that significantly improve the efficiency of MIND increasing the accumulated knowledge of each sub-network, and the optimization of the BachNorm layers across tasks inside the sub-networks. Overall, MIND outperforms all the state-of-the-art methods for rehearsal-free Class-Incremental learning (with an increment in classification accuracy of approx. +6% on CIFAR-100/10 and +10% on TinyImageNet/10) reaching up to approx. +40% accuracy in Domain-Incremental scenarios. Moreover, we ablated each contribution to demonstrate its impact on performance improvement. Our results showcase the superior performance of MIND indicating its potential for addressing the challenges posed by Class-incremental and Domain-Incremental learning in resource-constrained environments",
    "checked": true,
    "id": "fe951cfdf69d4f63270169bd37bcf02ba1483848",
    "semantic_title": "mind: multi-task incremental network distillation",
    "citation_count": 2,
    "authors": [
      "Jacopo Bonato",
      "Francesco Pelosin",
      "Luigi Sabetta",
      "Alessandro Nicolosi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28988": {
    "title": "HyperFast: Instant Classification for Tabular Data",
    "volume": "main",
    "abstract": "Training deep learning models and performing hyperparameter tuning can be computationally demanding and time-consuming. Meanwhile, traditional machine learning methods like gradient-boosting algorithms remain the preferred choice for most tabular data applications, while neural network alternatives require extensive hyperparameter tuning or work only in toy datasets under limited settings. In this paper, we introduce HyperFast, a meta-trained hypernetwork designed for instant classification of tabular data in a single forward pass. HyperFast generates a task-specific neural network tailored to an unseen dataset that can be directly used for classification inference, removing the need for training a model. We report extensive experiments with OpenML and genomic data, comparing HyperFast to competing tabular data neural networks, traditional ML methods, AutoML systems, and boosting machines. HyperFast shows highly competitive results, while being significantly faster. Additionally, our approach demonstrates robust adaptability across a variety of classification tasks with little to no fine-tuning, positioning HyperFast as a strong solution for numerous applications and rapid model deployment. HyperFast introduces a promising paradigm for fast classification, with the potential to substantially decrease the computational burden of deep learning. Our code, which offers a scikit-learn-like interface, along with the trained HyperFast model, can be found at https://github.com/AI-sandbox/HyperFast",
    "checked": true,
    "id": "4f4c74cbf053a7acc3ad91c2d3c632a29a10ebe7",
    "semantic_title": "hyperfast: instant classification for tabular data",
    "citation_count": 4,
    "authors": [
      "David Bonet",
      "Daniel Mas Montserrat",
      "Xavier Giró-i-Nieto",
      "Alexander G. Ioannidis"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28989": {
    "title": "A Theory of Non-acyclic Generative Flow Networks",
    "volume": "main",
    "abstract": "GFlowNets is a novel flow-based method for learning a stochastic policy to generate objects via a sequence of actions and with probability proportional to a given positive reward. We contribute to relaxing hypotheses limiting the application range of GFlowNets, in particular: acyclicity (or lack thereof). To this end, we extend the theory of GFlowNets on measurable spaces which includes continuous state spaces without cycle restrictions, and provide a generalization of cycles in this generalized context. We show that losses used so far push flows to get stuck into cycles and we define a family of losses solving this issue. Experiments on graphs and continuous tasks validate those principles",
    "checked": true,
    "id": "e085621bf6fa90a0437b3112b0408362e3bea7cc",
    "semantic_title": "a theory of non-acyclic generative flow networks",
    "citation_count": 0,
    "authors": [
      "Leo Brunswic",
      "Yinchuan Li",
      "Yushun Xu",
      "Yijun Feng",
      "Shangling Jui",
      "Lizhuang Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28990": {
    "title": "Where and How to Attack? A Causality-Inspired Recipe for Generating Counterfactual Adversarial Examples",
    "volume": "main",
    "abstract": "Deep neural networks (DNNs) have been demonstrated to be vulnerable to well-crafted adversarial examples, which are generated through either well-conceived L_p-norm restricted or unrestricted attacks. Nevertheless, the majority of those approaches assume that adversaries can modify any features as they wish, and neglect the causal generating process of the data, which is unreasonable and unpractical. For instance, a modification in income would inevitably impact features like the debt-to-income ratio within a banking system. By considering the underappreciated causal generating process, first, we pinpoint the source of the vulnerability of DNNs via the lens of causality, then give theoretical results to answer where to attack. Second, considering the consequences of the attack interventions on the current state of the examples to generate more realistic adversarial examples, we propose CADE, a framework that can generate Counterfactual ADversarial Examples to answer how to attack. The empirical results demonstrate CADE's effectiveness, as evidenced by its competitive performance across diverse attack scenarios, including white-box, transfer-based, and random intervention attacks",
    "checked": true,
    "id": "e29c1364baa88ecc2237b66936cf93c730735109",
    "semantic_title": "where and how to attack? a causality-inspired recipe for generating counterfactual adversarial examples",
    "citation_count": 1,
    "authors": [
      "Ruichu Cai",
      "Yuxuan Zhu",
      "Jie Qiao",
      "Zefeng Liang",
      "Furui Liu",
      "Zhifeng Hao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28991": {
    "title": "MSGNet: Learning Multi-Scale Inter-series Correlations for Multivariate Time Series Forecasting",
    "volume": "main",
    "abstract": "Multivariate time series forecasting poses an ongoing challenge across various disciplines. Time series data often exhibit diverse intra-series and inter-series correlations, contributing to intricate and interwoven dependencies that have been the focus of numerous studies. Nevertheless, a significant research gap remains in comprehending the varying inter-series correlations across different time scales among multiple time series, an area that has received limited attention in the literature. To bridge this gap, this paper introduces MSGNet, an advanced deep learning model designed to capture the varying inter-series correlations across multiple time scales using frequency domain analysis and adaptive graph convolution. By leveraging frequency domain analysis, MSGNet effectively extracts salient periodic patterns and decomposes the time series into distinct time scales. The model incorporates a self-attention mechanism to capture intra-series dependencies, while introducing an adaptive mixhop graph convolution layer to autonomously learn diverse inter-series correlations within each time scale. Extensive experiments are conducted on several real-world datasets to showcase the effectiveness of MSGNet. Furthermore, MSGNet possesses the ability to automatically learn explainable multi-scale inter-series correlations, exhibiting strong generalization capabilities even when applied to out-of-distribution samples",
    "checked": true,
    "id": "2133f29fe00e8c6b618789afd80412a6dfafe48a",
    "semantic_title": "msgnet: learning multi-scale inter-series correlations for multivariate time series forecasting",
    "citation_count": 7,
    "authors": [
      "Wanlin Cai",
      "Yuxuan Liang",
      "Xianggen Liu",
      "Jianshuai Feng",
      "Yuankai Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28992": {
    "title": "Kernelized Normalizing Constant Estimation: Bridging Bayesian Quadrature and Bayesian Optimization",
    "volume": "main",
    "abstract": "In this paper, we study the problem of estimating the normalizing constant through queries to the black-box function f, which is the integration of the exponential function of f scaled by a problem parameter lambda. We assume f belongs to a reproducing kernel Hilbert space (RKHS), and show that to estimate the normalizing constant within a small relative error, the level of difficulty depends on the value of lambda: When lambda approaches zero, the problem is similar to Bayesian quadrature (BQ), while when lambda approaches infinity, the problem is similar to Bayesian optimization (BO). More generally, the problem varies between BQ and BO. We find that this pattern holds true even when the function evaluations are noisy, bringing new aspects to this topic. Our findings are supported by both algorithm-independent lower bounds and algorithmic upper bounds, as well as simulation studies conducted on a variety of benchmark functions",
    "checked": true,
    "id": "0df4e304cba2c0df6e18f24a50095d50a2fd86cd",
    "semantic_title": "kernelized normalizing constant estimation: bridging bayesian quadrature and bayesian optimization",
    "citation_count": 0,
    "authors": [
      "Xu Cai",
      "Jonathan Scarlett"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28993": {
    "title": "EG-NAS: Neural Architecture Search with Fast Evolutionary Exploration",
    "volume": "main",
    "abstract": "Differentiable Architecture Search (DARTS) has achieved a rapid search for excellent architectures by optimizing architecture parameters through gradient descent. However, this efficiency comes with a significant challenge: the risk of premature convergence to local optima, resulting in subpar performance that falls short of expectations. To address this issue, we propose a novel and effective method called Evolutionary Gradient-Based Neural Architecture Search (EG-NAS). Our approach combines the strengths of both gradient descent and evolutionary strategy, allowing for the exploration of various optimization directions during the architecture search process. To begin with, we continue to employ gradient descent for updating network parameters to ensure efficiency. Subsequently, to mitigate the risk of premature convergence, we introduce an evolutionary strategy with global search capabilities to optimize the architecture parameters. By leveraging the best of both worlds, our method strikes a balance between efficient exploration and exploitation of the search space. Moreover, we have redefined the fitness function to not only consider accuracy but also account for individual similarity. This inclusion enhances the diversity and accuracy of the optimized directions identified by the evolutionary strategy. Extensive experiments on various datasets and search spaces demonstrate that EG-NAS achieves highly competitive performance at significantly low search costs compared to state-of-the-art methods. The code is available at https://github.com/caicaicheng/EG-NAS",
    "checked": true,
    "id": "359efea65a0db42f43570ff193015f2afd7e4a78",
    "semantic_title": "eg-nas: neural architecture search with fast evolutionary exploration",
    "citation_count": 0,
    "authors": [
      "Zicheng Cai",
      "Lei Chen",
      "Peng Liu",
      "Tongtao Ling",
      "Yutao Lai"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28994": {
    "title": "Mixup-Induced Domain Extrapolation for Domain Generalization",
    "volume": "main",
    "abstract": "Domain generalization aims to learn a well-performed classifier on multiple source domains for unseen target domains under domain shift. Domain-invariant representation (DIR) is an intuitive approach and has been of great concern. In practice, since the targets are variant and agnostic, only a few sources are not sufficient to reflect the entire domain population, leading to biased DIR. Derived from PAC-Bayes framework, we provide a novel generalization bound involving the number of domains sampled from the environment (N) and the radius of the Wasserstein ball centred on the target (r), which have rarely been considered before. Herein, we can obtain two natural and significant findings: when N increases, 1) the gap between the source and target sampling environments can be gradually mitigated; 2) the target can be better approximated within the Wasserstein ball. These findings prompt us to collect adequate domains against domain shift. For seeking convenience, we design a novel yet simple Extrapolation Domain strategy induced by the Mixup scheme, namely EDM. Through a reverse Mixup scheme to generate the extrapolated domains, combined with the interpolated domains, we expand the interpolation space spanned by the sources, providing more abundant domains to increase sampling intersections to shorten r. Moreover, EDM is easy to implement and be plugged-and-played. In experiments, EDM has been plugged into several methods in both closed and open set settings, achieving up to 5.73% improvement",
    "checked": true,
    "id": "ec146ada392be7824673182493d3f24784a1fffd",
    "semantic_title": "mixup-induced domain extrapolation for domain generalization",
    "citation_count": 0,
    "authors": [
      "Meng Cao",
      "Songcan Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28995": {
    "title": "Continuous-Time Graph Representation with Sequential Survival Process",
    "volume": "main",
    "abstract": "Over the past two decades, there has been a tremendous increase in the growth of representation learning methods for graphs, with numerous applications across various fields, including bioinformatics, chemistry, and the social sciences. However, current dynamic network approaches focus on discrete-time networks or treat links in continuous-time networks as instantaneous events. Therefore, these approaches have limitations in capturing the persistence or absence of links that continuously emerge and disappear over time for particular durations. To address this, we propose a novel stochastic process relying on survival functions to model the durations of links and their absences over time. This forms a generic new likelihood specification explicitly accounting for intermittent edge-persistent networks, namely GraSSP: Graph Representation with Sequential Survival Process. We apply the developed framework to a recent continuous time dynamic latent distance model characterizing network dynamics in terms of a sequence of piecewise linear movements of nodes in latent space. We quantitatively assess the developed framework in various downstream tasks, such as link prediction and network completion, demonstrating that the developed modeling framework accounting for link persistence and absence well tracks the intrinsic trajectories of nodes in a latent space and captures the underlying characteristics of evolving network structure",
    "checked": true,
    "id": "a63acda8848eaffd061ed7b7d92cbca9ece2691f",
    "semantic_title": "continuous-time graph representation with sequential survival process",
    "citation_count": 2,
    "authors": [
      "Abdulkadir Çelikkanat",
      "Nikolaos Nakis",
      "Morten Mørup"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28996": {
    "title": "Learning to Unlearn: Instance-Wise Unlearning for Pre-trained Classifiers",
    "volume": "main",
    "abstract": "Since the recent advent of regulations for data protection (e.g., the General Data Protection Regulation), there has been increasing demand in deleting information learned from sensitive data in pre-trained models without retraining from scratch. The inherent vulnerability of neural networks towards adversarial attacks and unfairness also calls for a robust method to remove or correct information in an instance-wise fashion, while retaining the predictive performance across remaining data. To this end, we consider instance-wise unlearning, of which the goal is to delete information on a set of instances from a pre-trained model, by either misclassifying each instance away from its original prediction or relabeling the instance to a different label. We also propose two methods that reduce forgetting on the remaining data: 1) utilizing adversarial examples to overcome forgetting at the representation-level and 2) leveraging weight importance metrics to pinpoint network parameters guilty of propagating unwanted information. Both methods only require the pre-trained model and data instances to forget, allowing painless application to real-life settings where the entire training set is unavailable. Through extensive experimentation on various image classification benchmarks, we show that our approach effectively preserves knowledge of remaining data while unlearning given instances in both single-task and continual unlearning scenarios",
    "checked": true,
    "id": "834310310d39110b8356f4646eac104144946e89",
    "semantic_title": "learning to unlearn: instance-wise unlearning for pre-trained classifiers",
    "citation_count": 12,
    "authors": [
      "Sungmin Cha",
      "Sungjun Cho",
      "Dasol Hwang",
      "Honglak Lee",
      "Taesup Moon",
      "Moontae Lee"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28997": {
    "title": "Variable Importance in High-Dimensional Settings Requires Grouping",
    "volume": "main",
    "abstract": "Explaining the decision process of machine learning algorithms is nowadays crucial for both model's performance enhancement and human comprehension. This can be achieved by assessing the variable importance of single variables, even for high-capacity non-linear methods, e.g. Deep Neural Networks (DNNs). While only removal-based approaches, such as Permutation Importance (PI), can bring statistical validity, they return misleading results when variables are correlated. Conditional Permutation Importance (CPI) bypasses PI's limitations in such cases. However, in high-dimensional settings, where high correlations between the variables cancel their conditional importance, the use of CPI as well as other methods leads to unreliable results, besides prohibitive computation costs. Grouping variables statistically via clustering or some prior knowledge gains some power back and leads to better interpretations. In this work, we introduce BCPI (Block-Based Conditional Permutation Importance), a new generic framework for variable importance computation with statistical guarantees handling both single and group cases. Furthermore, as handling groups with high cardinality (such as a set of observations of a given modality) are both time-consuming and resource-intensive, we also introduce a new stacking approach extending the DNN architecture with sub-linear layers adapted to the group structure. We show that the ensuing approach extended with stacking controls the type-I error even with highly-correlated groups and shows top accuracy across benchmarks. Furthermore, we perform a real-world data analysis in a large-scale medical dataset where we aim to show the consistency between our results and the literature for a biomarker prediction",
    "checked": true,
    "id": "fd0382778a031c745c5987db46e0fbe5e175fdbc",
    "semantic_title": "variable importance in high-dimensional settings requires grouping",
    "citation_count": 2,
    "authors": [
      "Ahmad Chamma",
      "Bertrand Thirion",
      "Denis Engemann"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28998": {
    "title": "Privacy Amplification by Iteration for ADMM with (Strongly) Convex Objective Functions",
    "volume": "main",
    "abstract": "We examine a private ADMM variant for (strongly) convex objectives which is a primal-dual iterative method. Each iteration has a user with a private function used to update the primal variable, masked by Gaussian noise for local privacy, without directly adding noise to the dual variable. Privacy amplification by iteration explores if noises from later iterations can enhance the privacy guarantee when releasing final variables after the last iteration. Cyffers et al. explored privacy amplification by iteration for the proximal ADMM variant, where a user's entire private function is accessed and noise is added to the primal variable. In contrast, we examine a private ADMM variant requiring just one gradient access to a user's function, but both primal and dual variables must be passed between successive iterations. To apply Balle et al.'s coupling framework to the gradient ADMM variant, we tackle technical challenges with novel ideas. First, we address the non-expansive mapping issue in ADMM iterations by using a customized norm. Second, because the dual variables are not masked with any noise directly, their privacy guarantees are achieved by treating two consecutive noisy ADMM iterations as a Markov operator. Our main result is that the privacy guarantee for the gradient ADMM variant can be amplified proportionally to the number of iterations. For strongly convex objective functions, this amplification exponentially increases with the number of iterations. These amplification results align with the previously studied special case of stochastic gradient descent",
    "checked": true,
    "id": "53d0f9c95f8e2341b31b2d0bd2b6b7d38a23aaea",
    "semantic_title": "privacy amplification by iteration for admm with (strongly) convex objective functions",
    "citation_count": 0,
    "authors": [
      "T-H. Hubert Chan",
      "Hao Xie",
      "Mengshi Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/28999": {
    "title": "Understanding Distributed Representations of Concepts in Deep Neural Networks without Supervision",
    "volume": "main",
    "abstract": "Understanding intermediate representations of the concepts learned by deep learning classifiers is indispensable for interpreting general model behaviors. Existing approaches to reveal learned concepts often rely on human supervision, such as pre-defined concept sets or segmentation processes. In this paper, we propose a novel unsupervised method for discovering distributed representations of concepts by selecting a principal subset of neurons. Our empirical findings demonstrate that instances with similar neuron activation states tend to share coherent concepts. Based on the observations, the proposed method selects principal neurons that construct an interpretable region, namely a Relaxed Decision Region (RDR), encompassing instances with coherent concepts in the feature space. It can be utilized to identify unlabeled subclasses within data and to detect the causes of misclassifications. Furthermore, the applicability of our method across various layers discloses distinct distributed representations over the layers, which provides deeper insights into the internal mechanisms of the deep learning model",
    "checked": true,
    "id": "731e0fa1a04b01e6dffea719ea164e2429c8ef28",
    "semantic_title": "understanding distributed representations of concepts in deep neural networks without supervision",
    "citation_count": 0,
    "authors": [
      "Wonjoon Chang",
      "Dahee Kwon",
      "Jaesik Choi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29000": {
    "title": "Incomplete Contrastive Multi-View Clustering with High-Confidence Guiding",
    "volume": "main",
    "abstract": "Incomplete multi-view clustering becomes an important research problem, since multi-view data with missing values are ubiquitous in real-world applications. Although great efforts have been made for incomplete multi-view clustering, there are still some challenges: 1) most existing methods didn't make full use of multi-view information to deal with missing values; 2) most methods just employ the consistent information within multi-view data but ignore the complementary information; 3) For the existing incomplete multi-view clustering methods, incomplete multi-view representation learning and clustering are treated as independent processes, which leads to performance gap. In this work, we proposed a novel Incomplete Contrastive Multi-View Clustering method with high-confidence guiding (ICMVC). Firstly, we proposed a multi-view consistency relation transfer plus graph convolutional network to tackle missing values problem. Secondly, instance-level attention fusion and high-confidence guiding are proposed to exploit the complementary information while instance-level contrastive learning for latent representation is designed to employ the consistent information. Thirdly, an end-to-end framework is proposed to integrate multi-view missing values handling, multi-view representation learning and clustering assignment for joint optimization. Experiments compared with state-of-the-art approaches demonstrated the effectiveness and superiority of our method. Our code is publicly available at https://github.com/liunian-Jay/ICMVC. The version with supplementary material can be found at http://arxiv.org/abs/2312.08697",
    "checked": true,
    "id": "beaa4eb3aced336795a0cb53748e322e12d8b02b",
    "semantic_title": "incomplete contrastive multi-view clustering with high-confidence guiding",
    "citation_count": 3,
    "authors": [
      "Guoqing Chao",
      "Yi Jiang",
      "Dianhui Chu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29001": {
    "title": "Offline Model-Based Optimization via Policy-Guided Gradient Search",
    "volume": "main",
    "abstract": "Offline optimization is an emerging problem in many experimental engineering domains including protein, drug or aircraft design, where online experimentation to collect evaluation data is too expensive or dangerous. To avoid that, one has to optimize an unknown function given only its offline evaluation at a fixed set of inputs. A naive solution to this problem is to learn a surrogate model of the unknown function and optimize this surrogate instead. However, such a naive optimizer is prone to erroneous overestimation of the surrogate (possibly due to over-fitting on a biased sample of function evaluation) on inputs outside the offline dataset. Prior approaches addressing this challenge have primarily focused on learning robust surrogate models. However, their search strategies are derived from the surrogate model rather than the actual offline data. To fill this important gap, we introduce a new learning-to-search perspective for offline optimization by reformulating it as an offline reinforcement learning problem. Our proposed policy-guided gradient search approach explicitly learns the best policy for a given surrogate model created from the offline data. Our empirical results on multiple benchmarks demonstrate that the learned optimization policy can be combined with existing offline surrogates to significantly improve the optimization performance",
    "checked": true,
    "id": "82820840d5b65fdeb13b78a8f2cb3ffe99d7d2ae",
    "semantic_title": "offline model-based optimization via policy-guided gradient search",
    "citation_count": 2,
    "authors": [
      "Yassine Chemingui",
      "Aryan Deshwal",
      "Trong Nghia Hoang",
      "Janardhan Rao Doppa"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29002": {
    "title": "Focus-Then-Decide: Segmentation-Assisted Reinforcement Learning",
    "volume": "main",
    "abstract": "Visual Reinforcement Learning (RL) is a promising approach to achieve human-like intelligence. However, it currently faces challenges in learning efficiently within noisy environments. In contrast, humans can quickly identify task-relevant objects in distraction-filled surroundings by applying previously acquired common knowledge. Recently, foundational models in natural language processing and computer vision have achieved remarkable successes, and the common knowledge within these models can significantly benefit downstream task training. Inspired by these achievements, we aim to incorporate common knowledge from foundational models into visual RL. We propose a novel Focus-Then-Decide (FTD) framework, allowing the agent to make decisions based solely on task-relevant objects. To achieve this, we introduce an attention mechanism to select task-relevant objects from the object set returned by a foundational segmentation model, and only use the task-relevant objects for the subsequent training of the decision module. Additionally, we specifically employed two generic self-supervised objectives to facilitate the rapid learning of this attention mechanism. Experimental results on challenging tasks based on DeepMind Control Suite and Franka Emika Robotics demonstrate that our method can quickly and accurately pinpoint objects of interest in noisy environments. Consequently, it achieves a significant performance improvement over current state-of-the-art algorithms. Project Page: https://www.lamda.nju.edu.cn/chenc/FTD.html Code: https://github.com/LAMDA-RL/FTD",
    "checked": true,
    "id": "97e74e171c44cb85afffbe47e2e2a55476ea451c",
    "semantic_title": "focus-then-decide: segmentation-assisted reinforcement learning",
    "citation_count": 0,
    "authors": [
      "Chao Chen",
      "Jiacheng Xu",
      "Weijian Liao",
      "Hao Ding",
      "Zongzhang Zhang",
      "Yang Yu",
      "Rui Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29003": {
    "title": "Data Shunt: Collaboration of Small and Large Models for Lower Costs and Better Performance",
    "volume": "main",
    "abstract": "Pretrained large models, particularly large language models, have garnered increasing attention, as they have demonstrated remarkable abilities through contextual learning. Pretrained large models are increasingly recognized as fundamental tools for solving various tasks. However, the substantial computational demands of large models have dissuaded most product teams and individuals from running them. In such scenarios, to leverage the exceptional performance of large models, one must solely depend on costly APIs, further burdening product teams and individuals. On the other hand, despite the overall inferior performance of small models compared to large models, there are certain distributions where small models can achieve comparable or even superior results. For instance, during training, small models may become trapped in a local optimum that is unique to certain distributions, leading to superior performance. Hence, we propose Data Shunt (DS), a general paradigm for collaboration of small and large models. DS not only substantially reduces the cost associated with deploying large models but also effectively enhances overall performance. Specifically, DS determines the shunting direction by evaluating the confidence level of small models. When the confidence level falls below a specific threshold, the input data is forwarded to large models. To further leverage the advantages of the small and large models, we introduce Prompt Pruning (PP) and 2-Stage Confidence Distillation (2CD), which facilitate mutual collaboration, leading to better results and less cost. The remarkable performance across diverse modalities and tasks demonstrates the superiority of the proposed DS over large models. For instance, ChatGPT achieves an accuracy of 94.43% on Amazon Product sentiment analysis, and DS achieves an accuracy of 95.64%, while the cost has been reduced to only 31.18%. The code for the proposed method are provided for research purposes https://github.com/Anfeather/Data-Shunt",
    "checked": true,
    "id": "69239ee34b04c5503774ffc0235bc1b239cca4fa",
    "semantic_title": "data shunt: collaboration of small and large models for lower costs and better performance",
    "citation_count": 1,
    "authors": [
      "Dong Chen",
      "Yueting Zhuang",
      "Shuo Zhang",
      "Jinfeng Liu",
      "Su Dong",
      "Siliang Tang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29004": {
    "title": "EPSD: Early Pruning with Self-Distillation for Efficient Model Compression",
    "volume": "main",
    "abstract": "Neural network compression techniques, such as knowledge distillation (KD) and network pruning, have received increasing attention. Recent work `Prune, then Distill' reveals that a pruned student-friendly teacher network can benefit the performance of KD. However, the conventional teacher-student pipeline, which entails cumbersome pre-training of the teacher and complicated compression steps, makes pruning with KD less efficient. In addition to compressing models, recent compression techniques also emphasize the aspect of efficiency. Early pruning demands significantly less computational cost in comparison to the conventional pruning methods as it does not require a large pre-trained model. Likewise, a special case of KD, known as self-distillation (SD), is more efficient since it requires no pre-training or student-teacher pair selection. This inspires us to collaborate early pruning with SD for efficient model compression. In this work, we propose the framework named Early Pruning with Self-Distillation (EPSD), which identifies and preserves distillable weights in early pruning for a given SD task. EPSD efficiently combines early pruning and self-distillation in a two-step process, maintaining the pruned network's trainability for compression. Instead of a simple combination of pruning and SD, EPSD enables the pruned network to favor SD by keeping more distillable weights before training to ensure better distillation of the pruned network. We demonstrated that EPSD improves the training of pruned networks, supported by visual and quantitative analyses. Our evaluation covered diverse benchmarks (CIFAR-10/100, Tiny-ImageNet, full ImageNet, CUB-200-2011, and Pascal VOC), with EPSD outperforming advanced pruning and SD techniques",
    "checked": true,
    "id": "592a19c3a82a5eede7175c23ffa4c25c1bd2dd59",
    "semantic_title": "epsd: early pruning with self-distillation for efficient model compression",
    "citation_count": 0,
    "authors": [
      "Dong Chen",
      "Ning Liu",
      "Yichen Zhu",
      "Zhengping Che",
      "Rui Ma",
      "Fachao Zhang",
      "Xiaofeng Mou",
      "Yi Chang",
      "Jian Tang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29005": {
    "title": "A Generalized Shuffle Framework for Privacy Amplification: Strengthening Privacy Guarantees and Enhancing Utility",
    "volume": "main",
    "abstract": "The shuffle model of local differential privacy is an advanced method of privacy amplification designed to enhance privacy protection with high utility. It achieves this by randomly shuffling sensitive data, making linking individual data points to specific individuals more challenging. However, most existing studies have focused on the shuffle model based on (ε0,0)-Locally Differentially Private (LDP) randomizers, with limited consideration for complex scenarios such as (ε0,δ0)-LDP or personalized LDP (PLDP). This hinders a comprehensive understanding of the shuffle model's potential and limits its application in various settings. To bridge this research gap, we propose a generalized shuffle framework that can be applied to PLDP setting. This generalization allows for a broader exploration of the privacy-utility trade-off and facilitates the design of privacy-preserving analyses in diverse contexts. We prove that the shuffled PLDP process approximately preserves μ-Gaussian Differential Privacy with μ = O(1/√n). This approach allows us to avoid the limitations and potential inaccuracies associated with inequality estimations. To strengthen the privacy guarantee, we improve the lower bound by utilizing hypothesis testing instead of relying on rough estimations like the Chernoff bound or Hoeffding's inequality. Furthermore, extensive comparative evaluations clearly show that our approach outperforms existing methods in achieving strong central privacy guarantees while preserving the utility of the global model. We have also carefully designed corresponding algorithms for average function, frequency estimation, and stochastic gradient descent",
    "checked": true,
    "id": "4880948f3434dcfc30e51572934425feba508e02",
    "semantic_title": "a generalized shuffle framework for privacy amplification: strengthening privacy guarantees and enhancing utility",
    "citation_count": 0,
    "authors": [
      "E Chen",
      "Yang Cao",
      "Yifei Ge"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29006": {
    "title": "Adaptive Discovering and Merging for Incremental Novel Class Discovery",
    "volume": "main",
    "abstract": "One important desideratum of lifelong learning aims to discover novel classes from unlabelled data in a continuous manner. The central challenge is twofold: discovering and learning novel classes while mitigating the issue of catastrophic forgetting of established knowledge. To this end, we introduce a new paradigm called Adaptive Discovering and Merging (ADM) to discover novel categories adaptively in the incremental stage and integrate novel knowledge into the model without affecting the original knowledge. To discover novel classes adaptively, we decouple representation learning and novel class discovery, and use Triple Comparison (TC) and Probability Regularization (PR) to constrain the probability discrepancy and diversity for adaptive category assignment. To merge the learned novel knowledge adaptively, we propose a hybrid structure with base and novel branches named Adaptive Model Merging (AMM), which reduces the interference of the novel branch on the old classes to preserve the previous knowledge, and merges the novel branch to the base model without performance loss and parameter growth. Extensive experiments on several datasets show that ADM significantly outperforms existing class-incremental Novel Class Discovery (class-iNCD) approaches. Moreover, our AMM also benefits the class-incremental Learning (class-IL) task by alleviating the catastrophic forgetting problem. The source code is included in the supplementary materials",
    "checked": true,
    "id": "e4aaa2d641cb7a5f6d5d8523af39a1ed5bb8660b",
    "semantic_title": "adaptive discovering and merging for incremental novel class discovery",
    "citation_count": 2,
    "authors": [
      "Guangyao Chen",
      "Peixi Peng",
      "Yangru Huang",
      "Mengyue Geng",
      "Yonghong Tian"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29007": {
    "title": "FedDAT: An Approach for Foundation Model Finetuning in Multi-Modal Heterogeneous Federated Learning",
    "volume": "main",
    "abstract": "Recently, foundation models have exhibited remarkable advancements in multi-modal learning. These models, equipped with millions (or billions) of parameters, typically require a substantial amount of data for finetuning. However, collecting and centralizing training data from diverse sectors becomes challenging due to distinct privacy regulations. Federated Learning (FL) emerges as a promising solution, enabling multiple clients to collaboratively train neural networks without centralizing their local data. To alleviate client computation burdens and communication overheads, previous works have adapted Parameter-efficient Finetuning (PEFT) methods for FL. Hereby, only a small fraction of the model parameters are optimized and communicated during federated communications. Nevertheless, most previous works have focused on a single modality and neglected one common phenomenon, i.e., the presence of data heterogeneity across the clients. Therefore, in this work, we propose a finetuning framework tailored to heterogeneous multi-modal FL, called Federated Dual-Aadapter Teacher (FedDAT). Specifically, our approach leverages a Dual-Adapter Teacher (DAT) to address data heterogeneity by regularizing the client local updates and applying Mutual Knowledge Distillation (MKD) for an efficient knowledge transfer. FedDAT is the first approach that enables an efficient distributed finetuning of foundation models for a variety of heterogeneous Vision-Language tasks. To demonstrate its effectiveness, we conduct extensive experiments on four multi-modality FL benchmarks with different types of data heterogeneity, where FedDAT substantially outperforms the existing centralized PEFT methods adapted for FL",
    "checked": true,
    "id": "3cb399d96cd70f5e9aa4ffecf7d329d1b0910745",
    "semantic_title": "feddat: an approach for foundation model finetuning in multi-modal heterogeneous federated learning",
    "citation_count": 10,
    "authors": [
      "Haokun Chen",
      "Yao Zhang",
      "Denis Krompass",
      "Jindong Gu",
      "Volker Tresp"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29008": {
    "title": "Uncertainty Quantification for Data-Driven Change-Point Learning via Cross-Validation",
    "volume": "main",
    "abstract": "Accurately detecting multiple change-points is critical for various applications, but determining the optimal number of change-points remains a challenge. Existing approaches based on information criteria attempt to balance goodness-of-fit and model complexity, but their performance varies depending on the model. Recently, data-driven selection criteria based on cross-validation has been proposed, but these methods can be prone to slight overfitting in finite samples. In this paper, we introduce a method that controls the probability of overestimation and provides uncertainty quantification for learning multiple change-points via cross-validation. We frame this problem as a sequence of model comparison problems and leverage high-dimensional inferential procedures. We demonstrate the effectiveness of our approach through experiments on finite-sample data, showing superior uncertainty quantification for overestimation compared to existing methods. Our approach has broad applicability and can be used in diverse change-point models",
    "checked": true,
    "id": "700b52ce9f5e697065ef76e25245b49f7cd2c676",
    "semantic_title": "uncertainty quantification for data-driven change-point learning via cross-validation",
    "citation_count": 0,
    "authors": [
      "Hui Chen",
      "Yinxu Jia",
      "Guanghui Wang",
      "Changliang Zou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29009": {
    "title": "Bridging the Semantic Latent Space between Brain and Machine: Similarity Is All You Need",
    "volume": "main",
    "abstract": "How our brain encodes complex concepts has been a longstanding mystery in neuroscience. The answer to this problem can lead to new understandings about how the brain retrieves information in large-scale data with high efficiency and robustness. Neuroscience studies suggest the brain represents concepts in a locality-sensitive hashing (LSH) strategy, i.e., similar concepts will be represented by similar responses. This finding has inspired the design of similarity-based algorithms, especially in contrastive learning. Here, we hypothesize that the brain and large neural network models, both using similarity-based learning rules, could contain a similar semantic embedding space. To verify that, this paper proposes a functional Magnetic Resonance Imaging (fMRI) semantic learning network named BrainSem, aimed at seeking a joint semantic latent space that bridges the brain and a Contrastive Language-Image Pre-training (CLIP) model. Given that our perception is inherently cross-modal, we introduce a fuzzy (one-to-many) matching loss function to encourage the models to extract high-level semantic components from neural signals. Our results claimed that using only a small set of fMRI recordings for semantic space alignment, we could obtain shared embedding valid for unseen categories out of the training set, which provided potential evidence for the semantic representation similarity between the brain and large neural networks. In a zero-shot classification task, our BrainSem achieves an 11.6% improvement over the state-of-the-art",
    "checked": true,
    "id": "8ce6128b6c7c3c76f0b6b74e96ce0d4ad5d66bcf",
    "semantic_title": "bridging the semantic latent space between brain and machine: similarity is all you need",
    "citation_count": 0,
    "authors": [
      "Jiaxuan Chen",
      "Yu Qi",
      "Yueming Wang",
      "Gang Pan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29010": {
    "title": "On Disentanglement of Asymmetrical Knowledge Transfer for Modality-Task Agnostic Federated Learning",
    "volume": "main",
    "abstract": "There has been growing concern regarding data privacy during the development and deployment of Multimodal Foundation Models for Artificial General Intelligence (AGI), while Federated Learning (FL) allows multiple clients to collaboratively train models in a privacy-preserving manner. This paper formulates and studies Modality-task Agnostic Federated Learning (AFL) to pave the way toward privacy-preserving AGI. A unique property of AFL is the asymmetrical knowledge relationships among clients due to modality gaps, task gaps, and domain shifts between clients. This raises a challenge in learning an optimal inter-client information-sharing scheme that maximizes positive transfer and minimizes negative transfer for AFL. However, prior FL methods, mostly focusing on symmetrical knowledge transfer, tend to exhibit insufficient positive transfer and fail to fully avoid negative transfer during inter-client collaboration. To address this issue, we propose DisentAFL, which leverages a two-stage Knowledge Disentanglement and Gating mechanism to explicitly decompose the original asymmetrical inter-client information-sharing scheme into several independent symmetrical inter-client information-sharing schemes, each of which corresponds to certain semantic knowledge type learned from the local tasks. Experimental results demonstrate the superiority of our method on AFL than baselines",
    "checked": true,
    "id": "91ee3b2343005ac3ad4571b810d5eadaee541cec",
    "semantic_title": "on disentanglement of asymmetrical knowledge transfer for modality-task agnostic federated learning",
    "citation_count": 0,
    "authors": [
      "Jiayi Chen",
      "Aidong Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29011": {
    "title": "Accelerate Multi-Agent Reinforcement Learning in Zero-Sum Games with Subgame Curriculum Learning",
    "volume": "main",
    "abstract": "Learning Nash equilibrium (NE) in complex zero-sum games with multi-agent reinforcement learning (MARL) can be extremely computationally expensive. Curriculum learning is an effective way to accelerate learning, but an under-explored dimension for generating a curriculum is the difficulty-to-learn of the subgames –games induced by starting from a specific state. In this work, we present a novel subgame curriculum learning framework for zero-sum games. It adopts an adaptive initial state distribution by resetting agents to some previously visited states where they can quickly learn to improve performance. Building upon this framework, we derive a subgame selection metric that approximates the squared distance to NE values and further adopt a particle-based state sampler for subgame generation. Integrating these techniques leads to our new algorithm, Subgame Automatic Curriculum Learning (SACL), which is a realization of the subgame curriculum learning framework. SACL can be combined with any MARL algorithm such as MAPPO. Experiments in the particle-world environment and Google Research Football environment show SACL produces much stronger policies than baselines. In the challenging hide-and-seek quadrant environment, SACL produces all four emergent stages and uses only half the samples of MAPPO with self-play. The project website is at https://sites.google.com/view/sacl-neurips",
    "checked": true,
    "id": "bf9acce5dbeda861a8acdc66d725c9621e7a513f",
    "semantic_title": "accelerate multi-agent reinforcement learning in zero-sum games with subgame curriculum learning",
    "citation_count": 1,
    "authors": [
      "Jiayu Chen",
      "Zelai Xu",
      "Yunfei Li",
      "Chao Yu",
      "Jiaming Song",
      "Huazhong Yang",
      "Fei Fang",
      "Yu Wang",
      "Yi Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29012": {
    "title": "Watch Your Head: Assembling Projection Heads to Save the Reliability of Federated Models",
    "volume": "main",
    "abstract": "Federated learning encounters substantial challenges with heterogeneous data, leading to performance degradation and convergence issues. While considerable progress has been achieved in mitigating such an impact, the reliability aspect of federated models has been largely disregarded. In this study, we conduct extensive experiments to investigate the reliability of both generic and personalized federated models. Our exploration uncovers a significant finding: federated models exhibit unreliability when faced with heterogeneous data, demonstrating poor calibration on in-distribution test data and low uncertainty levels on out-of-distribution data. This unreliability is primarily attributed to the presence of biased projection heads, which introduce miscalibration into the federated models. Inspired by this observation, we propose the \"Assembled Projection Heads\" (APH) method for enhancing the reliability of federated models. By treating the existing projection head parameters as priors, APH randomly samples multiple initialized parameters of projection heads from the prior and further performs targeted fine-tuning on locally available data under varying learning rates. Such a head ensemble introduces parameter diversity into the deterministic model, eliminating the bias and producing reliable predictions via head averaging. We evaluate the effectiveness of the proposed APH method across three prominent federated benchmarks. Experimental results validate the efficacy of APH in model calibration and uncertainty estimation. Notably, APH can be seamlessly integrated into various federated approaches but only requires less than 30% additional computation cost for 100x inferences within large models",
    "checked": true,
    "id": "809442c347e1dface035a0b8ef2fb8033d5606d6",
    "semantic_title": "watch your head: assembling projection heads to save the reliability of federated models",
    "citation_count": 1,
    "authors": [
      "Jinqian Chen",
      "Jihua Zhu",
      "Qinghai Zheng",
      "Zhongyu Li",
      "Zhiqiang Tian"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29013": {
    "title": "Discriminative Forests Improve Generative Diversity for Generative Adversarial Networks",
    "volume": "main",
    "abstract": "Improving the diversity of Artificial Intelligence Generated Content (AIGC) is one of the fundamental problems in the theory of generative models such as generative adversarial networks (GANs). Previous studies have demonstrated that the discriminator in GANs should have high capacity and robustness to achieve the diversity of generated data. However, a discriminator with high capacity tends to overfit and guide the generator toward collapsed equilibrium. In this study, we propose a novel discriminative forest GAN, named Forest-GAN, that replaces the discriminator to improve the capacity and robustness for modeling statistics in real-world data distribution. A discriminative forest is composed of multiple independent discriminators built on bootstrapped data. We prove that a discriminative forest has a generalization error bound, which is determined by the strength of individual discriminators and the correlations among them. Hence, a discriminative forest can provide very large capacity without any risk of overfitting, which subsequently improves the generative diversity. With the discriminative forest framework, we significantly improved the performance of AutoGAN with a new record FID of 19.27 from 30.71 on STL10 and improved the performance of StyleGAN2-ADA with a new record FID of 6.87 from 9.22 on LSUN-cat",
    "checked": true,
    "id": "afac88b17caba51ef734fc2c8b5ec5f4a32f55e4",
    "semantic_title": "discriminative forests improve generative diversity for generative adversarial networks",
    "citation_count": 0,
    "authors": [
      "Junjie Chen",
      "Jiahao Li",
      "Chen Song",
      "Bin Li",
      "Qingcai Chen",
      "Hongchang Gao",
      "Wendy Hui Wang",
      "Zenglin Xu",
      "Xinghua Shi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29014": {
    "title": "Efficient Algorithms for Non-gaussian Single Index Models with Generative Priors",
    "volume": "main",
    "abstract": "In this work, we focus on high-dimensional single index models with non-Gaussian sensing vectors and generative priors. More specifically, our goal is to estimate the underlying signal from i.i.d. realizations of the semi-parameterized single index model, where the underlying signal is contained in (up to a constant scaling) the range of a Lipschitz continuous generative model with bounded low-dimensional inputs, the sensing vector follows a non-Gaussian distribution, the noise is a random variable that is independent of the sensing vector, and the unknown non-linear link function is differentiable. Using the first- and second-order Stein's identity, we introduce efficient algorithms to obtain estimated vectors that achieve the near-optimal statistical rate. Experimental results on image datasets are provided to support our theory",
    "checked": true,
    "id": "ff9d81a270a6b8d0e6c7b9d8ca291945b2152d8d",
    "semantic_title": "efficient algorithms for non-gaussian single index models with generative priors",
    "citation_count": 0,
    "authors": [
      "Junren Chen",
      "Zhaoqiang Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29015": {
    "title": "Audio Scanning Network: Bridging Time and Frequency Domains for Audio Classification",
    "volume": "main",
    "abstract": "With the rapid growth of audio data, there's a pressing need for automatic audio classification. As a type of time-series data, audio exhibits waveform fluctuations in both the time and frequency domains that evolve over time, with similar instances sharing consistent patterns. This study introduces the Audio Scanning Network (ASNet), designed to leverage abundant information for achieving stable and effective audio classification. ASNet captures real-time changes in audio waveforms across both time and frequency domains through reservoir computing, supported by Reservoir Kernel Canonical Correlation Analysis (RKCCA) to explore correlations between time-domain and frequency-domain waveform fluctuations. This innovative approach empowers ASNet to comprehensively capture the changes and inherent correlations within the audio waveform, and without the need for time-consuming iterative training. Instead of converting audio into spectrograms, ASNet directly utilizes audio feature sequences to uncover associations between time and frequency fluctuations. Experiments on environmental sound and music genre classification tasks demonstrate ASNet's comparable performance to state-of-the-art methods",
    "checked": true,
    "id": "821adf02f44ec6b7ec6944c1dfdb4a86eceee984",
    "semantic_title": "audio scanning network: bridging time and frequency domains for audio classification",
    "citation_count": 0,
    "authors": [
      "Liangwei Chen",
      "Xiren Zhou",
      "Huanhuan Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29016": {
    "title": "Deep Contrastive Graph Learning with Clustering-Oriented Guidance",
    "volume": "main",
    "abstract": "Graph Convolutional Network (GCN) has exhibited remarkable potential in improving graph-based clustering. To handle the general clustering scenario without a prior graph, these models estimate an initial graph beforehand to apply GCN. Throughout the literature, we have witnessed that 1) most models focus on the initial graph while neglecting the original features. Therefore, the discriminability of the learned representation may be corrupted by a low-quality initial graph; 2) the training procedure lacks effective clustering guidance, which may lead to the incorporation of clustering-irrelevant information into the learned graph. To tackle these problems, the Deep Contrastive Graph Learning (DCGL) model is proposed for general data clustering. Specifically, we establish a pseudo-siamese network, which incorporates auto-encoder with GCN to emphasize both the graph structure and the original features. On this basis, feature-level contrastive learning is introduced to enhance the discriminative capacity, and the relationship between samples and centroids is employed as the clustering-oriented guidance. Afterward, a two-branch graph learning mechanism is designed to extract the local and global structural relationships, which are further embedded into a unified graph under the cluster-level contrastive guidance. Experimental results on several benchmark datasets demonstrate the superiority of DCGL against state-of-the-art algorithms",
    "checked": true,
    "id": "11af7af76f6b8ac748b8d370993e34b086810760",
    "semantic_title": "deep contrastive graph learning with clustering-oriented guidance",
    "citation_count": 0,
    "authors": [
      "Mulin Chen",
      "Bocheng Wang",
      "Xuelong Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29017": {
    "title": "On the Unstable Convergence Regime of Gradient Descent",
    "volume": "main",
    "abstract": "Traditional gradient descent (GD) has been fully investigated for convex or L-smoothness functions, and it is widely utilized in current neural network optimization. The classical descent lemma ensures that for a function with L-smoothness, the GD trajectory converges stably towards the minimum when the learning rate is below 2 / L. This convergence is marked by a consistent reduction in the loss function throughout the iterations. However, recent experimental studies have demonstrated that even when the L-smoothness condition is not met, or if the learning rate is increased leading to oscillations in the loss function during iterations, the GD trajectory still exhibits convergence over the long run. This phenomenon is referred to as the unstable convergence regime of GD. In this paper, we present a theoretical perspective to offer a qualitative analysis of this phenomenon. The unstable convergence is in fact an inherent property of GD for general twice differentiable functions. Specifically, the forwardinvariance of GD is established, i.e., it ensures that any point within a local region will always remain within this region under GD iteration. Then, based on the forward-invariance, for the initialization outside an open set containing the local minimum, the loss function will oscillate at the first several iterations and then become monotonely decreasing after the GD trajectory jumped into the open set. This work theoretically clarifies the unstable convergence phenomenon of GD discussed in previous experimental works. The unstable convergence of GD mainly depends on the selection of the initialization, and it is actually inevitable due to the complex nature of loss function",
    "checked": true,
    "id": "659bb4836451a0daa62a8fc6c5453e5021731bb4",
    "semantic_title": "on the unstable convergence regime of gradient descent",
    "citation_count": 0,
    "authors": [
      "Shuo Chen",
      "Jiaying Peng",
      "Xiaolong Li",
      "Yao Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29018": {
    "title": "PG-LBO: Enhancing High-Dimensional Bayesian Optimization with Pseudo-Label and Gaussian Process Guidance",
    "volume": "main",
    "abstract": "Variational Autoencoder based Bayesian Optimization (VAE-BO) has demonstrated its excellent performance in addressing high-dimensional structured optimization problems. However, current mainstream methods overlook the potential of utilizing a pool of unlabeled data to construct the latent space, while only concentrating on designing sophisticated models to leverage the labeled data. Despite their effective usage of labeled data, these methods often require extra network structures, additional procedure, resulting in computational inefficiency. To address this issue, we propose a novel method to effectively utilize unlabeled data with the guidance of labeled data. Specifically, we tailor the pseudo-labeling technique from semi-supervised learning to explicitly reveal the relative magnitudes of optimization objective values hidden within the unlabeled data. Based on this technique, we assign appropriate training weights to unlabeled data to enhance the construction of a discriminative latent space. Furthermore, we treat the VAE encoder and the Gaussian Process (GP) in Bayesian optimization as a unified deep kernel learning process, allowing the direct utilization of labeled data, which we term as Gaussian Process guidance. This directly and effectively integrates the goal of improving GP accuracy into the VAE training, thereby guiding the construction of the latent space. The extensive experiments demonstrate that our proposed method outperforms existing VAE-BO algorithms in various optimization scenarios. Our code will be published at https://github.com/TaicaiChen/PG-LBO",
    "checked": true,
    "id": "869828d6b92e40db27422955424968fc1fd6c6d8",
    "semantic_title": "pg-lbo: enhancing high-dimensional bayesian optimization with pseudo-label and gaussian process guidance",
    "citation_count": 2,
    "authors": [
      "Taicai Chen",
      "Yue Duan",
      "Dong Li",
      "Lei Qi",
      "Yinghuan Shi",
      "Yang Gao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29019": {
    "title": "DGPO: Discovering Multiple Strategies with Diversity-Guided Policy Optimization",
    "volume": "main",
    "abstract": "Most reinforcement learning algorithms seek a single optimal strategy that solves a given task. However, it can often be valuable to learn a diverse set of solutions, for instance, to make an agent's interaction with users more engaging, or improve the robustness of a policy to an unexpected perturbance. We propose Diversity-Guided Policy Optimization (DGPO), an on-policy algorithm that discovers multiple strategies for solving a given task. Unlike prior work, it achieves this with a shared policy network trained over a single run. Specifically, we design an intrinsic reward based on an information-theoretic diversity objective. Our final objective alternately constraints on the diversity of the strategies and on the extrinsic reward. We solve the constrained optimization problem by casting it as a probabilistic inference task and use policy iteration to maximize the derived lower bound. Experimental results show that our method efficiently discovers diverse strategies in a wide variety of reinforcement learning tasks. Compared to baseline methods, DGPO achieves comparable rewards, while discovering more diverse strategies, and often with better sample efficiency",
    "checked": true,
    "id": "09093b425cd2315a874cfd57053897b1a1065a6b",
    "semantic_title": "dgpo: discovering multiple strategies with diversity-guided policy optimization",
    "citation_count": 5,
    "authors": [
      "Wentse Chen",
      "Shiyu Huang",
      "Yuan Chiang",
      "Tim Pearce",
      "Wei-Wei Tu",
      "Ting Chen",
      "Jun Zhu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29020": {
    "title": "Exploiting Symmetric Temporally Sparse BPTT for Efficient RNN Training",
    "volume": "main",
    "abstract": "Recurrent Neural Networks (RNNs) are useful in temporal sequence tasks. However, training RNNs involves dense matrix multiplications which require hardware that can support a large number of arithmetic operations and memory accesses. Implementing online training of RNNs on the edge calls for optimized algorithms for an efficient deployment on hardware. Inspired by the spiking neuron model, the Delta RNN exploits temporal sparsity during inference by skipping over the update of hidden states from those inactivated neurons whose change of activation across two timesteps is below a defined threshold. This work describes a training algorithm for Delta RNNs that exploits temporal sparsity in the backward propagation phase to reduce computational requirements for training on the edge. Due to the symmetric computation graphs of forward and backward propagation during training, the gradient computation of inactivated neurons can be skipped. Results show a reduction of ∼80% in matrix operations for training a 56k parameter Delta LSTM on the Fluent Speech Commands dataset with negligible accuracy loss. Logic simulations of a hardware accelerator designed for the training algorithm show 2-10X speedup in matrix computations for an activation sparsity range of 50%-90%. Additionally, we show that the proposed Delta RNN training will be useful for online incremental learning on edge devices with limited computing resources",
    "checked": true,
    "id": "2da7ad53773bb2f73657e14d94c285a7a2ce5786",
    "semantic_title": "exploiting symmetric temporally sparse bptt for efficient rnn training",
    "citation_count": 0,
    "authors": [
      "Xi Chen",
      "Chang Gao",
      "Zuowen Wang",
      "Longbiao Cheng",
      "Sheng Zhou",
      "Shih-Chii Liu",
      "Tobi Delbruck"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29021": {
    "title": "Meta-Inverse Reinforcement Learning for Mean Field Games via Probabilistic Context Variables",
    "volume": "main",
    "abstract": "Designing suitable reward functions for numerous interacting intelligent agents is challenging in real-world applications. Inverse reinforcement learning (IRL) in mean field games (MFGs) offers a practical framework to infer reward functions from expert demonstrations. While promising, the assumption of agent homogeneity limits the capability of existing methods to handle demonstrations with heterogeneous and unknown objectives, which are common in practice. To this end, we propose a deep latent variable MFG model and an associated IRL method. Critically, our method can infer rewards from different yet structurally similar tasks without prior knowledge about underlying contexts or modifying the MFG model itself. Our experiments, conducted on simulated scenarios and a real-world spatial taxi-ride pricing problem, demonstrate the superiority of our approach over state-of-the-art IRL methods in MFGs",
    "checked": true,
    "id": "75b8a8592224e59274fd076925188183fa222009",
    "semantic_title": "meta-inverse reinforcement learning for mean field games via probabilistic context variables",
    "citation_count": 0,
    "authors": [
      "Yang Chen",
      "Xiao Lin",
      "Bo Yan",
      "Libo Zhang",
      "Jiamou Liu",
      "Neset Özkan Tan",
      "Michael Witbrock"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29022": {
    "title": "Exact Policy Recovery in Offline RL with Both Heavy-Tailed Rewards and Data Corruption",
    "volume": "main",
    "abstract": "We study offline reinforcement learning (RL) with heavy-tailed reward distribution and data corruption: (i) Moving beyond subGaussian reward distribution, we allow the rewards to have infinite variances; (ii) We allow corruptions where an attacker can arbitrarily modify a small fraction of the rewards and transitions in the dataset. We first derive a sufficient optimality condition for generalized Pessimistic Value Iteration (PEVI), which allows various estimators with proper confidence bounds and can be applied to multiple learning settings. In order to handle the data corruption and heavy-tailed reward setting, we prove that the trimmed-mean estimation achieves the minimax optimal error rate for robust mean estimation under heavy-tailed distributions. In the PEVI algorithm, we plug in the trimmed mean estimation and the confidence bound to solve the robust offline RL problem. Standard analysis reveals that data corruption induces a bias term in the suboptimality gap, which gives the false impression that any data corruption prevents optimal policy learning. By using the optimality condition for the generalized PEVI, we show that as long as the bias term is less than the ``action gap'', the policy returned by PEVI achieves the optimal value given sufficient data",
    "checked": true,
    "id": "f94c615c263097f771c982e83a0f210e59835a90",
    "semantic_title": "exact policy recovery in offline rl with both heavy-tailed rewards and data corruption",
    "citation_count": 1,
    "authors": [
      "Yiding Chen",
      "Xuezhou Zhang",
      "Qiaomin Xie",
      "Xiaojin Zhu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29023": {
    "title": "Progressive Poisoned Data Isolation for Training-Time Backdoor Defense",
    "volume": "main",
    "abstract": "Deep Neural Networks (DNN) are susceptible to backdoor attacks where malicious attackers manipulate the model's predictions via data poisoning. It is hence imperative to develop a strategy for training a clean model using a potentially poisoned dataset. Previous training-time defense mechanisms typically employ an one-time isolation process, often leading to suboptimal isolation outcomes. In this study, we present a novel and efficacious defense method, termed Progressive Isolation of Poisoned Data (PIPD), that progressively isolates poisoned data to enhance the isolation accuracy and mitigate the risk of benign samples being misclassified as poisoned ones. Once the poisoned portion of the dataset has been identified, we introduce a selective training process to train a clean model. Through the implementation of these techniques, we ensure that the trained model manifests a significantly diminished attack success rate against the poisoned data. Extensive experiments on multiple benchmark datasets and DNN models, assessed against nine state-of-the-art backdoor attacks, demonstrate the superior performance of our PIPD method for backdoor defense. For instance, our PIPD achieves an average True Positive Rate (TPR) of 99.95% and an average False Positive Rate (FPR) of 0.06% for diverse attacks over CIFAR-10 dataset, markedly surpassing the performance of state-of-the-art methods. The code is available at https://github.com/RorschachChen/PIPD.git",
    "checked": true,
    "id": "430a024aaa7b6f6fc759b9b1005a66717fb4d117",
    "semantic_title": "progressive poisoned data isolation for training-time backdoor defense",
    "citation_count": 0,
    "authors": [
      "Yiming Chen",
      "Haiwei Wu",
      "Jiantao Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29024": {
    "title": "Pushing the Limit of Fine-Tuning for Few-Shot Learning: Where Feature Reusing Meets Cross-Scale Attention",
    "volume": "main",
    "abstract": "Due to the scarcity of training samples, Few-Shot Learning (FSL) poses a significant challenge to capture discriminative object features effectively. The combination of transfer learning and meta-learning has recently been explored by pre-training the backbone features using labeled base data and subsequently fine-tuning the model with target data. However, existing meta-learning methods, which use embedding networks, suffer from scaling limitations when dealing with a few labeled samples, resulting in suboptimal results. Inspired by the latest advances in FSL, we further advance the approach of fine-tuning a pre-trained architecture by a strengthened hierarchical feature representation. The technical contributions of this work include: 1) a hybrid design named Intra-Block Fusion (IBF) to strengthen the extracted features within each convolution block; and 2) a novel Cross-Scale Attention (CSA) module to mitigate the scaling inconsistencies arising from the limited training samples, especially for cross-domain tasks. We conducted comprehensive evaluations on standard benchmarks, including three in-domain tasks (miniImageNet, CIFAR-FS, and FC100), as well as two cross-domain tasks (CDFSL and Meta-Dataset). The results have improved significantly over existing state-of-the-art approaches on all benchmark datasets. In particular, the FSL performance on the in-domain FC100 dataset is more than three points better than the latest PMF (Hu et al. 2022)",
    "checked": true,
    "id": "8c2c5cc1feeb69c9a630ea1d7b3d4faf2113f5bf",
    "semantic_title": "pushing the limit of fine-tuning for few-shot learning: where feature reusing meets cross-scale attention",
    "citation_count": 0,
    "authors": [
      "Ying-Yu Chen",
      "Jun-Wei Hsieh",
      "Xin Li",
      "Ming-Ching Chang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29025": {
    "title": "Fed-QSSL: A Framework for Personalized Federated Learning under Bitwidth and Data Heterogeneity",
    "volume": "main",
    "abstract": "Motivated by high resource costs of centralized machine learning schemes as well as data privacy concerns, federated learning (FL) emerged as an efficient alternative that relies on aggregating locally trained models rather than collecting clients' potentially private data. In practice, available resources and data distributions vary from one client to another, creating an inherent system heterogeneity that leads to deterioration of the performance of conventional FL algorithms. In this work, we present a federated quantization-based self-supervised learning scheme (Fed-QSSL) designed to address heterogeneity in FL systems. At clients' side, to tackle data heterogeneity we leverage distributed self-supervised learning while utilizing low-bit quantization to satisfy constraints imposed by local infrastructure and limited communication resources. At server's side, Fed-QSSL deploys de-quantization, weighted aggregation and re-quantization, ultimately creating models personalized to both data distribution as well as specific infrastructure of each client's device. We validated the proposed algorithm on real world datasets, demonstrating its efficacy, and theoretically analyzed impact of low-bit training on the convergence and robustness of the learned models",
    "checked": true,
    "id": "db4e9b958dd9993037c5f66659fc42baae7bf9c9",
    "semantic_title": "fed-qssl: a framework for personalized federated learning under bitwidth and data heterogeneity",
    "citation_count": 1,
    "authors": [
      "Yiyue Chen",
      "Haris Vikalo",
      "Chianing Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29026": {
    "title": "TopoGCL: Topological Graph Contrastive Learning",
    "volume": "main",
    "abstract": "Graph contrastive learning (GCL) has recently emerged as a new concept which allows for capitalizing on the strengths of graph neural networks (GNNs) to learn rich representations in a wide variety of applications which involve abundant unlabeled information. However, existing GCL approaches largely tend to overlook the important latent information on higher-order graph substructures. We address this limitation by introducing the concepts of topological invariance and extended persistence on graphs to GCL. In particular, we propose a new contrastive mode which targets topological representations of the two augmented views from the same graph, yielded by extracting latent shape properties of the graph at multiple resolutions. Along with the extended topological layer, we introduce a new extended persistence summary, namely, extended persistence landscapes (EPL) and derive its theoretical stability guarantees. Our extensive numerical results on biological, chemical, and social interaction graphs show that the new Topological Graph Contrastive Learning (TopoGCL) model delivers significant performance gains in unsupervised graph classification for 8 out of 12 considered datasets and also exhibits robustness under noisy scenarios",
    "checked": true,
    "id": "6a7e657c47d8e3d24c0679e733d6f83e51c21d6c",
    "semantic_title": "topogcl: topological graph contrastive learning",
    "citation_count": 0,
    "authors": [
      "Yuzhou Chen",
      "Jose Frias",
      "Yulia R. Gel"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29027": {
    "title": "Continuous Rotation Group Equivariant Network Inspired by Neural Population Coding",
    "volume": "main",
    "abstract": "Neural population coding can represent continuous information by neurons with a series of discrete preferred stimuli, and we find that the bell-shaped tuning curve plays an important role in this mechanism. Inspired by this, we incorporate a bell-shaped tuning curve into the discrete group convolution to achieve continuous group equivariance. Simply, we modulate group convolution kernels by Gauss functions to obtain bell-shaped tuning curves. Benefiting from the modulation, kernels also gain smooth gradients on geometric dimensions (e.g., location dimension and orientation dimension). It allows us to generate group convolution kernels from sparse weights with learnable geometric parameters, which can achieve both competitive performances and parameter efficiencies. Furthermore, we quantitatively prove that discrete group convolutions with proper tuning curves (bigger than 1x sampling step) can achieve continuous equivariance. Experimental results show that 1) our approach achieves very competitive performances on MNIST-rot with at least 75% fewer parameters compared with previous SOTA methods, which is efficient in parameter; 2) Especially with small sample sizes, our approach exhibits more pronounced performance improvements (up to 24%); 3) It also has excellent rotation generalization ability on various datasets such as MNIST, CIFAR, and ImageNet with both plain and ResNet architectures",
    "checked": true,
    "id": "ddebe7712cded96672460a9597934f5f7a16bc27",
    "semantic_title": "continuous rotation group equivariant network inspired by neural population coding",
    "citation_count": 0,
    "authors": [
      "Zhiqiang Chen",
      "Yang Chen",
      "Xiaolong Zou",
      "Shan Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29028": {
    "title": "Diagnosing and Rectifying Fake OOD Invariance: A Restructured Causal Approach",
    "volume": "main",
    "abstract": "Invariant representation learning (IRL) encourages the prediction from invariant causal features to labels deconfounded from the environments, advancing the technical roadmap of out-of-distribution (OOD) generalization. Despite spotlights around, recent theoretical result verified that some causal features recovered by IRLs merely pretend domain-invariantly in the training environments but fail in unseen domains. The fake invariance severely endangers OOD generalization since the trustful objective can not be diagnosed and existing causal remedies are invalid to rectify. In this paper, we review a IRL family (InvRat) under the Partially and Fully Informative Invariant Feature Structural Causal Models (PIIF SCM /FIIF SCM) respectively, to certify their weaknesses in representing fake invariant features, then, unify their causal diagrams to propose ReStructured SCM (RS-SCM). RS-SCM can ideally rebuild the spurious and the fake invariant features simultaneously. Given this, we further develop an approach based on conditional mutual information with respect to RS-SCM, then rigorously rectify the spurious and fake invariant effects. It can be easily implemented by a small feature selection subnet introduced in the IRL family, which is alternatively optimized to achieve our goal. Experiments verified the superiority of our approach to fight against the fake invariant issue across a variety of OOD generalization benchmarks",
    "checked": true,
    "id": "896223136a4746cc711405fe0ad6127941acf6ce",
    "semantic_title": "diagnosing and rectifying fake ood invariance: a restructured causal approach",
    "citation_count": 0,
    "authors": [
      "Ziliang Chen",
      "Yongsen Zheng",
      "Zhao-Rong Lai",
      "Quanlong Guan",
      "Liang Lin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29029": {
    "title": "Instrumental Variable Estimation for Causal Inference in Longitudinal Data with Time-Dependent Latent Confounders",
    "volume": "main",
    "abstract": "Causal inference from longitudinal observational data is a challenging problem due to the difficulty in correctly identifying the time-dependent confounders, especially in the presence of latent time-dependent confounders. Instrumental variable (IV) is a powerful tool for addressing the latent confounders issue, but the traditional IV technique cannot deal with latent time-dependent confounders in longitudinal studies. In this work, we propose a novel Time-dependent Instrumental Factor Model (TIFM) for time-varying causal effect estimation from data with latent time-dependent confounders. At each time-step, the proposed TIFM method employs the Recurrent Neural Network (RNN) architecture to infer latent IV, and then uses the inferred latent IV factor for addressing the confounding bias caused by the latent time-dependent confounders. We provide a theoretical analysis for the proposed TIFM method regarding causal effect estimation in longitudinal data. Extensive evaluation with synthetic datasets demonstrates the effectiveness of TIFM in addressing causal effect estimation over time. We further apply TIFM to a climate dataset to showcase the potential of the proposed method in tackling real-world problems",
    "checked": true,
    "id": "81107fdb4d86dbb4109fc6cc5cd089c26a243c46",
    "semantic_title": "instrumental variable estimation for causal inference in longitudinal data with time-dependent latent confounders",
    "citation_count": 0,
    "authors": [
      "Debo Cheng",
      "Ziqi Xu",
      "Jiuyong Li",
      "Lin Liu",
      "Jixue Liu",
      "Wentao Gao",
      "Thuc Duy Le"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29030": {
    "title": "Hierarchize Pareto Dominance in Multi-Objective Stochastic Linear Bandits",
    "volume": "main",
    "abstract": "Multi-objective Stochastic Linear bandit (MOSLB) plays a critical role in the sequential decision-making paradigm, however, most existing methods focus on the Pareto dominance among different objectives without considering any priority. In this paper, we study bandit algorithms under mixed Pareto-lexicographic orders, which can reflect decision makers' preferences. We adopt the Grossone approach to deal with these orders and develop the notion of Pareto-lexicographic optimality to evaluate the learners' performance. Our work represents a first attempt to address these important and realistic orders in bandit algorithms. To design algorithms under these orders, the upper confidence bound (UCB) policy and the prior free lexicographical filter are adapted to approximate the optimal arms at each round. Moreover, the framework of the algorithms involves two stages in pursuit of the balance between exploration and exploitation. Theoretical analysis as well as numerical experiments demonstrate the effectiveness of our algorithms",
    "checked": true,
    "id": "1fc48db7471a9248b52f19e98ebc4c1e63a5034f",
    "semantic_title": "hierarchize pareto dominance in multi-objective stochastic linear bandits",
    "citation_count": 0,
    "authors": [
      "Ji Cheng",
      "Bo Xue",
      "Jiaxiang Yi",
      "Qingfu Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29031": {
    "title": "FedGCR: Achieving Performance and Fairness for Federated Learning with Distinct Client Types via Group Customization and Reweighting",
    "volume": "main",
    "abstract": "To achieve better performance and greater fairness in Federated Learning (FL), much of the existing research has centered on individual clients, using domain adaptation techniques and redesigned aggregation schemes to counteract client data heterogeneity. However, an overlooked scenario exists where clients belong to distinctive groups, or, client types, in which groups of clients share similar characteristics such as device specifications or data patterns. Despite being common in group collaborations, this scenario has been overlooked in previous research, potentially leading to performance degradation and systemic biases against certain client types. To bridge this gap, we introduce Federated learning with Group Customization and Reweighting (FedGCR). FedGCR enhances both performance and fairness for FL with Distinct Client Types, consisting of a Federated Group Customization (FedGC) model to provide customization via a novel prompt tuning technique to mitigate the data disparity across different client-types, and a Federated Group Reweighting (FedGR) aggregation scheme to ensure uniform and unbiased performances between clients and between client types by a novel reweighting approach. Extensive experiment comparisons with prior FL methods in domain adaptation and fairness demonstrate the superiority of FedGCR in all metrics, including the overall accuracy and performance uniformity in both the group and the individual level. FedGCR achieves 82.74% accuracy and 12.26(↓) in performance uniformity on the Digit-Five dataset and 81.88% and 14.88%(↓) on DomainNet with a domain imbalance factor of 10, which significantly outperforms the state-of-the-art. Code is available at https://github.com/celinezheng/fedgcr",
    "checked": true,
    "id": "089b447a1a9d9cf3cd2d9b23faf4280ecc76a6b4",
    "semantic_title": "fedgcr: achieving performance and fairness for federated learning with distinct client types via group customization and reweighting",
    "citation_count": 0,
    "authors": [
      "Shu-Ling Cheng",
      "Chin-Yuan Yeh",
      "Ting-An Chen",
      "Eliana Pastor",
      "Ming-Syan Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29032": {
    "title": "Clarifying the Behavior and the Difficulty of Adversarial Training",
    "volume": "main",
    "abstract": "Adversarial training is usually difficult to optimize. This paper provides conceptual and analytic insights into the difficulty of adversarial training via a simple theoretical study, where we derive an approximate dynamics of a recursive multi-step attack in a simple setting. Despite the simplicity of our theory, it still reveals verifiable predictions about various phenomena in adversarial training under real-world settings. First, compared to vanilla training, adversarial training is more likely to boost the influence of input samples with large gradient norms in an exponential manner. Besides, adversarial training also strengthens the influence of the Hessian matrix of the loss w.r.t. network parameters, which is more likely to make network parameters oscillate and boosts the difficulty of adversarial training",
    "checked": true,
    "id": "8147d9decb0ef794ca5ac5b3412036c15679ca98",
    "semantic_title": "clarifying the behavior and the difficulty of adversarial training",
    "citation_count": 0,
    "authors": [
      "Xu Cheng",
      "Hao Zhang",
      "Yue Xin",
      "Wen Shen",
      "Quanshi Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29033": {
    "title": "Arithmetic Feature Interaction Is Necessary for Deep Tabular Learning",
    "volume": "main",
    "abstract": "Until recently, the question of the effective inductive bias of deep models on tabular data has remained unanswered. This paper investigates the hypothesis that arithmetic feature interaction is necessary for deep tabular learning. To test this point, we create a synthetic tabular dataset with a mild feature interaction assumption and examine a modified transformer architecture enabling arithmetical feature interactions, referred to as AMFormer. Results show that AMFormer outperforms strong counterparts in fine-grained tabular data modeling, data efficiency in training, and generalization. This is attributed to its parallel additive and multiplicative attention operators and prompt-based optimization, which facilitate the separation of tabular samples in an extended space with arithmetically-engineered features. Our extensive experiments on real-world data also validate the consistent effectiveness, efficiency, and rationale of AMFormer, suggesting it has established a strong inductive bias for deep learning on tabular data. Code is available at https://github.com/aigc-apps/AMFormer",
    "checked": true,
    "id": "4c58009a011a98f505e641a1a8873fba008f03be",
    "semantic_title": "arithmetic feature interaction is necessary for deep tabular learning",
    "citation_count": 0,
    "authors": [
      "Yi Cheng",
      "Renjun Hu",
      "Haochao Ying",
      "Xing Shi",
      "Jian Wu",
      "Wei Lin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29034": {
    "title": "CUTS+: High-Dimensional Causal Discovery from Irregular Time-Series",
    "volume": "main",
    "abstract": "Causal discovery in time-series is a fundamental problem in the machine learning community, enabling causal reasoning and decision-making in complex scenarios. Recently, researchers successfully discover causality by combining neural networks with Granger causality, but their performances degrade largely when encountering high-dimensional data because of the highly redundant network design and huge causal graphs. Moreover, the missing entries in the observations further hamper the causal structural learning. To overcome these limitations, We propose CUTS+, which is built on the Granger-causality-based causal discovery method CUTS and raises the scalability by introducing a technique called Coarse-to-fine-discovery (C2FD) and leveraging a message-passing-based graph neural network (MPGNN). Compared to previous methods on simulated, quasi-real, and real datasets, we show that CUTS+ largely improves the causal discovery performance on high-dimensional data with different types of irregular sampling",
    "checked": true,
    "id": "f66c14b41d385dfb071bb8e72a74d03111d0cade",
    "semantic_title": "cuts+: high-dimensional causal discovery from irregular time-series",
    "citation_count": 7,
    "authors": [
      "Yuxiao Cheng",
      "Lianglong Li",
      "Tingxiong Xiao",
      "Zongren Li",
      "Jinli Suo",
      "Kunlun He",
      "Qionghai Dai"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29035": {
    "title": "Generalized Variational Inference via Optimal Transport",
    "volume": "main",
    "abstract": "Variational Inference (VI) has gained popularity as a flexible approximate inference scheme for computing posterior distributions in Bayesian models. Original VI methods use Kullback-Leibler (KL) divergence to construct variational objectives. However, KL divergence has zero-forcing behavior and is completely agnostic to the metric of the underlying data distribution, resulting in bad approximations. To alleviate this issue, we propose a new variational objective by using Optimal Transport (OT) distance, which is a metric-aware divergence, to measure the difference between approximate posteriors and priors. The superior performance of OT distance enables us to learn more accurate approximations. We further enhance the objective by gradually including the OT term using a hyperparameter λ for over-parameterized models. We develop a Variational inference method with OT (VOT) which presents a gradient-based black-box framework for solving Bayesian models, even when the density function of approximate distribution is not available. We provide the consistency analysis of approximate posteriors and demonstrate the practical effectiveness on Bayesian neural networks and variational autoencoders",
    "checked": true,
    "id": "83b48887913cf24beb2c360052a8548fddfe5ce6",
    "semantic_title": "generalized variational inference via optimal transport",
    "citation_count": 0,
    "authors": [
      "Jinjin Chi",
      "Zhichao Zhang",
      "Zhiyao Yang",
      "Jihong Ouyang",
      "Hongbin Pei"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29036": {
    "title": "Operator-Learning-Inspired Modeling of Neural Ordinary Differential Equations",
    "volume": "main",
    "abstract": "Neural ordinary differential equations (NODEs), one of the most influential works of the differential equation-based deep learning, are to continuously generalize residual networks and opened a new field. They are currently utilized for various downstream tasks, e.g., image classification, time series classification, image generation, etc. Its key part is how to model the time-derivative of the hidden state, denoted dh(t)/dt. People have habitually used conventional neural network architectures, e.g., fully-connected layers followed by non-linear activations. In this paper, however, we present a neural operator-based method to define the time-derivative term. Neural operators were initially proposed to model the differential operator of partial differential equations (PDEs). Since the time-derivative of NODEs can be understood as a special type of the differential operator, our proposed method, called branched Fourier neural operator (BFNO), makes sense. In our experiments with general downstream tasks, our method significantly outperforms existing methods",
    "checked": true,
    "id": "b138810be1eae411e776c72298d98ab06a4af669",
    "semantic_title": "operator-learning-inspired modeling of neural ordinary differential equations",
    "citation_count": 0,
    "authors": [
      "Woojin Cho",
      "Seunghyeon Cho",
      "Hyundong Jin",
      "Jinsung Jeon",
      "Kookjin Lee",
      "Sanghyun Hong",
      "Dongeun Lee",
      "Jonghyun Choi",
      "Noseong Park"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29037": {
    "title": "Make Prompts Adaptable: Bayesian Modeling for Vision-Language Prompt Learning with Data-Dependent Prior",
    "volume": "main",
    "abstract": "Recent vision-language pre-trained (VLP) models have become the backbone for many downstream tasks, but they are utilized as frozen model without learning. Prompt learning is a method to improve the pre-trained VLP model by adding a learnable context vector to the inputs of the text encoder. In a few-shot learning scenario of the downstream task, MLE training can lead the context vector to over-fit dominant image features in the training data. This overfitting can potentially harm the generalization ability, especially in the presence of a distribution shift between the training and test dataset. This paper presents a Bayesian-based framework of prompt tuning, which could alleviate the over-fitting issues on few-shot learning application and increase the adaptability of prompts on unobserved instances. Specifically, modeling data-dependent prior enhances the adaptability of text features for both seen and unseen image features without the trade-off of performance between them. Based on the Bayesian framework, we utilize the Wasserstein gradient flow in the estimation of our target posterior distribution, which enables our prompt to be flexible in capturing the complex modes of image features. We demonstrate the effectiveness of our method on benchmark datasets for several experiments by showing statistically significant improvements on performance compared to existing methods",
    "checked": true,
    "id": "326d8eb214385bcec485b4d9004e3b0ac1a06ba2",
    "semantic_title": "make prompts adaptable: bayesian modeling for vision-language prompt learning with data-dependent prior",
    "citation_count": 1,
    "authors": [
      "Youngjae Cho",
      "HeeSun Bae",
      "Seungjae Shin",
      "Yeo Dong Youn",
      "Weonyoung Joo",
      "Il-Chul Moon"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29038": {
    "title": "SNN-PDE: Learning Dynamic PDEs from Data with Simplicial Neural Networks",
    "volume": "main",
    "abstract": "Dynamics of many complex systems, from weather and climate to spread of infectious diseases, can be described by partial differential equations (PDEs). Such PDEs involve unknown function(s), partial derivatives, and typically multiple independent variables. The traditional numerical methods for solving PDEs assume that the data are observed on a regular grid. However, in many applications, for example, weather and air pollution monitoring delivered by the arbitrary located weather stations of the National Weather Services, data records are irregularly spaced. Furthermore, in problems involving prediction analytics such as forecasting wildfire smoke plumes, the primary focus may be on a set of irregular locations associated with urban development. In recent years, deep learning (DL) methods and, in particular, graph neural networks (GNNs) have emerged as a new promising tool that can complement traditional PDE solvers in scenarios of the irregular spaced data, contributing to the newest research trend of physics informed machine learning (PIML). However, most existing PIML methods tend to be limited in their ability to describe higher dimensional structural properties exhibited by real world phenomena, especially, ones that live on manifolds. To address this fundamental challenge, we bring the elements of the Hodge theory and, in particular, simplicial convolution defined on the Hodge Laplacian to the emerging nexus of DL and PDEs. In contrast to conventional Laplacian and the associated convolution operation, the simplicial convolution allows us to rigorously describe diffusion across higher order structures and to better approximate the complex underlying topology and geometry of the data. The new approach, Simplicial Neural Networks for Partial Differential Equations (SNN PDE) offers a computationally efficient yet effective solution for time dependent PDEs. Our studies of a broad range of synthetic data and wildfire processes demonstrate that SNN PDE improves upon state of the art baselines in handling unstructured grids and irregular time intervals of complex physical systems and offers competitive forecasting capabilities for weather and air quality forecasting",
    "checked": true,
    "id": "b2928c9c56daf45d01b8df307cb7baf81ffbc7bc",
    "semantic_title": "snn-pde: learning dynamic pdes from data with simplicial neural networks",
    "citation_count": 0,
    "authors": [
      "Jae Choi",
      "Yuzhou Chen",
      "Huikyo Lee",
      "Hyun Kim",
      "Yulia R. Gel"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29039": {
    "title": "Unsupervised Object Interaction Learning with Counterfactual Dynamics Models",
    "volume": "main",
    "abstract": "We present COIL (Counterfactual Object Interaction Learning), a novel way of learning skills of object interactions on entity-centric environments. The goal is to learn primitive behaviors that can induce interactions without external reward or any supervision. Existing skill discovery methods are limited to locomotion, simple navigation tasks, or single-object manipulation tasks, mostly not inducing interaction between objects. Unlike a monolithic representation usually used in prior skill learning methods, we propose to use a structured goal representation that can query and scope which objects to interact with, which can serve as a basis for solving more complex downstream tasks. We design a novel counterfactual intrinsic reward through the use of either a forward model or successor features that can learn an interaction skill between a pair of objects given as a goal. Through experiments on continuous control environments such as Magnetic Block and 2.5-D Stacking Box, we demonstrate that an agent can learn object interaction behaviors (e.g., attaching or stacking one block to another) without any external rewards or domain-specific knowledge",
    "checked": true,
    "id": "b4c1587e4e43b77991211cddf214eb7232e792d0",
    "semantic_title": "unsupervised object interaction learning with counterfactual dynamics models",
    "citation_count": 3,
    "authors": [
      "Jongwook Choi",
      "Sungtae Lee",
      "Xinyu Wang",
      "Sungryull Sohn",
      "Honglak Lee"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29040": {
    "title": "DUEL: Duplicate Elimination on Active Memory for Self-Supervised Class-Imbalanced Learning",
    "volume": "main",
    "abstract": "Recent machine learning algorithms have been developed using well-curated datasets, which often require substantial cost and resources. On the other hand, the direct use of raw data often leads to overfitting towards frequently occurring class information. To address class imbalances cost-efficiently, we propose an active data filtering process during self-supervised pre-training in our novel framework, Duplicate Elimination (DUEL). This framework integrates an active memory inspired by human working memory and introduces distinctiveness information, which measures the diversity of the data in the memory, to optimize both the feature extractor and the memory. The DUEL policy, which replaces the most duplicated data with new samples, aims to enhance the distinctiveness information in the memory and thereby mitigate class imbalances. We validate the effectiveness of the DUEL framework in class-imbalanced environments, demonstrating its robustness and providing reliable results in downstream tasks. We also analyze the role of the DUEL policy in the training process through various metrics and visualizations",
    "checked": true,
    "id": "99661b266b019da370777d4a8b9b84dcebc2d868",
    "semantic_title": "duel: duplicate elimination on active memory for self-supervised class-imbalanced learning",
    "citation_count": 0,
    "authors": [
      "Won-Seok Choi",
      "Hyundo Lee",
      "Dong-Sig Han",
      "Junseok Park",
      "Heeyeon Koo",
      "Byoung-Tak Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29041": {
    "title": "Consistency-Guided Temperature Scaling Using Style and Content Information for Out-of-Domain Calibration",
    "volume": "main",
    "abstract": "Research interests in the robustness of deep neural networks against domain shifts have been rapidly increasing in recent years. Most existing works, however, focus on improving the accuracy of the model, not the calibration performance which is another important requirement for trustworthy AI systems. Temperature scaling (TS), an accuracy-preserving post-hoc calibration method, has been proven to be effective in in-domain settings, but not in out-of-domain (OOD) due to the difficulty in obtaining a validation set for the unseen domain beforehand. In this paper, we propose consistency-guided temperature scaling (CTS), a new temperature scaling strategy that can significantly enhance the OOD calibration performance by providing mutual supervision among data samples in the source domains. Motivated by our observation that over-confidence stemming from inconsistent sample predictions is the main obstacle to OOD calibration, we propose to guide the scaling process by taking consistencies into account in terms of two different aspects - style and content - which are the key components that can well-represent data samples in multi-domain settings. Experimental results demonstrate that our proposed strategy outperforms existing works, achieving superior OOD calibration performance on various datasets. This can be accomplished by employing only the source domains without compromising accuracy, making our scheme directly applicable to various trustworthy AI systems",
    "checked": true,
    "id": "91442877da5db45121eebb27178d2d34395fa0a8",
    "semantic_title": "consistency-guided temperature scaling using style and content information for out-of-domain calibration",
    "citation_count": 0,
    "authors": [
      "Wonjeong Choi",
      "Jungwuk Park",
      "Dong-Jun Han",
      "Younghyun Park",
      "Jaekyun Moon"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29042": {
    "title": "A Provably Accurate Randomized Sampling Algorithm for Logistic Regression",
    "volume": "main",
    "abstract": "In statistics and machine learning, logistic regression is a widely-used supervised learning technique primarily employed for binary classification tasks. When the number of observations greatly exceeds the number of predictor variables, we present a simple, randomized sampling-based algorithm for logistic regression problem that guarantees high-quality approximations to both the estimated probabilities and the overall discrepancy of the model. Our analysis builds upon two simple structural conditions that boil down to randomized matrix multiplication, a fundamental and well-understood primitive of randomized numerical linear algebra. We analyze the properties of estimated probabilities of logistic regression when leverage scores are used to sample observations, and prove that accurate approximations can be achieved with a sample whose size is much smaller than the total number of observations. To further validate our theoretical findings, we conduct comprehensive empirical evaluations. Overall, our work sheds light on the potential of using randomized sampling approaches to efficiently approximate the estimated probabilities in logistic regression, offering a practical and computationally efficient solution for large-scale datasets",
    "checked": true,
    "id": "83fc9c7aa541888b33f5ebd002683422386e9f99",
    "semantic_title": "a provably accurate randomized sampling algorithm for logistic regression",
    "citation_count": 1,
    "authors": [
      "Agniva Chowdhury",
      "Pradeep Ramuhalli"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29043": {
    "title": "Graph-Based Prediction and Planning Policy Network (GP3Net) for Scalable Self-Driving in Dynamic Environments Using Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "Recent advancements in motion planning for Autonomous Vehicles (AVs) show great promise in using expert driver behaviors in non-stationary driving environments. However, learning only through expert drivers needs more generalizability to recover from domain shifts and near-failure scenarios due to the dynamic behavior of traffic participants and weather conditions. A deep Graph-based Prediction and Planning Policy Network (GP3Net) framework is proposed for non-stationary environments that encodes the interactions between traffic participants with contextual information and provides a decision for safe maneuver for AV. A spatio-temporal graph models the interactions between traffic participants for predicting the future trajectories of those participants. The predicted trajectories are utilized to generate a future occupancy map around the AV with uncertainties embedded to anticipate the evolving non-stationary driving environments. Then the contextual information and future occupancy maps are input to the policy network of the GP3Net framework and trained using Proximal Policy Optimization (PPO) algorithm. The proposed GP3Net performance is evaluated on standard CARLA benchmarking scenarios with domain shifts of traffic patterns (urban, highway, and mixed). The results show that the GP3Net outperforms previous state-of-the-art imitation learning-based planning models for different towns. Further, in unseen new weather conditions, GP3Net completes the desired route with fewer traffic infractions. Finally, the results emphasize the advantage of including the prediction module to enhance safety measures in non-stationary environments",
    "checked": true,
    "id": "0700d371dd7bddc82c5a9b9a01c6eb7ccafc1824",
    "semantic_title": "graph-based prediction and planning policy network (gp3net) for scalable self-driving in dynamic environments using deep reinforcement learning",
    "citation_count": 1,
    "authors": [
      "Jayabrata Chowdhury",
      "Venkataramanan Shivaraman",
      "Suresh Sundaram",
      "PB Sujit"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29044": {
    "title": "Lyapunov-Stable Deep Equilibrium Models",
    "volume": "main",
    "abstract": "Deep equilibrium (DEQ) models have emerged as a promising class of implicit layer models, which abandon traditional depth by solving for the fixed points of a single nonlinear layer. Despite their success, the stability of the fixed points for these models remains poorly understood. By considering DEQ models as nonlinear dynamic systems, we propose a robust DEQ model named LyaDEQ with guaranteed provable stability via Lyapunov theory. The crux of our method is ensuring the Lyapunov stability of the DEQ model's fixed points, which enables the proposed model to resist minor initial perturbations. To avoid poor adversarial defense due to Lyapunov-stable fixed points being located near each other, we orthogonalize the layers after the Lyapunov stability module to separate different fixed points. We evaluate LyaDEQ models under well-known adversarial attacks, and experimental results demonstrate significant improvement in robustness. Furthermore, we show that the LyaDEQ model can be combined with other defense methods, such as adversarial training, to achieve even better adversarial robustness",
    "checked": true,
    "id": "f1680f1f912f9bd8cd605b83deb0ca29dae28ea1",
    "semantic_title": "lyapunov-stable deep equilibrium models",
    "citation_count": 0,
    "authors": [
      "Haoyu Chu",
      "Shikui Wei",
      "Ting Liu",
      "Yao Zhao",
      "Yuto Miyatake"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29045": {
    "title": "Make RepVGG Greater Again: A Quantization-Aware Approach",
    "volume": "main",
    "abstract": "The tradeoff between performance and inference speed is critical for practical applications. Architecture reparameterization obtains better tradeoffs and it is becoming an increasingly popular ingredient in modern convolutional neural networks. Nonetheless, its quantization performance is usually too poor to deploy (e.g. more than 20% top-1 accuracy drop on ImageNet) when INT8 inference is desired. In this paper, we dive into the underlying mechanism of this failure, where the original design inevitably enlarges quantization error. We propose a simple, robust, and effective remedy to have a quantization-friendly structure that also enjoys reparameterization benefits. Our method greatly bridges the gap between INT8 and FP32 accuracy for RepVGG. Without bells and whistles, the top-1 accuracy drop on ImageNet is reduced within 2% by standard post-training quantization. Extensive experiments on detection and semantic segmentation tasks verify its generalization",
    "checked": true,
    "id": "299df264a9c79de56bcd0c975a3471be22a589af",
    "semantic_title": "make repvgg greater again: a quantization-aware approach",
    "citation_count": 28,
    "authors": [
      "Xiangxiang Chu",
      "Liang Li",
      "Bo Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29046": {
    "title": "Meta-Reinforcement Learning via Exploratory Task Clustering",
    "volume": "main",
    "abstract": "Meta-reinforcement learning (meta-RL) aims to quickly solve new RL tasks by leveraging knowledge from prior tasks. Previous studies often assume a single-mode homogeneous task distribution, ignoring possible structured heterogeneity among tasks. Such an oversight can hamper effective exploration and adaptation, especially with limited samples. In this work, we harness the structured heterogeneity among tasks via clustering to improve meta-RL, which facilitates knowledge sharing at the cluster level. To facilitate exploration, we also develop a dedicated cluster-level exploratory policy to discover task clusters via divide-and-conquer. The knowledge from the discovered clusters helps to narrow the search space of task-specific policy learning, leading to more sample-efficient policy adaptation. We evaluate the proposed method on environments with parametric clusters (e.g., rewards and state dynamics in the MuJoCo suite) and non-parametric clusters (e.g., control skills in the Meta-World suite). The results demonstrate strong advantages of our solution against a set of representative meta-RL methods",
    "checked": true,
    "id": "06ad7a1b42b4a6a395ac01091e5cafeea23918ab",
    "semantic_title": "meta-reinforcement learning via exploratory task clustering",
    "citation_count": 1,
    "authors": [
      "Zhendong Chu",
      "Renqin Cai",
      "Hongning Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29047": {
    "title": "Task-Driven Causal Feature Distillation: Towards Trustworthy Risk Prediction",
    "volume": "main",
    "abstract": "Since artificial intelligence has seen tremendous recent successes in many areas, it has sparked great interest in its potential for trustworthy and interpretable risk prediction. However, most models lack causal reasoning and struggle with class imbalance, leading to poor precision and recall. To address this, we propose a Task-Driven Causal Feature Distillation model (TDCFD) to transform original feature values into causal feature attributions for the specific risk prediction task. The causal feature attribution helps describe how much contribution the value of this feature can make to the risk prediction result. After the causal feature distillation, a deep neural network is applied to produce trustworthy prediction results with causal interpretability and high precision/recall. We evaluate the performance of our TDCFD method on several synthetic and real datasets, and the results demonstrate its superiority over the state-of-the-art methods regarding precision, recall, interpretability, and causality",
    "checked": true,
    "id": "6c340a25d293e37e63208adec8843b6e47f87ab7",
    "semantic_title": "task-driven causal feature distillation: towards trustworthy risk prediction",
    "citation_count": 9,
    "authors": [
      "Zhixuan Chu",
      "Mengxuan Hu",
      "Qing Cui",
      "Longfei Li",
      "Sheng Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29048": {
    "title": "Resource Efficient Deep Learning Hardware Watermarks with Signature Alignment",
    "volume": "main",
    "abstract": "Deep learning intellectual properties (IPs) are high-value assets that are frequently susceptible to theft. This vulnerability has led to significant interest in defending the field's intellectual properties from theft. Recently, watermarking techniques have been extended to protect deep learning hardware from privacy. These technique embed modifications that change the hardware's behavior when activated. In this work, we propose the first method for embedding watermarks in deep learning hardware that incorporates the owner's key samples into the embedding methodology. This improves our watermarks' reliability and efficiency in identifying the hardware over those generated using randomly selected key samples. Our experimental results demonstrate that by considering the target key samples when generating the hardware modifications, we can significantly increase the embedding success rate while targeting fewer functional blocks, decreasing the required hardware overhead needed to defend it",
    "checked": true,
    "id": "e108bd845dc09cb82085ac1d6386c5203b5eb4c8",
    "semantic_title": "resource efficient deep learning hardware watermarks with signature alignment",
    "citation_count": 0,
    "authors": [
      "Joseph Clements",
      "Yingjie Lao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29049": {
    "title": "RLfOLD: Reinforcement Learning from Online Demonstrations in Urban Autonomous Driving",
    "volume": "main",
    "abstract": "Reinforcement Learning from Demonstrations (RLfD) has emerged as an effective method by fusing expert demonstrations into Reinforcement Learning (RL) training, harnessing the strengths of both Imitation Learning (IL) and RL. However, existing algorithms rely on offline demonstrations, which can introduce a distribution gap between the demonstrations and the actual training environment, limiting their performance. In this paper, we propose a novel approach, Reinforcement Learning from Online Demonstrations (RLfOLD), that leverages online demonstrations to address this limitation, ensuring the agent learns from relevant and up-to-date scenarios, thus effectively bridging the distribution gap. Unlike conventional policy networks used in typical actor-critic algorithms, RLfOLD introduces a policy network that outputs two standard deviations: one for exploration and the other for IL training. This novel design allows the agent to adapt to varying levels of uncertainty inherent in both RL and IL. Furthermore, we introduce an exploration process guided by an online expert, incorporating an uncertainty-based technique. Our experiments on the CARLA NoCrash benchmark demonstrate the effectiveness and efficiency of RLfOLD. Notably, even with a significantly smaller encoder and a single camera setup, RLfOLD surpasses state-of-the-art methods in this evaluation. These results, achieved with limited resources, highlight RLfOLD as a highly promising solution for real-world applications",
    "checked": true,
    "id": "4186f74da6d793c91c5ca4d8a10cf2f8638eade6",
    "semantic_title": "rlfold: reinforcement learning from online demonstrations in urban autonomous driving",
    "citation_count": 1,
    "authors": [
      "Daniel Coelho",
      "Miguel Oliveira",
      "Vitor Santos"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29050": {
    "title": "Big Learning Expectation Maximization",
    "volume": "main",
    "abstract": "Mixture models serve as one fundamental tool with versatile applications. However, their training techniques, like the popular Expectation Maximization (EM) algorithm, are notoriously sensitive to parameter initialization and often suffer from bad local optima that could be arbitrarily worse than the optimal. To address the long-lasting bad-local-optima challenge, we draw inspiration from the recent ground-breaking foundation models and propose to leverage their underlying big learning principle to upgrade the EM. Specifically, we present the Big Learning EM (BigLearn-EM), an EM upgrade that simultaneously performs joint, marginal, and orthogonally transformed marginal matchings between data and model distributions. Through simulated experiments, we empirically show that the BigLearn-EM is capable of delivering the optimal with high probability; comparisons on benchmark clustering datasets further demonstrate its effectiveness and advantages over existing techniques. The code is available at https://github.com/YulaiCong/Big-Learning-Expectation-Maximization",
    "checked": true,
    "id": "da57e36840b31d27342c98306890eef569cfd31c",
    "semantic_title": "big learning expectation maximization",
    "citation_count": 0,
    "authors": [
      "Yulai Cong",
      "Sijia Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29051": {
    "title": "Time-Aware Knowledge Representations of Dynamic Objects with Multidimensional Persistence",
    "volume": "main",
    "abstract": "Learning time-evolving objects such as multivariate time series and dynamic networks requires the development of novel knowledge representation mechanisms and neural network architectures, which allow for capturing implicit time-dependent information contained in the data. Such information is typically not directly observed but plays a key role in the learning task performance. In turn, lack of time dimension in knowledge encoding mechanisms for time-dependent data leads to frequent model updates, poor learning performance, and, as a result, subpar decision-making. Here we propose a new approach to a time-aware knowledge representation mechanism that notably focuses on implicit time-dependent topological information along multiple geometric dimensions. In particular, we propose a new approach, named Temporal MultiPersistence (TMP), which produces multidimensional topological fingerprints of the data by using the existing single parameter topological summaries. The main idea behind TMP is to merge the two newest directions in topological representation learning, that is, multi-persistence which simultaneously describes data shape evolution along multiple key parameters, and zigzag persistence to enable us to extract the most salient data shape information over time. We derive theoretical guarantees of TMP vectorizations and show its utility, in application to forecasting on benchmark traffic flow, Ethereum blockchain, and electrocardiogram datasets, demonstrating the competitive performance, especially, in scenarios of limited data records. In addition, our TMP method improves the computational efficiency of the state-of-the-art multipersistence summaries up to 59.5 times",
    "checked": true,
    "id": "c19f127fc7ea5d8eadfa03a55f4e9ab4250444bd",
    "semantic_title": "time-aware knowledge representations of dynamic objects with multidimensional persistence",
    "citation_count": 0,
    "authors": [
      "Baris Coskunuzer",
      "Ignacio Segovia-Dominguez",
      "Yuzhou Chen",
      "Yulia R. Gel"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29052": {
    "title": "BadRL: Sparse Targeted Backdoor Attack against Reinforcement Learning",
    "volume": "main",
    "abstract": "Backdoor attacks in reinforcement learning (RL) have previously employed intense attack strategies to ensure attack success. However, these methods suffer from high attack costs and increased detectability. In this work, we propose a novel approach, BadRL, which focuses on conducting highly sparse backdoor poisoning efforts during training and testing while maintaining successful attacks. Our algorithm, BadRL, strategically chooses state observations with high attack values to inject triggers during training and testing, thereby reducing the chances of detection. In contrast to the previous methods that utilize sample-agnostic trigger patterns, BadRL dynamically generates distinct trigger patterns based on targeted state observations, thereby enhancing its effectiveness. Theoretical analysis shows that the targeted backdoor attack is always viable and remains stealthy under specific assumptions. Empirical results on various classic RL tasks illustrate that BadRL can substantially degrade the performance of a victim agent with minimal poisoning efforts (0.003% of total training steps) during training and infrequent attacks during testing. Code is available at: https://github.com/7777777cc/code",
    "checked": true,
    "id": "7ade56c3e50df6afe1a2d3c43150612b0749e3c6",
    "semantic_title": "badrl: sparse targeted backdoor attack against reinforcement learning",
    "citation_count": 2,
    "authors": [
      "Jing Cui",
      "Yufei Han",
      "Yuzhe Ma",
      "Jianbin Jiao",
      "Junge Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29053": {
    "title": "Deletion-Robust Submodular Maximization with Knapsack Constraints",
    "volume": "main",
    "abstract": "Submodular maximization algorithms have found wide applications in various fields such as data summarization, recommendation systems, and active learning. In recent years, deletion-robust submodular maximization algorithms have garnered attention due to their significant implications in scenarios where some data points may be removed due to user preferences or privacy concerns, such as in recommendation systems and influence maximization. In this paper, we study the fundamental problem of submodular maximization with knapsack constraints and propose a robust streaming algorithm for it. To the best of our knowledge, our algorithm is the first to solve this problem for non-monotone submodular functions and can achieve an approximation ratio of 1/(6.82+2.63d)-ϵ under a near-optimal summary size of O(k+r), where k denotes the maximum cardinality of any feasible solution, d denotes the number of the knapsack constraints and r is the robustness parameter. For monotone submodular functions, our algorithm can achieve an approximation ratio of 1/(2+2d)-ϵ under a near-optimal summary size of O(k+r), significantly improving upon the best-known ratio of Ω((1/d-ϵ)^2). The empirical performance of our algorithm is extensively evaluated in several applications including influence maximization and recommendation systems, and the experimental results demonstrate the effectiveness of our algorithm",
    "checked": true,
    "id": "0f01c806fb96a9ba31d14acdc45678a542330f7a",
    "semantic_title": "deletion-robust submodular maximization with knapsack constraints",
    "citation_count": 0,
    "authors": [
      "Shuang Cui",
      "Kai Han",
      "He Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29054": {
    "title": "Continual Vision-Language Retrieval via Dynamic Knowledge Rectification",
    "volume": "main",
    "abstract": "The recent large-scale pre-trained models like CLIP have aroused great concern in vision-language tasks. However, when required to match image-text data collected in a streaming manner, namely Continual Vision-Language Retrieval (CVRL), their performances are still limited due to the catastrophic forgetting of the learned old knowledge. To handle this issue, advanced methods are proposed to distill the affinity knowledge between images and texts from the old model to the new one for anti-forgetting. Unfortunately, existing approaches neglect the impact of incorrect affinity, which prevents the balance between the anti-forgetting of old knowledge and the acquisition of new knowledge. Therefore, we propose a novel framework called Dynamic Knowledge Rectification (DKR) that simultaneously achieves incorrect knowledge filtering and rectification. Specifically, we first filter the incorrect affinity knowledge calculated by the old model on the new data. Then, a knowledge rectification method is designed to rectify the incorrect affinities while preserving the correct ones. In particular, for the new data that can only be correctly retrieved by the new model, we rectify them with the corresponding new affinity to protect them from negative transfer. Additionally, for those that can not be retrieved by either the old or the new model, we introduce paired ground-truth labels to promote the acquisition of both old and new knowledge. Extensive experiments on several benchmark datasets demonstrate the effectiveness of our DKR and its superiority against state-of-the-art methods",
    "checked": true,
    "id": "39c5c55a4683d4d6771dac038f03fa0a8dd5836e",
    "semantic_title": "continual vision-language retrieval via dynamic knowledge rectification",
    "citation_count": 1,
    "authors": [
      "Zhenyu Cui",
      "Yuxin Peng",
      "Xun Wang",
      "Manyu Zhu",
      "Jiahuan Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29055": {
    "title": "Inverse Weight-Balancing for Deep Long-Tailed Learning",
    "volume": "main",
    "abstract": "The performance of deep learning models often degrades rapidly when faced with imbalanced data characterized by a long-tailed distribution. Researchers have found that the fully connected layer trained by cross-entropy loss has large weight-norms for classes with many samples, but not for classes with few samples. How to address the data imbalance problem with both the encoder and the classifier seems an under-researched problem. In this paper, we propose an inverse weight-balancing (IWB) approach to guide model training and alleviate the data imbalance problem in two stages. In the first stage, an encoder and classifier (the fully connected layer) are trained using conventional cross-entropy loss. In the second stage, with a fixed encoder, the classifier is finetuned through an adaptive distribution for IWB in the decision space. Unlike existing inverse image frequency that implements a multiplicative margin adjustment transformation in the classification layer, our approach can be interpreted as an adaptive distribution alignment strategy using not only the class-wise number distribution but also the sample-wise difficulty distribution in both encoder and classifier. Experiments show that our method can greatly improve performance on imbalanced datasets such as CIFAR100-LT with different imbalance factors, ImageNet-LT, and iNaturelists2018",
    "checked": true,
    "id": "0d1bdd73b5e5453b5273a910042c79e934c24de7",
    "semantic_title": "inverse weight-balancing for deep long-tailed learning",
    "citation_count": 0,
    "authors": [
      "Wenqi Dang",
      "Zhou Yang",
      "Weisheng Dong",
      "Xin Li",
      "Guangming Shi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29056": {
    "title": "Learn the Force We Can: Enabling Sparse Motion Control in Multi-Object Video Generation",
    "volume": "main",
    "abstract": "We propose a novel unsupervised method to autoregressively generate videos from a single frame and a sparse motion input. Our trained model can generate unseen realistic object-to-object interactions. Although our model has never been given the explicit segmentation and motion of each object in the scene during training, it is able to implicitly separate their dynamics and extents. Key components in our method are the randomized conditioning scheme, the encoding of the input motion control, and the randomized and sparse sampling to enable generalization to out of distribution but realistic correlations. Our model, which we call YODA, has therefore the ability to move objects without physically touching them. Through extensive qualitative and quantitative evaluations on several datasets, we show that YODA is on par with or better than state of the art video generation prior work in terms of both controllability and video quality",
    "checked": true,
    "id": "b1965cd09d1f72c0d79ba07ea8e5ecd9d0f53bf1",
    "semantic_title": "learn the force we can: enabling sparse motion control in multi-object video generation",
    "citation_count": 2,
    "authors": [
      "Aram Davtyan",
      "Paolo Favaro"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29057": {
    "title": "Iterative Regularization with k-support Norm: An Important Complement to Sparse Recovery",
    "volume": "main",
    "abstract": "Sparse recovery is ubiquitous in machine learning and signal processing. Due to the NP-hard nature of sparse recovery, existing methods are known to suffer either from restrictive (or even unknown) applicability conditions, or high computational cost. Recently, iterative regularization methods have emerged as a promising fast approach because they can achieve sparse recovery in one pass through early stopping, rather than the tedious grid-search used in the traditional methods. However, most of those iterative methods are based on the l1 norm which requires restrictive applicability conditions and could fail in many cases. Therefore, achieving sparse recovery with iterative regularization methods under a wider range of conditions has yet to be further explored. To address this issue, we propose a novel iterative regularization algorithm, IRKSN, based on the k-support norm regularizer rather than the l1 norm. We provide conditions for sparse recovery with IRKSN, and compare them with traditional conditions for recovery with l1 norm regularizers. Additionally, we give an early stopping bound on the model error of IRKSN with explicit constants, achieving the standard linear rate for sparse recovery. Finally, we illustrate the applicability of our algorithm on several experiments, including a support recovery experiment with a correlated design matrix",
    "checked": true,
    "id": "8004580d2d47024e315ee1420f44287468b8b4ff",
    "semantic_title": "iterative regularization with k-support norm: an important complement to sparse recovery",
    "citation_count": 0,
    "authors": [
      "William de Vazelhes",
      "Bhaskar Mukhoty",
      "Xiao-Tong Yuan",
      "Bin Gu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29058": {
    "title": "SEA-GWNN: Simple and Effective Adaptive Graph Wavelet Neural Network",
    "volume": "main",
    "abstract": "The utilization of wavelet-based techniques in graph neural networks (GNNs) has gained considerable attention, particularly in the context of node classification. Although existing wavelet-based approaches have shown promise, they are constrained by their reliance on pre-defined wavelet filters, rendering them incapable of effectively adapting to signals that reside on graphs based on tasks at hand. Recent research endeavors address this issue through the introduction of a wavelet lifting transform. However, this technique necessitates the use of bipartite graphs, causing a transformation of the original graph structure into a bipartite configuration. This alteration of graph topology results in the generation of undesirable wavelet filters, thereby undermining the effectiveness of the method. In response to these challenges, we propose a novel simple and effective adaptive graph wavelet neural network (SEA-GWNN) class that employs the lifting scheme on arbitrary graph structures while upholding the original graph topology by leveraging multi-hop computation trees. A noteworthy aspect of the approach is the focus on local substructures represented as acyclic trees, wherein the lifting strategy is applied in a localized manner. This locally defined lifting scheme effectively combines high-pass and low-pass frequency information to enhance node representations. Furthermore, to reduce computing costs, we propose to decouple the higher- order lifting operators and induce them from the lower-order structures. Finally, we benchmark our model on several real- world datasets spanning four distinct categories, including citation networks, webpages, the film industry, and large-scale graphs and the experimental results showcase the efficacy of the proposed SEA-GWNN",
    "checked": true,
    "id": "69171fd2fdc94b2b8c7b49d0a36a11edb62afa56",
    "semantic_title": "sea-gwnn: simple and effective adaptive graph wavelet neural network",
    "citation_count": 0,
    "authors": [
      "Swakshar Deb",
      "Sejuti Rahman",
      "Shafin Rahman"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29059": {
    "title": "Self-Interpretable Graph Learning with Sufficient and Necessary Explanations",
    "volume": "main",
    "abstract": "Self-interpretable graph learning methods provide insights to unveil the black-box nature of GNNs by providing predictions with built-in explanations. However, current works suffer from performance degradation compared to GNNs trained without built-in explanations. We argue the main reason is that they fail to generate explanations satisfying both sufficiency and necessity, and the biased explanations further hurt GNNs' performance. In this work, we propose a novel framework for generating SUfficient aNd NecessarY explanations (SUNNY-GNN for short) that benefit GNNs' predictions. The key idea is to conduct augmentations by structurally perturbing given explanations and employ a contrastive loss to guide the learning of explanations toward sufficiency and necessity directions. SUNNY-GNN introduces two coefficients to generate hard and reliable contrastive samples. We further extend SUNNY-GNN to heterogeneous graphs. Empirical results on various GNNs and real-world graphs show that SUNNY-GNN yields accurate predictions and faithful explanations, outperforming the state-of-the-art methods by improving 3.5% prediction accuracy and 13.1% explainability fidelity on average. Our code and data are available at https://github.com/SJTU-Quant/SUNNY-GNN",
    "checked": true,
    "id": "3056fb32a8c65ce905e79035d6293d332d25e1d4",
    "semantic_title": "self-interpretable graph learning with sufficient and necessary explanations",
    "citation_count": 2,
    "authors": [
      "Jiale Deng",
      "Yanyan Shen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29060": {
    "title": "Semi-supervised TEE Segmentation via Interacting with SAM Equipped with Noise-Resilient Prompting",
    "volume": "main",
    "abstract": "Semi-supervised learning (SSL) is a powerful tool to address the challenge of insufficient annotated data in medical segmentation problems. However, existing semi-supervised methods mainly rely on internal knowledge for pseudo labeling, which is biased due to the distribution mismatch between the highly imbalanced labeled and unlabeled data. Segmenting left atrial appendage (LAA) from transesophageal echocardiogram (TEE) images is a typical medical image segmentation task featured by scarcity of professional annotations and diverse data distributions, for which existing SSL models cannot achieve satisfactory performance. In this paper, we propose a novel strategy to mitigate the inherent challenge of distribution mismatch in SSL by, for the first time, incorporating a large foundation model (i.e. SAM in our implementation) into an SSL model to improve the quality of pseudo labels. We further propose a new self-reconstruction mechanism to generate both noise-resilient prompts to demonically improve SAM's generalization capability over TEE images and self-perturbations to stabilize the training process and reduce the impact of noisy labels. We conduct extensive experiments on an in-house TEE dataset; experimental results demonstrate that our method achieves better performance than state-of-the-art SSL models",
    "checked": true,
    "id": "390ee8c941d061e7e3c787d69f0811147a39a9ff",
    "semantic_title": "semi-supervised tee segmentation via interacting with sam equipped with noise-resilient prompting",
    "citation_count": 0,
    "authors": [
      "Sen Deng",
      "Yidan Feng",
      "Haoneng Lin",
      "Yiting Fan",
      "Alex Pui-Wai  Lee",
      "Xiaowei Hu",
      "Jing Qin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29061": {
    "title": "Peer Learning: Learning Complex Policies in Groups from Scratch via Action Recommendations",
    "volume": "main",
    "abstract": "Peer learning is a novel high-level reinforcement learning framework for agents learning in groups. While standard reinforcement learning trains an individual agent in trial-and-error fashion, all on its own, peer learning addresses a related setting in which a group of agents, i.e., peers, learns to master a task simultaneously together from scratch. Peers are allowed to communicate only about their own states and actions recommended by others: \"What would you do in my situation?\". Our motivation is to study the learning behavior of these agents. We formalize the teacher selection process in the action advice setting as a multi-armed bandit problem and therefore highlight the need for exploration. Eventually, we analyze the learning behavior of the peers and observe their ability to rank the agents' performance within the study group and understand which agents give reliable advice. Further, we compare peer learning with single agent learning and a state-of-the-art action advice baseline. We show that peer learning is able to outperform single-agent learning and the baseline in several challenging discrete and continuous OpenAI Gym domains. Doing so, we also show that within such a framework complex policies from action recommendations beyond discrete action spaces can evolve",
    "checked": true,
    "id": "7b294832566a2692605cdad9dfe2984a5709661d",
    "semantic_title": "peer learning: learning complex policies in groups from scratch via action recommendations",
    "citation_count": 0,
    "authors": [
      "Cedric Derstroff",
      "Mattia Cerrato",
      "Jannis Brugger",
      "Jan Peters",
      "Stefan Kramer"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29062": {
    "title": "Conformal Autoregressive Generation: Beam Search with Coverage Guarantees",
    "volume": "main",
    "abstract": "We introduce two new extensions to the beam search algorithm based on conformal predictions (CP) to produce sets of sequences with theoretical coverage guarantees. The first method is very simple and proposes dynamically-sized subsets of beam search results but, unlike typical CP proceedures, has an upper bound on the achievable guarantee depending on a post-hoc calibration measure. Our second algorithm introduces the conformal set prediction procedure as part of the decoding process, producing a variable beam width which adapts to the current uncertainty. While more complex, this procedure can achieve coverage guarantees selected a priori. We provide marginal coverage bounds as well as calibration-conditional guarantees for each method, and evaluate them empirically on a selection of tasks drawing from natural language processing and chemistry",
    "checked": true,
    "id": "9582717ca2c7cf7f6d39fff408013d636accf4e6",
    "semantic_title": "conformal autoregressive generation: beam search with coverage guarantees",
    "citation_count": 8,
    "authors": [
      "Nicolas Deutschmann",
      "Marvin Alberts",
      "María Rodríguez Martínez"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29063": {
    "title": "Exploiting Label Skews in Federated Learning with Model Concatenation",
    "volume": "main",
    "abstract": "Federated Learning (FL) has emerged as a promising solution to perform deep learning on different data owners without exchanging raw data. However, non-IID data has been a key challenge in FL, which could significantly degrade the accuracy of the final model. Among different non-IID types, label skews have been challenging and common in image classification and other tasks. Instead of averaging the local models in most previous studies, we propose FedConcat, a simple and effective approach that concatenates these local models as the base of the global model to effectively aggregate the local knowledge. To reduce the size of the global model, we adopt the clustering technique to group the clients by their label distributions and collaboratively train a model inside each cluster. We theoretically analyze the advantage of concatenation over averaging by analyzing the information bottleneck of deep neural networks. Experimental results demonstrate that FedConcat achieves significantly higher accuracy than previous state-of-the-art FL methods in various heterogeneous label skew distribution settings and meanwhile has lower communication costs. Our code is publicly available at https://github.com/sjtudyq/FedConcat",
    "checked": true,
    "id": "bac764a24567b49ea12c1342061cdf4afe41ff1f",
    "semantic_title": "exploiting label skews in federated learning with model concatenation",
    "citation_count": 4,
    "authors": [
      "Yiqun Diao",
      "Qinbin Li",
      "Bingsheng He"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29064": {
    "title": "Multi-View Randomized Kernel Classification via Nonconvex Optimization",
    "volume": "main",
    "abstract": "Multi kernel learning (MKL) is a representative supervised multi-view learning method widely applied in multi-modal and multi-view applications. MKL aims to classify data by integrating complementary information from predefined kernels. Although existing MKL methods achieve promising performance, they fail to consider the tradeoff between diversity and classification accuracy of kernels, preventing further improvement of classification performance. In this paper, we tackle this problem by generating a number of high-quality base learning kernels and selecting a kernel subset with maximum pairwise diversity and minimum generalization errors. We first formulate this idea as a nonconvex quadratic integer programming problem. Then we transform this nonconvex problem into a convex optimization problem and prove it is equivalent to a semidefinite relaxation problem, which a semidefinite-based branch-and-bound algorithm can quickly solve. Experimental results on the real-world datasets demonstrate the superiority of the proposed method. The results also show that our method works for the support vector machine (SVM) classifier and other state-of-the-art kernel classifiers",
    "checked": true,
    "id": "00f43cc883c2762b65bd47a39a1161eb3d428e4a",
    "semantic_title": "multi-view randomized kernel classification via nonconvex optimization",
    "citation_count": 0,
    "authors": [
      "Xiaojian Ding",
      "Fan Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29065": {
    "title": "Turning Waste into Wealth: Leveraging Low-Quality Samples for Enhancing Continuous Conditional Generative Adversarial Networks",
    "volume": "main",
    "abstract": "Continuous Conditional Generative Adversarial Networks (CcGANs) enable generative modeling conditional on continuous scalar variables (termed regression labels). However, they can produce subpar fake images due to limited training data. Although Negative Data Augmentation (NDA) effectively enhances unconditional and class-conditional GANs by introducing anomalies into real training images, guiding the GANs away from low-quality outputs, its impact on CcGANs is limited, as it fails to replicate negative samples that may occur during the CcGAN sampling. We present a novel NDA approach called Dual-NDA specifically tailored for CcGANs to address this problem. Dual-NDA employs two types of negative samples: visually unrealistic images generated from a pre-trained CcGAN and label-inconsistent images created by manipulating real images' labels. Leveraging these negative samples, we introduce a novel discriminator objective alongside a modified CcGAN training algorithm. Empirical analysis on UTKFace and Steering Angle reveals that Dual-NDA consistently enhances the visual fidelity and label consistency of fake images generated by CcGANs, exhibiting a substantial performance gain over the vanilla NDA. Moreover, by applying Dual-NDA, CcGANs demonstrate a remarkable advancement beyond the capabilities of state-of-the-art conditional GANs and diffusion models, establishing a new pinnacle of performance. Our codes can be found at https://github.com/UBCDingXin/Dual-NDA",
    "checked": true,
    "id": "2135928fbe0fcea242f1c4e2500b3a0220c8a653",
    "semantic_title": "turning waste into wealth: leveraging low-quality samples for enhancing continuous conditional generative adversarial networks",
    "citation_count": 1,
    "authors": [
      "Xin Ding",
      "Yongwei Wang",
      "Zuheng Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29066": {
    "title": "Shrinking Your TimeStep: Towards Low-Latency Neuromorphic Object Recognition with Spiking Neural Networks",
    "volume": "main",
    "abstract": "Neuromorphic object recognition with spiking neural networks (SNNs) is the cornerstone of low-power neuromorphic computing. However, existing SNNs suffer from significant latency, utilizing 10 to 40 timesteps or more, to recognize neuromorphic objects. At low latencies, the performance of existing SNNs is drastically degraded. In this work, we propose the Shrinking SNN (SSNN) to achieve low-latency neuromorphic object recognition without reducing performance. Concretely, we alleviate the temporal redundancy in SNNs by dividing SNNs into multiple stages with progressively shrinking timesteps, which significantly reduces the inference latency. During timestep shrinkage, the temporal transformer smoothly transforms the temporal scale and preserves the information maximally. Moreover, we add multiple early classifiers to the SNN during training to mitigate the mismatch between the surrogate gradient and the true gradient, as well as the gradient vanishing/exploding, thus eliminating the performance degradation at low latency. Extensive experiments on neuromorphic datasets, CIFAR10-DVS, N-Caltech101, and DVS-Gesture have revealed that SSNN is able to improve the baseline accuracy by 6.55% ~ 21.41%. With only 5 average timesteps and without any data augmentation, SSNN is able to achieve an accuracy of 73.63% on CIFAR10-DVS. This work presents a heterogeneous temporal scale SNN and provides valuable insights into the development of high-performance, low-latency SNNs",
    "checked": false,
    "id": "cff2315c50d2be3f803af2c97de43ad637aee5c9",
    "semantic_title": "shrinking your timestep: towards low-latency neuromorphic object recognition with spiking neural network",
    "citation_count": 3,
    "authors": [
      "Yongqi Ding",
      "Lin Zuo",
      "Mengmeng Jing",
      "Pei He",
      "Yongjun Xiao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29067": {
    "title": "DGA-GNN: Dynamic Grouping Aggregation GNN for Fraud Detection",
    "volume": "main",
    "abstract": "Fraud detection has increasingly become a prominent research field due to the dramatically increased incidents of fraud. The complex connections involving thousands, or even millions of nodes, present challenges for fraud detection tasks. Many researchers have developed various graph-based methods to detect fraud from these intricate graphs. However, those methods neglect two distinct characteristics of the fraud graph: the non-additivity of certain attributes and the distinguishability of grouped messages from neighbor nodes. This paper introduces the Dynamic Grouping Aggregation Graph Neural Network (DGA-GNN) for fraud detection, which addresses these two characteristics by dynamically grouping attribute value ranges and neighbor nodes. In DGA-GNN, we initially propose the decision tree binning encoding to transform non-additive node attributes into bin vectors. This approach aligns well with the GNN's aggregation operation and avoids nonsensical feature generation. Furthermore, we devise a feedback dynamic grouping strategy to classify graph nodes into two distinct groups and then employ a hierarchical aggregation. This method extracts more discriminative features for fraud detection tasks. Extensive experiments on five datasets suggest that our proposed method achieves a 3% ~ 16% improvement over existing SOTA methods. Code is available at https://github.com/AtwoodDuan/DGA-GNN",
    "checked": true,
    "id": "4adb91d19b755be137fba14ffe92635bb2c22041",
    "semantic_title": "dga-gnn: dynamic grouping aggregation gnn for fraud detection",
    "citation_count": 0,
    "authors": [
      "Mingjiang Duan",
      "Tongya Zheng",
      "Yang Gao",
      "Gang Wang",
      "Zunlei Feng",
      "Xinyu Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29068": {
    "title": "Roll with the Punches: Expansion and Shrinkage of Soft Label Selection for Semi-supervised Fine-Grained Learning",
    "volume": "main",
    "abstract": "While semi-supervised learning (SSL) has yielded promising results, the more realistic SSL scenario remains to be explored, in which the unlabeled data exhibits extremely high recognition difficulty, e.g., fine-grained visual classification in the context of SSL (SS-FGVC). The increased recognition difficulty on fine-grained unlabeled data spells disaster for pseudo-labeling accuracy, resulting in poor performance of the SSL model. To tackle this challenge, we propose Soft Label Selection with Confidence-Aware Clustering based on Class Transition Tracking (SoC) by reconstructing the pseudo-label selection process by jointly optimizing Expansion Objective and Shrinkage Objective, which is based on a soft label manner. Respectively, the former objective encourages soft labels to absorb more candidate classes to ensure the attendance of ground-truth class, while the latter encourages soft labels to reject more noisy classes, which is theoretically proved to be equivalent to entropy minimization. In comparisons with various state-of-the-art methods, our approach demonstrates its superior performance in SS-FGVC. Checkpoints and source code are available at https://github.com/NJUyued/SoC4SS-FGVC",
    "checked": true,
    "id": "774584d59247153e63b1d03aefa68ed30a403c4a",
    "semantic_title": "roll with the punches: expansion and shrinkage of soft label selection for semi-supervised fine-grained learning",
    "citation_count": 0,
    "authors": [
      "Yue Duan",
      "Zhen Zhao",
      "Lei Qi",
      "Luping Zhou",
      "Lei Wang",
      "Yinghuan Shi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29069": {
    "title": "Provably Powerful Graph Neural Networks for Directed Multigraphs",
    "volume": "main",
    "abstract": "This paper analyses a set of simple adaptations that transform standard message-passing Graph Neural Networks (GNN) into provably powerful directed multigraph neural networks. The adaptations include multigraph port numbering, ego IDs, and reverse message passing. We prove that the combination of these theoretically enables the detection of any directed subgraph pattern. To validate the effectiveness of our proposed adaptations in practice, we conduct experiments on synthetic subgraph detection tasks, which demonstrate outstanding performance with almost perfect results. Moreover, we apply our proposed adaptations to two financial crime analysis tasks. We observe dramatic improvements in detecting money laundering transactions, improving the minority-class F1 score of a standard message-passing GNN by up to 30%, and closely matching or outperforming tree-based and GNN baselines. Similarly impressive results are observed on a real-world phishing detection dataset, boosting three standard GNNs' F1 scores by around 15% and outperforming all baselines. An extended version with appendices can be found on arXiv: https://arxiv.org/abs/2306.11586",
    "checked": true,
    "id": "cd4e7ee67fb67ff75660c7a42d043adda2d8b9c4",
    "semantic_title": "provably powerful graph neural networks for directed multigraphs",
    "citation_count": 2,
    "authors": [
      "Béni Egressy",
      "Luc von Niederhäusern",
      "Jovan Blanuša",
      "Erik Altman",
      "Roger Wattenhofer",
      "Kubilay Atasu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29070": {
    "title": "Causal Adversarial Perturbations for Individual Fairness and Robustness in Heterogeneous Data Spaces",
    "volume": "main",
    "abstract": "As responsible AI gains importance in machine learning algorithms, properties like fairness, adversarial robustness, and causality have received considerable attention in recent years. However, despite their individual significance, there remains a critical gap in simultaneously exploring and integrating these properties. In this paper, we propose a novel approach that examines the relationship between individual fairness, adversarial robustness, and structural causal models (SCMs) in heterogeneous data spaces, particularly when dealing with discrete sensitive attributes. We use SCMs and sensitive attributes to create a fair metric and apply it to measure semantic similarity among individuals. By introducing a novel causal adversarial perturbation (CAP) and applying adversarial training, we create a new regularizer that combines individual fairness, causality, and robustness in the classifier. Our method is evaluated on both real-world and synthetic datasets, demonstrating its effectiveness in achieving an accurate classifier that simultaneously exhibits fairness, adversarial robustness, and causal awareness",
    "checked": true,
    "id": "3b9041318a3ab7457480cd73421473b7704981ff",
    "semantic_title": "causal adversarial perturbations for individual fairness and robustness in heterogeneous data spaces",
    "citation_count": 1,
    "authors": [
      "Ahmad-Reza Ehyaei",
      "Kiarash Mohammadi",
      "Amir-Hossein Karimi",
      "Samira Samadi",
      "Golnoosh Farnadi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29071": {
    "title": "Double-Descent Curves in Neural Networks: A New Perspective Using Gaussian Processes",
    "volume": "main",
    "abstract": "Double-descent curves in neural networks describe the phenomenon that the generalisation error initially descends with increasing parameters, then grows after reaching an optimal number of parameters which is less than the number of data points, but then descends again in the overparameterized regime. In this paper, we use techniques from random matrix theory to characterize the spectral distribution of the empirical feature covariance matrix as a width-dependent perturbation of the spectrum of the neural network Gaussian process (NNGP) kernel, thus establishing a novel connection between the NNGP literature and the random matrix theory literature in the context of neural networks. Our analytical expressions allow us to explore the generalisation behavior of the corresponding kernel and GP regression. Furthermore, they offer a new interpretation of double-descent in terms of the discrepancy between the width-dependent empirical kernel and the width-independent NNGP kernel",
    "checked": false,
    "id": "4e30e5a27d1450131a903a56d7ba6ae62a777dd9",
    "semantic_title": "a continuous proof of the existence of the sle$_8$ curve",
    "citation_count": 2,
    "authors": [
      "Ouns El Harzli",
      "Bernardo Cuenca Grau",
      "Guillermo Valle-Pérez",
      "Ard A. Louis"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29072": {
    "title": "A Score-Based Deterministic Diffusion Algorithm with Smooth Scores for General Distributions",
    "volume": "main",
    "abstract": "Score matching based diffusion has shown to achieve the state of art results in generation modeling. In the original score matching based diffusion algorithm, the forward equation is a differential equation for which the probability density equation evolves according to a linear partial differential equation, the Fokker-Planck equation. A drawback of this approach is that one needs the data distribution to have a Lipschitz logarithmic gradient. This excludes a large class of data distributions that have a compact support. We present a deterministic diffusion process for which the vector fields are always Lipschitz and hence the score does not explode for probability measures with compact support. This deterministic diffusion process can be seen as a regularization of the porous media equation equation, which enables one to guarantee long term convergence of the forward process to the noise distribution. Though the porous media equation is itself not always guaranteed to have a Lipschitz vector field, it can be used to understand the closeness of the output of the algorithm to the data distribution as a function of the the time horizon and score matching error. This analysis enables us to show that the algorithm has better dependence on the score matching error than approaches based on stochastic diffusions. Using numerical experiments we verify our theoretical results on example one and two dimensional data distributions which are compactly supported. Additionally, we validate the approach on a modified MNIST data set for which the distribution is concentrated on a compact set. In each of the experiments, the approach using deterministic diffusion performs better that the diffusion algorithm with stochastic forward process, when considering the FID scores of the generated samples",
    "checked": true,
    "id": "c69848b2a071df03eba69be655635e75ed22e8cf",
    "semantic_title": "a score-based deterministic diffusion algorithm with smooth scores for general distributions",
    "citation_count": 0,
    "authors": [
      "Karthik Elamvazhuthi",
      "Xuechen Zhang",
      "Matthew Jacobs",
      "Samet Oymak",
      "Fabio Pasqualetti"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29073": {
    "title": "Feature Transportation Improves Graph Neural Networks",
    "volume": "main",
    "abstract": "Graph neural networks (GNNs) have shown remarkable success in learning representations for graph-structured data. However, GNNs still face challenges in modeling complex phenomena that involve feature transportation. In this paper, we propose a novel GNN architecture inspired by Advection-Diffusion-Reaction systems, called ADR-GNN. Advection models feature transportation, while diffusion captures the local smoothing of features, and reaction represents the non-linear transformation between feature channels. We provide an analysis of the qualitative behavior of ADR-GNN, that shows the benefit of combining advection, diffusion, and reaction. To demonstrate its efficacy, we evaluate ADR-GNN on real-world node classification and spatio-temporal datasets, and show that it improves or offers competitive performance compared to state-of-the-art networks",
    "checked": true,
    "id": "993885949975ad6b2dcf4f91840eb5c433d3d85f",
    "semantic_title": "feature transportation improves graph neural networks",
    "citation_count": 3,
    "authors": [
      "Moshe Eliasof",
      "Eldad Haber",
      "Eran Treister"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29074": {
    "title": "Exact Inference for Continuous-Time Gaussian Process Dynamics",
    "volume": "main",
    "abstract": "Many physical systems can be described as a continuous-time dynamical system. In practice, the true system is often unknown and has to be learned from measurement data. Since data is typically collected in discrete time, e.g. by sensors, most methods in Gaussian process (GP) dynamics model learning are trained on one-step ahead predictions. While this scheme is mathematically tempting, it can become problematic in several scenarios, e.g. if measurements are provided at irregularly-sampled time steps or physical system properties have to be conserved. Thus, we aim for a GP model of the true continuous-time dynamics. We tackle this task by leveraging higher-order numerical integrators. These integrators provide the necessary tools to discretize dynamical systems with arbitrary accuracy. However, most higher-order integrators require dynamics evaluations at intermediate time steps, making exact GP inference intractable. In previous work, this problem is often addressed by approximate inference techniques. However, exact GP inference is preferable in many scenarios, e.g. due to its mathematical guarantees. In order to enable direct inference, we propose to leverage multistep and Taylor integrators. We demonstrate how exact inference schemes can be derived for these types of integrators. Further, we derive tailored sampling schemes that allow one to draw consistent dynamics functions from the posterior. The learned model can thus be integrated with arbitrary integrators, just like a standard dynamical system. We show empirically and theoretically that our approach yields an accurate representation of the continuous-time system",
    "checked": true,
    "id": "57f381fadb89ebac6c1704be7cd59b82298860ce",
    "semantic_title": "exact inference for continuous-time gaussian process dynamics",
    "citation_count": 1,
    "authors": [
      "Katharina Ensinger",
      "Nicholas  Tagliapietra ",
      "Sebastian Ziesche",
      "Sebastian Trimpe"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29075": {
    "title": "Learning Hybrid Dynamics Models with Simulator-Informed Latent States",
    "volume": "main",
    "abstract": "Dynamics model learning deals with the task of inferring unknown dynamics from measurement data and predicting the future behavior of the system. A typical approach to address this problem is to train recurrent models. However, predictions with these models are often not physically meaningful. Further, they suffer from deteriorated behavior over time due to accumulating errors. Often, simulators building on first principles are available being physically meaningful by design. However, modeling simplifications typically cause inaccuracies in these models. Consequently, hybrid modeling is an emerging trend that aims to combine the best of both worlds. In this paper, we propose a new approach to hybrid modeling, where we inform the latent states of a learned model via a black-box simulator. This allows to control the predictions via the simulator preventing them from accumulating errors. This is especially challenging since, in contrast to previous approaches, access to the simulator's latent states is not available. We tackle the task by leveraging observers, a well-known concept from control theory, inferring unknown latent states from observations and dynamics over time. In our learning-based setting, we jointly learn the dynamics and an observer that infers the latent states via the simulator. Thus, the simulator constantly corrects the latent states, compensating for modeling mismatch caused by learning. To maintain flexibility, we train an RNN-based residuum for the latent states that cannot be informed by the simulator",
    "checked": true,
    "id": "27776d49cdb637a0fc3af3ea9bc8d2c22cfdd968",
    "semantic_title": "learning hybrid dynamics models with simulator-informed latent states",
    "citation_count": 1,
    "authors": [
      "Katharina Ensinger",
      "Sebastian Ziesche",
      "Sebastian Trimpe"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29076": {
    "title": "PAC-Bayes Generalisation Bounds for Dynamical Systems including Stable RNNs",
    "volume": "main",
    "abstract": "In this paper, we derive a PAC-Bayes bound on the generalisation gap, in a supervised time-series setting for a special class of discrete-time non-linear dynamical systems. This class includes stable recurrent neural networks (RNN), and the motivation for this work was its application to RNNs. In order to achieve the results, we impose some stability constraints, on the allowed models. Here, stability is understood in the sense of dynamical systems. For RNNs, these stability conditions can be expressed in terms of conditions on the weights. We assume the processes involved are essentially bounded and the loss functions are Lipschitz. The proposed bound on the generalisation gap depends on the mixing coefficient of the data distribution, and the essential supremum of the data. Furthermore, the bound converges to zero as the dataset size increases. In this paper, we 1) formalize the learning problem, 2) derive a PAC-Bayesian error bound for such systems, 3) discuss various consequences of this error bound, and 4) show an illustrative example, with discussions on computing the proposed bound. Unlike other available bounds the derived bound holds for non i.i.d. data (time-series) and it does not grow with the number of steps of the RNN",
    "checked": true,
    "id": "721d35df63e27dab30449b0fb00dcd7e7783a1f7",
    "semantic_title": "pac-bayes generalisation bounds for dynamical systems including stable rnns",
    "citation_count": 0,
    "authors": [
      "Deividas Eringis",
      "John Leth",
      "Zheng-Hua Tan",
      "Rafael Wisniewski",
      "Mihály Petreczky"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29077": {
    "title": "Non-parametric Representation Learning with Kernels",
    "volume": "main",
    "abstract": "Unsupervised and self-supervised representation learning has become popular in recent years for learning useful features from unlabelled data. Representation learning has been mostly developed in the neural network literature, and other models for representation learning are surprisingly unexplored. In this work, we introduce and analyze several kernel-based representation learning approaches: Firstly, we define two kernel Self-Supervised Learning (SSL) models using contrastive loss functions and secondly, a Kernel Autoencoder (AE) model based on the idea of embedding and reconstructing data. We argue that the classical representer theorems for supervised kernel machines are not always applicable for (self-supervised) representation learning, and present new representer theorems, which show that the representations learned by our kernel models can be expressed in terms of kernel matrices. We further derive generalisation error bounds for representation learning with kernel SSL and AE, and empirically evaluate the performance of these methods in both small data regimes as well as in comparison with neural network based models",
    "checked": true,
    "id": "bd0a3960fa87345d365d147e30e0d82a0c8fe563",
    "semantic_title": "non-parametric representation learning with kernels",
    "citation_count": 1,
    "authors": [
      "Pascal Esser",
      "Maximilian Fleissner",
      "Debarghya Ghoshdastidar"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29078": {
    "title": "Neural Gaussian Similarity Modeling for Differential Graph Structure Learning",
    "volume": "main",
    "abstract": "Graph Structure Learning (GSL) has demonstrated considerable potential in the analysis of graph-unknown non-Euclidean data across a wide range of domains. However, constructing an end-to-end graph structure learning model poses a challenge due to the impediment of gradient flow caused by the nearest neighbor sampling strategy. In this paper, we construct a differential graph structure learning model by replacing the non-differentiable nearest neighbor sampling with a differentiable sampling using the reparameterization trick. Under this framework, we argue that the act of sampling nearest neighbors may not invariably be essential, particularly in instances where node features exhibit a significant degree of similarity. To alleviate this issue, the bell-shaped Gaussian Similarity (GauSim) modeling is proposed to sample non-nearest neighbors. To adaptively model the similarity, we further propose Neural Gaussian Similarity (NeuralGauSim) with learnable parameters featuring flexible sampling behaviors. In addition, we develop a scalable method by transferring the large-scale graph to the transition graph to significantly reduce the complexity. Experimental results demonstrate the effectiveness of the proposed methods",
    "checked": true,
    "id": "f2b9fad35bdb06efb3d9eaca2f7a8b325eb77a61",
    "semantic_title": "neural gaussian similarity modeling for differential graph structure learning",
    "citation_count": 0,
    "authors": [
      "Xiaolong Fan",
      "Maoguo Gong",
      "Yue Wu",
      "Zedong Tang",
      "Jieyi Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29079": {
    "title": "Dynamic Sub-graph Distillation for Robust Semi-supervised Continual Learning",
    "volume": "main",
    "abstract": "Continual learning (CL) has shown promising results and comparable performance to learning at once in a fully supervised manner. However, CL strategies typically require a large number of labeled samples, making their real-life deployment challenging. In this work, we focus on semi-supervised continual learning (SSCL), where the model progressively learns from partially labeled data with unknown categories. We provide a comprehensive analysis of SSCL and demonstrate that unreliable distributions of unlabeled data lead to unstable training and refinement of the progressing stages. This problem severely impacts the performance of SSCL. To address the limitations, we propose a novel approach called Dynamic Sub-Graph Distillation (DSGD) for semi-supervised continual learning, which leverages both semantic and structural information to achieve more stable knowledge distillation on unlabeled data and exhibit robustness against distribution bias. Firstly, we formalize a general model of structural distillation and design a dynamic graph construction for the continual learning progress. Next, we define a structure distillation vector and design a dynamic sub-graph distillation algorithm, which enables end-to-end training and adaptability to scale up tasks. The entire proposed method is adaptable to various CL methods and supervision settings. Finally, experiments conducted on three datasets CIFAR10, CIFAR100, and ImageNet-100, with varying supervision ratios, demonstrate the effectiveness of our proposed approach in mitigating the catastrophic forgetting problem in semi-supervised continual learning scenarios. Our code is available: https://github.com/fanyan0411/DSGD",
    "checked": true,
    "id": "b9d5d00a91c5495e0421b7691428bf0f66c9fbf8",
    "semantic_title": "dynamic sub-graph distillation for robust semi-supervised continual learning",
    "citation_count": 1,
    "authors": [
      "Yan Fan",
      "Yu Wang",
      "Pengfei Zhu",
      "Qinghua Hu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29080": {
    "title": "Selective Focus: Investigating Semantics Sensitivity in Post-training Quantization for Lane Detection",
    "volume": "main",
    "abstract": "Lane detection (LD) plays a crucial role in enhancing the L2+ capabilities of autonomous driving, capturing widespread attention. The Post-Processing Quantization (PTQ) could facilitate the practical application of LD models, enabling fast speeds and limited memories without labeled data. However, prior PTQ methods do not consider the complex LD outputs that contain physical semantics, such as offsets, locations, etc., and thus cannot be directly applied to LD models. In this paper, we pioneeringly investigate semantic sensitivity to post-processing for lane detection with a novel Lane Distortion Score. Moreover, we identify two main factors impacting the LD performance after quantization, namely intra-head sensitivity and inter-head sensitivity, where a small quantization error in specific semantics can cause significant lane distortion. Thus, we propose a Selective Focus framework deployed with Semantic Guided Focus and Sensitivity Aware Selection modules, to incorporate post-processing information into PTQ reconstruction. Based on the observed intra-head sensitivity, Semantic Guided Focus is introduced to prioritize foreground-related semantics using a practical proxy. For inter-head sensitivity, we present Sensitivity Aware Selection, efficiently recognizing influential prediction heads and refining the optimization objectives at runtime. Extensive experiments have been done on a wide variety of models including keypoint-, anchor-, curve-, and segmentation-based ones. Our method produces quantized models in minutes on a single GPU and can achieve 6.4\\% F1 Score improvement on the CULane dataset. Code and supplementary statement can be found at https://github.com/PannenetsF/SelectiveFocus",
    "checked": true,
    "id": "a7ac174483f9fc886b87d65d458976d975773fae",
    "semantic_title": "selective focus: investigating semantics sensitivity in post-training quantization for lane detection",
    "citation_count": 0,
    "authors": [
      "Yunqian Fan",
      "Xiuying Wei",
      "Ruihao Gong",
      "Yuqing Ma",
      "Xiangguo Zhang",
      "Qi Zhang",
      "Xianglong Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29081": {
    "title": "Backdoor Adjustment via Group Adaptation for Debiased Coupon Recommendations",
    "volume": "main",
    "abstract": "Accurate prediction of coupon usage is crucial for promoting user consumption through targeted coupon recommendations. However, in real-world coupon recommendations, the coupon allocation process is not solely determined by the model trained with the history interaction data but is also interfered with by marketing tactics desired to fulfill specific commercial goals.This interference creates an imbalance in the interactions, which causes the data to deviate from the user's natural preferences. We refer to this deviation as the matching bias. Such biased interaction data affects the efficacy of the model, and thus it is necessary to employ debiasing techniques to prevent any negative impact. We investigate the mitigation of matching bias in coupon recommendations from a causal-effect perspective. By treating the attributes of users and coupons associated with marketing tactics as confounders, we find the confounders open the backdoor path between user-coupon matching and the conversion, which introduces spurious correlation. To remove the bad effect, we propose a novel training paradigm named Backdoor Adjustment via Group Adaptation (BAGA) for debiased coupon recommendations, which performs intervened training and inference, i.e., separately modeling each user-coupon group pair. However, modeling all possible group pairs greatly increases the computational complexity and cost. To address the efficiency challenge, we further present a simple but effective dual-tower multi-task framework and leverage the Customized Gate Control (CGC) model architecture, which separately models each user and coupon group with a separate expert module. We instantiate BAGA on five representative models: FM, DNN, NCF, MASKNET, and DEEPFM, and conduct comprehensive offline and online experiments to demonstrate the efficacy of our proposed paradigm",
    "checked": true,
    "id": "f2e29266fc37d9d22234362dad91f6320339ab1f",
    "semantic_title": "backdoor adjustment via group adaptation for debiased coupon recommendations",
    "citation_count": 1,
    "authors": [
      "Junpeng Fang",
      "Gongduo Zhang",
      "Qing Cui",
      "Caizhi Tang",
      "Lihong Gu",
      "Longfei Li",
      "Jinjie Gu",
      "Jun Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29082": {
    "title": "Improving GNN Calibration with Discriminative Ability: Insights and Strategies",
    "volume": "main",
    "abstract": "The widespread adoption of Graph Neural Networks (GNNs) has led to an increasing focus on their reliability. To address the issue of underconfidence in GNNs, various calibration methods have been developed to gain notable reductions in calibration error. However, we observe that existing approaches generally fail to enhance consistently, and in some cases even deteriorate, GNNs' ability to discriminate between correct and incorrect predictions. In this study, we advocate the significance of discriminative ability and the inclusion of relevant evaluation metrics. Our rationale is twofold: 1) Overlooking discriminative ability can inadvertently compromise the overall quality of the model; 2) Leveraging discriminative ability can significantly inform and improve calibration outcomes. Therefore, we thoroughly explore the reasons why existing calibration methods have ineffectiveness and even degradation regarding the discriminative ability of GNNs. Building upon these insights, we conduct GNN calibration experiments across multiple datasets using a straightforward example model, denoted as DC(GNN). Its excellent performance confirms the potential of integrating discriminative ability as a key consideration in the calibration of GNNs, thereby establishing a pathway toward more effective and reliable network calibration",
    "checked": true,
    "id": "ac2a39f3179d4b4607ba37d9517b6a52200b4d23",
    "semantic_title": "improving gnn calibration with discriminative ability: insights and strategies",
    "citation_count": 0,
    "authors": [
      "Yujie Fang",
      "Xin Li",
      "Qianyu Chen",
      "Mingzhong Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29083": {
    "title": "SUF: Stabilized Unconstrained Fine-Tuning for Offline-to-Online Reinforcement Learning",
    "volume": "main",
    "abstract": "Offline-to-online reinforcement learning (RL) provides a promising solution to improving suboptimal offline pre-trained policies through online fine-tuning. However, one efficient method, unconstrained fine-tuning, often suffers from severe policy collapse due to excessive distribution shift. To ensure stability, existing methods retain offline constraints and employ additional techniques during fine-tuning, which hurts efficiency. In this work, we introduce a novel perspective: eliminating the policy collapse without imposing constraints. We observe that such policy collapse arises from the mismatch between unconstrained fine-tuning and the conventional RL training framework. To this end, we propose Stabilized Unconstrained Fine-tuning (SUF), a streamlined framework that benefits from the efficiency of unconstrained fine-tuning while ensuring stability by modifying the Update-To-Data ratio. With just a few lines of code adjustments, SUF demonstrates remarkable adaptability to diverse backbones and superior performance over state-of-the-art baselines",
    "checked": true,
    "id": "5cb4faccf5ecd994f481fea3e2c58cb5303a0d77",
    "semantic_title": "suf: stabilized unconstrained fine-tuning for offline-to-online reinforcement learning",
    "citation_count": 0,
    "authors": [
      "Jiaheng Feng",
      "Mingxiao Feng",
      "Haolin Song",
      "Wengang Zhou",
      "Houqiang Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29084": {
    "title": "BaCon: Boosting Imbalanced Semi-supervised Learning via Balanced Feature-Level Contrastive Learning",
    "volume": "main",
    "abstract": "Semi-supervised Learning (SSL) reduces the need for extensive annotations in deep learning, but the more realistic challenge of imbalanced data distribution in SSL remains largely unexplored. In Class Imbalanced Semi-supervised Learning (CISSL), the bias introduced by unreliable pseudo-labels can be exacerbated by imbalanced data distributions. Most existing methods address this issue at instance-level through reweighting or resampling, but the performance is heavily limited by their reliance on biased backbone representation. Some other methods do perform feature-level adjustments like feature blending but might introduce unfavorable noise. In this paper, we discuss the bonus of a more balanced feature distribution for the CISSL problem, and further propose a Balanced Feature-Level Contrastive Learning method (BaCon). Our method directly regularizes the distribution of instances' representations in a well-designed contrastive manner. Specifically, class-wise feature centers are computed as the positive anchors, while negative anchors are selected by a straightforward yet effective mechanism. A distribution-related temperature adjustment is leveraged to control the class-wise contrastive degrees dynamically. Our method demonstrates its effectiveness through comprehensive experiments on the CIFAR10-LT, CIFAR100-LT, STL10-LT, and SVHN-LT datasets across various settings. For example, BaCon surpasses instance-level method FixMatch-based ABC on CIFAR10-LT with a 1.21% accuracy improvement, and outperforms state-of-the-art feature-level method CoSSL on CIFAR100-LT with a 0.63% accuracy improvement. When encountering more extreme imbalance degree, BaCon also shows better robustness than other methods",
    "checked": true,
    "id": "c30e338836ed74b5d196df4c2823aa803635c912",
    "semantic_title": "bacon: boosting imbalanced semi-supervised learning via balanced feature-level contrastive learning",
    "citation_count": 2,
    "authors": [
      "Qianhan Feng",
      "Lujing Xie",
      "Shijie Fang",
      "Tong Lin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29085": {
    "title": "Latent Diffusion Transformer for Probabilistic Time Series Forecasting",
    "volume": "main",
    "abstract": "The probability prediction of multivariate time series is a notoriously challenging but practical task. This research proposes to condense high-dimensional multivariate time series forecasting into a problem of latent space time series generation, to improve the expressiveness of each timestamp and make forecasting more manageable. To solve the problem that the existing work is hard to extend to high-dimensional multivariate time series, we present a latent multivariate time series diffusion framework called Latent Diffusion Transformer (LDT), which consists of a symmetric statistics-aware autoencoder and a diffusion-based conditional generator, to implement this idea. Through careful design, the time series autoencoder can compress multivariate timestamp patterns into a concise latent representation by considering dynamic statistics. Then, the diffusion-based conditional generator is able to efficiently generate realistic multivariate timestamp values on a continuous latent space under a novel self-conditioning guidance which is modeled in a non-autoregressive way. Extensive experiments demonstrate that our model achieves state-of-the-art performance on many popular high-dimensional multivariate time series datasets",
    "checked": true,
    "id": "900e91d93294a0d4f1213d57272e8801c3abdb06",
    "semantic_title": "latent diffusion transformer for probabilistic time series forecasting",
    "citation_count": 4,
    "authors": [
      "Shibo Feng",
      "Chunyan Miao",
      "Zhong Zhang",
      "Peilin Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29086": {
    "title": "Partial Multi-View Clustering via Self-Supervised Network",
    "volume": "main",
    "abstract": "Partial multi-view clustering is a challenging and practical research problem for data analysis in real-world applications, due to the potential data missing issue in different views. However, most existing methods have not fully explored the correlation information among various incomplete views. In addition, these existing clustering methods always ignore discovering discriminative features inside the data itself in this unsupervised task. To tackle these challenges, we propose Partial Multi-View Clustering via Self-Supervised \\textbf{N}etwork (PVC-SSN) in this paper. Specifically, we employ contrastive learning to obtain a more discriminative and consistent subspace representation, which is guided by a self-supervised module. Self-supervised learning can exploit effective cluster information through the data itself to guide the learning process of clustering tasks. Thus, it can pull together embedding features from the same cluster and push apart these from different clusters. Extensive experiments on several benchmark datasets show that the proposed PVC-SCN method outperforms several state-of-the-art clustering methods",
    "checked": true,
    "id": "cc4e5b6ccfd3d3595f632c987be55eba3a119afd",
    "semantic_title": "partial multi-view clustering via self-supervised network",
    "citation_count": 0,
    "authors": [
      "Wei Feng",
      "Guoshuai Sheng",
      "Qianqian Wang",
      "Quanxue Gao",
      "Zhiqiang Tao",
      "Bo Dong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29087": {
    "title": "Harnessing Manycore Processors with Distributed Memory for Accelerated Training of Sparse and Recurrent Models",
    "volume": "main",
    "abstract": "Current AI training infrastructure is dominated by single instruction multiple data (SIMD) and systolic array architectures, such as Graphics Processing Units (GPUs) and Tensor Processing Units (TPUs), that excel at accelerating parallel workloads and dense vector matrix multiplications. Potentially more efficient neural network models utilizing sparsity and recurrence cannot leverage the full power of SIMD processor and are thus at a severe disadvantage compared to today's prominent parallel architectures like Transformers and CNNs, thereby hindering the path towards more sustainable AI. To overcome this limitation, we explore sparse and recurrent model training on a massively parallel multiple instruction multiple data (MIMD) architecture with distributed local memory. We implement a training routine based on backpropagation though time (BPTT) for the brain-inspired class of Spiking Neural Networks (SNNs) that feature binary sparse activations. We observe a massive advantage in using sparse activation tensors with a MIMD processor, the Intelligence Processing Unit (IPU) compared to GPUs. On training workloads, our results demonstrate 5-10x throughput gains compared to A100 GPUs and up to 38x gains for higher levels of activation sparsity, without a significant slowdown in training convergence or reduction in final model performance. Furthermore, our results show highly promising trends for both single and multi IPU configurations as we scale up to larger model sizes. Our work paves the way towards more efficient, non-standard models via AI training hardware beyond GPUs, and competitive large scale SNN models",
    "checked": true,
    "id": "334d6b6925fadff2d6e82529686400656ebfdb24",
    "semantic_title": "harnessing manycore processors with distributed memory for accelerated training of sparse and recurrent models",
    "citation_count": 1,
    "authors": [
      "Jan Finkbeiner",
      "Thomas Gmeinder",
      "Mark Pupilli",
      "Alexander Titterton",
      "Emre Neftci"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29088": {
    "title": "Graph Learning in 4D: A Quaternion-Valued Laplacian to Enhance Spectral GCNs",
    "volume": "main",
    "abstract": "We introduce QuaterGCN, a spectral Graph Convolutional Network (GCN) with quaternion-valued weights at whose core lies the Quaternionic Laplacian, a quaternion-valued Laplacian matrix by whose proposal we generalize two widely-used Laplacian matrices: the classical Laplacian (defined for undirected graphs) and the complex-valued Sign-Magnetic Laplacian (proposed within the spectral GCN SigMaNet to handle digraphs with weights of arbitrary sign). In addition to its generality, QuaterGCN is the only Laplacian to completely preserve the (di)graph topology that we are aware of, as it can handle graphs and digraphs containing antiparallel pairs of edges (digons) of different weight without reducing them to a single (directed or undirected) edge as done by other Laplacians. Experimental results show the superior performance of QuaterGCN compared to other state-of-the-art GCNs, particularly in scenarios where the information the digons carry is crucial to successfully address the task at hand",
    "checked": true,
    "id": "fa7adde25e9534df32501e1458d7081c3f6a3a27",
    "semantic_title": "graph learning in 4d: a quaternion-valued laplacian to enhance spectral gcns",
    "citation_count": 0,
    "authors": [
      "Stefano Fiorini",
      "Stefano Coniglio",
      "Michele Ciavotta",
      "Enza Messina"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29089": {
    "title": "Learning Broadcast Protocols",
    "volume": "main",
    "abstract": "The problem of learning a computational model from examples has been receiving growing attention. For the particularly challenging problem of learning models of distributed systems, existing results are restricted to models with a fixed number of interacting processes. In this work we look for the first time (to the best of our knowledge) at the problem of learning a distributed system with an arbitrary number of processes, assuming only that there exists a cutoff, i.e., a number of processes that is sufficient to produce all observable behaviors. Specifically, we consider fine broadcast protocols, these are broadcast protocols (BPs) with a finite cutoff and no hidden states. We provide a learning algorithm that can infer a correct BP from a sample that is consistent with a fine BP, and a minimal equivalent BP if the sample is sufficiently complete. On the negative side we show that (a) characteristic sets of exponential size are unavoidable, (b) the consistency problem for fine BPs is NP hard, and (c) that fine BPs are not polynomially predictable",
    "checked": true,
    "id": "25ba92401f1809ea6f0145754c830ea139a7c2aa",
    "semantic_title": "learning broadcast protocols",
    "citation_count": 0,
    "authors": [
      "Dana Fisman",
      "Noa Izsak",
      "Swen Jacobs"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29090": {
    "title": "Beyond Expected Return: Accounting for Policy Reproducibility When Evaluating Reinforcement Learning Algorithms",
    "volume": "main",
    "abstract": "Many applications in Reinforcement Learning (RL) usually have noise or stochasticity present in the environment. Beyond their impact on learning, these uncertainties lead the exact same policy to perform differently, i.e. yield different return, from one roll-out to another. Common evaluation procedures in RL summarise the consequent return distributions using solely the expected return, which does not account for the spread of the distribution. Our work defines this spread as the policy reproducibility: the ability of a policy to obtain similar performance when rolled out many times, a crucial property in some real-world applications. We highlight that existing procedures that only use the expected return are limited on two fronts: first an infinite number of return distributions with a wide range of performance-reproducibility trade-offs can have the same expected return, limiting its effectiveness when used for comparing policies; second, the expected return metric does not leave any room for practitioners to choose the best trade-off value for considered applications. In this work, we address these limitations by recommending the use of Lower Confidence Bound, a metric taken from Bayesian optimisation that provides the user with a preference parameter to choose a desired performance-reproducibility trade-off. We also formalise and quantify policy reproducibility, and demonstrate the benefit of our metrics using extensive experiments of popular RL algorithms on common uncertain RL tasks",
    "checked": true,
    "id": "444dd65022d6503f61d96a1547ac0b8d78367351",
    "semantic_title": "beyond expected return: accounting for policy reproducibility when evaluating reinforcement learning algorithms",
    "citation_count": 0,
    "authors": [
      "Manon Flageat",
      "Bryan Lim",
      "Antoine Cully"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29091": {
    "title": "Symbolic Regression Enhanced Decision Trees for Classification Tasks",
    "volume": "main",
    "abstract": "We introduce a conceptually simple yet effective method to create small, compact decision trees - by using splits found via Symbolic Regression (SR). Traditional decision tree (DT) algorithms partition a dataset on axis-parallel splits. When the true boundaries are not along the feature axes, DT is likely to have a complicated structure and a dense decision boundary. In this paper, we introduce SR-Enhanced DT (SREDT) - a method which utilizes SR to increase the richness of the class of possible DT splits. We evaluate SREDT on both synthetic and real-world datasets. Despite its simplicity, our method produces surprisingly small trees that outperform both DT and oblique DT (ODT) on supervised classification tasks in terms of accuracy and F-score. We show empirically that SREDTs decrease inference time (compared to DT and ODT) and argue that they allow us to obtain more explainable descriptions of the decision process. SREDT also performs competitively against state-of-the-art tabular classification methods, including tree ensembles and deep models. Finally, we introduce a local search mechanism to improve SREDT and evaluate it on 56 PMLB datasets. This mechanism shows improved performance on 77.2% of the datasets, outperforming DT and ODT. In terms of F-Score, local SREDT outperforms DT and ODT in 82.5% and 73.7% of the datasets respectively and in terms of inference time, local SREDT requires 25.8% and 26.6% less inference time than DT and ODT respectively",
    "checked": true,
    "id": "4995983db49f098f8a9f3335fb524a5c4a4dd71c",
    "semantic_title": "symbolic regression enhanced decision trees for classification tasks",
    "citation_count": 1,
    "authors": [
      "Kei Sen Fong",
      "Mehul Motani"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29092": {
    "title": "Fast Machine Unlearning without Retraining through Selective Synaptic Dampening",
    "volume": "main",
    "abstract": "Machine unlearning, the ability for a machine learning model to forget, is becoming increasingly important to comply with data privacy regulations, as well as to remove harmful, manipulated, or outdated information. The key challenge lies in forgetting specific information while protecting model performance on the remaining data. While current state-of-the-art methods perform well, they typically require some level of retraining over the retained data, in order to protect or restore model performance. This adds computational overhead and mandates that the training data remain available and accessible, which may not be feasible. In contrast, other methods employ a retrain-free paradigm, however, these approaches are prohibitively computationally expensive and do not perform on par with their retrain-based counterparts. We present Selective Synaptic Dampening (SSD), a novel two-step, post hoc, retrain-free approach to machine unlearning which is fast, performant, and does not require long-term storage of the training data. First, SSD uses the Fisher information matrix of the training and forgetting data to select parameters that are disproportionately important to the forget set. Second, SSD induces forgetting by dampening these parameters proportional to their relative importance to the forget set with respect to the wider training data. We evaluate our method against several existing unlearning methods in a range of experiments using ResNet18 and Vision Transformer. Results show that the performance of SSD is competitive with retrain-based post hoc methods, demonstrating the viability of retrain-free post hoc unlearning approaches",
    "checked": true,
    "id": "2956e747798c825b110e793fc2521c29607b87e3",
    "semantic_title": "fast machine unlearning without retraining through selective synaptic dampening",
    "citation_count": 32,
    "authors": [
      "Jack Foster",
      "Stefan Schoepf",
      "Alexandra Brintrup"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29093": {
    "title": "Combinatorial Stochastic-Greedy Bandit",
    "volume": "main",
    "abstract": "We propose a novel combinatorial stochastic-greedy bandit (SGB) algorithm for combinatorial multi-armed bandit problems when no extra information other than the joint reward of the selected set of n arms at each time step t in [T] is observed. SGB adopts an optimized stochastic-explore-then-commit approach and is specifically designed for scenarios with a large set of base arms. Unlike existing methods that explore the entire set of unselected base arms during each selection step, our SGB algorithm samples only an optimized proportion of unselected arms and selects actions from this subset. We prove that our algorithm achieves a (1-1/e)-regret bound of O(n^(1/3) k^(2/3) T^(2/3) log(T)^(2/3)) for monotone stochastic submodular rewards, which outperforms the state-of-the-art in terms of the cardinality constraint k. Furthermore, we empirically evaluate the performance of our algorithm in the context of online constrained social influence maximization. Our results demonstrate that our proposed approach consistently outperforms the other algorithms, increasing the performance gap as k grows",
    "checked": true,
    "id": "3283738a53cbe4a726dd6c5ec78ce3da7e22f425",
    "semantic_title": "combinatorial stochastic-greedy bandit",
    "citation_count": 4,
    "authors": [
      "Fares Fourati",
      "Christopher John Quinn",
      "Mohamed-Slim Alouini",
      "Vaneet Aggarwal"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29094": {
    "title": "REGLO: Provable Neural Network Repair for Global Robustness Properties",
    "volume": "main",
    "abstract": "We present REGLO, a novel methodology for repairing pretrained neural networks to satisfy global robustness and individual fairness properties. A neural network is said to be globally robust with respect to a given input region if and only if all the input points in the region are locally robust. This notion of global robustness also captures the notion of individual fairness as a special case. We prove that any counterexample to a global robustness property must exhibit a corresponding large gradient. For ReLU networks, this result allows us to efficiently identify the linear regions that violate a given global robustness property. By formulating and solving a suitable robust convex optimization problem, REGLO then computes a minimal weight change that will provably repair these violating linear regions",
    "checked": true,
    "id": "585cc63cf645c1eaca2f6137414fa9929874de82",
    "semantic_title": "reglo: provable neural network repair for global robustness properties",
    "citation_count": 6,
    "authors": [
      "Feisi Fu",
      "Zhilu Wang",
      "Weichao Zhou",
      "Yixuan Wang",
      "Jiameng Fan",
      "Chao Huang",
      "Qi Zhu",
      "Xin Chen",
      "Wenchao Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29095": {
    "title": "QLABGrad: A Hyperparameter-Free and Convergence-Guaranteed Scheme for Deep Learning",
    "volume": "main",
    "abstract": "The learning rate is a critical hyperparameter for deep learning tasks since it determines the extent to which the model parameters are adjusted during the learning course. However, the choice of learning rates typically depends on empirical judgment, which may not result in satisfactory outcomes without intensive try-and-error experiments. In this study, we propose a novel learning rate adaptation scheme called QLABGrad. Without any user-specified hyperparameter, QLABGrad automatically determines the learning rate by optimizing the quadratic loss approximation-based (QLAB) function for a given gradient descent direction, where only one extra forward propagation is required. We theoretically prove the convergence of QLABGrad under the smooth Lipschitz condition on the loss function. Experiment results on multiple architectures, including MLP, CNN, and ResNet, on MNIST, CIFAR10, and ImageNet datasets, demonstrate that QLABGrad outperforms widely adopted schemes for deep learning",
    "checked": true,
    "id": "916483d77db7aee2e5f68f829eeeba2ad36383da",
    "semantic_title": "qlabgrad: a hyperparameter-free and convergence-guaranteed scheme for deep learning",
    "citation_count": 2,
    "authors": [
      "Minghan Fu",
      "Fang-Xiang Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29096": {
    "title": "DTL: Disentangled Transfer Learning for Visual Recognition",
    "volume": "main",
    "abstract": "When pre-trained models become rapidly larger, the cost of fine-tuning on downstream tasks steadily increases, too. To economically fine-tune these models, parameter-efficient transfer learning (PETL) is proposed, which only tunes a tiny subset of trainable parameters to efficiently learn quality representations. However, current PETL methods are facing the dilemma that during training the GPU memory footprint is not effectively reduced as trainable parameters. PETL will likely fail, too, if the full fine-tuning encounters the out-of-GPU-memory issue. This phenomenon happens because trainable parameters from these methods are generally entangled with the backbone, such that a lot of intermediate states have to be stored in GPU memory for gradient propagation. To alleviate this problem, we introduce Disentangled Transfer Learning (DTL), which disentangles the trainable parameters from the backbone using a lightweight Compact Side Network (CSN). By progressively extracting task-specific information with a few low-rank linear mappings and appropriately adding the information back to the backbone, CSN effectively realizes knowledge transfer in various downstream tasks. We conducted extensive experiments to validate the effectiveness of our method. The proposed method not only reduces a large amount of GPU memory usage and trainable parameters, but also outperforms existing PETL methods by a significant margin in accuracy, achieving new state-of-the-art on several standard benchmarks",
    "checked": true,
    "id": "d24b44f781815adf132766c0eff40702ea31e931",
    "semantic_title": "dtl: disentangled transfer learning for visual recognition",
    "citation_count": 5,
    "authors": [
      "Minghao Fu",
      "Ke Zhu",
      "Jianxin Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29097": {
    "title": "Noise-Aware Image Captioning with Progressively Exploring Mismatched Words",
    "volume": "main",
    "abstract": "Image captioning aims to automatically generate captions for images by learning a cross-modal generator from vision to language. The large amount of image-text pairs required for training is usually sourced from the internet due to the manual cost, which brings the noise with mismatched relevance that affects the learning process. Unlike traditional noisy label learning, the key challenge in processing noisy image-text pairs is to finely identify the mismatched words to make the most use of trustworthy information in the text, rather than coarsely weighing the entire examples. To tackle this challenge, we propose a Noise-aware Image Captioning method (NIC) to adaptively mitigate the erroneous guidance from noise by progressively exploring mismatched words. Specifically, NIC first identifies mismatched words by quantifying word-label reliability from two aspects: 1) inter-modal representativeness, which measures the significance of the current word by assessing cross-modal correlation via prediction certainty; 2) intra-modal informativeness, which amplifies the effect of current prediction by combining the quality of subsequent word generation. During optimization, NIC constructs the pseudo-word-labels considering the reliability of the origin word-labels and model convergence to periodically coordinate mismatched words. As a result, NIC can effectively exploit both clean and noisy image-text pairs to learn a more robust mapping function. Extensive experiments conducted on the MS-COCO and Conceptual Caption datasets validate the effectiveness of our method in various noisy scenarios",
    "checked": true,
    "id": "2d98517c2897311f9c1a56c63fbef077297477d4",
    "semantic_title": "noise-aware image captioning with progressively exploring mismatched words",
    "citation_count": 14,
    "authors": [
      "Zhongtian Fu",
      "Kefei Song",
      "Luping Zhou",
      "Yang Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29098": {
    "title": "Learning Small Decision Trees with Few Outliers: A Parameterized Perspective",
    "volume": "main",
    "abstract": "Decision trees is a fundamental tool in machine learning for representing, classifying, and generalizing data. It is desirable to construct ``small'' decision trees, by minimizing either the size (s) or the depth (d) of the decision tree (DT). Recently, the parameterized complexity of Decision Tree Learning has attracted a lot of attention. We consider a generalization of Decision Tree Learning where given a classification instance E and an integer t, the task is to find a ``small'' DT that disagrees with E in at most t examples. We consider two problems: DTSO and DTDO, where the goal is to construct a DT minimizing s and d, respectively. We first establish that both DTSO and DTDO are W[1]-hard when parameterized by s+y and d+y, respectively, where y is the maximum number of features in which two differently labeled examples can differ. We complement this result by showing that these problems become FPT if we include the parameter t. We also consider the kernelization complexity of these problems and establish several positive and negative results for both DTSO and DTDO",
    "checked": true,
    "id": "0574aa0fe87ffe299c9c4fd0592178f4e9e5f6a6",
    "semantic_title": "learning small decision trees with few outliers: a parameterized perspective",
    "citation_count": 0,
    "authors": [
      "Harmender Gahlawat",
      "Meirav Zehavi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29099": {
    "title": "Online Sensitivity Optimization in Differentially Private Learning",
    "volume": "main",
    "abstract": "Training differentially private machine learning models requires constraining an individual's contribution to the optimization process. This is achieved by clipping the 2-norm of their gradient at a predetermined threshold prior to averaging and batch sanitization. This selection adversely influences optimization in two opposing ways: it either exacerbates the bias due to excessive clipping at lower values, or augments sanitization noise at higher values. The choice significantly hinges on factors such as the dataset, model architecture, and even varies within the same optimization, demanding meticulous tuning usually accomplished through a grid search. In order to circumvent the privacy expenses incurred in hyperparameter tuning, we present a novel approach to dynamically optimize the clipping threshold. We treat this threshold as an additional learnable parameter, establishing a clean relationship between the threshold and the cost function. This allows us to optimize the former with gradient descent, with minimal repercussions on the overall privacy analysis. Our method is thoroughly assessed against alternative fixed and adaptive strategies across diverse datasets, tasks, model dimensions, and privacy levels. Our results indicate that it performs comparably or better in the evaluated scenarios, given the same privacy requirements",
    "checked": true,
    "id": "afceccf8025e53ba6d3900672459b2dbb8042327",
    "semantic_title": "online sensitivity optimization in differentially private learning",
    "citation_count": 0,
    "authors": [
      "Filippo Galli",
      "Catuscia Palamidessi",
      "Tommaso Cucinotta"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29100": {
    "title": "Compressing Image-to-Image Translation GANs Using Local Density Structures on Their Learned Manifold",
    "volume": "main",
    "abstract": "Generative Adversarial Networks (GANs) have shown remarkable success in modeling complex data distributions for image-to-image translation. Still, their high computational demands prohibit their deployment in practical scenarios like edge devices. Existing GAN compression methods mainly rely on knowledge distillation or convolutional classifiers' pruning techniques. Thus, they neglect the critical characteristic of GANs: their local density structure over their learned manifold. Accordingly, we approach GAN compression from a new perspective by explicitly encouraging the pruned model to preserve the density structure of the original parameter-heavy model on its learned manifold. We facilitate this objective for the pruned model by partitioning the learned manifold of the original generator into local neighborhoods around its generated samples. Then, we propose a novel pruning objective to regularize the pruned model to preserve the local density structure over each neighborhood, resembling the kernel density estimation method. Also, we develop a collaborative pruning scheme in which the discriminator and generator are pruned by two pruning agents. We design the agents to capture interactions between the generator and discriminator by exchanging their peer's feedback when determining corresponding models' architectures. Thanks to such a design, our pruning method can efficiently find performant sub-networks and can maintain the balance between the generator and discriminator more effectively compared to baselines during pruning, thereby showing more stable pruning dynamics. Our experiments on image translation GAN models, Pix2Pix and CycleGAN, with various benchmark datasets and architectures demonstrate our method's effectiveness",
    "checked": true,
    "id": "5d30d4b879df25658b6a696a87da8a38f88d8c31",
    "semantic_title": "compressing image-to-image translation gans using local density structures on their learned manifold",
    "citation_count": 1,
    "authors": [
      "Alireza Ganjdanesh",
      "Shangqian Gao",
      "Hirad Alipanah",
      "Heng Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29101": {
    "title": "ACT: Empowering Decision Transformer with Dynamic Programming via Advantage Conditioning",
    "volume": "main",
    "abstract": "Decision Transformer (DT), which employs expressive sequence modeling techniques to perform action generation, has emerged as a promising approach to offline policy optimization. However, DT generates actions conditioned on a desired future return, which is known to bear some weaknesses such as the susceptibility to environmental stochasticity. To overcome DT's weaknesses, we propose to empower DT with dynamic programming. Our method comprises three steps. First, we employ in-sample value iteration to obtain approximated value functions, which involves dynamic programming over the MDP structure. Second, we evaluate action quality in context with estimated advantages. We introduce two types of advantage estimators, IAE and GAE, which are suitable for different tasks. Third, we train an Advantage-Conditioned Transformer (ACT) to generate actions conditioned on the estimated advantages. Finally, during testing, ACT generates actions conditioned on a desired advantage. Our evaluation results validate that, by leveraging the power of dynamic programming, ACT demonstrates effective trajectory stitching and robust action generation in spite of the environmental stochasticity, outperforming baseline methods across various benchmarks. Additionally, we conduct an in-depth analysis of ACT's various design choices through ablation studies. Our code is available at https://github.com/LAMDA-RL/ACT",
    "checked": true,
    "id": "65240e368c4f67d4b31d2b1adb981e93db26d514",
    "semantic_title": "act: empowering decision transformer with dynamic programming via advantage conditioning",
    "citation_count": 5,
    "authors": [
      "Chen-Xiao Gao",
      "Chenyang Wu",
      "Mingjun Cao",
      "Rui Kong",
      "Zongzhang Zhang",
      "Yang Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29102": {
    "title": "Get a Head Start: On-Demand Pedagogical Policy Selection in Intelligent Tutoring",
    "volume": "main",
    "abstract": "Reinforcement learning (RL) is broadly employed in human-involved systems to enhance human outcomes. Off-policy evaluation (OPE) has been pivotal for RL in those realms since online policy learning and evaluation can be high-stake. Intelligent tutoring has raised tremendous attentions as highly challenging when applying OPE to human-involved systems, due to that students' subgroups can favor different pedagogical policies and the costly procedure that policies have to be induced fully offline and then directly deployed to the upcoming semester. In this work, we formulate on-demand pedagogical policy selection (ODPS) to tackle the challenges for OPE in intelligent tutoring. We propose a pipeline, EduPlanner, as a concrete solution for ODPS. Our pipeline results in an theoretically unbiased estimator, and enables efficient and customized policy selection by identifying subgroups over both historical data and on-arrival initial logs. We evaluate our approach on the Probability ITS that has been used in real classrooms for over eight years. Our study shows significant improvement on learning outcomes of students with EduPlanner, especially for the ones associated with low-performing subgroups",
    "checked": true,
    "id": "0f32cf298f4c37defb38c6fa57c4ea0031e588c0",
    "semantic_title": "get a head start: on-demand pedagogical policy selection in intelligent tutoring",
    "citation_count": 1,
    "authors": [
      "Ge Gao",
      "Xi Yang",
      "Min  Chi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29103": {
    "title": "Rethinking Causal Relationships Learning in Graph Neural Networks",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs) demonstrate their significance by effectively modeling complex interrelationships within graph-structured data. To enhance the credibility and robustness of GNNs, it becomes exceptionally crucial to bolster their ability to capture causal relationships. However, despite recent advancements that have indeed strengthened GNNs from a causal learning perspective, conducting an in-depth analysis specifically targeting the causal modeling prowess of GNNs remains an unresolved issue. In order to comprehensively analyze various GNN models from a causal learning perspective, we constructed an artificially synthesized dataset with known and controllable causal relationships between data and labels. The rationality of the generated data is further ensured through theoretical foundations. Drawing insights from analyses conducted using our dataset, we introduce a lightweight and highly adaptable GNN module designed to strengthen GNNs' causal learning capabilities across a diverse range of tasks. Through a series of experiments conducted on both synthetic datasets and other real-world datasets, we empirically validate the effectiveness of the proposed module. The codes are available at https://github.com/yaoyao-yaoyao-cell/CRCG",
    "checked": true,
    "id": "fc6c9584ec6d5e42b0bfccdca55280cd0f1b03b8",
    "semantic_title": "rethinking causal relationships learning in graph neural networks",
    "citation_count": 0,
    "authors": [
      "Hang Gao",
      "Chengyu Yao",
      "Jiangmeng Li",
      "Lingyu Si",
      "Yifan Jin",
      "Fengge Wu",
      "Changwen Zheng",
      "Huaping Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29104": {
    "title": "AVSegFormer: Audio-Visual Segmentation with Transformer",
    "volume": "main",
    "abstract": "Audio-visual segmentation (AVS) aims to locate and segment the sounding objects in a given video, which demands audio-driven pixel-level scene understanding. The existing methods cannot fully process the fine-grained correlations between audio and visual cues across various situations dynamically. They also face challenges in adapting to complex scenarios, such as evolving audio, the coexistence of multiple objects, and more. In this paper, we propose AVSegFormer, a novel framework for AVS that leverages the transformer architecture. Specifically, It comprises a dense audio-visual mixer, which can dynamically adjust interested visual features, and a sparse audio-visual decoder, which implicitly separates audio sources and automatically matches optimal visual features. Combining both components provides a more robust bidirectional conditional multi-modal representation, improving the segmentation performance in different scenarios. Extensive experiments demonstrate that AVSegFormer achieves state-of-the-art results on the AVS benchmark. The code is available at https://github.com/vvvb-github/AVSegFormer",
    "checked": true,
    "id": "710553070b9737e61a59408b3fc6d4172b5b7ba6",
    "semantic_title": "avsegformer: audio-visual segmentation with transformer",
    "citation_count": 20,
    "authors": [
      "Shengyi Gao",
      "Zhe Chen",
      "Guo Chen",
      "Wenhai Wang",
      "Tong Lu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29105": {
    "title": "Eliciting Kemeny Rankings",
    "volume": "main",
    "abstract": "We formulate the problem of eliciting agents' preferences with the goal of finding a Kemeny ranking as a Dueling Bandits problem. Here the bandits' arms correspond to alternatives that need to be ranked and the feedback corresponds to a pairwise comparison between alternatives by a randomly sampled agent. We consider both sampling with and without replacement, i.e., the possibility to ask the same agent about some comparison multiple times or not. We find approximation bounds for Kemeny rankings dependant on confidence intervals over estimated winning probabilities of arms. Based on these we state algorithms to find Probably Approximately Correct (PAC) solutions and elaborate on their sample complexity for sampling with or without replacement. Furthermore, if all agents' preferences are strict rankings over the alternatives, we provide means to prune confidence intervals and thereby guide a more efficient elicitation. We formulate several adaptive sampling methods that use look-aheads to estimate how much confidence intervals (and thus approximation guarantees) might be tightened. All described methods are compared on synthetic data",
    "checked": true,
    "id": "fbec5b25d50c500105e8cafda3c3213c58ff239e",
    "semantic_title": "eliciting kemeny rankings",
    "citation_count": 1,
    "authors": [
      "Anne-Marie George",
      "Christos Dimitrakakis"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29106": {
    "title": "Interactive Hyperparameter Optimization in Multi-Objective Problems via Preference Learning",
    "volume": "main",
    "abstract": "Hyperparameter optimization (HPO) is important to leverage the full potential of machine learning (ML). In practice, users are often interested in multi-objective (MO) problems, i.e., optimizing potentially conflicting objectives, like accuracy and energy consumption. To tackle this, the vast majority of MO-ML algorithms return a Pareto front of non-dominated machine learning models to the user. Optimizing the hyperparameters of such algorithms is non-trivial as evaluating a hyperparameter configuration entails evaluating the quality of the resulting Pareto front. In literature, there are known indicators that assess the quality of a Pareto front (e.g., hypervolume, R2) by quantifying different properties (e.g., volume, proximity to a reference point). However, choosing the indicator that leads to the desired Pareto front might be a hard task for a user. In this paper, we propose a human-centered interactive HPO approach tailored towards multi-objective ML leveraging preference learning to extract desiderata from users that guide the optimization. Instead of relying on the user guessing the most suitable indicator for their needs, our approach automatically learns an appropriate indicator. Concretely, we leverage pairwise comparisons of distinct Pareto fronts to learn such an appropriate quality indicator. Then, we optimize the hyperparameters of the underlying MO-ML algorithm towards this learned indicator using a state-of-the-art HPO approach. In an experimental study targeting the environmental impact of ML, we demonstrate that our approach leads to substantially better Pareto fronts compared to optimizing based on a wrong indicator pre-selected by the user, and performs comparable in the case of an advanced user knowing which indicator to pick",
    "checked": true,
    "id": "6643a1f70afa6ed06d615e13e83a7109cf80961c",
    "semantic_title": "interactive hyperparameter optimization in multi-objective problems via preference learning",
    "citation_count": 2,
    "authors": [
      "Joseph Giovanelli",
      "Alexander Tornede",
      "Tanja Tornede",
      "Marius Lindauer"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29107": {
    "title": "Layer Compression of Deep Networks with Straight Flows",
    "volume": "main",
    "abstract": "Very deep neural networks lead to significantly better performance on various real tasks. However, it usually causes slow inference and is hard to be deployed on real-world devices. How to reduce the number of layers to save memory and to accelerate the inference is an eye-catching topic. In this work, we introduce an intermediate objective, a continuous-time network, before distilling deep networks into shallow networks. First, we distill a given deep network into a continuous-time neural flow model, which can be discretized with an ODE solver and the inference requires passing through the network multiple times. By forcing the flow transport trajectory to be straight lines, we find that it is easier to compress the infinite step model into a one-step neural flow model, which only requires passing through the flow model once. Secondly, we refine the one-step flow model together with the final head layer with knowledge distillation and finally, we can replace the given deep network with this one-step flow network. Empirically, we demonstrate that our method outperforms direct distillation and other baselines on different model architectures (e.g. ResNet, ViT) on image classification and semantic segmentation tasks. We also manifest that our distilled model naturally serves as an early-exit dynamic inference model",
    "checked": true,
    "id": "848c79efda0617223e6b32af0838f5e59af92262",
    "semantic_title": "layer compression of deep networks with straight flows",
    "citation_count": 0,
    "authors": [
      "Chengyue Gong",
      "Xiaocong Du",
      "Bhargav Bhushanam",
      "Lemeng  Wu",
      "Xingchao Liu",
      "Dhruv Choudhary",
      "Arun Kejariwal",
      "Qiang Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29108": {
    "title": "Fast and Controllable Post-training Sparsity: Learning Optimal Sparsity Allocation with Global Constraint in Minutes",
    "volume": "main",
    "abstract": "Neural network sparsity has attracted many research interests due to its similarity to biological schemes and high energy efficiency. However, existing methods depend on long-time training or fine-tuning, which prevents large-scale applications. Recently, some works focusing on post-training sparsity (PTS) have emerged. They get rid of the high training cost but usually suffer from distinct accuracy degradation due to neglect of the reasonable sparsity rate at each layer. Previous methods for finding sparsity rates mainly focus on the training-aware scenario, which usually fails to converge stably under the PTS setting with limited data and much less training cost. In this paper, we propose a fast and controllable post-training sparsity (FCPTS) framework. By incorporating a differentiable bridge function and a controllable optimization objective, our method allows for rapid and accurate sparsity allocation learning in minutes, with the added assurance of convergence to a predetermined global sparsity rate. Equipped with these techniques, we can surpass the state-of-the-art methods by a large margin, e.g., over 30\\% improvement for ResNet-50 on ImageNet under the sparsity rate of 80\\%. Our plug-and-play code and supplementary materials are open-sourced at https://github.com/ModelTC/FCPTS",
    "checked": true,
    "id": "e4a90aafb60b1a9944f055afc9d903bc06b21a86",
    "semantic_title": "fast and controllable post-training sparsity: learning optimal sparsity allocation with global constraint in minutes",
    "citation_count": 0,
    "authors": [
      "Ruihao Gong",
      "Yang Yong",
      "Zining Wang",
      "Jinyang Guo",
      "Xiuying Wei",
      "Yuqing Ma",
      "Xianglong Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29109": {
    "title": "DeepSaDe: Learning Neural Networks That Guarantee Domain Constraint Satisfaction",
    "volume": "main",
    "abstract": "As machine learning models, specifically neural networks, are becoming increasingly popular, there are concerns regarding their trustworthiness, specially in safety-critical applications, e.g. actions of an autonomous vehicle must be safe. There are approaches that can train neural networks where such domain requirements are enforced as constraints, but they either cannot guarantee that the constraint will be satisfied by all possible predictions (even on unseen data) or they are limited in the type of constraints that can be enforced. In this paper, we present an approach to train neural networks which can enforce a wide variety of constraints and guarantee that the constraint is satisfied by all possible predictions. The approach builds on earlier work where learning linear models is formulated as a constraint satisfaction problem (CSP). To make this idea applicable to neural networks, two crucial new elements are added: constraint propagation over the network layers, and weight updates based on a mix of gradient descent and CSP solving. Evaluation on various machine learning tasks demonstrates that our approach is flexible enough to enforce a wide variety of domain constraints and is able to guarantee them in neural networks",
    "checked": true,
    "id": "d932b9aee9270ef3c504361984025c50aa0a76dd",
    "semantic_title": "deepsade: learning neural networks that guarantee domain constraint satisfaction",
    "citation_count": 0,
    "authors": [
      "Kshitij Goyal",
      "Sebastijan Dumancic",
      "Hendrik Blockeel"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29110": {
    "title": "Taming the Sigmoid Bottleneck: Provably Argmaxable Sparse Multi-Label Classification",
    "volume": "main",
    "abstract": "Sigmoid output layers are widely used in multi-label classification (MLC) tasks, in which multiple labels can be assigned to any input. In many practical MLC tasks, the number of possible labels is in the thousands, often exceeding the number of input features and resulting in a low-rank output layer. In multi-class classification, it is known that such a low-rank output layer is a bottleneck that can result in unargmaxable classes: classes which cannot be predicted for any input. In this paper, we show that for MLC tasks, the analogous sigmoid bottleneck results in exponentially many unargmaxable label combinations. We explain how to detect these unargmaxable outputs and demonstrate their presence in three widely used MLC datasets. We then show that they can be prevented in practice by introducing a Discrete Fourier Transform (DFT) output layer, which guarantees that all sparse label combinations with up to k active labels are argmaxable. Our DFT layer trains faster and is more parameter efficient, matching the F1@k score of a sigmoid layer while using up to 50% fewer trainable parameters. Our code is publicly available at https://github.com/andreasgrv/sigmoid-bottleneck",
    "checked": true,
    "id": "69fcfa2cd23d4d29a26658e24f874cd0ba4b37b1",
    "semantic_title": "taming the sigmoid bottleneck: provably argmaxable sparse multi-label classification",
    "citation_count": 1,
    "authors": [
      "Andreas Grivas",
      "Antonio Vergari",
      "Adam Lopez"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29111": {
    "title": "Summarizing Stream Data for Memory-Constrained Online Continual Learning",
    "volume": "main",
    "abstract": "Replay-based methods have proved their effectiveness on online continual learning by rehearsing past samples from an auxiliary memory. With many efforts made on improving training schemes based on the memory, however, the information carried by each sample in the memory remains under-investigated. Under circumstances with restricted storage space, the informativeness of the memory becomes critical for effective replay. Although some works design specific strategies to select representative samples, by only employing a small number of original images, the storage space is still not well utilized. To this end, we propose to Summarize the knowledge from the Stream Data (SSD) into more informative samples by distilling the training characteristics of real images. Through maintaining the consistency of training gradients and relationship to the past tasks, the summarized samples are more representative for the stream data compared to the original images. Extensive experiments are conducted on multiple online continual learning benchmarks to support that the proposed SSD method significantly enhances the replay effects. We demonstrate that with limited extra computational overhead, SSD provides more than 3% accuracy boost for sequential CIFAR-100 under extremely restricted memory buffer. Code in https://github.com/vimar-gu/SSD",
    "checked": true,
    "id": "9299b1f5aa68b60c0c4a478ad1f5968d9437086a",
    "semantic_title": "summarizing stream data for memory-constrained online continual learning",
    "citation_count": 4,
    "authors": [
      "Jianyang Gu",
      "Kai Wang",
      "Wei Jiang",
      "Yang You"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29112": {
    "title": "G-Adapter: Towards Structure-Aware Parameter-Efficient Transfer Learning for Graph Transformer Networks",
    "volume": "main",
    "abstract": "It has become a popular paradigm to transfer the knowledge of large-scale pre-trained models to various downstream tasks via fine-tuning the entire model parameters. However, with the growth of model scale and the rising number of downstream tasks, this paradigm inevitably meets the challenges in terms of computation consumption and memory footprint issues. Recently, Parameter-Efficient Fine-Tuning (PEFT) (e.g., Adapter, LoRA, BitFit) shows a promising paradigm to alleviate these concerns by updating only a portion of parameters. Despite these PEFTs having demonstrated satisfactory performance in natural language processing, it remains under-explored for the question: whether these techniques could be transferred to graph-based tasks with Graph Transformer Networks (GTNs)? Therefore, in this paper, we fill this gap by providing extensive benchmarks with traditional PEFTs on a range of graph-based downstream tasks. Our empirical study shows that it is sub-optimal to directly transfer existing PEFTs to graph-based tasks due to the issue of feature distribution shift. To address this issue, we propose a novel structure-aware PEFT approach, named G-Adapter, which leverages graph convolution operation to introduce graph structure information (e.g., graph adjacency matrix) as an inductive bias to guide the updating process. Further, we propose Bregman proximal point optimization to alleviate feature distribution shift by preventing the model from aggressive update. Extensive experiments demonstrate that G-Adapter obtains state-of-the-art performance compared to counterparts on nine graph benchmark datasets based on diverse pre-trained GTNs, and delivers tremendous memory footprint efficiency compared to the conventional paradigm",
    "checked": true,
    "id": "e0bc91243e4e446f6b8871b4fc40b4a413f93c73",
    "semantic_title": "g-adapter: towards structure-aware parameter-efficient transfer learning for graph transformer networks",
    "citation_count": 8,
    "authors": [
      "Anchun Gui",
      "Jinqiang Ye",
      "Han Xiao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29113": {
    "title": "FedCSL: A Scalable and Accurate Approach to Federated Causal Structure Learning",
    "volume": "main",
    "abstract": "As an emerging research direction, federated causal structure learning (CSL) aims at learning causal relationships from decentralized data across multiple clients while preserving data privacy. Existing federated CSL algorithms suffer from scalability and accuracy issues, since they require computationally expensive CSL algorithms to be executed at each client. Furthermore, in real-world scenarios, the number of samples held by each client varies significantly, and existing methods still assign equal weights to the learned structural information from each client, which severely harms the learning accuracy of those methods. To address these two limitations, we propose FedCSL, a scalable and accurate method for federated CSL. Specifically, FedCSL consists of two novel strategies: (1) a federated local-to-global learning strategy that enables FedCSL to scale to high-dimensional data for tackling the scalability issue, and (2) a novel weighted aggregation strategy that does not rely on any complex encryption techniques while preserving data privacy for tackling the accuracy issue. Extensive experiments on benchmark datasets, high-dimensional synthetic datasets and a real-world dataset verify the efficacy of the proposed FedCSL method. The source code is available at https://github.com/Xianjie-Guo/FedCSL",
    "checked": true,
    "id": "e9897de565235b28cea91bf3e29833384d8545bd",
    "semantic_title": "fedcsl: a scalable and accurate approach to federated causal structure learning",
    "citation_count": 0,
    "authors": [
      "Xianjie Guo",
      "Kui Yu",
      "Lin Liu",
      "Jiuyong Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29114": {
    "title": "Ternary Spike: Learning Ternary Spikes for Spiking Neural Networks",
    "volume": "main",
    "abstract": "The Spiking Neural Network (SNN), as one of the biologically inspired neural network infrastructures, has drawn increasing attention recently. It adopts binary spike activations to transmit information, thus the multiplications of activations and weights can be substituted by additions, which brings high energy efficiency. However, in the paper, we theoretically and experimentally prove that the binary spike activation map cannot carry enough information, thus causing information loss and resulting in accuracy decreasing. To handle the problem, we propose a ternary spike neuron to transmit information. The ternary spike neuron can also enjoy the event-driven and multiplication-free operation advantages of the binary spike neuron but will boost the information capacity. Furthermore, we also embed a trainable factor in the ternary spike neuron to learn the suitable spike amplitude, thus our SNN will adopt different spike amplitudes along layers, which can better suit the phenomenon that the membrane potential distributions are different along layers. To retain the efficiency of the vanilla ternary spike, the trainable ternary spike SNN will be converted to a standard one again via a re-parameterization technique in the inference. Extensive experiments with several popular network structures over static and dynamic datasets show that the ternary spike can consistently outperform state-of-the-art methods. Our code is open-sourced at https://github.com/yfguo91/Ternary-Spike",
    "checked": true,
    "id": "d20f4c275d9c4121cc23a7b14ff0331c62d4005c",
    "semantic_title": "ternary spike: learning ternary spikes for spiking neural networks",
    "citation_count": 6,
    "authors": [
      "Yufei Guo",
      "Yuanpei Chen",
      "Xiaode Liu",
      "Weihang Peng",
      "Yuhan Zhang",
      "Xuhui Huang",
      "Zhe Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29115": {
    "title": "From Past to Future: Rethinking Eligibility Traces",
    "volume": "main",
    "abstract": "In this paper, we introduce a fresh perspective on the challenges of credit assignment and policy evaluation. First, we delve into the nuances of eligibility traces and explore instances where their updates may result in unexpected credit assignment to preceding states. From this investigation emerges the concept of a novel value function, which we refer to as the ????????????? ????? ????????. Unlike traditional state value functions, bidirectional value functions account for both future expected returns (rewards anticipated from the current state onward) and past expected returns (cumulative rewards from the episode's start to the present). We derive principled update equations to learn this value function and, through experimentation, demonstrate its efficacy in enhancing the process of policy evaluation. In particular, our results indicate that the proposed learning approach can, in certain challenging contexts, perform policy evaluation more rapidly than TD(λ)–a method that learns forward value functions, v^π, ????????. Overall, our findings present a new perspective on eligibility traces and potential advantages associated with the novel value function it inspires, especially for policy evaluation",
    "checked": true,
    "id": "a9d27f7568ab5e847d8e4d321183255516db092f",
    "semantic_title": "from past to future: rethinking eligibility traces",
    "citation_count": 1,
    "authors": [
      "Dhawal Gupta",
      "Scott M. Jordan",
      "Shreyas Chaudhari",
      "Bo Liu",
      "Philip S. Thomas",
      "Bruno Castro da Silva"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29116": {
    "title": "Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability",
    "volume": "main",
    "abstract": "Fine-tuning pre-trained neural network models has become a widely adopted approach across various domains. However, it can lead to the distortion of pre-trained feature extractors that already possess strong generalization capabilities. Mitigating feature distortion during adaptation to new target domains is crucial. Recent studies have shown promising results in handling feature distortion by aligning the head layer on in-distribution datasets before performing fine-tuning. Nonetheless, a significant limitation arises from the treatment of batch normalization layers during fine-tuning, leading to suboptimal performance. In this paper, we propose Domain-Aware Fine-Tuning (DAFT), a novel approach that incorporates batch normalization conversion and the integration of linear probing and fine-tuning. Our batch normalization conversion method effectively mitigates feature distortion by reducing modifications to the neural network during fine-tuning. Additionally, we introduce the integration of linear probing and fine-tuning to optimize the head layer with gradual adaptation of the feature extractor. By leveraging batch normalization layers and integrating linear probing and fine-tuning, our DAFT significantly mitigates feature distortion and achieves improved model performance on both in-distribution and out-of-distribution datasets. Extensive experiments demonstrate that our method outperforms other baseline methods, demonstrating its effectiveness in not only improving performance but also mitigating feature distortion",
    "checked": true,
    "id": "a8058e756429c3ea47f96ca7ea2f98245ad69308",
    "semantic_title": "domain-aware fine-tuning: enhancing neural network adaptability",
    "citation_count": 1,
    "authors": [
      "Seokhyeon Ha",
      "Sunbeom Jeong",
      "Jungwoo Lee"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29117": {
    "title": "Forced Exploration in Bandit Problems",
    "volume": "main",
    "abstract": "The multi-armed bandit(MAB) is a classical sequential decision problem. Most work requires assumptions about the reward distribution (e.g., bounded), while practitioners may have difficulty obtaining information about these distributions to design models for their problems, especially in non-stationary MAB problems. This paper aims to design a multi-armed bandit algorithm that can be implemented without using information about the reward distribution while still achieving substantial regret upper bounds. To this end, we propose a novel algorithm alternating between greedy rule and forced exploration. Our method can be applied to Gaussian, Bernoulli and other subgaussian distributions, and its implementation does not require additional information. We employ a unified analysis method for different forced exploration strategies and provide problem-dependent regret upper bounds for stationary and piecewise-stationary settings. Furthermore, we compare our algorithm with popular bandit algorithms on different reward distributions",
    "checked": true,
    "id": "153efa261008f474c2139273b7c28a2a7642bb91",
    "semantic_title": "forced exploration in bandit problems",
    "citation_count": 0,
    "authors": [
      "Qi Han",
      "Li Zhu",
      "Fei Guo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29118": {
    "title": "Complexity of Neural Network Training and ETR: Extensions with Effectively Continuous Functions",
    "volume": "main",
    "abstract": "The training problem of neural networks (NNs) is known to be ER-complete with respect to ReLU and linear activation functions. We show that the training problem for NNs equipped with arbitrary activation functions is polynomial-time bireducible to the existential theory of the reals extended with the corresponding activation functions. For effectively continuous activation functions (e.g., the sigmoid function), we obtain an inclusion to low levels of the arithmetical hierarchy. Consequently, the sigmoid activation function leads to the existential theory of the reals with the exponential function, and hence the decidability of training NNs using the sigmoid activation function is equivalent to the decidability of the existential theory of the reals with the exponential function, a long-standing open problem. In contrast, we obtain that the training problem is undecidable if sinusoidal activation functions are considered",
    "checked": true,
    "id": "9bc96c48125a8ba3fe43f4c6363aa77a6f2a98aa",
    "semantic_title": "complexity of neural network training and etr: extensions with effectively continuous functions",
    "citation_count": 4,
    "authors": [
      "Teemu Hankala",
      "Miika Hannula",
      "Juha Kontinen",
      "Jonni Virtema"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29119": {
    "title": "Composite Active Learning: Towards Multi-Domain Active Learning with Theoretical Guarantees",
    "volume": "main",
    "abstract": "Active learning (AL) aims to improve model performance within a fixed labeling budget by choosing the most informative data points to label. Existing AL focuses on the single-domain setting, where all data come from the same domain (e.g., the same dataset). However, many real-world tasks often involve multiple domains. For example, in visual recognition, it is often desirable to train an image classifier that works across different environments (e.g., different backgrounds), where images from each environment constitute one domain. Such a multi-domain AL setting is challenging for prior methods because they (1) ignore the similarity among different domains when assigning labeling budget and (2) fail to handle distribution shift of data across different domains. In this paper, we propose the first general method, dubbed composite active learning (CAL), for multi-domain AL. Our approach explicitly considers the domain-level and instance-level information in the problem; CAL first assigns domain-level budgets according to domain-level importance, which is estimated by optimizing an upper error bound that we develop; with the domain-level budgets, CAL then leverages a certain instance-level query strategy to select samples to label from each domain. Our theoretical analysis shows that our method achieves a better error bound compared to current AL methods. Our empirical results demonstrate that our approach significantly outperforms the state-of-the-art AL methods on both synthetic and real-world multi-domain datasets. Code is available at https://github.com/Wang-ML-Lab/multi-domain-active-learning",
    "checked": true,
    "id": "f3d41fa02922e9a952ba043ea9bb759d59a8f318",
    "semantic_title": "composite active learning: towards multi-domain active learning with theoretical guarantees",
    "citation_count": 0,
    "authors": [
      "Guang-Yuan Hao",
      "Hengguan Huang",
      "Haotian Wang",
      "Jie Gao",
      "Hao Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29120": {
    "title": "Double-Layer Hybrid-Label Identification Feature Selection for Multi-View Multi-Label Learning",
    "volume": "main",
    "abstract": "Multi-view multi-label feature selection aims to select informative features where the data are collected from multiple sources with multiple interdependent class labels. For fully exploiting multi-view information, most prior works mainly focus on the common part in the ideal circumstance. However, the inconsistent part hidden in each view, including noises and specific elements, may affect the quality of mapping between labels and feature representations. Meanwhile, ignoring the specific part might lead to a suboptimal result, as each label is supposed to possess specific characteristics of its own. To deal with the double problems in multi-view multi-label feature selection, we propose a unified loss function which is a totally splitting structure for observed labels as hybrid labels that is, common labels, view-to-all specific labels and noisy labels, and the view-to-all specific labels further splits into several specific labels of each view. The proposed method simultaneously considers the consistency and complementarity of different views. Through exploring the feature weights of hybrid labels, the mapping relationships between labels and features can be established sequentially based on their attributes. Additionally, the interrelatedness among hybrid labels is also investigated and injected into the function. Specific to the specific labels of each view, we construct the novel regularization paradigm incorporating logic operations. Finally, the convergence of the result is proved after applying the multiplicative update rules. Experiments on six datasets demonstrate the effectiveness and superiority of our method compared with the state-of-the-art methods",
    "checked": true,
    "id": "c431d022bc9ca64b78a58266b7efa38e625e9038",
    "semantic_title": "double-layer hybrid-label identification feature selection for multi-view multi-label learning",
    "citation_count": 1,
    "authors": [
      "Pingting Hao",
      "Kunpeng Liu",
      "Wanfu Gao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29121": {
    "title": "Multiagent Gumbel MuZero: Efficient Planning in Combinatorial Action Spaces",
    "volume": "main",
    "abstract": "AlphaZero and MuZero have achieved state-of-the-art (SOTA) performance in a wide range of domains, including board games and robotics, with discrete and continuous action spaces. However, to obtain an improved policy, they often require an excessively large number of simulations, especially for domains with large action spaces. As the simulation budget decreases, their performance drops significantly. In addition, many important real-world applications have combinatorial (or exponential) action spaces, making it infeasible to search directly over all possible actions. In this paper, we extend AlphaZero and MuZero to learn and plan in more complex multiagent (MA) Markov decision processes, where the action spaces increase exponentially with the number of agents. Our new algorithms, MA Gumbel AlphaZero and MA Gumbel MuZero, respectively without and with model learning, achieve superior performance on cooperative multiagent control problems, while reducing the number of environmental interactions by up to an order of magnitude compared to model-free approaches. In particular, we significantly improve prior performance when planning with much fewer simulation budgets. The code and appendix are available at https://github.com/tjuHaoXiaotian/MA-MuZero",
    "checked": true,
    "id": "b163603692616b3dd244c38a13c74812c71b2f30",
    "semantic_title": "multiagent gumbel muzero: efficient planning in combinatorial action spaces",
    "citation_count": 1,
    "authors": [
      "Xiaotian Hao",
      "Jianye Hao",
      "Chenjun Xiao",
      "Kai Li",
      "Dong Li",
      "Yan Zheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29122": {
    "title": "Calibrated One Round Federated Learning with Bayesian Inference in the Predictive Space",
    "volume": "main",
    "abstract": "Federated Learning (FL) involves training a model over a dataset distributed among clients, with the constraint that each client's dataset is localized and possibly heterogeneous. In FL, small and noisy datasets are common, highlighting the need for well-calibrated models that represent the uncertainty of predictions. The closest FL techniques to achieving such goals are the Bayesian FL methods which collect parameter samples from local posteriors, and aggregate them to approximate the global posterior. To improve scalability for larger models, one common Bayesian approach is to approximate the global predictive posterior by multiplying local predictive posteriors. In this work, we demonstrate that this method gives systematically overconfident predictions, and we remedy this by proposing β-Predictive Bayes, a Bayesian FL algorithm that interpolates between a mixture and product of the predictive posteriors, using a tunable parameter β. This parameter is tuned to improve the global ensemble's calibration, before it is distilled to a single model. Our method is evaluated on a variety of regression and classification datasets to demonstrate its superiority in calibration to other baselines, even as data heterogeneity increases. Code available at https://github.com/hasanmohsin/betaPredBayesFL. Our paper's full version is at https://arxiv.org/abs/2312.09817",
    "checked": true,
    "id": "38e19b0007d7ac07e041becdb0761fedc9901c37",
    "semantic_title": "calibrated one round federated learning with bayesian inference in the predictive space",
    "citation_count": 3,
    "authors": [
      "Mohsin Hasan",
      "Guojun Zhang",
      "Kaiyang Guo",
      "Xi Chen",
      "Pascal Poupart"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29123": {
    "title": "Selective Deep Autoencoder for Unsupervised Feature Selection",
    "volume": "main",
    "abstract": "In light of the advances in big data, high-dimensional datasets are often encountered. Incorporating them into data-driven models can enhance performance; however, this comes at the cost of high computation and the risk of overfitting, particularly due to abundant redundant features. Identifying an informative subset of the features helps in reducing the dimensionality and enhancing model interpretability. In this paper, we propose a novel framework for unsupervised feature selection, called Selective Deep Auto-Encoder (SDAE). It aims to reduce the number of features used in unlabeled datasets without compromising the quality of information obtained. It achieves this by selecting sufficient features - from the original feature set - capable of representing the entire feature space and reconstructing them. Architecturally, it leverages the use of highly nonlinear latent representations in deep Autoencoders and intrinsically learns, in an unsupervised fashion, the relevant and globally representative subset of features through a customized Selective Layer. Extensive experimental results on three high-dimensional public datasets have shown promising feature selection performance by SDAE in comparison to other existing state-of-the-art unsupervised feature selection methods",
    "checked": true,
    "id": "a94cfa78d58776c1ce8b0db0428df74b29ed7a7f",
    "semantic_title": "selective deep autoencoder for unsupervised feature selection",
    "citation_count": 0,
    "authors": [
      "Wael Hassanieh",
      "Abdallah Chehade"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29124": {
    "title": "Fairness under Covariate Shift: Improving Fairness-Accuracy Tradeoff with Few Unlabeled Test Samples",
    "volume": "main",
    "abstract": "Covariate shift in the test data is a common practical phenomena that can significantly downgrade both the accuracy and the fairness performance of the model. Ensuring fairness across different sensitive groups under covariate shift is of paramount importance due to societal implications like criminal justice. We operate in the unsupervised regime where only a small set of unlabeled test samples along with a labeled training set is available. Towards improving fairness under this highly challenging yet realistic scenario, we make three contributions. First is a novel composite weighted entropy based objective for prediction accuracy which is optimized along with a representation matching loss for fairness. We experimentally verify that optimizing with our loss formulation outperforms a number of state-of-the-art baselines in the pareto sense with respect to the fairness-accuracy tradeoff on several standard datasets. Our second contribution is a new setting we term Asymmetric Covariate Shift that, to the best of our knowledge, has not been studied before. Asymmetric covariate shift occurs when distribution of covariates of one group shifts significantly compared to the other groups and this happens when a dominant group is over-represented. While this setting is extremely challenging for current baselines, We show that our proposed method significantly outperforms them. Our third contribution is theoretical, where we show that our weighted entropy term along with prediction loss on the training set approximates test loss under covariate shift. Empirically and through formal sample complexity bounds, we show that this approximation to the unseen test loss does not depend on importance sampling variance which affects many other baselines",
    "checked": true,
    "id": "02b7668ae7857ce03040bbd1920653a8d8d0c711",
    "semantic_title": "fairness under covariate shift: improving fairness-accuracy tradeoff with few unlabeled test samples",
    "citation_count": 1,
    "authors": [
      "Shreyas Havaldar",
      "Jatin Chauhan",
      "Karthikeyan Shanmugam",
      "Jay Nandy",
      "Aravindan Raghuveer"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29125": {
    "title": "A New Mechanism for Eliminating Implicit Conflict in Graph Contrastive Learning",
    "volume": "main",
    "abstract": "Graph contrastive learning (GCL) has attracted considerable attention because it can self-supervisedly extract low-dimensional representation of graph data. InfoNCE-based loss function is widely used in graph contrastive learning, which pulls the representations of positive pairs close to each other and pulls the representations of negative pairs away from each other. Recent works mainly focus on designing new augmentation methods or sampling strategies. However, we argue that the widely used InfoNCE-based methods may contain an implicit conflict which seriously confuses models when learning from negative pairs. This conflict is engendered by the encoder's message-passing mechanism and the InfoNCE loss function. As a result, the learned representations between negative samples cannot be far away from each other, compromising the model performance. To our best knowledge, this is the first time to report and analysis this conflict of GCL. To address this problem, we propose a simple but effective method called Partial ignored Graph Contrastive Learning (PiGCL). Specifically, PiGCL first dynamically captures the conflicts during training by detecting the gradient of representation similarities. It then enables the loss function to ignore the conflict, allowing the encoder to adaptively learn the ignored information without self-supervised samples. Extensive experiments demonstrate the effectiveness of our method",
    "checked": true,
    "id": "648ffe6ff511d8fbddf2a356855e5e5cd40bc43d",
    "semantic_title": "a new mechanism for eliminating implicit conflict in graph contrastive learning",
    "citation_count": 0,
    "authors": [
      "Dongxiao He",
      "Jitao Zhao",
      "Cuiying Huo",
      "Yongqi Huang",
      "Yuxiao Huang",
      "Zhiyong Feng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29126": {
    "title": "Improving Distinguishability of Class for Graph Neural Networks",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs) have received widespread attention and applications due to their excellent performance in graph representation learning. Most existing GNNs can only aggregate 1-hop neighbors in a GNN layer, so they usually stack multiple GNN layers to obtain more information from larger neighborhoods. However, many studies have shown that model performance experiences a significant degradation with the increase of GNN layers. In this paper, we first introduce the concept of distinguishability of class to indirectly evaluate the learned node representations, and verify the positive correlation between distinguishability of class and model performance. Then, we propose a Graph Neural Network guided by Distinguishability of class (Disc-GNN) to monitor the representation learning, so as to learn better node representations and improve model performance. Specifically, we first perform inter-layer filtering and initial compensation based on Local Distinguishability of Class (LDC) in each layer, so that the learned node representations have the ability to distinguish different classes. Furthermore, we add a regularization term based on Global Distinguishability of Class (GDC) to achieve global optimization of model performance. Extensive experiments on six real-world datasets have shown that the competitive performance of Disc-GNN to the state-of-the-art methods on node classification and node clustering tasks",
    "checked": true,
    "id": "428685ee522ba82bbed74ecf743ff892c5b1da83",
    "semantic_title": "improving distinguishability of class for graph neural networks",
    "citation_count": 0,
    "authors": [
      "Dongxiao He",
      "Shuwei Liu",
      "Meng Ge",
      "Zhizhi Yu",
      "Guangquan Xu",
      "Zhiyong Feng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29127": {
    "title": "Decoupling Meta-Reinforcement Learning with Gaussian Task Contexts and Skills",
    "volume": "main",
    "abstract": "Offline meta-reinforcement learning (meta-RL) methods, which adapt to unseen target tasks with prior experience, are essential in robot control tasks. Current methods typically utilize task contexts and skills as prior experience, where task contexts are related to the information within each task and skills represent a set of temporally extended actions for solving subtasks. However, these methods still suffer from limited performance when adapting to unseen target tasks, mainly because the learned prior experience lacks generalization, i.e., they are unable to extract effective prior experience from meta-training tasks by exploration and learning of continuous latent spaces. We propose a framework called decoupled meta-reinforcement learning (DCMRL), which (1) contrastively restricts the learning of task contexts through pulling in similar task contexts within the same task and pushing away different task contexts of different tasks, and (2) utilizes a Gaussian quantization variational autoencoder (GQ-VAE) for clustering the Gaussian distributions of the task contexts and skills respectively, and decoupling the exploration and learning processes of their spaces. These cluster centers which serve as representative and discrete distributions of task context and skill are stored in task context codebook and skill codebook, respectively. DCMRL can acquire generalizable prior experience and achieve effective adaptation to unseen target tasks during the meta-testing phase. Experiments in the navigation and robot manipulation continuous control tasks show that DCMRL is more effective than previous meta-RL methods with more generalizable prior experience",
    "checked": true,
    "id": "d31def7275a87c53b8646f449ba4b467cd25efce",
    "semantic_title": "decoupling meta-reinforcement learning with gaussian task contexts and skills",
    "citation_count": 0,
    "authors": [
      "Hongcai He",
      "Anjie Zhu",
      "Shuang Liang",
      "Feiyu Chen",
      "Jie Shao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29128": {
    "title": "IS-DARTS: Stabilizing DARTS through Precise Measurement on Candidate Importance",
    "volume": "main",
    "abstract": "Among existing Neural Architecture Search methods, DARTS is known for its efficiency and simplicity. This approach applies continuous relaxation of network representation to construct a weight-sharing supernet and enables the identification of excellent subnets in just a few GPU days. However, performance collapse in DARTS results in deteriorating architectures filled with parameter-free operations and remains a great challenge to the robustness. To resolve this problem, we reveal that the fundamental reason is the biased estimation of the candidate importance in the search space through theoretical and experimental analysis, and more precisely select operations via information-based measurements. Furthermore, we demonstrate that the excessive concern over the supernet and inefficient utilization of data in bi-level optimization also account for suboptimal results. We adopt a more realistic objective focusing on the performance of subnets and simplify it with the help of the informationbased measurements. Finally, we explain theoretically why progressively shrinking the width of the supernet is necessary and reduce the approximation error of optimal weights in DARTS. Our proposed method, named IS-DARTS, comprehensively improves DARTS and resolves the aforementioned problems. Extensive experiments on NAS-Bench-201 and DARTS-based search space demonstrate the effectiveness of IS-DARTS",
    "checked": true,
    "id": "409582f19188915e4ac91000d1b56c974ad21176",
    "semantic_title": "is-darts: stabilizing darts through precise measurement on candidate importance",
    "citation_count": 1,
    "authors": [
      "Hongyi He",
      "Longjun Liu",
      "Haonan Zhang",
      "Nanning Zheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29129": {
    "title": "Not All Tasks Are Equally Difficult: Multi-Task Deep Reinforcement Learning with Dynamic Depth Routing",
    "volume": "main",
    "abstract": "Multi-task reinforcement learning endeavors to accomplish a set of different tasks with a single policy. To enhance data efficiency by sharing parameters across multiple tasks, a common practice segments the network into distinct modules and trains a routing network to recombine these modules into task-specific policies. However, existing routing approaches employ a fixed number of modules for all tasks, neglecting that tasks with varying difficulties commonly require varying amounts of knowledge. This work presents a Dynamic Depth Routing (D2R) framework, which learns strategic skipping of certain intermediate modules, thereby flexibly choosing different numbers of modules for each task. Under this framework, we further introduce a ResRouting method to address the issue of disparate routing paths between behavior and target policies during off-policy training. In addition, we design an automatic route-balancing mechanism to encourage continued routing exploration for unmastered tasks without disturbing the routing of mastered ones. We conduct extensive experiments on various robotics manipulation tasks in the Meta-World benchmark, where D2R achieves state-of-the-art performance with significantly improved learning efficiency",
    "checked": false,
    "id": "5fbd5c7208ea2358279f75694c0f28e2f18d4e96",
    "semantic_title": "not all tasks are equally difficult: multi-task reinforcement learning with dynamic depth routing",
    "citation_count": 1,
    "authors": [
      "Jinmin He",
      "Kai Li",
      "Yifan Zang",
      "Haobo Fu",
      "Qiang Fu",
      "Junliang Xing",
      "Jian Cheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29130": {
    "title": "Enhancing Semi-supervised Domain Adaptation via Effective Target Labeling",
    "volume": "main",
    "abstract": "Existing semi-supervised domain adaptation (SSDA) models have exhibited impressive performance on the target domain by effectively utilizing few labeled target samples per class (e.g., 3 samples per class). To guarantee an equal number of labeled target samples for each class, however, they require domain experts to manually recognize a considerable amount of the unlabeled target data. Moreover, as the target samples are not equally informative for shaping the decision boundaries of the learning models, it is crucial to select the most informative target samples for labeling, which is, however, impossible for human selectors. As a remedy, we propose an EFfective Target Labeling (EFTL) framework that harnesses active learning and pseudo-labeling strategies to automatically select some informative target samples to annotate. Concretely, we introduce a novel sample query strategy, called non-maximal degree node suppression (NDNS), that iteratively performs maximal degree node query and non-maximal degree node removal to select representative and diverse target samples for labeling. To learn target-specific characteristics, we propose a novel pseudo-labeling strategy that attempts to label low-confidence target samples accurately via clustering consistency (CC), and then inject information of the model uncertainty into our query process. CC enhances the utilization of the annotation budget and increases the number of \"labeled\" target samples while requiring no additional manual effort. Our proposed EFTL framework can be easily coupled with existing SSDA models, showing significant improvements on three benchmarks",
    "checked": true,
    "id": "466f667d14c5244ef576afb7ef45394ad41c6667",
    "semantic_title": "enhancing semi-supervised domain adaptation via effective target labeling",
    "citation_count": 0,
    "authors": [
      "Jiujun He",
      "Bin Liu",
      "Guosheng Yin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29131": {
    "title": "Generative Calibration of Inaccurate Annotation for Label Distribution Learning",
    "volume": "main",
    "abstract": "Label distribution learning (LDL) is an effective learning paradigm for handling label ambiguity. When applying LDL, it typically requires datasets annotated with label distributions. However, obtaining supervised data for LDL is a challenging task. Due to the randomness of label annotation, the annotator can produce inaccurate annotation results for the instance, affecting the accuracy and generalization ability of the LDL model. To address this problem, we propose a generative approach to calibrate the inaccurate annotation for LDL using variational inference techniques. Specifically, we assume that instances with similar features share latent similar label distributions. The feature vectors and label distributions are generated by Gaussian mixture and Dirichlet mixture, respectively. The relationship between them is established through a shared categorical variable, which effectively utilizes the label distribution of instances with similar features, and achieves a more accurate label distribution through the generative approach. Furthermore, we use a confusion matrix to model the factors that contribute to the inaccuracy during the annotation process, which captures the relationship between label distributions and inaccurate label distributions. Finally, the label distribution is used to calibrate the available information in the noisy dataset to obtain the ground-truth label distribution",
    "checked": true,
    "id": "d605b275f3e7911e61bb5cff06649d3e6a4740da",
    "semantic_title": "generative calibration of inaccurate annotation for label distribution learning",
    "citation_count": 1,
    "authors": [
      "Liang He",
      "Yunan Lu",
      "Weiwei Li",
      "Xiuyi Jia"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29132": {
    "title": "Exploring Channel-Aware Typical Features for Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "Detecting out-of-distribution (OOD) data is essential to ensure the reliability of machine learning models when deployed in real-world scenarios. Different from most previous test-time OOD detection methods that focus on designing OOD scores, we delve into the challenges in OOD detection from the perspective of typicality and regard the feature's high-probability region as the feature's typical set. However, the existing typical-feature-based OOD detection method implies an assumption: the proportion of typical feature sets for each channel is fixed. According to our experimental analysis, each channel contributes differently to OOD detection. Adopting a fixed proportion for all channels results in several channels losing too many typical features or incorporating too many abnormal features, resulting in low performance. Therefore, exploring the channel-aware typical features is crucial to better-separating ID and OOD data. Driven by this insight, we propose expLoring channel-Aware tyPical featureS (LAPS). Firstly, LAPS obtains the channel-aware typical set by calibrating the channel-level typical set with the global typical set from the mean and standard deviation. Then, LAPS rectifies the features into channel-aware typical sets to obtain channel-aware typical features. Finally, LAPS leverages the channel-aware typical features to calculate the energy score for OOD detection. Theoretical and visual analyses verify that LAPS achieves a better bias-variance trade-off. Experiments verify the effectiveness and generalization of LAPS under different architectures and OOD scores",
    "checked": true,
    "id": "755390c365c4a39445f73ed09fe673f2b823876d",
    "semantic_title": "exploring channel-aware typical features for out-of-distribution detection",
    "citation_count": 1,
    "authors": [
      "Rundong He",
      "Yue Yuan",
      "Zhongyi Han",
      "Fan Wang",
      "Wan Su",
      "Yilong Yin",
      "Tongliang Liu",
      "Yongshun Gong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29133": {
    "title": "Learning Only When It Matters: Cost-Aware Long-Tailed Classification",
    "volume": "main",
    "abstract": "Most current long-tailed classification approaches assume the cost-agnostic scenario, where the training distribution of classes is long-tailed while the testing distribution of classes is balanced. Meanwhile, the misclassification costs of all instances are the same. On the other hand, in many real-world applications, it is more proper to assume that the training and testing distributions of classes are the same, while the misclassification cost of tail-class instances is varied. In this work, we model such a scenario as cost-aware long-tailed classification, in which the identification of high-cost tail instances and focusing learning on them thereafter is essential. In consequence, we propose the learning strategy of augmenting new instances based on adaptive region partition in the feature space. We conduct theoretical analysis to show that under the assumption that the feature-space distance and the misclassification cost are correlated, the identification of high-cost tail instances can be realized by building region partitions with a low variance of risk within each region. The resulting AugARP approach could significantly outperform baseline approaches on both benchmark datasets and real-world product sales datasets",
    "checked": true,
    "id": "92462be2506c6ad05ca58c0fae0dd4a98b55a5ba",
    "semantic_title": "learning only when it matters: cost-aware long-tailed classification",
    "citation_count": 0,
    "authors": [
      "Yu-Cheng He",
      "Yao-Xiang Ding",
      "Han-Jia Ye",
      "Zhi-Hua Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29134": {
    "title": "SoundCount: Sound Counting from Raw Audio with Dyadic Decomposition Neural Network",
    "volume": "main",
    "abstract": "In this paper, we study an underexplored, yet important and challenging problem: counting the number of distinct sounds in raw audio characterized by a high degree of polyphonicity. We do so by systematically proposing a novel end-to-end trainable neural network~(which we call DyDecNet, consisting of a dyadic decomposition front-end and backbone network), and quantifying the difficulty level of counting depending on sound polyphonicity. The dyadic decomposition front-end progressively decomposes the raw waveform dyadically along the frequency axis to obtain time-frequency representation in multi-stage, coarse-to-fine manner. Each intermediate waveform convolved by a parent filter is further processed by a pair of child filters that evenly split the parent filter's carried frequency response, with the higher-half child filter encoding the detail and lower-half child filter encoding the approximation. We further introduce an energy gain normalization to normalize sound loudness variance and spectrum overlap, and apply it to each intermediate parent waveform before feeding it to the two child filters. To better quantify sound counting difficulty level, we further design three polyphony-aware metrics: polyphony ratio, max polyphony and mean polyphony. We test DyDecNet on various datasets to show its superiority, and we further show dyadic decomposition network can be used as a general front-end to tackle other acoustic tasks",
    "checked": true,
    "id": "7fffc4ccb166ee539dc77e7fe252defbefe881d3",
    "semantic_title": "soundcount: sound counting from raw audio with dyadic decomposition neural network",
    "citation_count": 0,
    "authors": [
      "Yuhang He",
      "Zhuangzhuang Dai",
      "Niki Trigoni",
      "Long Chen",
      "Andrew Markham"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29135": {
    "title": "Training-Free Quantum Architecture Search",
    "volume": "main",
    "abstract": "Variational quantum algorithm (VQA) derives advantages from its error resilience and high flexibility in quantum resource requirements, rendering it broadly applicable in the noisy intermediate-scale quantum era. As the performance of VQA highly relies on the structure of the parameterized quantum circuit, it is worthwhile to propose quantum architecture search (QAS) algorithms to automatically search for high-performance circuits. Nevertheless, existing QAS methods are time-consuming, requiring circuit training to assess circuit performance. This study pioneers training-free QAS by utilizing two training-free proxies to rank quantum circuits, in place of the expensive circuit training employed in conventional QAS. Taking into account the precision and computational overhead of the path-based and expressibility-based proxies, we devise a two-stage progressive training-free QAS (TF-QAS). Initially, directed acyclic graphs (DAGs) are employed for circuit representation, and a zero-cost proxy based on the number of paths in the DAG is designed to filter out a substantial portion of unpromising circuits. Subsequently, an expressibility-based proxy, finely reflecting circuit performance, is employed to identify high-performance circuits from the remaining candidates. These proxies evaluate circuit performance without circuit training, resulting in a remarkable reduction in computational cost compared to current training-based QAS methods. Simulations on three VQE tasks demonstrate that TF-QAS achieves a substantial enhancement of sampling efficiency ranging from 5 to 57 times compared to state-of-the-art QAS, while also being 6 to 17 times faster",
    "checked": true,
    "id": "5063c4ff2b54b3423bf2c260406144f8b8112d06",
    "semantic_title": "training-free quantum architecture search",
    "citation_count": 4,
    "authors": [
      "Zhimin He",
      "Maijie Deng",
      "Shenggen Zheng",
      "Lvzhou Li",
      "Haozhen Situ"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29136": {
    "title": "Imitate the Good and Avoid the Bad: An Incremental Approach to Safe Reinforcement Learning",
    "volume": "main",
    "abstract": "A popular framework for enforcing safe actions in Reinforcement Learning (RL) is Constrained RL, where trajectory based constraints on expected cost (or other cost measures) are employed to enforce safety and more importantly these constraints are enforced while maximizing expected reward. Most recent approaches for solving Constrained RL convert the trajectory based cost constraint into a surrogate problem that can be solved using minor modifications to RL methods. A key drawback with such approaches is an over or underestimation of the cost constraint at each state. Therefore, we provide an approach that does not modify the trajectory based cost constraint and instead imitates \"good\" trajectories and avoids \"bad\" trajectories generated from incrementally improving policies. We employ an oracle that utilizes a reward threshold (which is varied with learning) and the overall cost constraint to label trajectories as \"good\" or \"bad\". A key advantage of our approach is that we are able to work from any starting policy or set of trajectories and improve on it. In an exhaustive set of experiments, we demonstrate that our approach is able to outperform top benchmark approaches for solving Constrained RL problems, with respect to expected cost, CVaR cost, or even unknown cost constraints",
    "checked": true,
    "id": "c452ced559d49a35485fc25233618cf588892013",
    "semantic_title": "imitate the good and avoid the bad: an incremental approach to safe reinforcement learning",
    "citation_count": 3,
    "authors": [
      "Huy Hoang",
      "Tien Mai",
      "Pradeep Varakantham"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29137": {
    "title": "Few-Shot Learning via Repurposing Ensemble of Black-Box Models",
    "volume": "main",
    "abstract": "This paper investigates the problem of exploiting existing solution models of previous tasks to address a related target task with limited training data. Existing approaches addressing this problem often require access to the internal parameterization of the existing solution models and possibly their training data, which is not possible in many practical settings. To relax this requirement, We approach this problem from a new perspective of black-box re-purposing, which augments the target inputs and leverages their corresponding outputs generated by existing black-box APIs into a feature ensemble. We hypothesize that such feature ensemble can be learned to incorporate and encode relevant black-box knowledge into the feature representation of target data, which will compensate for their scarcity. This hypothesis is confirmed via the reported successes of our proposed black-box ensemble in solving multiple few-shot learning tasks derived from various benchmark datasets. All reported results show consistently that the set of heterogeneous black-box solutions of previous tasks can indeed be reused and combined effectively to solve a reasonably related target task without requiring access to a large training dataset. This is the first step towards enabling new possibilities to further supplement existing techniques in transfer or meta learning with black-box knowledge",
    "checked": true,
    "id": "39bead979df02b3cc4fef422b7aa078e6111e9a2",
    "semantic_title": "few-shot learning via repurposing ensemble of black-box models",
    "citation_count": 0,
    "authors": [
      "Minh Hoang",
      "Trong Nghia Hoang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29138": {
    "title": "Transitivity-Preserving Graph Representation Learning for Bridging Local Connectivity and Role-Based Similarity",
    "volume": "main",
    "abstract": "Graph representation learning (GRL) methods, such as graph neural networks and graph transformer models, have been successfully used to analyze graph-structured data, mainly focusing on node classification and link prediction tasks. However, the existing studies mostly only consider local connectivity while ignoring long-range connectivity and the roles of nodes. In this paper, we propose Unified Graph Transformer Networks (UGT) that effectively integrate local and global structural information into fixed-length vector representations. First, UGT learns local structure by identifying the local sub-structures and aggregating features of the k-hop neighborhoods of each node. Second, we construct virtual edges, bridging distant nodes with structural similarity to capture the long-range dependencies. Third, UGT learns unified representations through self-attention, encoding structural distance and p-step transition probability between node pairs. Furthermore, we propose a self-supervised learning task that effectively learns transition probability to fuse local and global structural features, which could then be transferred to other downstream tasks. Experimental results on real-world benchmark datasets over various downstream tasks showed that UGT significantly outperformed baselines that consist of state-of-the-art models. In addition, UGT reaches the third-order Weisfeiler-Lehman power to distinguish non-isomorphic graph pairs",
    "checked": true,
    "id": "9d9e1922d34c0824db39e0f55828bf0c4da52e75",
    "semantic_title": "transitivity-preserving graph representation learning for bridging local connectivity and role-based similarity",
    "citation_count": 0,
    "authors": [
      "Van Thuy Hoang",
      "O-Joun Lee"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29139": {
    "title": "Colored Noise in PPO: Improved Exploration and Performance through Correlated Action Sampling",
    "volume": "main",
    "abstract": "Proximal Policy Optimization (PPO), a popular on-policy deep reinforcement learning method, employs a stochastic policy for exploration. In this paper, we propose a colored noise-based stochastic policy variant of PPO. Previous research highlighted the importance of temporal correlation in action noise for effective exploration in off-policy reinforcement learning. Building on this, we investigate whether correlated noise can also enhance exploration in on-policy methods like PPO. We discovered that correlated noise for action selection improves learning performance and outperforms the currently popular uncorrelated white noise approach in on-policy methods. Unlike off-policy learning, where pink noise was found to be highly effective, we found that a colored noise, intermediate between white and pink, performed best for on-policy learning in PPO. We examined the impact of varying the amount of data collected for each update by modifying the number of parallel simulation environments for data collection and observed that with a larger number of parallel environments, more strongly correlated noise is beneficial. Due to the significant impact and ease of implementation, we recommend switching to correlated noise as the default noise source in PPO",
    "checked": true,
    "id": "e658cc60b922ff23d17e4864f4c38174f153a72b",
    "semantic_title": "colored noise in ppo: improved exploration and performance through correlated action sampling",
    "citation_count": 0,
    "authors": [
      "Jakob Hollenstein",
      "Georg Martius",
      "Justus Piater"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29140": {
    "title": "Foreseeing Reconstruction Quality of Gradient Inversion: An Optimization Perspective",
    "volume": "main",
    "abstract": "Gradient inversion attacks can leak data privacy when clients share weight updates with the server in federated learning (FL). Existing studies mainly use L2 or cosine distance as the loss function for gradient matching in the attack. Our empirical investigation shows that the vulnerability ranking varies with the loss function used. Gradient norm, which is commonly used as a vulnerability proxy for gradient inversion attack, cannot explain this as it remains constant regardless of the loss function for gradient matching. In this paper, we propose a loss-aware vulnerability proxy (LAVP) for the first time. LAVP refers to either the maximum or minimum eigenvalue of the Hessian with respect to gradient matching loss at ground truth. This suggestion is based on our theoretical findings regarding the local optimization of the gradient inversion in proximity to the ground truth, which corresponds to the worst case attack scenario. We demonstrate the effectiveness of LAVP on various architectures and datasets, showing its consistent superiority over the gradient norm in capturing sample vulnerabilities. The performance of each proxy is measured in terms of Spearman's rank correlation with respect to several similarity scores. This work will contribute to enhancing FL security against any potential loss functions beyond L2 or cosine distance in the future",
    "checked": true,
    "id": "91392efa48243d62bdc7202f3d8c9546cc87a138",
    "semantic_title": "foreseeing reconstruction quality of gradient inversion: an optimization perspective",
    "citation_count": 0,
    "authors": [
      "Hyeong Gwon Hong",
      "Yooshin Cho",
      "Hanbyel Cho",
      "Jaesung Ahn",
      "Junmo Kim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29141": {
    "title": "Complete Neural Networks for Complete Euclidean Graphs",
    "volume": "main",
    "abstract": "Neural networks for point clouds, which respect their natural invariance to permutation and rigid motion, have enjoyed recent success in modeling geometric phenomena, from molecular dynamics to recommender systems. Yet, to date, no architecture with polynomial complexity is known to be complete, that is, able to distinguish between any pair of non-isomorphic point clouds. We fill this theoretical gap by showing that point clouds can be completely determined, up to permutation and rigid motion, by applying the 3-WL graph isomorphism test to the point cloud's centralized Gram matrix. Moreover, we formulate an Euclidean variant of the 2-WL test and show that it is also sufficient to achieve completeness. We then show how our complete Euclidean WL tests can be simulated by an Euclidean graph neural network of moderate size and demonstrate their separation capability on highly symmetrical point clouds",
    "checked": true,
    "id": "025353f68f43cc338877f9439fcaeb349f5c1c30",
    "semantic_title": "complete neural networks for complete euclidean graphs",
    "citation_count": 8,
    "authors": [
      "Snir Hordan",
      "Tal Amir",
      "Steven J. Gortler",
      "Nadav Dym"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29142": {
    "title": "Structured Probabilistic Coding",
    "volume": "main",
    "abstract": "This paper presents a new supervised representation learning framework, namely structured probabilistic coding (SPC), to learn compact and informative representations from input related to the target task. SPC is an encoder-only probabilistic coding technology with a structured regularization from the target space. It can enhance the generalization ability of pre-trained language models for better language understanding. Specifically, our probabilistic coding simultaneously performs information encoding and task prediction in one module to more fully utilize the effective information from input data. It uses variational inference in the output space to reduce randomness and uncertainty. Besides, to better control the learning process of probabilistic representations, a structured regularization is proposed to promote uniformity across classes in the latent space. With the regularization term, SPC can preserve the Gaussian structure of the latent code and achieve better coverage of the hidden space with class uniformly. Experimental results on 12 natural language understanding tasks demonstrate that our SPC effectively improves the performance of pre-trained language models for classification and regression. Extensive experiments show that SPC can enhance the generalization capability, robustness to label noise, and clustering quality of output representations",
    "checked": true,
    "id": "1d935104d8be3dfe749578dc20213679fe6c59e5",
    "semantic_title": "structured probabilistic coding",
    "citation_count": 0,
    "authors": [
      "Dou Hu",
      "Lingwei Wei",
      "Yaxin Liu",
      "Wei Zhou",
      "Songlin Hu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29143": {
    "title": "A Sequentially Fair Mechanism for Multiple Sensitive Attributes",
    "volume": "main",
    "abstract": "In the standard use case of Algorithmic Fairness, the goal is to eliminate the relationship between a sensitive variable and a corresponding score. Throughout recent years, the scientific community has developed a host of definitions and tools to solve this task, which work well in many practical applications. However, the applicability and effectivity of these tools and definitions becomes less straightfoward in the case of multiple sensitive attributes. To tackle this issue, we propose a sequential framework, which allows to progressively achieve fairness across a set of sensitive features. We accomplish this by leveraging multi-marginal Wasserstein barycenters, which extends the standard notion of Strong Demographic Parity to the case with multiple sensitive characteristics. This method also provides a closed-form solution for the optimal, sequentially fair predictor, permitting a clear interpretation of inter-sensitive feature correlations. Our approach seamlessly extends to approximate fairness, enveloping a framework accommodating the trade-off between risk and unfairness. This extension permits a targeted prioritization of fairness improvements for a specific attribute within a set of sensitive attributes, allowing for a case specific adaptation. A data-driven estimation procedure for the derived solution is developed, and comprehensive numerical experiments are conducted on both synthetic and real datasets. Our empirical findings decisively underscore the practical efficacy of our post-processing approach in fostering fair decision-making",
    "checked": true,
    "id": "3d9bdbb91e6be20337654e65e237b3d6552f782b",
    "semantic_title": "a sequentially fair mechanism for multiple sensitive attributes",
    "citation_count": 4,
    "authors": [
      "Francois Hu",
      "Philipp Ratz",
      "Arthur Charpentier"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29144": {
    "title": "Relax Image-Specific Prompt Requirement in SAM: A Single Generic Prompt for Segmenting Camouflaged Objects",
    "volume": "main",
    "abstract": "Camouflaged object detection (COD) approaches heavily rely on pixel-level annotated datasets. Weakly-supervised COD (WSCOD) approaches use sparse annotations like scribbles or points to reduce annotation efforts, but this can lead to decreased accuracy. The Segment Anything Model (SAM) shows remarkable segmentation ability with sparse prompts like points. However, manual prompt is not always feasible, as it may not be accessible in real-world application. Additionally, it only provides localization information instead of semantic one, which can intrinsically cause ambiguity in interpreting targets. In this work, we aim to eliminate the need for manual prompt. The key idea is to employ Cross-modal Chains of Thought Prompting (CCTP) to reason visual prompts using the semantic information given by a generic text prompt. To that end, we introduce a test-time instance-wise adaptation mechanism called Generalizable SAM (GenSAM) to automatically generate and optimize visual prompts from the generic task prompt for WSCOD. In particular, CCTP maps a single generic text prompt onto image-specific consensus foreground and background heatmaps using vision-language models, acquiring reliable visual prompts. Moreover, to test-time adapt the visual prompts, we further propose Progressive Mask Generation (PMG) to iteratively reweight the input image, guiding the model to focus on the targeted region in a coarse-to-fine manner. Crucially, all network parameters are fixed, avoiding the need for additional training. Experiments on three benchmarks demonstrate that GenSAM outperforms point supervision approaches and achieves comparable results to scribble supervision ones, solely relying on general task descriptions. Our codes is in https://github.com/jyLin8100/GenSAM",
    "checked": true,
    "id": "cad3c30b2116a4306cc0b605fdd5d29345e26d00",
    "semantic_title": "relax image-specific prompt requirement in sam: a single generic prompt for segmenting camouflaged objects",
    "citation_count": 3,
    "authors": [
      "Jian Hu",
      "Jiayi Lin",
      "Shaogang Gong",
      "Weitong Cai"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29145": {
    "title": "Sequential Fusion Based Multi-Granularity Consistency for Space-Time Transformer Tracking",
    "volume": "main",
    "abstract": "Regarded as a template-matching task for a long time, visual object tracking has witnessed significant progress in space-wise exploration. However, since tracking is performed on videos with substantial time-wise information, it is important to simultaneously mine the temporal contexts which have not yet been deeply explored. Previous supervised works mostly consider template reform as the breakthrough point, but they are often limited by additional computational burdens or the quality of chosen templates. To address this issue, we propose a Space-Time Consistent Transformer Tracker (STCFormer), which uses a sequential fusion framework with multi-granularity consistency constraints to learn spatiotemporal context information. We design a sequential fusion framework that recombines template and search images based on tracking results from chronological frames, fusing updated tracking states in training. To further overcome the over-reliance on the fixed template without increasing computational complexity, we design three space-time consistent constraints: Label Consistency Loss (LCL) for label-level consistency, Attention Consistency Loss (ACL) for patch-level ROI consistency, and Semantic Consistency Loss (SCL) for feature-level semantic consistency. Specifically, in ACL and SCL, the label information is used to constrain the attention and feature consistency of the target and the background, respectively, to avoid mutual interference. Extensive experiments have shown that our STCFormer outperforms many of the best-performing trackers on several popular benchmarks",
    "checked": true,
    "id": "e1ea34a65a4b420bfa6242b37ac18c17c5a9d3f1",
    "semantic_title": "sequential fusion based multi-granularity consistency for space-time transformer tracking",
    "citation_count": 0,
    "authors": [
      "Kun Hu",
      "Wenjing Yang",
      "Wanrong Huang",
      "Xianchen Zhou",
      "Mingyu Cao",
      "Jing Ren",
      "Huibin Tan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29146": {
    "title": "FedMut: Generalized Federated Learning via Stochastic Mutation",
    "volume": "main",
    "abstract": "Although Federated Learning (FL) enables collaborative model training without sharing the raw data of clients, it encounters low-performance problems caused by various heterogeneous scenarios. Due to the limitation of dispatching the same global model to clients for local training, traditional Federated Average (FedAvg)-based FL models face the problem of easily getting stuck into a sharp solution, which results in training a low-performance global model. To address this problem, this paper presents a novel FL approach named FedMut, which mutates the global model according to the gradient change to generate several intermediate models for the next round of training. Each intermediate model will be dispatched to a client for local training. Eventually, the global model converges into a flat area within the range of mutated models and has a well-generalization compared with the global model trained by FedAvg. Experimental results on well-known datasets demonstrate the effectiveness of our FedMut approach in various data heterogeneity scenarios",
    "checked": true,
    "id": "057bc3b18e8a3c089557fddacd50a5e419d5ba43",
    "semantic_title": "fedmut: generalized federated learning via stochastic mutation",
    "citation_count": 1,
    "authors": [
      "Ming Hu",
      "Yue Cao",
      "Anran Li",
      "Zhiming Li",
      "Chengwei Liu",
      "Tianlin Li",
      "Mingsong Chen",
      "Yang Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29147": {
    "title": "PrefAce: Face-Centric Pretraining with Self-Structure Aware Distillation",
    "volume": "main",
    "abstract": "Video-based facial analysis is important for autonomous agents to understand human expressions and sentiments. However, limited labeled data is available to learn effective facial representations. This paper proposes a novel self-supervised face-centric pretraining framework, called PrefAce, which learns transferable video facial representation without labels. The self-supervised learning is performed with an effective landmark-guided global-local tube distillation. Meanwhile, a novel instance-wise update FaceFeat Cache is built to enforce more discriminative and diverse representations for downstream tasks. Extensive experiments demonstrate that the proposed framework learns universal instance-aware facial representations with fine-grained landmark details from videos. The point is that it can transfer across various facial analysis tasks, e.g., Facial Attribute Recognition (FAR), Facial Expression Recognition (FER), DeepFake Detection (DFD), and Lip Synchronization (LS). Our framework also outperforms the state-of-the-art on various downstream tasks, even in low data regimes. Code is available at https://github.com/siyuan-h/PrefAce",
    "checked": true,
    "id": "08305362b5b9aa61b5ee26995c4e11c01363d7b2",
    "semantic_title": "preface: face-centric pretraining with self-structure aware distillation",
    "citation_count": 0,
    "authors": [
      "Siyuan Hu",
      "Zheng Wang",
      "Peng Hu",
      "Xi Peng",
      "Jie Wu",
      "Hongyuan Zhu",
      "Yew Soon Ong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29148": {
    "title": "PA2D-MORL: Pareto Ascent Directional Decomposition Based Multi-Objective Reinforcement Learning",
    "volume": "main",
    "abstract": "Multi-objective reinforcement learning (MORL) provides an effective solution for decision-making problems involving conflicting objectives. However, achieving high-quality approximations to the Pareto policy set remains challenging, especially in complex tasks with continuous or high-dimensional state-action space. In this paper, we propose the Pareto Ascent Directional Decomposition based Multi-Objective Reinforcement Learning (PA2D-MORL) method, which constructs an efficient scheme for multi-objective problem decomposition and policy improvement, leading to a superior approximation of Pareto policy set. The proposed method leverages Pareto ascent direction to select the scalarization weights and computes the multi-objective policy gradient, which determines the policy optimization direction and ensures joint improvement on all objectives. Meanwhile, multiple policies are selectively optimized under an evolutionary framework to approximate the Pareto frontier from different directions. Additionally, a Pareto adaptive fine-tuning approach is applied to enhance the density and spread of the Pareto frontier approximation. Experiments on various multi-objective robot control tasks show that the proposed method clearly outperforms the current state-of-the-art algorithm in terms of both quality and stability of the outcomes",
    "checked": true,
    "id": "da70a1eed85873535f33a9c15c263b923119876f",
    "semantic_title": "pa2d-morl: pareto ascent directional decomposition based multi-objective reinforcement learning",
    "citation_count": 0,
    "authors": [
      "Tianmeng Hu",
      "Biao Luo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29149": {
    "title": "Spotting the Unseen: Reciprocal Consensus Network Guided by Visual Archetypes",
    "volume": "main",
    "abstract": "Humans often require only a few visual archetypes to spot novel objects. Based on this observation, we present a strategy rooted in ``spotting the unseen\" by establishing dense correspondences between potential query image regions and a visual archetype, and we propose the Consensus Network (CoNet). Our method leverages relational patterns intra and inter images via Auto-Correlation Representation (ACR) and Mutual-Correlation Representation (MCR). Within each image, the ACR module is capable of encoding both local self-similarity and global context simultaneously. Between the query and support images, the MCR module computes the cross-correlation across two image representations and introduces a reciprocal consistency constraint, which can incorporate to exclude outliers and enhance model robustness. To overcome the challenges of low-resource training data, particularly in one-shot learning scenarios, we incorporate an adaptive margin strategy to better handle diverse instances. The experimental results indicate the effectiveness of the proposed method across diverse domains such as object detection in natural scenes, and text spotting in both historical manuscripts and natural scenes, which demonstrates its sparkling generalization ability. Our code is available at: https://github.com/infinite-hwb/conet",
    "checked": true,
    "id": "8fe15a260ec147964bee7f3601bb813ab107b975",
    "semantic_title": "spotting the unseen: reciprocal consensus network guided by visual archetypes",
    "citation_count": 0,
    "authors": [
      "Wenbo Hu",
      "Hongjian Zhan",
      "Xinchen Ma",
      "Yue Lu",
      "Ching Y. Suen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29150": {
    "title": "Terrain Diffusion Network: Climatic-Aware Terrain Generation with Geological Sketch Guidance",
    "volume": "main",
    "abstract": "Sketch-based terrain generation seeks to create realistic landscapes for virtual environments in various applications such as computer games, animation and virtual reality. Recently, deep learning based terrain generation has emerged, notably the ones based on generative adversarial networks (GAN). However, these methods often struggle to fulfill the requirements of flexible user control and maintain generative diversity for realistic terrain. Therefore, we propose a novel diffusion-based method, namely terrain diffusion network (TDN), which actively incorporates user guidance for enhanced controllability, taking into account terrain features like rivers, ridges, basins, and peaks. Instead of adhering to a conventional monolithic denoising process, which often compromises the fidelity of terrain details or the alignment with user control, a multi-level denoising scheme is proposed to generate more realistic terrains by taking into account fine-grained details, particularly those related to climatic patterns influenced by erosion and tectonic activities. Specifically, three terrain synthesisers are designed for structural, intermediate, and fine-grained level denoising purposes, which allow each synthesiser concentrate on a distinct terrain aspect. Moreover, to maximise the efficiency of our TDN, we further introduce terrain and sketch latent spaces for the synthesizers with pre-trained terrain autoencoders. Comprehensive experiments on a new dataset constructed from NASA Topology Images clearly demonstrate the effectiveness of our proposed method, achieving the state-of-the-art performance. Our code is available at https://github.com/TDNResearch/TDN",
    "checked": true,
    "id": "994c32544dae13b9a2801f50a94978b291d5a3d4",
    "semantic_title": "terrain diffusion network: climatic-aware terrain generation with geological sketch guidance",
    "citation_count": 0,
    "authors": [
      "Zexin Hu",
      "Kun Hu",
      "Clinton Mo",
      "Lei Pan",
      "Zhiyong Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29151": {
    "title": "Energy Efficient Streaming Time Series Classification with Attentive Power Iteration",
    "volume": "main",
    "abstract": "Efficiently processing time series data streams in real-time on resource-constrained devices offers significant advantages in terms of enhanced computational energy efficiency and reduced time-related risks. We introduce an innovative streaming time series classification network that utilizes attentive power iteration, enabling real-time processing on resource-constrained devices. Our model continuously updates a compact representation of the entire time series, enhancing classification accuracy while conserving energy and processing time. Notably, it excels in streaming scenarios without requiring complete time series access, enabling swift decisions. Experimental results show that our approach excels in classification accuracy and energy efficiency, with over 70% less consumption and threefold faster task completion than benchmarks. This work advances real-time responsiveness, energy conservation, and operational effectiveness for constrained devices, contributing to optimizing various applications",
    "checked": true,
    "id": "c71ba9ae0a734d8120db657c9f5804e22df447cc",
    "semantic_title": "energy efficient streaming time series classification with attentive power iteration",
    "citation_count": 0,
    "authors": [
      "Hao Huang",
      "Tapan Shah",
      "Scott Evans",
      "Shinjae Yoo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29152": {
    "title": "SEC: More Accurate Clustering Algorithm via Structural Entropy",
    "volume": "main",
    "abstract": "As one of the most popular machine learning tools in the field of unsupervised learning, clustering has been widely used in various practical applications. While numerous methods have been proposed for clustering, a commonly encountered issue is that the existing clustering methods rely heavily on local neighborhood information during the optimization process, which leads to suboptimal performance on real-world datasets. Besides, most existing clustering methods use Euclidean distances or densities to measure the similarity between data points. This could constrain the effectiveness of the algorithms for handling datasets with irregular patterns. Thus, a key challenge is how to effectively capture the global structural information in clustering instances to improve the clustering quality. In this paper, we propose a new clustering algorithm, called SEC. This algorithm uses the global structural information extracted from an encoding tree to guide the clustering optimization process. Based on the relation between data points in the instance, a sparse graph of the clustering instance can be constructed. By leveraging the sparse graph constructed, we propose an iterative encoding tree method, where hierarchical abstractions of the encoding tree are iteratively extracted as new clustering features to obtain better clustering results. To avoid the influence of easily misclustered data points located on the boundaries of the clustering partitions, which we call \"fringe points\", we propose an iterative pre-deletion and reassignment technique such that the algorithm can delete and reassign the \"fringe points\" to obtain more resilient and precise clustering results. Empirical experiments on both synthetic and real-world datasets demonstrate that our proposed algorithm outperforms state-of-the-art clustering methods and achieves better clustering performances. On average, the clustering accuracy (ACC) is increased by 1.7% and the normalized mutual information (NMI) by 7.9% compared with the current state-of-the-art (SOTA) algorithm on synthetic datasets. On real-world datasets, our method outperforms other clustering methods with an average increase of 12.3% in ACC and 5.2% in NMI, respectively",
    "checked": true,
    "id": "33161ebb66fe12c10d4f5b3e13c537d6c0dd48cf",
    "semantic_title": "sec: more accurate clustering algorithm via structural entropy",
    "citation_count": 0,
    "authors": [
      "Junyu Huang",
      "Qilong Feng",
      "Jiahui Wang",
      "Ziyun Huang",
      "Jinhui Xu",
      "Jianxin Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29153": {
    "title": "eTag: Class-Incremental Learning via Embedding Distillation and Task-Oriented Generation",
    "volume": "main",
    "abstract": "Class incremental learning (CIL) aims to solve the notorious forgetting problem, which refers to the fact that once the network is updated on a new task, its performance on previously-learned tasks degenerates catastrophically. Most successful CIL methods store exemplars (samples of learned tasks) to train a feature extractor incrementally, or store prototypes (features of learned tasks) to estimate the incremental feature distribution. However, the stored exemplars would violate the data privacy concerns, while the fixed prototypes might not reasonably be consistent with the incremental feature distribution, hindering the exploration of real-world CIL applications. In this paper, we propose a data-free CIL method with embedding distillation and Task-oriented generation (eTag), which requires neither exemplar nor prototype. Embedding distillation prevents the feature extractor from forgetting by distilling the outputs from the networks' intermediate blocks. Task-oriented generation enables a lightweight generator to produce dynamic features, fitting the needs of the top incremental classifier. Experimental results confirm that the proposed eTag considerably outperforms state-of-the-art methods on several benchmark datasets",
    "checked": true,
    "id": "7e8eb02e342dc53eceea470e25741ebf5dd92354",
    "semantic_title": "etag: class-incremental learning via embedding distillation and task-oriented generation",
    "citation_count": 2,
    "authors": [
      "Libo Huang",
      "Yan Zeng",
      "Chuanguang Yang",
      "Zhulin An",
      "Boyu Diao",
      "Yongjun Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29154": {
    "title": "PPO-Clip Attains Global Optimality: Towards Deeper Understandings of Clipping",
    "volume": "main",
    "abstract": "Proximal Policy Optimization algorithm employing a clipped surrogate objective (PPO-Clip) is a prominent exemplar of the policy optimization methods. However, despite its remarkable empirical success, PPO-Clip lacks theoretical substantiation to date. In this paper, we contribute to the field by establishing the first global convergence results of a PPO-Clip variant in both tabular and neural function approximation settings. Our findings highlight the O(1/√T ) min-iterate convergence rate specifically in the context of neural function approximation. We tackle the inherent challenges in analyzing PPO-Clip through three central concepts: (i) We introduce a generalized version of the PPO-Clip objective, illuminated by its connection with the hinge loss. (ii) Employing entropic mirror descent, we establish asymptotic convergence for tabular PPO-Clip with direct policy parameterization. (iii) Inspired by the tabular analysis, we streamline convergence analysis by introducing a two-step policy improvement approach. This decouples policy search from complex neural policy parameterization using a regression-based update scheme. Furthermore, we gain deeper insights into the efficacy of PPO-Clip by interpreting these generalized objectives. Our theoretical findings also mark the first characterization of the influence of the clipping mechanism on PPO-Clip convergence. Importantly, the clipping range affects only the pre-constant of the convergence rate",
    "checked": true,
    "id": "daa625386ce9aa6039556d77cb28942bb17898e4",
    "semantic_title": "ppo-clip attains global optimality: towards deeper understandings of clipping",
    "citation_count": 1,
    "authors": [
      "Nai-Chieh Huang",
      "Ping-Chun Hsieh",
      "Kuo-Hao Ho",
      "I-Chen Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29155": {
    "title": "HDMixer: Hierarchical Dependency with Extendable Patch for Multivariate Time Series Forecasting",
    "volume": "main",
    "abstract": "Multivariate time series (MTS) prediction has been widely adopted in various scenarios. Recently, some methods have employed patching to enhance local semantics and improve model performance. However, length-fixed patch are prone to losing temporal boundary information, such as complete peaks and periods. Moreover, existing methods mainly focus on modeling long-term dependencies across patches, while paying little attention to other dimensions (e.g., short-term dependencies within patches and complex interactions among cross-variavle patches). To address these challenges, we propose a pure MLP-based HDMixer, aiming to acquire patches with richer semantic information and efficiently modeling hierarchical interactions. Specifically, we design a Length-Extendable Patcher (LEP) tailored to MTS, which enriches the boundary information of patches and alleviates semantic incoherence in series. Subsequently, we devise a Hierarchical Dependency Explorer (HDE) based on pure MLPs. This explorer effectively models short-term dependencies within patches, long-term dependencies across patches, and complex interactions among variables. Extensive experiments on 9 real-world datasets demonstrate the superiority of our approach. The code is available at https://github.com/hqh0728/HDMixer",
    "checked": true,
    "id": "c3bc5903647aef09c31a9a0866d59989e3f874eb",
    "semantic_title": "hdmixer: hierarchical dependency with extendable patch for multivariate time series forecasting",
    "citation_count": 2,
    "authors": [
      "Qihe Huang",
      "Lei Shen",
      "Ruixin Zhang",
      "Jiahuan Cheng",
      "Shouhong Ding",
      "Zhengyang Zhou",
      "Yang Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29156": {
    "title": "Measuring Task Similarity and Its Implication in Fine-Tuning Graph Neural Networks",
    "volume": "main",
    "abstract": "The paradigm of pre-training and fine-tuning graph neural networks has attracted wide research attention. In previous studies, the pre-trained models are viewed as universally versatile, and applied for a diverse range of downstream tasks. In many situations, however, this practice results in limited or even negative transfer. This paper, for the first time, emphasizes the specific application scope of graph pre-trained models: not all downstream tasks can effectively benefit from a graph pre-trained model. In light of this, we introduce the measure task consistency to quantify the similarity between graph pre-training and downstream tasks. This measure assesses the extent to which downstream tasks can benefit from specific pre-training tasks. Moreover, a novel fine-tuning strategy, Bridge-Tune, is proposed to further diminish the impact of the difference between pre-training and downstream tasks. The key innovation in Bridge-Tune is an intermediate step that bridges pre-training and downstream tasks. This step takes into account the task differences and further refines the pre-trained model. The superiority of the presented fine-tuning strategy is validated via numerous experiments with different pre-trained models and downstream tasks",
    "checked": true,
    "id": "7bda10706047e154e22259c4b20d70240296963e",
    "semantic_title": "measuring task similarity and its implication in fine-tuning graph neural networks",
    "citation_count": 1,
    "authors": [
      "Renhong Huang",
      "Jiarong Xu",
      "Xin Jiang",
      "Chenglu Pan",
      "Zhiming Yang",
      "Chunping Wang",
      "Yang Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29157": {
    "title": "Factorized Explainer for Graph Neural Networks",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs) have received increasing attention due to their ability to learn from graph-structured data. To open the black-box of these deep learning models, post-hoc instance-level explanation methods have been proposed to understand GNN predictions. These methods seek to discover substructures that explain the prediction behavior of a trained GNN. In this paper, we show analytically that for a large class of explanation tasks, conventional approaches, which are based on the principle of graph information bottleneck (GIB), admit trivial solutions that do not align with the notion of explainability. Instead, we argue that a modified GIB principle may be used to avoid the aforementioned trivial solutions. We further introduce a novel factorized explanation model with theoretical performance guarantees. The modified GIB is used to analyze the structural properties of the proposed factorized explainer. We conduct extensive experiments on both synthetic and real-world datasets to validate the effectiveness of our proposed factorized explainer",
    "checked": true,
    "id": "95e4bd5df8d573c4e95095f6fa7526e45afa2ea2",
    "semantic_title": "factorized explainer for graph neural networks",
    "citation_count": 4,
    "authors": [
      "Rundong Huang",
      "Farhad Shirani",
      "Dongsheng Luo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29158": {
    "title": "Stochastic Bayesian Optimization with Unknown Continuous Context Distribution via Kernel Density Estimation",
    "volume": "main",
    "abstract": "Bayesian optimization (BO) is a sample-efficient method and has been widely used for optimizing expensive black-box functions. Recently, there has been a considerable interest in BO literature in optimizing functions that are affected by context variable in the environment, which is uncontrollable by decision makers. In this paper, we focus on the optimization of functions' expectations over continuous context variable, subject to an unknown distribution. To address this problem, we propose two algorithms that employ kernel density estimation to learn the probability density function (PDF) of continuous context variable online. The first algorithm is simpler, which directly optimizes the expectation under the estimated PDF. Considering that the estimated PDF may have high estimation error when the true distribution is complicated, we further propose the second algorithm that optimizes the distributionally robust objective. Theoretical results demonstrate that both algorithms have sub-linear Bayesian cumulative regret on the expectation objective. Furthermore, we conduct numerical experiments to empirically demonstrate the effectiveness of our algorithms",
    "checked": true,
    "id": "a46be2d3e4d845c6ee751432c56dedcc5b3b5369",
    "semantic_title": "stochastic bayesian optimization with unknown continuous context distribution via kernel density estimation",
    "citation_count": 1,
    "authors": [
      "Xiaobin Huang",
      "Lei Song",
      "Ke Xue",
      "Chao Qian"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29159": {
    "title": "One Step Learning, One Step Review",
    "volume": "main",
    "abstract": "Visual fine-tuning has garnered significant attention with the rise of pre-trained vision models. The current prevailing method, full fine-tuning, suffers from the issue of knowledge forgetting as it focuses solely on fitting the downstream training set. In this paper, we propose a novel weight rollback-based fine-tuning method called OLOR (One step Learning, One step Review). OLOR combines fine-tuning with optimizers, incorporating a weight rollback term into the weight update term at each step. This ensures consistency in the weight range of upstream and downstream models, effectively mitigating knowledge forgetting and enhancing fine-tuning performance. In addition, a layer-wise penalty is presented to employ penalty decay and the diversified decay rate to adjust the weight rollback levels of layers for adapting varying downstream tasks. Through extensive experiments on various tasks such as image classification, object detection, semantic segmentation, and instance segmentation, we demonstrate the general applicability and state-of-the-art performance of our proposed OLOR. Code is available at https://github.com/rainbow-xiao/OLOR-AAAI-2024",
    "checked": true,
    "id": "d7c068343efe8d23bb285c0b3df4e60e926455d8",
    "semantic_title": "one step learning, one step review",
    "citation_count": 0,
    "authors": [
      "Xiaolong Huang",
      "Qiankun Li",
      "Xueran Li",
      "Xuesong Gao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29160": {
    "title": "Higher-Order Graph Convolutional Network with Flower-Petals Laplacians on Simplicial Complexes",
    "volume": "main",
    "abstract": "Despite the recent successes of vanilla Graph Neural Networks (GNNs) on various tasks, their foundation on pairwise networks inherently limits their capacity to discern latent higher-order interactions in complex systems. To bridge this capability gap, we propose a novel approach exploiting the rich mathematical theory of simplicial complexes (SCs) - a robust tool for modeling higher-order interactions. Current SC-based GNNs are burdened by high complexity and rigidity, and quantifying higher-order interaction strengths remains challenging. Innovatively, we present a higher-order Flower-Petals (FP) model, incorporating FP Laplacians into SCs. Further, we introduce a Higher-order Graph Convolutional Network (HiGCN) grounded in FP Laplacians, capable of discerning intrinsic features across varying topological scales. By employing learnable graph filters, a parameter group within each FP Laplacian domain, we can identify diverse patterns where the filters' weights serve as a quantifiable measure of higher-order interaction strengths. The theoretical underpinnings of HiGCN's advanced expressiveness are rigorously demonstrated. Additionally, our empirical investigations reveal that the proposed model accomplishes state-of-the-art performance on a range of graph tasks and provides a scalable and flexible solution to explore higher-order interactions in graphs. Codes and datasets are available at https://github.com/Yiminghh/HiGCN",
    "checked": true,
    "id": "c4c8bcedf9abfa8ebc7c9ad8eed6204e7a2d1481",
    "semantic_title": "higher-order graph convolutional network with flower-petals laplacians on simplicial complexes",
    "citation_count": 8,
    "authors": [
      "Yiming Huang",
      "Yujie Zeng",
      "Qiang Wu",
      "Linyuan Lü"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29161": {
    "title": "Protein 3D Graph Structure Learning for Robust Structure-Based Protein Property Prediction",
    "volume": "main",
    "abstract": "Protein structure-based property prediction has emerged as a promising approach for various biological tasks, such as protein function prediction and sub-cellular location estimation. The existing methods highly rely on experimental protein structure data and fail in scenarios where these data are unavailable. Predicted protein structures from AI tools (e.g., AlphaFold2) were utilized as alternatives. However, we observed that current practices, which simply employ accurately predicted structures during inference, suffer from notable degradation in prediction accuracy. While similar phenomena have been extensively studied in general fields (e.g., Computer Vision) as model robustness, their impact on protein property prediction remains unexplored. In this paper, we first investigate the reason behind the performance decrease when utilizing predicted structures, attributing it to the structure embedding bias from the perspective of structure representation learning. To study this problem, we identify a Protein 3D Graph Structure Learning Problem for Robust Protein Property Prediction (PGSL-RP3), collect benchmark datasets, and present a protein Structure embedding Alignment Optimization framework (SAO) to mitigate the problem of structure embedding bias between the predicted and experimental protein structures. Extensive experiments have shown that our framework is model-agnostic and effective in improving the property prediction of both predicted structures and experimental structures",
    "checked": true,
    "id": "25900d78c9f0e06a44662892661b28619962bf98",
    "semantic_title": "protein 3d graph structure learning for robust structure-based protein property prediction",
    "citation_count": 7,
    "authors": [
      "Yufei Huang",
      "Siyuan Li",
      "Lirong Wu",
      "Jin Su",
      "Haitao Lin",
      "Odin Zhang",
      "Zihan Liu",
      "Zhangyang Gao",
      "Jiangbin Zheng",
      "Stan Z. Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29162": {
    "title": "Binding-Adaptive Diffusion Models for Structure-Based Drug Design",
    "volume": "main",
    "abstract": "Structure-based drug design (SBDD) aims to generate 3D ligand molecules that bind to specific protein targets. Existing 3D deep generative models including diffusion models have shown great promise for SBDD. However, it is complex to capture the essential protein-ligand interactions exactly in 3D space for molecular generation. To address this problem, we propose a novel framework, namely Binding-Adaptive Diffusion Models (BindDM). In BindDM, we adaptively extract subcomplex, the essential part of binding sites responsible for protein-ligand interactions. Then the selected protein-ligand subcomplex is processed with SE(3)-equivariant neural networks, and transmitted back to each atom of the complex for augmenting the target-aware 3D molecule diffusion generation with binding interaction information. We iterate this hierarchical complex-subcomplex process with cross-hierarchy interaction node for adequately fusing global binding context between the complex and its corresponding subcomplex. Empirical studies on the CrossDocked2020 dataset show BindDM can generate molecules with more realistic 3D structures and higher binding affinities towards the protein targets, with up to -5.92 Avg. Vina Score, while maintaining proper molecular properties. Our code is available at https://github.com/YangLing0818/BindDM",
    "checked": true,
    "id": "9989ae61660bd8e2ea95d144ca8768cfa734dc43",
    "semantic_title": "binding-adaptive diffusion models for structure-based drug design",
    "citation_count": 3,
    "authors": [
      "Zhilin Huang",
      "Ling Yang",
      "Zaixi Zhang",
      "Xiangxin Zhou",
      "Yu Bao",
      "Xiawu Zheng",
      "Yuwei Yang",
      "Yu Wang",
      "Wenming Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29163": {
    "title": "Optimal Survival Trees: A Dynamic Programming Approach",
    "volume": "main",
    "abstract": "Survival analysis studies and predicts the time of death, or other singular unrepeated events, based on historical data, while the true time of death for some instances is unknown. Survival trees enable the discovery of complex nonlinear relations in a compact human comprehensible model, by recursively splitting the population and predicting a distinct survival distribution in each leaf node. We use dynamic programming to provide the first survival tree method with optimality guarantees, enabling the assessment of the optimality gap of heuristics. We improve the scalability of our method through a special algorithm for computing trees up to depth two. The experiments show that our method's run time even outperforms some heuristics for realistic cases while obtaining similar out-of-sample performance with the state-of-the-art",
    "checked": true,
    "id": "476ecaae2f6739a00985539ed87dbabbb92c03cd",
    "semantic_title": "optimal survival trees: a dynamic programming approach",
    "citation_count": 0,
    "authors": [
      "Tim Huisman",
      "Jacobus G. M. van der Linden",
      "Emir Demirović"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29164": {
    "title": "ProCC: Progressive Cross-Primitive Compatibility for Open-World Compositional Zero-Shot Learning",
    "volume": "main",
    "abstract": "Open-World Compositional Zero-shot Learning (OW-CZSL) aims to recognize novel compositions of state and object primitives in images with no priors on the compositional space, which induces a tremendously large output space containing all possible state-object compositions. Existing works either learn the joint compositional state-object embedding or predict simple primitives with separate classifiers. However, the former method heavily relies on external word embedding methods, and the latter ignores the interactions of interdependent primitives, respectively. In this paper, we revisit the primitive prediction approach and propose a novel method, termed Progressive Cross-primitive Compatibility (ProCC), to mimic the human learning process for OW-CZSL tasks. Specifically, the cross-primitive compatibility module explicitly learns to model the interactions of state and object features with the trainable memory units, which efficiently acquires cross-primitive visual attention to reason high-feasibility compositions, without the aid of external knowledge. Moreover, to alleviate the invalid cross-primitive interactions, especially for partial-supervision conditions (pCZSL), we design a progressive training paradigm to optimize the primitive classifiers conditioned on pre-trained features in an easy-to-hard manner. Extensive experiments on three widely used benchmark datasets demonstrate that our method outperforms other representative methods on both OW-CZSL and pCZSL settings by large margins",
    "checked": true,
    "id": "df4d6f2e8e1363c13eb9b16f6db740ce7e14d7d0",
    "semantic_title": "procc: progressive cross-primitive compatibility for open-world compositional zero-shot learning",
    "citation_count": 1,
    "authors": [
      "Fushuo Huo",
      "Wenchao Xu",
      "Song Guo",
      "Jingcai Guo",
      "Haozhao Wang",
      "Ziming Liu",
      "Xiaocheng Lu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29165": {
    "title": "Non-exemplar Online Class-Incremental Continual Learning via Dual-Prototype Self-Augment and Refinement",
    "volume": "main",
    "abstract": "This paper investigates a new, practical, but challenging problem named Non-exemplar Online Class-incremental continual Learning (NO-CL), which aims to preserve the discernibility of base classes without buffering data examples and efficiently learn novel classes continuously in a single-pass (i.e., online) data stream. The challenges of this task are mainly two-fold: (1) Both base and novel classes suffer from severe catastrophic forgetting as no previous samples are available for replay. (2) As the online data can only be observed once, there is no way to fully re-train the whole model, e.g., re-calibrate the decision boundaries via prototype alignment or feature distillation. In this paper, we propose a novel Dual-prototype Self-augment and Refinement method (DSR) for NO-CL problem, which consists of two strategies: 1) Dual class prototypes: vanilla and high-dimensional prototypes are exploited to utilize the pre-trained information and obtain robust quasi-orthogonal representations rather than example buffers for both privacy preservation and memory reduction. 2) Self-augment and refinement: Instead of updating the whole network, we optimize high-dimensional prototypes alternatively with the extra projection module based on self-augment vanilla prototypes, through a bi-level optimization problem. Extensive experiments demonstrate the effectiveness and superiority of the proposed DSR in NO-CL",
    "checked": true,
    "id": "9a71f679c5dcb2e770099bb6b5a03f0353e9e0fa",
    "semantic_title": "non-exemplar online class-incremental continual learning via dual-prototype self-augment and refinement",
    "citation_count": 6,
    "authors": [
      "Fushuo Huo",
      "Wenchao Xu",
      "Jingcai Guo",
      "Haozhao Wang",
      "Yunfeng Fan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29166": {
    "title": "New Classes of the Greedy-Applicable Arm Feature Distributions in the Sparse Linear Bandit Problem",
    "volume": "main",
    "abstract": "We consider the sparse contextual bandit problem where arm feature affects reward through the inner product of sparse parameters. Recent studies have developed sparsity-agnostic algorithms based on the greedy arm selection policy. However, the analysis of these algorithms requires strong assumptions on the arm feature distribution to ensure that the greedily selected samples are sufficiently diverse; One of the most common assumptions, relaxed symmetry, imposes approximate origin-symmetry on the distribution, which cannot allow distributions that has origin-asymmetric support. In this paper, we show that the greedy algorithm is applicable to a wider range of the arm feature distributions from two aspects. Firstly, we show that a mixture distribution that has a greedy-applicable component is also greedy-applicable. Second, we propose new distribution classes, related to Gaussian mixture, discrete, and radial distribution, for which the sample diversity is guaranteed. The proposed classes can describe distributions with origin-asymmetric support and, in conjunction with the first claim, provide theoretical guarantees of the greedy policy for a very wide range of the arm feature distributions",
    "checked": true,
    "id": "49e7258b911313e86db8032af558d6403ed42d45",
    "semantic_title": "new classes of the greedy-applicable arm feature distributions in the sparse linear bandit problem",
    "citation_count": 0,
    "authors": [
      "Koji Ichikawa",
      "Shinji Ito",
      "Daisuke Hatano",
      "Hanna Sumita",
      "Takuro Fukunaga",
      "Naonori Kakimura",
      "Ken-ichi Kawarabayashi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29167": {
    "title": "Fairness without Demographics through Shared Latent Space-Based Debiasing",
    "volume": "main",
    "abstract": "Ensuring fairness in machine learning (ML) is crucial, particularly in applications that impact diverse populations. The majority of existing works heavily rely on the availability of protected features like race and gender. However, practical challenges such as privacy concerns and regulatory restrictions often prohibit the use of this data, limiting the scope of traditional fairness research. To address this, we introduce a Shared Latent Space-based Debiasing (SLSD) method that transforms data from both the target domain, which lacks protected features, and a separate source domain, which contains these features, into correlated latent representations. This allows for joint training of a cross-domain protected group estimator on the representations. We then debias the downstream ML model with an adversarial learning technique that leverages the group estimator. We also present a relaxed variant of SLSD, the R-SLSD, that occasionally accesses a small subset of protected features from the target domain during its training phase. Our extensive experiments on benchmark datasets demonstrate that our methods consistently outperform existing state-of-the-art models in standard group fairness metrics",
    "checked": true,
    "id": "78fb2cf12542c9c3751c4c7a9c45f1edda2eab65",
    "semantic_title": "fairness without demographics through shared latent space-based debiasing",
    "citation_count": 0,
    "authors": [
      "Rashidul Islam",
      "Huiyuan Chen",
      "Yiwei Cai"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29168": {
    "title": "TMPNN: High-Order Polynomial Regression Based on Taylor Map Factorization",
    "volume": "main",
    "abstract": "The paper presents Taylor Map Polynomial Neural Network (TMPNN), a novel form of very high-order polynomial regression, in which the same coefficients for a lower-to-moderate-order polynomial regression are iteratively reapplied so as to achieve a higher-order model without the number of coefficients to be fit exploding in the usual curse-of-dimensionality way. This method naturally implements multi-target regression and can capture internal relationships between targets. We also introduce an approach for model interpretation in the form of systems of differential equations. By benchmarking on Feynman regression, UCI, Friedman-1, and real-life industrial datasets, we demonstrate that the proposed method performs comparably to the state-of-the-art regression methods and outperforms them on specific tasks",
    "checked": true,
    "id": "b355d2878e541aa766e037fd2e6328162646e99d",
    "semantic_title": "tmpnn: high-order polynomial regression based on taylor map factorization",
    "citation_count": 1,
    "authors": [
      "Andrei Ivanov",
      "Stefan Ailuro"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29169": {
    "title": "Personalized Reinforcement Learning with a Budget of Policies",
    "volume": "main",
    "abstract": "Personalization in machine learning (ML) tailors models' decisions to the individual characteristics of users. While this approach has seen success in areas like recommender systems, its expansion into high-stakes fields such as healthcare and autonomous driving is hindered by the extensive regulatory approval processes involved. To address this challenge, we propose a novel framework termed represented Markov Decision Processes (r-MDPs) that is designed to balance the need for personalization with the regulatory constraints. In an r-MDP, we cater to a diverse user population, each with unique preferences, through interaction with a small set of representative policies. Our objective is twofold: efficiently match each user to an appropriate representative policy and simultaneously optimize these policies to maximize overall social welfare. We develop two deep reinforcement learning algorithms that efficiently solve r-MDPs. These algorithms draw inspiration from the principles of classic K-means clustering and are underpinned by robust theoretical foundations. Our empirical investigations, conducted across a variety of simulated environments, showcase the algorithms' ability to facilitate meaningful personalization even under constrained policy budgets. Furthermore, they demonstrate scalability, efficiently adapting to larger policy budgets",
    "checked": true,
    "id": "919a8eda73555eec0c966d12cfe81f9d55a780d4",
    "semantic_title": "personalized reinforcement learning with a budget of policies",
    "citation_count": 0,
    "authors": [
      "Dmitry Ivanov",
      "Omer Ben-Porat"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29170": {
    "title": "Delivering Inflated Explanations",
    "volume": "main",
    "abstract": "In the quest for Explainable Artificial Intelligence (XAI) one of the questions that frequently arises given a decision made by an AI system is, ``why was the decision made in this way?'' Formal approaches to explainability build a formal model of the AI system and use this to reason about the properties of the system. Given a set of feature values for an instance to be explained, and a resulting decision, a formal abductive explanation is a set of features, such that if they take the given value will always lead to the same decision. This explanation is useful, it shows that only some features were used in making the final decision. But it is narrow, it only shows that if the selected features take their given values the decision is unchanged. It is possible that some features may change values and still lead to the same decision. In this paper we formally define inflated explanations which is a set of features, and for each feature a set of values (always including the value of the instance being explained), such that the decision will remain unchanged, for any of the values allowed for any of the features in the (inflated) abductive explanation. Inflated formal explanations are more informative than common abductive explanations since e.g. they allow us to see if the exact value of a feature is important, or it could be any nearby value. Overall they allow us to better understand the role of each feature in the decision. We show that we can compute inflated explanations for not that much greater cost than abductive explanations, and that we can extend duality results for abductive explanations also to inflated explanations",
    "checked": true,
    "id": "d33555653bbbbd5eb1d4edfd79f241742ab932f0",
    "semantic_title": "delivering inflated explanations",
    "citation_count": 3,
    "authors": [
      "Yacine Izza",
      "Alexey Ignatiev",
      "Peter J. Stuckey",
      "Joao Marques-Silva"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29171": {
    "title": "Unified Framework for Diffusion Generative Models in SO(3): Applications in Computer Vision and Astrophysics",
    "volume": "main",
    "abstract": "Diffusion-based generative models represent the current state-of-the-art for image generation. However, standard diffusion models are based on Euclidean geometry and do not translate directly to manifold-valued data. In this work, we develop extensions of both score-based generative models (SGMs) and Denoising Diffusion Probabilistic Models (DDPMs) to the Lie group of 3D rotations, SO(3). SO(3) is of particular interest in many disciplines such as robotics, biochemistry and astronomy/cosmology science. Contrary to more general Riemannian manifolds, SO(3) admits a tractable solution to heat diffusion, and allows us to implement efficient training of diffusion models. We apply both SO(3) DDPMs and SGMs to synthetic densities on SO(3) and demonstrate state-of-the-art results. Additionally, we demonstrate the practicality of our model on pose estimation tasks and in predicting correlated galaxy orientations for astrophysics/cosmology",
    "checked": true,
    "id": "8e287eddfae221fbd2f11d6d8aa58e26f3a3940e",
    "semantic_title": "unified framework for diffusion generative models in so(3): applications in computer vision and astrophysics",
    "citation_count": 1,
    "authors": [
      "Yesukhei Jagvaral",
      "Francois Lanusse",
      "Rachel Mandelbaum"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29172": {
    "title": "GO-DICE: Goal-Conditioned Option-Aware Offline Imitation Learning via Stationary Distribution Correction Estimation",
    "volume": "main",
    "abstract": "Offline imitation learning (IL) refers to learning expert behavior solely from demonstrations, without any additional interaction with the environment. Despite significant advances in offline IL, existing techniques find it challenging to learn policies for long-horizon tasks and require significant re-training when task specifications change. Towards addressing these limitations, we present GO-DICE an offline IL technique for goal-conditioned long-horizon sequential tasks. GO-DICE discerns a hierarchy of sub-tasks from demonstrations and uses these to learn separate policies for sub-task transitions and action execution, respectively; this hierarchical policy learning facilitates long-horizon reasoning.Inspired by the expansive DICE-family of techniques, policy learning at both the levels transpires within the space of stationary distributions. Further, both policies are learnt with goal conditioning to minimize need for retraining when task goals change. Experimental results substantiate that GO-DICE outperforms recent baselines, as evidenced by a marked improvement in the completion rate of increasingly challenging pick-and-place Mujoco robotic tasks. GO-DICE is also capable of leveraging imperfect demonstration and partial task segmentation when available, both of which boost task performance relative to learning from expert demonstrations alone",
    "checked": true,
    "id": "0d592c6530b6496dcaec9a646d533b7af75cc1ec",
    "semantic_title": "go-dice: goal-conditioned option-aware offline imitation learning via stationary distribution correction estimation",
    "citation_count": 1,
    "authors": [
      "Abhinav Jain",
      "Vaibhav Unhelkar"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29173": {
    "title": "Instance-Conditional Timescales of Decay for Non-Stationary Learning",
    "volume": "main",
    "abstract": "Slow concept drift is a ubiquitous, yet under-studied problem in practical machine learning systems. In such settings, although recent data is more indicative of future data, naively prioritizing recent instances runs the risk of losing valuable information from the past. We propose an optimization-driven approach towards balancing instance importance over large training windows. First, we model instance relevance using a mixture of multiple timescales of decay, allowing us to capture rich temporal trends. Second, we learn an auxiliary scorer model that recovers the appropriate mixture of timescales as a function of the instance itself. Finally, we propose a nested optimization objective for learning the scorer, by which it maximizes forward transfer for the learned model. Experiments on a large real-world dataset of 39M photos over a 9 year period show upto 15% relative gains in accuracy compared to other robust learning baselines. We replicate our gains on two collections of real-world datasets for non-stationary learning, and extend our work to continual learning settings where, too, we beat SOTA methods by large margins",
    "checked": true,
    "id": "c0c1130469038371856fcf3e15cbb8572362bbbe",
    "semantic_title": "instance-conditional timescales of decay for non-stationary learning",
    "citation_count": 3,
    "authors": [
      "Nishant  Jain",
      "Pradeep Shenoy"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29174": {
    "title": "Universal Weak Coreset",
    "volume": "main",
    "abstract": "Coresets for k-means and k-median problems yield a small summary of the data, which preserves the clustering cost with respect to any set of k centers. Recently coresets have also been constructed for constrained k-means and k-median problems. However, the notion of coresets has the drawback that (i) they can only be applied in settings where the input points are allowed to have weights, and (ii) in general metric spaces, the size of the coresets can depend logarithmically on the number of points. The notion of weak coresets, which has less stringent requirements than coresets, has been studied in the context of classical k-means and k-median problems. A weak coreset is a pair (J,S) of subsets of points, where S acts as a summary of the point set and J as a set of potential centers. This pair satisfies the properties that (i) S is a good summary of the data as long as the k centers are chosen from J only, and (ii) there is a good choice of k centers in J with a cost close to the optimal cost. We develop this framework, which we call universal weak coresets, for constrained clustering settings. In conjunction with recent coreset constructions for constrained settings, our designs give greater data compression, are conceptually simpler, and apply to a wide range of constrained k-median and k-means problems",
    "checked": true,
    "id": "a7a11163ea38f8e7209d24a79468dd519431efd9",
    "semantic_title": "universal weak coreset",
    "citation_count": 0,
    "authors": [
      "Ragesh Jaiswal",
      "Amit Kumar"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29175": {
    "title": "Transportable Representations for Domain Generalization",
    "volume": "main",
    "abstract": "One key assumption in machine learning literature is that the testing and training data come from the same distribution, which is often violated in practice. The anchors that allow generalizations to take place are causal, and provenient in terms of the stability and modularity of the mechanisms underlying the system of variables. Building on the theory of causal transportability, we define the notion of ``transportable representations\", and show that these representations are suitable candidates for the domain generalization task. Specifically, considering that the graphical assumptions about the underlying system are provided, the transportable representations can be characterized accordingly, and the distribution of label conditioned on the representation can be computed in terms of the source distributions. Finally, we relax the assumption of having access to the underlying graph by proving a graphical-invariance duality theorem, which delineates certain probabilistic invariances present in the source data as a sound and complete criterion for generalizable classification. Our findings provide a unifying theoretical basis for several existing approaches to the domain generalization problem",
    "checked": true,
    "id": "66e2b2ae2f99b2282001db5cfce2039e8ddcb088",
    "semantic_title": "transportable representations for domain generalization",
    "citation_count": 0,
    "authors": [
      "Kasra Jalaldoust",
      "Elias Bareinboim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29176": {
    "title": "Meta-Learning-Based Adaptive Stability Certificates for Dynamical Systems",
    "volume": "main",
    "abstract": "This paper addresses the problem of Neural Network (NN) based adaptive stability certification in a dynamical system. The state-of-the-art methods, such as Neural Lyapunov Functions (NLFs), use NN-based formulations to assess the stability of a non-linear dynamical system and compute a Region of Attraction (ROA) in the state space. However, under parametric uncertainty, if the values of system parameters vary over time, the NLF methods fail to adapt to such changes and may lead to conservative stability assessment performance. We circumvent this issue by integrating Model Agnostic Meta-learning (MAML) with NLFs and propose meta-NLFs. In this process, we train a meta-function that adapts to any parametric shifts and updates into an NLF for the system with new test-time parameter values. We demonstrate the stability assessment performance of meta-NLFs on some standard benchmark autonomous dynamical systems",
    "checked": true,
    "id": "b3faf31c7c402839fa528702cdd7c00901fd4bde",
    "semantic_title": "meta-learning-based adaptive stability certificates for dynamical systems",
    "citation_count": 1,
    "authors": [
      "Amit Jena",
      "Dileep Kalathil",
      "Le Xie"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29177": {
    "title": "Rethinking Dimensional Rationale in Graph Contrastive Learning from Causal Perspective",
    "volume": "main",
    "abstract": "Graph contrastive learning is a general learning paradigm excelling at capturing invariant information from diverse perturbations in graphs. Recent works focus on exploring the structural rationale from graphs, thereby increasing the discriminability of the invariant information. However, such methods may incur in the mis-learning of graph models towards the interpretability of graphs, and thus the learned noisy and task-agnostic information interferes with the prediction of graphs. To this end, with the purpose of exploring the intrinsic rationale of graphs, we accordingly propose to capture the dimensional rationale from graphs, which has not received sufficient attention in the literature. The conducted exploratory experiments attest to the feasibility of the aforementioned roadmap. To elucidate the innate mechanism behind the performance improvement arising from the dimensional rationale, we rethink the dimensional rationale in graph contrastive learning from a causal perspective and further formalize the causality among the variables in the pre-training stage to build the corresponding structural causal model. On the basis of the understanding of the structural causal model, we propose the dimensional rationale-aware graph contrastive learning approach, which introduces a learnable dimensional rationale acquiring network and a redundancy reduction constraint. The learnable dimensional rationale acquiring network is updated by leveraging a bi-level meta-learning technique, and the redundancy reduction constraint disentangles the redundant features through a decorrelation process during learning. Empirically, compared with state-of-the-art methods, our method can yield significant performance boosts on various benchmarks with respect to discriminability and transferability. The code implementation of our method is available at https://github.com/ByronJi/DRGCL",
    "checked": true,
    "id": "20e2b8f13b489fc839c1024635ffca72301c6a4b",
    "semantic_title": "rethinking dimensional rationale in graph contrastive learning from causal perspective",
    "citation_count": 4,
    "authors": [
      "Qirui Ji",
      "Jiangmeng Li",
      "Jie Hu",
      "Rui Wang",
      "Changwen Zheng",
      "Fanjiang Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29178": {
    "title": "MusER: Musical Element-Based Regularization for Generating Symbolic Music with Emotion",
    "volume": "main",
    "abstract": "Generating music with emotion is an important task in automatic music generation, in which emotion is evoked through a variety of musical elements (such as pitch and duration) that change over time and collaborate with each other. However, prior research on deep learning-based emotional music generation has rarely explored the contribution of different musical elements to emotions, let alone the deliberate manipulation of these elements to alter the emotion of music, which is not conducive to fine-grained element-level control over emotions. To address this gap, we present a novel approach employing musical element-based regularization in the latent space to disentangle distinct elements, investigate their roles in distinguishing emotions, and further manipulate elements to alter musical emotions. Specifically, we propose a novel VQ-VAE-based model named MusER. MusER incorporates a regularization loss to enforce the correspondence between the musical element sequences and the specific dimensions of latent variable sequences, providing a new solution for disentangling discrete sequences. Taking advantage of the disentangled latent vectors, a two-level decoding strategy that includes multiple decoders attending to latent vectors with different semantics is devised to better predict the elements. By visualizing latent space, we conclude that MusER yields a disentangled and interpretable latent space and gain insights into the contribution of distinct elements to the emotional dimensions (i.e., arousal and valence). Experimental results demonstrate that MusER outperforms the state-of-the-art models for generating emotional music in both objective and subjective evaluation. Besides, we rearrange music through element transfer and attempt to alter the emotion of music by transferring emotion-distinguishable elements",
    "checked": true,
    "id": "22e8ad6929812934616d788c9bd99dbd0951fd42",
    "semantic_title": "muser: musical element-based regularization for generating symbolic music with emotion",
    "citation_count": 0,
    "authors": [
      "Shulei Ji",
      "Xinyu Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29179": {
    "title": "FedFixer: Mitigating Heterogeneous Label Noise in Federated Learning",
    "volume": "main",
    "abstract": "Federated Learning (FL) heavily depends on label quality for its performance. However, the label distribution among individual clients is always both noisy and heterogeneous. The high loss incurred by client-specific samples in heterogeneous label noise poses challenges for distinguishing between client-specific and noisy label samples, impacting the effectiveness of existing label noise learning approaches. To tackle this issue, we propose FedFixer, where the personalized model is introduced to cooperate with the global model to effectively select clean client-specific samples. In the dual models, updating the personalized model solely at a local level can lead to overfitting on noisy data due to limited samples, consequently affecting both the local and global models' performance. To mitigate overfitting, we address this concern from two perspectives. Firstly, we employ a confidence regularizer to alleviate the impact of unconfident predictions caused by label noise. Secondly, a distance regularizer is implemented to constrain the disparity between the personalized and global models. We validate the effectiveness of FedFixer through extensive experiments on benchmark datasets. The results demonstrate that FedFixer can perform well in filtering noisy label samples on different clients, especially in highly heterogeneous label noise scenarios",
    "checked": true,
    "id": "32d9c38efef60d0c701e270dd65472a1fa2fbe5d",
    "semantic_title": "fedfixer: mitigating heterogeneous label noise in federated learning",
    "citation_count": 1,
    "authors": [
      "Xinyuan Ji",
      "Zhaowei Zhu",
      "Wei Xi",
      "Olga Gadyatskaya",
      "Zilong Song",
      "Yong Cai",
      "Yang Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29180": {
    "title": "Stratified GNN Explanations through Sufficient Expansion",
    "volume": "main",
    "abstract": "Explaining the decisions made by Graph Neural Networks (GNNs) is vital for establishing trust and ensuring fairness in critical applications such as medicine and science. The prevalence of hierarchical structure in real-world graphs/networks raises an important question on GNN interpretability: \"On each level of the graph structure, which specific fraction imposes the highest influence over the prediction?\" Currently, the prevailing two categories of methods are incapable of achieving multi-level GNN explanation due to their flat or motif-centric nature. In this work, we formulate the problem of learning multi-level explanations out of GNN models and introduce a stratified explainer module, namely STFExplainer, that utilizes the concept of sufficient expansion to generate explanations on each stratum. Specifically, we learn a higher-level subgraph generator by leveraging both hierarchical structure and GNN-encoded input features. Experiment results on both synthetic and real-world datasets demonstrate the superiority of our stratified explainer on standard interpretability tasks and metrics such as fidelity and explanation recall, with an average improvement of 11% and 8% over the best alternative on each data type. The case study on material domains also confirms the value of our approach through detected multi-level graph patterns accurately reconstructing the knowledge-based ground truth",
    "checked": true,
    "id": "24a1674e332118a0322784060fc3f3ca0c4f10be",
    "semantic_title": "stratified gnn explanations through sufficient expansion",
    "citation_count": 0,
    "authors": [
      "Yuwen Ji",
      "Lei Shi",
      "Zhimeng Liu",
      "Ge Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29181": {
    "title": "FedLPS: Heterogeneous Federated Learning for Multiple Tasks with Local Parameter Sharing",
    "volume": "main",
    "abstract": "Federated Learning (FL) has emerged as a promising solution in Edge Computing (EC) environments to process the proliferation of data generated by edge devices. By collaboratively optimizing the global machine learning models on distributed edge devices, FL circumvents the need for transmitting raw data and enhances user privacy. Despite practical successes, FL still confronts significant challenges including constrained edge device resources, multiple tasks deployment, and data heterogeneity. However, existing studies focus on mitigating the FL training costs of each single task whereas neglecting the resource consumption across multiple tasks in heterogeneous FL scenarios. In this paper, we propose Heterogeneous Federated Learning with Local Parameter Sharing (FedLPS) to fill this gap. FedLPS leverages principles from transfer learning to facilitate the deployment of multiple tasks on a single device by dividing the local model into a shareable encoder and task-specific encoders. To further reduce resource consumption, a channel-wise model pruning algorithm that shrinks the footprint of local models while accounting for both data and system heterogeneity is employed in FedLPS. Additionally, a novel heterogeneous model aggregation algorithm is proposed to aggregate the heterogeneous predictors in FedLPS. We implemented the proposed FedLPS on a real FL platform and compared it with state-of-the-art (SOTA) FL frameworks. The experimental results on five popular datasets and two modern DNN models illustrate that the proposed FedLPS significantly outperforms the SOTA FL frameworks by up to 4.88% and reduces the computational resource consumption by 21.3%. Our code is available at: https://github.com/jyzgh/FedLPS",
    "checked": true,
    "id": "29db1570a64234695bd6548fc4e0f67282afeca9",
    "semantic_title": "fedlps: heterogeneous federated learning for multiple tasks with local parameter sharing",
    "citation_count": 0,
    "authors": [
      "Yongzhe Jia",
      "Xuyun Zhang",
      "Amin Beheshti",
      "Wanchun Dou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29182": {
    "title": "Long-Tailed Partial Label Learning by Head Classifier and Tail Classifier Cooperation",
    "volume": "main",
    "abstract": "In partial label learning (PLL), each instance is associated with a set of candidate labels, among which only one is correct. The traditional PLL almost all implicitly assume that the distribution of the classes is balanced. However, in real-world applications, the distribution of the classes is imbalanced or long-tailed, leading to the long-tailed partial label learning problem. The previous methods solve this problem mainly by ameliorating the ability to learn in the tail classes, which will sacrifice the performance of the head classes. While keeping the performance of the head classes may degrade the performance of the tail classes. Therefore, in this paper, we construct two classifiers, i.e., a head classifier for keeping the performance of dominant classes and a tail classifier for improving the performance of the tail classes. Then, we propose a classifier weight estimation module to automatically estimate the shot belongingness (head class or tail class) of the samples and allocate the weights for the head classifier and tail classifier when making prediction. This cooperation improves the prediction ability for both the head classes and the tail classes. The experiments on the benchmarks demonstrate the proposed approach improves the accuracy of the SOTA methods by a substantial margin. Code and data are available at: https://github.com/pruirui/HTC-LTPLL",
    "checked": true,
    "id": "7ef18659f01b6c21cbb7d7acecce93d70c4bacc3",
    "semantic_title": "long-tailed partial label learning by head classifier and tail classifier cooperation",
    "citation_count": 0,
    "authors": [
      "Yuheng Jia",
      "Xiaorui Peng",
      "Ran Wang",
      "Min-Ling Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29183": {
    "title": "Which Is More Effective in Label Noise Cleaning, Correction or Filtering?",
    "volume": "main",
    "abstract": "Most noise cleaning methods adopt one of the correction and filtering modes to build robust models. However, their effectiveness, applicability, and hyper-parameter insensitivity have not been carefully studied. We compare the two cleaning modes via a rebuilt error bound in noisy environments. At the dataset level, Theorem 5 implies that correction is more effective than filtering when the cleaned datasets have close noise rates. At the sample level, Theorem 6 indicates that confident label noises (large noise probabilities) are more suitable to be corrected, and unconfident noises (medium noise probabilities) should be filtered. Besides, an imperfect hyper-parameter may have fewer negative impacts on filtering than correction. Unlike existing methods with a single cleaning mode, the proposed Fusion cleaning framework of Correction and Filtering (FCF) combines the advantages of different modes to deal with diverse suspicious labels. Experimental results demonstrate that our FCF method can achieve state-of-the-art performance on benchmark datasets",
    "checked": true,
    "id": "8218343ca7bdbdc394c7dbf99395b7bbe9a038dc",
    "semantic_title": "which is more effective in label noise cleaning, correction or filtering?",
    "citation_count": 0,
    "authors": [
      "Gaoxia Jiang",
      "Jia Zhang",
      "Xuefei Bai",
      "Wenjian Wang",
      "Deyu Meng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29184": {
    "title": "Navigating Real-World Partial Label Learning: Unveiling Fine-Grained Images with Attributes",
    "volume": "main",
    "abstract": "Partial label learning (PLL), a significant research area, addresses the challenge of annotating each sample with a candidate label set containing the true label when obtaining accurate labels is infeasible. However, existing PLL methods often rely on generic datasets like CIFAR, where annotators can readily differentiate candidate labels and are unlikely to confuse, making it less realistic for real-world partial label applications. In response, our research focuses on a rarely studied problem, PLL on fine-grained images with attributes. And we propose a novel framework called Shared to Learn, Distinct to Disambiguate (SoDisam). Within the candidate label set, the categories may exhibit numerous shared attribute features, posing a challenge in accurately distinguishing them. Rather than perceiving it as an impediment, we capitalize on these shared attributes as definitive sources of supervision. This insight guides us to learn attribute space visual representation to focus on the information from these shared attributes. Moreover, we introduce an attribute attention mechanism tailored to harness the remaining distinct attributes. This mechanism directs the originally holistic feature towards specific regions, capturing corresponding discriminative features. In addition, a dynamic disambiguation module is introduced, continuously adjusting the two aforementioned mechanisms and achieve the final disambiguation process. Extensive experiments demonstrate the effectiveness of our approach on fine-grained partial label datasets. The proposed SoDisam framework not only addresses the challenges associated with fine-grained partial label learning but also provides a more realistic representation of real-world partial label scenarios",
    "checked": true,
    "id": "f2ecccd3b476feae9e4db83e40c230b0a0abe623",
    "semantic_title": "navigating real-world partial label learning: unveiling fine-grained images with attributes",
    "citation_count": 0,
    "authors": [
      "Haoran Jiang",
      "Zhihao Sun",
      "YingJie Tian"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29185": {
    "title": "DHGCN: Dynamic Hop Graph Convolution Network for Self-Supervised Point Cloud Learning",
    "volume": "main",
    "abstract": "Recent works attempt to extend Graph Convolution Networks (GCNs) to point clouds for classification and segmentation tasks. These works tend to sample and group points to create smaller point sets locally and mainly focus on extracting local features through GCNs, while ignoring the relationship between point sets. In this paper, we propose the Dynamic Hop Graph Convolution Network (DHGCN) for explicitly learning the contextual relationships between the voxelized point parts, which are treated as graph nodes. Motivated by the intuition that the contextual information between point parts lies in the pairwise adjacent relationship, which can be depicted by the hop distance of the graph quantitatively, we devise a novel self-supervised part-level hop distance reconstruction task and design a novel loss function accordingly to facilitate training. In addition, we propose the Hop Graph Attention (HGA), which takes the learned hop distance as input for producing attention weights to allow edge features to contribute distinctively in aggregation. Eventually, the proposed DHGCN is a plug-and-play module that is compatible with point-based backbone networks. Comprehensive experiments on different backbones and tasks demonstrate that our self-supervised method achieves state-of-the-art performance. Our source codes are available at: https://github.com/Jinec98/DHGCN",
    "checked": true,
    "id": "3f69dedaff5f4fd430f18c0272f0905d3ba53d15",
    "semantic_title": "dhgcn: dynamic hop graph convolution network for self-supervised point cloud learning",
    "citation_count": 2,
    "authors": [
      "Jincen Jiang",
      "Lizhi Zhao",
      "Xuequan Lu",
      "Wei Hu",
      "Imran Razzak",
      "Meili Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29186": {
    "title": "FMRNet: Image Deraining via Frequency Mutual Revision",
    "volume": "main",
    "abstract": "The wavelet transform has emerged as a powerful tool in deciphering structural information within images. And now, the latest research suggests that combining the prowess of wavelet transform with neural networks can lead to unparalleled image deraining results. By harnessing the strengths of both the spatial domain and frequency space, this innovative approach is poised to revolutionize the field of image processing. The fascinating challenge of developing a comprehensive framework that takes into account the intrinsic frequency property and the correlation between rain residue and background is yet to be fully explored. In this work, we propose to investigate the potential relationships among rain-free and residue components at the frequency domain, forming a frequency mutual revision network (FMRNet) for image deraining. Specifically, we explore the mutual representation of rain residue and background components at frequency domain, so as to better separate the rain layer from clean background while preserving structural textures of the degraded images. Meanwhile, the rain distribution prediction from the low-frequency coefficient, which can be seen as the degradation prior is used to refine the separation of rain residue and background components. Inversely, the updated rain residue is used to benefit the low-frequency rain distribution prediction, forming the multi-layer mutual learning. Extensive experiments demonstrate that our proposed FMRNet delivers significant performance gains for seven datasets on image deraining task, surpassing the state-of-the-art method ELFormer by 1.14 dB in PSNR on the Rain100L dataset, while with similar computation cost. Code and retrained models are available at https://github.com/kuijiang94/FMRNet",
    "checked": true,
    "id": "c3e391a1a4f292fd32ff4ffb6d5f4d952c511ffa",
    "semantic_title": "fmrnet: image deraining via frequency mutual revision",
    "citation_count": 1,
    "authors": [
      "Kui Jiang",
      "Junjun Jiang",
      "Xianming Liu",
      "Xin Xu",
      "Xianzheng Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29187": {
    "title": "Racing Control Variable Genetic Programming for Symbolic Regression",
    "volume": "main",
    "abstract": "Symbolic regression, as one of the most crucial tasks in AI for science, discovers governing equations from experimental data. Popular approaches based on genetic programming, Monte Carlo tree search, or deep reinforcement learning learn symbolic regression from a fixed dataset. These methods require massive datasets and long training time especially when learning complex equations involving many variables. Recently, Control Variable Genetic Programming (CVGP) has been introduced which accelerates the regression process by discovering equations from designed control variable experiments. However, the set of experiments is fixed a-priori in CVGP and we observe that sub-optimal selection of experiment schedules delay the discovery process significantly. To overcome this limitation, we propose Racing Control Variable Genetic Programming (Racing-CVGP), which carries out multiple experiment schedules simultaneously. A selection scheme similar to that used in selecting good symbolic equations in the genetic programming process is implemented to ensure that promising experiment schedules eventually win over the average ones. The unfavorable schedules are terminated early to save time for the promising ones. We evaluate Racing-CVGP on several synthetic and real-world datasets corresponding to true physics laws. We demonstrate that Racing-CVGP outperforms CVGP and a series of symbolic regressors which discover equations from fixed datasets",
    "checked": true,
    "id": "c236b4f25b8373de22e9b888e09af792cac33779",
    "semantic_title": "racing control variable genetic programming for symbolic regression",
    "citation_count": 0,
    "authors": [
      "Nan Jiang",
      "Yexiang Xue"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29188": {
    "title": "Learning Diverse Risk Preferences in Population-Based Self-Play",
    "volume": "main",
    "abstract": "Among the remarkable successes of Reinforcement Learning (RL), self-play algorithms have played a crucial role in solving competitive games. However, current self-play RL methods commonly optimize the agent to maximize the expected win-rates against its current or historical copies, resulting in a limited strategy style and a tendency to get stuck in local optima. To address this limitation, it is important to improve the diversity of policies, allowing the agent to break stalemates and enhance its robustness when facing with different opponents. In this paper, we present a novel perspective to promote diversity by considering that agents could have diverse risk preferences in the face of uncertainty. To achieve this, we introduce a novel reinforcement learning algorithm called Risk-sensitive Proximal Policy Optimization (RPPO), which smoothly interpolates between worst-case and best-case policy learning, enabling policy learning with desired risk preferences. Furthermore, by seamlessly integrating RPPO with population-based self-play, agents in the population optimize dynamic risk-sensitive objectives using experiences gained from playing against diverse opponents. Our empirical results demonstrate that our method achieves comparable or superior performance in competitive games and, importantly, leads to the emergence of diverse behavioral modes. Code is available at https://github.com/Jackory/RPBT",
    "checked": true,
    "id": "e9090851d13ce106a0f674c5c19768df1611a170",
    "semantic_title": "learning diverse risk preferences in population-based self-play",
    "citation_count": 1,
    "authors": [
      "Yuhua Jiang",
      "Qihan Liu",
      "Xiaoteng Ma",
      "Chenghao Li",
      "Yiqin Yang",
      "Jun Yang",
      "Bin Liang",
      "Qianchuan Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29189": {
    "title": "Deep Incomplete Multi-View Learning Network with Insufficient Label Information",
    "volume": "main",
    "abstract": "Due to the efficiency of integrating semantic consensus and complementary information across different views, multi-view classification methods have attracted much attention in recent years. However, multi-view data often suffers from both the miss of view features and insufficient label information, which significantly decrease the performance of traditional multi-view classification methods in practice. Learning for such simultaneous lack of feature and label is crucial but rarely studied. To tackle these problems, we propose a novel Deep Incomplete Multi-view Learning Network (DIMvLN) by incorporating graph networks and semi-supervised learning in this paper. Specifically, DIMvLN firstly designs the deep graph networks to effectively recover missing data with assigning pseudo-labels of large amounts of unlabeled instances and refine the incomplete feature information. Meanwhile, to enhance the label information, a novel pseudo-label generation strategy with the similarity constraints of unlabeled instances is proposed to exploit additional supervisory information and guide the completion module to preserve more semantic information of absent multi-view data. Besides, we design view-specific representation extractors with the autoencoder structure and contrastive loss to learn high-level semantic representations for each view, promote cross-view consistencies and augment the separability between different categories. Finally, extensive experimental results demonstrate the effectiveness of our DIMvLN, attaining noteworthy performance improvements compared to state-of-the-art competitors on several public benchmark datasets. Code will be available at GitHub",
    "checked": true,
    "id": "7d88dadb4ca75db357624710543067b73efa7260",
    "semantic_title": "deep incomplete multi-view learning network with insufficient label information",
    "citation_count": 0,
    "authors": [
      "Zhangqi Jiang",
      "Tingjin Luo",
      "Xinyan Liang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29190": {
    "title": "Provably Convergent Federated Trilevel Learning",
    "volume": "main",
    "abstract": "Trilevel learning, also called trilevel optimization (TLO), has been recognized as a powerful modelling tool for hierarchical decision process and widely applied in many machine learning applications, such as robust neural architecture search, hyperparameter optimization, and domain adaptation. Tackling TLO problems has presented a great challenge due to their nested decision-making structure. In addition, existing works on TLO face the following key challenges: 1) they all focus on the non-distributed setting, which may lead to privacy breach; 2) they do not offer any non-asymptotic convergence analysis which characterizes how fast an algorithm converges. To address the aforementioned challenges, this paper proposes an asynchronous federated trilevel optimization method to solve TLO problems. The proposed method utilizes u-cuts to construct a hyper-polyhedral approximation for the TLO problem and solve it in an asynchronous manner. We demonstrate that the proposed u-cuts are applicable to not only convex functions but also a wide range of non-convex functions that meet the u-weakly convex assumption. Furthermore, we theoretically analyze the non-asymptotic convergence rate for the proposed method by showing its iteration complexity to obtain ϵ-stationary point is upper bounded by O(1/ϵ²). Extensive experiments on real-world datasets have been conducted to elucidate the superiority of the proposed method, e.g., it has a faster convergence rate with a maximum acceleration of approximately 80%",
    "checked": true,
    "id": "90ad039546745b5eedfc3dfcaebba3f42a095cfb",
    "semantic_title": "provably convergent federated trilevel learning",
    "citation_count": 1,
    "authors": [
      "Yang Jiao",
      "Kai Yang",
      "Tiancheng Wu",
      "Chengtao Jian",
      "Jianwei Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29191": {
    "title": "Performative Federated Learning: A Solution to Model-Dependent and Heterogeneous Distribution Shifts",
    "volume": "main",
    "abstract": "We consider a federated learning (FL) system consisting of multiple clients and a server, where the clients aim to collaboratively learn a common decision model from their distributed data. Unlike the conventional FL framework that assumes the client's data is static, we consider scenarios where the clients' data distributions may be reshaped by the deployed decision model. In this work, we leverage the idea of distribution shift mappings in performative prediction to formalize this model-dependent data distribution shift and propose a performative FL framework. We first introduce necessary and sufficient conditions for the existence of a unique performative stable solution and characterize its distance to the performative optimal solution. Then we propose the performative FedAvg algorithm and show that it converges to the performative stable solution at a rate of O(1/T) under both full and partial participation schemes. In particular, we use novel proof techniques and show how the clients' heterogeneity influences the convergence. Numerical results validate our analysis and provide valuable insights into real-world applications",
    "checked": true,
    "id": "a66a0220ca04a7d6028fa601928e7152ad961252",
    "semantic_title": "performative federated learning: a solution to model-dependent and heterogeneous distribution shifts",
    "citation_count": 3,
    "authors": [
      "Kun Jin",
      "Tongxin Yin",
      "Zhongzhu Chen",
      "Zeyu Sun",
      "Xueru Zhang",
      "Yang Liu",
      "Mingyan Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29192": {
    "title": "Fractional Deep Reinforcement Learning for Age-Minimal Mobile Edge Computing",
    "volume": "main",
    "abstract": "Mobile edge computing (MEC) is a promising paradigm for real-time applications with intensive computational needs (e.g., autonomous driving), as it can reduce the processing delay. In this work, we focus on the timeliness of computational-intensive updates, measured by Age-of-Information (AoI), and study how to jointly optimize the task updating and offloading policies for AoI with fractional form. Specifically, we consider edge load dynamics and formulate a task scheduling problem to minimize the expected time-average AoI. The uncertain edge load dynamics, the nature of the fractional objective, and hybrid continuous-discrete action space (due to the joint optimization) make this problem challenging and existing approaches not directly applicable. To this end, we propose a fractional reinforcement learning (RL) framework and prove its convergence. We further design a model-free fractional deep RL (DRL) algorithm, where each device makes scheduling decisions with the hybrid action space without knowing the system dynamics and decisions of other devices. Experimental results show that our proposed algorithms reduce the average AoI by up to 57.6% compared with several non-fractional benchmarks",
    "checked": true,
    "id": "9e8244418986db148964227323fd5e4ab26fd764",
    "semantic_title": "fractional deep reinforcement learning for age-minimal mobile edge computing",
    "citation_count": 1,
    "authors": [
      "Lyudong Jin",
      "Ming Tang",
      "Meng Zhang",
      "Hao Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29193": {
    "title": "Finite-Time Frequentist Regret Bounds of Multi-Agent Thompson Sampling on Sparse Hypergraphs",
    "volume": "main",
    "abstract": "We study the multi-agent multi-armed bandit (MAMAB) problem, where agents are factored into overlapping groups. Each group represents a hyperedge, forming a hypergraph over the agents. At each round of interaction, the learner pulls a joint arm (composed of individual arms for each agent) and receives a reward according to the hypergraph structure. Specifically, we assume there is a local reward for each hyperedge, and the reward of the joint arm is the sum of these local rewards. Previous work introduced the multi-agent Thompson sampling (MATS) algorithm and derived a Bayesian regret bound. However, it remains an open problem how to derive a frequentist regret bound for Thompson sampling in this multi-agent setting. To address these issues, we propose an efficient variant of MATS, the epsilon-exploring Multi-Agent Thompson Sampling (eps-MATS) algorithm, which performs MATS exploration with probability epsilon while adopts a greedy policy otherwise. We prove that eps-MATS achieves a worst-case frequentist regret bound that is sublinear in both the time horizon and the local arm size. We also derive a lower bound for this setting, which implies our frequentist regret upper bound is optimal up to constant and logarithm terms, when the hypergraph is sufficiently sparse. Thorough experiments on standard MAMAB problems demonstrate the superior performance and the improved computational efficiency of eps-MATS compared with existing algorithms in the same setting",
    "checked": true,
    "id": "94324b139ed86982a60e380eed381a94141355a6",
    "semantic_title": "finite-time frequentist regret bounds of multi-agent thompson sampling on sparse hypergraphs",
    "citation_count": 1,
    "authors": [
      "Tianyuan Jin",
      "Hao-Lun Hsu",
      "William Chang",
      "Pan Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29194": {
    "title": "GLDL: Graph Label Distribution Learning",
    "volume": "main",
    "abstract": "Label Distribution Learning (LDL), as a more general learning setting than generic single-label and multi-label learning, has been commonly used in computer vision and many other applications. To date, existing LDL approaches are designed and applied to data without considering the interdependence between instances. In this paper, we propose a Graph Label Distribution Learning (GLDL) framework, which explicitly models three types of relationships: instance-instance, label-label, and instance-label, to learn the label distribution for networked data. A label-label network is learned to capture label-to-label correlation, through which GLDL can accurately learn label distributions for nodes. Dual graph convolution network (GCN) Co-training with heterogeneous message passing ensures two GCNs, one focusing on instance-instance relationship and the other one targeting label-label correlation, are jointly trained such that instance-instance relationship can help induce label-label correlation and vice versa. Our theoretical study derives the error bound of GLDL. For verification, four benchmark datasets with label distributions for nodes are created using common graph benchmarks. The experiments show that considering dependency helps learn better label distributions for networked data, compared to state-of-the-art LDL baseline. In addition, GLDL not only outperforms simple GCN and graph attention networks (GAT) using distribution loss but is also superior to its variant considering label-label relationship as a static network. GLDL and its benchmarks are the first research endeavors to address LDL for graphs. Code and benchmark data are released for public access",
    "checked": true,
    "id": "3c437a698f0e4cebcdba79cda70a7f5d229b6a6b",
    "semantic_title": "gldl: graph label distribution learning",
    "citation_count": 0,
    "authors": [
      "Yufei Jin",
      "Richard Gao",
      "Yi He",
      "Xingquan Zhu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29195": {
    "title": "Sterling: Synergistic Representation Learning on Bipartite Graphs",
    "volume": "main",
    "abstract": "A fundamental challenge of bipartite graph representation learning is how to extract informative node embeddings. Self-Supervised Learning (SSL) is a promising paradigm to address this challenge. Most recent bipartite graph SSL methods are based on contrastive learning which learns embeddings by discriminating positive and negative node pairs. Contrastive learning usually requires a large number of negative node pairs, which could lead to computational burden and semantic errors. In this paper, we introduce a novel synergistic representation learning model (STERLING) to learn node embeddings without negative node pairs. STERLING preserves the unique local and global synergies in bipartite graphs. The local synergies are captured by maximizing the similarity of the inter-type and intra-type positive node pairs, and the global synergies are captured by maximizing the mutual information of co-clusters. Theoretical analysis demonstrates that STERLING could improve the connectivity between different node types in the embedding space. Extensive empirical evaluation on various benchmark datasets and tasks demonstrates the effectiveness of STERLING for extracting node embeddings",
    "checked": true,
    "id": "02fa878ecc2c4426d22482a8e5cda952877b5f70",
    "semantic_title": "sterling: synergistic representation learning on bipartite graphs",
    "citation_count": 7,
    "authors": [
      "Baoyu Jing",
      "Yuchen Yan",
      "Kaize Ding",
      "Chanyoung Park",
      "Yada Zhu",
      "Huan Liu",
      "Hanghang Tong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29196": {
    "title": "FoX: Formation-Aware Exploration in Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "Recently, deep multi-agent reinforcement learning (MARL) has gained significant popularity due to its success in various cooperative multi-agent tasks. However, exploration still remains a challenging problem in MARL due to the partial observability of the agents and the exploration space that can grow exponentially as the number of agents increases. Firstly, in order to address the scalability issue of the exploration space, we define a formation-based equivalence relation on the exploration space and aim to reduce the search space by exploring only meaningful states in different formations. Then, we propose a novel formation-aware exploration (FoX) framework that encourages partially observable agents to visit the states in diverse formations by guiding them to be well aware of their current formation solely based on their own observations. Numerical results show that the proposed FoX framework significantly outperforms the state-of-the-art MARL algorithms on Google Research Football (GRF) and sparse Starcraft II multi-agent challenge (SMAC) tasks",
    "checked": true,
    "id": "b1f843c14c8fed4395d84d621d7be7c958280a1f",
    "semantic_title": "fox: formation-aware exploration in multi-agent reinforcement learning",
    "citation_count": 0,
    "authors": [
      "Yonghyeon Jo",
      "Sunwoo Lee",
      "Junghyuk Yeom",
      "Seungyul Han"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29197": {
    "title": "FLAME: A Small Language Model for Spreadsheet Formulas",
    "volume": "main",
    "abstract": "Spreadsheets are a vital tool for end-user data management. Using large language models for formula authoring assistance in these environments can be difficult, as these models are expensive to train and challenging to deploy due to their size (up to billions of parameters). We present FLAME, a transformer-based model trained exclusively on Excel formulas that leverages domain insights to achieve competitive performance while being substantially smaller (60M parameters) and training on two orders of magnitude less data. We curate a training dataset using sketch deduplication, introduce an Excel-specific formula tokenizer, and use domain-specific versions of masked span prediction and noisy auto-encoding as pre-training objectives. We evaluate FLAME on formula repair, formula completion, and similarity-based formula retrieval. FLAME can outperform much larger models, such as the Davinci (175B) and Cushman (12B) variants of Codex and CodeT5 (220M), in 10 of 14 evaluation settings for the repair and completion tasks. For formula retrieval, FLAME outperforms CodeT5, CodeBERT, and GraphCodeBERT",
    "checked": true,
    "id": "63396fceb84286b02796dc58e55c07ec1095c4dc",
    "semantic_title": "flame: a small language model for spreadsheet formulas",
    "citation_count": 5,
    "authors": [
      "Harshit Joshi",
      "Abishai Ebenezer",
      "José Cambronero Sanchez",
      "Sumit Gulwani",
      "Aditya Kanade",
      "Vu Le",
      "Ivan Radiček",
      "Gust Verbruggen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29198": {
    "title": "Towards Safe Policy Learning under Partial Identifiability: A Causal Approach",
    "volume": "main",
    "abstract": "Learning personalized treatment policies is a formative challenge in many real-world applications, including in healthcare, econometrics, artificial intelligence. However, the effectiveness of candidate policies is not always identifiable, i.e., it is not uniquely computable from the combination of the available data and assumptions about the generating mechanisms. This paper studies policy learning from data collected in various non-identifiable settings, i.e., (1) observational studies with unobserved confounding; (2) randomized experiments with partial observability; and (3) their combinations. We derive sharp, closed-formed bounds from observational and experimental data over the conditional treatment effects. Based on these novel bounds, we further characterize the problem of safe policy learning and develop an algorithm that trains a policy from data guaranteed to achieve, at least, the performance of the baseline policy currently deployed. Finally, we validate our proposed algorithm on synthetic data and a large clinical trial, demonstrating that it guarantees safe behaviors and robust performance",
    "checked": true,
    "id": "dd77342a066d29028ab9ee549ca5f0cb6f326806",
    "semantic_title": "towards safe policy learning under partial identifiability: a causal approach",
    "citation_count": 0,
    "authors": [
      "Shalmali Joshi",
      "Junzhe Zhang",
      "Elias Bareinboim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29199": {
    "title": "Patch-Wise Graph Contrastive Learning for Image Translation",
    "volume": "main",
    "abstract": "Recently, patch-wise contrastive learning is drawing attention for the image translation by exploring the semantic correspondence between the input image and the output image. To further explore the patch-wise topology for high-level semantic understanding, here we exploit the graph neural network to capture the topology-aware features. Specifically, we construct the graph based on the patch-wise similarity from a pretrained encoder, whose adjacency matrix is shared to enhance the consistency of patch-wise relation between the input and the output. Then, we obtain the node feature from the graph neural network, and enhance the correspondence between the nodes by increasing mutual information using the contrastive loss. In order to capture the hierarchical semantic structure, we further propose the graph pooling. Experimental results demonstrate the state-of-art results for the image translation thanks to the semantic encoding by the constructed graphs",
    "checked": true,
    "id": "424a115b4831da8118a2ca426458d6c0a50537ba",
    "semantic_title": "patch-wise graph contrastive learning for image translation",
    "citation_count": 1,
    "authors": [
      "Chanyong Jung",
      "Gihyun Kwon",
      "Jong Chul Ye"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29200": {
    "title": "NN-Steiner: A Mixed Neural-Algorithmic Approach for the Rectilinear Steiner Minimum Tree Problem",
    "volume": "main",
    "abstract": "Recent years have witnessed rapid advances in the use of neural networks to solve combinatorial optimization problems. Nevertheless, designing the \"right\" neural model that can effectively handle a given optimization problem can be challenging, and often there is no theoretical understanding or justification of the resulting neural model. In this paper, we focus on the rectilinear Steiner minimum tree (RSMT) problem, which is of critical importance in IC layout design and as a result has attracted numerous heuristic approaches in the VLSI literature. Our contributions are two-fold. On the methodology front, we propose NN-Steiner which is a novel mixed neural-algorithmic framework for computing RSMTs that leverages the celebrated PTAS algorithmic framework of Arora to solve this problem (and other geometric optimization problems). Our NN-Steiner replaces key algorithmic components within Arora's PTAS by suitable neural components. In particular, NN-Steiner only needs four neural network (NN) components that are called repeatedly within an algorithmic framework. Crucially, each of the four NN components is only of bounded size independent of input size, and thus easy to train. Furthermore, as the NN component is learning a generic algorithmic step, once learned, the resulting mixed neural-algorithmic framework generalizes to much larger instances not seen in training. Our NN-Steiner, to our best knowledge, is the first neural architecture of bounded size that has capacity to approximately solve RSMT (and variants). On the empirical front, we show how NN-Steiner can be implemented and demonstrate the effectiveness of our resulting approach, especially in terms of generalization, by comparing with state-of-the-art methods (both neural and non-neural based)",
    "checked": true,
    "id": "fbfbc2dcc21e99ce162ab239b2d35f81c0499696",
    "semantic_title": "nn-steiner: a mixed neural-algorithmic approach for the rectilinear steiner minimum tree problem",
    "citation_count": 2,
    "authors": [
      "Andrew B. Kahng",
      "Robert R. Nerem",
      "Yusu Wang",
      "Chien-Yi Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29201": {
    "title": "Measuring Self-Supervised Representation Quality for Downstream Classification Using Discriminative Features",
    "volume": "main",
    "abstract": "Self-supervised learning (SSL) has shown impressive results in downstream classification tasks. However, there is limited work in understanding their failure modes and interpreting their learned representations. In this paper, we study the representation space of state-of-the-art self-supervised models including SimCLR, SwaV, MoCo, BYOL, DINO, SimSiam, VICReg and Barlow Twins. Without the use of class label information, we discover discriminative features that correspond to unique physical attributes in images, present mostly in correctly-classified representations. Using these features, we can compress the representation space by up to$40% without significantly affecting linear classification performance. We then propose Self-Supervised Representation Quality Score (or Q-Score), an unsupervised score that can reliably predict if a given sample is likely to be mis-classified during linear evaluation, achieving AUPRC of 91.45 on ImageNet-100 and 78.78 on ImageNet-1K. Q-Score can also be used as a regularization term on pre-trained encoders to remedy low-quality representations. Fine-tuning with Q-Score regularization can boost the linear probing accuracy of SSL models by up to 5.8% on ImageNet-100 and 3.7% on ImageNet-1K compared to their baselines. Finally, using gradient heatmaps and Salient ImageNet masks, we define a metric to quantify the interpretability of each representation. We show that discriminative features are strongly correlated to core attributes and, enhancing these features through Q-score regularization makes SSL representations more interpretable",
    "checked": true,
    "id": "edb11b3a7591443bdaff83d011bf9b2c82d3e671",
    "semantic_title": "measuring self-supervised representation quality for downstream classification using discriminative features",
    "citation_count": 4,
    "authors": [
      "Neha Kalibhat",
      "Kanika Narang",
      "Hamed Firooz",
      "Maziar Sanjabi",
      "Soheil Feizi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29202": {
    "title": "Recall-Oriented Continual Learning with Generative Adversarial Meta-Model",
    "volume": "main",
    "abstract": "The stability-plasticity dilemma is a major challenge in continual learning, as it involves balancing the conflicting objectives of maintaining performance on previous tasks while learning new tasks. In this paper, we propose the recalloriented continual learning framework to address this challenge. Inspired by the human brain's ability to separate the mechanisms responsible for stability and plasticity, our framework consists of a two-level architecture where an inference network effectively acquires new knowledge and a generative network recalls past knowledge when necessary. In particular, to maximize the stability of past knowledge, we investigate the complexity of knowledge depending on different representations, and thereby introducing generative adversarial meta-model (GAMM) that incrementally learns task-specific parameters instead of input data samples of the task. Through our experiments, we show that our framework not only effectively learns new knowledge without any disruption but also achieves high stability of previous knowledge in both task-aware and task-agnostic learning scenarios. Our code is available at: https://github.com/bigdata-inha/recall-orientedcl-framework",
    "checked": true,
    "id": "2313a0e7ba83c2a82607de7869e5eeb6e06d5803",
    "semantic_title": "recall-oriented continual learning with generative adversarial meta-model",
    "citation_count": 0,
    "authors": [
      "Haneol Kang",
      "Dong-Wan Choi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29203": {
    "title": "Coupling Graph Neural Networks with Fractional Order Continuous Dynamics: A Robustness Study",
    "volume": "main",
    "abstract": "In this work, we rigorously investigate the robustness of graph neural fractional-order differential equation (FDE) models. This framework extends beyond traditional graph neural (integer-order) ordinary differential equation (ODE) models by implementing the time-fractional Caputo derivative. Utilizing fractional calculus allows our model to consider long-term memory during the feature updating process, diverging from the memoryless Markovian updates seen in traditional graph neural ODE models. The superiority of graph neural FDE models over graph neural ODE models has been established in environments free from attacks or perturbations. While traditional graph neural ODE models have been verified to possess a degree of stability and resilience in the presence of adversarial attacks in existing literature, the robustness of graph neural FDE models, especially under adversarial conditions, remains largely unexplored. This paper undertakes a detailed assessment of the robustness of graph neural FDE models. We establish a theoretical foundation outlining the robustness characteristics of graph neural FDE models, highlighting that they maintain more stringent output perturbation bounds in the face of input and graph topology disturbances, compared to their integer-order counterparts. Our empirical evaluations further confirm the enhanced robustness of graph neural FDE models, highlighting their potential in adversarially robust applications",
    "checked": true,
    "id": "eb43e619ac5bf6cc41285a5c875168c5790e134f",
    "semantic_title": "coupling graph neural networks with fractional order continuous dynamics: a robustness study",
    "citation_count": 2,
    "authors": [
      "Qiyu Kang",
      "Kai Zhao",
      "Yang Song",
      "Yihang Xie",
      "Yanan Zhao",
      "Sijie Wang",
      "Rui She",
      "Wee Peng Tay"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29204": {
    "title": "Neural Oscillators for Generalization of Physics-Informed Machine Learning",
    "volume": "main",
    "abstract": "A primary challenge of physics-informed machine learning (PIML) is its generalization beyond the training domain, especially when dealing with complex physical problems represented by partial differential equations (PDEs). This paper aims to enhance the generalization capabilities of PIML, facilitating practical, real-world applications where accurate predictions in unexplored regions are crucial. We leverage the inherent causality and temporal sequential characteristics of PDE solutions to fuse PIML models with recurrent neural architectures based on systems of ordinary differential equations, referred to as neural oscillators. Through effectively capturing long-time dependencies and mitigating the exploding and vanishing gradient problem, neural oscillators foster improved generalization in PIML tasks. Extensive experimentation involving time-dependent nonlinear PDEs and biharmonic beam equations demonstrates the efficacy of the proposed approach. Incorporating neural oscillators outperforms existing state-of-the-art methods on benchmark problems across various metrics. Consequently, the proposed method improves the generalization capabilities of PIML, providing accurate solutions for extrapolation and prediction beyond the training data",
    "checked": true,
    "id": "541382a655777d077498c0415ec42cf4fe524364",
    "semantic_title": "neural oscillators for generalization of physics-informed machine learning",
    "citation_count": 5,
    "authors": [
      "Taniya Kapoor",
      "Abhishek Chandra",
      "Daniel M. Tartakovsky",
      "Hongrui Wang",
      "Alfredo Nunez",
      "Rolf Dollevoet"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29205": {
    "title": "SHAP@k: Efficient and Probably Approximately Correct (PAC) Identification of Top-K Features",
    "volume": "main",
    "abstract": "The SHAP framework provides a principled method to explain the predictions of a model by computing feature importance. Motivated by applications in finance, we introduce the Top-k Identification Problem (TkIP) (and its ordered variant TkIP- O), where the objective is to identify the subset (or ordered subset for TkIP-O) of k features corresponding to the highest SHAP values with PAC guarantees. While any sampling-based method that estimates SHAP values (such as KernelSHAP and SamplingSHAP) can be trivially adapted to solve TkIP, doing so is highly sample inefficient. Instead, we leverage the connection between SHAP values and multi-armed bandits (MAB) to show that both TkIP and TkIP-O can be reduced to variants of problems in MAB literature. This reduction allows us to use insights from the MAB literature to develop sample-efficient variants of KernelSHAP and SamplingSHAP. We propose KernelSHAP@k and SamplingSHAP@k for solving TkIP; along with KernelSHAP-O and SamplingSHAP-O to solve the ordering problem in TkIP-O. We perform extensive experiments using several credit-related datasets to show that our methods offer significant improvements of up to 40× in sample efficiency and 39× in runtime",
    "checked": true,
    "id": "cc83c582cb7fe0afc3547da5ff35ad901ff566c5",
    "semantic_title": "shap@k: efficient and probably approximately correct (pac) identification of top-k features",
    "citation_count": 1,
    "authors": [
      "Sanjay Kariyappa",
      "Leonidas Tsepenekas",
      "Freddy Lécué",
      "Daniele Magazzeni"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29206": {
    "title": "Communication-Efficient Collaborative Regret Minimization in Multi-Armed Bandits",
    "volume": "main",
    "abstract": "In this paper, we study the collaborative learning model, which concerns the tradeoff between parallelism and communication overhead in multi-agent multi-armed bandits. For regret minimization in multi-armed bandits, we present the first set of tradeoffs between the number of rounds of communication between the agents and the regret of the collaborative learning process",
    "checked": true,
    "id": "ba9cb67dd7be95eaff53161f5668cc8cb1c97dca",
    "semantic_title": "communication-efficient collaborative regret minimization in multi-armed bandits",
    "citation_count": 0,
    "authors": [
      "Nikolai Karpov",
      "Qin Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29207": {
    "title": "Adversarially Balanced Representation for Continuous Treatment Effect Estimation",
    "volume": "main",
    "abstract": "Individual treatment effect (ITE) estimation requires adjusting for the covariate shift between populations with different treatments, and deep representation learning has shown great promise in learning a balanced representation of covariates. However the existing methods mostly consider the scenario of binary treatments. In this paper, we consider the more practical and challenging scenario in which the treatment is a continuous variable (e.g. dosage of a medication), and we address the two main challenges of this setup. We propose the adversarial counterfactual regression network (ACFR) that adversarially minimizes the representation imbalance in terms of KL divergence, and also maintains the impact of the treatment value on the outcome prediction by leveraging an attention mechanism. Theoretically we demonstrate that ACFR objective function is grounded in an upper bound on counterfactual outcome prediction error. Our experimental evaluation on semi-synthetic datasets demonstrates the empirical superiority of ACFR over a range of state-of-the-art methods",
    "checked": true,
    "id": "fc3e03820980ab8f0a8c13ecfcbff3da0b4c0e95",
    "semantic_title": "adversarially balanced representation for continuous treatment effect estimation",
    "citation_count": 2,
    "authors": [
      "Amirreza Kazemi",
      "Martin Ester"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29208": {
    "title": "Shaping Up SHAP: Enhancing Stability through Layer-Wise Neighbor Selection",
    "volume": "main",
    "abstract": "Machine learning techniques, such as deep learning and ensemble methods, are widely used in various domains due to their ability to handle complex real-world tasks. However, their black-box nature has raised multiple concerns about the fairness, trustworthiness, and transparency of computer-assisted decision-making. This has led to the emergence of local post-hoc explainability methods, which offer explanations for individual decisions made by black-box algorithms. Among these methods, Kernel SHAP is widely used due to its model-agnostic nature and its well-founded theoretical framework. Despite these strengths, Kernel SHAP suffers from high instability: different executions of the method with the same inputs can lead to significantly different explanations, which diminishes the relevance of the explanations. The contribution of this paper is two-fold. On the one hand, we show that Kernel SHAP's instability is caused by its stochastic neighbor selection procedure, which we adapt to achieve full stability without compromising explanation fidelity. On the other hand, we show that by restricting the neighbors generation to perturbations of size 1 -- which we call the coalitions of Layer 1 -- we obtain a novel feature-attribution method that is fully stable, computationally efficient, and still meaningful",
    "checked": true,
    "id": "91fcf84617ffed1a9cbc8026e4ae50af2170ee6b",
    "semantic_title": "shaping up shap: enhancing stability through layer-wise neighbor selection",
    "citation_count": 0,
    "authors": [
      "Gwladys Kelodjou",
      "Laurence Rozé",
      "Véronique Masson",
      "Luis Galárraga",
      "Romaric Gaudel",
      "Maurice Tchuente",
      "Alexandre Termier"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29209": {
    "title": "IOFM: Using the Interpolation Technique on the Over-Fitted Models to Identify Clean-Annotated Samples",
    "volume": "main",
    "abstract": "Most recent state-of-the-art algorithms for handling noisy label problems are based on the memorization effect, which is a phenomenon that deep neural networks (DNNs) memorize clean data before noisy ones. While the memorization effect can be a powerful tool, there are several cases where memorization effect does not occur. Examples are imbalanced class distributions and heavy contamination on labels. To address this limitation, we introduce a whole new approach called the interpolation with the over-fitted model (IOFM), which leverages over-fitted deep neural networks. The IOFM utilizes a new finding of over-fitted DNNs: for a given training sample, its neighborhoods chosen from the feature space are distributed differently on the original input space depending on the cleanness of the target sample. The IOFM has notable features in two aspects: 1) it yields superior results even when the training data are imbalanced or heavily noisy, 2) since we utilize over-fitted deep neural networks, a fine-tuning procedure to select the optimal training epoch, which is an essential yet sensitive factor for the success of the memorization effect, is not required, and thus, the IOFM can be used for non-experts. Through extensive experiments, we show that our method can serve as a promising alternative to existing solutions dealing with noisy labels, offering improved performance even in challenging situations",
    "checked": true,
    "id": "6d7b3f6862402e28771d3f68bcc9e4d29ce2a2ec",
    "semantic_title": "iofm: using the interpolation technique on the over-fitted models to identify clean-annotated samples",
    "citation_count": 0,
    "authors": [
      "Dongha Kim",
      "Yongchan Choi",
      "Kunwoong Kim",
      "Ilsang Ohn",
      "Yongdai Kim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29210": {
    "title": "When Model Meets New Normals: Test-Time Adaptation for Unsupervised Time-Series Anomaly Detection",
    "volume": "main",
    "abstract": "Time-series anomaly detection deals with the problem of detecting anomalous timesteps by learning normality from the sequence of observations. However, the concept of normality evolves over time, leading to a \"new normal problem\", where the distribution of normality can be changed due to the distribution shifts between training and test data. This paper highlights the prevalence of the new normal problem in unsupervised time-series anomaly detection studies. To tackle this issue, we propose a simple yet effective test-time adaptation strategy based on trend estimation and a self-supervised approach to learning new normalities during inference. Extensive experiments on real-world benchmarks demonstrate that incorporating the proposed strategy into the anomaly detector consistently improves the model's performances compared to the existing baselines, leading to robustness to the distribution shifts",
    "checked": true,
    "id": "aeba35c6f157ac4e83b6f6b1c6ceae7b7e40d324",
    "semantic_title": "when model meets new normals: test-time adaptation for unsupervised time-series anomaly detection",
    "citation_count": 2,
    "authors": [
      "Dongmin Kim",
      "Sunghyun  Park",
      "Jaegul Choo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29211": {
    "title": "Adaptive Shortcut Debiasing for Online Continual Learning",
    "volume": "main",
    "abstract": "We propose a novel framework DropTop that suppresses the shortcut bias in online continual learning (OCL) while being adaptive to the varying degree of the shortcut bias incurred by continuously changing environment. By the observed high-attention property of the shortcut bias, highly-activated features are considered candidates for debiasing. More importantly, resolving the limitation of the online environment where prior knowledge and auxiliary data are not ready, two novel techniques---feature map fusion and adaptive intensity shifting---enable us to automatically determine the appropriate level and proportion of the candidate shortcut features to be dropped. Extensive experiments on five benchmark datasets demonstrate that, when combined with various OCL algorithms, DropTop increases the average accuracy by up to 10.4% and decreases the forgetting by up to 63.2%",
    "checked": true,
    "id": "eaab62c1865f2359e98117f063e1190d16ec602a",
    "semantic_title": "adaptive shortcut debiasing for online continual learning",
    "citation_count": 0,
    "authors": [
      "Doyoung Kim",
      "Dongmin Park",
      "Yooju Shin",
      "Jihwan Bang",
      "Hwanjun Song",
      "Jae-Gil Lee"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29212": {
    "title": "MetaMix: Meta-State Precision Searcher for Mixed-Precision Activation Quantization",
    "volume": "main",
    "abstract": "Mixed-precision quantization of efficient networks often suffer from activation instability encountered in the exploration of bit selections. To address this problem, we propose a novel method called MetaMix which consists of bit selection and weight training phases. The bit selection phase iterates two steps, (1) the mixed-precision-aware weight update, and (2) the bit-search training with the fixed mixed-precision-aware weights, both of which combined reduce activation instability in mixed-precision quantization and contribute to fast and high-quality bit selection. The weight training phase exploits the weights and step sizes trained in the bit selection phase and fine-tunes them thereby offering fast training. Our experiments with efficient and hard-to-quantize networks, i.e., MobileNet v2 and v3, and ResNet-18 on ImageNet show that our proposed method pushes the boundary of mixed-precision quantization, in terms of accuracy vs. operations, by outperforming both mixed- and single-precision SOTA methods",
    "checked": true,
    "id": "a576bc6f271e565a156e6338a52af52f51ddaa72",
    "semantic_title": "metamix: meta-state precision searcher for mixed-precision activation quantization",
    "citation_count": 1,
    "authors": [
      "Han-Byul Kim",
      "Joo Hyung Lee",
      "Sungjoo Yoo",
      "Hong-Seok Kim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29213": {
    "title": "Curved Representation Space of Vision Transformers",
    "volume": "main",
    "abstract": "Neural networks with self-attention (a.k.a. Transformers) like ViT and Swin have emerged as a better alternative to traditional convolutional neural networks (CNNs). However, our understanding of how the new architecture works is still limited. In this paper, we focus on the phenomenon that Transformers show higher robustness against corruptions than CNNs, while not being overconfident. This is contrary to the intuition that robustness increases with confidence. We resolve this contradiction by empirically investigating how the output of the penultimate layer moves in the representation space as the input data moves linearly within a small area. In particular, we show the following. (1) While CNNs exhibit fairly linear relationship between the input and output movements, Transformers show nonlinear relationship for some data. For those data, the output of Transformers moves in a curved trajectory as the input moves linearly. (2) When a data is located in a curved region, it is hard to move it out of the decision region since the output moves along a curved trajectory instead of a straight line to the decision boundary, resulting in high robustness of Transformers. (3) If a data is slightly modified to jump out of the curved region, the movements afterwards become linear and the output goes to the decision boundary directly. In other words, there does exist a decision boundary near the data, which is hard to find only because of the curved representation space. This explains the underconfident prediction of Transformers. Also, we examine mathematical properties of the attention operation that induce nonlinear response to linear perturbation. Finally, we share our additional findings, regarding what contributes to the curved representation space of Transformers, and how the curvedness evolves during training",
    "checked": true,
    "id": "dc6a210c59befd2ed79298cc58552d20fc03057b",
    "semantic_title": "curved representation space of vision transformers",
    "citation_count": 3,
    "authors": [
      "Juyeop Kim",
      "Junha Park",
      "Songkuk Kim",
      "Jong-Seok Lee"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29214": {
    "title": "Robust Distributed Gradient Aggregation Using Projections onto Gradient Manifolds",
    "volume": "main",
    "abstract": "We study the distributed gradient aggregation problem where individual clients contribute to learning a central model by sharing parameter gradients constructed from local losses. However, errors in some gradients, caused by low-quality data or adversaries, can degrade the learning process when naively combined. Existing robust gradient aggregation approaches assume that local data represent the global data-generating distribution, which may not always apply to heterogeneous (non-i.i.d.) client data. We propose a new algorithm that can robustly aggregate gradients from potentially heterogeneous clients. Our approach leverages the manifold structure inherent in heterogeneous client gradients and evaluates gradient anomaly degrees by projecting them onto this manifold. This algorithm is implemented as a simple and efficient method that accumulates random projections within the subspace defined by the nearest neighbors within a gradient cloud. Our experiments demonstrate consistent performance improvements over state-of-the-art robust aggregation algorithms",
    "checked": true,
    "id": "5b444308ffd1e3fa02372aeafe5b5005267c59c9",
    "semantic_title": "robust distributed gradient aggregation using projections onto gradient manifolds",
    "citation_count": 0,
    "authors": [
      "Kwang In Kim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29215": {
    "title": "Stitching Sub-trajectories with Conditional Diffusion Model for Goal-Conditioned Offline RL",
    "volume": "main",
    "abstract": "Offline Goal-Conditioned Reinforcement Learning (Offline GCRL) is an important problem in RL that focuses on acquiring diverse goal-oriented skills solely from pre-collected behavior datasets. In this setting, the reward feedback is typically absent except when the goal is achieved, which makes it difficult to learn policies especially from a finite dataset of suboptimal behaviors. In addition, realistic scenarios involve long-horizon planning, which necessitates the extraction of useful skills within sub-trajectories. Recently, the conditional diffusion model has been shown to be a promising approach to generate high-quality long-horizon plans for RL. However, their practicality for the goal-conditioned setting is still limited due to a number of technical assumptions made by the methods. In this paper, we propose SSD (Sub-trajectory Stitching with Diffusion), a model-based offline GCRL method that leverages the conditional diffusion model to address these limitations. In summary, we use the diffusion model that generates future plans conditioned on the target goal and value, with the target value estimated from the goal-relabeled offline dataset. We report state-of-the-art performance in the standard benchmark set of GCRL tasks, and demonstrate the capability to successfully stitch the segments of suboptimal trajectories in the offline data to generate high-quality plans",
    "checked": true,
    "id": "a033d9ab7a726ed1325c80d52621032b90143349",
    "semantic_title": "stitching sub-trajectories with conditional diffusion model for goal-conditioned offline rl",
    "citation_count": 1,
    "authors": [
      "Sungyoon Kim",
      "Yunseon Choi",
      "Daiki E. Matsunaga",
      "Kee-Eung Kim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29216": {
    "title": "Cross-Class Feature Augmentation for Class Incremental Learning",
    "volume": "main",
    "abstract": "We propose a novel class incremental learning approach, which incorporates a feature augmentation technique motivated by adversarial attacks. We employ a classifier learned in the past to complement training examples of previous tasks. The proposed approach has an unique perspective to utilize the previous knowledge in class incremental learning since it augments features of arbitrary target classes using examples in other classes via adversarial attacks on a previously learned classifier. By allowing the Cross-Class Feature Augmentations (CCFA), each class in the old tasks conveniently populates samples in the feature space, which alleviates the collapse of the decision boundaries caused by sample deficiency for the previous tasks, especially when the number of stored exemplars is small. This idea can be easily incorporated into existing class incremental learning algorithms without any architecture modification. Extensive experiments on the standard benchmarks show that our method consistently outperforms existing class incremental learning methods by significant margins in various scenarios, especially under an environment with an extremely limited memory budget",
    "checked": true,
    "id": "1c987336125fa4f0ed4267165de5cf66fd7e6f5c",
    "semantic_title": "cross-class feature augmentation for class incremental learning",
    "citation_count": 0,
    "authors": [
      "Taehoon Kim",
      "Jaeyoo Park",
      "Bohyung Han"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29217": {
    "title": "Robust Policy Learning via Offline Skill Diffusion",
    "volume": "main",
    "abstract": "Skill-based reinforcement learning (RL) approaches have shown considerable promise, especially in solving long-horizon tasks via hierarchical structures. These skills, learned task-agnostically from offline datasets, can accelerate the policy learning process for new tasks. Yet, the application of these skills in different domains remains restricted due to their inherent dependency on the datasets, which poses a challenge when attempting to learn a skill-based policy via RL for a target domain different from the datasets' domains. In this paper, we present a novel offline skill learning framework DuSkill which employs a guided Diffusion model to generate versatile skills extended from the limited skills in datasets, thereby enhancing the robustness of policy learning for tasks in different domains. Specifically, we devise a guided diffusion-based skill decoder in conjunction with the hierarchical encoding to disentangle the skill embedding space into two distinct representations, one for encapsulating domain-invariant behaviors and the other for delineating the factors that induce domain variations in the behaviors. Our DuSkill framework enhances the diversity of skills learned offline, thus enabling to accelerate the learning procedure of high-level policies for different domains. Through experiments, we show that DuSkill outperforms other skill-based imitation learning and RL algorithms for several long-horizon tasks, demonstrating its benefits in few-shot imitation and online RL",
    "checked": true,
    "id": "2ece9399d6b7475c0b2745299f995275dbc120cf",
    "semantic_title": "robust policy learning via offline skill diffusion",
    "citation_count": 0,
    "authors": [
      "Woo Kyung Kim",
      "Minjong Yoo",
      "Honguk Woo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29218": {
    "title": "Relaxed Stationary Distribution Correction Estimation for Improved Offline Policy Optimization",
    "volume": "main",
    "abstract": "One of the major challenges of offline reinforcement learning (RL) is dealing with distribution shifts that stem from the mismatch between the trained policy and the data collection policy. Stationary distribution correction estimation algorithms (DICE) have addressed this issue by regularizing the policy optimization with f-divergence between the state-action visitation distributions of the data collection policy and the optimized policy. While such regularization naturally integrates to derive an objective to get optimal state-action visitation, such an implicit policy optimization framework has shown limited performance in practice. We observe that the reduced performance is attributed to the biased estimate and the properties of conjugate functions of f-divergence regularization. In this paper, we improve the regularized implicit policy optimization framework by relieving the bias and reshaping the conjugate function by relaxing the constraints. We show that the relaxation adjusts the degree of involvement of the sub-optimal samples in optimization, and we derive a new offline RL algorithm that benefits from the relaxed framework, improving from a previous implicit policy optimization algorithm by a large margin",
    "checked": true,
    "id": "ca5d80551936ea2aca2ee7797813e06902436a07",
    "semantic_title": "relaxed stationary distribution correction estimation for improved offline policy optimization",
    "citation_count": 0,
    "authors": [
      "Woosung Kim",
      "Donghyeon Ki",
      "Byung-Jun Lee"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29219": {
    "title": "Structure-Aware Multimodal Sequential Learning for Visual Dialog",
    "volume": "main",
    "abstract": "With the ability to collect vast amounts of image and natural language data from the web, there has been a remarkable advancement in Large-scale Language Models (LLMs). This progress has led to the emergence of chatbots and dialogue systems capable of fluent conversations with humans. As the variety of devices enabling interactions between humans and agents expands, and the performance of text-based dialogue systems improves, there has been recently proposed research on visual dialog. However, visual dialog requires understanding sequences of pairs consisting of images and sentences, making it challenging to gather sufficient data for training large-scale models from the web. In this paper, we propose a new multimodal learning method leveraging existing large-scale models designed for each modality, to enable model training for visual dialog with small visual dialog datasets. The key ideas of our approach are: 1) storing the history or context during the progression of visual dialog in the form of spatiotemporal graphs, and 2) introducing small modulation blocks between modality-specific models and the graphs to align the semantic spaces. For implementation, we introduce a novel structure-aware cross-attention method, which retrieves relevant image and text knowledge for utterance generation from the pretrained models. For experiments, we achieved a new state-of-the-art performance on three visual dialog datasets, including the most challenging one COMET",
    "checked": true,
    "id": "afda762674a99e19ca6f48ec0bfed4a24bb3c38d",
    "semantic_title": "structure-aware multimodal sequential learning for visual dialog",
    "citation_count": 0,
    "authors": [
      "Young-Jin Kim",
      "Min-Jun Kim",
      "Kyunghwan An",
      "Jinwoo Ahn",
      "Jaeseok Kim",
      "Yu-Jung Heo",
      "Du-Seong Chang",
      "Eun-Sol Kim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29220": {
    "title": "A Class of Topological Pseudodistances for Fast Comparison of Persistence Diagrams",
    "volume": "main",
    "abstract": "Persistence diagrams (PD)s play a central role in topological data analysis, and are used in an ever increasing variety of applications. The comparison of PD data requires computing distances among large sets of PDs, with metrics which are accurate, theoretically sound, and fast to compute. Especially for denser multi-dimensional PDs, such comparison metrics are lacking. While on the one hand, Wasserstein-type distances have high accuracy and theoretical guarantees, they incur high computational cost. On the other hand, distances between vectorizations such as Persistence Statistics (PS)s have lower computational cost, but lack the accuracy guarantees and theoretical properties of a true distance over PD space. In this work we introduce a class of pseudodistances called Extended Topological Pseudodistances (ETD)s, which have tunable complexity, and can approximate Sliced and classical Wasserstein distances at the high-complexity extreme, while being computationally lighter and close to Persistence Statistics at the lower complexity extreme, and thus allow users to interpolate between the two metrics. We build theoretical comparisons to show how to fit our new distances at an intermediate level between persistence vectorizations and Wasserstein distances. We also experimentally verify that ETDs outperform PSs in terms of accuracy and outperform Wasserstein and Sliced Wasserstein distances in terms of computational complexity",
    "checked": true,
    "id": "8cb0db7393250f33c20a77a0b3817c911a414a0c",
    "semantic_title": "a class of topological pseudodistances for fast comparison of persistence diagrams",
    "citation_count": 0,
    "authors": [
      "Rolando Kindelan Nuñez",
      "Mircea Petrache",
      "Mauricio Cerda",
      "Nancy Hitschfeld"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29221": {
    "title": "SALSA: Semantically-Aware Latent Space Autoencoder",
    "volume": "main",
    "abstract": "In deep learning for drug discovery, molecular representations are often based on sequences, known as SMILES, which allow for straightforward implementation of natural language processing methodologies, one being the sequence-to-sequence autoencoder. However, we observe that training an autoencoder solely on SMILES is insufficient to learn molecular representations that are semantically meaningful, where semantics are specified by the structural (graph-to-graph) similarities between molecules. We demonstrate by example that SMILES-based autoencoders may map structurally similar molecules to distant codes, resulting in an incoherent latent space that does not necessarily respect the semantic similarities between molecules. To address this shortcoming we propose Semantically-Aware Latent Space Autoencoder (SALSA) for molecular representations: a SMILES-based transformer autoencoder modified with a contrastive task aimed at learning graph-to-graph similarities between molecules. To accomplish this, we develop a novel dataset comprised of sets of structurally similar molecules and opt for a supervised contrastive loss that is able to incorporate full sets of positive samples. We evaluate semantic awareness of SALSA representations by comparing to its ablated counterparts, and show empirically that SALSA learns representations that maintain 1) structural awareness, 2) physicochemical awareness, 3) biological awareness, and 4) semantic continuity",
    "checked": true,
    "id": "26b5f05118b20aacc16ba1aac22034aa4ddb16e3",
    "semantic_title": "salsa: semantically-aware latent space autoencoder",
    "citation_count": 1,
    "authors": [
      "Kathryn E. Kirchoff",
      "Travis Maxfield",
      "Alexander Tropsha",
      "Shawn M. Gomez"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29222": {
    "title": "Principle Component Trees and Their Persistent Homology",
    "volume": "main",
    "abstract": "Low dimensional models like PCA are often used to simplify complex datasets by learning a single approximating subspace. This paradigm has expanded to union of subspaces models, like those learned by subspace clustering. In this paper, we present Principal Component Trees (PCTs), a graph structure that generalizes these ideas to identify mixtures of components that together describe the subspace structure of high-dimensional datasets. Each node in a PCT corresponds to a principal component of the data, and the edges between nodes indicate the components that must be mixed to produce a subspace that approximates a portion of the data. In order to construct PCTs, we propose two angle-distribution hypothesis tests to detect subspace clusters in the data. To analyze, compare, and select the best PCT model, we define two persistent homology measures that describe their shape. We show our construction yields two key properties of PCTs, namely ancestral orthogonality and non-decreasing singular values. Our main theoretical results show that learning PCTs reduces to PCA under multivariate normality, and that PCTs are efficient parameterizations of intersecting union of subspaces. Finally, we use PCTs to analyze neural network latent space, word embeddings, and reference image datasets",
    "checked": true,
    "id": "66d0ea43841a2052c831c161a94e9c2ac6b36fb6",
    "semantic_title": "principle component trees and their persistent homology",
    "citation_count": 0,
    "authors": [
      "Ben Kizaric",
      "Daniel Pimentel-Alarcón"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29223": {
    "title": "Pantypes: Diverse Representatives for Self-Explainable Models",
    "volume": "main",
    "abstract": "Prototypical self-explainable classifiers have emerged to meet the growing demand for interpretable AI systems. These classifiers are designed to incorporate high transparency in their decisions by basing inference on similarity with learned prototypical objects. While these models are designed with diversity in mind, the learned prototypes often do not sufficiently represent all aspects of the input distribution, particularly those in low density regions. Such lack of sufficient data representation, known as representation bias, has been associated with various detrimental properties related to machine learning diversity and fairness. In light of this, we introduce pantypes, a new family of prototypical objects designed to capture the full diversity of the input distribution through a sparse set of objects. We show that pantypes can empower prototypical self-explainable models by occupying divergent regions of the latent space and thus fostering high diversity, interpretability and fairness",
    "checked": true,
    "id": "a853b37c233906b14962e8079b998c97f88fada2",
    "semantic_title": "pantypes: diverse representatives for self-explainable models",
    "citation_count": 0,
    "authors": [
      "Rune Kjærsgaard",
      "Ahcène Boubekki",
      "Line Clemmensen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29224": {
    "title": "Shuffled Deep Regression",
    "volume": "main",
    "abstract": "Shuffled regression is the problem of learning regression models from shuffled data that consists of a set of input features and a set of target outputs where the correspondence between the input and output is unknown. This study proposes a new deep learning method for shuffled regression called Shuffled Deep Regression (SDR). We derive the sparse and stochastic variant of the Expectation-Maximization algorithm for SDR that iteratively updates discrete latent variables and the parameters of neural networks. The effectiveness of the proposal is confirmed by benchmark data experiments",
    "checked": true,
    "id": "fa02456b42f270ab9f4c89687f57e112b928a643",
    "semantic_title": "shuffled deep regression",
    "citation_count": 0,
    "authors": [
      "Masahiro Kohjima"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29225": {
    "title": "Approximating the Shapley Value without Marginal Contributions",
    "volume": "main",
    "abstract": "The Shapley value, which is arguably the most popular approach for assigning a meaningful contribution value to players in a cooperative game, has recently been used intensively in explainable artificial intelligence. Its meaningfulness is due to axiomatic properties that only the Shapley value satisfies, which, however, comes at the expense of an exact computation growing exponentially with the number of agents. Accordingly, a number of works are devoted to the efficient approximation of the Shapley value, most of them revolve around the notion of an agent's marginal contribution. In this paper, we propose with SVARM and Stratified SVARM two parameter-free and domain-independent approximation algorithms based on a representation of the Shapley value detached from the notion of marginal contribution. We prove unmatched theoretical guarantees regarding their approximation quality and provide empirical results including synthetic games as well as common explainability use cases comparing ourselves with state-of-the-art methods",
    "checked": true,
    "id": "ffb5d0aa78c28753d133b7b2ed9bccb3e8eb41b4",
    "semantic_title": "approximating the shapley value without marginal contributions",
    "citation_count": 10,
    "authors": [
      "Patrick Kolpaczki",
      "Viktor Bengs",
      "Maximilian Muschalik",
      "Eyke Hüllermeier"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29226": {
    "title": "Improved Bandits in Many-to-One Matching Markets with Incentive Compatibility",
    "volume": "main",
    "abstract": "Two-sided matching markets have been widely studied in the literature due to their rich applications. Since participants are usually uncertain about their preferences, online algorithms have recently been adopted to learn them through iterative interactions. An existing work initiates the study of this problem in a many-to-one setting with responsiveness. However, their results are far from optimal and lack guarantees of incentive compatibility. We first extend an existing algorithm for the one-to-one setting to this more general setting and show it achieves a near-optimal bound for player-optimal regret. Nevertheless, due to the substantial requirement for collaboration, a single player's deviation could lead to a huge increase in its own cumulative rewards and a linear regret for others. In this paper, we aim to enhance the regret bound in many-to-one markets while ensuring incentive compatibility. We first propose the adaptively explore-then-deferred-acceptance (AETDA) algorithm for responsiveness setting and derive an upper bound for player-optimal stable regret while demonstrating its guarantee of incentive compatibility. This result is a significant improvement over existing works. And to the best of our knowledge, it constitutes the first player-optimal guarantee in matching markets that offers such robust assurances. We also consider broader substitutable preferences, one of the most general conditions to ensure the existence of a stable matching and cover responsiveness. We devise an online DA (ODA) algorithm and establish an upper bound for the player-pessimal stable regret for this setting",
    "checked": true,
    "id": "92eaf8bad2d14ff1fc8d87a26fea9667019197cd",
    "semantic_title": "improved bandits in many-to-one matching markets with incentive compatibility",
    "citation_count": 1,
    "authors": [
      "Fang Kong",
      "Shuai Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29227": {
    "title": "Unknown-Aware Graph Regularization for Robust Semi-supervised Learning from Uncurated Data",
    "volume": "main",
    "abstract": "Recent advances in semi-supervised learning (SSL) have relied on the optimistic assumption that labeled and unlabeled data share the same class distribution. However, this assumption is often violated in real-world scenarios, where unlabeled data may contain out-of-class samples. SSL with such uncurated unlabeled data leads training models to be corrupted. In this paper, we propose a robust SSL method for learning from uncurated real-world data within the context of open-set semi-supervised learning (OSSL). Unlike previous works that rely on feature similarity distance, our method exploits uncertainty in logits. By leveraging task-dependent predictions of logits, our method is capable of robust learning even in the presence of highly correlated outliers. Our key contribution is to present an unknown-aware graph regularization (UAG), a novel technique that enhances the performance of uncertainty-based OSSL frameworks. The technique addresses not only the conflict between training objectives for inliers and outliers but also the limitation of applying the same training rule for all outlier classes, which are existed on previous uncertainty-based approaches. Extensive experiments demonstrate that UAG surpasses state-of-the-art OSSL methods by a large margin across various protocols. Codes are available at https://github.com/heejokong/UAGreg",
    "checked": true,
    "id": "b82ea48f183d1708229ead6ea820b55f4b3ef8e5",
    "semantic_title": "unknown-aware graph regularization for robust semi-supervised learning from uncurated data",
    "citation_count": 0,
    "authors": [
      "Heejo Kong",
      "Suneung Kim",
      "Ho-Joong Kim",
      "Seong-Whan Lee"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29228": {
    "title": "Ghost Noise for Regularizing Deep Neural Networks",
    "volume": "main",
    "abstract": "Batch Normalization (BN) is widely used to stabilize the optimization process and improve the test performance of deep neural networks. The regularization effect of BN depends on the batch size and explicitly using smaller batch sizes with Batch Normalization, a method known as Ghost Batch Normalization (GBN), has been found to improve generalization in many settings. We investigate the effectiveness of GBN by disentangling the induced ``Ghost Noise'' from normalization and quantitatively analyzing the distribution of noise as well as its impact on model performance. Inspired by our analysis, we propose a new regularization technique called Ghost Noise Injection (GNI) that imitates the noise in GBN without incurring the detrimental train-test discrepancy effects of small batch training. We experimentally show that GNI can provide a greater generalization benefit than GBN. Ghost Noise Injection can also be beneficial in otherwise non-noisy settings such as layer-normalized networks, providing additional evidence of the usefulness of Ghost Noise in Batch Normalization as a regularizer",
    "checked": true,
    "id": "f5dd8ab972e48b41121664ae05af0da17f1731a1",
    "semantic_title": "ghost noise for regularizing deep neural networks",
    "citation_count": 1,
    "authors": [
      "Atli Kosson",
      "Dongyang Fan",
      "Martin Jaggi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29229": {
    "title": "Zero-Shot Task Adaptation with Relevant Feature Information",
    "volume": "main",
    "abstract": "We propose a method to learn prediction models such as classifiers for unseen target tasks where labeled and unlabeled data are absent but a few relevant input features for solving the tasks are given. Although machine learning requires data for training, data are often difficult to collect in practice. On the other hand, for many applications, a few relevant features would be more easily obtained. Although zero-shot learning or zero-shot domain adaptation use external knowledge to adapt to unseen classes or tasks without data, relevant features have not been used in existing studies. The proposed method improves the generalization performance on the target tasks, where there are no data but a few relevant features are given, by meta-learning from labeled data in related tasks. In the meta-learning phase, it is essential to simulate test phases on target tasks where prediction model learning is required without data. To this end, our neural network-based prediction model is meta-learned such that it correctly responds to perturbations of the relevant features on randomly generated synthetic data. By this modeling, the prediction model can explicitly learn the discriminability of the relevant features without real target data. When unlabeled training data are available in the target tasks, the proposed method can incorporate such data to boost the performance in a unified framework. Our experiments demonstrate that the proposed method outperforms various existing methods with four real-world datasets",
    "checked": true,
    "id": "08727d6b6c0586df6751cbf2bc70c3c60dbd9029",
    "semantic_title": "zero-shot task adaptation with relevant feature information",
    "citation_count": 0,
    "authors": [
      "Atsutoshi Kumagai",
      "Tomoharu Iwata",
      "Yasuhiro Fujiwara"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29230": {
    "title": "Friendly Attacks to Improve Channel Coding Reliability",
    "volume": "main",
    "abstract": "This paper introduces a novel approach called \"friendly attack\" aimed at enhancing the performance of error correction channel codes. Inspired by the concept of adversarial attacks, our method leverages the idea of introducing slight perturbations to the neural network input, resulting in a substantial impact on the network's performance. By introducing small perturbations to fixed-point modulated codewords before transmission, we effectively improve the decoder's performance without violating the input power constraint. The perturbation design is accomplished by a modified iterative fast gradient method. This study investigates various decoder architectures suitable for computing gradients to obtain the desired perturbations. Specifically, we consider belief propagation (BP) for LDPC codes; the error correcting code transformer, BP and neural BP (NBP) for polar codes, and neural BCJR for convolutional codes. We demonstrate that the proposed friendly attack method can improve the reliability across different channels, modulations, codes, and decoders. This method allows us to increase the reliability of communication with a legacy receiver by simply modifying the transmitted codeword appropriately",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anastasiia Kurmukova",
      "Deniz Gunduz"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29231": {
    "title": "Evolving Parameterized Prompt Memory for Continual Learning",
    "volume": "main",
    "abstract": "Recent studies have demonstrated the potency of leveraging prompts in Transformers for continual learning (CL). Nevertheless, employing a discrete key-prompt bottleneck can lead to selection mismatches and inappropriate prompt associations during testing. Furthermore, this approach hinders adaptive prompting due to the lack of shareability among nearly identical instances at more granular level. To address these challenges, we introduce the Evolving Parameterized Prompt Memory (EvoPrompt), a novel method involving adaptive and continuous prompting attached to pre-trained Vision Transformer (ViT), conditioned on specific instance. We formulate a continuous prompt function as a neural bottleneck and encode the collection of prompts on network weights. We establish a paired prompt memory system consisting of a stable reference and a flexible working prompt memory. Inspired by linear mode connectivity, we progressively fuse the working prompt memory and reference prompt memory during inter-task periods, resulting in continually evolved prompt memory. This fusion involves aligning functionally equivalent prompts using optimal transport and aggregating them in parameter space with an adjustable bias based on prompt node attribution. Additionally, to enhance backward compatibility, we propose compositional classifier initialization, which leverages prior prototypes from pre-trained models to guide the initialization of new classifiers in a subspace-aware manner. Comprehensive experiments validate that our approach achieves state-of-the-art performance in both class and domain incremental learning scenarios",
    "checked": true,
    "id": "04d8a278de740c92f77c1e7a7ba68038a832731e",
    "semantic_title": "evolving parameterized prompt memory for continual learning",
    "citation_count": 2,
    "authors": [
      "Muhammad Rifki  Kurniawan",
      "Xiang Song",
      "Zhiheng Ma",
      "Yuhang He",
      "Yihong Gong",
      "Yang  Qi",
      "Xing Wei"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29232": {
    "title": "AesFA: An Aesthetic Feature-Aware Arbitrary Neural Style Transfer",
    "volume": "main",
    "abstract": "Neural style transfer (NST) has evolved significantly in recent years. Yet, despite its rapid progress and advancement, existing NST methods either struggle to transfer aesthetic information from a style effectively or suffer from high computational costs and inefficiencies in feature disentanglement due to using pre-trained models. This work proposes a lightweight but effective model, AesFA---Aesthetic Feature-Aware NST. The primary idea is to decompose the image via its frequencies to better disentangle aesthetic styles from the reference image while training the entire model in an end-to-end manner to exclude pre-trained models at inference completely. To improve the network's ability to extract more distinct representations and further enhance the stylization quality, this work introduces a new aesthetic feature: contrastive loss. Extensive experiments and ablations show the approach not only outperforms recent NST methods in terms of stylization quality, but it also achieves faster inference. Codes are available at https://github.com/Sooyyoungg/AesFA",
    "checked": true,
    "id": "16f8331f1e6d1bcab3643b818992c08aabcd03b8",
    "semantic_title": "aesfa: an aesthetic feature-aware arbitrary neural style transfer",
    "citation_count": 0,
    "authors": [
      "Joonwoo Kwon",
      "Sooyoung  Kim",
      "Yuewei Lin",
      "Shinjae Yoo",
      "Jiook Cha"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29233": {
    "title": "HDformer: A Higher-Dimensional Transformer for Detecting Diabetes Utilizing Long-Range Vascular Signals",
    "volume": "main",
    "abstract": "Diabetes mellitus is a global concern, and early detection can prevent serious complications. 50% of those with diabetes live undiagnosed, disproportionately afflicting low-income groups. Non-invasive methods have emerged for timely detection; however, their limited accuracy constrains clinical usage. In this research, we present a novel Higher Dimensional Transformer (HDformer), the first Transformer-based architecture which utilizes long-range photoplethysmography (PPG) to detect diabetes. The long-range PPG maximizes signal contextual information when compared to the less-than 30 second signals commonly used in existing research. To increase the computational efficiency of HDformer's long-range processing, a new attention module, Time Square Attention (TSA), is invented to achieve linear computational complexity with respect to the token volume while retaining the local/global dependencies. TSA converts the 1D inputs into 2D representations, grouping the adjacent points into a single 2D token. It then generates dynamic patches and feeds them into a gated mixture-of-experts (MoE) network, optimizing the learning on different attention areas. HDformer achieves state-of-the-art results (sensitivity 98.4, accuracy 97.3, specificity 92.8, AUC 0.929) on the standard MIMIC-III dataset, surpassing existing research. Furthermore, we develop an end-to-end solution where a low-cost wearable is prototyped to connect with the HDformer in the Cloud via a mobile app. This scalable, convenient, and affordable approach provides instantaneous detection and continuous monitoring for individuals. It aids doctors in easily screening for diabetes and safeguards underprivileged communities. The enhanced versatility of HDformer allows for efficient processing and learning of long-range signals in general one-dimensional time-series sequences, particularly for all biomedical waveforms",
    "checked": true,
    "id": "b4d3bddcd58c042a2a75c1ce467a8428d66630cb",
    "semantic_title": "hdformer: a higher-dimensional transformer for detecting diabetes utilizing long-range vascular signals",
    "citation_count": 0,
    "authors": [
      "Ella Lan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29234": {
    "title": "Generative Model Perception Rectification Algorithm for Trade-Off between Diversity and Quality",
    "volume": "main",
    "abstract": "How to balance the diversity and quality of results from generative models through perception rectification poses a significant challenge. Abnormal perception in generative models is typically caused by two factors: inadequate model structure and imbalanced data distribution. In response to this issue, we propose the dynamic model perception rectification algorithm (DMPRA) for generalized generative models. The core idea is to gain a comprehensive perception of the data in the generative model by appropriately highlighting the low-density samples in the perception space, also known as the minor group samples. The entire process can be summarized as \"search-evaluation-adjustment\". To identify low-density regions in the data manifold within the perception space of generative models, we introduce a filtering method based on extended neighborhood sampling. Based on the informational value of samples from low-density regions, our proposed mechanism generates informative weights to assess the significance of these samples in correcting the models' perception. By using dynamic adjustment, DMPRA ensures simultaneous enhancement of diversity and quality in the presence of imbalanced data distribution. Experimental results indicate that the algorithm has effectively improved Generative Adversarial Nets (GANs), Normalizing Flows (Flows), Variational Auto-Encoders (VAEs), and Diffusion Models (Diffusion)",
    "checked": true,
    "id": "91f094ebc25d619ddac374cec7ed05d1fd7ba5ae",
    "semantic_title": "generative model perception rectification algorithm for trade-off between diversity and quality",
    "citation_count": 0,
    "authors": [
      "Guipeng Lan",
      "Shuai Xiao",
      "Jiachen Yang",
      "Jiabao Wen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29235": {
    "title": "CoLAL: Co-learning Active Learning for Text Classification",
    "volume": "main",
    "abstract": "In the machine learning field, the challenge of effectively learning with limited data has become increasingly crucial. Active Learning (AL) algorithms play a significant role in this by enhancing model performance. We introduce a novel AL algorithm, termed Co-learning (CoLAL), designed to select the most diverse and representative samples within a training dataset. This approach utilizes noisy labels and predictions made by the primary model on unlabeled data. By leveraging a probabilistic graphical model, we combine two multi-class classifiers into a binary one. This classifier determines if both the main and the peer models agree on a prediction. If they do, the unlabeled sample is assumed to be easy to classify and is thus not beneficial to increase the target model's performance. We prioritize data that represents the unlabeled set without overlapping decision boundaries. The discrepancies between these boundaries can be estimated by the probability that two models result in the same prediction. Through theoretical analysis and experimental validation, we reveal that the integration of noisy labels into the peer model effectively identifies target model's potential inaccuracies. We evaluated the CoLAL method across seven benchmark datasets: four text datasets (AGNews, DBPedia, PubMed, SST-2) and text-based state-of-the-art (SOTA) baselines, and three image datasets (CIFAR100, MNIST, OpenML-155) and computer vision SOTA baselines. The results show that our CoLAL method significantly outperforms existing SOTA in text-based AL, and is competitive with SOTA image-based AL techniques",
    "checked": true,
    "id": "2be12359dfed0085c4333f52a48491df6a5ca0aa",
    "semantic_title": "colal: co-learning active learning for text classification",
    "citation_count": 0,
    "authors": [
      "Linh Le",
      "Genghong Zhao",
      "Xia Zhang",
      "Guido Zuccon",
      "Gianluca Demartini"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29236": {
    "title": "Doubly Perturbed Task Free Continual Learning",
    "volume": "main",
    "abstract": "Task-free online continual learning (TF-CL) is a challenging problem where the model incrementally learns tasks without explicit task information. Although training with entire data from the past, present as well as future is considered as the gold standard, naive approaches in TF-CL with the current samples may be conflicted with learning with samples in the future, leading to catastrophic forgetting and poor plasticity. Thus, a proactive consideration of an unseen future sample in TF-CL becomes imperative. Motivated by this intuition, we propose a novel TF-CL framework considering future samples and show that injecting adversarial perturbations on both input data and decision-making is effective. Then, we propose a novel method named Doubly Perturbed Continual Learning (DPCL) to efficiently implement these input and decision-making perturbations. Specifically, for input perturbation, we propose an approximate perturbation method that injects noise into the input data as well as the feature vector and then interpolates the two perturbed samples. For decision-making process perturbation, we devise multiple stochastic classifiers. We also investigate a memory management scheme and learning rate scheduling reflecting our proposed double perturbations. We demonstrate that our proposed method outperforms the state-of-the-art baseline methods by large margins on various TF-CL benchmarks",
    "checked": false,
    "id": "3bad26d71cd567fc3cc4adbbe494a5af12a0ce95",
    "semantic_title": "doubly perturbed task-free continual learning",
    "citation_count": 0,
    "authors": [
      "Byung Hyun Lee",
      "Min-hwan Oh",
      "Se Young Chun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29237": {
    "title": "OWQ: Outlier-Aware Weight Quantization for Efficient Fine-Tuning and Inference of Large Language Models",
    "volume": "main",
    "abstract": "Large language models (LLMs) with hundreds of billions of parameters require powerful server-grade GPUs for inference, limiting their practical deployment. To address this challenge, we introduce the outlier-aware weight quantization (OWQ) method, which aims to minimize LLM's footprint through low-precision representation. OWQ prioritizes a small subset of structured weights sensitive to quantization, storing them in high-precision, while applying highly tuned quantization to the remaining dense weights. This sensitivity-aware mixed-precision scheme reduces the quantization error notably, and extensive experiments demonstrate that 3.1-bit models using OWQ perform comparably to 4-bit models optimized by OPTQ. Furthermore, OWQ incorporates a parameter-efficient fine-tuning for task-specific adaptation, called weak column tuning (WCT), enabling accurate task-specific LLM adaptation with minimal memory overhead in the optimized format. OWQ represents a notable advancement in the flexibility, efficiency, and practicality of LLM optimization literature. The source code is available at https://github.com/xvyaward/owq",
    "checked": true,
    "id": "aa44b28b7c4c4a56d1f59ab4669215b667822c25",
    "semantic_title": "owq: outlier-aware weight quantization for efficient fine-tuning and inference of large language models",
    "citation_count": 12,
    "authors": [
      "Changhun Lee",
      "Jungyu Jin",
      "Taesu Kim",
      "Hyungjun Kim",
      "Eunhyeok Park"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29238": {
    "title": "DiSCO: Diffusion Schrödinger Bridge for Molecular Conformer Optimization",
    "volume": "main",
    "abstract": "The generation of energetically optimal 3D molecular conformers is crucial in cheminformatics and drug discovery. While deep generative models have been utilized for direct generation in Euclidean space, this approach encounters challenges, including the complexity of navigating a vast search space. Recent generative models that implement simplifications to circumvent these challenges have achieved state-of-the-art results, but this simplified approach unavoidably creates a gap between the generated conformers and the ground-truth conformational landscape. To bridge this gap, we introduce DiSCO: Diffusion Schrödinger Bridge for Molecular Conformer Optimization, a novel diffusion framework that enables direct learning of nonlinear diffusion processes in prior-constrained Euclidean space for the optimization of 3D molecular conformers. Through the incorporation of an SE(3)-equivariant Schrödinger bridge, we establish the roto-translational equivariance of the generated conformers. Our framework is model-agnostic and offers an easily implementable solution for the post hoc optimization of conformers produced by any generation method. Through comprehensive evaluations and analyses, we establish the strengths of our framework, substantiating the application of the Schrödinger bridge for molecular conformer optimization. First, our approach consistently outperforms four baseline approaches, producing conformers with higher diversity and improved quality. Then, we show that the intermediate conformers generated during our diffusion process exhibit valid and chemically meaningful characteristics. We also demonstrate the robustness of our method when starting from conformers of diverse quality, including those unseen during training. Lastly, we show that the precise generation of low-energy conformers via our framework helps in enhancing the downstream prediction of molecular properties. The code is available at https://github.com/Danyeong-Lee/DiSCO",
    "checked": true,
    "id": "991ebe4a9859d0a97301994880ee45bdecf2d316",
    "semantic_title": "disco: diffusion schrödinger bridge for molecular conformer optimization",
    "citation_count": 1,
    "authors": [
      "Danyeong Lee",
      "Dohoon Lee",
      "Dongmin Bang",
      "Sun Kim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29239": {
    "title": "Spear and Shield: Adversarial Attacks and Defense Methods for Model-Based Link Prediction on Continuous-Time Dynamic Graphs",
    "volume": "main",
    "abstract": "Real-world graphs are dynamic, constantly evolving with new interactions, such as financial transactions in financial networks. Temporal Graph Neural Networks (TGNNs) have been developed to effectively capture the evolving patterns in dynamic graphs. While these models have demonstrated their superiority, being widely adopted in various important fields, their vulnerabilities against adversarial attacks remain largely unexplored. In this paper, we propose T-SPEAR, a simple and effective adversarial attack method for link prediction on continuous-time dynamic graphs, focusing on investigating the vulnerabilities of TGNNs. Specifically, before the training procedure of a victim model, which is a TGNN for link prediction, we inject edge perturbations to the data that are unnoticeable in terms of the four constraints we propose, and yet effective enough to cause malfunction of the victim model. Moreover, we propose a robust training approach T-SHIELD to mitigate the impact of adversarial attacks. By using edge filtering and enforcing temporal smoothness to node embeddings, we enhance the robustness of the victim model. Our experimental study shows that T-SPEAR significantly degrades the victim model's performance on link prediction tasks, and even more, our attacks are transferable to other TGNNs, which differ from the victim model assumed by the attacker. Moreover, we demonstrate that T-SHIELD effectively filters out adversarial edges and exhibits robustness against adversarial attacks, surpassing the link prediction performance of the naive TGNN by up to 11.2% under T-SPEAR. The code and datasets are available at https://github.com/wooner49/T-spear-shield",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongjin Lee",
      "Juho Lee",
      "Kijung Shin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29240": {
    "title": "The Choice of Noninformative Priors for Thompson Sampling in Multiparameter Bandit Models",
    "volume": "main",
    "abstract": "Thompson sampling (TS) has been known for its outstanding empirical performance supported by theoretical guarantees across various reward models in the classical stochastic multi-armed bandit problems. Nonetheless, its optimality is often restricted to specific priors due to the common observation that TS is fairly insensitive to the choice of the prior when it comes to asymptotic regret bounds. However, when the model contains multiple parameters, the optimality of TS highly depends on the choice of priors, which casts doubt on the generalizability of previous findings to other models. To address this gap, this study explores the impact of selecting noninformative priors, offering insights into the performance of TS when dealing with new models that lack theoretical understanding. We first extend the regret analysis of TS to the model of uniform distributions with unknown supports, which would be the simplest non-regular model. Our findings reveal that changing noninformative priors can significantly affect the expected regret, aligning with previously known results in other multiparameter bandit models. Although the uniform prior is shown to be optimal, we highlight the inherent limitation of its optimality, which is limited to specific parameterizations and emphasizes the significance of the invariance property of priors. In light of this limitation, we propose a slightly modified TS-based policy, called TS with Truncation (TS-T), which can achieve the asymptotic optimality for the Gaussian models and the uniform models by using the reference prior and the Jeffreys prior that are invariant under one-to-one reparameterizations. This policy provides an alternative approach to achieving optimality by employing fine-tuned truncation, which would be much easier than hunting for optimal priors in practice",
    "checked": true,
    "id": "258f1a3d8a064ea309dad6c8884da11b81c459a9",
    "semantic_title": "the choice of noninformative priors for thompson sampling in multiparameter bandit models",
    "citation_count": 0,
    "authors": [
      "Jongyeong Lee",
      "Chao-Kai Chiang",
      "Masashi Sugiyama"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29241": {
    "title": "Learning Uncertainty-Aware Temporally-Extended Actions",
    "volume": "main",
    "abstract": "In reinforcement learning, temporal abstraction in the action space, exemplified by action repetition, is a technique to facilitate policy learning through extended actions. However, a primary limitation in previous studies of action repetition is its potential to degrade performance, particularly when sub-optimal actions are repeated. This issue often negates the advantages of action repetition. To address this, we propose a novel algorithm named Uncertainty-aware Temporal Extension (UTE). UTE employs ensemble methods to accurately measure uncertainty during action extension. This feature allows policies to strategically choose between emphasizing exploration or adopting an uncertainty-averse approach, tailored to their specific needs. We demonstrate the effectiveness of UTE through experiments in Gridworld and Atari 2600 environments. Our findings show that UTE outperforms existing action repetition algorithms, effectively mitigating their inherent limitations and significantly enhancing policy learning efficiency",
    "checked": true,
    "id": "eedf0e644545d12b1bcebbc9dbf00ca92c79d169",
    "semantic_title": "learning uncertainty-aware temporally-extended actions",
    "citation_count": 0,
    "authors": [
      "Joongkyu Lee",
      "Seung Joon Park",
      "Yunhao Tang",
      "Min-hwan Oh"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29242": {
    "title": "Any-Way Meta Learning",
    "volume": "main",
    "abstract": "Although meta-learning seems promising performance in the realm of rapid adaptability, it is constrained by fixed cardinality. When faced with tasks of varying cardinalities that were unseen during training, the model lacks its ability. In this paper, we address and resolve this challenge by harnessing `label equivalence' emerged from stochastic numeric label assignments during episodic task sampling. Questioning what defines ``true\" meta-learning, we introduce the ``any-way\" learning paradigm, an innovative model training approach that liberates model from fixed cardinality constraints. Surprisingly, this model not only matches but often outperforms traditional fixed-way models in terms of performance, convergence speed, and stability. This disrupts established notions about domain generalization. Furthermore, we argue that the inherent label equivalence naturally lacks semantic information. To bridge this semantic information gap arising from label equivalence, we further propose a mechanism for infusing semantic class information into the model. This would enhance the model's comprehension and functionality. Experiments conducted on renowned architectures like MAML and ProtoNet affirm the effectiveness of our method",
    "checked": true,
    "id": "6eaaa9b11450937fccc7e70588b43b8d76b07f97",
    "semantic_title": "any-way meta learning",
    "citation_count": 0,
    "authors": [
      "JunHoo Lee",
      "Yearim Kim",
      "Hyunho Lee",
      "Nojun Kwak"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29243": {
    "title": "Mixed-Effects Contextual Bandits",
    "volume": "main",
    "abstract": "We study a novel variant of a contextual bandit problem with multi-dimensional reward feedback formulated as a mixed-effects model, where the correlations between multiple feedback are induced by sharing stochastic coefficients called random effects. We propose a novel algorithm, Mixed-Effects Contextual UCB (ME-CUCB), achieving tildeO(d sqrt(mT)) regret bound after T rounds where d is the dimension of contexts and m is the dimension of outcomes, with either known or unknown covariance structure. This is a tighter regret bound than that of the naive canonical linear bandit algorithm ignoring the correlations among rewards. We prove a lower bound of Omega(d sqrt(mT)) matching the upper bound up to logarithmic factors. To our knowledge, this is the first work providing a regret analysis for mixed-effects models and algorithms involving weighted least-squares estimators. Our theoretical analysis faces a significant technical challenge in that the error terms do not constitute martingales since the weights depend on the rewards. We overcome this challenge by using covering numbers, of theoretical interest in its own right. We provide numerical experiments demonstrating the advantage of our proposed algorithm, supporting the theoretical claims",
    "checked": true,
    "id": "08a668bed6eb3bef7227f672b8d6e42e533cb826",
    "semantic_title": "mixed-effects contextual bandits",
    "citation_count": 1,
    "authors": [
      "Kyungbok Lee",
      "Myunghee Cho Paik",
      "Min-hwan Oh",
      "Gi-Soo Kim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29244": {
    "title": "Proxyformer: Nyström-Based Linear Transformer with Trainable Proxy Tokens",
    "volume": "main",
    "abstract": "Transformer-based models have demonstrated remarkable performance in various domains, including natural language processing, image processing and generative modeling. The most significant contributor to the successful performance of Transformer models is the self-attention mechanism, which allows for a comprehensive understanding of the interactions between tokens in the input sequence. However, there is a well-known scalability issue, the quadratic dependency (i.e. O(n^2)) of self-attention operations on the input sequence length n, making the handling of lengthy sequences challenging. To address this limitation, there has been a surge of research on efficient transformers, aiming to alleviate the quadratic dependency on the input sequence length. Among these, the Nyströmformer, which utilizes the Nyström method to decompose the attention matrix, achieves superior performance in both accuracy and throughput. However, its landmark selection exhibits redundancy, and the model incurs computational overhead when calculating the pseudo-inverse matrix. We propose a novel Nyström method-based transformer, called Proxyformer. Unlike the traditional approach of selecting landmarks from input tokens, the Proxyformer utilizes trainable neural memory, called proxy tokens, for landmarks. By integrating contrastive learning, input injection, and a specialized dropout for the decomposed matrix, Proxyformer achieves top-tier performance for long sequence tasks in the Long Range Arena benchmark",
    "checked": true,
    "id": "77386366ddbb5e446f13d4b766893b1419559775",
    "semantic_title": "proxyformer: nyström-based linear transformer with trainable proxy tokens",
    "citation_count": 1,
    "authors": [
      "Sangho Lee",
      "Hayun Lee",
      "Dongkun Shin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29245": {
    "title": "Multi-Architecture Multi-Expert Diffusion Models",
    "volume": "main",
    "abstract": "In this paper, we address the performance degradation of efficient diffusion models by introducing Multi-architecturE Multi-Expert diffusion models (MEME). We identify the need for tailored operations at different time-steps in diffusion processes and leverage this insight to create compact yet high-performing models. MEME assigns distinct architectures to different time-step intervals, balancing convolution and self-attention operations based on observed frequency characteristics. We also introduce a soft interval assignment strategy for comprehensive training. Empirically, MEME operates 3.3 times faster than baselines while improving image generation quality (FID scores) by 0.62 (FFHQ) and 0.37 (CelebA). Though we validate the effectiveness of assigning more optimal architecture per time-step, where efficient models outperform the larger models, we argue that MEME opens a new design choice for diffusion models that can be easily applied in other scenarios, such as large multi-expert models",
    "checked": true,
    "id": "94f09cfdc0bad324733a25a3a4a1919975c21afd",
    "semantic_title": "multi-architecture multi-expert diffusion models",
    "citation_count": 14,
    "authors": [
      "Yunsung Lee",
      "JinYoung Kim",
      "Hyojun Go",
      "Myeongho Jeong",
      "Shinhyeok Oh",
      "Seungtaek Choi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29246": {
    "title": "PC-Conv: Unifying Homophily and Heterophily with Two-Fold Filtering",
    "volume": "main",
    "abstract": "Recently, many carefully designed graph representation learning methods have achieved impressive performance on either strong heterophilic or homophilic graphs, but not both. Therefore, they are incapable of generalizing well across real-world graphs with different levels of homophily. This is attributed to their neglect of homophily in heterophilic graphs, and vice versa. In this paper, we propose a two-fold filtering mechanism to mine homophily in heterophilic graphs, and vice versa. In particular, we extend the graph heat equation to perform heterophilic aggregation of global information from a long distance. The resultant filter can be exactly approximated by the Possion-Charlier (PC) polynomials. To further exploit information at multiple orders, we introduce a powerful graph convolution PC-Conv and its instantiation PCNet for the node classification task. Compared to the state-of-the-art GNNs, PCNet shows competitive performance on well-known homophilic and heterophilic graphs. Our implementation is available at https://github.com/uestclbh/PC-Conv",
    "checked": true,
    "id": "8eb5594ee315aee37900aefc24a0b7600feee2c8",
    "semantic_title": "pc-conv: unifying homophily and heterophily with two-fold filtering",
    "citation_count": 6,
    "authors": [
      "Bingheng Li",
      "Erlin Pan",
      "Zhao Kang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29247": {
    "title": "All Beings Are Equal in Open Set Recognition",
    "volume": "main",
    "abstract": "In open-set recognition (OSR), a promising strategy is exploiting pseudo-unknown data outside given K known classes as an additional K+1-th class to explicitly model potential open space. However, treating unknown classes without distinction is unequal for them relative to known classes due to the category-agnostic and scale-agnostic of the unknowns. This inevitably not only disrupts the inherent distributions of unknown classes but also incurs both class-wise and instance-wise imbalances between known and unknown classes. Ideally, the OSR problem should model the whole class space as K+∞, but enumerating all unknowns is impractical. Since the core of OSR is to effectively model the boundaries of known classes, this means just focusing on the unknowns nearing the boundaries of targeted known classes seems sufficient. Thus, as a compromise, we convert the open classes from infinite to K, with a novel concept Target-Aware Universum (TAU) and propose a simple yet effective framework Dual Contrastive Learning with Target-Aware Universum (DCTAU). In details, guided by the targeted known classes, TAU automatically expands the unknown classes from the previous 1 to K, effectively alleviating the distribution disruption and the imbalance issues mentioned above. Then, a novel Dual Contrastive (DC) loss is designed, where all instances irrespective of known or TAU are considered as positives to contrast with their respective negatives. Experimental results indicate DCTAU sets a new state-of-the-art",
    "checked": true,
    "id": "f7550372ffcdcd71ecc2719c07bc0f5697f9f341",
    "semantic_title": "all beings are equal in open set recognition",
    "citation_count": 0,
    "authors": [
      "Chaohua Li",
      "Enhao Zhang",
      "Chuanxing Geng",
      "Songcan Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29248": {
    "title": "GxVAEs: Two Joint VAEs Generate Hit Molecules from Gene Expression Profiles",
    "volume": "main",
    "abstract": "The de novo generation of hit-like molecules that show bioactivity and drug-likeness is an important task in computer-aided drug discovery. Although artificial intelligence can generate molecules with desired chemical properties, most previous studies have ignored the influence of disease-related cellular environments. This study proposes a novel deep generative model called GxVAEs to generate hit-like molecules from gene expression profiles by leveraging two joint variational autoencoders (VAEs). The first VAE, ProfileVAE, extracts latent features from gene expression profiles. The extracted features serve as the conditions that guide the second VAE, which is called MolVAE, in generating hit-like molecules. GxVAEs bridge the gap between molecular generation and the cellular environment in a biological system, and produce molecules that are biologically meaningful in the context of specific diseases. Experiments and case studies on the generation of therapeutic molecules show that GxVAEs outperforms current state-of-the-art baselines and yield hit-like molecules with potential bioactivity and drug-like properties. We were able to successfully generate the potential molecular structures with therapeutic effects for various diseases from patients' disease profiles",
    "checked": true,
    "id": "3e0581204f76877b7408c7492cbdaa81c1d14531",
    "semantic_title": "gxvaes: two joint vaes generate hit molecules from gene expression profiles",
    "citation_count": 0,
    "authors": [
      "Chen Li",
      "Yoshihiro Yamanishi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29249": {
    "title": "Towards Continual Learning Desiderata via HSIC-Bottleneck Orthogonalization and Equiangular Embedding",
    "volume": "main",
    "abstract": "Deep neural networks are susceptible to catastrophic forgetting when trained on sequential tasks. Various continual learning (CL) methods often rely on exemplar buffers or/and network expansion for balancing model stability and plasticity, which, however, compromises their practical value due to privacy and memory concerns. Instead, this paper considers a strict yet realistic setting, where the training data from previous tasks is unavailable and the model size remains relatively constant during sequential training. To achieve such desiderata, we propose a conceptually simple yet effective method that attributes forgetting to layer-wise parameter overwriting and the resulting decision boundary distortion. This is achieved by the synergy between two key components: HSIC-Bottleneck Orthogonalization (HBO) implements non-overwritten parameter updates mediated by Hilbert-Schmidt independence criterion in an orthogonal space and EquiAngular Embedding (EAE) enhances decision boundary adaptation between old and new tasks with predefined basis vectors. Extensive experiments demonstrate that our method achieves competitive accuracy performance, even with absolute superiority of zero exemplar buffer and 1.02x the base model",
    "checked": true,
    "id": "ee8123e41d17aba5b0da9e57c4b10db73cb2ff19",
    "semantic_title": "towards continual learning desiderata via hsic-bottleneck orthogonalization and equiangular embedding",
    "citation_count": 1,
    "authors": [
      "Depeng Li",
      "Tianqi Wang",
      "Junwei Chen",
      "Qining Ren",
      "Kenji Kawaguchi",
      "Zhigang Zeng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29250": {
    "title": "Regroup Median Loss for Combating Label Noise",
    "volume": "main",
    "abstract": "The deep model training procedure requires large-scale datasets of annotated data. Due to the difficulty of annotating a large number of samples, label noise caused by incorrect annotations is inevitable, resulting in low model performance and poor model generalization. To combat label noise, current methods usually select clean samples based on the small-loss criterion and use these samples for training. Due to some noisy samples similar to clean ones, these small-loss criterion-based methods are still affected by label noise. To address this issue, in this work, we propose Regroup Median Loss (RML) to reduce the probability of selecting noisy samples and correct losses of noisy samples. RML randomly selects samples with the same label as the training samples based on a new loss processing method. Then, we combine the stable mean loss and the robust median loss through a proposed regrouping strategy to obtain robust loss estimation for noisy samples. To further improve the model performance against label noise, we propose a new sample selection strategy and build a semi-supervised method based on RML. Compared to state-of-the-art methods, for both the traditionally trained and semi-supervised models, RML achieves a significant improvement on synthetic and complex real-world datasets. The source is at https://github.com/Feng-peng-Li/Regroup-Loss-Median-to-Combat-Label-Noise",
    "checked": true,
    "id": "0555abb21de07be44d1361aeff0c9b501c721614",
    "semantic_title": "regroup median loss for combating label noise",
    "citation_count": 0,
    "authors": [
      "Fengpeng Li",
      "Kemou Li",
      "Jinyu Tian",
      "Jiantao Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29251": {
    "title": "Parsing All Adverse Scenes: Severity-Aware Semantic Segmentation with Mask-Enhanced Cross-Domain Consistency",
    "volume": "main",
    "abstract": "Although recent methods in Unsupervised Domain Adaptation (UDA) have achieved success in segmenting rainy or snowy scenes by improving consistency, they face limitations when dealing with more challenging scenarios like foggy and night scenes. We argue that these prior methods excessively focus on weather-specific features in adverse scenes, which exacerbates the existing domain gaps. To address this issue, we propose a new metric to evaluate the severity of all adverse scenes and offer a novel perspective that enables task unification across all adverse scenarios. Our method focuses on Severity, allowing our model to learn more consistent features and facilitate domain distribution alignment, thereby alleviating domain gaps. Unlike the vague descriptions of consistency in previous methods, we introduce Cross-domain Consistency, which is quantified using the Structure Similarity Index Measure (SSIM) to measure the distance between the source and target domains. Specifically, our unified model consists of two key modules: the Merging Style Augmentation Module (MSA) and the Severity Perception Mask Module (SPM). The MSA module transforms all adverse scenes into augmented scenes, effectively eliminating weather-specific features and enhancing Cross-domain Consistency. The SPM module incorporates a Severity Perception mechanism, guiding a Mask operation that enables our model to learn highly consistent features from the augmented scenes. Our unified framework, named PASS (Parsing All adverSe Scenes), achieves significant performance improvements over state-of-the-art methods on widely-used benchmarks for all adverse scenes. Notably, the performance of PASS is superior to Semi-Unified models and even surpasses weather-specific models",
    "checked": true,
    "id": "a42620480bca28d02a0bb238bde6f4c6dfaa7882",
    "semantic_title": "parsing all adverse scenes: severity-aware semantic segmentation with mask-enhanced cross-domain consistency",
    "citation_count": 2,
    "authors": [
      "Fuhao Li",
      "Ziyang Gong",
      "Yupeng Deng",
      "Xianzheng Ma",
      "Renrui Zhang",
      "Zhenming Ji",
      "Xiangwei Zhu",
      "Hong Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29252": {
    "title": "Learning Spatially Collaged Fourier Bases for Implicit Neural Representation",
    "volume": "main",
    "abstract": "Existing approaches to Implicit Neural Representation (INR) can be interpreted as a global scene representation via a linear combination of Fourier bases of different frequencies. However, such universal basis functions can limit the representation capability in local regions where a specific component is unnecessary, resulting in unpleasant artifacts. To this end, we introduce a learnable spatial mask that effectively dispatches distinct Fourier bases into respective regions. This translates into collaging Fourier patches, thus enabling an accurate representation of complex signals. Comprehensive experiments demonstrate the superior reconstruction quality of the proposed approach over existing baselines across various INR tasks, including image fitting, video representation, and 3D shape representation. Our method outperforms all other baselines, improving the image fitting PSNR by over 3dB and 3D reconstruction to 98.81 IoU and 0.0011 Chamfer Distance",
    "checked": true,
    "id": "3f0506a037fa3a435b8c76d3ef915db5b9447f04",
    "semantic_title": "learning spatially collaged fourier bases for implicit neural representation",
    "citation_count": 2,
    "authors": [
      "Jason Chun Lok Li",
      "Chang Liu",
      "Binxiao Huang",
      "Ngai Wong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29253": {
    "title": "High-Dimensional Analysis for Generalized Nonlinear Regression: From Asymptotics to Algorithm",
    "volume": "main",
    "abstract": "Overparameterization often leads to benign overfitting, where deep neural networks can be trained to overfit the training data but still generalize well on unseen data. However, it lacks a generalized asymptotic framework for nonlinear regressions and connections to conventional complexity notions. In this paper, we propose a generalized high-dimensional analysis for nonlinear regression models, including various nonlinear feature mapping methods and subsampling. Specifically, we first provide an implicit regularization parameter and asymptotic equivalents related to a classical complexity notion, i.e., effective dimension. We then present a high-dimensional analysis for nonlinear ridge regression and extend it to ridgeless regression in the under-parameterized and over-parameterized regimes, respectively. We find that the limiting risks decrease with the effective dimension. Motivated by these theoretical findings, we propose an algorithm, namely RFRed, to improve generalization ability. Finally, we validate our theoretical findings and the proposed algorithm through several experiments",
    "checked": true,
    "id": "fc0b0467ee9c786d1869560cfca306ac9112fb66",
    "semantic_title": "high-dimensional analysis for generalized nonlinear regression: from asymptotics to algorithm",
    "citation_count": 1,
    "authors": [
      "Jian Li",
      "Yong Liu",
      "Weiping Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29254": {
    "title": "FedNS: A Fast Sketching Newton-Type Algorithm for Federated Learning",
    "volume": "main",
    "abstract": "Recent Newton-type federated learning algorithms have demonstrated linear convergence with respect to the communication rounds. However, communicating Hessian matrices is often unfeasible due to their quadratic communication complexity. In this paper, we introduce a novel approach to tackle this issue while still achieving fast convergence rates. Our proposed method, named as Federated Newton Sketch methods (FedNS), approximates the centralized Newton's method by communicating the sketched square-root Hessian instead of the exact Hessian. To enhance communication efficiency, we reduce the sketch size to match the effective dimension of the Hessian matrix. We provide convergence analysis based on statistical learning for the federated Newton sketch approaches. Specifically, our approaches reach super-linear convergence rates w.r.t. the communication rounds for the first time. We validate the effectiveness of our algorithms through various experiments, which coincide with our theoretical findings",
    "checked": true,
    "id": "f3cae8dd01b5da7b44921c5ed3ada6048dacb513",
    "semantic_title": "fedns: a fast sketching newton-type algorithm for federated learning",
    "citation_count": 0,
    "authors": [
      "Jian Li",
      "Yong Liu",
      "Weiping Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29255": {
    "title": "Hierarchical Topology Isomorphism Expertise Embedded Graph Contrastive Learning",
    "volume": "main",
    "abstract": "Graph contrastive learning (GCL) aims to align the positive features while differentiating the negative features in the latent space by minimizing a pair-wise contrastive loss. As the embodiment of an outstanding discriminative unsupervised graph representation learning approach, GCL achieves impressive successes in various graph benchmarks. However, such an approach falls short of recognizing the topology isomorphism of graphs, resulting in that graphs with relatively homogeneous node features cannot be sufficiently discriminated. By revisiting classic graph topology recognition works, we disclose that the corresponding expertise intuitively complements GCL methods. To this end, we propose a novel hierarchical topology isomorphism expertise embedded graph contrastive learning, which introduces knowledge distillations to empower GCL models to learn the hierarchical topology isomorphism expertise, including the graph-tier and subgraph-tier. On top of this, the proposed method holds the feature of plug-and-play, and we empirically demonstrate that the proposed method is universal to multiple state-of-the-art GCL models. The solid theoretical analyses are further provided to prove that compared with conventional GCL methods, our method acquires the tighter upper bound of Bayes classification error. We conduct extensive experiments on real-world benchmarks to exhibit the performance superiority of our method over candidate GCL methods, e.g., for the real-world graph representation learning experiments, the proposed method beats the state-of-the-art method by 0.23% on unsupervised representation learning setting, 0.43% on transfer learning setting. Our code is available at https://github.com/jyf123/HTML",
    "checked": true,
    "id": "fcc9360992c3942d98a1cfea528d909c836d0a44",
    "semantic_title": "hierarchical topology isomorphism expertise embedded graph contrastive learning",
    "citation_count": 1,
    "authors": [
      "Jiangmeng Li",
      "Yifan Jin",
      "Hang Gao",
      "Wenwen Qiang",
      "Changwen Zheng",
      "Fuchun Sun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29256": {
    "title": "Curriculum-Enhanced Residual Soft An-Isotropic Normalization for Over-Smoothness in Deep GNNs",
    "volume": "main",
    "abstract": "Despite Graph neural networks' significant performance gain over many classic techniques in various graph-related downstream tasks, their successes are restricted in shallow models due to over-smoothness and the difficulties of optimizations among many other issues. In this paper, to alleviate the over-smoothing issue, we propose a soft graph normalization method to preserve the diversities of node embeddings and prevent indiscrimination due to possible over-closeness. Combined with residual connections, we analyze the reason why the method can effectively capture the knowledge in both input graph structures and node features even with deep networks. Additionally, inspired by Curriculum Learning that learns easy examples before the hard ones, we propose a novel label-smoothing-based learning framework to enhance the optimization of deep GNNs, which iteratively smooths labels in an auxiliary graph and constructs many gradual non-smooth tasks for extracting increasingly complex knowledge and gradually discriminating nodes from coarse to fine. The method arguably reduces the risk of overfitting and generalizes better results. Finally, extensive experiments are carried out to demonstrate the effectiveness and potential of the proposed model and learning framework through comparison with twelve existing baselines including the state-of-the-art methods on twelve real-world node classification benchmarks",
    "checked": true,
    "id": "9d7c68dcb9fea45f6049ac5663bcfd670c517cd2",
    "semantic_title": "curriculum-enhanced residual soft an-isotropic normalization for over-smoothness in deep gnns",
    "citation_count": 0,
    "authors": [
      "Jin Li",
      "Qirong Zhang",
      "Shuling Xu",
      "Xinlong Chen",
      "Longkun Guo",
      "Yang-Geng Fu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29257": {
    "title": "Tensorized Label Learning on Anchor Graph",
    "volume": "main",
    "abstract": "Graph-based multimedia data clustering has attracted much attention due to the impressive clustering performance for arbitrarily shaped multimedia data. However, existing graph-based clustering methods need post-processing to get labels for multimedia data with high computational complexity. Moreover, it is sub-optimal for label learning due to the fact that they exploit the complementary information embedded in data with different types pixel by pixel. To handle these problems, we present a novel label learning model with good interpretability for clustering. To be specific, our model decomposes anchor graph into the products of two matrices with orthogonal non-negative constraint to directly get soft label without any post-processing, which remarkably reduces the computational complexity. To well exploit the complementary information embedded in multimedia data, we introduce tensor Schatten p-norm regularization on the label tensor which is composed of soft labels of multimedia data. The solution can be obtained by iteratively optimizing four decoupled sub-problems, which can be solved more efficiently with good convergence. Experimental results on various datasets demonstrate the efficiency of our model",
    "checked": true,
    "id": "9b90833e0de88768c7d4ea4c240eeb215d1e1452",
    "semantic_title": "tensorized label learning on anchor graph",
    "citation_count": 0,
    "authors": [
      "Jing Li",
      "Quanxue Gao",
      "Qianqian Wang",
      "Wei Xia"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29258": {
    "title": "EMGAN: Early-Mix-GAN on Extracting Server-Side Model in Split Federated Learning",
    "volume": "main",
    "abstract": "Split Federated Learning (SFL) is an emerging edge-friendly version of Federated Learning (FL), where clients process a small portion of the entire model. While SFL was considered to be resistant to Model Extraction Attack (MEA) by design, a recent work shows it is not necessarily the case. In general, gradient-based MEAs are not effective on a target model that is changing, as is the case in training-from-scratch applications. In this work, we propose a strong MEA during the SFL training phase. The proposed Early-Mix-GAN (EMGAN) attack effectively exploits gradient queries regardless of data assumptions. EMGAN adopts three key components to address the problem of inconsistent gradients. Specifically, it employs (i) Early-learner approach for better adaptability, (ii) Multi-GAN approach to introduce randomness in generator training to mitigate mode collapse, and (iii) ProperMix to effectively augment the limited amount of synthetic data for a better approximation of the target domain data distribution. EMGAN achieves excellent results in extracting server-side models. With only 50 training samples, EMGAN successfully extracts a 5-layer server-side model of VGG-11 on CIFAR-10, with 7% less accuracy than the target model. With zero training data, the extracted model achieves 81.3% accuracy, which is significantly better than the 45.5% accuracy of the model extracted by the SoTA method. The code is available at \"https://github.com/zlijingtao/SFL-MEA",
    "checked": true,
    "id": "467cabe6f85318ef74987895cbf2f1e46f5c1d01",
    "semantic_title": "emgan: early-mix-gan on extracting server-side model in split federated learning",
    "citation_count": 0,
    "authors": [
      "Jingtao Li",
      "Xing Chen",
      "Li Yang",
      "Adnan Siraj Rakin",
      "Deliang Fan",
      "Chaitali Chakrabarti"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29259": {
    "title": "Contrastive Continual Learning with Importance Sampling and Prototype-Instance Relation Distillation",
    "volume": "main",
    "abstract": "Recently, because of the high-quality representations of contrastive learning methods, rehearsal-based contrastive continual learning has been proposed to explore how to continually learn transferable representation embeddings to avoid the catastrophic forgetting issue in traditional continual settings. Based on this framework, we propose Contrastive Continual Learning via Importance Sampling (CCLIS) to preserve knowledge by recovering previous data distributions with a new strategy for Replay Buffer Selection (RBS), which minimize estimated variance to save hard negative samples for representation learning with high quality. Furthermore, we present the Prototype-instance Relation Distillation (PRD) loss, a technique designed to maintain the relationship between prototypes and sample representations using a self-distillation process. Experiments on standard continual learning benchmarks reveal that our method notably outperforms existing baselines in terms of knowledge preservation and thereby effectively counteracts catastrophic forgetting in online contexts. The code is available at https://github.com/lijy373/CCLIS",
    "checked": true,
    "id": "d039bf8fdb276dae1ed0219b62fb35e69482c6bd",
    "semantic_title": "contrastive continual learning with importance sampling and prototype-instance relation distillation",
    "citation_count": 1,
    "authors": [
      "Jiyong Li",
      "Dilshod Azizov",
      "Yang LI",
      "Shangsong Liang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29260": {
    "title": "Twice Class Bias Correction for Imbalanced Semi-supervised Learning",
    "volume": "main",
    "abstract": "Differing from traditional semi-supervised learning, class-imbalanced semi-supervised learning presents two distinct challenges: (1) The imbalanced distribution of training samples leads to model bias towards certain classes, and (2) the distribution of unlabeled samples is unknown and potentially distinct from that of labeled samples, which further contributes to class bias in the pseudo-labels during the training. To address these dual challenges, we introduce a novel approach called Twice Class Bias Correction (TCBC). We begin by utilizing an estimate of the class distribution from the participating training samples to correct the model, enabling it to learn the posterior probabilities of samples under a class-balanced prior. This correction serves to alleviate the inherent class bias of the model. Building upon this foundation, we further estimate the class bias of the current model parameters during the training process. We apply a secondary correction to the model's pseudo-labels for unlabeled samples, aiming to make the assignment of pseudo-labels across different classes of unlabeled samples as equitable as possible. Through extensive experimentation on CIFAR10/100-LT, STL10-LT, and the sizable long-tailed dataset SUN397, we provide conclusive evidence that our proposed TCBC method reliably enhances the performance of class-imbalanced semi-supervised learning",
    "checked": true,
    "id": "753a51a66c30d196c3db6efca5d296325ff56c82",
    "semantic_title": "twice class bias correction for imbalanced semi-supervised learning",
    "citation_count": 0,
    "authors": [
      "Lan Li",
      "Bowen Tao",
      "Lu Han",
      "De-chuan Zhan",
      "Han-jia Ye"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29261": {
    "title": "Dynamic Regret of Adversarial MDPs with Unknown Transition and Linear Function Approximation",
    "volume": "main",
    "abstract": "We study reinforcement learning (RL) in episodic MDPs with adversarial full-information losses and the unknown transition. Instead of the classical static regret, we adopt dynamic regret as the performance measure which benchmarks the learner's performance with changing policies, making it more suitable for non-stationary environments. The primary challenge is to handle the uncertainties of unknown transition and unknown non-stationarity of environments simultaneously. We propose a general framework to decouple the two sources of uncertainties and show the dynamic regret bound naturally decomposes into two terms, one due to constructing confidence sets to handle the unknown transition and the other due to choosing sub-optimal policies under the unknown non-stationarity. To this end, we first employ the two-layer online ensemble structure to handle the adaptation error due to the unknown non-stationarity, which is model-agnostic. Subsequently, we instantiate the framework to three fundamental MDP models, including tabular MDPs, linear MDPs and linear mixture MDPs, and present corresponding approaches to control the exploration error due to the unknown transition. We provide dynamic regret guarantees respectively and show they are optimal in terms of the number of episodes K and the non-stationarity P̄ᴋ by establishing matching lower bounds. To the best of our knowledge, this is the first work that achieves the dynamic regret exhibiting optimal dependence on K and P̄ᴋ without prior knowledge about the non-stationarity for adversarial MDPs with unknown transition",
    "checked": true,
    "id": "be9b15474b9f1009c370e21108c07cee47b9cf65",
    "semantic_title": "dynamic regret of adversarial mdps with unknown transition and linear function approximation",
    "citation_count": 1,
    "authors": [
      "Long-Fei Li",
      "Peng Zhao",
      "Zhi-Hua Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29262": {
    "title": "Feature Fusion from Head to Tail for Long-Tailed Visual Recognition",
    "volume": "main",
    "abstract": "The imbalanced distribution of long-tailed data presents a considerable challenge for deep learning models, as it causes them to prioritize the accurate classification of head classes but largely disregard tail classes. The biased decision boundary caused by inadequate semantic information in tail classes is one of the key factors contributing to their low recognition accuracy. To rectify this issue, we propose to augment tail classes by grafting the diverse semantic information from head classes, referred to as head-to-tail fusion (H2T). We replace a portion of feature maps from tail classes with those belonging to head classes. These fused features substantially enhance the diversity of tail classes. Both theoretical analysis and practical experimentation demonstrate that H2T can contribute to a more optimized solution for the decision boundary. We seamlessly integrate H2T in the classifier adjustment stage, making it a plug-and-play module. Its simplicity and ease of implementation allow for smooth integration with existing long-tailed recognition methods, facilitating a further performance boost. Extensive experiments on various long-tailed benchmarks demonstrate the effectiveness of the proposed H2T. The source code is available at https://github.com/Keke921/H2T",
    "checked": true,
    "id": "0758e0a178a1bc1125a7e17f9b31e3b79419b97d",
    "semantic_title": "feature fusion from head to tail for long-tailed visual recognition",
    "citation_count": 0,
    "authors": [
      "Mengke Li",
      "Zhikai HU",
      "Yang Lu",
      "Weichao Lan",
      "Yiu-ming Cheung",
      "Hui Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29263": {
    "title": "Narrowing the Gap between Supervised and Unsupervised Sentence Representation Learning with Large Language Model",
    "volume": "main",
    "abstract": "Sentence Representation Learning (SRL) is a fundamental task in Natural Language Processing (NLP), with the Contrastive Learning of Sentence Embeddings (CSE) being the mainstream technique due to its superior performance. An intriguing phenomenon in CSE is the significant performance gap between supervised and unsupervised methods, with their only difference lying in the training data. Previous works attribute this performance gap to differences in two representation properties (alignment and uniformity). However, since alignment and uniformity only measure the results, they fail to answer \"What aspects of the training data contribute to the performance gap?\" and \"How can the performance gap be narrowed?\", In this paper, we conduct empirical experiments to answer these \"What\" and \"How\" questions. We first answer the \"What\" question by thoroughly comparing the behavior of supervised and unsupervised CSE during their respective training processes. From the comparison, we identify the similarity pattern as a key factor to the performance gap, and introduce a metric, called Relative Fitting Difficulty (RFD), to measure the complexity of the similarity pattern. Then, based on the insights gained from the \"What\" question, we tackle the \"How\" question by increasing the pattern complexity of the training data. We achieve this by leveraging the In-Context Learning (ICL) capability of the Large Language Model (LLM) to generate data that simulates complex patterns. By utilizing the hierarchical patterns in the LLM-generated data, we effectively narrow the gap between supervised and unsupervised CSE. We release our codes and appendix at https://github.com/BDBC-KG-NLP/NGCSE",
    "checked": true,
    "id": "5798d2efb64925117bd5bfe6f328240d3158c590",
    "semantic_title": "narrowing the gap between supervised and unsupervised sentence representation learning with large language model",
    "citation_count": 0,
    "authors": [
      "Mingxin Li",
      "Richong Zhang",
      "Zhijie Nie",
      "Yongyi Mao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29264": {
    "title": "AdapterGNN: Parameter-Efficient Fine-Tuning Improves Generalization in GNNs",
    "volume": "main",
    "abstract": "Fine-tuning pre-trained models has recently yielded remarkable performance gains in graph neural networks (GNNs). In addition to pre-training techniques, inspired by the latest work in the natural language fields, more recent work has shifted towards applying effective fine-tuning approaches, such as parameter-efficient fine-tuning (PEFT). However, given the substantial differences between GNNs and transformer-based models, applying such approaches directly to GNNs proved to be less effective. In this paper, we present a comprehensive comparison of PEFT techniques for GNNs and propose a novel PEFT method specifically designed for GNNs, called AdapterGNN. AdapterGNN preserves the knowledge of the large pre-trained model and leverages highly expressive adapters for GNNs, which can adapt to downstream tasks effectively with only a few parameters, while also improving the model's generalization ability. Extensive experiments show that AdapterGNN achieves higher performance than other PEFT methods and is the only one consistently surpassing full fine-tuning (outperforming it by 1.6% and 5.7% in the chemistry and biology domains respectively, with only 5% and 4% of its parameters tuned) with lower generalization gaps. Moreover, we empirically show that a larger GNN model can have a worse generalization ability, which differs from the trend observed in large transformer-based models. Building upon this, we provide a theoretical justification for PEFT can improve generalization of GNNs by applying generalization bounds. Our code is available at https://github.com/Lucius-lsr/AdapterGNN",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengrui Li",
      "Xueting Han",
      "Jing Bai"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29265": {
    "title": "Robust Visual Imitation Learning with Inverse Dynamics Representations",
    "volume": "main",
    "abstract": "Imitation learning (IL) has achieved considerable success in solving complex sequential decision-making problems. However, current IL methods mainly assume that the environment for learning policies is the same as the environment for collecting expert datasets. Therefore, these methods may fail to work when there are slight differences between the learning and expert environments, especially for challenging problems with high-dimensional image observations. However, in real-world scenarios, it is rare to have the chance to collect expert trajectories precisely in the target learning environment. To address this challenge, we propose a novel robust imitation learning approach, where we develop an inverse dynamics state representation learning objective to align the expert environment and the learning environment. With the abstract state representation, we design an effective reward function, which thoroughly measures the similarity between behavior data and expert data not only element-wise, but also from the trajectory level. We conduct extensive experiments to evaluate the proposed approach under various visual perturbations and in diverse visual control tasks. Our approach can achieve a near-expert performance in most environments, and significantly outperforms the state-of-the-art visual IL methods and robust IL methods",
    "checked": true,
    "id": "4dc63943662861eb92521e23b1a755148652325f",
    "semantic_title": "robust visual imitation learning with inverse dynamics representations",
    "citation_count": 0,
    "authors": [
      "Siyuan Li",
      "Xun Wang",
      "Rongchang Zuo",
      "Kewu Sun",
      "Lingfei Cui",
      "Jishiyu Ding",
      "Peng Liu",
      "Zhe Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29266": {
    "title": "Cumulative Difference Learning VAE for Time-Series with Temporally Correlated Inflow-Outflow",
    "volume": "main",
    "abstract": "Time-series generation has crucial practical significance for decision-making under uncertainty. Existing methods have various limitations like accumulating errors over time, significantly impacting downstream tasks. We develop a novel generation method, DT-VAE, that incorporates generalizable domain knowledge, is mathematically justified, and significantly outperforms existing methods by mitigating error accumulation through a cumulative difference learning mechanism. We evaluate the performance of DT-VAE on several downstream tasks using both semi-synthetic and real time-series datasets, including benchmark datasets and our newly curated COVID-19 hospitalization datasets. The COVID-19 datasets enrich existing resources for time-series analysis. Additionally, we introduce Diverse Trend Preserving (DTP), a time-series clustering-based evaluation for direct and interpretable assessments of generated samples, serving as a valuable tool for evaluating time-series generative models",
    "checked": true,
    "id": "7ac1f2f252c0f745cff02c8c92ed9b97c9a41c1a",
    "semantic_title": "cumulative difference learning vae for time-series with temporally correlated inflow-outflow",
    "citation_count": 1,
    "authors": [
      "Tianchun Li",
      "Chengxiang Wu",
      "Pengyi Shi",
      "Xiaoqian Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29267": {
    "title": "Federated X-armed Bandit",
    "volume": "main",
    "abstract": "This work establishes the first framework of federated X-armed bandit, where different clients face heterogeneous local objective functions defined on the same domain and are required to collaboratively figure out the global optimum. We propose the first federated algorithm for such problems, named Fed-PNE. By utilizing the topological structure of the global objective inside the hierarchical partitioning and the weak smoothness property, our algorithm achieves sublinear cumulative regret with respect to both the number of clients and the evaluation budget. Meanwhile, it only requires logarithmic communications between the central server and clients, protecting the client privacy. Experimental results on synthetic functions and real datasets validate the advantages of Fed-PNE over various centralized and federated baseline algorithms",
    "checked": true,
    "id": "8d84a2d1133d992b9db84c4b8f79141a7a7dbc19",
    "semantic_title": "federated x-armed bandit",
    "citation_count": 2,
    "authors": [
      "Wenjie Li",
      "Qifan Song",
      "Jean Honorio",
      "Guang Lin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29268": {
    "title": "Unsupervised Training Sequence Design: Efficient and Generalizable Agent Training",
    "volume": "main",
    "abstract": "To train generalizable Reinforcement Learning (RL) agents, researchers recently proposed the Unsupervised Environment Design (UED) framework, in which a teacher agent creates a very large number of training environments and a student agent trains on the experiences in these environments to be robust against unseen testing scenarios. For example, to train a student to master the \"stepping over stumps\" task, the teacher will create numerous training environments with varying stump heights and shapes. In this paper, we argue that UED neglects training efficiency and its need for very large number of environments (henceforth referred to as infinite horizon training) makes it less suitable to training robots and non-expert humans. In real-world applications where either creating new training scenarios is expensive or training efficiency is of critical importance, we want to maximize both the learning efficiency and learning outcome of the student. To achieve efficient finite horizon training, we propose a novel Markov Decision Process (MDP) formulation for the teacher agent, referred to as Unsupervised Training Sequence Design (UTSD). Specifically, we encode salient information from the student policy (e.g., behaviors and learning progress) into the teacher's state space, enabling the teacher to closely track the student's learning progress and consequently discover the optimal training sequences with finite lengths. Additionally, we explore the teacher's efficient adaptation to unseen students at test time by employing the context-based meta-learning approach, which leverages the teacher's past experiences with various students. Finally, we empirically demonstrate our teacher's capability to design efficient and effective training sequences for students with varying capabilities",
    "checked": true,
    "id": "5c34c53fa2b98d81c52ca6a906d73cd4f162079a",
    "semantic_title": "unsupervised training sequence design: efficient and generalizable agent training",
    "citation_count": 0,
    "authors": [
      "Wenjun Li",
      "Pradeep Varakantham"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29269": {
    "title": "Image Content Generation with Causal Reasoning",
    "volume": "main",
    "abstract": "The emergence of ChatGPT has once again sparked research in generative artificial intelligence (GAI). While people have been amazed by the generated results, they have also noticed the reasoning potential reflected in the generated textual content. However, this current ability for causal reasoning is primarily limited to the domain of language generation, such as in models like GPT-3. In visual modality, there is currently no equivalent research. Considering causal reasoning in visual content generation is significant. This is because visual information contains infinite granularity. Particularly, images can provide more intuitive and specific demonstrations for certain reasoning tasks, especially when compared to coarse-grained text. Hence, we propose a new image generation task called visual question answering with image (VQAI) and establish a dataset of the same name based on the classic Tom and Jerry animated series. Additionally, we develop a new paradigm for image generation to tackle the challenges of this task. Finally, we perform extensive experiments and analyses, including visualizations of the generated content and discussions on the potentials and limitations. The code and data are publicly available under the license of CC BY-NC-SA 4.0 for academic and non-commercial usage at: https://github.com/IEIT-AGI/MIX-Shannon/blob/main/projects/VQAI/lgd_vqai.md",
    "checked": true,
    "id": "d99f91a811512ecd1c9f1d80dcdd712d5c5abc88",
    "semantic_title": "image content generation with causal reasoning",
    "citation_count": 3,
    "authors": [
      "Xiaochuan Li",
      "Baoyu Fan",
      "Runze Zhang",
      "Liang Jin",
      "Di Wang",
      "Zhenhua Guo",
      "Yaqian Zhao",
      "Rengang Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29270": {
    "title": "Deep Active Learning with Noise Stability",
    "volume": "main",
    "abstract": "Uncertainty estimation for unlabeled data is crucial to active learning. With a deep neural network employed as the backbone model, the data selection process is highly challenging due to the potential over-confidence of the model inference. Existing methods resort to special learning fashions (e.g. adversarial) or auxiliary models to address this challenge. This tends to result in complex and inefficient pipelines, which would render the methods impractical. In this work, we propose a novel algorithm that leverages noise stability to estimate data uncertainty. The key idea is to measure the output derivation from the original observation when the model parameters are randomly perturbed by noise. We provide theoretical analyses by leveraging the small Gaussian noise theory and demonstrate that our method favors a subset with large and diverse gradients. Our method is generally applicable in various tasks, including computer vision, natural language processing, and structural data analysis. It achieves competitive performance compared against state-of-the-art active learning baselines",
    "checked": true,
    "id": "416d9ed1c0523fac88189e731fac520829fa159d",
    "semantic_title": "deep active learning with noise stability",
    "citation_count": 5,
    "authors": [
      "Xingjian Li",
      "Pengkun Yang",
      "Yangcheng Gu",
      "Xueying Zhan",
      "Tianyang Wang",
      "Min Xu",
      "Chengzhong Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29271": {
    "title": "Distribution-Conditioned Adversarial Variational Autoencoder for Valid Instrumental Variable Generation",
    "volume": "main",
    "abstract": "Instrumental variables (IVs), widely applied in economics and healthcare, enable consistent counterfactual prediction in the presence of hidden confounding factors, effectively addressing endogeneity issues. The prevailing IV-based counterfactual prediction methods typically rely on the availability of valid IVs (satisfying Relevance, Exclusivity, and Exogeneity), a requirement which often proves elusive in real-world scenarios. Various data-driven techniques are being developed to create valid IVs (or representations of IVs) from a pool of IV candidates. However, most of these techniques still necessitate the inclusion of valid IVs within the set of candidates. This paper proposes a distribution-conditioned adversarial variational autoencoder to tackle this challenge. Specifically: 1) for Relevance and Exclusivity, we deduce the corresponding evidence lower bound following the Bayesian network structure and build the variational autoencoder; accordingly, 2) for Exogeneity , we design an adversarial game to encourage latent factors originating from the marginal distribution, compelling the independence between IVs and other outcome-related factors. Extensive experimental results validate the effectiveness, stability and generality of our proposed model in generating valid IV factors in the absence of valid IV candidates",
    "checked": true,
    "id": "138dc57a492d52755634fe5e0fbd8f67c5b18d23",
    "semantic_title": "distribution-conditioned adversarial variational autoencoder for valid instrumental variable generation",
    "citation_count": 1,
    "authors": [
      "Xinshu Li",
      "Lina Yao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29272": {
    "title": "Agile Multi-Source-Free Domain Adaptation",
    "volume": "main",
    "abstract": "Efficiently utilizing rich knowledge in pretrained models has become a critical topic in the era of large models. This work focuses on adaptively utilize knowledge from multiple source-pretrained models to an unlabeled target domain without accessing the source data. Despite being a practically useful setting, existing methods require extensive parameter tuning over each source model, which is computationally expensive when facing abundant source domains or larger source models. To address this challenge, we propose a novel approach which is free of the parameter tuning over source backbones. Our technical contribution lies in the Bi-level ATtention ENsemble (Bi-ATEN) module, which learns both intra-domain weights and inter-domain ensemble weights to achieve a fine balance between instance specificity and domain consistency. By slightly tuning source bottlenecks, we achieve comparable or even superior performance on a challenging benchmark DomainNet with less than 3% trained parameters and 8 times of throughput compared with SOTA method. Furthermore, with minor modifications, the proposed module can be easily equipped to existing methods and gain more than 4% performance boost. Code is available at https://github.com/TL-UESTC/Bi-ATEN",
    "checked": true,
    "id": "1a8e9e03a04a2caf356b7e7a5800817392bd513e",
    "semantic_title": "agile multi-source-free domain adaptation",
    "citation_count": 0,
    "authors": [
      "Xinyao Li",
      "Jingjing Li",
      "Fengling Li",
      "Lei Zhu",
      "Ke Lu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29273": {
    "title": "Towards Effective and General Graph Unlearning via Mutual Evolution",
    "volume": "main",
    "abstract": "With the rapid advancement of AI applications, the growing needs for data privacy and model robustness have highlighted the importance of machine unlearning, especially in thriving graph-based scenarios. However, most existing graph unlearning strategies primarily rely on well-designed architectures or manual process, rendering them less user-friendly and posing challenges in terms of deployment efficiency. Furthermore, striking a balance between unlearning performance and framework generalization is also a pivotal concern. To address the above issues, we propose Mutual Evolution Graph Unlearning (MEGU), a new mutual evolution paradigm that simultaneously evolves the predictive and unlearning capacities of graph unlearning. By incorporating aforementioned two components, MEGU ensures complementary optimization in a unified training framework that aligns with the prediction and unlearning requirements. Extensive experiments on 9 graph benchmark datasets demonstrate the superior performance of MEGU in addressing unlearning requirements at the feature, node, and edge levels. Specifically, MEGU achieves average performance improvements of 2.7%, 2.5%, and 3.2% across these three levels of unlearning tasks when compared to state-of-the-art baselines. Furthermore, MEGU exhibits satisfactory training efficiency, reducing time and space overhead by an average of 159.8x and 9.6x, respectively, in comparison to retraining GNN from scratch",
    "checked": true,
    "id": "639b0206d0258803f1928d1a24b09e3d02db84a0",
    "semantic_title": "towards effective and general graph unlearning via mutual evolution",
    "citation_count": 0,
    "authors": [
      "Xunkai Li",
      "Yulin Zhao",
      "Zhengyu Wu",
      "Wentao Zhang",
      "Rong-Hua Li",
      "Guoren Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29274": {
    "title": "Component Fourier Neural Operator for Singularly Perturbed Differential Equations",
    "volume": "main",
    "abstract": "Solving Singularly Perturbed Differential Equations (SPDEs) poses computational challenges arising from the rapid transitions in their solutions within thin regions. The effectiveness of deep learning in addressing differential equations motivates us to employ these methods for solving SPDEs. In this paper, we introduce Component Fourier Neural Operator (ComFNO), an innovative operator learning method that builds upon Fourier Neural Operator (FNO), while simultaneously incorporating valuable prior knowledge obtained from asymptotic analysis. Our approach is not limited to FNO and can be applied to other neural network frameworks, such as Deep Operator Network (DeepONet), leading to potential similar SPDEs solvers. Experimental results across diverse classes of SPDEs demonstrate that ComFNO significantly improves accuracy compared to vanilla FNO. Furthermore, ComFNO exhibits natural adaptability to diverse data distributions and performs well in few-shot scenarios, showcasing its excellent generalization ability in practical situations",
    "checked": true,
    "id": "7d8ffe3c369c525986807a7f693da9927dd527c6",
    "semantic_title": "component fourier neural operator for singularly perturbed differential equations",
    "citation_count": 0,
    "authors": [
      "Ye Li",
      "Ting Du",
      "Yiwen Pang",
      "Zhongyi Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29275": {
    "title": "Learning to Prompt Knowledge Transfer for Open-World Continual Learning",
    "volume": "main",
    "abstract": "This paper studies the problem of continual learning in an open-world scenario, referred to as Open-world Continual Learning (OwCL). OwCL is increasingly rising while it is highly challenging in two-fold: i) learning a sequence of tasks without forgetting knowns in the past, and ii) identifying unknowns (novel objects/classes) in the future. Existing OwCL methods suffer from the adaptability of task-aware boundaries between knowns and unknowns, and do not consider the mechanism of knowledge transfer. In this work, we propose Pro-KT, a novel prompt-enhanced knowledge transfer model for OwCL. Pro-KT includes two key components: (1) a prompt bank to encode and transfer both task-generic and task-specific knowledge, and (2) a task-aware open-set boundary to identify unknowns in the new tasks. Experimental results using two real-world datasets demonstrate that the proposed Pro-KT outperforms the state-of-the-art counterparts in both the detection of unknowns and the classification of knowns markedly. Code released at https://github.com/YujieLi42/Pro-KT",
    "checked": true,
    "id": "d0f7186c789be44346e4ab2ae4cc617beeb882ff",
    "semantic_title": "learning to prompt knowledge transfer for open-world continual learning",
    "citation_count": 4,
    "authors": [
      "Yujie Li",
      "Xin Yang",
      "Hao Wang",
      "Xiangkun Wang",
      "Tianrui Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29276": {
    "title": "SPD-DDPM: Denoising Diffusion Probabilistic Models in the Symmetric Positive Definite Space",
    "volume": "main",
    "abstract": "Symmetric positive definite(SPD) matrices have shown important value and applications in statistics and machine learning, such as FMRI analysis and traffic prediction. Previous works on SPD matrices mostly focus on discriminative models, where predictions are made directly on E(X|y), where y is a vector and X is an SPD matrix. However, these methods are challenging to handle for large-scale data. In this paper, inspired by denoising diffusion probabilistic model(DDPM), we propose a novel generative model, termed SPD-DDPM, by introducing Gaussian distribution in the SPD space to estimate E(X|y). Moreover, our model can estimate p(X) unconditionally and flexibly without giving y. On the one hand, the model conditionally learns p(X|y) and utilizes the mean of samples to obtain E(X|y) as a prediction. On the other hand, the model unconditionally learns the probability distribution of the data p(X) and generates samples that conform to this distribution. Furthermore, we propose a new SPD net which is much deeper than the previous networks and allows for the inclusion of conditional factors. Experiment results on toy data and real taxi data demonstrate that our models effectively fit the data distribution both unconditionally and conditionally",
    "checked": true,
    "id": "56fe6acdeb4d231a704ed4dc04ebfcc44d7f670e",
    "semantic_title": "spd-ddpm: denoising diffusion probabilistic models in the symmetric positive definite space",
    "citation_count": 2,
    "authors": [
      "Yunchen Li",
      "Zhou Yu",
      "Gaoqi He",
      "Yunhang Shen",
      "Ke Li",
      "Xing Sun",
      "Shaohui Lin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29277": {
    "title": "Backpropagation Through Agents",
    "volume": "main",
    "abstract": "A fundamental challenge in multi-agent reinforcement learning (MARL) is to learn the joint policy in an extremely large search space, which grows exponentially with the number of agents. Moreover, fully decentralized policy factorization significantly restricts the search space, which may lead to sub-optimal policies. In contrast, the auto-regressive joint policy can represent a much richer class of joint policies by factorizing the joint policy into the product of a series of conditional individual policies. While such factorization introduces the action dependency among agents explicitly in sequential execution, it does not take full advantage of the dependency during learning. In particular, the subsequent agents do not give the preceding agents feedback about their decisions. In this paper, we propose a new framework Back-Propagation Through Agents (BPTA) that directly accounts for both agents' own policy updates and the learning of their dependent counterparts. This is achieved by propagating the feedback through action chains. With the proposed framework, our Bidirectional Proximal Policy Optimisation (BPPO) outperforms the state-of-the-art methods. Extensive experiments on matrix games, StarCraftII v2, Multi-agent MuJoCo, and Google Research Football demonstrate the effectiveness of the proposed method",
    "checked": true,
    "id": "706ec08463fd32cb92c92d36bd174631e0fbca72",
    "semantic_title": "backpropagation through agents",
    "citation_count": 0,
    "authors": [
      "Zhiyuan Li",
      "Wenshuai Zhao",
      "Lijun Wu",
      "Joni Pajarinen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29278": {
    "title": "Multi-Granularity Causal Structure Learning",
    "volume": "main",
    "abstract": "Unveiling, modeling, and comprehending the causal mechanisms underpinning natural phenomena stand as fundamental endeavors across myriad scientific disciplines. Meanwhile, new knowledge emerges when discovering causal relationships from data. Existing causal learning algorithms predominantly focus on the isolated effects of variables, overlook the intricate interplay of multiple variables and their collective behavioral patterns. Furthermore, the ubiquity of high-dimensional data exacts a substantial temporal cost for causal algorithms. In this paper, we develop a novel method called MgCSL (Multi-granularity Causal Structure Learning), which first leverages sparse auto-encoder to explore coarse-graining strategies and causal abstractions from micro-variables to macro-ones. MgCSL then takes multi-granularity variables as inputs to train multilayer perceptrons and to delve the causality between variables. To enhance the efficacy on high-dimensional data, MgCSL introduces a simplified acyclicity constraint to adeptly search the directed acyclic graph among variables. Experimental results show that MgCSL outperforms competitive baselines, and finds out explainable causal connections on fMRI datasets",
    "checked": true,
    "id": "a83ba57c135c0ff9b5d811853f1b2329dbcf3026",
    "semantic_title": "multi-granularity causal structure learning",
    "citation_count": 0,
    "authors": [
      "Jiaxuan Liang",
      "Jun Wang",
      "Guoxian Yu",
      "Shuyin Xia",
      "Guoyin Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29279": {
    "title": "Inducing Clusters Deep Kernel Gaussian Process for Longitudinal Data",
    "volume": "main",
    "abstract": "We consider the problem of predictive modeling from irregularly and sparsely sampled longitudinal data with unknown, complex correlation structures and abrupt discontinuities. To address these challenges, we introduce a novel inducing clusters longitudinal deep kernel Gaussian Process (ICDKGP). ICDKGP approximates the data generating process by a zero-mean GP with a longitudinal deep kernel that models the unknown complex correlation structure in the data and a deterministic non-zero mean function to model the abrupt discontinuities. To improve the scalability and interpretability of ICDKGP, we introduce inducing clusters corresponding to centers of clusters in the training data. We formulate the training of ICDKGP as a constrained optimization problem and derive its evidence lower bound. We introduce a novel relaxation of the resulting problem which under rather mild assumptions yields a solution with error bounded relative to the original problem. We describe the results of extensive experiments demonstrating that ICDKGP substantially outperforms the state-of-the-art longitudinal methods on data with both smoothly and non-smoothly varying outcomes",
    "checked": true,
    "id": "c13e0b2bd58ef9964143c72a8964de5a20eadabd",
    "semantic_title": "inducing clusters deep kernel gaussian process for longitudinal data",
    "citation_count": 0,
    "authors": [
      "Junjie Liang",
      "Weijieying Ren",
      "Hanifi Sahar",
      "Vasant Honavar"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29280": {
    "title": "Self-Supervised Multi-Modal Knowledge Graph Contrastive Hashing for Cross-Modal Search",
    "volume": "main",
    "abstract": "Deep cross-modal hashing technology provides an effective and efficient cross-modal unified representation learning solution for cross-modal search. However, the existing methods neglect the implicit fine-grained multimodal knowledge relations between these modalities such as when the image contains information that is not directly described in the text. To tackle this problem, we propose a novel self-supervised multi-grained multi-modal knowledge graph contrastive hashing method for cross-modal search (CMGCH). Firstly, in order to capture implicit fine-grained cross-modal semantic associations, a multi-modal knowledge graph is constructed, which represents the implicit multimodal knowledge relations between the image and text as inter-modal and intra-modal semantic associations. Secondly, a cross-modal graph contrastive attention network is proposed to reason on the multi-modal knowledge graph to sufficiently learn the implicit fine-grained inter-modal and intra-modal knowledge relations. Thirdly, a cross-modal multi-granularity contrastive embedding learning mechanism is proposed, which fuses the global coarse-grained and local fine-grained embeddings by multihead attention mechanism for inter-modal and intra-modal contrastive learning, so as to enhance the cross-modal unified representations with stronger discriminativeness and semantic consistency preserving power. With the joint training of intra-modal and inter-modal contrast, the invariant and modal-specific information of different modalities can be maintained in the final unified cross-modal unified hash space. Extensive experiments on several cross-modal benchmark datasets demonstrate that the proposed CMGCH outperforms the state-of the-art methods",
    "checked": true,
    "id": "186a8a64a58832cd1b950adcb9fe8932b5af830d",
    "semantic_title": "self-supervised multi-modal knowledge graph contrastive hashing for cross-modal search",
    "citation_count": 0,
    "authors": [
      "Meiyu Liang",
      "Junping Du",
      "Zhengyang Liang",
      "Yongwang Xing",
      "Wei Huang",
      "Zhe Xue"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29281": {
    "title": "DC-NAS: Divide-and-Conquer Neural Architecture Search for Multi-Modal Classification",
    "volume": "main",
    "abstract": "Neural architecture search-based multi-modal classification (NAS-MMC) methods can individually obtain the optimal classifier for different multi-modal data sets in an automatic manner. However, most existing NAS-MMC methods are dramatically time consuming due to the requirement for training and evaluating enormous models. In this paper, we propose an efficient evolutionary-based NAS-MMC method called divide-and-conquer neural architecture search (DC-NAS). Specifically, the evolved population is first divided into k+1 sub-populations, and then k sub-populations of them evolve on k small-scale data sets respectively that are obtained by splitting the entire data set using the k-fold stratified sampling technique; the remaining one evolves on the entire data set. To solve the sub-optimal fusion model problem caused by the training strategy of partial data, two kinds of sub-populations that are trained using partial data and entire data exchange the learned knowledge via two special knowledge bases. With the two techniques mentioned above, DC-NAS achieves the training time reduction and classification performance improvement. Experimental results show that DC-NAS achieves the state-of-the-art results in term of classification performance, training efficiency and the number of model parameters than the compared NAS-MMC methods on three popular multi-modal tasks including multi-label movie genre classification, action recognition with RGB and body joints and dynamic hand gesture recognition",
    "checked": true,
    "id": "954fe47c6091041c5f5fe3a160bfc7da3ee0ceb9",
    "semantic_title": "dc-nas: divide-and-conquer neural architecture search for multi-modal classification",
    "citation_count": 0,
    "authors": [
      "Xinyan Liang",
      "Pinhan Fu",
      "Qian Guo",
      "Keyin Zheng",
      "Yuhua Qian"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29282": {
    "title": "Value at Adversarial Risk: A Graph Defense Strategy against Cost-Aware Attacks",
    "volume": "main",
    "abstract": "Deep learning methods on graph data have achieved remarkable efficacy across a variety of real-world applications, such as social network analysis and transaction risk detection. Nevertheless, recent studies have illuminated a concerning fact: even the most expressive Graph Neural Networks (GNNs) are vulnerable to graph adversarial attacks. While several methods have been proposed to enhance the robustness of GNN models against adversarial attacks, few have focused on a simple yet realistic approach: valuing the adversarial risks and focused safeguards at the node level. This empowers defenders to allocate heightened security level to vulnerable nodes, while lower to robust nodes. With this new perspective, we propose a novel graph defense strategy RisKeeper, such that the adversarial risk can be directly kept in the input graph. We start at valuing the adversarial risk, by introducing a cost-aware projected gradient descent attack that takes into account both cost avoidance and compliance with costs budgets. Subsequently, we present a learnable approach to ascertain the ideal security level for each individual node by solving a bi-level optimization problem. Through extensive experiments on four real-world datasets, we demonstrate that our method achieves superior performance surpassing state-of-the-art methods. Our in-depth case studies provide further insights into vulnerable and robust structural patterns, serving as inspiration for practitioners to exercise heightened vigilance",
    "checked": true,
    "id": "ae94d933dd396f792f81b44b3ab5d16c3b876296",
    "semantic_title": "value at adversarial risk: a graph defense strategy against cost-aware attacks",
    "citation_count": 0,
    "authors": [
      "Junlong Liao",
      "Wenda Fu",
      "Cong Wang",
      "Zhongyu Wei",
      "Jiarong Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29283": {
    "title": "Invariant Random Forest: Tree-Based Model Solution for OOD Generalization",
    "volume": "main",
    "abstract": "Out-Of-Distribution (OOD) generalization is an essential topic in machine learning. However, recent research is only focusing on the corresponding methods for neural networks. This paper introduces a novel and effective solution for OOD generalization of decision tree models, named Invariant Decision Tree (IDT). IDT enforces a penalty term with regard to the unstable/varying behavior of a split across different environments during the growth of the tree. Its ensemble version, the Invariant Random Forest (IRF), is constructed. Our proposed method is motivated by a theoretical result under mild conditions, and validated by numerical tests with both synthetic and real datasets. The superior performance compared to non-OOD tree models implies that considering OOD generalization for tree models is absolutely necessary and should be given more attention",
    "checked": true,
    "id": "40662853a8d0942c39d11c4c2c6280a2a981732b",
    "semantic_title": "invariant random forest: tree-based model solution for ood generalization",
    "citation_count": 1,
    "authors": [
      "Yufan Liao",
      "Qi Wu",
      "Xing Yan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29284": {
    "title": "Ahpatron: A New Budgeted Online Kernel Learning Machine with Tighter Mistake Bound",
    "volume": "main",
    "abstract": "In this paper, we study the mistake bound of online kernel learning on a budget. We propose a new budgeted online kernel learning model, called Ahpatron, which significantly improves the mistake bound of previous work and resolves an open problem related to upper bounds of hypothesis space constraints. We first present an aggressive variant of Perceptron, named AVP, a model without budget, which uses an active updating rule. Then we design a new budget maintenance mechanism, which removes a half of examples, and projects the removed examples onto a hypothesis space spanned by the remaining examples. Ahpatron adopts the above mechanism to approximate AVP. Theoretical analyses prove that Ahpatron has tighter mistake bounds, and experimental results show that Ahpatron outperforms the state-of-the-art algorithms on the same or a smaller budget",
    "checked": true,
    "id": "d4beb3e8107b9e0cfa49f371304bc9542e44fd1a",
    "semantic_title": "ahpatron: a new budgeted online kernel learning machine with tighter mistake bound",
    "citation_count": 0,
    "authors": [
      "Yun Liao",
      "Junfan Li",
      "Shizhong Liao",
      "Qinghua Hu",
      "Jianwu Dang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29285": {
    "title": "Spiking NeRF: Representing the Real-World Geometry by a Discontinuous Representation",
    "volume": "main",
    "abstract": "A crucial reason for the success of existing NeRF-based methods is to build a neural density field for the geometry representation via multiple perceptron layers (MLPs). MLPs are continuous functions, however, real geometry or density field is frequently discontinuous at the interface between the air and the surface. Such a contrary brings the problem of unfaithful geometry representation. To this end, this paper proposes spiking NeRF, which leverages spiking neurons and a hybrid Artificial Neural Network (ANN)-Spiking Neural Network (SNN) framework to build a discontinuous density field for faithful geometry representation. Specifically, we first demonstrate the reason why continuous density fields will bring inaccuracy. Then, we propose to use the spiking neurons to build a discontinuous density field. We conduct a comprehensive analysis for the problem of existing spiking neuron models and then provide the numerical relationship between the parameter of the spiking neuron and the theoretical accuracy of geometry. Based on this, we propose a bounded spiking neuron to build the discontinuous density field. Our method achieves SOTA performance. The source code and the supplementary material are available at https://github.com/liaozhanfeng/Spiking-NeRF",
    "checked": true,
    "id": "79373e1527652ca9838fd824151b988991fbfb85",
    "semantic_title": "spiking nerf: representing the real-world geometry by a discontinuous representation",
    "citation_count": 1,
    "authors": [
      "Zhanfeng Liao",
      "Yan Liu",
      "Qian Zheng",
      "Gang Pan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29286": {
    "title": "Mitigating Label Noise through Data Ambiguation",
    "volume": "main",
    "abstract": "Label noise poses an important challenge in machine learning, especially in deep learning, in which large models with high expressive power dominate the field. Models of that kind are prone to memorizing incorrect labels, thereby harming generalization performance. Many methods have been proposed to address this problem, including robust loss functions and more complex label correction approaches. Robust loss functions are appealing due to their simplicity, but typically lack flexibility, while label correction usually adds substantial complexity to the training setup. In this paper, we suggest to address the shortcomings of both methodologies by \"ambiguating\" the target information, adding additional, complementary candidate labels in case the learner is not sufficiently convinced of the observed training label. More precisely, we leverage the framework of so-called superset learning to construct set-valued targets based on a confidence threshold, which deliver imprecise yet more reliable beliefs about the ground-truth, effectively helping the learner to suppress the memorization effect. In an extensive empirical evaluation, our method demonstrates favorable learning behavior on synthetic and real-world noise, confirming the effectiveness in detecting and correcting erroneous training labels",
    "checked": true,
    "id": "079b8a865ef0c90ad2eed714416acfb49a6eaae3",
    "semantic_title": "mitigating label noise through data ambiguation",
    "citation_count": 1,
    "authors": [
      "Julian Lienen",
      "Eyke Hüllermeier"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29287": {
    "title": "Episodic Return Decomposition by Difference of Implicitly Assigned Sub-trajectory Reward",
    "volume": "main",
    "abstract": "Real-world decision-making problems are usually accompanied by delayed rewards, which affects the sample efficiency of Reinforcement Learning, especially in the extremely delayed case where the only feedback is the episodic reward obtained at the end of an episode. Episodic return decomposition is a promising way to deal with the episodic-reward setting. Several corresponding algorithms have shown remarkable effectiveness of the learned step-wise proxy rewards from return decomposition. However, these existing methods lack either attribution or representation capacity, leading to inefficient decomposition in the case of long-term episodes. In this paper, we propose a novel episodic return decomposition method called Diaster (Difference of implicitly assigned sub-trajectory reward). Diaster decomposes any episodic reward into credits of two divided sub-trajectories at any cut point, and the step-wise proxy rewards come from differences in expectation. We theoretically and empirically verify that the decomposed proxy reward function can guide the policy to be nearly optimal. Experimental results show that our method outperforms previous state-of-the-art methods in terms of both sample efficiency and performance. The code is available at https://github.com/HxLyn3/Diaster",
    "checked": true,
    "id": "72b587b0dabb66e86cf00b965a4f60d2f5f6bdfe",
    "semantic_title": "episodic return decomposition by difference of implicitly assigned sub-trajectory reward",
    "citation_count": 0,
    "authors": [
      "Haoxin Lin",
      "Hongqiu Wu",
      "Jiaji Zhang",
      "Yihao Sun",
      "Junyin Ye",
      "Yang Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29288": {
    "title": "Jointly Modeling Spatio-Temporal Features of Tactile Signals for Action Classification",
    "volume": "main",
    "abstract": "Tactile signals collected by wearable electronics are essential in modeling and understanding human behavior. One of the main applications of tactile signals is action classification, especially in healthcare and robotics. However, existing tactile classification methods fail to capture the spatial and temporal features of tactile signals simultaneously, which results in sub-optimal performances. In this paper, we design Spatio-Temporal Aware tactility Transformer (STAT) to utilize continuous tactile signals for action classification. We propose spatial and temporal embeddings along with a new temporal pretraining task in our model, which aims to enhance the transformer in modeling the spatio-temporal features of tactile signals. Specially, the designed temporal pretraining task is to differentiate the time order of tubelet inputs to model the temporal properties explicitly. Experimental results on a public action classification dataset demonstrate that our model outperforms state-of-the-art methods in all metrics",
    "checked": true,
    "id": "8762c42a3d931089839eb974e53e81ff5f1a1073",
    "semantic_title": "jointly modeling spatio-temporal features of tactile signals for action classification",
    "citation_count": 0,
    "authors": [
      "Jimmy Lin",
      "Junkai Li",
      "Jiasi Gao",
      "Weizhi Ma",
      "Yang Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29289": {
    "title": "ERL-TD: Evolutionary Reinforcement Learning Enhanced with Truncated Variance and Distillation Mutation",
    "volume": "main",
    "abstract": "Recently, an emerging research direction called Evolutionary Reinforcement Learning (ERL) has been proposed, which combines evolutionary algorithm with reinforcement learning (RL) for tackling the tasks of sequential decision making. However, the recently proposed ERL algorithms often suffer from two challenges: the inaccuracy of policy estimation caused by the overestimation bias in RL and the insufficiency of exploration caused by inefficient mutations. To alleviate these problems, we propose an Evolutionary Reinforcement Learning algorithm enhanced with Truncated variance and Distillation mutation, called ERL-TD. We utilize multiple Q-networks to evaluate state-action pairs, so that multiple networks can provide more accurate evaluations for state-action pairs, in which the variance of evaluations can be adopted to control the overestimation bias in RL. Moreover, we propose a new distillation mutation to provide a promising mutation direction, which is different from traditional mutation generating a large number of random solutions. We evaluate ERL-TD on the continuous control benchmarks from the OpenAI Gym and DeepMind Control Suite. The experiments show that ERL-TD shows excellent performance and outperforms all baseline RL algorithms on the test suites",
    "checked": true,
    "id": "3cf00f63deaa349aea7f2966b156fb639ddc9e71",
    "semantic_title": "erl-td: evolutionary reinforcement learning enhanced with truncated variance and distillation mutation",
    "citation_count": 0,
    "authors": [
      "Qiuzhen Lin",
      "Yangfan Chen",
      "Lijia Ma",
      "Wei-Neng Chen",
      "Jianqiang  Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29290": {
    "title": "Hypergraph Neural Architecture Search",
    "volume": "main",
    "abstract": "In recent years, Hypergraph Neural Networks (HGNNs) have achieved considerable success by manually designing architectures, which are capable of extracting effective patterns with high-order interactions from non-Euclidean data. However, such mechanism is extremely inefficient, demanding tremendous human efforts to tune diverse model parameters. In this paper, we propose a novel Hypergraph Neural Architecture Search (HyperNAS) to automatically design the optimal HGNNs. The proposed model constructs a search space suitable for hypergraphs, and derives hypergraph architectures through differentiable search strategies. A hypergraph structure-aware distance criterion is introduced as a guideline for obtaining an optimal hypergraph architecture via the leave-one-out method. Experimental results for node classification on benchmark Cora, Citeseer, Pubmed citation networks and hypergraph datasets show that HyperNAS outperforms existing HGNNs models and graph NAS methods",
    "checked": true,
    "id": "b1876166751b049701f0800d5a4ae9d7288c07db",
    "semantic_title": "hypergraph neural architecture search",
    "citation_count": 0,
    "authors": [
      "Wei Lin",
      "Xu Peng",
      "Zhengtao Yu",
      "Taisong Jin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29291": {
    "title": "Scaling Few-Shot Learning for the Open World",
    "volume": "main",
    "abstract": "Few-shot learning (FSL) aims to enable learning models with the ability to automatically adapt to novel (unseen) domains in open-world scenarios. Nonetheless, there exists a significant disparity between the vast number of new concepts encountered in the open world and the restricted available scale of existing FSL works, which primarily focus on a limited number of novel classes. Such a gap hinders the practical applicability of FSL in realistic scenarios. To bridge this gap, we propose a new problem named Few-Shot Learning with Many Novel Classes (FSL-MNC) by substantially enlarging the number of novel classes, exceeding the count in the traditional FSL setup by over 500-fold. This new problem exhibits two major challenges, including the increased computation overhead during meta-training and the degraded classification performance by the large number of classes during meta-testing. To overcome these challenges, we propose a Simple Hierarchy Pipeline (SHA-Pipeline). Due to the inefficiency of traditional protocols of EML, we re-design a lightweight training strategy to reduce the overhead brought by much more novel classes. To capture discriminative semantics across numerous novel classes, we effectively reconstruct and leverage the class hierarchy information during meta-testing. Experiments show that the proposed SHA-Pipeline significantly outperforms not only the ProtoNet baseline but also the state-of-the-art alternatives across different numbers of novel classes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhipeng Lin",
      "Wenjing Yang",
      "Haotian Wang",
      "Haoang Chi",
      "Long Lan",
      "Ji Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29292": {
    "title": "Towards Inductive Robustness: Distilling and Fostering Wave-Induced Resonance in Transductive GCNs against Graph Adversarial Attacks",
    "volume": "main",
    "abstract": "Graph neural networks (GNNs) have recently been shown to be vulnerable to adversarial attacks, where slight perturbations in the graph structure can lead to erroneous predictions. However, current robust models for defending against such attacks inherit the transductive limitations of graph convolutional networks (GCNs). As a result, they are constrained by fixed structures and do not naturally generalize to unseen nodes. Here, we discover that transductive GCNs inherently possess a distillable robustness, achieved through a wave-induced resonance process. Based on this, we foster this resonance to facilitate inductive and robust learning. Specifically, we first prove that the signal formed by GCN-driven message passing (MP) is equivalent to the edge-based Laplacian wave, where, within a wave system, resonance can naturally emerge between the signal and its transmitting medium. This resonance provides inherent resistance to malicious perturbations inflicted on the signal system. We then prove that merely three MP iterations within GCNs can induce signal resonance between nodes and edges, manifesting as a coupling between nodes and their distillable surrounding local subgraph. Consequently, we present Graph Resonance-fostering Network (GRN) to foster this resonance via learning node representations from their distilled resonating subgraphs. By capturing the edge-transmitted signals within this subgraph and integrating them with the node signal, GRN embeds these combined signals into the central node's representation. This node-wise embedding approach allows for generalization to unseen nodes. We validate our theoretical findings with experiments, and demonstrate that GRN generalizes robustness to unseen nodes, whilst maintaining state-of-the-art classification accuracy on perturbed graphs. Appendices can be found on arXiv version: https://arxiv.org/abs/2312.08651",
    "checked": true,
    "id": "fb176a4c32df17bff557ec985f380d1d59fc4c63",
    "semantic_title": "towards inductive robustness: distilling and fostering wave-induced resonance in transductive gcns against graph adversarial attacks",
    "citation_count": 0,
    "authors": [
      "Ao Liu",
      "Wenshan Li",
      "Tao Li",
      "Beibei Li",
      "Hanyuan Huang",
      "Pan Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29293": {
    "title": "Attention-Induced Embedding Imputation for Incomplete Multi-View Partial Multi-Label Classification",
    "volume": "main",
    "abstract": "As a combination of emerging multi-view learning methods and traditional multi-label classification tasks, multi-view multi-label classification has shown broad application prospects. The diverse semantic information contained in heterogeneous data effectively enables the further development of multi-label classification. However, the widespread incompleteness problem on multi-view features and labels greatly hinders the practical application of multi-view multi-label classification. Therefore, in this paper, we propose an attention-induced missing instances imputation technique to enhance the generalization ability of the model. Different from existing incomplete multi-view completion methods, we attempt to approximate the latent features of missing instances in embedding space according to cross-view joint attention, instead of recovering missing views in kernel space or original feature space. Accordingly, multi-view completed features are dynamically weighted by the confidence derived from joint attention in the late fusion phase. In addition, we propose a multi-view multi-label classification framework based on label-semantic feature learning, utilizing the statistical weak label correlation matrix and graph attention network to guide the learning process of label-specific features. Finally, our model is compatible with missing multi-view and partial multi-label data simultaneously and extensive experiments on five datasets confirm the advancement and effectiveness of our embedding imputation method and multi-view multi-label classification model",
    "checked": true,
    "id": "629531ac1749e830045e96c2db388f9f365e5812",
    "semantic_title": "attention-induced embedding imputation for incomplete multi-view partial multi-label classification",
    "citation_count": 1,
    "authors": [
      "Chengliang Liu",
      "Jinlong Jia",
      "Jie Wen",
      "Yabo Liu",
      "Xiaoling Luo",
      "Chao Huang",
      "Yong Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29294": {
    "title": "Learning Temporal Resolution in Spectrogram for Audio Classification",
    "volume": "main",
    "abstract": "The audio spectrogram is a time-frequency representation that has been widely used for audio classification. One of the key attributes of the audio spectrogram is the temporal resolution, which depends on the hop size used in the Short-Time Fourier Transform (STFT). Previous works generally assume the hop size should be a constant value (e.g., 10 ms). However, a fixed temporal resolution is not always optimal for different types of sound. The temporal resolution affects not only classification accuracy but also computational cost. This paper proposes a novel method, DiffRes, that enables differentiable temporal resolution modeling for audio classification. Given a spectrogram calculated with a fixed hop size, DiffRes merges non-essential time frames while preserving important frames. DiffRes acts as a \"drop-in\" module between an audio spectrogram and a classifier and can be jointly optimized with the classification task. We evaluate DiffRes on five audio classification tasks, using mel-spectrograms as the acoustic features, followed by off-the-shelf classifier backbones. Compared with previous methods using the fixed temporal resolution, the DiffRes-based method can achieve the equivalent or better classification accuracy with at least 25% computational cost reduction. We further show that DiffRes can improve classification accuracy by increasing the temporal resolution of input acoustic features, without adding to the computational cost",
    "checked": true,
    "id": "7d7fca9642df216979c03ccca9e83e0c86ec1932",
    "semantic_title": "learning temporal resolution in spectrogram for audio classification",
    "citation_count": 3,
    "authors": [
      "Haohe Liu",
      "Xubo Liu",
      "Qiuqiang Kong",
      "Wenwu Wang",
      "Mark D. Plumbley"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29295": {
    "title": "Language-Guided Transformer for Federated Multi-Label Classification",
    "volume": "main",
    "abstract": "Federated Learning (FL) is an emerging paradigm that enables multiple users to collaboratively train a robust model in a privacy-preserving manner without sharing their private data. Most existing approaches of FL only consider traditional single-label image classification, ignoring the impact when transferring the task to multi-label image classification. Nevertheless, it is still challenging for FL to deal with user heterogeneity in their local data distribution in the real-world FL scenario, and this issue becomes even more severe in multi-label image classification. Inspired by the recent success of Transformers in centralized settings, we propose a novel FL framework for multi-label classification. Since partial label correlation may be observed by local clients during training, direct aggregation of locally updated models would not produce satisfactory performances. Thus, we propose a novel FL framework of Language-Guided Transformer (FedLGT) to tackle this challenging task, which aims to exploit and transfer knowledge across different clients for learning a robust global model. Through extensive experiments on various multi-label datasets (e.g., FLAIR, MS-COCO, etc.), we show that our FedLGT is able to achieve satisfactory performance and outperforms standard FL techniques under multi-label FL scenarios. Code is available at https://github.com/Jack24658735/FedLGT",
    "checked": true,
    "id": "8a32502f310345940547754bc813be280ff1699f",
    "semantic_title": "language-guided transformer for federated multi-label classification",
    "citation_count": 1,
    "authors": [
      "I-Jieh Liu",
      "Ci-Siang Lin",
      "Fu-En Yang",
      "Yu-Chiang Frank Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29296": {
    "title": "UPDP: A Unified Progressive Depth Pruner for CNN and Vision Transformer",
    "volume": "main",
    "abstract": "Traditional channel-wise pruning methods by reducing network channels struggle to effectively prune efficient CNN models with depth-wise convolutional layers and certain efficient modules, such as popular inverted residual blocks. Prior depth pruning methods by reducing network depths are not suitable for pruning some efficient models due to the existence of some normalization layers. Moreover, finetuning subnet with directly removing activation layers would corrupt the original model weights, hindering the pruned model from achieving high performance. To address these issues, we propose a novel depth pruning method for efficient models. Our approach proposes a novel block pruning strategy and progressive training method for the subnet. Additionally, we extend our pruning method to vision transformer models. Experimental results demonstrate that our method consistently outperforms existing depth pruning methods across various pruning configurations. We obtained three pruned ConvNeXtV1 models with our method applying on ConvNeXtV1, which surpass most SOTA efficient models with comparable inference performance. Our method also achieves state-of-the-art pruning performance on the vision transformer model",
    "checked": true,
    "id": "b2e90c65a948100bb29db8cac5e72a9ba835e0c6",
    "semantic_title": "updp: a unified progressive depth pruner for cnn and vision transformer",
    "citation_count": 2,
    "authors": [
      "Ji Liu",
      "Dehua Tang",
      "Yuanxian Huang",
      "Li Zhang",
      "Xiaocheng Zeng",
      "Dong Li",
      "Mingjie Lu",
      "Jinzhang Peng",
      "Yu Wang",
      "Fan Jiang",
      "Lu Tian",
      "Ashish Sirasao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29297": {
    "title": "FedASMU: Efficient Asynchronous Federated Learning with Dynamic Staleness-Aware Model Update",
    "volume": "main",
    "abstract": "As a promising approach to deal with distributed data, Federated Learning (FL) achieves major advancements in recent years. FL enables collaborative model training by exploiting the raw data dispersed in multiple edge devices. However, the data is generally non-independent and identically distributed, i.e., statistical heterogeneity, and the edge devices significantly differ in terms of both computation and communication capacity, i.e., system heterogeneity. The statistical heterogeneity leads to severe accuracy degradation while the system heterogeneity significantly prolongs the training process. In order to address the heterogeneity issue, we propose an Asynchronous Staleness-aware Model Update FL framework, i.e., FedASMU, with two novel methods. First, we propose an asynchronous FL system model with a dynamical model aggregation method between updated local models and the global model on the server for superior accuracy and high efficiency. Then, we propose an adaptive local model adjustment method by aggregating the fresh global model with local models on devices to further improve the accuracy. Extensive experimentation with 6 models and 5 public datasets demonstrates that FedASMU significantly outperforms baseline approaches in terms of accuracy (0.60% to 23.90% higher) and efficiency (3.54% to 97.98% faster)",
    "checked": true,
    "id": "a5508ced410b12b1b786456383ec83dbe7ebf32b",
    "semantic_title": "fedasmu: efficient asynchronous federated learning with dynamic staleness-aware model update",
    "citation_count": 6,
    "authors": [
      "Ji Liu",
      "Juncheng Jia",
      "Tianshi Che",
      "Chao Huo",
      "Jiaxiang Ren",
      "Yang Zhou",
      "Huaiyu Dai",
      "Dejing Dou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29298": {
    "title": "Towards Making Learnware Specification and Market Evolvable",
    "volume": "main",
    "abstract": "The learnware paradigm aims to establish a market of numerous well-performed machine learning models, enabling users to leverage existing helpful models for their tasks instead of starting from scratch. Each learnware in the market is a model submitted by its developer, associated with a specification generated with the help of learnware market, representing the model's specialty and utility and enabling it to be identified for new user tasks. As the market continuously scales up, accommodating an ever-increasing number of learnwares, the critical challenge of the learnware paradigm is to effectively and efficiently identify the most helpful learnware(s) for a new user task without accessing the user's raw data. In this paper, to achieve increasingly accurate learnware characterization and identification along with a growing number of learnwares in the market, we propose an approach called Evolvable Learnware Specification with Index (ELSI). Specifically, based on the key idea of leveraging the task information within learnware specifications, we tackle the challenge of ascertaining the capabilities of models beyond their original training tasks, thereby enabling learnware specifications and the entire market to evolve continuously. Furthermore, through organizing learnwares and constructing specification indexes, we design a practical procedure to accurately and efficiently identify helpful learnwares without examining the entire market. Theoretical analysis and extensive experiments on a learnware market prototype encompassing thousands of models and covering six real-world scenarios validate the effectiveness and efficiency of our approach",
    "checked": true,
    "id": "e934e6abf064299adeb1872ab62b21de996373ed",
    "semantic_title": "towards making learnware specification and market evolvable",
    "citation_count": 1,
    "authors": [
      "Jian-Dong Liu",
      "Zhi-Hao Tan",
      "Zhi-Hua  Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29299": {
    "title": "TimesURL: Self-Supervised Contrastive Learning for Universal Time Series Representation Learning",
    "volume": "main",
    "abstract": "Learning universal time series representations applicable to various types of downstream tasks is challenging but valuable in real applications. Recently, researchers have attempted to leverage the success of self-supervised contrastive learning (SSCL) in Computer Vision(CV) and Natural Language Processing(NLP) to tackle time series representation. Nevertheless, due to the special temporal characteristics, relying solely on empirical guidance from other domains may be ineffective for time series and difficult to adapt to multiple downstream tasks. To this end, we review three parts involved in SSCL including 1) designing augmentation methods for positive pairs, 2) constructing (hard) negative pairs, and 3) designing SSCL loss. For 1) and 2), we find that unsuitable positive and negative pair construction may introduce inappropriate inductive biases, which neither preserve temporal properties nor provide sufficient discriminative features. For 3), just exploring segment- or instance-level semantics information is not enough for learning universal representation. To remedy the above issues, we propose a novel self-supervised framework named TimesURL. Specifically, we first introduce a frequency-temporal-based augmentation to keep the temporal property unchanged. And then, we construct double Universums as a special kind of hard negative to guide better contrastive learning. Additionally, we introduce time reconstruction as a joint optimization objective with contrastive learning to capture both segment-level and instance-level information. As a result, TimesURL can learn high-quality universal representations and achieve state-of-the-art performance in 6 different downstream tasks, including short- and long-term forecasting, imputation, classification, anomaly detection and transfer learning",
    "checked": true,
    "id": "65f345b4bbf6a147d8bb5c479fb0cd7b39f2eab5",
    "semantic_title": "timesurl: self-supervised contrastive learning for universal time series representation learning",
    "citation_count": 3,
    "authors": [
      "Jiexi Liu",
      "Songcan Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29300": {
    "title": "Faster Stochastic Variance Reduction Methods for Compositional MiniMax Optimization",
    "volume": "main",
    "abstract": "This paper delves into the realm of stochastic optimization for compositional minimax optimization—a pivotal challenge across various machine learning domains, including deep AUC and reinforcement learning policy evaluation. Despite its significance, the problem of compositional minimax optimization is still under-explored. Adding to the complexity, current methods of compositional minimax optimization are plagued by sub-optimal complexities or heavy reliance on sizable batch sizes. To respond to these constraints, this paper introduces a novel method, called Nested STOchastic Recursive Momentum (NSTORM), which can achieve the optimal sample complexity and obtain the nearly accuracy solution, matching the existing minimax methods. We also demonstrate that NSTORM can achieve the same sample complexity under the Polyak-Lojasiewicz (PL)-condition—an insightful extension of its capabilities. Yet, NSTORM encounters an issue with its requirement for low learning rates, potentially constraining its real-world applicability in machine learning. To overcome this hurdle, we present ADAptive NSTORM (ADA-NSTORM) with adaptive learning rates. We demonstrate that ADA-NSTORM can achieve the same sample complexity but the experimental results show its more effectiveness. All the proposed complexities indicate that our proposed methods can match lower bounds to existing minimax optimizations, without requiring a large batch size in each iteration. Extensive experiments support the efficiency of our proposed methods",
    "checked": true,
    "id": "a8eaa2098318cb5a5008ea9fa8042ac39886a59e",
    "semantic_title": "faster stochastic variance reduction methods for compositional minimax optimization",
    "citation_count": 0,
    "authors": [
      "Jin Liu",
      "Xiaokang Pan",
      "Junwen Duan",
      "Hong-Dong Li",
      "Youqi Li",
      "Zhe Qu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29301": {
    "title": "Sketched Newton Value Iteration for Large-Scale Markov Decision Processes",
    "volume": "main",
    "abstract": "Value Iteration (VI) is one of the most classic algorithms for solving Markov Decision Processes (MDPs), which lays the foundations for various more advanced reinforcement learning algorithms, such as Q-learning. VI may take a large number of iterations to converge as it is a first-order method. In this paper, we introduce the Newton Value Iteration (NVI) algorithm, which eliminates the impact of action space dimension compared to some previous second-order methods. Consequently, NVI can efficiently handle MDPs with large action spaces. Building upon NVI, we propose a novel approach called Sketched Newton Value Iteration (SNVI) to tackle MDPs with both large state and action spaces. SNVI not only inherits the stability and fast convergence advantages of second-order algorithms, but also significantly reduces computational complexity, making it highly scalable. Extensive experiments demonstrate the superiority of our algorithms over traditional VI and previously proposed second-order VI algorithms",
    "checked": true,
    "id": "1e58e5cdb2630ebe24a5d58a9c8fa0fb59aa6d07",
    "semantic_title": "sketched newton value iteration for large-scale markov decision processes",
    "citation_count": 0,
    "authors": [
      "Jinsong Liu",
      "Chenghan Xie",
      "Qi Deng",
      "Dongdong Ge",
      "Yinyu Ye"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29302": {
    "title": "Beyond OOD State Actions: Supported Cross-Domain Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "Offline reinforcement learning (RL) aims to learn a policy using only pre-collected and fixed data. Although avoiding the time-consuming online interactions in RL, it poses challenges for out-of-distribution (OOD) state actions and often suffers from data inefficiency for training. Despite many efforts being devoted to addressing OOD state actions, the latter (data inefficiency) receives little attention in offline RL. To address this, this paper proposes the cross-domain offline RL, which assumes offline data incorporate additional source-domain data from varying transition dynamics (environments), and expects it to contribute to the offline data efficiency. To do so, we identify a new challenge of OOD transition dynamics, beyond the common OOD state actions issue, when utilizing cross-domain offline data. Then, we propose our method BOSA, which employs two support-constrained objectives to address the above OOD issues. Through extensive experiments in the cross-domain offline RL setting, we demonstrate BOSA can greatly improve offline data efficiency: using only 10% of the target data, BOSA could achieve 74.4% of the SOTA offline RL performance that uses 100% of the target data. Additionally, we also show BOSA can be effortlessly plugged into model-based offline RL and noising data augmentation techniques (used for generating source-domain data), which naturally avoids the potential dynamics mismatch between target-domain data and newly generated source-domain data",
    "checked": true,
    "id": "c7439db4be4e9034472c852ca3a04e5a5d283b37",
    "semantic_title": "beyond ood state actions: supported cross-domain offline reinforcement learning",
    "citation_count": 10,
    "authors": [
      "Jinxin Liu",
      "Ziqi Zhang",
      "Zhenyu Wei",
      "Zifeng Zhuang",
      "Yachen Kang",
      "Sibo Gai",
      "Donglin Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29303": {
    "title": "OVD-Explorer: Optimism Should Not Be the Sole Pursuit of Exploration in Noisy Environments",
    "volume": "main",
    "abstract": "In reinforcement learning, the optimism in the face of uncertainty (OFU) is a mainstream principle for directing exploration towards less explored areas, characterized by higher uncertainty. However, in the presence of environmental stochasticity (noise), purely optimistic exploration may lead to excessive probing of high-noise areas, consequently impeding exploration efficiency. Hence, in exploring noisy environments, while optimism-driven exploration serves as a foundation, prudent attention to alleviating unnecessary over-exploration in high-noise areas becomes beneficial. In this work, we propose Optimistic Value Distribution Explorer (OVD-Explorer) to achieve a noise-aware optimistic exploration for continuous control. OVD-Explorer proposes a new measurement of the policy's exploration ability considering noise in optimistic perspectives, and leverages gradient ascent to drive exploration. Practically, OVD-Explorer can be easily integrated with continuous control RL algorithms. Extensive evaluations on the MuJoCo and GridChaos tasks demonstrate the superiority of OVD-Explorer in achieving noise-aware optimistic exploration",
    "checked": true,
    "id": "91edf9d897ddfa4c39f013d6acc0f2a0905ef789",
    "semantic_title": "ovd-explorer: optimism should not be the sole pursuit of exploration in noisy environments",
    "citation_count": 2,
    "authors": [
      "Jinyi Liu",
      "Zhi Wang",
      "Yan Zheng",
      "Jianye Hao",
      "Chenjia Bai",
      "Junjie  Ye",
      "Zhen Wang",
      "Haiyin Piao",
      "Yang Sun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29304": {
    "title": "Rethinking Propagation for Unsupervised Graph Domain Adaptation",
    "volume": "main",
    "abstract": "Unsupervised Graph Domain Adaptation (UGDA) aims to transfer knowledge from a labelled source graph to an unlabelled target graph in order to address the distribution shifts between graph domains. Previous works have primarily focused on aligning data from the source and target graph in the representation space learned by graph neural networks (GNNs). However, the inherent generalization capability of GNNs has been largely overlooked. Motivated by our empirical analysis, we reevaluate the role of GNNs in graph domain adaptation and uncover the pivotal role of the propagation process in GNNs for adapting to different graph domains. We provide a comprehensive theoretical analysis of UGDA and derive a generalization bound for multi-layer GNNs. By formulating GNN Lipschitz for k-layer GNNs, we show that the target risk bound can be tighter by removing propagation layers in source graph and stacking multiple propagation layers in target graph. Based on the empirical and theoretical analysis mentioned above, we propose a simple yet effective approach called A2GNN for graph domain adaptation. Through extensive experiments on real-world datasets, we demonstrate the effectiveness of our proposed A2GNN framework",
    "checked": true,
    "id": "710df0e715107808680f7dded097ce9ae2d7867b",
    "semantic_title": "rethinking propagation for unsupervised graph domain adaptation",
    "citation_count": 2,
    "authors": [
      "Meihan Liu",
      "Zeyu Fang",
      "Zhen Zhang",
      "Ming Gu",
      "Sheng Zhou",
      "Xin Wang",
      "Jiajun Bu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29305": {
    "title": "ECHO-GL: Earnings Calls-Driven Heterogeneous Graph Learning for Stock Movement Prediction",
    "volume": "main",
    "abstract": "Stock movement prediction serves an important role in quantitative trading. Despite advances in existing models that enhance stock movement prediction by incorporating stock relations, these prediction models face two limitations, i.e., constructing either insufficient or static stock relations, which fail to effectively capture the complex dynamic stock relations because such complex dynamic stock relations are influenced by various factors in the ever-changing financial market. To tackle the above limitations, we propose a novel stock movement prediction model ECHO-GL based on stock relations derived from earnings calls. ECHO-GL not only constructs comprehensive stock relations by exploiting the rich semantic information in the earnings calls but also captures the movement signals between related stocks based on multimodal and heterogeneous graph learning. Moreover, ECHO-GL customizes learnable stock stochastic processes based on the post earnings announcement drift (PEAD) phenomenon to generate the temporal stock price trajectory, which can be easily plugged into any investment strategy with different time horizons to meet investment demands. Extensive experiments on two financial datasets demonstrate the effectiveness of ECHO-GL on stock price movement prediction tasks together with high prediction accuracy and trading profitability",
    "checked": true,
    "id": "a5c9794a2746d76fbb3b918b64bb212e13155a5e",
    "semantic_title": "echo-gl: earnings calls-driven heterogeneous graph learning for stock movement prediction",
    "citation_count": 0,
    "authors": [
      "Mengpu Liu",
      "Mengying Zhu",
      "Xiuyuan Wang",
      "Guofang Ma",
      "Jianwei Yin",
      "Xiaolin Zheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29306": {
    "title": "Decentralized Scheduling with QoS Constraints: Achieving O(1) QoS Regret of Multi-Player Bandits",
    "volume": "main",
    "abstract": "We consider a decentralized multi-player multi-armed bandit (MP-MAB) problem where players cannot observe the actions and rewards of other players and no explicit communication or coordination between players is possible. Prior studies mostly focus on maximizing the sum of rewards of the players over time. However, the total reward maximization learning may lead to imbalanced reward among players, leading to poor Quality of Service (QoS) for some players. In contrast, our objective is to let each player n achieve a predetermined expected average reward over time, i.e., achieving a predetermined level of QoS. We develop a novel decentralized MP-MAB algorithm to accomplish this objective by leveraging the methodology of randomized matching. We prove that our decentralized algorithm can ensure that all players have an O(1) QoS regret. We also reveal an analog between our MP-MAB model and the online wireless queuing systems, which builds a connection between QoS in MP-MAB learning and stability in queuing theory",
    "checked": true,
    "id": "7a4aafb9b27dec6eb67eb40a792a327024f510ef",
    "semantic_title": "decentralized scheduling with qos constraints: achieving o(1) qos regret of multi-player bandits",
    "citation_count": 1,
    "authors": [
      "Qingsong Liu",
      "Zhixuan Fang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29307": {
    "title": "ASWT-SGNN: Adaptive Spectral Wavelet Transform-Based Self-Supervised Graph Neural Network",
    "volume": "main",
    "abstract": "Graph Comparative Learning (GCL) is a self-supervised method that combines the advantages of Graph Convolutional Networks (GCNs) and comparative learning, making it promising for learning node representations. However, the GCN encoders used in these methods rely on the Fourier transform to learn fixed graph representations, which is inherently limited by the uncertainty principle involving spatial and spectral localization trade-offs. To overcome the inflexibility of existing methods and the computationally expensive eigen-decomposition and dense matrix multiplication, this paper proposes an Adaptive Spectral Wavelet Transform-based Self-Supervised Graph Neural Network (ASWT-SGNN). The proposed method employs spectral adaptive polynomials to approximate the filter function and optimize the wavelet using contrast loss. This design enables the creation of local filters in both spectral and spatial domains, allowing flexible aggregation of neighborhood information at various scales and facilitating controlled transformation between local and global information. Compared to existing methods, the proposed approach reduces computational complexity and addresses the limitation of graph convolutional neural networks, which are constrained by graph size and lack flexible control over the neighborhood aspect. Extensive experiments on eight benchmark datasets demonstrate that ASWT-SGNN accurately approximates the filter function in high-density spectral regions, avoiding costly eigen-decomposition. Furthermore, ASWT-SGNN achieves comparable performance to state-of-the-art models in node classification tasks",
    "checked": true,
    "id": "a4d96caad5e7ba476babb2e3ffa3e2655399465c",
    "semantic_title": "aswt-sgnn: adaptive spectral wavelet transform-based self-supervised graph neural network",
    "citation_count": 1,
    "authors": [
      "Ruyue Liu",
      "Rong Yin",
      "Yong Liu",
      "Weiping Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29308": {
    "title": "Density Matters: Improved Core-Set for Active Domain Adaptive Segmentation",
    "volume": "main",
    "abstract": "Active domain adaptation has emerged as a solution to balance the expensive annotation cost and the performance of trained models in semantic segmentation. However, existing works usually ignore the correlation between selected samples and its local context in feature space, which leads to inferior usage of annotation budgets. In this work, we revisit the theoretical bound of the classical Core-set method and identify that the performance is closely related to the local sample distribution around selected samples. To estimate the density of local samples efficiently, we introduce a local proxy estimator with Dynamic Masked Convolution and develop a Density-aware Greedy algorithm to optimize the bound. Extensive experiments demonstrate the superiority of our approach. Moreover, with very few labels, our scheme achieves comparable performance to the fully supervised counterpart",
    "checked": true,
    "id": "bb75514d0dd5761ff0f665b952b6d14a4228f362",
    "semantic_title": "density matters: improved core-set for active domain adaptive segmentation",
    "citation_count": 0,
    "authors": [
      "Shizhan Liu",
      "Zhengkai Jiang",
      "Yuxi Li",
      "Jinlong Peng",
      "Yabiao Wang",
      "Weiyao Lin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29309": {
    "title": "RPSC: Robust Pseudo-Labeling for Semantic Clustering",
    "volume": "main",
    "abstract": "Clustering methods achieve performance improvement by jointly learning representation and cluster assignment. However, they do not consider the confidence of pseudo-labels which are not optimal as supervised information, resulting into error accumulation. To address this issue, we propose a Robust Pseudo-labeling for Semantic Clustering (RPSC) approach, which includes two stages. In the first stage (RPSC-Self), we design a semantic pseudo-labeling scheme by using the consistency of samples, i.e., samples with same semantics should be close to each other in the embedding space. To exploit robust semantic pseudo-labels for self-supervised learning, we propose a soft contrastive loss (SCL) which encourage the model to believe high-confidence sematic pseudo-labels and be less driven by low-confidence pseudo-labels. In the second stage (RPSC-Semi), we first determine the semantic pseudo-label of a sample based on the distance between itself and cluster centers, followed by screening out reliable semantic pseudo-label by exploiting the consistency. These reliable pseudo-labels are used as supervised information in the pseudo-semi-supervised learning algorithm to further improve the performance. Experimental results show that RPSC outperforms 18 competitive clustering algorithms significantly on six challenging image benchmarks. In particular, RPSC achieves an accuracy of 0.688 on ImageNet-Dogs, which is an up to 24% improvement, compared with the second-best method. Meanwhile, we conduct ablation studies to investigate effects of different augmented strategies on RPSC as well as contributions of terms in SCL to clustering performance. Besides, experimental results indicate that SCL can be easily integrated into existing clustering methods and bring performance improvement",
    "checked": true,
    "id": "314a05b573c635c883802dce7e5eb4773fe383e9",
    "semantic_title": "rpsc: robust pseudo-labeling for semantic clustering",
    "citation_count": 0,
    "authors": [
      "Sihang Liu",
      "Wenming Cao",
      "Ruigang Fu",
      "Kaixiang Yang",
      "Zhiwen Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29310": {
    "title": "Sample-Level Cross-View Similarity Learning for Incomplete Multi-View Clustering",
    "volume": "main",
    "abstract": "Incomplete multi-view clustering has attracted much attention due to its ability to handle partial multi-view data. Recently, similarity-based methods have been developed to explore the complete relationship among incomplete multi-view data. Although widely applied to partial scenarios, most of the existing approaches are still faced with two limitations. Firstly, fusing similarities constructed individually on each view fails to yield a complete unified similarity. Moreover, incomplete similarity generation may lead to anomalous similarity values with column sum constraints, affecting the final clustering results. To solve the above challenging issues, we propose a Sample-level Cross-view Similarity Learning (SCSL) method for Incomplete Multi-view Clustering. Specifically, we project all samples to the same dimension and simultaneously construct a complete similarity matrix across views based on the inter-view sample relationship and the intra-view sample relationship. In addition, a simultaneously learning consensus representation ensures the validity of the projection, which further enhances the quality of the similarity matrix through the graph Laplacian regularization. Experimental results on six benchmark datasets demonstrate the ability of SCSL in processing incomplete multi-view clustering tasks. Our code is publicly available at https://github.com/Tracesource/SCSL",
    "checked": true,
    "id": "4225e8dba43c81a5900ec8e27051315d1ce57303",
    "semantic_title": "sample-level cross-view similarity learning for incomplete multi-view clustering",
    "citation_count": 0,
    "authors": [
      "Suyuan Liu",
      "Junpu Zhang",
      "Yi Wen",
      "Xihong Yang",
      "Siwei Wang",
      "Yi Zhang",
      "En Zhu",
      "Chang Tang",
      "Long Zhao",
      "Xinwang Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29311": {
    "title": "UFDA: Universal Federated Domain Adaptation with Practical Assumptions",
    "volume": "main",
    "abstract": "Conventional Federated Domain Adaptation (FDA) approaches usually demand an abundance of assumptions, which makes them significantly less feasible for real-world situations and introduces security hazards. This paper relaxes the assumptions from previous FDAs and studies a more practical scenario named Universal Federated Domain Adaptation (UFDA). It only requires the black-box model and the label set information of each source domain, while the label sets of different source domains could be inconsistent, and the target-domain label set is totally blind. Towards a more effective solution for our newly proposed UFDA scenario, we propose a corresponding methodology called Hot-Learning with Contrastive Label Disambiguation (HCLD). It particularly tackles UFDA's domain shifts and category gaps problems by using one-hot outputs from the black-box models of various source domains. Moreover, to better distinguish the shared and unknown classes, we further present a cluster-level strategy named Mutual-Voting Decision (MVD) to extract robust consensus knowledge across peer classes from both source and target domains. Extensive experiments on three benchmark datasets demonstrate that our method achieves comparable performance for our UFDA scenario with much fewer assumptions, compared to previous methodologies with comprehensive additional assumptions",
    "checked": true,
    "id": "926018ec46c14cf937711312e8784fbe573f1749",
    "semantic_title": "ufda: universal federated domain adaptation with practical assumptions",
    "citation_count": 0,
    "authors": [
      "Xinhui Liu",
      "Zhenghao Chen",
      "Luping Zhou",
      "Dong Xu",
      "Wei Xi",
      "Gairui Bai",
      "Yihan Zhao",
      "Jizhong Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29312": {
    "title": "Unify Named Entity Recognition Scenarios via Contrastive Real-Time Updating Prototype",
    "volume": "main",
    "abstract": "Supervised named entity recognition (NER) aims to classify entity mentions into a fixed number of pre-defined types. However, in real-world scenarios, unknown entity types are continually involved. Naive fine-tuning will result in catastrophic forgetting on old entity types. Existing continual methods usually depend on knowledge distillation to alleviate forgetting, which are less effective on long task sequences. Moreover, most of them are specific to the class-incremental scenario and cannot adapt to the online scenario, which is more common in practice. In this paper, we propose a unified framework called Contrastive Real-time Updating Prototype (CRUP) that can handle different scenarios for NER. Specifically, we train a Gaussian projection model by a regularized contrastive objective. After training on each batch, we store the mean vectors of representations belong to new entity types as their prototypes. Meanwhile, we update existing prototypes belong to old types only based on representations of the current batch. The final prototypes will be used for the nearest class mean classification. In this way, CRUP can handle different scenarios through its batch-wise learning. Moreover, CRUP can alleviate forgetting in continual scenarios only with current data instead of old data. To comprehensively evaluate CRUP, we construct extensive benchmarks based on various datasets. Experimental results show that CRUP significantly outperforms baselines in continual scenarios and is also competitive in the supervised scenario",
    "checked": true,
    "id": "8c412d0d39af7da161b952557b90c31e8dcde559",
    "semantic_title": "unify named entity recognition scenarios via contrastive real-time updating prototype",
    "citation_count": 0,
    "authors": [
      "Yanhe Liu",
      "Peng Wang",
      "Wenjun Ke",
      "Guozheng Li",
      "Xiye Chen",
      "Jiteng Zhao",
      "Ziyu Shang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29313": {
    "title": "Effect Size Estimation for Duration Recommendation in Online Experiments: Leveraging Hierarchical Models and Objective Utility Approaches",
    "volume": "main",
    "abstract": "The selection of the assumed effect size (AES) critically determines the duration of an experiment, and hence its accuracy and efficiency. Traditionally, experimenters determine AES based on domain knowledge. However, this method becomes impractical for online experimentation services managing numerous experiments, and a more automated approach is hence of great demand. We initiate the study of data-driven AES selection in for online experimentation services by introducing two solutions. The first employs a three-layer Gaussian Mixture Model considering the heteroskedasticity across experiments, and it seeks to estimate the true expected effect size among positive experiments. The second method, grounded in utility theory, aims to determine the optimal effect size by striking a balance between the experiment's cost and the precision of decision-making. Through comparisons with baseline methods using both simulated and real data, we showcase the superior performance of the proposed approaches",
    "checked": true,
    "id": "4182c01f915f2c0c9ad24f982742b8c4f2ac34ac",
    "semantic_title": "effect size estimation for duration recommendation in online experiments: leveraging hierarchical models and objective utility approaches",
    "citation_count": 0,
    "authors": [
      "Yu Liu",
      "Runzhe Wan",
      "James McQueen",
      "Doug Hains",
      "Jinxiang Gu",
      "Rui Song"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29314": {
    "title": "Causality-Inspired Invariant Representation Learning for Text-Based Person Retrieval",
    "volume": "main",
    "abstract": "Text-based Person Retrieval (TPR) aims to retrieve relevant images of specific pedestrians based on the given textual query. The mainstream approaches primarily leverage pretrained deep neural networks to learn the mapping of visual and textual modalities into a common latent space for cross-modality matching. Despite their remarkable achievements, existing efforts mainly focus on learning the statistical cross-modality correlation found in training data, other than the intrinsic causal correlation. As a result, they often struggle to retrieve accurately in the face of environmental changes such as illumination, pose, and occlusion, or when encountering images with similar attributes. In this regard, we pioneer the observation of TPR from a causal view. Specifically, we assume that each image is composed of a mixture of causal factors (which are semantically consistent with text descriptions) and non-causal factors (retrieval-irrelevant, e.g., background), and only the former can lead to reliable retrieval judgments. Our goal is to extract text-critical robust visual representation (i.e., causal factors) and establish domain invariant cross-modality correlations for accurate and reliable retrieval. However, causal/non-causal factors are unobserved, so we emphasize that ideal causal factors that can simulate causal scenes should satisfy two basic principles:1） Independence: being independent of non-causal factors, and 2）Sufficiency: being causally sufficient for TPR across different environments. Building on that, we propose an Invariant Representation Learning method for TPR (IRLT), that enforces the visual representations to satisfy the two aforementioned critical properties. Extensive experiments on three datasets clearly demonstrate the advantages of IRLT over leading baselines in terms of accuracy and generalization",
    "checked": true,
    "id": "a15de2653c2750a09eaf4e252aa700b50b694c2e",
    "semantic_title": "causality-inspired invariant representation learning for text-based person retrieval",
    "citation_count": 1,
    "authors": [
      "Yu Liu",
      "Guihe Qin",
      "Haipeng Chen",
      "Zhiyong Cheng",
      "Xun Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29315": {
    "title": "Detection-Based Intermediate Supervision for Visual Question Answering",
    "volume": "main",
    "abstract": "Recently, neural module networks (NMNs) have yielded ongoing success in answering compositional visual questions, especially those involving multi-hop visual and logical reasoning. NMNs decompose the complex question into several sub-tasks using instance-modules from the reasoning paths of that question and then exploit intermediate supervisions to guide answer prediction, thereby improving inference interpretability. However, their performance may be hindered due to sketchy modeling of intermediate supervisions. For instance, (1) a prior assumption that each instance-module refers to only one grounded object yet overlooks other potentially associated grounded objects, impeding full cross-modal alignment learning; (2) IoU-based intermediate supervisions may introduce noise signals as the bounding box overlap issue might guide the model's focus towards irrelevant objects. To address these issues, a novel method, Detection-based Intermediate Supervision (DIS), is proposed, which adopts a generative detection framework to facilitate multiple grounding supervisions via sequence generation. As such, DIS offers more comprehensive and accurate intermediate supervisions, thereby boosting answer prediction performance. Furthermore, by considering intermediate results, DIS enhances the consistency in answering compositional questions and their sub-questions. Extensive experiments demonstrate the superiority of our proposed DIS, showcasing both improved accuracy and state-of-the-art reasoning consistency compared to prior approaches",
    "checked": true,
    "id": "de0f5cca5cbf90f645773562662dff295c0d777b",
    "semantic_title": "detection-based intermediate supervision for visual question answering",
    "citation_count": 0,
    "authors": [
      "Yuhang Liu",
      "Daowan Peng",
      "Wei Wei",
      "Yuanyuan Fu",
      "Wenfeng Xie",
      "Dangyang Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29316": {
    "title": "Text Diffusion with Reinforced Conditioning",
    "volume": "main",
    "abstract": "Diffusion models have demonstrated exceptional capability in generating high-quality images, videos, and audio. Due to their adaptiveness in iterative refinement, they provide a strong potential for achieving better non-autoregressive sequence generation. However, existing text diffusion models still fall short in their performance due to a challenge in handling the discreteness of language. This paper thoroughly analyzes text diffusion models and uncovers two significant limitations: degradation of self-conditioning during training and misalignment between training and sampling. Motivated by our findings, we propose a novel Text Diffusion model called TReC, which mitigates the degradation with Reinforced Conditioning and the misalignment by Time-Aware Variance Scaling. Our extensive experiments demonstrate the competitiveness of TReC against autoregressive, non-autoregressive, and diffusion baselines. Moreover, qualitative analysis shows its advanced ability to fully utilize the diffusion process in refining samples",
    "checked": true,
    "id": "146cf9e9eeb1807034a8622504529c5ef93e956f",
    "semantic_title": "text diffusion with reinforced conditioning",
    "citation_count": 0,
    "authors": [
      "Yuxuan Liu",
      "Tianchi Yang",
      "Shaohan Huang",
      "Zihan Zhang",
      "Haizhen Huang",
      "Furu Wei",
      "Weiwei Deng",
      "Feng Sun",
      "Qi Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29317": {
    "title": "Diffusion Language-Shapelets for Semi-supervised Time-Series Classification",
    "volume": "main",
    "abstract": "Semi-supervised time-series classification could effectively alleviate the issue of lacking labeled data. However, existing approaches usually ignore model interpretability, making it difficult for humans to understand the principles behind the predictions of a model. Shapelets are a set of discriminative subsequences that show high interpretability in time series classification tasks. Shapelet learning-based methods have demonstrated promising classification performance. Unfortunately, without enough labeled data, the shapelets learned by existing methods are often poorly discriminative, and even dissimilar to any subsequence of the original time series. To address this issue, we propose the Diffusion Language-Shapelets model (DiffShape) for semi-supervised time series classification. In DiffShape, a self-supervised diffusion learning mechanism is designed, which uses real subsequences as a condition. This helps to increase the similarity between the learned shapelets and real subsequences by using a large amount of unlabeled data. Furthermore, we introduce a contrastive language-shapelets learning strategy that improves the discriminability of the learned shapelets by incorporating the natural language descriptions of the time series. Experiments have been conducted on the UCR time series archive, and the results reveal that the proposed DiffShape method achieves state-of-the-art performance and exhibits superior interpretability over baselines",
    "checked": true,
    "id": "7755152a055770da299724fa2ca4576caa148de8",
    "semantic_title": "diffusion language-shapelets for semi-supervised time-series classification",
    "citation_count": 1,
    "authors": [
      "Zhen Liu",
      "Wenbin Pei",
      "Disen Lan",
      "Qianli Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29318": {
    "title": "Decentralized Sum-of-Nonconvex Optimization",
    "volume": "main",
    "abstract": "We consider the optimization problem of minimizing the sum-of-nonconvex function, i.e., a convex function that is the average of nonconvex components. The existing stochastic algorithms for such a problem only focus on a single machine and the centralized scenario. In this paper, we study the sum-of-nonconvex optimization in the decentralized setting. We present a new theoretical analysis of the PMGT-SVRG algorithm for this problem and prove the linear convergence of their approach. However, the convergence rate of the PMGT-SVRG algorithm has a linear dependency on the condition number, which is undesirable for the ill-conditioned problem. To remedy this issue, we propose an accelerated stochastic decentralized first-order algorithm by incorporating the techniques of acceleration, gradient tracking, and multi-consensus mixing into the SVRG algorithm. The convergence rate of the proposed method has a square-root dependency on the condition number. The numerical experiments validate the theoretical guarantee of our proposed algorithms on both synthetic and real-world datasets",
    "checked": true,
    "id": "54e601b0816299a0e09945b230684bacb12a2612",
    "semantic_title": "decentralized sum-of-nonconvex optimization",
    "citation_count": 0,
    "authors": [
      "Zhuanghua Liu",
      "Bryan Kian Hsiang Low"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29319": {
    "title": "Incremental Quasi-Newton Methods with Faster Superlinear Convergence Rates",
    "volume": "main",
    "abstract": "We consider the finite-sum optimization problem, where each component function is strongly convex and has Lipschitz continuous gradient and Hessian. The recently proposed incremental quasi-Newton method is based on BFGS update and achieves a local superlinear convergence rate that is dependent on the condition number of the problem. This paper proposes a more efficient quasi-Newton method by incorporating the symmetric rank-1 update into the incremental framework, which results in the condition-number-free local superlinear convergence rate. Furthermore, we can boost our method by applying the block update on the Hessian approximation, which leads to an even faster local convergence rate. The numerical experiments show the proposed methods significantly outperform the baseline methods",
    "checked": true,
    "id": "63be8b9cec54a452242f074b33e753740030be3a",
    "semantic_title": "incremental quasi-newton methods with faster superlinear convergence rates",
    "citation_count": 1,
    "authors": [
      "Zhuanghua Liu",
      "Luo Luo",
      "Bryan Kian Hsiang Low"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29320": {
    "title": "DART: Dual-Modal Adaptive Online Prompting and Knowledge Retention for Test-Time Adaptation",
    "volume": "main",
    "abstract": "As an up-and-coming area, CLIP-based pre-trained vision-language models can readily facilitate downstream tasks through the zero-shot or few-shot fine-tuning manners. However, they still face critical challenges in test-time generalization due to the shifts between the training and test data distributions, hindering the further improvement of the performance. To address this crucial problem, the latest works have introduced Test-Time Adaptation (TTA) techniques to CLIP which dynamically learn text prompts using only test samples. However, their limited learning capacity due to the overlook of visual modality information, and the underutilization of knowledge in previously seen test samples result in reduced performance. In this paper, we propose a novel Dual-modal Adaptive online prompting and knowledge ReTention method called DART to overcome these challenges. To increase the learning capacity, DART captures knowledge from each test sample by learning class-specific text prompts and instance-level image prompts. Additionally, to fully leverage the knowledge from previously seen test samples, DART utilizes dual-modal knowledge retention prompts to adaptively retain the acquired knowledge, thereby enhancing the predictions on subsequent test samples. Extensive experiments on various large-scale benchmarks demonstrate the effectiveness of our proposed DART against state-of-the-art methods",
    "checked": true,
    "id": "5006e5be4c3cb1c4af84e4a2717ef886b3a22464",
    "semantic_title": "dart: dual-modal adaptive online prompting and knowledge retention for test-time adaptation",
    "citation_count": 1,
    "authors": [
      "Zichen Liu",
      "Hongbo Sun",
      "Yuxin Peng",
      "Jiahuan Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29321": {
    "title": "Backdoor Attacks via Machine Unlearning",
    "volume": "main",
    "abstract": "As a new paradigm to erase data from a model and protect user privacy, machine unlearning has drawn significant attention. However, existing studies on machine unlearning mainly focus on its effectiveness and efficiency, neglecting the security challenges introduced by this technique. In this paper, we aim to bridge this gap and study the possibility of conducting malicious attacks leveraging machine unlearning. Specifically, we consider the backdoor attack via machine unlearning, where an attacker seeks to inject a backdoor in the unlearned model by submitting malicious unlearning requests, so that the prediction made by the unlearned model can be changed when a particular trigger presents. In our study, we propose two attack approaches. The first attack approach does not require the attacker to poison any training data of the model. The attacker can achieve the attack goal only by requesting to unlearn a small subset of his contributed training data. The second approach allows the attacker to poison a few training instances with a pre-defined trigger upfront, and then activate the attack via submitting a malicious unlearning request. Both attack approaches are proposed with the goal of maximizing the attack utility while ensuring attack stealthiness. The effectiveness of the proposed attacks is demonstrated with different machine unlearning algorithms as well as different models on different datasets",
    "checked": true,
    "id": "c946004572ad5ed60c9071fb95c27290030e5412",
    "semantic_title": "backdoor attacks via machine unlearning",
    "citation_count": 6,
    "authors": [
      "Zihao Liu",
      "Tianhao Wang",
      "Mengdi Huai",
      "Chenglin Miao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29322": {
    "title": "Cooperative Knowledge Distillation: A Learner Agnostic Approach",
    "volume": "main",
    "abstract": "Knowledge distillation is a simple but powerful way to transfer knowledge between a teacher model to a student model. Existing work suffers from at least one of the following key limitations in terms of direction and scope of transfer which restrict its use: all knowledge is transferred from teacher to student regardless of whether or not that knowledge is useful, the student is the only one learning in this exchange, and typically distillation transfers knowledge only from a single teacher to a single student. We formulate a novel form of knowledge distillation in which many models can act as both students and teachers which we call cooperative distillation. The models cooperate as follows: a model (the student) identifies specific deficiencies in it's performance and searches for another model (the teacher) who encodes learned knowledge into instructional virtual instances via counterfactual instance generation. Because different models may have different strengths and weaknesses, all models can act as either students or teachers (cooperation) when appropriate and only distill knowledge in areas specific to their strengths (focus). Since counterfactuals as a paradigm are not tied to any specific algorithm, we can use this method to distill knowledge between learners of different architectures, algorithms, and even feature spaces. We demonstrate our approach not only outperforms baselines such as transfer learning, self-supervised learning, and multiple knowledge distillation algorithms on several datasets, but it can also be used in settings where the aforementioned techniques cannot",
    "checked": true,
    "id": "90c07aa7c9379d61b56a348dcc1138d6d5dcd92d",
    "semantic_title": "cooperative knowledge distillation: a learner agnostic approach",
    "citation_count": 0,
    "authors": [
      "Michael Livanos",
      "Ian Davidson",
      "Stephen Wong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29323": {
    "title": "On the Convergence of an Adaptive Momentum Method for Adversarial Attacks",
    "volume": "main",
    "abstract": "Adversarial examples are commonly created by solving a constrained optimization problem, typically using sign-based methods like Fast Gradient Sign Method (FGSM). These attacks can benefit from momentum with a constant parameter, such as Momentum Iterative FGSM (MI-FGSM), to enhance black-box transferability. However, the monotonic time-varying momentum parameter is required to guarantee convergence in theory, creating a theory-practice gap. Additionally, recent work shows that sign-based methods fail to converge to the optimum in several convex settings, exacerbating the issue. To address these concerns, we propose a novel method which incorporates both an innovative adaptive momentum parameter without monotonicity assumptions and an adaptive step-size scheme that replaces the sign operation. Furthermore, we derive a regret upper bound for general convex functions. Experiments on multiple models demonstrate the efficacy of our method in generating adversarial examples with human-imperceptible noise while achieving high attack success rates, indicating its superiority over previous adversarial example generation methods",
    "checked": true,
    "id": "8ffcd8d2d174fe304ecb51d9ff03d3094e01991f",
    "semantic_title": "on the convergence of an adaptive momentum method for adversarial attacks",
    "citation_count": 1,
    "authors": [
      "Sheng Long",
      "Wei Tao",
      "Shuohao LI",
      "Jun Lei",
      "Jun Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29324": {
    "title": "Layer Collaboration in the Forward-Forward Algorithm",
    "volume": "main",
    "abstract": "Backpropagation, which uses the chain rule, is the de-facto standard algorithm for optimizing neural networks nowadays. Recently, Hinton (2022) proposed the forward-forward algorithm, a promising alternative that optimizes neural nets layer-by-layer, without propagating gradients throughout the network. Although such an approach has several advantages over back-propagation and shows promising results, the fact that each layer is being trained independently limits the optimization process. Specifically, it prevents the network's layers from collaborating to learn complex and rich features. In this work, we study layer collaboration in the forward-forward algorithm. We show that the current version of the forward-forward algorithm is suboptimal when considering information flow in the network, resulting in a lack of collaboration between layers of the network. We propose an improved version that supports layer collaboration to better utilize the network structure, while not requiring any additional assumptions or computations. We empirically demonstrate the efficacy of the proposed version when considering both information flow and objective metrics. Additionally, we provide a theoretical motivation for the proposed method, inspired by functional entropy theory",
    "checked": true,
    "id": "aaf41f264b73342af2d78032c977924038e0223c",
    "semantic_title": "layer collaboration in the forward-forward algorithm",
    "citation_count": 4,
    "authors": [
      "Guy Lorberbom",
      "Itai Gat",
      "Yossi Adi",
      "Alexander Schwing",
      "Tamir Hazan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29325": {
    "title": "CGS-Mask: Making Time Series Predictions Intuitive for All",
    "volume": "main",
    "abstract": "Artificial intelligence (AI) has immense potential in time series prediction, but most explainable tools have limited capabilities in providing a systematic understanding of important features over time. These tools typically rely on evaluating a single time point, overlook the time ordering of inputs, and neglect the time-sensitive nature of time series applications. These factors make it difficult for users, particularly those without domain knowledge, to comprehend AI model decisions and obtain meaningful explanations. We propose CGS-Mask, a post-hoc and model-agnostic cellular genetic strip mask-based saliency approach to address these challenges. CGS-Mask uses consecutive time steps as a cohesive entity to evaluate the impact of features on the final prediction, providing binary and sustained feature importance scores over time. Our algorithm optimizes the mask population iteratively to obtain the optimal mask in a reasonable time. We evaluated CGS-Mask on synthetic and real-world datasets, and it outperformed state-of-the-art methods in elucidating the importance of features over time. According to our pilot user study via a questionnaire survey, CGS-Mask is the most effective approach in presenting easily understandable time series prediction results, enabling users to comprehend the decision-making process of AI models with ease",
    "checked": true,
    "id": "0ed9d51638450ed41b336dc1998c4b85c8d02482",
    "semantic_title": "cgs-mask: making time series predictions intuitive for all",
    "citation_count": 0,
    "authors": [
      "Feng Lu",
      "Wei Li",
      "Yifei Sun",
      "Cheng Song",
      "Yufei Ren",
      "Albert Y. Zomaya"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29326": {
    "title": "Improving Expressive Power of Spectral Graph Neural Networks with Eigenvalue Correction",
    "volume": "main",
    "abstract": "In recent years, spectral graph neural networks, characterized by polynomial filters, have garnered increasing attention and have achieved remarkable performance in tasks such as node classification. These models typically assume that eigenvalues for the normalized Laplacian matrix are distinct from each other, thus expecting a polynomial filter to have a high fitting ability. However, this paper empirically observes that normalized Laplacian matrices frequently possess repeated eigenvalues. Moreover, we theoretically establish that the number of distinguishable eigenvalues plays a pivotal role in determining the expressive power of spectral graph neural networks. In light of this observation, we propose an eigenvalue correction strategy that can free polynomial filters from the constraints of repeated eigenvalue inputs. Concretely, the proposed eigenvalue correction strategy enhances the uniform distribution of eigenvalues, thus mitigating repeated eigenvalues, and improving the fitting capacity and expressive power of polynomial filters. Extensive experimental results on both synthetic and real-world datasets demonstrate the superiority of our method",
    "checked": true,
    "id": "8967cc60bb838327e450f11bd9ce07767ea541b5",
    "semantic_title": "improving expressive power of spectral graph neural networks with eigenvalue correction",
    "citation_count": 1,
    "authors": [
      "Kangkang Lu",
      "Yanhua Yu",
      "Hao Fei",
      "Xuan Li",
      "Zixuan Yang",
      "Zirui Guo",
      "Meiyu Liang",
      "Mengran Yin",
      "Tat-Seng Chua"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29327": {
    "title": "UniADS: Universal Architecture-Distiller Search for Distillation Gap",
    "volume": "main",
    "abstract": "In this paper, we present UniADS, the first Universal Architecture-Distiller Search framework for co-optimizing student architecture and distillation policies. Teacher-student distillation gap limits the distillation gains. Previous approaches seek to discover the ideal student architecture while ignoring distillation settings. In UniADS, we construct a comprehensive search space encompassing an architectural search for student models, knowledge transformations in distillation strategies, distance functions, loss weights, and other vital settings. To efficiently explore the search space, we utilize the NSGA-II genetic algorithm for better crossover and mutation configurations and employ the Successive Halving algorithm for search space pruning, resulting in improved search efficiency and promising results. Extensive experiments are performed on different teacher-student pairs using CIFAR-100 and ImageNet datasets. The experimental results consistently demonstrate the superiority of our method over existing approaches. Furthermore, we provide a detailed analysis of the search results, examining the impact of each variable and extracting valuable insights and practical guidance for distillation design and implementation",
    "checked": true,
    "id": "93e419003755b67f1928f8620815fd47ceae82ea",
    "semantic_title": "uniads: universal architecture-distiller search for distillation gap",
    "citation_count": 3,
    "authors": [
      "Liming Lu",
      "Zhenghan Chen",
      "Xiaoyu Lu",
      "Yihang Rao",
      "Lujun Li",
      "Shuchao Pang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29328": {
    "title": "NodeMixup: Tackling Under-Reaching for Graph Neural Networks",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs) have become mainstream methods for solving the semi-supervised node classification problem. However, due to the uneven location distribution of labeled nodes in the graph, labeled nodes are only accessible to a small portion of unlabeled nodes, leading to the under-reaching issue. In this study, we firstly reveal under-reaching by conducting an empirical investigation on various well-known graphs. Then, we demonstrate that under-reaching results in unsatisfactory distribution alignment between labeled and unlabeled nodes through systematic experimental analysis, significantly degrading GNNs' performance. To tackle under-reaching for GNNs, we propose an architecture-agnostic method dubbed NodeMixup. The fundamental idea is to (1) increase the reachability of labeled nodes by labeled-unlabeled pairs mixup, (2) leverage graph structures via fusing the neighbor connections of intra-class node pairs to improve performance gains of mixup, and (3) use neighbor label distribution similarity incorporating node degrees to determine sampling weights for node mixup. Extensive experiments demonstrate the efficacy of NodeMixup in assisting GNNs in handling under-reaching. The source code is available at https://github.com/WeigangLu/NodeMixup",
    "checked": true,
    "id": "16a5e3150652f96dde10852769d3b62d25f3b7c9",
    "semantic_title": "nodemixup: tackling under-reaching for graph neural networks",
    "citation_count": 1,
    "authors": [
      "Weigang Lu",
      "Ziyu Guan",
      "Wei Zhao",
      "Yaming Yang",
      "Long Jin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29329": {
    "title": "Federated Learning with Extremely Noisy Clients via Negative Distillation",
    "volume": "main",
    "abstract": "Federated learning (FL) has shown remarkable success in cooperatively training deep models, while typically struggling with noisy labels. Advanced works propose to tackle label noise by a re-weighting strategy with a strong assumption, i.e., mild label noise. However, it may be violated in many real-world FL scenarios because of highly contaminated clients, resulting in extreme noise ratios, e.g., >90%. To tackle extremely noisy clients, we study the robustness of the re-weighting strategy, showing a pessimistic conclusion: minimizing the weight of clients trained over noisy data outperforms re-weighting strategies. To leverage models trained on noisy clients, we propose a novel approach, called negative distillation (FedNed). FedNed first identifies noisy clients and employs rather than discards the noisy clients in a knowledge distillation manner. In particular, clients identified as noisy ones are required to train models using noisy labels and pseudo-labels obtained by global models. The model trained on noisy labels serves as a ‘bad teacher' in knowledge distillation, aiming to decrease the risk of providing incorrect information. Meanwhile, the model trained on pseudo-labels is involved in model aggregation if not identified as a noisy client. Consequently, through pseudo-labeling, FedNed gradually increases the trustworthiness of models trained on noisy clients, while leveraging all clients for model aggregation through negative distillation. To verify the efficacy of FedNed, we conduct extensive experiments under various settings, demonstrating that FedNed can consistently outperform baselines and achieve state-of-the-art performance",
    "checked": true,
    "id": "b9fb64d0d26c9dda06a8a69bdd158d0b53d40aa6",
    "semantic_title": "federated learning with extremely noisy clients via negative distillation",
    "citation_count": 0,
    "authors": [
      "Yang Lu",
      "Lin Chen",
      "Yonggang Zhang",
      "Yiliang Zhang",
      "Bo Han",
      "Yiu-ming Cheung",
      "Hanzi Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29330": {
    "title": "Decoupled Contrastive Multi-View Clustering with High-Order Random Walks",
    "volume": "main",
    "abstract": "In recent, some robust contrastive multi-view clustering (MvC) methods have been proposed, which construct data pairs from neighborhoods to alleviate the false negative issue, i.e., some intra-cluster samples are wrongly treated as negative pairs. Although promising performance has been achieved by these methods, the false negative issue is still far from addressed and the false positive issue emerges because all in- and out-of-neighborhood samples are simply treated as positive and negative, respectively. To address the issues, we propose a novel robust method, dubbed decoupled contrastive multi-view clustering with high-order random walks (DIVIDE). In brief, DIVIDE leverages random walks to progressively identify data pairs in a global instead of local manner. As a result, DIVIDE could identify in-neighborhood negatives and out-of-neighborhood positives. Moreover, DIVIDE embraces a novel MvC architecture to perform inter- and intra-view contrastive learning in different embedding spaces, thus boosting clustering performance and embracing the robustness against missing views. To verify the efficacy of DIVIDE, we carry out extensive experiments on four benchmark datasets comparing with nine state-of-the-art MvC methods in both complete and incomplete MvC settings. The code is released on https://github.com/XLearning-SCU/2024-AAAI-DIVIDE",
    "checked": true,
    "id": "ea7cc8dca6e73a455d2f8ec63dde767b34e856ee",
    "semantic_title": "decoupled contrastive multi-view clustering with high-order random walks",
    "citation_count": 4,
    "authors": [
      "Yiding Lu",
      "Yijie Lin",
      "Mouxing Yang",
      "Dezhong Peng",
      "Peng Hu",
      "Xi Peng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29331": {
    "title": "Are You Concerned about Limited Function Evaluations: Data-Augmented Pareto Set Learning for Expensive Multi-Objective Optimization",
    "volume": "main",
    "abstract": "Optimizing multiple conflicting black-box objectives simultaneously is a prevalent occurrence in many real-world applications, such as neural architecture search, and machine learning. These problems are known as expensive multi-objective optimization problems (EMOPs) when the function evaluations are computationally or financially costly. Multi-objective Bayesian optimization (MOBO) offers an efficient approach to discovering a set of Pareto optimal solutions. However, the data deficiency issue caused by limited function evaluations has posed a great challenge to current optimization methods. Moreover, most current methods tend to prioritize the quality of candidate solutions, while ignoring the quantity of promising samples. In order to tackle these issues, our paper proposes a novel multi-objective Bayesian optimization algorithm with a data augmentation strategy that provides ample high-quality samples for Pareto set learning (PSL). Specifically, it utilizes Generative Adversarial Networks (GANs) to enrich data and a dominance prediction model to screen out high-quality samples, mitigating the predicament of limited function evaluations in EMOPs. Additionally, we adopt the regularity model to expensive multi-objective Bayesian optimization for PSL. Experimental results on both synthetic and real-world problems demonstrate that our algorithm outperforms several state-of-the-art and classical algorithms",
    "checked": true,
    "id": "04289ccc35e69708dee81ff1a67dad4563c8f84b",
    "semantic_title": "are you concerned about limited function evaluations: data-augmented pareto set learning for expensive multi-objective optimization",
    "citation_count": 1,
    "authors": [
      "Yongfan Lu",
      "Bingdong Li",
      "Aimin Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29332": {
    "title": "Autoregressive Omni-Aware Outpainting for Open-Vocabulary 360-Degree Image Generation",
    "volume": "main",
    "abstract": "A 360-degree (omni-directional) image provides an all-encompassing spherical view of a scene. Recently, there has been an increasing interest in synthesising 360-degree images from conventional narrow field of view (NFoV) images captured by digital cameras and smartphones, for providing immersive experiences in various scenarios such as virtual reality. Yet, existing methods typically fall short in synthesizing intricate visual details or ensure the generated images align consistently with user-provided prompts. In this study, autoregressive omni-aware generative network (AOG-Net) is proposed for 360-degree image generation by outpainting an incomplete 360-degree image progressively with NFoV and text guidances joinly or individually. This autoregressive scheme not only allows for deriving finer-grained and text-consistent patterns by dynamically generating and adjusting the process but also offers users greater flexibility to edit their conditions throughout the generation process. A global-local conditioning mechanism is devised to comprehensively formulate the outpainting guidance in each autoregressive step. Text guidances, omni-visual cues, NFoV inputs and omni-geometry are encoded and further formulated with cross-attention based transformers into a global stream and a local stream into a conditioned generative backbone model. As AOG-Net is compatible to leverage large-scale models for the conditional encoder and the generative prior, it enables the generation to use extensive open-vocabulary text guidances. Comprehensive experiments on two commonly used 360-degree image datasets for both indoor and outdoor settings demonstrate the state-of-the-art performance of our proposed method. Our code is available at https://github.com/zhuqiangLu/AOG-NET-360",
    "checked": true,
    "id": "1e291a7c189f4cce66cf647fdae9546465f73341",
    "semantic_title": "autoregressive omni-aware outpainting for open-vocabulary 360-degree image generation",
    "citation_count": 3,
    "authors": [
      "Zhuqiang Lu",
      "Kun Hu",
      "Chaoyue Wang",
      "Lei Bai",
      "Zhiyong Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29333": {
    "title": "Leveraging Diffusion Perturbations for Measuring Fairness in Computer Vision",
    "volume": "main",
    "abstract": "Computer vision models have been known to encode harmful biases, leading to the potentially unfair treatment of historically marginalized groups, such as people of color. However, there remains a lack of datasets balanced along demographic traits that can be used to evaluate the downstream fairness of these models. In this work, we demonstrate that diffusion models can be leveraged to create such a dataset. We first use a diffusion model to generate a large set of images depicting various occupations. Subsequently, each image is edited using inpainting to generate multiple variants, where each variant refers to a different perceived race. Using this dataset, we benchmark several vision-language models on a multi-class occupation classification task. We find that images generated with non-Caucasian labels have a significantly higher occupation misclassification rate than images generated with Caucasian labels, and that several misclassifications are suggestive of racial biases. We measure a model's downstream fairness by computing the standard deviation in the probability of predicting the true occupation label across the different identity groups. Using this fairness metric, we find significant disparities between the evaluated vision-and-language models. We hope that our work demonstrates the potential value of diffusion methods for fairness evaluations",
    "checked": true,
    "id": "4560e2a2be6b4de619308284bb4e01020d8384f2",
    "semantic_title": "leveraging diffusion perturbations for measuring fairness in computer vision",
    "citation_count": 1,
    "authors": [
      "Nicholas Lui",
      "Bryan Chia",
      "William Berrios",
      "Candace Ross",
      "Douwe Kiela"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29334": {
    "title": "Three Heads Are Better than One: Complementary Experts for Long-Tailed Semi-supervised Learning",
    "volume": "main",
    "abstract": "We address the challenging problem of Long-Tailed Semi-Supervised Learning (LTSSL) where labeled data exhibit imbalanced class distribution and unlabeled data follow an unknown distribution. Unlike in balanced SSL, the generated pseudo-labels are skewed towards head classes, intensifying the training bias. Such a phenomenon is even amplified as more unlabeled data will be mislabeled as head classes when the class distribution of labeled and unlabeled datasets are mismatched. To solve this problem, we propose a novel method named ComPlementary Experts (CPE). Specifically, we train multiple experts to model various class distributions, each of them yielding high-quality pseudo-labels within one form of class distribution. Besides, we introduce Classwise Batch Normalization for CPE to avoid performance degradation caused by feature distribution mismatch between head and non-head classes. CPE achieves state-of-the-art performances on CIFAR-10-LT, CIFAR-100-LT, and STL-10-LT dataset benchmarks. For instance, on CIFAR-10-LT, CPE improves test accuracy by over >2.22% compared to baselines. Code is available at https://github.com/machengcheng2016/CPE-LTSSL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengcheng Ma",
      "Ismail Elezi",
      "Jiankang Deng",
      "Weiming Dong",
      "Changsheng Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29335": {
    "title": "Discerning Temporal Difference Learning",
    "volume": "main",
    "abstract": "Temporal difference learning (TD) is a foundational concept in reinforcement learning (RL), aimed at efficiently assessing a policy's value function. TD(λ), a potent variant, incorporates a memory trace to distribute the prediction error into the historical context. However, this approach often neglects the significance of historical states and the relative importance of propagating the TD error, influenced by challenges such as visitation imbalance or outcome noise. To address this, we propose a novel TD algorithm named discerning TD learning (DTD), which allows flexible emphasis functions—predetermined or adapted during training—to allocate efforts effectively across states. We establish the convergence properties of our method within a specific class of emphasis functions and showcase its promising potential for adaptation to deep RL contexts. Empirical results underscore that employing a judicious emphasis function not only improves value estimation but also expedites learning across diverse scenarios",
    "checked": true,
    "id": "0284bd57a8a494da33f817fdc6d27427218c1749",
    "semantic_title": "discerning temporal difference learning",
    "citation_count": 0,
    "authors": [
      "Jianfei Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29336": {
    "title": "One-Step Forward and Backtrack: Overcoming Zig-Zagging in Loss-Aware Quantization Training",
    "volume": "main",
    "abstract": "Weight quantization is an effective technique to compress deep neural networks for their deployment on edge devices with limited resources. Traditional loss-aware quantization methods commonly use the quantized gradient to replace the full-precision gradient. However, we discover that the gradient error will lead to an unexpected zig-zagging-like issue in the gradient descent learning procedures, where the gradient directions rapidly oscillate or zig-zag, and such issue seriously slows down the model convergence. Accordingly, this paper proposes a one-step forward and backtrack way for loss-aware quantization to get more accurate and stable gradient direction to defy this issue. During the gradient descent learning, a one-step forward search is designed to find the trial gradient of the next-step, which is adopted to adjust the gradient of current step towards the direction of fast convergence. After that, we backtrack the current step to update the full-precision and quantized weights through the current-step gradient and the trial gradient. A series of theoretical analysis and experiments on benchmark deep models have demonstrated the effectiveness and competitiveness of the proposed method, and our method especially outperforms others on the convergence performance",
    "checked": true,
    "id": "5b27038a5aa8b12bd41eb02de26a6cf28eefbe94",
    "semantic_title": "one-step forward and backtrack: overcoming zig-zagging in loss-aware quantization training",
    "citation_count": 0,
    "authors": [
      "Lianbo Ma",
      "Yuee Zhou",
      "Jianlun Ma",
      "Guo Yu",
      "Qing Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29337": {
    "title": "U-Mixer: An Unet-Mixer Architecture with Stationarity Correction for Time Series Forecasting",
    "volume": "main",
    "abstract": "Time series forecasting is a crucial task in various domains. Caused by factors such as trends, seasonality, or irregular fluctuations, time series often exhibits non-stationary. It obstructs stable feature propagation through deep layers, disrupts feature distributions, and complicates learning data distribution changes. As a result, many existing models struggle to capture the underlying patterns, leading to degraded forecasting performance. In this study, we tackle the challenge of non-stationarity in time series forecasting with our proposed framework called U-Mixer. By combining Unet and Mixer, U-Mixer effectively captures local temporal dependencies between different patches and channels separately to avoid the influence of distribution variations among channels, and merge low- and high-levels features to obtain comprehensive data representations. The key contribution is a novel stationarity correction method, explicitly restoring data distribution by constraining the difference in stationarity between the data before and after model processing to restore the non-stationarity information, while ensuring the temporal dependencies are preserved. Through extensive experiments on various real-world time series datasets, U-Mixer demonstrates its effectiveness and robustness, and achieves 14.5% and 7.7% improvements over state-of-the-art (SOTA) methods",
    "checked": true,
    "id": "f9cbec81bf3d1a00ba33e38be93b0837f3a1a081",
    "semantic_title": "u-mixer: an unet-mixer architecture with stationarity correction for time series forecasting",
    "citation_count": 2,
    "authors": [
      "Xiang Ma",
      "Xuemei Li",
      "Lexin Fang",
      "Tianlong Zhao",
      "Caiming Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29338": {
    "title": "Transformer-Based Video-Structure Multi-Instance Learning for Whole Slide Image Classification",
    "volume": "main",
    "abstract": "Pathological images play a vital role in clinical cancer diagnosis. Computer-aided diagnosis utilized on digital Whole Slide Images (WSIs) has been widely studied. The major challenge of using deep learning models for WSI analysis is the huge size of WSI images and existing methods struggle between end-to-end learning and proper modeling of contextual information. Most state-of-the-art methods utilize a two-stage strategy, in which they use a pre-trained model to extract features of small patches cut from a WSI and then input these features into a classification model. These methods can not perform end-to-end learning and consider contextual information at the same time. To solve this problem, we propose a framework that models a WSI as a pathologist's observing video and utilizes Transformer to process video clips with a divide-and-conquer strategy, which helps achieve both context-awareness and end-to-end learning. Extensive experiments on three public WSI datasets show that our proposed method outperforms existing SOTA methods in both WSI classification and positive region detection",
    "checked": true,
    "id": "a27eb4b9b2e7b44a4f8534ae294599e0a54f3b29",
    "semantic_title": "transformer-based video-structure multi-instance learning for whole slide image classification",
    "citation_count": 0,
    "authors": [
      "Yingfan Ma",
      "Xiaoyuan Luo",
      "Kexue Fu",
      "Manning Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29339": {
    "title": "PPIDSG: A Privacy-Preserving Image Distribution Sharing Scheme with GAN in Federated Learning",
    "volume": "main",
    "abstract": "Federated learning (FL) has attracted growing attention since it allows for privacy-preserving collaborative training on decentralized clients without explicitly uploading sensitive data to the central server. However, recent works have revealed that it still has the risk of exposing private data to adversaries. In this paper, we conduct reconstruction attacks and enhance inference attacks on various datasets to better understand that sharing trained classification model parameters to a central server is the main problem of privacy leakage in FL. To tackle this problem, a privacy-preserving image distribution sharing scheme with GAN (PPIDSG) is proposed, which consists of a block scrambling-based encryption algorithm, an image distribution sharing method, and local classification training. Specifically, our method can capture the distribution of a target image domain which is transformed by the block encryption algorithm, and upload generator parameters to avoid classifier sharing with negligible influence on model performance. Furthermore, we apply a feature extractor to motivate model utility and train it separately from the classifier. The extensive experimental results and security analyses demonstrate the superiority of our proposed scheme compared to other state-of-the-art defense methods. The code is available at https://github.com/ytingma/PPIDSG",
    "checked": true,
    "id": "475ab7430052c9fa4b54bee506557823f2c0529c",
    "semantic_title": "ppidsg: a privacy-preserving image distribution sharing scheme with gan in federated learning",
    "citation_count": 0,
    "authors": [
      "Yuting Ma",
      "Yuanzhi Yao",
      "Xiaohua Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29340": {
    "title": "Hard Regularization to Prevent Deep Online Clustering Collapse without Data Augmentation",
    "volume": "main",
    "abstract": "Online deep clustering refers to the joint use of a feature extraction network and a clustering model to assign cluster labels to each new data point or batch as it is processed. While faster and more versatile than offline methods, online clustering can easily reach the collapsed solution where the encoder maps all inputs to the same point and all are put into a single cluster. Successful existing models have employed various techniques to avoid this problem, most of which require data augmentation or which aim to make the average soft assignment across the dataset the same for each cluster. We propose a method that does not require data augmentation, and that, differently from existing methods, regularizes the hard assignments. Using a Bayesian framework, we derive an intuitive optimization objective that can be straightforwardly included in the training of the encoder network. Tested on four image datasets, it consistently avoids collapse more robustly than other methods and leads to more accurate clustering. We also conduct further experiments and analyses justifying our choice to regularize the hard cluster assignments. Code is available at https://github.com/Lou1sM/online_hard_clustering",
    "checked": true,
    "id": "de15c60f449c6930eb967d33bff9d56f114c2b29",
    "semantic_title": "hard regularization to prevent deep online clustering collapse without data augmentation",
    "citation_count": 0,
    "authors": [
      "Louis Mahon",
      "Thomas Lukasiewicz"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29341": {
    "title": "Simple Weak Coresets for Non-decomposable Classification Measures",
    "volume": "main",
    "abstract": "While coresets have been growing in terms of their application, barring few exceptions, they have mostly been limited to unsupervised settings. We consider supervised classification problems, and non-decomposable evaluation measures in such settings. We show that stratified uniform sampling based coresets have excellent empirical performance that are backed by theoretical guarantees too. We focus on the F1 score and Matthews Correlation Coefficient, two widely used non-decomposable objective functions that are nontrivial to optimize for and show that uniform coresets attain a lower bound for coreset size, and have good empirical performance, comparable with ``smarter'' coreset construction strategies",
    "checked": true,
    "id": "3f74ef65357d83a2c510e4163d68077598884436",
    "semantic_title": "simple weak coresets for non-decomposable classification measures",
    "citation_count": 0,
    "authors": [
      "Jayesh Malaviya",
      "Anirban Dasgupta",
      "Rachit Chhaya"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29342": {
    "title": "One Self-Configurable Model to Solve Many Abstract Visual Reasoning Problems",
    "volume": "main",
    "abstract": "Abstract Visual Reasoning (AVR) comprises a wide selection of various problems similar to those used in human IQ tests. Recent years have brought dynamic progress in solving particular AVR tasks, however, in the contemporary literature AVR problems are largely dealt with in isolation, leading to highly specialized task-specific methods. With the aim of developing universal learning systems in the AVR domain, we propose the unified model for solving Single-Choice Abstract visual Reasoning tasks (SCAR), capable of solving various single-choice AVR tasks, without making any a priori assumptions about the task structure, in particular the number and location of panels. The proposed model relies on a novel Structure-Aware dynamic Layer (SAL), which adapts its weights to the structure of the considered AVR problem. Experiments conducted on Raven's Progressive Matrices, Visual Analogy Problems, and Odd One Out problems show that SCAR (SAL-based models, in general) effectively solves diverse AVR tasks, and its performance is on par with the state-of-the-art task-specific baselines. What is more, SCAR demonstrates effective knowledge reuse in multi-task and transfer learning settings. To our knowledge, this work is the first successful attempt to construct a general single-choice AVR solver relying on self-configurable architecture and unified solving method. With this work we aim to stimulate and foster progress on task-independent research paths in the AVR domain, with the long-term goal of development of a general AVR solver",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mikołaj Małkiński",
      "Jacek Mańdziuk"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29343": {
    "title": "Permutation-Based Hypothesis Testing for Neural Networks",
    "volume": "main",
    "abstract": "Neural networks are powerful predictive models, but they provide little insight into the nature of relationships between predictors and outcomes. Although numerous methods have been proposed to quantify the relative contributions of input features, statistical inference and hypothesis testing of feature associations remain largely unexplored. We propose a permutation-based approach to testing that uses the partial derivatives of the network output with respect to specific inputs to assess both the significance of input features and whether significant features are linearly associated with the network output. These tests, which can be flexibly applied to a variety of network architectures, enhance the explanatory power of neural networks, and combined with powerful predictive capability, extend the applicability of these models",
    "checked": true,
    "id": "53a74f85f86fda324db82908718e051323af1635",
    "semantic_title": "permutation-based hypothesis testing for neural networks",
    "citation_count": 0,
    "authors": [
      "Francesca Mandel",
      "Ian Barnett"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29344": {
    "title": "Online Markov Decision Processes Configuration with Continuous Decision Space",
    "volume": "main",
    "abstract": "In this paper, we investigate the optimal online configuration of episodic Markov decision processes when the space of the possible configurations is continuous. Specifically, we study the interaction between a learner (referred to as the configurator) and an agent with a fixed, unknown policy, when the learner aims to minimize her losses by choosing transition functions in online fashion. The losses may be unrelated to the agent's rewards. This problem applies to many real-world scenarios where the learner seeks to manipulate the Markov decision process to her advantage. We study both deterministic and stochastic settings, where the losses are either fixed or sampled from an unknown probability distribution. We design two algorithms whose peculiarity is to rely on occupancy measures to explore with optimism the continuous space of transition functions, achieving constant regret in deterministic settings and sublinear regret in stochastic settings, respectively. Moreover, we prove that the regret bound is tight with respect to any constant factor in deterministic settings. Finally, we compare the empiric performance of our algorithms with a baseline in synthetic experiments",
    "checked": true,
    "id": "014328421b9ddba3ae3ce553b1d2b8b4f02a8b78",
    "semantic_title": "online markov decision processes configuration with continuous decision space",
    "citation_count": 1,
    "authors": [
      "Davide Maran",
      "Pierriccardo Olivieri",
      "Francesco Emanuele Stradi",
      "Giuseppe Urso",
      "Nicola Gatti",
      "Marcello Restelli"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29345": {
    "title": "GradTree: Learning Axis-Aligned Decision Trees with Gradient Descent",
    "volume": "main",
    "abstract": "Decision Trees (DTs) are commonly used for many machine learning tasks due to their high degree of interpretability. However, learning a DT from data is a difficult optimization problem, as it is non-convex and non-differentiable. Therefore, common approaches learn DTs using a greedy growth algorithm that minimizes the impurity locally at each internal node. Unfortunately, this greedy procedure can lead to inaccurate trees. In this paper, we present a novel approach for learning hard, axis-aligned DTs with gradient descent. The proposed method uses backpropagation with a straight-through operator on a dense DT representation, to jointly optimize all tree parameters. Our approach outperforms existing methods on binary classification benchmarks and achieves competitive results for multi-class tasks. The implementation is available under: https://github.com/s-marton/GradTree",
    "checked": true,
    "id": "22ae193e9f91925f3ecafebb759f242fcdf36202",
    "semantic_title": "gradtree: learning axis-aligned decision trees with gradient descent",
    "citation_count": 1,
    "authors": [
      "Sascha Marton",
      "Stefan Lüdtke",
      "Christian Bartelt",
      "Heiner Stuckenschmidt"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29346": {
    "title": "Optimal Attack and Defense for Reinforcement Learning",
    "volume": "main",
    "abstract": "To ensure the usefulness of Reinforcement Learning (RL) in real systems, it is crucial to ensure they are robust to noise and adversarial attacks. In adversarial RL, an external attacker has the power to manipulate the victim agent's interaction with the environment. We study the full class of online manipulation attacks, which include (i) state attacks, (ii) observation attacks (which are a generalization of perceived-state attacks), (iii) action attacks, and (iv) reward attacks. We show the attacker's problem of designing a stealthy attack that maximizes its own expected reward, which often corresponds to minimizing the victim's value, is captured by a Markov Decision Process (MDP) that we call a meta-MDP since it is not the true environment but a higher level environment induced by the attacked interaction. We show that the attacker can derive optimal attacks by planning in polynomial time or learning with polynomial sample complexity using standard RL techniques. We argue that the optimal defense policy for the victim can be computed as the solution to a stochastic Stackelberg game, which can be further simplified into a partially-observable turn-based stochastic game (POTBSG). Neither the attacker nor the victim would benefit from deviating from their respective optimal policies, thus such solutions are truly robust. Although the defense problem is NP-hard, we show that optimal Markovian defenses can be computed (learned) in polynomial time (sample complexity) in many scenarios",
    "checked": true,
    "id": "1a582dceac91b886e2f5452515a55c4b5cac73f6",
    "semantic_title": "optimal attack and defense for reinforcement learning",
    "citation_count": 0,
    "authors": [
      "Jeremy McMahan",
      "Young Wu",
      "Xiaojin Zhu",
      "Qiaomin Xie"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29347": {
    "title": "QCS-SGM+: Improved Quantized Compressed Sensing with Score-Based Generative Models",
    "volume": "main",
    "abstract": "In practical compressed sensing (CS), the obtained measurements typically necessitate quantization to a limited number of bits prior to transmission or storage. This nonlinear quantization process poses significant recovery challenges, particularly with extreme coarse quantization such as 1-bit. Recently, an efficient algorithm called QCS-SGM was proposed for quantized CS (QCS) which utilizes score-based generative models (SGM) as an implicit prior. Due to the adeptness of SGM in capturing the intricate structures of natural signals, QCS-SGM substantially outperforms previous QCS methods. However, QCS-SGM is constrained to (approximately) row-orthogonal sensing matrices as the computation of the likelihood score becomes intractable otherwise. To address this limitation, we introduce an advanced variant of QCS-SGM, termed QCS-SGM+, capable of handling general matrices effectively. The key idea is a Bayesian inference perspective on the likelihood score computation, wherein expectation propagation is employed for its approximate computation. Extensive experiments are conducted, demonstrating the substantial superiority of QCS-SGM+ over QCS-SGM for general sensing matrices beyond mere row-orthogonality",
    "checked": true,
    "id": "ea38102e398ec76d961860daa9d334967e019de6",
    "semantic_title": "qcs-sgm+: improved quantized compressed sensing with score-based generative models",
    "citation_count": 0,
    "authors": [
      "Xiangming Meng",
      "Yoshiyuki Kabashima"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29348": {
    "title": "Learning Representations on the Unit Sphere: Investigating Angular Gaussian and Von Mises-Fisher Distributions for Online Continual Learning",
    "volume": "main",
    "abstract": "We use the maximum a posteriori estimation principle for learning representations distributed on the unit sphere. We propose to use the angular Gaussian distribution, which corresponds to a Gaussian projected on the unit-sphere and derive the associated loss function. We also consider the von Mises-Fisher distribution, which is the conditional of a Gaussian in the unit-sphere. The learned representations are pushed toward fixed directions, which are the prior means of the Gaussians; allowing for a learning strategy that is resilient to data drift. This makes it suitable for online continual learning, which is the problem of training neural networks on a continuous data stream, where multiple classification tasks are presented sequentially so that data from past tasks are no longer accessible, and data from the current task can be seen only once. To address this challenging scenario, we propose a memory-based representation learning technique equipped with our new loss functions. Our approach does not require negative data or knowledge of task boundaries and performs well with smaller batch sizes while being computationally efficient. We demonstrate with extensive experiments that the proposed method outperforms the current state-of-the-art methods on both standard evaluation scenarios and realistic scenarios with blurry task boundaries. For reproducibility, we use the same training pipeline for every compared method and share the code at https://github.com/Nicolas1203/ocl-fd",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicolas Michel",
      "Giovanni Chierchia",
      "Romain Negrel",
      "Jean-François Bercher"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29349": {
    "title": "HOP to the Next Tasks and Domains for Continual Learning in NLP",
    "volume": "main",
    "abstract": "Continual Learning (CL) aims to learn a sequence of problems (i.e., tasks and domains) by transferring knowledge acquired on previous problems, whilst avoiding forgetting of past ones. Different from previous approaches which focused on CL for one NLP task or domain in a specific use-case, in this paper, we address a more general CL setting to learn from a sequence of problems in a unique framework. Our method, HOP, permits to hop across tasks and domains by addressing the CL problem along three directions: (i) we employ a set of adapters to generalize a large pre-trained model to unseen problems, (ii) we compute high-order moments over the distribution of embedded representations to distinguish independent and correlated statistics across different tasks and domains, (iii) we process this enriched information with auxiliary heads specialized for each end problem. Extensive experimental campaign on 4 NLP applications, 5 benchmarks and 2 CL setups demonstrates the effectiveness of our HOP",
    "checked": true,
    "id": "8dd24b8476aa1a758765337fbaa889c4a923852d",
    "semantic_title": "hop to the next tasks and domains for continual learning in nlp",
    "citation_count": 2,
    "authors": [
      "Umberto Michieli",
      "Mete Ozay"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29350": {
    "title": "Leveraging Local Variance for Pseudo-Label Selection in Semi-supervised Learning",
    "volume": "main",
    "abstract": "Semi-supervised learning algorithms that use pseudo-labeling have become increasingly popular for improving model performance by utilizing both labeled and unlabeled data. In this paper, we offer a fresh perspective on the selection of pseudo-labels, inspired by theoretical insights. We suggest that pseudo-labels with a high degree of local variance are more prone to inaccuracies. Based on this premise, we introduce the Local Variance Match (LVM) method, which aims to optimize the selection of pseudo-labels in semi-supervised learning (SSL) tasks. Our methodology is validated through a series of experiments on widely-used image classification datasets, such as CIFAR-10, CIFAR-100, and SVHN, spanning various labeled data quantity scenarios. The empirical findings show that the LVM method substantially outpaces current SSL techniques, achieving state-of-the-art results in many of these scenarios. For instance, we observed an error rate of 5.41% on CIFAR-10 with a single label for each class, 35.87% on CIFAR-100 when using four labels per class, and 1.94% on SVHN with four labels for each class. Notably, the standout error rate of 5.41% is less than 1% shy of the performance in a fully-supervised learning environment. In experiments on ImageNet with 100k labeled data, the LVM also reached state-of-the-art outcomes. Additionally, the efficacy of the LVM method is further validated by its stellar performance in speech recognition experiments",
    "checked": true,
    "id": "42185869f79262f8c20c82a5302e5dc2ff5bca4f",
    "semantic_title": "leveraging local variance for pseudo-label selection in semi-supervised learning",
    "citation_count": 0,
    "authors": [
      "Zeping Min",
      "Jinfeng Bai",
      "Chengfei Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29351": {
    "title": "Input Margins Can Predict Generalization Too",
    "volume": "main",
    "abstract": "Understanding generalization in deep neural networks is an active area of research. A promising avenue of exploration has been that of margin measurements: the shortest distance to the decision boundary for a given sample or its representation internal to the network. While margins have been shown to be correlated with the generalization ability of a model when measured at its hidden representations (hidden margins), no such link between large margins and generalization has been established for input margins. We show that while input margins are not generally predictive of generalization, they can be if the search space is appropriately constrained. We develop such a measure based on input margins, which we refer to as 'constrained margins'. The predictive power of this new measure is demonstrated on the 'Predicting Generalization in Deep Learning' (PGDL) dataset and contrasted with hidden representation margins. We find that constrained margins achieve highly competitive scores and outperform other margin measurements in general. This provides a novel insight on the relationship between generalization and classification margins, and highlights the importance of considering the data manifold for investigations of generalization in DNNs",
    "checked": true,
    "id": "4297b1d85a8e91182451e5883d7f472427ecefde",
    "semantic_title": "input margins can predict generalization too",
    "citation_count": 2,
    "authors": [
      "Coenraad Mouton",
      "Marthinus Wilhelmus Theunissen",
      "Marelie H Davel"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29352": {
    "title": "Beyond TreeSHAP: Efficient Computation of Any-Order Shapley Interactions for Tree Ensembles",
    "volume": "main",
    "abstract": "While shallow decision trees may be interpretable, larger ensemble models like gradient-boosted trees, which often set the state of the art in machine learning problems involving tabular data, still remain black box models. As a remedy, the Shapley value (SV) is a well-known concept in explainable artificial intelligence (XAI) research for quantifying additive feature attributions of predictions. The model-specific TreeSHAP methodology solves the exponential complexity for retrieving exact SVs from tree-based models. Expanding beyond individual feature attribution, Shapley interactions reveal the impact of intricate feature interactions of any order. In this work, we present TreeSHAP-IQ, an efficient method to compute any-order additive Shapley interactions for predictions of tree-based models. TreeSHAP-IQ is supported by a mathematical framework that exploits polynomial arithmetic to compute the interaction scores in a single recursive traversal of the tree, akin to Linear TreeSHAP. We apply TreeSHAP-IQ on state-of-the-art tree ensembles and explore interactions on well-established benchmark datasets",
    "checked": true,
    "id": "9efdab70a5c1358deeddc0ccaf273235b4195cc1",
    "semantic_title": "beyond treeshap: efficient computation of any-order shapley interactions for tree ensembles",
    "citation_count": 4,
    "authors": [
      "Maximilian Muschalik",
      "Fabian Fumagalli",
      "Barbara Hammer",
      "Eyke Hüllermeier"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29353": {
    "title": "Continuous Treatment Effect Estimation Using Gradient Interpolation and Kernel Smoothing",
    "volume": "main",
    "abstract": "We address the Individualized continuous treatment effect (ICTE) estimation problem where we predict the effect of any continuous valued treatment on an individual using ob- servational data. The main challenge in this estimation task is the potential confounding of treatment assignment with in- dividual's covariates in the training data, whereas during in- ference ICTE requires prediction on independently sampled treatments. In contrast to prior work that relied on regularizers or unstable GAN training, we advocate the direct approach of augmenting training individuals with independently sam- pled treatments and inferred counterfactual outcomes. We in- fer counterfactual outcomes using a two-pronged strategy: a Gradient Interpolation for close-to-observed treatments, and a Gaussian Process based Kernel Smoothing which allows us to down weigh high variance inferences. We evaluate our method on five benchmarks and show that our method out- performs six state-of-the-art methods on the counterfactual estimation error. We analyze the superior performance of our method by showing that (1) our inferred counterfactual re- sponses are more accurate, and (2) adding them to the train- ing data reduces the distributional distance between the con- founded training distribution and test distribution where treat- ment is independent of covariates. Our proposed method is model-agnostic and we show that it improves ICTE accuracy of several existing models",
    "checked": true,
    "id": "8bf1333cc8c35de07c33b6afb71b80ef80c5986b",
    "semantic_title": "continuous treatment effect estimation using gradient interpolation and kernel smoothing",
    "citation_count": 3,
    "authors": [
      "Lokesh Nagalapatti",
      "Akshay Iyer",
      "Abir De",
      "Sunita Sarawagi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29354": {
    "title": "Revisiting Disentanglement in Downstream Tasks: A Study on Its Necessity for Abstract Visual Reasoning",
    "volume": "main",
    "abstract": "In representation learning, a disentangled representation is highly desirable as it encodes generative factors of data in a separable and compact pattern. Researchers have advocated leveraging disentangled representations to complete downstream tasks with encouraging empirical evidence. This paper further investigates the necessity of disentangled representation in downstream applications. Specifically, we show that dimension-wise disentangled representations are unnecessary on a fundamental downstream task, abstract visual reasoning. We provide extensive empirical evidence against the necessity of disentanglement, covering multiple datasets, representation learning methods, and downstream network architectures. Furthermore, our findings suggest that the informativeness of representations is a better indicator of downstream performance than disentanglement. Finally, the positive correlation between informativeness and disentanglement explains the claimed usefulness of disentangled representations in previous works. The source code is available at https://github.com/Richard-coder-Nai/disentanglement-lib-necessity.git",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruiqian Nai",
      "Zixin Wen",
      "Ji Li",
      "Yuanzhi Li",
      "Yang Gao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29355": {
    "title": "Thompson Sampling for Real-Valued Combinatorial Pure Exploration of Multi-Armed Bandit",
    "volume": "main",
    "abstract": "We study the real-valued combinatorial pure exploration of the multi-armed bandit (R-CPE-MAB) problem. In R-CPE-MAB, a player is given stochastic arms, and the reward of each arm follows an unknown distribution. In each time step, a player pulls a single arm and observes its reward. The player's goal is to identify the optimal action from a finite-sized real-valued action set with as few arm pulls as possible. Previous methods in the R-CPE-MAB require enumerating all of the feasible actions of the combinatorial optimization problem one is considering. In general, since the size of the action set grows exponentially large with respect to the number of arms, this is almost practically impossible when the number of arms is large. We introduce an algorithm named the Generalized Thompson Sampling Explore (GenTS-Explore) algorithm, which is the first algorithm that can work even when the size of the action set is exponentially large with respect to the number of arms. We also introduce a novel problem-dependent sample complexity lower bound of the R-CPE-MAB problem, and show that the GenTS-Explore algorithm achieves the optimal sample complexity up to a problem-dependent constant factor",
    "checked": true,
    "id": "cdc7a78c7bf2df7fbbcc14ded3fd7f098ea906fa",
    "semantic_title": "thompson sampling for real-valued combinatorial pure exploration of multi-armed bandit",
    "citation_count": 3,
    "authors": [
      "Shintaro Nakamura",
      "Masashi Sugiyama"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29356": {
    "title": "Efficient Learning of PDEs via Taylor Expansion and Sparse Decomposition into Value and Fourier Domains",
    "volume": "main",
    "abstract": "Accelerating the learning of Partial Differential Equations (PDEs) from experimental data will speed up the pace of scientific discovery. Previous randomized algorithms exploit sparsity in PDE updates for acceleration. However such methods are applicable to a limited class of decomposable PDEs, which have sparse features in the value domain. We propose Reel, which accelerates the learning of PDEs via random projection and has much broader applicability. Reel exploits the sparsity by decomposing dense updates into sparse ones in both the value and frequency domains. This decomposition enables efficient learning when the source of the updates consists of gradually changing terms across large areas (sparse in the frequency domain) in addition to a few rapid updates concentrated in a small set of \"interfacial\" regions (sparse in the value domain). Random projection is then applied to compress the sparse signals for learning. To expand the model applicability, Taylor series expansion is used in Reel to approximate the nonlinear PDE updates with polynomials in the decomposable form. Theoretically, we derive a constant factor approximation between the projected loss function and the original one with poly-logarithmic number of projected dimensions. Experimentally, we provide empirical evidence that our proposed Reel can lead to faster learning of PDE models (70-98% reduction in training time when the data is compressed to 1% of its original size) with comparable quality as the non-compressed models",
    "checked": true,
    "id": "5ae70025b764c952c68d89128e105deab08b636f",
    "semantic_title": "efficient learning of pdes via taylor expansion and sparse decomposition into value and fourier domains",
    "citation_count": 0,
    "authors": [
      "Md Nasim",
      "Yexiang Xue"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29357": {
    "title": "Secure Distributed Sparse Gaussian Process Models Using Multi-Key Homomorphic Encryption",
    "volume": "main",
    "abstract": "Distributed sparse Gaussian process (dGP) models provide an ability to achieve accurate predictive performance using data from multiple devices in a time efficient and scalable manner. The distributed computation of model, however, risks exposure of privately owned data to public manipulation. In this paper we propose a secure solution for dGP regression models using multi-key homomorphic encryption. Experimental results show that with a little sacrifice in terms of time complexity, we achieve a secure dGP model without deteriorating the predictive performance compared to traditional non-secure dGP models. We also present a practical implementation of the proposed model using several Nvidia Jetson Nano Developer Kit modules to simulate a real-world scenario. Thus, secure dGP model plugs the data security issues of dGP and provide a secure and trustworthy solution for multiple devices to use privately owned data for model computation in a distributed environment availing speed, scalability and robustness of dGP",
    "checked": true,
    "id": "17b59e7079b5825836dc818aec3e4f6d3e0c9a56",
    "semantic_title": "secure distributed sparse gaussian process models using multi-key homomorphic encryption",
    "citation_count": 0,
    "authors": [
      "Adil Nawaz",
      "Guopeng Chen",
      "Muhammad Umair Raza",
      "Zahid Iqbal",
      "Jianqiang  Li",
      "Victor C.M. Leung",
      "Jie Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29358": {
    "title": "Multiple Hypothesis Dropout: Estimating the Parameters of Multi-Modal Output Distributions",
    "volume": "main",
    "abstract": "In many real-world applications, from robotics to pedestrian trajectory prediction, there is a need to predict multiple real-valued outputs to represent several potential scenarios. Current deep learning techniques to address multiple-output problems are based on two main methodologies: (1) mixture density networks, which suffer from poor stability at high dimensions, or (2) multiple choice learning (MCL), an approach that uses M single-output functions, each only producing a point estimate hypothesis. This paper presents a Mixture of Multiple-Output functions (MoM) approach using a novel variant of dropout, Multiple Hypothesis Dropout. Unlike traditional MCL-based approaches, each multiple-output function not only estimates the mean but also the variance for its hypothesis. This is achieved through a novel stochastic winner-take-all loss which allows each multiple-output function to estimate variance through the spread of its subnetwork predictions. Experiments on supervised learning problems illustrate that our approach outperforms existing solutions for reconstructing multimodal output distributions. Additional studies on unsupervised learning problems show that estimating the parameters of latent posterior distributions within a discrete autoencoder significantly improves codebook efficiency, sample quality, precision and recall",
    "checked": true,
    "id": "12558edc6968698a8f575f3213bf85342e8bdf69",
    "semantic_title": "multiple hypothesis dropout: estimating the parameters of multi-modal output distributions",
    "citation_count": 0,
    "authors": [
      "David D. Nguyen",
      "David Liebowitz",
      "Salil S. Kanhere",
      "Surya Nepal"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29359": {
    "title": "On Inference Stability for Diffusion Models",
    "volume": "main",
    "abstract": "Denoising Probabilistic Models (DPMs) represent an emerging domain of generative models that excel in generating diverse and high-quality images. However, most current training methods for DPMs often neglect the correlation between timesteps, limiting the model's performance in generating images effectively. Notably, we theoretically point out that this issue can be caused by the cumulative estimation gap between the predicted and the actual trajectory. To minimize that gap, we propose a novel sequence-aware loss that aims to reduce the estimation gap to enhance the sampling quality. Furthermore, we theoretically show that our proposed loss function is a tighter upper bound of the estimation loss in comparison with the conventional loss in DPMs. Experimental results on several benchmark datasets including CIFAR10, CelebA, and CelebA-HQ consistently show a remarkable improvement of our proposed method regarding the image generalization quality measured by FID and Inception Score compared to several DPM baselines. Our code and pre-trained checkpoints are available at https://github.com/VinAIResearch/SA-DPM",
    "checked": true,
    "id": "bfdea3592e8767a2134b50e56c0c247e05cc8523",
    "semantic_title": "on inference stability for diffusion models",
    "citation_count": 0,
    "authors": [
      "Viet Nguyen",
      "Giang Vu",
      "Tung Nguyen Thanh",
      "Khoat Than",
      "Toan Tran"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29360": {
    "title": "Improve Robustness of Reinforcement Learning against Observation Perturbations via l∞ Lipschitz Policy Networks",
    "volume": "main",
    "abstract": "Deep Reinforcement Learning (DRL) has achieved remarkable advances in sequential decision tasks. However, recent works have revealed that DRL agents are susceptible to slight perturbations in observations. This vulnerability raises concerns regarding the effectiveness and robustness of deploying such agents in real-world applications. In this work, we propose a novel robust reinforcement learning method called SortRL, which improves the robustness of DRL policies against observation perturbations from the perspective of the network architecture. We employ a novel architecture for the policy network that incorporates global $l_\\infty$ Lipschitz continuity and provide a convenient method to enhance policy robustness based on the output margin. Besides, a training framework is designed for SortRL, which solves given tasks while maintaining robustness against $l_\\infty$ bounded perturbations on the observations. Several experiments are conducted to evaluate the effectiveness of our method, including classic control tasks and video games. The results demonstrate that SortRL achieves state-of-the-art robustness performance against different perturbation strength",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Buqing Nie",
      "Jingtian Ji",
      "Yangqing Fu",
      "Yue Gao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29361": {
    "title": "Multi-Class Support Vector Machine with Maximizing Minimum Margin",
    "volume": "main",
    "abstract": "Support Vector Machine (SVM) stands out as a prominent machine learning technique widely applied in practical pattern recognition tasks. It achieves binary classification by maximizing the \"margin\", which represents the minimum distance between instances and the decision boundary. Although many efforts have been dedicated to expanding SVM for multi-class case through strategies such as one versus one and one versus the rest, satisfactory solutions remain to be developed. In this paper, we propose a novel method for multi-class SVM that incorporates pairwise class loss considerations and maximizes the minimum margin. Adhering to this concept, we embrace a new formulation that imparts heightened flexibility to multi-class SVM. Furthermore, the correlations between the proposed method and multiple forms of multi-class SVM are analyzed. The proposed regularizer, akin to the concept of \"margin\", can serve as a seamless enhancement over the softmax in deep learning, providing guidance for network parameter learning. Empirical evaluations demonstrate the effectiveness and superiority of our proposed method over existing multi-classification methods. Complete version is available at https://arxiv.org/pdf/2312.06578.pdf. Code is available at https://github.com/zz-haooo/M3SVM",
    "checked": true,
    "id": "ce04bbde8a4b9547377fe6b4c8f9c61a2c3de8ab",
    "semantic_title": "multi-class support vector machine with maximizing minimum margin",
    "citation_count": 2,
    "authors": [
      "Feiping Nie",
      "Zhezheng Hao",
      "Rong Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29362": {
    "title": "Symmetric Q-learning: Reducing Skewness of Bellman Error in Online Reinforcement Learning",
    "volume": "main",
    "abstract": "In deep reinforcement learning, estimating the value function to evaluate the quality of states and actions is essential. The value function is often trained using the least squares method, which implicitly assumes a Gaussian error distribution. However, a recent study suggested that the error distribution for training the value function is often skewed because of the properties of the Bellman operator, and violates the implicit assumption of normal error distribution in the least squares method. To address this, we proposed a method called Symmetric Q-learning, in which the synthetic noise generated from a zero-mean distribution is added to the target values to generate a Gaussian error distribution. We evaluated the proposed method on continuous control benchmark tasks in MuJoCo. It improved the sample efficiency of a state-of-the-art reinforcement learning method by reducing the skewness of the error distribution",
    "checked": true,
    "id": "ecff18fb91dc607b88a0bfc5271f22b62418f2f7",
    "semantic_title": "symmetric q-learning: reducing skewness of bellman error in online reinforcement learning",
    "citation_count": 0,
    "authors": [
      "Motoki Omura",
      "Takayuki Osa",
      "Yusuke Mukuta",
      "Tatsuya Harada"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29363": {
    "title": "A Primal-Dual Algorithm for Hybrid Federated Learning",
    "volume": "main",
    "abstract": "Very few methods for hybrid federated learning, where clients only hold subsets of both features and samples, exist. Yet, this scenario is very important in practical settings. We provide a fast, robust algorithm for hybrid federated learning that hinges on Fenchel Duality. We prove the convergence of the algorithm to the same solution as if the model was trained centrally in a variety of practical regimes. Furthermore, we provide experimental results that demonstrate the performance improvements of the algorithm over a commonly used method in federated learning, FedAvg, and an existing hybrid FL algorithm, HyFEM. We also provide privacy considerations and necessary steps to protect client data",
    "checked": true,
    "id": "531e1bf99f0156570b93e557457d0821701dc023",
    "semantic_title": "a primal-dual algorithm for hybrid federated learning",
    "citation_count": 3,
    "authors": [
      "Tom Overman",
      "Garrett Blum",
      "Diego Klabjan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29364": {
    "title": "Multi-Objective Bayesian Optimization with Active Preference Learning",
    "volume": "main",
    "abstract": "There are a lot of real-world black-box optimization problems that need to optimize multiple criteria simultaneously. However, in a multi-objective optimization (MOO) problem, identifying the whole Pareto front requires the prohibitive search cost, while in many practical scenarios, the decision maker (DM) only needs a specific solution among the set of the Pareto optimal solutions. We propose a Bayesian optimization (BO) approach to identifying the most preferred solution in the MOO with expensive objective functions, in which a Bayesian preference model of the DM is adaptively estimated by an interactive manner based on the two types of supervisions called the pairwise preference and improvement request. To explore the most preferred solution, we define an acquisition function in which the uncertainty both in the objective function and the DM preference is incorporated. Further, to minimize the interaction cost with the DM, we also propose an active learning strategy for the preference estimation. We empirically demonstrate the effectiveness of our proposed method through the benchmark function optimization and the hyper-parameter optimization problems for machine learning models",
    "checked": true,
    "id": "b50000826c7040e7aa60cd8a47cba3585788e6ee",
    "semantic_title": "multi-objective bayesian optimization with active preference learning",
    "citation_count": 0,
    "authors": [
      "Ryota Ozaki",
      "Kazuki Ishikawa",
      "Youhei Kanzaki",
      "Shion Takeno",
      "Ichiro Takeuchi",
      "Masayuki Karasuyama"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29365": {
    "title": "Towards Fair Graph Federated Learning via Incentive Mechanisms",
    "volume": "main",
    "abstract": "Graph federated learning (FL) has emerged as a pivotal paradigm enabling multiple agents to collaboratively train a graph model while preserving local data privacy. Yet, current efforts overlook a key issue: agents are self-interested and would hesitant to share data without fair and satisfactory incentives. This paper is the first endeavor to address this issue by studying the incentive mechanism for graph federated learning. We identify a unique phenomenon in graph federated learning: the presence of agents posing potential harm to the federation and agents contributing with delays. This stands in contrast to previous FL incentive mechanisms that assume all agents contribute positively and in a timely manner. In view of this, this paper presents a novel incentive mechanism tailored for fair graph federated learning, integrating incentives derived from both model gradient and payoff. To achieve this, we first introduce an agent valuation function aimed at quantifying agent contributions through the introduction of two criteria: gradient alignment and graph diversity. Moreover, due to the high heterogeneity in graph federated learning, striking a balance between accuracy and fairness becomes particularly crucial. We introduce motif prototypes to enhance accuracy, communicated between the server and agents, enhancing global model aggregation and aiding agents in local model optimization. Extensive experiments show that our model achieves the best trade-off between accuracy and the fairness of model gradient, as well as superior payoff fairness",
    "checked": true,
    "id": "dab910880ff36a1e3ea0b73b4d0262b888d4dc94",
    "semantic_title": "towards fair graph federated learning via incentive mechanisms",
    "citation_count": 0,
    "authors": [
      "Chenglu Pan",
      "Jiarong Xu",
      "Yue Yu",
      "Ziqi Yang",
      "Qingbiao Wu",
      "Chunping Wang",
      "Lei Chen",
      "Yang Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29366": {
    "title": "A Graph Dynamics Prior for Relational Inference",
    "volume": "main",
    "abstract": "Relational inference aims to identify interactions between parts of a dynamical system from the observed dynamics. Current state-of-the-art methods fit the dynamics with a graph neural network (GNN) on a learnable graph. They use one-step message-passing GNNs---intuitively the right choice since non-locality of multi-step or spectral GNNs may confuse direct and indirect interactions. But the effective interaction graph depends on the sampling rate and it is rarely localized to direct neighbors, leading to poor local optima for the one-step model. In this work, we propose a graph dynamics prior (GDP) for relational inference. GDP constructively uses error amplification in non-local polynomial filters to steer the solution to the ground-truth graph. To deal with non-uniqueness, GDP simultaneously fits a ``shallow'' one-step model and a polynomial multi-step model with shared graph topology. Experiments show that GDP reconstructs graphs far more accurately than earlier methods, with remarkable robustness to under-sampling. Since appropriate sampling rates for unknown dynamical systems are not known a priori, this robustness makes GDP suitable for real applications in scientific machine learning. Reproducible code is available at https://github.com/DaDaCheng/GDP",
    "checked": true,
    "id": "b2834a68ba9483a09c2c7958c2d784582c2ba4a4",
    "semantic_title": "a graph dynamics prior for relational inference",
    "citation_count": 0,
    "authors": [
      "Liming Pan",
      "Cheng Shi",
      "Ivan Dokmanic"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29367": {
    "title": "Learning Reduced Fluid Dynamics",
    "volume": "main",
    "abstract": "Predicting the state evolution of ultra high-dimensional, time-reversible fluid dynamic systems is a crucial but computationally expensive task. Existing physics-informed neural networks either incur high inference cost or cannot preserve the time-reversible nature of the underlying dynamics system. We propose a model-based approach to identify low-dimensional, time reversible, nonlinear fluid dynamic systems. Our method utilizes the symplectic structure of reduced Eulerian fluid and use stochastic Riemann optimization to obtain a low-dimensional bases that minimize the expected trajectory-wise dimension-reduction error over a given distribution of initial conditions. We show that such minimization is well-defined since the reduced trajectories are differentiable with respect to the subspace bases over the entire Grassmannian manifold, under proper choices of timestep sizes and numerical integrators. Finally, we propose a loss function measuring the trajectory-wise discrepancy between the original and reduced models. By tensor precomputation, we show that gradient information of such loss function can be evaluated efficiently over a long trajectory without time-integrating the high-dimensional dynamic system. Through evaluations on a row of simulation benchmarks, we show that our method reduces the discrepancy by 50-90 percent over conventional reduced models and we outperform PINNs by exactly preserving the time reversibility",
    "checked": true,
    "id": "fbe2247668edcf89ba50507bb39720cd834a5642",
    "semantic_title": "learning reduced fluid dynamics",
    "citation_count": 0,
    "authors": [
      "Zherong Pan",
      "Xifeng Gao",
      "Kui Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29368": {
    "title": "FedLF: Layer-Wise Fair Federated Learning",
    "volume": "main",
    "abstract": "Fairness has become an important concern in Federated Learning (FL). An unfair model that performs well for some clients while performing poorly for others can reduce the willingness of clients to participate. In this work, we identify a direct cause of unfairness in FL - the use of an unfair direction to update the global model, which favors some clients while conflicting with other clients' gradients at the model and layer levels. To address these issues, we propose a layer-wise fair Federated Learning algorithm (FedLF). Firstly, we formulate a multi-objective optimization problem with an effective fair-driven objective for FL. A layer-wise fair direction is then calculated to mitigate the model and layer-level gradient conflicts and reduce the improvement bias. We further provide the theoretical analysis on how FedLF can improve fairness and guarantee convergence. Extensive experiments on different learning tasks and models demonstrate that FedLF outperforms the SOTA FL algorithms in terms of accuracy and fairness. The source code is available at https://github.com/zibinpan/FedLF",
    "checked": true,
    "id": "e10f75386b82db67ead35462bfcb32941e6e1b17",
    "semantic_title": "fedlf: layer-wise fair federated learning",
    "citation_count": 0,
    "authors": [
      "Zibin Pan",
      "Chi Li",
      "Fangchen Yu",
      "Shuyi Wang",
      "Haijin Wang",
      "Xiaoying Tang",
      "Junhua Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29369": {
    "title": "Convolutional Channel-Wise Competitive Learning for the Forward-Forward Algorithm",
    "volume": "main",
    "abstract": "The Forward-Forward (FF) Algorithm has been recently proposed to alleviate the issues of backpropagation (BP) commonly used to train deep neural networks. However, its current formulation exhibits limitations such as the generation of negative data, slower convergence, and inadequate performance on complex tasks. In this paper we take the main ideas of FF and improve them by leveraging channel-wise competitive learning in the context of convolutional neural networks for image classification tasks. A layer-wise loss function is introduced that promotes competitive learning and eliminates the need for negative data construction. To enhance both the learning of compositional features and feature space partitioning, a channel-wise feature separator and extractor block is proposed that complements the competitive learning process. Our method outperforms recent FF-based models on image classification tasks, achieving testing errors of 0.58%, 7.69%, 21.89%, and 48.77% on MNIST, Fashion-MNIST, CIFAR-10 and CIFAR-100 respectively. Our approach bridges the performance gap between FF learning and BP methods, indicating the potential of our proposed approach to learn useful representations in a layer-wise modular fashion, enabling more efficient and flexible learning. Our source code and supplementary material are available at https://github.com/andreaspapac/CwComp",
    "checked": true,
    "id": "1de401b94256935401359d8352117609bdc10513",
    "semantic_title": "convolutional channel-wise competitive learning for the forward-forward algorithm",
    "citation_count": 2,
    "authors": [
      "Andreas Papachristodoulou",
      "Christos Kyrkou",
      "Stelios Timotheou",
      "Theocharis Theocharides"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29370": {
    "title": "REPrune: Channel Pruning via Kernel Representative Selection",
    "volume": "main",
    "abstract": "Channel pruning is widely accepted to accelerate modern convolutional neural networks (CNNs). The resulting pruned model benefits from its immediate deployment on general-purpose software and hardware resources. However, its large pruning granularity, specifically at the unit of a convolution filter, often leads to undesirable accuracy drops due to the inflexibility of deciding how and where to introduce sparsity to the CNNs. In this paper, we propose REPrune, a novel channel pruning technique that emulates kernel pruning, fully exploiting the finer but structured granularity. REPrune identifies similar kernels within each channel using agglomerative clustering. Then, it selects filters that maximize the incorporation of kernel representatives while optimizing the maximum cluster coverage problem. By integrating with a simultaneous training-pruning paradigm, REPrune promotes efficient, progressive pruning throughout training CNNs, avoiding the conventional train-prune-finetune sequence. Experimental results highlight that REPrune performs better in computer vision tasks than existing methods, effectively achieving a balance between acceleration ratio and performance retention",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mincheol Park",
      "Dongjin  Kim",
      "Cheonjun Park",
      "Yuna Park",
      "Gyeong Eun Gong",
      "Won Woo Ro",
      "Suhyun Kim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29371": {
    "title": "ConceptBed: Evaluating Concept Learning Abilities of Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "The ability to understand visual concepts and replicate and compose these concepts from images is a central goal for computer vision. Recent advances in text-to-image (T2I) models have lead to high definition and realistic image quality generation by learning from large databases of images and their descriptions. However, the evaluation of T2I models has focused on photorealism and limited qualitative measures of visual understanding. To quantify the ability of T2I models in learning and synthesizing novel visual concepts (a.k.a. personalized T2I), we introduce ConceptBed, a large-scale dataset that consists of 284 unique visual concepts, and 33K composite text prompts. Along with the dataset, we propose an evaluation metric, Concept Confidence Deviation (CCD), that uses the confidence of oracle concept classifiers to measure the alignment between concepts generated by T2I generators and concepts contained in target images. We evaluate visual concepts that are either objects, attributes, or styles, and also evaluate four dimensions of compositionality: counting, attributes, relations, and actions. Our human study shows that CCD is highly correlated with human understanding of concepts. Our results point to a trade-off between learning the concepts and preserving the compositionality which existing approaches struggle to overcome. The data, code, and interactive demo is available at: https://conceptbed.github.io/",
    "checked": true,
    "id": "324f605cddecf16ea11bf44ee570d9293357e149",
    "semantic_title": "conceptbed: evaluating concept learning abilities of text-to-image diffusion models",
    "citation_count": 9,
    "authors": [
      "Maitreya Patel",
      "Tejas Gokhale",
      "Chitta Baral",
      "Yezhou Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29372": {
    "title": "CrystalBox: Future-Based Explanations for Input-Driven Deep RL Systems",
    "volume": "main",
    "abstract": "We present CrystalBox, a novel, model-agnostic, posthoc explainability framework for Deep Reinforcement Learning (DRL) controllers in the large family of input-driven environments which includes computer systems. We combine the natural decomposability of reward functions in input-driven environments with the explanatory power of decomposed returns. We propose an efficient algorithm to generate future-based explanations across both discrete and continuous control environments. Using applications such as adaptive bitrate streaming and congestion control, we demonstrate CrystalBox's capability to generate high-fidelity explanations. We further illustrate its higher utility across three practical use cases: contrastive explanations, network observability, and guided reward design, as opposed to prior explainability techniques that identify salient features",
    "checked": true,
    "id": "da9c3f069f06ca6aa31667ab0c3857d6c9556faf",
    "semantic_title": "crystalbox: future-based explanations for input-driven deep rl systems",
    "citation_count": 0,
    "authors": [
      "Sagar Patel",
      "Sangeetha Abdu Jyothi",
      "Nina Narodytska"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29373": {
    "title": "HAGO-Net: Hierarchical Geometric Massage Passing for Molecular Representation Learning",
    "volume": "main",
    "abstract": "Molecular representation learning has emerged as a game-changer at the intersection of AI and chemistry, with great potential in applications such as drug design and materials discovery. A substantial obstacle in successfully applying molecular representation learning is the difficulty of effectively and completely characterizing and learning molecular geometry, which has not been well addressed to date. To overcome this challenge, we propose a novel framework that features a novel geometric graph, termed HAGO-Graph, and a specifically designed geometric graph learning model, HAGO-Net. In the framework, the foundation is HAGO-Graph, which enables a complete characterization of molecular geometry in a hierarchical manner. Specifically, we leverage the concept of n-body in physics to characterize geometric patterns at multiple spatial scales. We then specifically design a message passing scheme, HAGO-MPS, and implement the scheme as a geometric graph neural network, HAGO-Net, to effectively learn the representation of HAGO-Graph by horizontal and vertical aggregation. We further prove DHAGO-Net, the derivative function of HAGO-Net, is an equivariant model. The proposed models are validated by extensive comparisons on four challenging benchmarks. Notably, the models exhibited state-of-the-art performance in molecular chirality identification and property prediction, achieving state-of-the-art performance on five properties of QM9 dataset. The models also achieved competitive results on molecular dynamics prediction task",
    "checked": true,
    "id": "290d0b33a7d52a8c5c3e73fc6e626f3b08ecb3ad",
    "semantic_title": "hago-net: hierarchical geometric massage passing for molecular representation learning",
    "citation_count": 2,
    "authors": [
      "Hongbin Pei",
      "Taile Chen",
      "Chen A",
      "Huiqi Deng",
      "Jing Tao",
      "Pinghui Wang",
      "Xiaohong Guan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29374": {
    "title": "CARAT: Contrastive Feature Reconstruction and Aggregation for Multi-Modal Multi-Label Emotion Recognition",
    "volume": "main",
    "abstract": "Multi-modal multi-label emotion recognition (MMER) aims to identify relevant emotions from multiple modalities. The challenge of MMER is how to effectively capture discriminative features for multiple labels from heterogeneous data. Recent studies are mainly devoted to exploring various fusion strategies to integrate multi-modal information into a unified representation for all labels. However, such a learning scheme not only overlooks the specificity of each modality but also fails to capture individual discriminative features for different labels. Moreover, dependencies of labels and modalities cannot be effectively modeled. To address these issues, this paper presents ContrAstive feature Reconstruction and AggregaTion (CARAT) for the MMER task. Specifically, we devise a reconstruction-based fusion mechanism to better model fine-grained modality-to-label dependencies by contrastively learning modal-separated and label-specific features. To further exploit the modality complementarity, we introduce a shuffle-based aggregation strategy to enrich co-occurrence collaboration among labels. Experiments on two benchmark datasets CMU-MOSEI and M3ED demonstrate the effectiveness of CARAT over state-of-the-art methods. Code is available at https://github.com/chengzju/CARAT",
    "checked": true,
    "id": "59f14ca9ea290e63c89ffcde7c95c2d1e4c83615",
    "semantic_title": "carat: contrastive feature reconstruction and aggregation for multi-modal multi-label emotion recognition",
    "citation_count": 2,
    "authors": [
      "Cheng Peng",
      "Ke Chen",
      "Lidan Shou",
      "Gang Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29375": {
    "title": "Variational Hybrid-Attention Framework for Multi-Label Few-Shot Aspect Category Detection",
    "volume": "main",
    "abstract": "Multi-label few-shot aspect category detection (FS-ACD) is a challenging sentiment analysis task, which aims to learn a multi-label learning paradigm with limited training data. The difficulty of this task is how to use limited data to generalize effective discriminative representations for different categories. Nowadays, all advanced FS-ACD works utilize the prototypical network to learn label prototypes to represent different aspects. However, such point-based estimation methods are inherently noise-susceptible and bias-vulnerable. To this end, this paper proposes a novel Variational Hybrid-Attention Framework (VHAF) for the FS-ACD task. Specifically, to alleviate the data noise, we adopt a hybrid-attention mechanism to generate more discriminative aspect-specific embeddings. Then, based on these embeddings, we introduce the variational distribution inference to obtain the aspect-specific distribution as a more robust aspect representation, which can eliminate the scarce data bias for better inference. Moreover, we further leverage an adaptive threshold estimation to help VHAF better identify multiple relevant aspects. Extensive experiments on three datasets demonstrate the effectiveness of our VHAF over other state-of-the-art methods. Code is available at https://github.com/chengzju/VHAF",
    "checked": true,
    "id": "5926372c377655cd22b9e58a55e7c0c1f9b401ff",
    "semantic_title": "variational hybrid-attention framework for multi-label few-shot aspect category detection",
    "citation_count": 0,
    "authors": [
      "Cheng Peng",
      "Ke Chen",
      "Lidan Shou",
      "Gang Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29376": {
    "title": "Hypothesis, Verification, and Induction: Grounding Large Language Models with Self-Driven Skill Learning",
    "volume": "main",
    "abstract": "Large language models (LLMs) show their powerful automatic reasoning and planning capability with a wealth of semantic knowledge about the human world. However, the grounding problem still hinders the applications of LLMs in the real-world environment. Existing studies try to fine-tune the LLM or utilize pre-defined behavior APIs to bridge the LLMs and the environment, which not only costs huge human efforts to customize for every single task but also weakens the generality strengths of LLMs. To autonomously ground the LLM onto the environment, we proposed the Hypothesis, Verification, and Induction (HYVIN) framework to automatically and progressively ground the LLM with self-driven skill learning. HYVIN first employs the LLM to propose the hypothesis of sub-goals to achieve tasks and then verify the feasibility of the hypothesis via interacting with the underlying environment. Once verified, HYVIN can then learn generalized skills with the guidance of these successfully grounded subgoals. These skills can be further utilized to accomplish more complex tasks that fail to pass the verification phase. Verified in the famous instruction following task set, BabyAI, HYVIN achieves comparable performance in the most challenging tasks compared with imitation learning methods that cost millions of demonstrations, proving the effectiveness of learned skills and showing the feasibility and efficiency of our framework",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaohui Peng",
      "Xing Hu",
      "Qi Yi",
      "Rui Zhang",
      "Jiaming Guo",
      "Di Huang",
      "Zikang Tian",
      "Ruizhi Chen",
      "Zidong Du",
      "Qi Guo",
      "Yunji Chen",
      "Ling Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29377": {
    "title": "Recurrent Graph Neural Networks and Their Connections to Bisimulation and Logic",
    "volume": "main",
    "abstract": "The success of Graph Neural Networks (GNNs) in practice has motivated extensive research on their theoretical properties. This includes recent results that characterise node classifiers expressible by GNNs in terms of first order logic. Most of the analysis, however, has been focused on GNNs with fixed number of message-passing iterations (i.e., layers), which cannot realise many simple classifiers such as reachability of a node with a given label. In this paper, we start to fill this gap and study the foundations of GNNs that can perform more than a fixed number of message-passing iterations. We first formalise two generalisations of the basic GNNs: recurrent GNNs (RecGNNs), which repeatedly apply message-passing iterations until the node classifications become stable, and graph-size GNNs (GSGNNs), which exploit a built-in function of the input graph size to decide the number of message-passings. We then formally prove that GNN classifiers are strictly less expressive than RecGNN ones, and RecGNN classifiers are strictly less expressive than GSGNN ones. To get this result, we identify novel semantic characterisations of the three formalisms in terms of suitable variants of bisimulation, which we believe have their own value for our understanding of GNNs. Finally, we prove syntactic logical characterisations of RecGNNs and GSGNNs analogous to the logical characterisation of plain GNNs, where we connect the two formalisms to monadic monotone fixpoint logic---a generalisation of first-order logic that supports recursion",
    "checked": true,
    "id": "478dfb1bb3b634176c06631b3c53c01bfc566fbc",
    "semantic_title": "recurrent graph neural networks and their connections to bisimulation and logic",
    "citation_count": 0,
    "authors": [
      "Maximilian Pflueger",
      "David Tena Cucala",
      "Egor V. Kostylev"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29378": {
    "title": "Learning Performance Maximizing Ensembles with Explainability Guarantees",
    "volume": "main",
    "abstract": "In this paper we propose a method for the optimal allocation of observations between an intrinsically explainable glass box model and a black box model. An optimal allocation being defined as one which, for any given explainability level (i.e. the proportion of observations for which the explainable model is the prediction function), maximizes the performance of the ensemble on the underlying task, and maximizes performance of the explainable model on the observations allocated to it, subject to the maximal ensemble performance condition. The proposed method is shown to produce such explainability optimal allocations on a benchmark suite of tabular datasets across a variety of explainable and black box model types. These learned allocations are found to consistently maintain ensemble performance at very high explainability levels (explaining 74% of observations on average), and in some cases even outperform both the component explainable and black box models while improving explainability",
    "checked": true,
    "id": "a016ee15dcf20c411d70ccc7550816c64510dab4",
    "semantic_title": "learning performance maximizing ensembles with explainability guarantees",
    "citation_count": 0,
    "authors": [
      "Vincent Pisztora",
      "Jia Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29379": {
    "title": "Reconciling Predictive and Statistical Parity: A Causal Approach",
    "volume": "main",
    "abstract": "Since the rise of fair machine learning as a critical field of inquiry, many different notions on how to quantify and measure discrimination have been proposed in the literature. Some of these notions, however, were shown to be mutually incompatible. Such findings make it appear that numerous different kinds of fairness exist, thereby making a consensus on the appropriate measure of fairness harder to reach, hindering the applications of these tools in practice. In this paper, we investigate one of these key impossibility results that relates the notions of statistical and predictive parity. Specifically, we derive a new causal decomposition formula for the fairness measures associated with predictive parity, and obtain a novel insight into how this criterion is related to statistical parity through the legal doctrines of disparate treatment, disparate impact, and the notion of business necessity. Our results show that through a more careful causal analysis, the notions of statistical and predictive parity are not really mutually exclusive, but complementary and spanning a spectrum of fairness notions through the concept of business necessity. Finally, we demonstrate the importance of our findings on a real-world example",
    "checked": true,
    "id": "eeadc941de6e2eafb2a7c3d18a20d01e0f07029e",
    "semantic_title": "reconciling predictive and statistical parity: a causal approach",
    "citation_count": 2,
    "authors": [
      "Drago Plecko",
      "Elias Bareinboim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29380": {
    "title": "Adaptive Feature Imputation with Latent Graph for Deep Incomplete Multi-View Clustering",
    "volume": "main",
    "abstract": "In recent years, incomplete multi-view clustering (IMVC), which studies the challenging multi-view clustering problem on missing views, has received growing research interests. Previous IMVC methods suffer from the following issues: (1) the inaccurate imputation for missing data, which leads to suboptimal clustering performance, and (2) most existing IMVC models merely consider the explicit presence of graph structure in data, ignoring the fact that latent graphs of different views also provide valuable information for the clustering task. To overcome such challenges, we present a novel method, termed Adaptive feature imputation with latent graph for incomplete multi-view clustering (AGDIMC). Specifically, it captures the embbedded features of each view by incorporating the view-specific deep encoders. Then, we construct partial latent graphs on complete data, which can consolidate the intrinsic relationships within each view while preserving the topological information. With the aim of estimating the missing sample based on the available information, we utilize an adaptive imputation layer to impute the embedded feature of missing data by using cross-view soft cluster assignments and global cluster centroids. As the imputation progresses, the portion of complete data increases, contributing to enhancing the discriminative information contained in global pseudo-labels. Meanwhile, to alleviate the negative impact caused by inferior impute samples and the discrepancy of cluster structures, we further design an adaptive imputation strategy based on the global pseudo-label and the local cluster assignment. Experimental results on multiple real-world datasets demonstrate the effectiveness of our method over existing approaches",
    "checked": true,
    "id": "e8d34fa76ea840807823304af295f2e327ce7533",
    "semantic_title": "adaptive feature imputation with latent graph for deep incomplete multi-view clustering",
    "citation_count": 0,
    "authors": [
      "Jingyu Pu",
      "Chenhang Cui",
      "Xinyue Chen",
      "Yazhou Ren",
      "Xiaorong Pu",
      "Zhifeng Hao",
      "Philip S. Yu",
      "Lifang He"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29381": {
    "title": "MDGNN: Multi-Relational Dynamic Graph Neural Network for Comprehensive and Dynamic Stock Investment Prediction",
    "volume": "main",
    "abstract": "The stock market is a crucial component of the financial system, but predicting the movement of stock prices is challenging due to the dynamic and intricate relations arising from various aspects such as economic indicators, financial reports, global news, and investor sentiment. Traditional sequential methods and graph-based models have been applied in stock movement prediction, but they have limitations in capturing the multifaceted and temporal influences in stock price movements. To address these challenges, the Multi-relational Dynamic Graph Neural Network (MDGNN) framework is proposed, which utilizes a discrete dynamic graph to comprehensively capture multifaceted relations among stocks and their evolution over time. The representation generated from the graph offers a complete perspective on the interrelationships among stocks and associated entities. Additionally, the power of the Transformer structure is leveraged to encode the temporal evolution of multiplex relations, providing a dynamic and effective approach to predicting stock investment. Further, our proposed MDGNN framework achieves the best performance in public datasets compared with the state-of-the-art stock investment methods",
    "checked": true,
    "id": "48eb5d1f22c5174e9dea801b68517e411c1681e9",
    "semantic_title": "mdgnn: multi-relational dynamic graph neural network for comprehensive and dynamic stock investment prediction",
    "citation_count": 0,
    "authors": [
      "Hao Qian",
      "Hongting Zhou",
      "Qian Zhao",
      "Hao Chen",
      "Hongxiang Yao",
      "Jingwei Wang",
      "Ziqi Liu",
      "Fei Yu",
      "Zhiqiang Zhang",
      "Jun Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29382": {
    "title": "Towards Modeling Uncertainties of Self-Explaining Neural Networks via Conformal Prediction",
    "volume": "main",
    "abstract": "Despite the recent progress in deep neural networks (DNNs), it remains challenging to explain the predictions made by DNNs. Existing explanation methods for DNNs mainly focus on post-hoc explanations where another explanatory model is employed to provide explanations. The fact that post-hoc methods can fail to reveal the actual original reasoning process of DNNs raises the need to build DNNs with built-in interpretability. Motivated by this, many self-explaining neural networks have been proposed to generate not only accurate predictions but also clear and intuitive insights into why a particular decision was made. However, existing self-explaining networks are limited in providing distribution-free uncertainty quantification for the two simultaneously generated prediction outcomes (i.e., a sample's final prediction and its corresponding explanations for interpreting that prediction). Importantly, they also fail to establish a connection between the confidence values assigned to the generated explanations in the interpretation layer and those allocated to the final predictions in the ultimate prediction layer. To tackle the aforementioned challenges, in this paper, we design a novel uncertainty modeling framework for self-explaining networks, which not only demonstrates strong distribution-free uncertainty modeling performance for the generated explanations in the interpretation layer but also excels in producing efficient and effective prediction sets for the final predictions based on the informative high-level basis explanations. We perform the theoretical analysis for the proposed framework. Extensive experimental evaluation demonstrates the effectiveness of the proposed uncertainty framework",
    "checked": true,
    "id": "8881ec855fa5596bfd41d8f6c0ecc366e89faaa6",
    "semantic_title": "towards modeling uncertainties of self-explaining neural networks via conformal prediction",
    "citation_count": 0,
    "authors": [
      "Wei Qian",
      "Chenxu Zhao",
      "Yangyi Li",
      "Fenglong Ma",
      "Chao Zhang",
      "Mengdi Huai"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29383": {
    "title": "Upper Bounding Barlow Twins: A Novel Filter for Multi-Relational Clustering",
    "volume": "main",
    "abstract": "Multi-relational clustering is a challenging task due to the fact that diverse semantic information conveyed in multi-layer graphs is difficult to extract and fuse. Recent methods integrate topology structure and node attribute information through graph filtering. However, they often use a low-pass filter without fully considering the correlation among multiple graphs. To overcome this drawback, we propose to learn a graph filter motivated by the theoretical analysis of Barlow Twins. We find that input with a negative semi-definite inner product provides a lower bound for Barlow Twins loss, which prevents it from reaching a better solution. We thus learn a filter that yields an upper bound for Barlow Twins. Afterward, we design a simple clustering architecture and demonstrate its state-of-the-art performance on four benchmark datasets. The source code is available at https://github.com/XweiQ/BTGF",
    "checked": true,
    "id": "019d3d28991fcff9a1b0dc07d44ef2f28913fa6c",
    "semantic_title": "upper bounding barlow twins: a novel filter for multi-relational clustering",
    "citation_count": 0,
    "authors": [
      "Xiaowei Qian",
      "Bingheng Li",
      "Zhao Kang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29384": {
    "title": "EarnHFT: Efficient Hierarchical Reinforcement Learning for High Frequency Trading",
    "volume": "main",
    "abstract": "High-frequency trading (HFT) is using computer algorithms to make trading decisions in short time scales (e.g., second-level), which is widely used in the Cryptocurrency (Crypto) market, (e.g., Bitcoin). Reinforcement learning (RL) in financial research has shown stellar performance on many quantitative trading tasks. However, most methods focus on low-frequency trading, e.g., day-level, which cannot be directly applied to HFT because of two challenges. First, RL for HFT involves dealing with extremely long trajectories (e.g., 2.4 million steps per month), which is hard to optimize and evaluate. Second, the dramatic price fluctuations and market trend changes of Crypto make existing algorithms fail to maintain satisfactory performances. To tackle these challenges, we propose an Efficient hieArchical Reinforcement learNing method for High Frequency Trading (EarnHFT), a novel three-stage hierarchical RL framework for HFT. In stage I, we compute a Q-teacher, i.e., the optimal action value based on dynamic programming, for enhancing the performance and training efficiency of second level RL agents. In stage II, we construct a pool of diverse RL agents for different market trends, distinguished by return rates, where hundreds of RL agents are trained with different preferences of return rates and only a tiny fraction of them will be selected into the pool based on their profitability. In stage III, we train a minute-level router which dynamically picks a second-level agent from the pool to achieve stable performance across different markets. Through extensive experiments in various market trends on Crypto markets in a high-fidelity simulation trading environment, we demonstrate that EarnHFT significantly outperforms 6 state-of-art baselines in 6 popular financial criteria, exceeding the runner-up by 30% in profitability",
    "checked": true,
    "id": "3326c3b02cc9823716a045d51f8305f27c1eac0e",
    "semantic_title": "earnhft: efficient hierarchical reinforcement learning for high frequency trading",
    "citation_count": 0,
    "authors": [
      "Molei Qin",
      "Shuo Sun",
      "Wentao Zhang",
      "Haochong Xia ",
      "Xinrun Wang",
      "Bo An"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29385": {
    "title": "Resisting Backdoor Attacks in Federated Learning via Bidirectional Elections and Individual Perspective",
    "volume": "main",
    "abstract": "Existing approaches defend against backdoor attacks in federated learning (FL) mainly through a) mitigating the impact of infected models, or b) excluding infected models. The former negatively impacts model accuracy, while the latter usually relies on globally clear boundaries between benign and infected model updates. However, in reality, model updates can easily become mixed and scattered throughout due to the diverse distributions of local data. This work focuses on excluding infected models in FL. Unlike previous perspectives from a global view, we propose Snowball, a novel anti-backdoor FL framework through bidirectional elections from an individual perspective inspired by one principle deduced by us and two principles in FL and deep learning. It is characterized by a) bottom-up election, where each candidate model update votes to several peer ones such that a few model updates are elected as selectees for aggregation; and b) top-down election, where selectees progressively enlarge themselves through picking up from the candidates. We compare Snowball with state-of-the-art defenses to backdoor attacks in FL on five real-world datasets, demonstrating its superior resistance to backdoor attacks and slight impact on the accuracy of the global model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Qin",
      "Feiyi Chen",
      "Chen Zhi",
      "Xueqiang Yan",
      "Shuiguang Deng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29386": {
    "title": "Tree Search-Based Evolutionary Bandits for Protein Sequence Optimization",
    "volume": "main",
    "abstract": "While modern biotechnologies allow synthesizing new proteins and function measurements at scale, efficiently exploring a protein sequence space and engineering it remains a daunting task due to the vast sequence space of any given protein. Protein engineering is typically conducted through an iterative process of adding mutations to the wild-type or lead sequences, recombination of mutations, and running new rounds of screening. To enhance the efficiency of such a process, we propose a tree search-based bandit learning method, which expands a tree starting from the initial sequence with the guidance of a bandit machine learning model. Under simplified assumptions and a Gaussian Process prior, we provide theoretical analysis and a Bayesian regret bound, demonstrating that the method can efficiently discover a near-optimal design. The full algorithm is compatible with a suite of randomized tree search heuristics, machine learning models, pre-trained embeddings, and bandit techniques. We test various instances of the algorithm across benchmark protein datasets using simulated screens. Experiment results demonstrate that the algorithm is both sample-efficient, diversity-promoting, and able to find top designs using reasonably small mutation counts",
    "checked": true,
    "id": "55e8b67017af1113a264055f7e615379a724b65e",
    "semantic_title": "tree search-based evolutionary bandits for protein sequence optimization",
    "citation_count": 0,
    "authors": [
      "Jiahao Qiu",
      "Hui Yuan",
      "Jinghong Zhang",
      "Wentao Chen",
      "Huazheng Wang",
      "Mengdi Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29387": {
    "title": "Multi-Level Cross-Modal Alignment for Image Clustering",
    "volume": "main",
    "abstract": "Recently, the cross-modal pretraining model has been employed to produce meaningful pseudo-labels to supervise the training of an image clustering model. However, numerous erroneous alignments in a cross-modal pretraining model could produce poor-quality pseudo labels and degrade clustering performance. To solve the aforementioned issue, we propose a novel Multi-level Cross-modal Alignment method to improve the alignments in a cross-modal pretraining model for downstream tasks, by building a smaller but better semantic space and aligning the images and texts in three levels, i.e., instance-level, prototype-level, and semantic-level. Theoretical results show that our proposed method converges, and suggests effective means to reduce the expected clustering risk of our method. Experimental results on five benchmark datasets clearly show the superiority of our new method",
    "checked": true,
    "id": "adbf9444eb62a9ccb9f6190d673a2f21e73a3f5b",
    "semantic_title": "multi-level cross-modal alignment for image clustering",
    "citation_count": 0,
    "authors": [
      "Liping Qiu",
      "Qin Zhang",
      "Xiaojun Chen",
      "Shaotian Cai"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29388": {
    "title": "Integer Is Enough: When Vertical Federated Learning Meets Rounding",
    "volume": "main",
    "abstract": "Vertical Federated Learning (VFL) is a solution increasingly used by companies with the same user group but differing features, enabling them to collaboratively train a machine learning model. VFL ensures that clients exchange intermediate results extracted by their local models, without sharing raw data. However, in practice, VFL encounters several challenges, such as computational and communication overhead, privacy leakage risk, and adversarial attack. Our study reveals that the usage of floating-point (FP) numbers is a common factor causing these issues, as they can be redundant and contain too much information. To address this, we propose a new architecture called rounding layer, which converts intermediate results to integers. Our theoretical analysis and empirical results demonstrate the benefits of the rounding layer in reducing computation and memory overhead, providing privacy protection, preserving model performance, and mitigating adversarial attacks. We hope this paper inspires further research into novel architectures to address practical issues in VFL",
    "checked": true,
    "id": "9dde49c2fd168fe7c74942e222667da75319c365",
    "semantic_title": "integer is enough: when vertical federated learning meets rounding",
    "citation_count": 0,
    "authors": [
      "Pengyu Qiu",
      "Yuwen Pu",
      "Yongchao Liu",
      "Wenyan Liu",
      "Yun Yue",
      "Xiaowei Zhu",
      "Lichun Li",
      "Jinbao Li",
      "Shouling Ji"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29389": {
    "title": "Towards Multi-Mode Outlier Robust Tensor Ring Decomposition",
    "volume": "main",
    "abstract": "Conventional Outlier Robust Tensor Decomposition (ORTD) approaches generally represent sparse outlier corruption within a specific mode. However, such an assumption, which may hold for matrices, proves inadequate when applied to high-order tensors. In the tensor domain, the outliers are prone to be corrupted in multiple modes simultaneously. Addressing this limitation, this study proposes a novel ORTD approach by recovering low-rank tensors contaminated by outliers spanning multiple modes. In particular, we conceptualize outliers within high-order tensors as latent tensor group sparsity by decomposing the corrupted tensor into a sum of multiple latent components, where each latent component is exclusive to outliers within a particular direction. Thus, it can effectively mitigate the outlier corruptions prevalent in high-order tensors across multiple modes. To theoretically guarantee recovery performance, we rigorously analyze a non-asymptotic upper bound of the estimation error for the proposed ORTD approach. In the optimization process, we develop an efficient alternate direction method of multipliers (ADMM) algorithm. Empirical validation of the approach's efficacy is undertaken through comprehensive experimentation",
    "checked": true,
    "id": "49cad67c32f7c3b35fc15021a8740167c7fa471e",
    "semantic_title": "towards multi-mode outlier robust tensor ring decomposition",
    "citation_count": 0,
    "authors": [
      "Yuning Qiu",
      "Guoxu Zhou",
      "Andong Wang",
      "Zhenhao Huang",
      "Qibin Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29390": {
    "title": "Learning the Topology and Behavior of Discrete Dynamical Systems",
    "volume": "main",
    "abstract": "Discrete dynamical systems are commonly used to model the spread of contagions on real-world networks. Under the PAC framework, existing research has studied the problem of learning the behavior of a system, assuming that the underlying network is known. In this work, we focus on a more challenging setting: to learn both the behavior and the underlying topology of a black-box system. We show that, in general, this learning problem is computationally intractable. On the positive side, we present efficient learning methods under the PAC model when the underlying graph of the dynamical system belongs to certain classes. Further, we examine a relaxed setting where the topology of an unknown system is partially observed. For this case, we develop an efficient PAC learner to infer the system and establish the sample complexity. Lastly, we present a formal analysis of the expressive power of the hypothesis class of dynamical systems where both the topology and behavior are unknown, using the well-known Natarajan dimension formalism. Our results provide a theoretical foundation for learning both the topology and behavior of discrete dynamical systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zirou Qiu",
      "Abhijin Adiga",
      "Madhav V. Marathe",
      "S. S. Ravi",
      "Daniel J. Rosenkrantz",
      "Richard E. Stearns",
      "Anil Vullikanti"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29391": {
    "title": "LDS2AE: Local Diffusion Shared-Specific Autoencoder for Multimodal Remote Sensing Image Classification with Arbitrary Missing Modalities",
    "volume": "main",
    "abstract": "Recent research on the joint classification of multimodal remote sensing data has achieved great success. However, due to the limitations imposed by imaging conditions, the case of missing modalities often occurs in practice. Most previous researchers regard the classification in case of different missing modalities as independent tasks. They train a specific classification model for each fixed missing modality by extracting multimodal joint representation, which cannot handle the classification of arbitrary (including multiple and random) missing modalities. In this work, we propose a local diffusion shared-specific autoencoder (LDS2AE), which solves the classification of arbitrary missing modalities with a single model. The LDS2AE captures the data distribution of different modalities to learn multimodal shared feature for classification by designing a novel local diffusion autoencoder which consists of a modality-shared encoder and several modality-specific decoders. The modality-shared encoder is designed to extract multimodal shared feature by employing the same parameters to map multimodal data into a shared subspace. The modality-specific decoders put the multimodal shared feature to reconstruct the image of each modality, which facilitates the shared feature to learn unique information of different modalities. In addition, we incorporate masked training to the diffusion autoencoder to achieve local diffusion, which significantly reduces the training cost of model. The approach is tested on widely-used multimodal remote sensing datasets, demonstrating the effectiveness of the proposed LDS2AE in addressing the classification of arbitrary missing modalities. The code is available at https://github.com/Jiahuiqu/LDS2AE",
    "checked": true,
    "id": "233e91943e0a528c3995f4eab5f8f3a795eda353",
    "semantic_title": "lds2ae: local diffusion shared-specific autoencoder for multimodal remote sensing image classification with arbitrary missing modalities",
    "citation_count": 1,
    "authors": [
      "Jiahui Qu",
      "Yuanbo Yang",
      "Wenqian Dong",
      "Yufei Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29392": {
    "title": "Dual-Level Curriculum Meta-Learning for Noisy Few-Shot Learning Tasks",
    "volume": "main",
    "abstract": "Few-shot learning (FSL) is essential in many practical applications. However, the limited training examples make the models more vulnerable to label noise, which can lead to poor generalization capability. To address this critical challenge, we propose a curriculum meta-learning model that employs a novel dual-level class-example sampling strategy to create a robust curriculum for adaptive task distribution formulation and robust model training. The dual-level framework proposes a heuristic class sampling criterion that measures pairwise class boundary complexity to form a class curriculum; it uses effective example sampling through an under-trained proxy model to form an example curriculum. By utilizing both class-level and example-level information, our approach is more robust to handle limited training data and noisy labels that commonly occur in few-shot learning tasks. The model has efficient convergence behavior, which is verified through rigorous convergence analysis. Additionally, we establish a novel error bound through a hierarchical PAC-Bayesian analysis for curriculum meta-learning under noise. We conduct extensive experiments that demonstrate the effectiveness of our framework in outperforming existing noisy few-shot learning methods under various few-shot classification benchmarks. Our code is available at https://github.com/ritmininglab/DCML",
    "checked": true,
    "id": "16c0f839637a4a6d615716b9fb9b6827b678893a",
    "semantic_title": "dual-level curriculum meta-learning for noisy few-shot learning tasks",
    "citation_count": 0,
    "authors": [
      "Xiaofan Que",
      "Qi Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29393": {
    "title": "DSD²: Can We Dodge Sparse Double Descent and Compress the Neural Network Worry-Free?",
    "volume": "main",
    "abstract": "Neoteric works have shown that modern deep learning models can exhibit a sparse double descent phenomenon. Indeed, as the sparsity of the model increases, the test performance first worsens since the model is overfitting the training data; then, the overfitting reduces, leading to an improvement in performance, and finally, the model begins to forget critical information, resulting in underfitting. Such a behavior prevents using traditional early stop criteria. In this work, we have three key contributions. First, we propose a learning framework that avoids such a phenomenon and improves generalization. Second, we introduce an entropy measure providing more insights into the insurgence of this phenomenon and enabling the use of traditional stop criteria. Third, we provide a comprehensive quantitative analysis of contingent factors such as re-initialization methods, model width and depth, and dataset noise. The contributions are supported by empirical evidence in typical setups. Our code is available at https://github.com/VGCQ/DSD2",
    "checked": true,
    "id": "b7e01111c4a2c539cb76c7aef9f97a05cb6470eb",
    "semantic_title": "dsd²: can we dodge sparse double descent and compress the neural network worry-free?",
    "citation_count": 6,
    "authors": [
      "Victor Quétu",
      "Enzo Tartaglione"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29394": {
    "title": "Fair Participation via Sequential Policies",
    "volume": "main",
    "abstract": "Leading approaches to algorithmic fairness and policy-induced distribution shift are often misaligned with long-term objectives in sequential settings. We aim to correct these shortcomings by ensuring that both the objective and fairness constraints account for policy-induced distribution shift. First, we motivate this problem using an example in which individuals subject to algorithmic predictions modulate their willingness to participate with the policy maker. Fairness in this example is measured by the variance of group participation rates. Next, we develop a method for solving the resulting constrained, non-linear optimization problem and prove that this method converges to a fair, locally optimal policy given first-order information. Finally, we experimentally validate our claims in a semi-synthetic setting",
    "checked": true,
    "id": "4f501c5436433a6c2f6066b927bceac08e384cd6",
    "semantic_title": "fair participation via sequential policies",
    "citation_count": 1,
    "authors": [
      "Reilly Raab",
      "Ross Boczar",
      "Maryam Fazel",
      "Yang Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29395": {
    "title": "Understanding the Generalization of Pretrained Diffusion Models on Out-of-Distribution Data",
    "volume": "main",
    "abstract": "This work tackles the important task of understanding out-of-distribution behavior in two prominent types of generative models, i.e., GANs and Diffusion models. Understanding this behavior is crucial in understanding their broader utility and risks as these systems are increasingly deployed in our daily lives. Our first contribution is demonstrating that diffusion spaces outperform GANs' latent spaces in inverting high-quality OOD images. We also provide a theoretical analysis attributing this to the lack of prior holes in diffusion spaces. Our second significant contribution is to provide a theoretical hypothesis that diffusion spaces can be projected onto a bounded hypersphere, enabling image manipulation through geodesic traversal between inverted images. Our analysis shows that different geodesics share common attributes for the same manipulation, which we leverage to perform various image manipulations. We conduct thorough empirical evaluations to support and validate our claims. Finally, our third and final contribution introduces a novel approach to the few-shot sampling for out-of-distribution data by inverting a few images to sample from the cluster formed by the inverted latents. The proposed technique achieves state-of-the-art results for the few-shot generation task in terms of image quality. Our research underscores the promise of diffusion spaces in out-of-distribution imaging and offers avenues for further exploration. Please find more details about our project at \\url{http://cvit.iiit.ac.in/research/projects/cvit-projects/diffusionOOD}",
    "checked": true,
    "id": "f1f43f40830b8fa199bce791e2ff4b63b485319e",
    "semantic_title": "understanding the generalization of pretrained diffusion models on out-of-distribution data",
    "citation_count": 1,
    "authors": [
      "Sai Niranjan Ramachandran",
      "Rudrabha Mukhopadhyay",
      "Madhav Agarwal",
      "C.V. Jawahar",
      "Vinay Namboodiri"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29396": {
    "title": "GINN-LP: A Growing Interpretable Neural Network for Discovering Multivariate Laurent Polynomial Equations",
    "volume": "main",
    "abstract": "Traditional machine learning is generally treated as a black-box optimization problem and does not typically produce interpretable functions that connect inputs and outputs. However, the ability to discover such interpretable functions is desirable. In this work, we propose GINN-LP, an interpretable neural network to discover the form and coefficients of the underlying equation of a dataset, when the equation is assumed to take the form of a multivariate Laurent Polynomial. This is facilitated by a new type of interpretable neural network block, named the \"power-term approximator block\", consisting of logarithmic and exponential activation functions. GINN-LP is end-to-end differentiable, making it possible to use backpropagation for training. We propose a neural network growth strategy that will enable finding the suitable number of terms in the Laurent polynomial that represents the data, along with sparsity regularization to promote the discovery of concise equations. To the best of our knowledge, this is the first model that can discover arbitrary multivariate Laurent polynomial terms without any prior information on the order. Our approach is first evaluated on a subset of data used in SRBench, a benchmark for symbolic regression. We first show that GINN-LP outperforms the state-of-the-art symbolic regression methods on datasets generated using 48 real-world equations in the form of multivariate Laurent polynomials. Next, we propose an ensemble method that combines our method with a high-performing symbolic regression method, enabling us to discover non-Laurent polynomial equations. We achieve state-of-the-art results in equation discovery, showing an absolute improvement of 7.1% over the best contender, by applying this ensemble method to 113 datasets within SRBench with known ground-truth equations",
    "checked": true,
    "id": "15cd06c0b64fa2be6ba50eb29c45718b7ddcebac",
    "semantic_title": "ginn-lp: a growing interpretable neural network for discovering multivariate laurent polynomial equations",
    "citation_count": 0,
    "authors": [
      "Nisal Ranasinghe",
      "Damith Senanayake",
      "Sachith Seneviratne",
      "Malin Premaratne",
      "Saman Halgamuge"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29397": {
    "title": "Using Stratified Sampling to Improve LIME Image Explanations",
    "volume": "main",
    "abstract": "We investigate the use of a stratified sampling approach for LIME Image, a popular model-agnostic explainable AI method for computer vision tasks, in order to reduce the artifacts generated by typical Monte Carlo sampling. Such artifacts are due to the undersampling of the dependent variable in the synthetic neighborhood around the image being explained, which may result in inadequate explanations due to the impossibility of fitting a linear regressor on the sampled data. We then highlight a connection with the Shapley theory, where similar arguments about undersampling and sample relevance were suggested in the past. We derive all the formulas and adjustment factors required for an unbiased stratified sampling estimator. Experiments show the efficacy of the proposed approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Rashid",
      "Elvio G. Amparore",
      "Enrico Ferrari",
      "Damiano Verda"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29398": {
    "title": "NESTER: An Adaptive Neurosymbolic Method for Causal Effect Estimation",
    "volume": "main",
    "abstract": "Causal effect estimation from observational data is a central problem in causal inference. Methods based on potential outcomes framework solve this problem by exploiting inductive biases and heuristics from causal inference. Each of these methods addresses a specific aspect of causal effect estimation, such as controlling propensity score, enforcing randomization, etc., by designing neural network (NN) architectures and regularizers. In this paper, we propose an adaptive method called Neurosymbolic Causal Effect Estimator (NESTER), a generalized method for causal effect estimation. NESTER integrates the ideas used in existing methods based on multi-head NNs for causal effect estimation into one framework. We design a Domain Specific Language (DSL) tailored for causal effect estimation based on causal inductive biases used in literature. We conduct a theoretical analysis to investigate NESTER's efficacy in estimating causal effects. Our comprehensive empirical results show that NESTER performs better than state-of-the-art methods on benchmark datasets",
    "checked": true,
    "id": "8b95944a59f90244ed846fc4ac665809cb194fe3",
    "semantic_title": "nester: an adaptive neurosymbolic method for causal effect estimation",
    "citation_count": 0,
    "authors": [
      "Abbavaram Gowtham Reddy",
      "Vineeth N Balasubramanian"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29399": {
    "title": "Towards Learning and Explaining Indirect Causal Effects in Neural Networks",
    "volume": "main",
    "abstract": "Recently, there has been a growing interest in learning and explaining causal effects within Neural Network (NN) models. By virtue of NN architectures, previous approaches consider only direct and total causal effects assuming independence among input variables. We view an NN as a structural causal model (SCM) and extend our focus to include indirect causal effects by introducing feedforward connections among input neurons. We propose an ante-hoc method that captures and maintains direct, indirect, and total causal effects during NN model training. We also propose an algorithm for quantifying learned causal effects in an NN model and efficient approximation strategies for quantifying causal effects in high-dimensional data. Extensive experiments conducted on synthetic and real-world datasets demonstrate that the causal effects learned by our ante-hoc method better approximate the ground truth effects compared to existing methods",
    "checked": true,
    "id": "792449963d1a26d3a713dcf120d83576dc28d4de",
    "semantic_title": "towards learning and explaining indirect causal effects in neural networks",
    "citation_count": 0,
    "authors": [
      "Abbavaram Gowtham Reddy",
      "Saketh Bachu",
      "Harsharaj Pathak",
      "Benin Godfrey L",
      "Varshaneya V",
      "Vineeth N Balasubramanian",
      "Satyanarayan Kar"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29400": {
    "title": "Towards Improved Proxy-Based Deep Metric Learning via Data-Augmented Domain Adaptation",
    "volume": "main",
    "abstract": "Deep Metric Learning (DML) plays an important role in modern computer vision research, where we learn a distance metric for a set of image representations. Recent DML techniques utilize the proxy to interact with the corresponding image samples in the embedding space. However, existing proxy-based DML methods focus on learning individual proxy-to-sample distance, while the overall distribution of samples and proxies lacks attention. In this paper, we present a novel proxy-based DML framework that focuses on aligning the sample and proxy distributions to improve the efficiency of proxy-based DML losses. Specifically, we propose the Data-Augmented Domain Adaptation (DADA) method to adapt the domain gap between the group of samples and proxies. To the best of our knowledge, we are the first to leverage domain adaptation to boost the performance of proxy-based DML. We show that our method can be easily plugged into existing proxy-based DML losses. Our experiments on benchmarks, including the popular CUB-200-2011, CARS196, Stanford Online Products, and In-Shop Clothes Retrieval, show that our learning algorithm significantly improves the existing proxy losses and achieves superior results compared to the existing methods. The code and Appendix are available at: https://github.com/Noahsark/DADA",
    "checked": true,
    "id": "0f7dcbc6c91294c8465f221e87ff71b8f52bd836",
    "semantic_title": "towards improved proxy-based deep metric learning via data-augmented domain adaptation",
    "citation_count": 1,
    "authors": [
      "Li Ren",
      "Chen Chen",
      "Liqiang Wang",
      "Kien Hua"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29401": {
    "title": "Statistical Spatially Inhomogeneous Diffusion Inference",
    "volume": "main",
    "abstract": "Inferring a diffusion equation from discretely observed measurements is a statistical challenge of significant importance in a variety of fields, from single-molecule tracking in biophysical systems to modeling financial instruments. Assuming that the underlying dynamical process obeys a d-dimensional stochastic differential equation of the form dx_t = b(x_t)dt + \\Sigma(x_t)dw_t, we propose neural network-based estimators of both the drift b and the spatially-inhomogeneous diffusion tensor D = \\Sigma\\Sigma^T/2 and provide statistical convergence guarantees when b and D are s-Hölder continuous. Notably, our bound aligns with the minimax optimal rate N^{-\\frac{2s}{2s+d}} for nonparametric function estimation even in the presence of correlation within observational data, which necessitates careful handling when establishing fast-rate generalization bounds. Our theoretical results are bolstered by numerical experiments demonstrating accurate inference of spatially-inhomogeneous diffusion tensors",
    "checked": true,
    "id": "6164f8f6443ce9555b7d3e037da5dbf452009feb",
    "semantic_title": "statistical spatially inhomogeneous diffusion inference",
    "citation_count": 0,
    "authors": [
      "Yinuo Ren",
      "Yiping Lu",
      "Lexing Ying",
      "Grant M. Rotskoff"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29402": {
    "title": "Protect Your Score: Contact-Tracing with Differential Privacy Guarantees",
    "volume": "main",
    "abstract": "The pandemic in 2020 and 2021 had enormous economic and societal consequences, and studies show that contact tracing algorithms can be key in the early containment of the virus. While large strides have been made towards more effective contact tracing algorithms, we argue that privacy concerns currently hold deployment back. The essence of a contact tracing algorithm constitutes the communication of a risk score. Yet, it is precisely the communication and release of this score to a user that an adversary can leverage to gauge the private health status of an individual. We pinpoint a realistic attack scenario and propose a contact tracing algorithm with differential privacy guarantees against this attack. The algorithm is tested on the two most widely used agent-based COVID19 simulators and demonstrates superior performance in a wide range of settings. Especially for realistic test scenarios and while releasing each risk score with epsilon=1 differential privacy, we achieve a two to ten-fold reduction in the infection rate of the virus. To the best of our knowledge, this presents the first contact tracing algorithm with differential privacy guarantees when revealing risk scores for COVID19",
    "checked": false,
    "id": "e543ca684d99690c962bae4f466e4c90960fd77d",
    "semantic_title": "protect your score: contact tracing with differential privacy guarantees",
    "citation_count": 1,
    "authors": [
      "Rob Romijnders",
      "Christos Louizos",
      "Yuki M. Asano",
      "Max Welling"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29403": {
    "title": "Limitations of Face Image Generation",
    "volume": "main",
    "abstract": "Text-to-image diffusion models have achieved widespread popularity due to their unprecedented image generation capability. In particular, their ability to synthesize and modify human faces has spurred research into using generated face images in both training data augmentation and model performance assessments. In this paper, we study the efficacy and shortcomings of generative models in the context of face generation. Utilizing a combination of qualitative and quantitative measures, including embedding-based metrics and user studies, we present a framework to audit the characteristics of generated faces conditioned on a set of social attributes. We applied our framework on faces generated through state-of-the-art text-to-image diffusion models. We identify several limitations of face image generation that include faithfulness to the text prompt, demographic disparities, and distributional shifts. Furthermore, we present an analytical model that provides insights into how training data selection contributes to the performance of generative models. Our survey data and analytics code can be found online at https://github.com/wi-pi/Limitations_of_Face_Generation",
    "checked": true,
    "id": "df3e765df291d2fbc0f038b9798e2b6e9447faef",
    "semantic_title": "limitations of face image generation",
    "citation_count": 0,
    "authors": [
      "Harrison Rosenberg",
      "Shimaa Ahmed",
      "Guruprasad Ramesh",
      "Kassem Fawaz",
      "Ramya Korlakai Vinayak"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29404": {
    "title": "Scaling Up Semi-supervised Learning with Unconstrained Unlabelled Data",
    "volume": "main",
    "abstract": "We propose UnMixMatch, a semi-supervised learning framework which can learn effective representations from unconstrained unlabelled data in order to scale up performance. Most existing semi-supervised methods rely on the assumption that labelled and unlabelled samples are drawn from the same distribution, which limits the potential for improvement through the use of free-living unlabeled data. Consequently, the generalizability and scalability of semi-supervised learning are often hindered by this assumption. Our method aims to overcome these constraints and effectively utilize unconstrained unlabelled data in semi-supervised learning. UnMixMatch consists of three main components: a supervised learner with hard augmentations that provides strong regularization, a contrastive consistency regularizer to learn underlying representations from the unlabelled data, and a self-supervised loss to enhance the representations that are learnt from the unlabelled data. We perform extensive experiments on 4 commonly used datasets and demonstrate superior performance over existing semi-supervised methods with a performance boost of 4.79%. Extensive ablation and sensitivity studies show the effectiveness and impact of each of the proposed components of our method. The code for our work is publicly available",
    "checked": true,
    "id": "393b2bba2dd1e25f4988e36e5c9abc4f560d5fe3",
    "semantic_title": "scaling up semi-supervised learning with unconstrained unlabelled data",
    "citation_count": 1,
    "authors": [
      "Shuvendu Roy",
      "Ali Etemad"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29405": {
    "title": "SimPSI: A Simple Strategy to Preserve Spectral Information in Time Series Data Augmentation",
    "volume": "main",
    "abstract": "Data augmentation is a crucial component in training neural networks to overcome the limitation imposed by data size, and several techniques have been studied for time series. Although these techniques are effective in certain tasks, they have yet to be generalized to time series benchmarks. We find that current data augmentation techniques ruin the core information contained within the frequency domain. To address this issue, we propose a simple strategy to preserve spectral information (SimPSI) in time series data augmentation. SimPSI preserves the spectral information by mixing the original and augmented input spectrum weighted by a preservation map, which indicates the importance score of each frequency. Specifically, our experimental contributions are to build three distinct preservation maps: magnitude spectrum, saliency map, and spectrum-preservative map. We apply SimPSI to various time series data augmentations and evaluate its effectiveness across a wide range of time series benchmarks. Our experimental results support that SimPSI considerably enhances the performance of time series data augmentations by preserving core spectral information. The source code used in the paper is available at https://github.com/Hyun-Ryu/simpsi",
    "checked": true,
    "id": "3d71eda90c77cf6db28f76706ecbf75255b7980e",
    "semantic_title": "simpsi: a simple strategy to preserve spectral information in time series data augmentation",
    "citation_count": 1,
    "authors": [
      "Hyun Ryu",
      "Sunjae Yoon",
      "Hee Suk Yoon",
      "Eunseop Yoon",
      "Chang D. Yoo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29406": {
    "title": "Learning the Causal Structure of Networked Dynamical Systems under Latent Nodes and Structured Noise",
    "volume": "main",
    "abstract": "This paper considers learning the hidden causal network of a linear networked dynamical system (NDS) from the time series data at some of its nodes -- partial observability. The dynamics of the NDS are driven by colored noise that generates spurious associations across pairs of nodes, rendering the problem much harder. To address the challenge of noise correlation and partial observability, we assign to each pair of nodes a feature vector computed from the time series data of observed nodes. The feature embedding is engineered to yield structural consistency: there exists an affine hyperplane that consistently partitions the set of features, separating the feature vectors corresponding to connected pairs of nodes from those corresponding to disconnected pairs. The causal inference problem is thus addressed via clustering the designed features. We demonstrate with simple baseline supervised methods the competitive performance of the proposed causal inference mechanism under broad connectivity regimes and noise correlation levels, including a real world network. Further, we devise novel technical guarantees of structural consistency for linear NDS under the considered regime",
    "checked": true,
    "id": "d2a51aa81abd6f3c6cebf37658985ab1bd647b1d",
    "semantic_title": "learning the causal structure of networked dynamical systems under latent nodes and structured noise",
    "citation_count": 2,
    "authors": [
      "Augusto Santos",
      "Diogo Rente",
      "Rui Seabra",
      " José M. F. Moura"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29407": {
    "title": "XKD: Cross-Modal Knowledge Distillation with Domain Alignment for Video Representation Learning",
    "volume": "main",
    "abstract": "We present XKD, a novel self-supervised framework to learn meaningful representations from unlabelled videos. XKD is trained with two pseudo objectives. First, masked data reconstruction is performed to learn modality-specific representations from audio and visual streams. Next, self-supervised cross-modal knowledge distillation is performed between the two modalities through a teacher-student setup to learn complementary information. We introduce a novel domain alignment strategy to tackle domain discrepancy between audio and visual modalities enabling effective cross-modal knowledge distillation. Additionally, to develop a general-purpose network capable of handling both audio and visual streams, modality-agnostic variants of XKD are introduced, which use the same pretrained backbone for different audio and visual tasks. Our proposed cross-modal knowledge distillation improves video action classification by 8% to 14% on UCF101, HMDB51, and Kinetics400. Additionally, XKD improves multimodal action classification by 5.5% on Kinetics-Sound. XKD shows state-of-the-art performance in sound classification on ESC50, achieving top-1 accuracy of 96.5%",
    "checked": true,
    "id": "8e3fb0a7cbbe27ebbab9833cff2a60b0ed61e516",
    "semantic_title": "xkd: cross-modal knowledge distillation with domain alignment for video representation learning",
    "citation_count": 7,
    "authors": [
      "Pritam Sarkar",
      "Ali Etemad"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29408": {
    "title": "Understanding and Leveraging the Learning Phases of Neural Networks",
    "volume": "main",
    "abstract": "The learning dynamics of deep neural networks are not well understood. The information bottleneck (IB) theory proclaimed separate fitting and compression phases. But they have since been heavily debated. We comprehensively analyze the learning dynamics by investigating a layer's reconstruction ability of the input and prediction performance based on the evolution of parameters during training. We empirically show the existence of three phases using common datasets and architectures such as ResNet and VGG: (i) near constant reconstruction loss, (ii) decrease, and (iii) increase. We also derive an empirically grounded data model and prove the existence of phases for single-layer networks. Technically, our approach leverages classical complexity analysis. It differs from IB by relying on measuring reconstruction loss rather than information theoretic measures to relate information of intermediate layers and inputs. Our work implies a new best practice for transfer learning: We show empirically that the pre-training of a classifier should stop well before its performance is optimal",
    "checked": true,
    "id": "365485c022e86c1f36d3d40754dbe3eeac9d44f8",
    "semantic_title": "understanding and leveraging the learning phases of neural networks",
    "citation_count": 0,
    "authors": [
      "Johannes Schneider",
      "Mohit Prabhushankar"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29409": {
    "title": "What Do Hebbian Learners Learn? Reduction Axioms for Iterated Hebbian Learning",
    "volume": "main",
    "abstract": "This paper is a contribution to neural network semantics, a foundational framework for neuro-symbolic AI. The key insight of this theory is that logical operators can be mapped to operators on neural network states. In this paper, we do this for a neural network learning operator. We map a dynamic operator [φ] to iterated Hebbian learning, a simple learning policy that updates a neural network by repeatedly applying Hebb's learning rule until the net reaches a fixed-point. Our main result is that we can \"translate away\" [φ]-formulas via reduction axioms. This means that completeness for the logic of iterated Hebbian learning follows from completeness of the base logic. These reduction axioms also provide (1) a human-interpretable description of iterated Hebbian learning as a kind of plausibility upgrade, and (2) an approach to building neural networks with guarantees on what they can learn",
    "checked": true,
    "id": "468f212bd1036799fe9cbcb86689f3c833c9fe75",
    "semantic_title": "what do hebbian learners learn? reduction axioms for iterated hebbian learning",
    "citation_count": 0,
    "authors": [
      "Caleb Schultz Kisby",
      "Saúl A. Blanco",
      "Lawrence S. Moss"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29410": {
    "title": "Leaving the Nest: Going beyond Local Loss Functions for Predict-Then-Optimize",
    "volume": "main",
    "abstract": "Predict-then-Optimize is a framework for using machine learning to perform decision-making under uncertainty. The central research question it asks is, \"How can we use the structure of a decision-making task to tailor ML models for that specific task?\" To this end, recent work has proposed learning task-specific loss functions that capture this underlying structure. However, current approaches make restrictive assumptions about the form of these losses and their impact on ML model behavior. These assumptions both lead to approaches with high computational cost, and when they are violated in practice, poor performance. In this paper, we propose solutions to these issues, avoiding the aforementioned assumptions and utilizing the ML model's features to increase the sample efficiency of learning loss functions. We empirically show that our method achieves state-of-the-art results in four domains from the literature, often requiring an order of magnitude fewer samples than comparable methods from past work. Moreover, our approach outperforms the best existing method by nearly 200% when the localness assumption is broken",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanket Shah",
      "Bryan Wilder",
      "Andrew Perrault",
      "Milind Tambe"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29411": {
    "title": "Phoneme Hallucinator: One-Shot Voice Conversion via Set Expansion",
    "volume": "main",
    "abstract": "Voice conversion (VC) aims at altering a person's voice to make it sound similar to the voice of another person while preserving linguistic content. Existing methods suffer from a dilemma between content intelligibility and speaker similarity; i.e., methods with higher intelligibility usually have a lower speaker similarity, while methods with higher speaker similarity usually require plenty of target speaker voice data to achieve high intelligibility. In this work, we propose a novel method Phoneme Hallucinator that achieves the best of both worlds. Phoneme Hallucinator is a one-shot VC model; it adopts a novel model to hallucinate diversified and high-fidelity target speaker phonemes based just on a short target speaker voice (e.g. 3 seconds). The hallucinated phonemes are then exploited to perform neighbor-based voice conversion. Our model is a text-free, any-to-any VC model that requires no text annotations and supports conversion to any unseen speaker. Quantitative and qualitative evaluations show that Phoneme Hallucinator outperforms existing VC methods for both intelligibility and speaker similarity",
    "checked": true,
    "id": "2ca45369dea213c6490ddb7e97f53553213ed277",
    "semantic_title": "phoneme hallucinator: one-shot voice conversion via set expansion",
    "citation_count": 2,
    "authors": [
      "Siyuan Shan",
      "Yang Li",
      "Amartya Banerjee",
      "Junier B. Oliva"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29412": {
    "title": "No Internal Regret with Non-convex Loss Functions",
    "volume": "main",
    "abstract": "Internal regret is a measure of performance of an online learning algorithm, which measures the change in performance by substituting every occurrence of a given action i by an alternative action j. Algorithms for minimizing internal regret are known for the finite experts setting, including a general reduction to the problem of minimizing external regret for this case. The reduction however crucially depends on the finiteness of the action space. In this work we approach the problem of minimizing internal regret for a continuous action space. For the full information setting, we show how to obtain O(sqrt(T)) internal regret for the class of Lipschitz functions, as well as non-Lipschitz dispersed functions, i.e. the non-Lipschitzness may not concentrate in a small region of the action space. We also consider extensions to partial feedback settings, and again obtain sublinear internal regret. Finally we discuss applications of internal regret minimization over continuous spaces to correlated equilibria in pricing problems and auction design, as well as to data-driven hyperparameter tuning",
    "checked": true,
    "id": "84adcedb9f9daff3e90997273897c09ba5d53bea",
    "semantic_title": "no internal regret with non-convex loss functions",
    "citation_count": 1,
    "authors": [
      "Dravyansh Sharma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29413": {
    "title": "Symbolic Cognitive Diagnosis via Hybrid Optimization for Intelligent Education Systems",
    "volume": "main",
    "abstract": "Cognitive diagnosis assessment is a fundamental and crucial task for student learning. It models the student-exercise interaction, and discovers the students' proficiency levels on each knowledge attribute. In real-world intelligent education systems, generalization and interpretability of cognitive diagnosis methods are of equal importance. However, most existing methods can hardly make the best of both worlds due to the complicated student-exercise interaction. To this end, this paper proposes a symbolic cognitive diagnosis~(SCD) framework to simultaneously enhance generalization and interpretability. The SCD framework incorporates the symbolic tree to explicably represent the complicated student-exercise interaction function, and utilizes gradient-based optimization methods to effectively learn the student and exercise parameters. Meanwhile, the accompanying challenge is that we need to tunnel the discrete symbolic representation and continuous parameter optimization. To address this challenge, we propose to hybridly optimize the representation and parameters in an alternating manner. To fulfill SCD, it alternately learns the symbolic tree by derivative-free genetic programming and learns the student and exercise parameters via gradient-based Adam. The extensive experimental results on various real-world datasets show the superiority of SCD on both generalization and interpretability. The ablation study verifies the efficacy of each ingredient in SCD, and the case study explicitly showcases how the interpretable ability of SCD works",
    "checked": true,
    "id": "f279c004037d1edcbc0f5cc27a3dd471ab73a929",
    "semantic_title": "symbolic cognitive diagnosis via hybrid optimization for intelligent education systems",
    "citation_count": 2,
    "authors": [
      "Junhao Shen",
      "Hong Qian",
      "Wei Zhang",
      "Aimin Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29414": {
    "title": "BBScore: A Brownian Bridge Based Metric for Assessing Text Coherence",
    "volume": "main",
    "abstract": "Measuring the coherence of text is a vital aspect of evaluating the quality of written content. Recent advancements in neural coherence modeling have demonstrated their efficacy in capturing entity coreference and discourse relations, thereby enhancing coherence evaluation. However, many existing methods heavily depend on static embeddings or focus narrowly on nearby context, constraining their capacity to measure the overarching coherence of long texts. In this paper, we posit that coherent texts inherently manifest a sequential and cohesive interplay among sentences, effectively conveying the central theme, purpose, or standpoint. To explore this abstract relationship, we introduce the \"BB Score,\" a novel reference-free metric grounded in Brownian bridge theory for assessing text coherence. Our findings showcase that when synergized with a simple additional classification component, this metric attains a performance level comparable to state-of-the-art techniques on standard artificial discrimination tasks. We also establish in downstream tasks that this metric effectively differentiates between human-written documents and text generated by large language models within specific domains. Furthermore, we illustrate the efficacy of this approach in detecting written styles attributed to various large language models, underscoring its potential for generalizability. In summary, we present a novel Brownian bridge coherence metric capable of measuring both local and global text coherence, while circumventing the need for end-to-end model training. This flexibility allows for its application in various downstream tasks",
    "checked": true,
    "id": "ebec0233853564fc55ff651c6c53a98ac641ed13",
    "semantic_title": "bbscore: a brownian bridge based metric for assessing text coherence",
    "citation_count": 2,
    "authors": [
      "Zhecheng Sheng",
      "Tianhao Zhang",
      "Chen Jiang",
      "Dongyeop Kang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29415": {
    "title": "Building Variable-Sized Models via Learngene Pool",
    "volume": "main",
    "abstract": "Recently, Stitchable Neural Networks (SN-Net) is proposed to stitch some pre-trained networks for quickly building numerous networks with different complexity and performance trade-offs. In this way, the burdens of designing or training the variable-sized networks, which can be used in application scenarios with diverse resource constraints, are alleviated. However, SN-Net still faces a few challenges. 1) Stitching from multiple independently pre-trained anchors introduces high storage resource consumption. 2) SN-Net faces challenges to build smaller models for low resource constraints. 3). SN-Net uses an unlearned initialization method for stitch layers, limiting the final performance. To overcome these challenges, motivated by the recently proposed Learngene framework, we propose a novel method called Learngene Pool. Briefly, Learngene distills the critical knowledge from a large pre-trained model into a small part (termed as learngene) and then expands this small part into a few variable-sized models. In our proposed method, we distill one pre-trained large model into multiple small models whose network blocks are used as learngene instances to construct the learngene pool. Since only one large model is used, we do not need to store more large models as SN-Net and after distilling, smaller learngene instances can be created to build small models to satisfy low resource constraints. We also insert learnable transformation matrices between the instances to stitch them into variable-sized models to improve the performance of these models. Exhaustive experiments have been implemented and the results validate the effectiveness of the proposed Learngene Pool compared with SN-Net",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boyu Shi",
      "Shiyu Xia",
      "Xu Yang",
      "Haokun Chen",
      "Zhiqiang Kou",
      "Xin Geng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29416": {
    "title": "CLIP-Guided Federated Learning on Heterogeneity and Long-Tailed Data",
    "volume": "main",
    "abstract": "Federated learning (FL) provides a decentralized machine learning paradigm where a server collaborates with a group of clients to learn a global model without accessing the clients' data. User heterogeneity is a significant challenge for FL, which together with the class-distribution imbalance further enhances the difficulty of FL. Great progress has been made in large vision-language models, such as Contrastive Language-Image Pre-training (CLIP), which paves a new way for image classification and object recognition. Inspired by the success of CLIP on few-shot and zero-shot learning, we use CLIP to optimize the federated learning between server and client models under its vision-language supervision. It is promising to mitigate the user heterogeneity and class-distribution balance due to the powerful cross-modality representation and rich open-vocabulary prior knowledge. In this paper, we propose the CLIP-guided FL (CLIP2FL) method on heterogeneous and long-tailed data. In CLIP2FL, the knowledge of the off-the-shelf CLIP model is transferred to the client-server models, and a bridge is built between the client and server. Specifically, for client-side learning, knowledge distillation is conducted between client models and CLIP to improve the ability of client-side feature representation. For server-side learning, in order to mitigate the heterogeneity and class-distribution imbalance, we generate federated features to retrain the server model. A prototype contrastive learning with the supervision of the text encoder of CLIP is introduced to generate federated features depending on the client-side gradients, and they are used to retrain a balanced server classifier. Extensive experimental results on several benchmarks demonstrate that CLIP2FL achieves impressive performance and effectively deals with data heterogeneity and long-tail distribution. The code is available at https://github.com/shijiangming1/CLIP2FL",
    "checked": true,
    "id": "197483eda35e9ad841d76b4dff055aef8dd34ec4",
    "semantic_title": "clip-guided federated learning on heterogeneity and long-tailed data",
    "citation_count": 1,
    "authors": [
      "Jiangming Shi",
      "Shanshan Zheng",
      "Xiangbo Yin",
      "Yang Lu",
      "Yuan Xie",
      "Yanyun Qu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29417": {
    "title": "Structural Information Enhanced Graph Representation for Link Prediction",
    "volume": "main",
    "abstract": "Link prediction is a fundamental task of graph machine learning, and Graph Neural Network (GNN) based methods have become the mainstream approach due to their good performance. However, the typical practice learns node representations through neighborhood aggregation, lacking awareness of the structural relationships between target nodes. Recently, some methods have attempted to address this issue by node labeling tricks. However, they still rely on the node-centric neighborhood message passing of GNNs, which we believe involves two limitations in terms of information perception and transmission for link prediction. First, it cannot perceive long-range structural information due to the restricted receptive fields. Second, there may be information loss of node-centric model on link-centric task. In addition, we empirically find that the neighbor node features could introduce noise for link prediction. To address these issues, we propose a structural information enhanced link prediction framework, which involves removing the neighbor node features while fitting neighborhood graph structures more focused through GNN. Furthermore, we introduce Binary Structural Transformer (BST) to encode the structural relationships between target nodes, complementing the deficiency of GNN. Our approach achieves remarkable results on multiple popular benchmarks, including ranking first on ogbl-ppa, ogbl-citation2 and Pubmed",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Shi",
      "Bin Hu",
      "Deng Zhao",
      "Jianshan He",
      "Zhiqiang Zhang",
      "Jun Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29418": {
    "title": "A Closer Look at Curriculum Adversarial Training: From an Online Perspective",
    "volume": "main",
    "abstract": "Curriculum adversarial training empirically finds that gradually increasing the hardness of adversarial examples can further improve the adversarial robustness of the trained model compared to conventional adversarial training. However, theoretical understanding of this strategy remains limited. In an attempt to bridge this gap, we analyze the adversarial training process from an online perspective. Specifically, we treat adversarial examples in different iterations as samples from different adversarial distributions. We then introduce the time series prediction framework and deduce novel generalization error bounds. Our theoretical results not only demonstrate the effectiveness of the conventional adversarial training algorithm but also explain why curriculum adversarial training methods can further improve adversarial generalization. We conduct comprehensive experiments to support our theory",
    "checked": true,
    "id": "f5a524ee5b26fc3b1a5f0572bb654d71ec78aceb",
    "semantic_title": "a closer look at curriculum adversarial training: from an online perspective",
    "citation_count": 0,
    "authors": [
      "Lianghe Shi",
      "Weiwei Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29419": {
    "title": "Double-Bounded Optimal Transport for Advanced Clustering and Classification",
    "volume": "main",
    "abstract": "Optimal transport (OT) is attracting increasing attention in machine learning. It aims to transport a source distribution to a target one at minimal cost. In its vanilla form, the source and target distributions are predetermined, which contracts to the real-world case involving undetermined targets. In this paper, we propose Doubly Bounded Optimal Transport (DB-OT), which assumes that the target distribution is restricted within two boundaries instead of a fixed one, thus giving more freedom for the transport to find solutions. Based on the entropic regularization of DB-OT, three scaling-based algorithms are devised for calculating the optimal solution. We also show that our DB-OT is helpful for barycenter-based clustering, which can avoid the excessive concentration of samples in a single cluster. Then we further develop DB-OT techniques for long-tailed classification which is an emerging and open problem. We first propose a connection between OT and classification, that is, in the classification task, training involves optimizing the Inverse OT to learn the representations, while testing involves optimizing the OT for predictions. with this OT perspective, we first apply DB-OT to improve the loss, and the Balanced Softmax is shown as a special case. Then we apply DB-OT for inference in the testing process. Even with vanilla Softmax trained features, our experiments show that our method can achieve good results with our improved inference scheme in the testing stage",
    "checked": true,
    "id": "95412ecb51d00b0202929274819b8ada28ddb9ff",
    "semantic_title": "double-bounded optimal transport for advanced clustering and classification",
    "citation_count": 0,
    "authors": [
      "Liangliang Shi",
      "Zhaoqi Shen",
      "Junchi Yan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29420": {
    "title": "Teacher as a Lenient Expert: Teacher-Agnostic Data-Free Knowledge Distillation",
    "volume": "main",
    "abstract": "Data-free knowledge distillation (DFKD) aims to distill pretrained knowledge to a student model with the help of a generator without using original data. In such data-free scenarios, achieving stable performance of DFKD is essential due to the unavailability of validation data. Unfortunately, this paper has discovered that existing DFKD methods are quite sensitive to different teacher models, occasionally showing catastrophic failures of distillation, even when using well-trained teacher models. Our observation is that the generator in DFKD is not always guaranteed to produce precise yet diverse samples using the existing representative strategy of minimizing both class-prior and adversarial losses. Through our empirical study, we focus on the fact that class-prior not only decreases the diversity of generated samples, but also cannot completely address the problem of generating unexpectedly low-quality samples depending on teacher models. In this paper, we propose the teacher-agnostic data-free knowledge distillation (TA-DFKD) method, with the goal of more robust and stable performance regardless of teacher models. Our basic idea is to assign the teacher model a lenient expert role for evaluating samples, rather than a strict supervisor that enforces its class-prior on the generator. Specifically, we design a sample selection approach that takes only clean samples verified by the teacher model without imposing restrictions on the power of generating diverse samples. Through extensive experiments, we show that our method successfully achieves both robustness and training stability across various teacher models, while outperforming the existing DFKD methods",
    "checked": true,
    "id": "7550e647e544e02e74843f26a5e36b96c2d4fc7b",
    "semantic_title": "teacher as a lenient expert: teacher-agnostic data-free knowledge distillation",
    "citation_count": 1,
    "authors": [
      "Hyunjune Shin",
      "Dong-Wan Choi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29421": {
    "title": "SemTra: A Semantic Skill Translator for Cross-Domain Zero-Shot Policy Adaptation",
    "volume": "main",
    "abstract": "This work explores the zero-shot adaptation capability of semantic skills, semantically interpretable experts' behavior patterns, in cross-domain settings, where a user input in interleaved multi-modal snippets can prompt a new long-horizon task for different domains. In these cross-domain settings, we present a semantic skill translator framework SemTra which utilizes a set of multi-modal models to extract skills from the snippets, and leverages the reasoning capabilities of a pretrained language model to adapt these extracted skills to the target domain. The framework employs a two-level hierarchy for adaptation: task adaptation and skill adaptation. During task adaptation, seq-to-seq translation by the language model transforms the extracted skills into a semantic skill sequence, which is tailored to fit the cross-domain contexts. Skill adaptation focuses on optimizing each semantic skill for the target domain context, through parametric instantiations that are facilitated by language prompting and contrastive learning-based context inferences. This hierarchical adaptation empowers the framework to not only infer a complex task specification in one-shot from the interleaved multi-modal snippets, but also adapt it to new domains with zero-shot learning abilities. We evaluate our framework with Meta-World, Franka Kitchen, RLBench, and CARLA environments. The results clarify the framework's superiority in performing long-horizon tasks and adapting to different domains, showing its broad applicability in practical use cases, such as cognitive robots interpreting abstract instructions and autonomous vehicles operating under varied configurations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sangwoo Shin",
      "Minjong Yoo",
      "Jeongwoo Lee",
      "Honguk Woo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29422": {
    "title": "Region-Disentangled Diffusion Model for High-Fidelity PPG-to-ECG Translation",
    "volume": "main",
    "abstract": "The high prevalence of cardiovascular diseases (CVDs) calls for accessible and cost-effective continuous cardiac monitoring tools. Despite Electrocardiography (ECG) being the gold standard, continuous monitoring remains a challenge, leading to the exploration of Photoplethysmography (PPG), a promising but more basic alternative available in consumer wearables. This notion has recently spurred interest in translating PPG to ECG signals. In this work, we introduce Region-Disentangled Diffusion Model (RDDM), a novel diffusion model designed to capture the complex temporal dynamics of ECG. Traditional Diffusion models like Denoising Diffusion Probabilistic Models (DDPM) face challenges in capturing such nuances due to the indiscriminate noise addition process across the entire signal. Our proposed RDDM overcomes such limitations by incorporating a novel forward process that selectively adds noise to specific regions of interest (ROI) such as QRS complex in ECG signals, and a reverse process that disentangles the denoising of ROI and non-ROI regions. Quantitative experiments demonstrate that RDDM can generate high-fidelity ECG from PPG in as few as 10 diffusion steps, making it highly effective and computationally efficient. Additionally, to rigorously validate the usefulness of the generated ECG signals, we introduce CardioBench, a comprehensive evaluation benchmark for a variety of cardiac-related tasks including heart rate and blood pressure estimation, stress classification, and the detection of atrial fibrillation and diabetes. Our thorough experiments show that RDDM achieves state-of-the-art performance on CardioBench. To the best of our knowledge, RDDM is the first diffusion model for cross-modal signal-to-signal translation in the bio-signal domain",
    "checked": true,
    "id": "648ba63d01937913d115b60916060f1920bcd8a3",
    "semantic_title": "region-disentangled diffusion model for high-fidelity ppg-to-ecg translation",
    "citation_count": 1,
    "authors": [
      "Debaditya Shome",
      "Pritam Sarkar",
      "Ali Etemad"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29423": {
    "title": "Fusing Conditional Submodular GAN and Programmatic Weak Supervision",
    "volume": "main",
    "abstract": "Programmatic Weak Supervision (PWS) and generative models serve as crucial tools that enable researchers to maximize the utility of existing datasets without resorting to laborious data gathering and manual annotation processes. PWS uses various weak supervision techniques to estimate the underlying class labels of data, while generative models primarily concentrate on sampling from the underlying distribution of the given dataset. Although these methods have the potential to complement each other, they have mostly been studied independently. Recently, WSGAN proposed a mechanism to fuse these two models. Their approach utilizes the discrete latent factors of InfoGAN for the training of the label models and leverages the class-dependent information of the label models to generate images of specific classes. However, the disentangled latent factor learned by the InfoGAN may not necessarily be class specific and hence could potentially affect the label model's accuracy. Moreover, the prediction of the label model is often noisy in nature and can have a detrimental impact on the quality of images generated by GAN. In our work, we address these challenges by (i) implementing a noise-aware classifier using the pseudo labels generated by the label model, (ii) utilizing the prediction of the noise-aware classifier for training the label model as well as generation of class-conditioned images. Additionally, We also investigate the effect of training the classifier with a subset of the dataset within a defined uncertainty budget on pseudo labels. We accomplish this by formalizing the subset selection problem as submodular maximization with a knapsack constraint on the entropy of pseudo labels. We conduct experiments on multiple datasets and demonstrate the efficacy of our methods on several tasks vis-a-vis the current state-of-the-art methods. Our implementation is available at https://github.com/kyrs/subpws-gan",
    "checked": true,
    "id": "e518e446bf605870ff164e9b49cee5f3b586ab3e",
    "semantic_title": "fusing conditional submodular gan and programmatic weak supervision",
    "citation_count": 1,
    "authors": [
      "Kumar Shubham",
      "Pranav Sastry",
      "Prathosh AP"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29424": {
    "title": "Partial Label Learning with a Partner",
    "volume": "main",
    "abstract": "In partial label learning (PLL), each instance is associated with a set of candidate labels among which only one is ground-truth. The majority of the existing works focuses on constructing robust classifiers to estimate the labeling confidence of candidate labels in order to identify the correct one. However, these methods usually struggle to rectify mislabeled samples. To help existing PLL methods identify and rectify mislabeled samples, in this paper, we introduce a novel partner classifier and propose a novel ``mutual supervision'' paradigm. Specifically, we instantiate the partner classifier predicated on the implicit fact that non-candidate labels of a sample should not be assigned to it, which is inherently accurate and has not been fully investigated in PLL. Furthermore, a novel collaborative term is formulated to link the base classifier and the partner one. During each stage of mutual supervision, both classifiers will blur each other's predictions through a blurring mechanism to prevent overconfidence in a specific label. Extensive experiments demonstrate that the performance and disambiguation ability of several well-established stand-alone and deep-learning based PLL approaches can be significantly improved by coupling with this learning paradigm",
    "checked": true,
    "id": "94c626f1d15dd2c0bd8f521dcbdd0f27fa213dfa",
    "semantic_title": "partial label learning with a partner",
    "citation_count": 2,
    "authors": [
      "Chongjie Si",
      "Zekun Jiang",
      "Xuehui Wang",
      "Yan Wang",
      "Xiaokang Yang",
      "Wei Shen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29425": {
    "title": "Online Submodular Maximization via Online Convex Optimization",
    "volume": "main",
    "abstract": "We study monotone submodular maximization under general matroid constraints in the online setting. We prove that online optimization of a large class of submodular functions, namely, threshold potential functions, reduces to online convex optimization (OCO). This is precisely because functions in this class admit a concave relaxation; as a result, OCO policies, coupled with an appropriate rounding scheme, can be used to achieve sublinear regret in the combinatorial setting. We also show that our reduction extends to many different versions of the online learning problem, including the dynamic regret, bandit, and optimistic-learning settings",
    "checked": true,
    "id": "63f3702333b7bc92573d773f0f96fe8ee6be7102",
    "semantic_title": "online submodular maximization via online convex optimization",
    "citation_count": 1,
    "authors": [
      "Tareq Si Salem",
      "Gözde Özcan",
      "Iasonas Nikolaou",
      "Evimaria Terzi",
      "Stratis Ioannidis"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29426": {
    "title": "Robustly Train Normalizing Flows via KL Divergence Regularization",
    "volume": "main",
    "abstract": "In this paper, we find that the training of Normalizing Flows (NFs) are easily affected by the outliers and a small number (or high dimensionality) of training samples. To solve this problem, we propose a Kullback–Leibler (KL) divergence regularization on the Jacobian matrix of NFs. We prove that such regularization is equivalent to adding a set of samples whose covariance matrix is the identity matrix to the training set. Thus, it reduces the negative influence of the outliers and the small sample number on the estimation of the covariance matrix, simultaneously. Therefore, our regularization makes the training of NFs robust. Ultimately, we evaluate the performance of NFs on out-of-distribution (OoD) detection tasks. The excellent results obtained demonstrate the effectiveness of the proposed regularization term. For example, with the help of the proposed regularization, the OoD detection score increases at most 30% compared with the one without the regularization",
    "checked": true,
    "id": "d798f3520e3a6cb80a9589f7653bc3fac77ff652",
    "semantic_title": "robustly train normalizing flows via kl divergence regularization",
    "citation_count": 0,
    "authors": [
      "Kun Song",
      "Ruben Solozabal",
      "Hao Li",
      "Martin Takáč",
      "Lu Ren",
      "Fakhri Karray"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29427": {
    "title": "Non-exemplar Domain Incremental Object Detection via Learning Domain Bias",
    "volume": "main",
    "abstract": "Domain incremental object detection (DIOD) aims to gradually learn a unified object detection model from a dataset stream composed of different domains, achieving good performance in all encountered domains. The most critical obstacle to this goal is the catastrophic forgetting problem, where the performance of the model improves rapidly in new domains but deteriorates sharply in old ones after a few sessions. To address this problem, we propose a non-exemplar DIOD method named learning domain bias (LDB), which learns domain bias independently at each new session, avoiding saving examples from old domains. Concretely, a base model is first obtained through training during session 1. Then, LDB freezes the weights of the base model and trains individual domain bias for each new incoming domain, adapting the base model to the distribution of new domains. At test time, since the domain ID is unknown, we propose a domain selector based on nearest mean classifier (NMC), which selects the most appropriate domain bias for a test image. Extensive experimental evaluations on two series of datasets demonstrate the effectiveness of the proposed LDB method in achieving high accuracy on new and old domain datasets. The code is available at https://github.com/SONGX1997/LDB",
    "checked": true,
    "id": "9545c715d9ad7f417eeb2e9337d7204675a2aa30",
    "semantic_title": "non-exemplar domain incremental object detection via learning domain bias",
    "citation_count": 1,
    "authors": [
      "Xiang Song",
      "Yuhang He",
      "Songlin Dong",
      "Yihong Gong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29428": {
    "title": "Reinforcement Learning as a Parsimonious Alternative to Prediction Cascades: A Case Study on Image Segmentation",
    "volume": "main",
    "abstract": "Deep learning architectures have achieved state-of-the-art (SOTA) performance on computer vision tasks such as object detection and image segmentation. This may be attributed to the use of over-parameterized, monolithic deep learning architectures executed on large datasets. Although such large architectures lead to increased accuracy, this is usually accompanied by a larger increase in computation and memory requirements during inference. While this is a non-issue in traditional machine learning (ML) pipelines, the recent confluence of machine learning and fields like the Internet of Things (IoT) has rendered such large architectures infeasible for execution in low-resource settings. For some datasets, large monolithic pipelines may be overkill for simpler inputs. To address this problem, previous efforts have proposed decision cascades where inputs are passed through models of increasing complexity until desired performance is achieved. However, we argue that cascaded prediction leads to sub-optimal throughput and increased computational cost due to wasteful intermediate computations. To address this, we propose PaSeR (Parsimonious Segmentation with Reinforcement Learning) a non-cascading, cost-aware learning pipeline as an efficient alternative to cascaded decision architectures. Through experimental evaluation on both real-world and standard datasets, we demonstrate that PaSeR achieves better accuracy while minimizing computational cost relative to cascaded models. Further, we introduce a new metric IoU/GigaFlop to evaluate the balance between cost and performance. On the real-world task of battery material phase segmentation, PaSeR yields a minimum performance improvement of 174% on the IoU/GigaFlop metric with respect to baselines. We also demonstrate PaSeR's adaptability to complementary models trained on a noisy MNIST dataset, where it achieved a minimum performance improvement on IoU/GigaFlop of 13.4% over SOTA models. Code and data are available at github.com/scailab/paser",
    "checked": true,
    "id": "9253363b62453f74f6eeb1f8bff3477bf168dce2",
    "semantic_title": "reinforcement learning as a parsimonious alternative to prediction cascades: a case study on image segmentation",
    "citation_count": 0,
    "authors": [
      "Bharat Srikishan",
      "Anika Tabassum",
      "Srikanth Allu",
      "Ramakrishnan Kannan",
      "Nikhil Muralidhar"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29429": {
    "title": "United We Stand: Using Epoch-Wise Agreement of Ensembles to Combat Overfit",
    "volume": "main",
    "abstract": "Deep neural networks have become the method of choice for solving many classification tasks, largely because they can fit very complex functions defined over raw data. The downside of such powerful learners is the danger of overfit. In this paper, we introduce a novel ensemble classifier for deep networks that effectively overcomes overfitting by combining models generated at specific intermediate epochs during training. Our method allows for the incorporation of useful knowledge obtained by the models during the overfitting phase without deterioration of the general performance, which is usually missed when early stopping is used. To motivate this approach, we begin with the theoretical analysis of a regression model, whose prediction - that the variance among classifiers increases when overfit occurs - is demonstrated empirically in deep networks in common use. Guided by these results, we construct a new ensemble-based prediction method, where the prediction is determined by the class that attains the most consensual prediction throughout the training epochs. Using multiple image and text classification datasets, we show that when regular ensembles suffer from overfit, our method eliminates the harmful reduction in generalization due to overfit, and often even surpasses the performance obtained by early stopping. Our method is easy to implement and can be integrated with any training scheme and architecture, without additional prior knowledge beyond the training set. It is thus a practical and useful tool to overcome overfit",
    "checked": true,
    "id": "b8b6e8a79db7692833ebd3eb7a9ad5303b604be7",
    "semantic_title": "united we stand: using epoch-wise agreement of ensembles to combat overfit",
    "citation_count": 0,
    "authors": [
      "Uri Stern",
      "Daniel Shwartz",
      "Daphna Weinshall"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29430": {
    "title": "Multi-Dimensional Fair Federated Learning",
    "volume": "main",
    "abstract": "Federated learning (FL) has emerged as a promising collaborative and secure paradigm for training a model from decentralized data without compromising privacy. Group fairness and client fairness are two dimensions of fairness that are important for FL. Standard FL can result in disproportionate disadvantages for certain clients, and it still faces the challenge of treating different groups equitably in a population. The problem of privately training fair FL models without compromising the generalization capability of disadvantaged clients remains open. In this paper, we propose a method, called mFairFL, to address this problem and achieve group fairness and client fairness simultaneously. mFairFL leverages differential multipliers to construct an optimization objective for empirical risk minimization with fairness constraints. Before aggregating locally trained models, it first detects conflicts among their gradients, and then iteratively curates the direction and magnitude of gradients to mitigate these conflicts. Theoretical analysis proves mFairFL facilitates the fairness in model development. The experimental evaluations based on three benchmark datasets show significant advantages of mFairFL compared to seven state-of-the-art baselines",
    "checked": true,
    "id": "3f2a1972e5f7c6a3bf0e04466643ab2444a28120",
    "semantic_title": "multi-dimensional fair federated learning",
    "citation_count": 0,
    "authors": [
      "Cong Su",
      "Guoxian Yu",
      "Jun Wang",
      "Hui Li",
      "Qingzhong Li",
      "Han Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29431": {
    "title": "Sharpness-Aware Model-Agnostic Long-Tailed Domain Generalization",
    "volume": "main",
    "abstract": "Domain Generalization (DG) aims to improve the generalization ability of models trained on a specific group of source domains, enabling them to perform well on new, unseen target domains. Recent studies have shown that methods that converge to smooth optima can enhance the generalization performance of supervised learning tasks such as classification. In this study, we examine the impact of smoothness-enhancing formulations on domain adversarial training, which combines task loss and adversarial loss objectives. Our approach leverages the fact that converging to a smooth minimum with respect to task loss can stabilize the task loss and lead to better performance on unseen domains. Furthermore, we recognize that the distribution of objects in the real world often follows a long-tailed class distribution, resulting in a mismatch between machine learning models and our expectations of their performance on all classes of datasets with long-tailed class distributions. To address this issue, we consider the domain generalization problem from the perspective of the long-tail distribution and propose using the maximum square loss to balance different classes which can improve model generalizability. Our method's effectiveness is demonstrated through comparisons with state-of-the-art methods on various domain generalization datasets. Code: https://github.com/bamboosir920/SAMALTDG",
    "checked": true,
    "id": "88b8afb9003c710e8984a67ef22194eac9ee284d",
    "semantic_title": "sharpness-aware model-agnostic long-tailed domain generalization",
    "citation_count": 0,
    "authors": [
      "Houcheng Su",
      "Weihao Luo",
      "Daixian Liu",
      "Mengzhu Wang",
      "Jing Tang",
      "Junyang Chen",
      "Cong Wang",
      "Zhenghan Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29432": {
    "title": "Multiscale Attention Wavelet Neural Operator for Capturing Steep Trajectories in Biochemical Systems",
    "volume": "main",
    "abstract": "In biochemical modeling, some foundational systems can exhibit sudden and profound behavioral shifts, such as the cellular signaling pathway models, in which the physiological responses promptly react to environmental changes, resulting in steep changes in their dynamic model trajectories. These steep changes are one of the major challenges in biochemical modeling governed by nonlinear differential equations. One promising way to tackle this challenge is converting the input data from the time domain to the frequency domain through Fourier Neural Operators, which enhances the ability to analyze data periodicity and regularity. However, the effectiveness of these Fourier based methods diminishes in scenarios with complex abrupt switches. To address this limitation, an innovative Multiscale Attention Wavelet Neural Operator (MAWNO) method is proposed in this paper, which comprehensively combines the attention mechanism with the versatile wavelet transforms to effectively capture these abrupt switches. Specifically, the wavelet transform scrutinizes data across multiple scales to extract the characteristics of abrupt signals into wavelet coefficients, while the self-attention mechanism is adeptly introduced to enhance the wavelet coefficients in high-frequency signals that can better characterize the abrupt switches. Experimental results substantiate MAWNO's supremacy in terms of accuracy on three classical biochemical models featuring periodic and steep trajectories. https://github.com/SUDERS/MAWNO",
    "checked": true,
    "id": "b93eb74da4845805ef08081926bc92082747ed15",
    "semantic_title": "multiscale attention wavelet neural operator for capturing steep trajectories in biochemical systems",
    "citation_count": 0,
    "authors": [
      "Jiayang Su",
      "Junbo Ma",
      "Songyang Tong",
      "Enze Xu",
      "Minghan Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29433": {
    "title": "GSENet:Global Semantic Enhancement Network for Lane Detection",
    "volume": "main",
    "abstract": "Lane detection is the cornerstone of autonomous driving. Although existing methods have achieved promising results, there are still limitations in addressing challenging scenarios such as abnormal weather, occlusion, and curves. These scenarios with low visibility usually require to rely on the broad information of the entire scene provided by global semantics and local texture information to predict the precise position and shape of the lane lines. In this paper, we propose a Global Semantic Enhancement Network for lane detection, which involves a complete set of systems for feature extraction and global features transmission. Traditional methods for global feature extraction usually require deep convolution layer stacks. However, this approach of obtaining global features solely through a larger receptive field not only fails to capture precise global features but also leads to an overly deep model, which results in slow inference speed. To address these challenges, we propose a novel operation called the Global feature Extraction Module (GEM). Additionally, we introduce the Top Layer Auxiliary Module (TLAM) as a channel for feature distillation, which facilitates a bottom-up transmission of global features. Furthermore, we introduce two novel loss functions: the Angle Loss, which account for the angle between predicted and ground truth lanes, and the Generalized Line IoU Loss function that considers the scenarios where significant deviations occur between the prediction of lanes and ground truth in some harsh conditions. The experimental results reveal that the proposed method exhibits remarkable superiority over the current state-of-the-art techniques for lane detection.Our codes are available at:https://github.com/crystal250/GSENet",
    "checked": false,
    "id": "b0763d6367b70a874a8b6046551c7fd6a053ae4d",
    "semantic_title": "gsenet: global semantic enhancement network for lane detection",
    "citation_count": 1,
    "authors": [
      "Junhao Su",
      "Zhenghan Chen",
      "Chenghao He",
      "Dongzhi Guan",
      "Changpeng Cai",
      "Tongxi Zhou",
      "Jiashen Wei",
      "Wenhua Tian",
      "Zhihuai Xie"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29434": {
    "title": "Federated Adaptive Prompt Tuning for Multi-Domain Collaborative Learning",
    "volume": "main",
    "abstract": "Federated learning (FL) enables multiple clients to collaboratively train a global model without disclosing their data. Previous researches often require training the complete model parameters. However, the emergence of powerful pre-trained models makes it possible to achieve higher performance with fewer learnable parameters in FL. In this paper, we propose a federated adaptive prompt tuning algorithm, FedAPT, for multi-domain collaborative image classification with powerful foundation models, like CLIP. Compared with direct federated prompt tuning, our core idea is to adaptively unlock specific domain knowledge for each test sample in order to provide them with personalized prompts. To implement this idea, we design an adaptive prompt tuning module, which consists of a meta prompt, an adaptive network, and some keys. The server randomly generates a set of keys and assigns a unique key to each client. Then all clients cooperatively train the global adaptive network and meta prompt with the local datasets and the frozen keys. Ultimately, the global aggregation model can assign a personalized prompt to CLIP based on the domain features of each test sample. We perform extensive experiments on two multi-domain image classification datasets across two different settings -- supervised and unsupervised. The results show that FedAPT can achieve better performance with less than 10% of the number of parameters of the fully trained model, and the global model can perform well in diverse client domains simultaneously",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shangchao Su",
      "Mingzhao Yang",
      "Bin Li",
      "Xiangyang Xue"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29435": {
    "title": "Towards Real-World Test-Time Adaptation: Tri-net Self-Training with Balanced Normalization",
    "volume": "main",
    "abstract": "Test-Time Adaptation aims to adapt source domain model to testing data at inference stage with success demonstrated in adapting to unseen corruptions. However, these attempts may fail under more challenging real-world scenarios. Existing works mainly consider real-world test-time adaptation under non-i.i.d. data stream and continual domain shift. In this work, we first complement the existing real-world TTA protocol with a globally class imbalanced testing set. We demonstrate that combining all settings together poses new challenges to existing methods. We argue the failure of state-of-the-art methods is first caused by indiscriminately adapting normalization layers to imbalanced testing data. To remedy this shortcoming, we propose a balanced batchnorm layer to swap out the regular batchnorm at inference stage. The new batchnorm layer is capable of adapting without biasing towards majority classes. We are further inspired by the success of self-training (ST) in learning from unlabeled data and adapt ST for test-time adaptation. However, ST alone is prone to over adaption which is responsible for the poor performance under continual domain shift. Hence, we propose to improve self-training under continual domain shift by regularizing model updates with an anchored loss. The final TTA model, termed as TRIBE, is built upon a tri-net architecture with balanced batchnorm layers. We evaluate TRIBE on four datasets representing real-world TTA settings. TRIBE consistently achieves the state-of-the-art performance across multiple evaluation protocols. The code is available at https://github.com/Gorilla-Lab-SCUT/TRIBE",
    "checked": true,
    "id": "aef70e95752ec9948a0e4631dbd780dbd1f82224",
    "semantic_title": "towards real-world test-time adaptation: tri-net self-training with balanced normalization",
    "citation_count": 4,
    "authors": [
      "Yongyi Su",
      "Xun Xu",
      "Kui Jia"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29436": {
    "title": "Unraveling Batch Normalization for Realistic Test-Time Adaptation",
    "volume": "main",
    "abstract": "While recent test-time adaptations exhibit efficacy by adjusting batch normalization to narrow domain disparities, their effectiveness diminishes with realistic mini-batches due to inaccurate target estimation. As previous attempts merely introduce source statistics to mitigate this issue, the fundamental problem of inaccurate target estimation still persists, leaving the intrinsic test-time domain shifts unresolved. This paper delves into the problem of mini-batch degradation. By unraveling batch normalization, we discover that the inexact target statistics largely stem from the substantially reduced class diversity in batch. Drawing upon this insight, we introduce a straightforward tool, Test-time Exponential Moving Average (TEMA), to bridge the class diversity gap between training and testing batches. Importantly, our TEMA adaptively extends the scope of typical methods beyond the current batch to incorporate a diverse set of class information, which in turn boosts an accurate target estimation. Built upon this foundation, we further design a novel layer-wise rectification strategy to consistently promote test-time performance. Our proposed method enjoys a unique advantage as it requires neither training nor tuning parameters, offering a truly hassle-free solution. It significantly enhances model robustness against shifted domains and maintains resilience in diverse real-world scenarios with various batch sizes, achieving state-of-the-art performance on several major benchmarks. Code is available at https://github.com/kiwi12138/RealisticTTA",
    "checked": true,
    "id": "345b6dad228af53d2bef88d047d80bdd7f868032",
    "semantic_title": "unraveling batch normalization for realistic test-time adaptation",
    "citation_count": 0,
    "authors": [
      "Zixian Su",
      "Jingwei Guo",
      "Kai Yao",
      "Xi Yang",
      "Qiufeng Wang",
      "Kaizhu Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29437": {
    "title": "CUDC: A Curiosity-Driven Unsupervised Data Collection Method with Adaptive Temporal Distances for Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "Offline reinforcement learning (RL) aims to learn an effective policy from a pre-collected dataset. Most existing works are to develop sophisticated learning algorithms, with less emphasis on improving the data collection process. Moreover, it is even challenging to extend the single-task setting and collect a task-agnostic dataset that allows an agent to perform multiple downstream tasks. In this paper, we propose a Curiosity-driven Unsupervised Data Collection (CUDC) method to expand feature space using adaptive temporal distances for task-agnostic data collection and ultimately improve learning efficiency and capabilities for multi-task offline RL. To achieve this, CUDC estimates the probability of the k-step future states being reachable from the current states, and adapts how many steps into the future that the dynamics model should predict. With this adaptive reachability mechanism in place, the feature representation can be diversified, and the agent can navigate itself to collect higher-quality data with curiosity. Empirically, CUDC surpasses existing unsupervised methods in efficiency and learning performance in various downstream offline RL tasks of the DeepMind control suite",
    "checked": true,
    "id": "50aa1554c4e8b230784d7903e31116022984894c",
    "semantic_title": "cudc: a curiosity-driven unsupervised data collection method with adaptive temporal distances for offline reinforcement learning",
    "citation_count": 0,
    "authors": [
      "Chenyu Sun",
      "Hangwei Qian",
      "Chunyan Miao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29438": {
    "title": "T2MAC: Targeted and Trusted Multi-Agent Communication through Selective Engagement and Evidence-Driven Integration",
    "volume": "main",
    "abstract": "Communication stands as a potent mechanism to harmonize the behaviors of multiple agents. However, existing work primarily concentrates on broadcast communication, which not only lacks practicality, but also leads to information redundancy. This surplus, one-fits-all information could adversely impact the communication efficiency. Furthermore, existing works often resort to basic mechanisms to integrate observed and received information, impairing the learning process. To tackle these difficulties, we propose Targeted and Trusted Multi-Agent Communication (T2MAC), a straightforward yet effective method that enables agents to learn selective engagement and evidence-driven integration. With T2MAC, agents have the capability to craft individualized messages, pinpoint ideal communication windows, and engage with reliable partners, thereby refining communication efficiency. Following the reception of messages, the agents integrate information observed and received from different sources at an evidence level. This process enables agents to collectively use evidence garnered from multiple perspectives, fostering trusted and cooperative behaviors. We evaluate our method on a diverse set of cooperative multi-agent tasks, with varying difficulties, involving different scales and ranging from Hallway, MPE to SMAC. The experiments indicate that the proposed model not only surpasses the state-of-the-art methods in terms of cooperative performance and communication efficiency, but also exhibits impressive generalization",
    "checked": true,
    "id": "d13a4b9825c17763d01549f6432bc58ca109d606",
    "semantic_title": "t2mac: targeted and trusted multi-agent communication through selective engagement and evidence-driven integration",
    "citation_count": 0,
    "authors": [
      "Chuxiong Sun",
      "Zehua Zang",
      "Jiabao Li",
      "Jiangmeng Li",
      "Xiao Xu",
      "Rui Wang",
      "Changwen Zheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29439": {
    "title": "On the Role of Server Momentum in Federated Learning",
    "volume": "main",
    "abstract": "Federated Averaging (FedAvg) is known to experience convergence issues when encountering significant clients system heterogeneity and data heterogeneity. Server momentum has been proposed as an effective mitigation. However, existing server momentum works are restrictive in the momentum formulation, do not properly schedule hyperparameters and focus only on system homogeneous settings, which leaves the role of server momentum still an under-explored problem. In this paper, we propose a general framework for server momentum, that (a) covers a large class of momentum schemes that are unexplored in federated learning (FL), (b) enables a popular stagewise hyperparameter scheduler, (c) allows heterogeneous and asynchronous local computing. We provide rigorous convergence analysis for the proposed framework. To our best knowledge, this is the first work that thoroughly analyzes the performances of server momentum with a hyperparameter scheduler and system heterogeneity. Extensive experiments validate the effectiveness of our proposed framework. Due to page limit, we leave all proofs to the full version https://arxiv.org/abs/2312.12670",
    "checked": true,
    "id": "1c0bc03ce81f1c167e3895f632a9e0eb47c17637",
    "semantic_title": "on the role of server momentum in federated learning",
    "citation_count": 3,
    "authors": [
      "Jianhui Sun",
      "Xidong Wu",
      "Heng Huang",
      "Aidong Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29440": {
    "title": "RedCore: Relative Advantage Aware Cross-Modal Representation Learning for Missing Modalities with Imbalanced Missing Rates",
    "volume": "main",
    "abstract": "Multimodal learning is susceptible to modality missing, which poses a major obstacle for its practical applications and, thus, invigorates increasing research interest. In this paper, we investigate two challenging problems: 1) when modality missing exists in the training data, how to exploit the incomplete samples while guaranteeing that they are properly supervised? 2) when the missing rates of different modalities vary, causing or exacerbating the imbalance among modalities, how to address the imbalance and ensure all modalities are well-trained. To tackle these two challenges, we first introduce the variational information bottleneck (VIB) method for the cross-modal representation learning of missing modalities, which capitalizes on the available modalities and the labels as supervision. Then, accounting for the imbalanced missing rates, we define relative advantage to quantify the advantage of each modality over others. Accordingly, a bi-level optimization problem is formulated to adaptively regulate the supervision of all modalities during training. As a whole, the proposed approach features Relative advantage aware Cross-modal representation learning (abbreviated as RedCore) for missing modalities with imbalanced missing rates. Extensive empirical results demonstrate that RedCore outperforms competing models in that it exhibits superior robustness against either large or imbalanced missing rates. The code is available at: https://github.com/sunjunaimer/RedCore",
    "checked": true,
    "id": "a3e2e28db522356c4c98f9e6a7a5657a2e0f4f46",
    "semantic_title": "redcore: relative advantage aware cross-modal representation learning for missing modalities with imbalanced missing rates",
    "citation_count": 0,
    "authors": [
      "Jun Sun",
      "Xinxin Zhang",
      "Shoukang Han",
      "Yu-Ping Ruan",
      "Taihao Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29441": {
    "title": "Dual Self-Paced Cross-Modal Hashing",
    "volume": "main",
    "abstract": "Cross-modal hashing~(CMH) is an efficient technique to retrieve relevant data across different modalities, such as images, texts, and videos, which has attracted more and more attention due to its low storage cost and fast query speed. Although existing CMH methods achieve remarkable processes, almost all of them treat all samples of varying difficulty levels without discrimination, thus leaving them vulnerable to noise or outliers. Based on this observation, we reveal and study dual difficulty levels implied in cross-modal hashing learning, \\ie instance-level and feature-level difficulty. To address this problem, we propose a novel Dual Self-Paced Cross-Modal Hashing (DSCMH) that mimics human cognitive learning to learn hashing from ``easy'' to ``hard'' in both instance and feature levels, thereby embracing robustness against noise/outliers. Specifically, our DSCMH assigns weights to each instance and feature to measure their difficulty or reliability, and then uses these weights to automatically filter out the noisy and irrelevant data points in the original space. By gradually increasing the weights during training, our method can focus on more instances and features from ``easy'' to ``hard'' in training, thus mitigating the adverse effects of noise or outliers. Extensive experiments are conducted on three widely-used benchmark datasets to demonstrate the effectiveness and robustness of the proposed DSCMH over 12 state-of-the-art CMH methods",
    "checked": true,
    "id": "074b1d7a36323c1109e1a3c8037bfa5a6f19e512",
    "semantic_title": "dual self-paced cross-modal hashing",
    "citation_count": 0,
    "authors": [
      "Yuan Sun",
      "Jian Dai",
      "Zhenwen Ren",
      "Yingke Chen",
      "Dezhong Peng",
      "Peng Hu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29442": {
    "title": "ACAMDA: Improving Data Efficiency in Reinforcement Learning through Guided Counterfactual Data Augmentation",
    "volume": "main",
    "abstract": "Data augmentation plays a crucial role in improving the data efficiency of reinforcement learning (RL). However, the generation of high-quality augmented data remains a significant challenge. To overcome this, we introduce ACAMDA (Adversarial Causal Modeling for Data Augmentation), a novel framework that integrates two causality-based tasks: causal structure recovery and counterfactual estimation. The unique aspect of ACAMDA lies in its ability to recover temporal causal relationships from limited non-expert datasets. The identification of the sequential cause-and-effect allows the creation of realistic yet unobserved scenarios. We utilize this characteristic to generate guided counterfactual datasets, which, in turn, substantially reduces the need for extensive data collection. By simulating various state-action pairs under hypothetical actions, ACAMDA enriches the training dataset for diverse and heterogeneous conditions. Our experimental evaluation shows that ACAMDA outperforms existing methods, particularly when applied to novel and unseen domains",
    "checked": true,
    "id": "9bf5b5be1c16241aff6a0f1be7446458e41ac23e",
    "semantic_title": "acamda: improving data efficiency in reinforcement learning through guided counterfactual data augmentation",
    "citation_count": 1,
    "authors": [
      "Yuewen Sun",
      "Erli Wang",
      "Biwei Huang",
      "Chaochao Lu",
      "Lu Feng",
      "Changyin Sun",
      "Kun Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29443": {
    "title": "Learning Not to Regret",
    "volume": "main",
    "abstract": "The literature on game-theoretic equilibrium finding predominantly focuses on single games or their repeated play. Nevertheless, numerous real-world scenarios feature playing a game sampled from a distribution of similar, but not identical games, such as playing poker with different public cards or trading correlated assets on the stock market. As these similar games feature similar equilibra, we investigate a way to accelerate equilibrium finding on such a distribution. We present a novel ``learning not to regret'' framework, enabling us to meta-learn a regret minimizer tailored to a specific distribution. Our key contribution, Neural Predictive Regret Matching, is uniquely meta-learned to converge rapidly for the chosen distribution of games, while having regret minimization guarantees on any game. We validated our algorithms' faster convergence on a distribution of river poker games. Our experiments show that the meta-learned algorithms outpace their non-meta-learned counterparts, achieving more than tenfold improvements",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Sychrovský",
      "Michal Šustr",
      "Elnaz Davoodi",
      "Michael Bowling",
      "Marc Lanctot",
      "Martin Schmid"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29444": {
    "title": "Optimal Transport with Cyclic Symmetry",
    "volume": "main",
    "abstract": "We propose novel fast algorithms for optimal transport (OT) utilizing a cyclic symmetry structure of input data. Such OT with cyclic symmetry appears universally in various real-world examples: image processing, urban planning, and graph processing. Our main idea is to reduce OT to a small optimization problem that has significantly fewer variables by utilizing cyclic symmetry and various optimization techniques. On the basis of this reduction, our algorithms solve the small optimization problem instead of the original OT. As a result, our algorithms obtain the optimal solution and the objective function value of the original OT faster than solving the original OT directly. In this paper, our focus is on two crucial OT formulations: the linear programming OT (LOT) and the strongly convex-regularized OT, which includes the well-known entropy-regularized OT (EROT). Experiments show the effectiveness of our algorithms for LOT and EROT in synthetic/real-world data that has a strict/approximate cyclic symmetry structure. Through theoretical and experimental results, this paper successfully introduces the concept of symmetry into the OT research field for the first time",
    "checked": true,
    "id": "ad5fe4710c788ecc3dfa0b6bacac07dda0ba9197",
    "semantic_title": "optimal transport with cyclic symmetry",
    "citation_count": 0,
    "authors": [
      "Shoichiro Takeda",
      "Yasunori Akagi",
      "Naoki Marumo",
      "Kenta Niwa"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29445": {
    "title": "Cross-Gate MLP with Protein Complex Invariant Embedding Is a One-Shot Antibody Designer",
    "volume": "main",
    "abstract": "Antibodies are crucial proteins produced by the immune system in response to foreign substances or antigens. The specificity of an antibody is determined by its complementarity-determining regions (CDRs), which are located in the variable domains of the antibody chains and form the antigen-binding site. Previous studies have utilized complex techniques to generate CDRs, but they suffer from inadequate geometric modeling. Moreover, the common iterative refinement strategies lead to an inefficient inference. In this paper, we propose a simple yet effective model that can co-design 1D sequences and 3D structures of CDRs in a one-shot manner. To achieve this, we decouple the antibody CDR design problem into two stages: (i) geometric modeling of protein complex structures and (ii) sequence-structure co-learning. We develop a novel macromolecular structure invariant embedding, typically for protein complexes, that captures both intra- and inter-component interactions among the backbone atoms, including Calpha, N, C, and O atoms, to achieve comprehensive geometric modeling. Then, we introduce a simple cross-gate MLP for sequence-structure co-learning, allowing sequence and structure representations to implicitly refine each other. This enables our model to design desired sequences and structures in a one-shot manner. Extensive experiments are conducted to evaluate our results at both the sequence and structure level, which demonstrate that our model achieves superior performance compared to the state-of-the-art antibody CDR design methods",
    "checked": true,
    "id": "ab2549ec1c36ec876cfdd2bdffcadfe5b4fa6202",
    "semantic_title": "cross-gate mlp with protein complex invariant embedding is a one-shot antibody designer",
    "citation_count": 10,
    "authors": [
      "Cheng Tan",
      "Zhangyang Gao",
      "Lirong Wu",
      "Jun Xia",
      "Jiangbin Zheng",
      "Xihong Yang",
      "Yue Liu",
      "Bozhen Hu",
      "Stan Z. Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29446": {
    "title": "FedCompetitors: Harmonious Collaboration in Federated Learning with Competing Participants",
    "volume": "main",
    "abstract": "Federated learning (FL) provides a privacy-preserving approach for collaborative training of machine learning models. Given the potential data heterogeneity, it is crucial to select appropriate collaborators for each FL participant (FL-PT) based on data complementarity. Recent studies have addressed this challenge. Similarly, it is imperative to consider the inter-individual relationships among FL-PTs where some FL-PTs engage in competition. Although FL literature has acknowledged the significance of this scenario, practical methods for establishing FL ecosystems remain largely unexplored. In this paper, we extend a principle from the balance theory, namely \"the friend of my enemy is my enemy\", to ensure the absence of conflicting interests within an FL ecosystem. The extended principle and the resulting problem are formulated via graph theory and integer linear programming. A polynomial-time algorithm is proposed to determine the collaborators of each FL-PT. The solution guarantees high scalability, allowing even competing FL-PTs to smoothly join the ecosystem without conflict of interest. The proposed framework jointly considers competition and data heterogeneity. Extensive experiments on real-world and synthetic data demonstrate its efficacy compared to five alternative approaches, and its ability to establish efficient collaboration networks among FL-PTs",
    "checked": true,
    "id": "28b9c624492b3c72ba63ea82c7833e450322acc9",
    "semantic_title": "fedcompetitors: harmonious collaboration in federated learning with competing participants",
    "citation_count": 0,
    "authors": [
      "Shanli Tan",
      "Hao Cheng",
      "Xiaohu Wu",
      "Han Yu",
      "Tiantian He",
      "Yew Soon Ong",
      "Chongjun Wang",
      "Xiaofeng Tao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29447": {
    "title": "Harnessing the Power of Beta Scoring in Deep Active Learning for Multi-Label Text Classification",
    "volume": "main",
    "abstract": "Within the scope of natural language processing, the domain of multi-label text classification is uniquely challenging due to its expansive and uneven label distribution. The complexity deepens due to the demand for an extensive set of annotated data for training an advanced deep learning model, especially in specialized fields where the labeling task can be labor-intensive and often requires domain-specific knowledge. Addressing these challenges, our study introduces a novel deep active learning strategy, capitalizing on the Beta family of proper scoring rules within the Expected Loss Reduction framework. It computes the expected increase in scores using the Beta Scoring Rules, which are then transformed into sample vector representations. These vector representations guide the diverse selection of informative sample, directly linking this process to the model's expected proper score. Comprehensive evaluations across both synthetic and real datasets reveal our method's capability to often outperform established acquisition techniques in multi-label text classification, presenting encouraging outcomes across various architectural and dataset scenarios",
    "checked": true,
    "id": "74d8f1c6bb30e7dbbf83a1aa4d2736dcceecec07",
    "semantic_title": "harnessing the power of beta scoring in deep active learning for multi-label text classification",
    "citation_count": 0,
    "authors": [
      "Wei Tan",
      "Ngoc Dang Nguyen",
      "Lan Du",
      "Wray Buntine"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29448": {
    "title": "A Two-Stage Information Extraction Network for Incomplete Multi-View Multi-Label Classification",
    "volume": "main",
    "abstract": "Recently, multi-view multi-label classification (MvMLC) has received a significant amount of research interest and many methods have been proposed based on the assumptions of view completion and label completion. However, in real-world scenarios, multi-view multi-label data tends to be incomplete due to various uncertainties involved in data collection and manual annotation. As a result, the conventional MvMLC methods fail. In this paper, we propose a new two-stage MvMLC network to solve this incomplete MvMLC issue with partial missing views and missing labels. Different from the existing works, our method attempts to leverage the diverse information from the partially missing data based on the information theory. Specifically, our method aims to minimize task-irrelevant information while maximizing task-relevant information through the principles of information bottleneck theory and mutual information extraction. The first stage of our network involves training view-specific classifiers to concentrate the task-relevant information. Subsequently, in the second stage, the hidden states of these classifiers serve as input for an alignment model, an autoencoder-based mutual information extraction framework, and a weighted fusion classifier to make the final prediction. Extensive experiments performed on five datasets validate that our method outperforms other state-of-the-art methods. Code is available at https://github.com/KevinTan10/TSIEN",
    "checked": true,
    "id": "c69874729eabdd82c0067c48646bd009a5cbe8de",
    "semantic_title": "a two-stage information extraction network for incomplete multi-view multi-label classification",
    "citation_count": 2,
    "authors": [
      "Xin Tan",
      "Ce Zhao",
      "Chengliang Liu",
      "Jie Wen",
      "Zhanyan Tang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29449": {
    "title": "An Effective Augmented Lagrangian Method for Fine-Grained Multi-View Optimization",
    "volume": "main",
    "abstract": "The significance of multi-view learning in effectively mitigating the intricate intricacies entrenched within heterogeneous data has garnered substantial attention in recent years. Notwithstanding the favorable achievements showcased by recent strides in this area, a confluence of noteworthy challenges endures. To be specific, a majority of extant methodologies unceremoniously assign weights to data points view-wisely. This ineluctably disregards the intrinsic reality that disparate views confer diverse contributions to each individual sample, consequently neglecting the rich wellspring of sample-level structural insights harbored within the dataset. In this paper, we proposed an effective Augmented Lagrangian MethOd for fiNe-graineD (ALMOND) multi-view optimization. This innovative approach scrutinizes the interplay among multiple views at the granularity of individual samples, thereby fostering the enhanced preservation of local structural coherence. The Augmented Lagrangian Method (ALM) is elaborately incorporated into our framework, which enables us to achieve an optimal solution without involving an inexplicable intermediate variable as previous methods do. Empirical experiments on multi-view clustering tasks across heterogeneous datasets serve to incontrovertibly showcase the effectiveness of our proposed methodology, corroborating its preeminence over incumbent state-of-the-art alternatives",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuze Tan",
      "Hecheng Cai",
      "Shudong Huang",
      "Shuping Wei",
      "Fan Yang",
      "Jiancheng Lv"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29450": {
    "title": "SimCalib: Graph Neural Network Calibration Based on Similarity between Nodes",
    "volume": "main",
    "abstract": "Graph neural networks (GNNs) have exhibited impressive performance in modeling graph data as exemplified in various applications. Recently, the GNN calibration problem has attracted increasing attention, especially in cost-sensitive scenarios. Previous work has gained empirical insights on the issue, and devised effective approaches for it, but theoretical supports still fall short. In this work, we shed light on the relationship between GNN calibration and nodewise similarity via theoretical analysis. A novel calibration framework, named SimCalib, is accordingly proposed to consider similarity between nodes at global and local levels. At the global level, the Mahalanobis distance between the current node and class prototypes is integrated to implicitly consider similarity between the current node and all nodes in the same class. At the local level, the similarity of node representation movement dynamics, quantified by nodewise homophily and relative degree, is considered. Informed about the application of nodewise movement patterns in analyzing nodewise behavior on the over-smoothing problem, we empirically present a possible relationship between over-smoothing and GNN calibration problem. Experimentally, we discover a correlation between nodewise similarity and model calibration improvement, in alignment with our theoretical results. Additionally, we conduct extensive experiments investigating different design factors and demonstrate the effectiveness of our proposed SimCalib framework for GNN calibration by achieving state-of-the-art performance on 14 out of 16 benchmarks",
    "checked": true,
    "id": "14ddef2aa451d5aa62ba8c75c056517c029bebc3",
    "semantic_title": "simcalib: graph neural network calibration based on similarity between nodes",
    "citation_count": 1,
    "authors": [
      "Boshi Tang",
      "Zhiyong Wu",
      "Xixin Wu",
      "Qiaochu Huang",
      "Jun Chen",
      "Shun Lei",
      "Helen Meng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29451": {
    "title": "DP-AdamBC: Your DP-Adam Is Actually DP-SGD (Unless You Apply Bias Correction)",
    "volume": "main",
    "abstract": "The Adam optimizer is a popular choice in contemporary deep learning due to its strong empirical performance. However we observe that in privacy sensitive scenarios, the traditional use of Differential Privacy (DP) with the Adam optimizer leads to sub-optimal performance on several tasks. We find that this performance degradation is due to a DP bias in Adam's second moment estimator, introduced by the addition of independent noise in the gradient computation to enforce DP guarantees. This DP bias leads to a different scaling for low variance parameter updates, that is inconsistent with the behavior of non-private Adam, and Adam's sign descent interpretation. We propose the DP-AdamBC optimization algorithm, which corrects for the bias in the second moment estimation and retrieves the expected behaviour of Adam. Empirically, DP-AdamBC significantly improves the optimization performance of DP-Adam by up to 3.5% in final accuracy in image, text, and graph node classification tasks",
    "checked": true,
    "id": "c44aeefe635f00089d10072ce30eafa862072bf1",
    "semantic_title": "dp-adambc: your dp-adam is actually dp-sgd (unless you apply bias correction)",
    "citation_count": 4,
    "authors": [
      "Qiaoyue Tang",
      "Frederick Shpilevskiy",
      "Mathias Lécuyer"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29452": {
    "title": "Non-monotone Sequential Submodular Maximization",
    "volume": "main",
    "abstract": "In this paper, we study a fundamental problem in submodular optimization known as sequential submodular maximization. The primary objective of this problem is to select and rank a sequence of items to optimize a group of submodular functions. The existing research on this problem has predominantly concentrated on the monotone setting, assuming that the submodular functions are non-decreasing. However, in various real-world scenarios, like diversity-aware recommendation systems, adding items to an existing set might negatively impact the overall utility. In response, we propose to study this problem with non-monotone submodular functions and develop approximation algorithms for both flexible and fixed length constraints, as well as a special case with identical utility functions. The empirical evaluations further validate the effectiveness of our proposed algorithms in the domain of video recommendations",
    "checked": true,
    "id": "fd134bf283cad4d1decad0cb41a4b76980535a8e",
    "semantic_title": "non-monotone sequential submodular maximization",
    "citation_count": 0,
    "authors": [
      "Shaojie Tang",
      "Jing Yuan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29453": {
    "title": "Comprehensive View Embedding Learning for Single-Cell Multimodal Integration",
    "volume": "main",
    "abstract": "Motivation: Advances in single-cell measurement techniques provide rich multimodal data, which helps us to explore the life state of cells more deeply. However, multimodal integration, or, learning joint embeddings from multimodal data remains a current challenge. The difficulty in integrating unpaired single-cell multimodal data is that different modalities have different feature spaces, which easily leads to information loss in joint embedding. And few existing methods have fully exploited and fused the information in single-cell multimodal data. Result: In this study, we propose CoVEL, a deep learning method for unsupervised integration of single-cell multimodal data. CoVEL learns single-cell representations from a comprehensive view, including regulatory relationships between modalities, fine-grained representations of cells, and relationships between different cells. The comprehensive view embedding enables CoVEL to remove the gap between modalities while protecting biological heterogeneity. Experimental results on multiple public datasets show that CoVEL is accurate and robust to single-cell multimodal integration. Data availability: https://github.com/shapsider/scintegration",
    "checked": true,
    "id": "29764f8c1848569cb6d4c382222d2d2a16404200",
    "semantic_title": "comprehensive view embedding learning for single-cell multimodal integration",
    "citation_count": 1,
    "authors": [
      "Zhenchao Tang",
      "Jiehui Huang",
      "Guanxing Chen",
      "Calvin Yu-Chian Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29454": {
    "title": "z-SignFedAvg: A Unified Stochastic Sign-Based Compression for Federated Learning",
    "volume": "main",
    "abstract": "Federated Learning (FL) is a promising privacy-preserving distributed learning paradigm but suffers from high communi- cation cost when training large-scale machine learning models. Sign-based methods, such as SignSGD, have been proposed as a biased gradient compression technique for reducing the communication cost. However, sign-based algorithms could diverge under heterogeneous data, which thus motivated the de- velopment of advanced techniques, such as the error-feedback method and stochastic sign-based compression, to fix this issue. Nevertheless, these methods still suffer from slower convergence rates, and none of them allows multiple local SGD updates like FedAvg. In this paper, we propose a novel noisy perturbation scheme with a general symmetric noise distribution for sign-based compression, which not only al- lows one to flexibly control the bias-variance tradeoff for the compressed gradient, but also provides a unified viewpoint to existing stochastic sign-based methods. More importantly, the proposed scheme enables the development of the very first sign-based FedAvg algorithm (z-SignFedAvg) to accelerate the convergence. Theoretically, we show that z-SignFedAvg achieves a faster convergence rate than existing sign-based methods and, under the uniformly distributed noise, can enjoy the same convergence rate as its uncompressed counterpart. Extensive experiments are conducted to demonstrate that the z-SignFedAvg can achieve competitive empirical performance on real datasets and outperforms existing schemes",
    "checked": true,
    "id": "29660f98f83d3c3c17c9e08e423ca76db58c0dbd",
    "semantic_title": "z-signfedavg: a unified stochastic sign-based compression for federated learning",
    "citation_count": 6,
    "authors": [
      "Zhiwei Tang",
      "Yanmeng Wang",
      "Tsung-Hui Chang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29455": {
    "title": "Deciphering Raw Data in Neuro-Symbolic Learning with Provable Guarantees",
    "volume": "main",
    "abstract": "Neuro-symbolic hybrid systems are promising for integrating machine learning and symbolic reasoning, where perception models are facilitated with information inferred from a symbolic knowledge base through logical reasoning. Despite empirical evidence showing the ability of hybrid systems to learn accurate perception models, the theoretical understanding of learnability is still lacking. Hence, it remains unclear why a hybrid system succeeds for a specific task and when it may fail given a different knowledge base. In this paper, we introduce a novel way of characterising supervision signals from a knowledge base, and establish a criterion for determining the knowledge's efficacy in facilitating successful learning. This, for the first time, allows us to address the two questions above by inspecting the knowledge base under investigation. Our analysis suggests that many knowledge bases satisfy the criterion, thus enabling effective learning, while some fail to satisfy it, indicating potential failures. Comprehensive experiments confirm the utility of our criterion on benchmark tasks",
    "checked": true,
    "id": "a20b03d0e9435ffcf007a1aa07f2da6d4eb8cb51",
    "semantic_title": "deciphering raw data in neuro-symbolic learning with provable guarantees",
    "citation_count": 0,
    "authors": [
      "Lue Tao",
      "Yu-Xuan Huang",
      "Wang-Zhou Dai",
      "Yuan Jiang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29456": {
    "title": "Efficient Nonparametric Tensor Decomposition for Binary and Count Data",
    "volume": "main",
    "abstract": "In numerous applications, binary reactions or event counts are observed and stored within high-order tensors. Tensor decompositions (TDs) serve as a powerful tool to handle such high-dimensional and sparse data. However, many traditional TDs are explicitly or implicitly designed based on the Gaussian distribution, which is unsuitable for discrete data. Moreover, most TDs rely on predefined multi-linear structures, such as CP and Tucker formats. Therefore, they may not be effective enough to handle complex real-world datasets. To address these issues, we propose ENTED, an Efficient Nonparametric TEnsor Decomposition for binary and count tensors. Specifically, we first employ a nonparametric Gaussian process (GP) to replace traditional multi-linear structures. Next, we utilize the Pólya-Gamma augmentation which provides a unified framework to establish conjugate models for binary and count distributions. Finally, to address the computational issue of GPs, we enhance the model by incorporating sparse orthogonal variational inference of inducing points, which offers a more effective covariance approximation within GPs and stochastic natural gradient updates for nonparametric models. We evaluate our model on several real-world tensor completion tasks, considering binary and count datasets. The results manifest both better performance and computational advantages of the proposed model",
    "checked": true,
    "id": "ecb24c73d53042663d55ff8f08cea00aefb8972f",
    "semantic_title": "efficient nonparametric tensor decomposition for binary and count data",
    "citation_count": 0,
    "authors": [
      "Zerui Tao",
      "Toshihisa Tanaka",
      "Qibin Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29457": {
    "title": "FFT-Based Dynamic Token Mixer for Vision",
    "volume": "main",
    "abstract": "Multi-head-self-attention (MHSA)-equipped models have achieved notable performance in computer vision. Their computational complexity is proportional to quadratic numbers of pixels in input feature maps, resulting in slow processing, especially when dealing with high-resolution images. New types of token-mixer are proposed as an alternative to MHSA to circumvent this problem: an FFT-based token-mixer involves global operations similar to MHSA but with lower computational complexity. However, despite its attractive properties, the FFT-based token-mixer has not been carefully examined in terms of its compatibility with the rapidly evolving MetaFormer architecture. Here, we propose a novel token-mixer called Dynamic Filter and novel image recognition models, DFFormer and CDFFormer, to close the gaps above. The results of image classification and downstream tasks, analysis, and visualization show that our models are helpful. Notably, their throughput and memory efficiency when dealing with high-resolution image recognition is remarkable. Our results indicate that Dynamic Filter is one of the token-mixer options that should be seriously considered. The code is available at https://github.com/okojoalg/dfformer",
    "checked": true,
    "id": "54b330de40291611da2188c2ebce1ab2c0725348",
    "semantic_title": "fft-based dynamic token mixer for vision",
    "citation_count": 1,
    "authors": [
      "Yuki Tatsunami",
      "Masato Taki"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29458": {
    "title": "An Information-Flow Perspective on Algorithmic Fairness",
    "volume": "main",
    "abstract": "This work presents insights gained by investigating the relationship between algorithmic fairness and the concept of secure information flow. The problem of enforcing secure information flow is well-studied in the context of information security: If secret information may \"flow\" through an algorithm or program in such a way that it can influence the program's output, then that is considered insecure information flow as attackers could potentially observe (parts of) the secret. There is a strong correspondence between secure information flow and algorithmic fairness: if protected attributes such as race, gender, or age are treated as secret program inputs, then secure information flow means that these \"secret\" attributes cannot influence the result of a program. While most research in algorithmic fairness evaluation concentrates on studying the impact of algorithms (often treating the algorithm as a black-box), the concepts derived from information flow can be used both for the analysis of disparate treatment as well as disparate impact w.r.t. a structural causal model. In this paper, we examine the relationship between quantitative as well as qualitative information-flow properties and fairness. Moreover, based on this duality, we derive a new quantitative notion of fairness called fairness spread, which can be easily analyzed using quantitative information flow and which strongly relates to counterfactual fairness. We demonstrate that off-the-shelf tools for information-flow properties can be used in order to formally analyze a program's algorithmic fairness properties, including the new notion of fairness spread as well as established notions such as demographic parity",
    "checked": true,
    "id": "df0105b4fa20a678bb8fe7c99da5bd354b4b45ad",
    "semantic_title": "an information-flow perspective on algorithmic fairness",
    "citation_count": 0,
    "authors": [
      "Samuel Teuber",
      "Bernhard Beckert"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29459": {
    "title": "Amalgamating Multi-Task Models with Heterogeneous Architectures",
    "volume": "main",
    "abstract": "Multi-task learning (MTL) is essential for real-world applications that handle multiple tasks simultaneously, such as selfdriving cars. MTL methods improve the performance of all tasks by utilizing information across tasks to learn a robust shared representation. However, acquiring sufficient labeled data tends to be extremely expensive, especially when having to support many tasks. Recently, Knowledge Amalgamation (KA) has emerged as an effective strategy for addressing the lack of labels by instead learning directly from pretrained models (teachers). KA learns one unified multi-task student that masters all tasks across all teachers. Existing KA for MTL works are limited to teachers with identical architectures, and thus propose layer-to-layer based approaches. Unfortunately, in practice, teachers may have heterogeneous architectures; their layers may not be aligned and their dimensionalities or scales may be incompatible. Amalgamating multi-task teachers with heterogeneous architectures remains an open problem. For this, we design Versatile Common Feature Consolidator (VENUS), the first solution to this problem. VENUS fuses knowledge from the shared representations of each teacher into one unified generalized representation for all tasks. Specifically, we design the Feature Consolidator network that leverages an array of teacher-specific trainable adaptors. These adaptors enable the student to learn from multiple teachers, even if they have incompatible learned representations. We demonstrate that VENUS outperforms five alternative methods on numerous benchmark datasets across a broad spectrum of experiments",
    "checked": true,
    "id": "767d77e12148fbab5f95d49f4e831c81122c5130",
    "semantic_title": "amalgamating multi-task models with heterogeneous architectures",
    "citation_count": 0,
    "authors": [
      "Jidapa Thadajarassiri",
      "Walter Gerych",
      "Xiangnan Kong",
      "Elke Rundensteiner"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29460": {
    "title": "ConSequence: Synthesizing Logically Constrained Sequences for Electronic Health Record Generation",
    "volume": "main",
    "abstract": "Generative models can produce synthetic patient records for analytical tasks when real data is unavailable or limited. However, current methods struggle with adhering to domain-specific knowledge and removing invalid data. We present ConSequence, an effective approach to integrating domain knowledge into sequential generative neural network outputs. Our rule-based formulation includes temporal aggregation and antecedent evaluation modules, ensured by an efficient matrix multiplication formulation, to satisfy hard and soft logical constraints across time steps. Existing constraint methods often fail to guarantee constraint satisfaction, lack the ability to handle temporal constraints, and hinder the learning and computational efficiency of the model. In contrast, our approach efficiently handles all types of constraints with guaranteed logical coherence. We demonstrate ConSequence's effectiveness in generating electronic health records, outperforming competitors in achieving complete temporal and spatial constraint satisfaction without compromising runtime performance or generative quality. Specifically, ConSequence successfully prevents all rule violations while improving the model quality in reducing its test perplexity by 5% and incurring less than a 13% slowdown in generation speed compared to an unconstrained model",
    "checked": true,
    "id": "7cbe5415e89e91c4423ee78035d1e8d97921b86b",
    "semantic_title": "consequence: synthesizing logically constrained sequences for electronic health record generation",
    "citation_count": 0,
    "authors": [
      "Brandon Theodorou",
      "Shrusti Jain",
      "Cao Xiao",
      "Jimeng Sun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29461": {
    "title": "N-gram Unsupervised Compoundation and Feature Injection for Better Symbolic Music Understanding",
    "volume": "main",
    "abstract": "The first step to apply deep learning techniques for symbolic music understanding is to transform musical pieces (mainly in MIDI format) into sequences of predefined tokens like note pitch, note velocity, and chords. Subsequently, the sequences are fed into a neural sequence model to accomplish specific tasks. Music sequences exhibit strong correlations between adjacent elements, making them prime candidates for N-gram techniques from Natural Language Processing (NLP). Consider classical piano music: specific melodies might recur throughout a piece, with subtle variations each time. In this paper, we propose a novel method, NG-Midiformer, for understanding symbolic music sequences that leverages the N-gram approach. Our method involves first processing music pieces into word-like sequences with our proposed unsupervised compoundation, followed by using our N-gram Transformer encoder, which can effectively incorporate N-gram information to enhance the primary encoder part for better understanding of music sequences. The pre-training process on large-scale music datasets enables the model to thoroughly learn the N-gram information contained within music sequences, and subsequently apply this information for making inferences during the fine-tuning stage. Experiment on various datasets demonstrate the effectiveness of our method and achieved state-of-the-art performance on a series of music understanding downstream tasks. The code and model weights will be released at https://github.com/CinqueOrigin/NG-Midiformer",
    "checked": true,
    "id": "61ac379707e35651e1716a0b5eca05ffff7e12b0",
    "semantic_title": "n-gram unsupervised compoundation and feature injection for better symbolic music understanding",
    "citation_count": 0,
    "authors": [
      "Jinhao Tian",
      "Zuchao Li",
      "Jiajia Li",
      "Ping Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29462": {
    "title": "DeRDaVa: Deletion-Robust Data Valuation for Machine Learning",
    "volume": "main",
    "abstract": "Data valuation is concerned with determining a fair valuation of data from data sources to compensate them or to identify training examples that are the most or least useful for predictions. With the rising interest in personal data ownership and data protection regulations, model owners will likely have to fulfil more data deletion requests. This raises issues that have not been addressed by existing works: Are the data valuation scores still fair with deletions? Must the scores be expensively recomputed? The answer is no. To avoid recomputations, we propose using our data valuation framework DeRDaVa upfront for valuing each data source's contribution to preserving robust model performance after anticipated data deletions. DeRDaVa can be efficiently approximated and will assign higher values to data that are more useful or less likely to be deleted. We further generalize DeRDaVa to Risk-DeRDaVa to cater to risk-averse/seeking model owners who are concerned with the worst/best-cases model utility. We also empirically demonstrate the practicality of our solutions",
    "checked": true,
    "id": "462163cba5036db50bdbc1c9a7bdeaafca3f7cb2",
    "semantic_title": "derdava: deletion-robust data valuation for machine learning",
    "citation_count": 0,
    "authors": [
      "Xiao Tian",
      "Rachael Hwee Ling Sim",
      "Jue  Fan ",
      "Bryan Kian Hsiang Low"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29463": {
    "title": "Weisfeiler and Lehman Go Paths: Learning Topological Features via Path Complexes",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs), despite achieving remarkable performance across different tasks, are theoretically bounded by the 1-Weisfeiler-Lehman test, resulting in limitations in terms of graph expressivity. Even though prior works on topological higher-order GNNs overcome that boundary, these models often depend on assumptions about sub-structures of graphs. Specifically, topological GNNs leverage the prevalence of cliques, cycles, and rings to enhance the message-passing procedure. Our study presents a novel perspective by focusing on simple paths within graphs during the topological message-passing process, thus liberating the model from restrictive inductive biases. We prove that by lifting graphs to path complexes, our model can generalize the existing works on topology while inheriting several theoretical results on simplicial complexes and regular cell complexes. Without making prior assumptions about graph sub-structures, our method outperforms earlier works in other topological domains and achieves state-of-the-art results on various benchmarks",
    "checked": true,
    "id": "20d7fbf647e7d2c8a4aead99f56e1a815fa263c9",
    "semantic_title": "weisfeiler and lehman go paths: learning topological features via path complexes",
    "citation_count": 2,
    "authors": [
      "Quang Truong",
      "Peter Chin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29464": {
    "title": "Attribute-Missing Graph Clustering Network",
    "volume": "main",
    "abstract": "Deep clustering with attribute-missing graphs, where only a subset of nodes possesses complete attributes while those of others are missing, is an important yet challenging topic in various practical applications. It has become a prevalent learning paradigm in existing studies to perform data imputation first and subsequently conduct clustering using the imputed information. However, these ``two-stage\" methods disconnect the clustering and imputation processes, preventing the model from effectively learning clustering-friendly graph embedding. Furthermore, they are not tailored for clustering tasks, leading to inferior clustering results. To solve these issues, we propose a novel Attribute-Missing Graph Clustering (AMGC) method to alternately promote clustering and imputation in a unified framework, where we iteratively produce the clustering-enhanced nearest neighbor information to conduct the data imputation process and utilize the imputed information to implicitly refine the clustering distribution through model optimization. Specifically, in the imputation step, we take the learned clustering information as imputation prompts to help each attribute-missing sample gather highly correlated features within its clusters for data completion, such that the intra-class compactness can be improved. Moreover, to support reliable clustering, we maximize inter-class separability by conducting cost-efficient dual non-contrastive learning over the imputed latent features, which in turn promotes greater graph encoding capability for clustering sub-network. Extensive experiments on five datasets have verified the superiority of AMGC against competitors",
    "checked": true,
    "id": "4eac0846a1c9433eee0d2808bf3be848d0d974e2",
    "semantic_title": "attribute-missing graph clustering network",
    "citation_count": 6,
    "authors": [
      "Wenxuan Tu",
      "Renxiang Guan",
      "Sihang Zhou",
      "Chuan Ma",
      "Xin Peng",
      "Zhiping Cai",
      "Zhe Liu",
      "Jieren Cheng",
      "Xinwang Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29465": {
    "title": "Parameterized Projected Bellman Operator",
    "volume": "main",
    "abstract": "Approximate value iteration (AVI) is a family of algorithms for reinforcement learning (RL) that aims to obtain an approximation of the optimal value function. Generally, AVI algorithms implement an iterated procedure where each step consists of (i) an application of the Bellman operator and (ii) a projection step into a considered function space. Notoriously, the Bellman operator leverages transition samples, which strongly determine its behavior, as uninformative samples can result in negligible updates or long detours, whose detrimental effects are further exacerbated by the computationally intensive projection step. To address these issues, we propose a novel alternative approach based on learning an approximate version of the Bellman operator rather than estimating it through samples as in AVI approaches. This way, we are able to (i) generalize across transition samples and (ii) avoid the computationally intensive projection step. For this reason, we call our novel operator projected Bellman operator (PBO). We formulate an optimization problem to learn PBO for generic sequential decision-making problems, and we theoretically analyze its properties in two representative classes of RL problems. Furthermore, we theoretically study our approach under the lens of AVI and devise algorithmic implementations to learn PBO in offline and online settings by leveraging neural network parameterizations. Finally, we empirically showcase the benefits of PBO w.r.t. the regular Bellman operator on several RL problems",
    "checked": true,
    "id": "86c3ebce58f0b129f52eedabe60aae63cf075ddb",
    "semantic_title": "parameterized projected bellman operator",
    "citation_count": 3,
    "authors": [
      "Théo Vincent",
      "Alberto Maria Metelli",
      "Boris Belousov",
      "Jan Peters",
      "Marcello Restelli",
      "Carlo D'Eramo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29466": {
    "title": "Causal Strategic Learning with Competitive Selection",
    "volume": "main",
    "abstract": "We study the problem of agent selection in causal strategic learning under multiple decision makers and address two key challenges that come with it. Firstly, while much of prior work focuses on studying a fixed pool of agents that remains static regardless of their evaluations, we consider the impact of selection procedure by which agents are not only evaluated, but also selected. When each decision maker unilaterally selects agents by maximising their own utility, we show that the optimal selection rule is a trade-off between selecting the best agents and providing incentives to maximise the agents' improvement. Furthermore, this optimal selection rule relies on incorrect predictions of agents' outcomes. Hence, we study the conditions under which a decision maker's optimal selection rule will not lead to deterioration of agents' outcome nor cause unjust reduction in agents' selection chance. To that end, we provide an analytical form of the optimal selection rule and a mechanism to retrieve the causal parameters from observational data, under certain assumptions on agents' behaviour. Secondly, when there are multiple decision makers, the interference between selection rules introduces another source of biases in estimating the underlying causal parameters. To address this problem, we provide a cooperative protocol which all decision makers must collectively adopt to recover the true causal parameters. Lastly, we complement our theoretical results with simulation studies. Our results highlight not only the importance of causal modeling as a strategy to mitigate the effect of gaming, as suggested by previous work, but also the need of a benevolent regulator to enable it",
    "checked": true,
    "id": "8adcba25754cae0b01f939795a5b8e1b507d3bf6",
    "semantic_title": "causal strategic learning with competitive selection",
    "citation_count": 1,
    "authors": [
      "Kiet Q. H. Vo",
      "Muneeb Aadil",
      "Siu Lun Chau",
      "Krikamol Muandet"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29467": {
    "title": "Data Disparity and Temporal Unavailability Aware Asynchronous Federated Learning for Predictive Maintenance on Transportation Fleets",
    "volume": "main",
    "abstract": "Predictive maintenance has emerged as a critical application in modern transportation, leveraging sensor data to forecast potential damages proactively using machine learning. However, privacy concerns limit data sharing, making Federated learning an appealing approach to preserve data privacy. Nevertheless, challenges arise due to disparities in data distribution and temporal unavailability caused by individual usage patterns in transportation. In this paper, we present a novel asynchronous federated learning approach to address system heterogeneity and facilitate machine learning for predictive maintenance on transportation fleets. The approach introduces a novel data disparity aware aggregation scheme and a federated early stopping method for training. To validate the effectiveness of our approach, we evaluate it on two independent real-world datasets from the transportation domain: 1) oil dilution prediction of car combustion engines and 2) remaining lifetime prediction of plane turbofan engines. Our experiments show that we reliably outperform five state-of-the-art baselines, including federated and classical machine learning models. Moreover, we show that our approach generalises to various prediction model architectures",
    "checked": true,
    "id": "cd87d20764d8293ff171e8e41659aa12c7632e6a",
    "semantic_title": "data disparity and temporal unavailability aware asynchronous federated learning for predictive maintenance on transportation fleets",
    "citation_count": 0,
    "authors": [
      "Leonie von Wahl",
      "Niklas Heidenreich",
      "Prasenjit Mitra",
      "Michael Nolting",
      "Nicolas Tempelmeier"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29468": {
    "title": "Federated Graph Learning under Domain Shift with Generalizable Prototypes",
    "volume": "main",
    "abstract": "Federated Graph Learning is a privacy-preserving collaborative approach for training a shared model on graph-structured data in the distributed environment. However, in real-world scenarios, the client graph data usually originate from diverse domains, this unavoidably hinders the generalization performance of the final global model. To address this challenge, we start the first attempt to investigate this scenario by learning a well-generalizable model. In order to improve the performance of the global model from different perspectives, we propose a novel framework called Federated Graph Learning with Generalizable Prototypes (FGGP). It decouples the global model into two levels and bridges them via prototypes. These prototypes, which are semantic centers derived from the feature extractor, can provide valuable classification information. At the classification model level, we innovatively eschew the traditional classifiers, then instead leverage clustered prototypes to capture fruitful domain information and enhance the discriminative capability of the classes, improving the performance of multi-domain predictions. Furthermore, at the feature extractor level, we go beyond traditional approaches by implicitly injecting distinct global knowledge and employing contrastive learning to obtain more powerful prototypes while enhancing the feature extractor generalization ability. Experimental results on various datasets are presented to validate the effectiveness of the proposed method",
    "checked": true,
    "id": "4da120105437ec608d6c42b515383bdf2351ba2f",
    "semantic_title": "federated graph learning under domain shift with generalizable prototypes",
    "citation_count": 5,
    "authors": [
      "Guancheng Wan",
      "Wenke Huang",
      "Mang Ye"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29469": {
    "title": "Unlocking the Power of Open Set: A New Perspective for Open-Set Noisy Label Learning",
    "volume": "main",
    "abstract": "Learning from noisy data has attracted much attention, where most methods focus on closed-set label noise. However, a more common scenario in the real world is the presence of both open-set and closed-set noise. Existing methods typically identify and handle these two types of label noise separately by designing a specific strategy for each type. However, in many real-world scenarios, it would be challenging to identify open-set examples, especially when the dataset has been severely corrupted. Unlike the previous works, we explore how models behave when faced with open-set examples, and find that a part of open-set examples gradually get integrated into certain known classes, which is beneficial for the separation among known classes. Motivated by the phenomenon, we propose a novel two-step contrastive learning method CECL (Class Expansion Contrastive Learning) which aims to deal with both types of label noise by exploiting the useful information of open-set examples. Specifically, we incorporate some open-set examples into closed-set classes to enhance performance while treating others as delimiters to improve representative ability. Extensive experiments on synthetic and real-world datasets with diverse label noise demonstrate the effectiveness of CECL",
    "checked": false,
    "id": "e4f96ddef122e874c383a3cfc397a35cb65f421b",
    "semantic_title": "unlocking the power of open set : a new perspective for open-set noisy label learning",
    "citation_count": 1,
    "authors": [
      "Wenhai Wan",
      "Xinrui Wang",
      "Ming-Kun Xie",
      "Shao-Yuan Li",
      "Sheng-Jun Huang",
      "Songcan Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29470": {
    "title": "DiffAIL: Diffusion Adversarial Imitation Learning",
    "volume": "main",
    "abstract": "Imitation learning aims to solve the problem of defining reward functions in real-world decision-making tasks. The current popular approach is the Adversarial Imitation Learning (AIL) framework, which matches expert state-action occupancy measures to obtain a surrogate reward for forward reinforcement learning. However, the traditional discriminator is a simple binary classifier and doesn't learn an accurate distribution, which may result in failing to identify expert-level state-action pairs induced by the policy interacting with the environment. To address this issue, we propose a method named diffusion adversarial imitation learning (DiffAIL), which introduces the diffusion model into the AIL framework. Specifically, DiffAIL models the state-action pairs as unconditional diffusion models and uses diffusion loss as part of the discriminator's learning objective, which enables the discriminator to capture better expert demonstrations and improve generalization. Experimentally, the results show that our method achieves state-of-the-art performance and significantly surpasses expert demonstration on two benchmark tasks, including the standard state-action setting and state-only settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bingzheng Wang",
      "Guoqiang Wu",
      "Teng Pang",
      "Yan Zhang",
      "Yilong Yin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29471": {
    "title": "DR-Label: Label Deconstruction and Reconstruction of GNN Models for Catalysis Systems",
    "volume": "main",
    "abstract": "Attaining the equilibrium geometry of a catalyst-adsorbate system is key to fundamentally assessing its effective properties, such as adsorption energy. While machine learning methods with advanced representation or supervision strategies have been applied to boost and guide the relaxation processes of catalysis systems, existing methods that produce linearly aggregated geometry predictions are susceptible to edge representations ambiguity, and are therefore vulnerable to graph variations. In this paper, we present a novel graph neural network (GNN) supervision and prediction strategy DR-Label. Our approach mitigates the multiplicity of solutions in edge representation and encourages model predictions that are independent of graph structural variations. DR-Label first Deconstructs finer-grained equilibrium state information to the model by projecting the node-level supervision signal to each edge. Reversely, the model Reconstructs a more robust equilibrium state prediction by converting edge-level predictions to node-level via a sphere-fitting algorithm. When applied to three fundamentally different models, DR-Label consistently enhanced performance. Leveraging the graph structure invariance of the DR-Label strategy, we further propose DRFormer, which applied explicit intermediate positional update and achieves a new state-of-the-art performance on the Open Catalyst 2020 (OC20) dataset and the Cu-based single-atom alloys CO adsorption (SAA) dataset. We expect our work to highlight vital principles for advancing geometric GNN models for catalysis systems and beyond. Our code is available at https://github.com/bowenwang77/DR-Label",
    "checked": true,
    "id": "7a6ea01655322f614d7ac84d7a94ab11c171c315",
    "semantic_title": "dr-label: label deconstruction and reconstruction of gnn models for catalysis systems",
    "citation_count": 1,
    "authors": [
      "Bowen Wang",
      "Chen Liang",
      "Jiaze Wang",
      "Jiezhong Qiu",
      "Furui Liu",
      "Shaogang Hao",
      "Dong Li",
      "Guangyong Chen",
      "Xiaolong Zou",
      "Pheng Ann Heng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29472": {
    "title": "GAD-PVI: A General Accelerated Dynamic-Weight Particle-Based Variational Inference Framework",
    "volume": "main",
    "abstract": "Particle-based Variational Inference (ParVI) methods approximate the target distribution by iteratively evolving finite weighted particle systems. Recent advances of ParVI methods reveal the benefits of accelerated position update strategies and dynamic weight adjustment approaches. In this paper, we propose the first ParVI framework that possesses both accelerated position update and dynamical weight adjustment simultaneously, named the General Accelerated Dynamic-Weight Particle-based Variational Inference (GAD-PVI) framework. Generally, GAD-PVI simulates the semi-Hamiltonian gradient flow on a novel Information-Fisher-Rao space, which yields an additional decrease on the local functional dissipation. GAD-PVI is compatible with different dissimilarity functionals and associated smoothing approaches under three information metrics. Experiments on both synthetic and real-world data demonstrate the faster convergence and reduced approximation error of GAD-PVI methods over the state-of-the-art",
    "checked": true,
    "id": "42eff22c60d2864952cedcab7bc8f41d04d39d43",
    "semantic_title": "gad-pvi: a general accelerated dynamic-weight particle-based variational inference framework",
    "citation_count": 1,
    "authors": [
      "Fangyikang Wang",
      "Huminhao Zhu",
      "Chao Zhang",
      "Hanbin Zhao",
      "Hui Qian"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29473": {
    "title": "Generative Model-Based Feature Knowledge Distillation for Action Recognition",
    "volume": "main",
    "abstract": "Knowledge distillation (KD), a technique widely employed in computer vision, has emerged as a de facto standard for improving the performance of small neural networks. However, prevailing KD-based approaches in video tasks primarily focus on designing loss functions and fusing cross-modal information. This overlooks the spatial-temporal feature semantics, resulting in limited advancements in model compression. Addressing this gap, our paper introduces an innovative knowledge distillation framework, with the generative model for training a lightweight student model. In particular, the framework is organized into two steps: the initial phase is Feature Representation, wherein a generative model-based attention module is trained to represent feature semantics; Subsequently, the Generative-based Feature Distillation phase encompasses both Generative Distillation and Attention Distillation, with the objective of transferring attention-based feature semantics with the generative model. The efficacy of our approach is demonstrated through comprehensive experiments on diverse popular datasets, proving considerable enhancements in video action recognition task. Moreover, the effectiveness of our proposed framework is validated in the context of more intricate video action detection task. Our code is available at https://github.com/aaai-24/Generative-based-KD",
    "checked": true,
    "id": "b4986187b799aff146c5a7e7dbbb6cab9e0fb24a",
    "semantic_title": "generative model-based feature knowledge distillation for action recognition",
    "citation_count": 0,
    "authors": [
      "Guiqin Wang",
      "Peng Zhao",
      "Yanjiang Shi",
      "Cong Zhao",
      "Shusen Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29474": {
    "title": "Gradient-Guided Modality Decoupling for Missing-Modality Robustness",
    "volume": "main",
    "abstract": "Multimodal learning with incomplete input data (missing modality) is very practical and challenging. In this work, we conduct an in-depth analysis of this challenge and find that modality dominance has a significant negative impact on the model training, greatly degrading the missing modality performance. Motivated by Grad-CAM, we introduce a novel indicator, gradients, to monitor and reduce modality dominance which widely exists in the missing-modality scenario. In aid of this indicator, we present a novel Gradient-guided Modality Decoupling (GMD) method to decouple the dependency on dominating modalities. Specifically, GMD removes the conflicted gradient components from different modalities to achieve this decoupling, significantly improving the performance. In addition, to flexibly handle modal-incomplete data, we design a parameter-efficient Dynamic Sharing (DS) framework which can adaptively switch on/off the network parameters based on whether one modality is available. We conduct extensive experiments on three popular multimodal benchmarks, including BraTS 2018 for medical segmentation, CMU-MOSI, and CMU-MOSEI for sentiment analysis. The results show that our method can significantly outperform the competitors, showing the effectiveness of the proposed solutions. Our code is released here: https://github.com/HaoWang420/Gradient-guided-Modality-Decoupling",
    "checked": true,
    "id": "cc4d8a9a10ced5e7a3f45b9a3e678597ddab24ea",
    "semantic_title": "gradient-guided modality decoupling for missing-modality robustness",
    "citation_count": 0,
    "authors": [
      "Hao Wang",
      "Shengda Luo",
      "Guosheng Hu",
      "Jianguo Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29475": {
    "title": "V2A-Mapper: A Lightweight Solution for Vision-to-Audio Generation by Connecting Foundation Models",
    "volume": "main",
    "abstract": "Building artificial intelligence (AI) systems on top of a set of foundation models (FMs) is becoming a new paradigm in AI research. Their representative and generative abilities learnt from vast amounts of data can be easily adapted and transferred to a wide range of downstream tasks without extra training from scratch. However, leveraging FMs in cross-modal generation remains under-researched when audio modality is involved. On the other hand, automatically generating semantically-relevant sound from visual input is an important problem in cross-modal generation studies. To solve this vision-to-audio (V2A) generation problem, existing methods tend to design and build complex systems from scratch using modestly sized datasets. In this paper, we propose a lightweight solution to this problem by leveraging foundation models, specifically CLIP, CLAP, and AudioLDM. We first investigate the domain gap between the latent space of the visual CLIP and the auditory CLAP models. Then we propose a simple yet effective mapper mechanism (V2A-Mapper) to bridge the domain gap by translating the visual input between CLIP and CLAP spaces. Conditioned on the translated CLAP embedding, pretrained audio generative FM AudioLDM is adopted to produce high-fidelity and visually-aligned sound. Compared to previous approaches, our method only requires a quick training of the V2A-Mapper. We further analyze and conduct extensive experiments on the choice of the V2A-Mapper and show that a generative mapper is better at fidelity and variability (FD) while a regression mapper is slightly better at relevance (CS). Both objective and subjective evaluation on two V2A datasets demonstrate the superiority of our proposed method compared to current state-of-the-art approaches - trained with 86% fewer parameters but achieving 53% and 19% improvement in FD and CS, respectively. Supplementary materials such as audio samples are provided at our demo website: https://v2a-mapper.github.io/",
    "checked": true,
    "id": "f041825c3ac6baa6fe274c5b89daa0635d1ee94d",
    "semantic_title": "v2a-mapper: a lightweight solution for vision-to-audio generation by connecting foundation models",
    "citation_count": 6,
    "authors": [
      "Heng Wang",
      "Jianbo Ma",
      "Santiago Pascual",
      "Richard Cartwright",
      "Weidong Cai"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29476": {
    "title": "Practical Privacy-Preserving MLaaS: When Compressive Sensing Meets Generative Networks",
    "volume": "main",
    "abstract": "The Machine-Learning-as-a-Service (MLaaS) framework allows one to grab low-hanging fruit of machine learning techniques and data science, without either much expertise for this sophisticated sphere or provision of specific infrastructures. However, the requirement of revealing all training data to the service provider raises new concerns in terms of privacy leakage, storage consumption, efficiency, bandwidth, etc. In this paper, we propose a lightweight privacy-preserving MLaaS framework by combining Compressive Sensing (CS) and Generative Networks. It's constructed on the favorable facts observed in recent works that general inference tasks could be fulfilled with generative networks and classifier trained on compressed measurements, since the generator could model the data distribution and capture discriminative information which are useful for classification. To improve the performance of the MLaaS framework, the supervised generative models of the server are trained and optimized with prior knowledge provided by the client. In order to prevent the service provider from recovering the original data as well as identifying the queried results, a noise-addition mechanism is designed and adopted into the compressed data domain. Empirical results confirmed its performance superiority in accuracy and resource consumption against the state-of-the-art privacy preserving MLaaS frameworks",
    "checked": true,
    "id": "3dfd47569ea44488e2186984546c034ae1e515e0",
    "semantic_title": "practical privacy-preserving mlaas: when compressive sensing meets generative networks",
    "citation_count": 0,
    "authors": [
      "Jia Wang",
      "Wuqiang Su",
      "Zushu Huang",
      "Jie Chen",
      "Chengwen Luo",
      "Jianqiang  Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29477": {
    "title": "Towards Stability and Generalization Bounds in Decentralized Minibatch Stochastic Gradient Descent",
    "volume": "main",
    "abstract": "Decentralized Stochastic Gradient Descent (D-SGD) represents an efficient communication approach tailored for mastering insights from vast, distributed datasets. Inspired by parallel optimization paradigms, the incorporation of minibatch serves to diminish variance, consequently expediting the optimization process. Nevertheless, as per our current understanding, the existing literature has not thoroughly explored the learning theory foundation of Decentralized Minibatch Stochastic Gradient Descent (DM-SGD). In this paper, we try to address this theoretical gap by investigating the generalization properties of DM-SGD. We establish the sharper generalization bounds for the DM-SGD algorithm with replacement (without replacement) on (non)convex and (non)smooth cases. Moreover, our results consistently recover to the results of Centralized Stochastic Gradient Descent (C-SGD). In addition, we derive generalization analysis for Zero-Order (ZO) version of DM-SGD",
    "checked": true,
    "id": "f111d7302652acc103bcb1f305e33363031a6c60",
    "semantic_title": "towards stability and generalization bounds in decentralized minibatch stochastic gradient descent",
    "citation_count": 0,
    "authors": [
      "Jiahuan Wang",
      "Hong Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29478": {
    "title": "SURER: Structure-Adaptive Unified Graph Neural Network for Multi-View Clustering",
    "volume": "main",
    "abstract": "Deep Multi-view Graph Clustering (DMGC) aims to partition instances into different groups using the graph information extracted from multi-view data. The mainstream framework of DMGC methods applies graph neural networks to embed structure information into the view-specific representations and fuse them for the consensus representation. However, on one hand, we find that the graph learned in advance is not ideal for clustering as it is constructed by original multi-view data and localized connecting. On the other hand, most existing methods learn the consensus representation in a late fusion manner, which fails to propagate the structure relations across multiple views. Inspired by the observations, we propose a Structure-adaptive Unified gRaph nEural network for multi-view clusteRing (SURER), which can jointly learn a heterogeneous multi-view unified graph and robust graph neural networks for multi-view clustering. Specifically, we first design a graph structure learning module to refine the original view-specific attribute graphs, which removes false edges and discovers the potential connection. According to the view-specific refined attribute graphs, we integrate them into a unified heterogeneous graph by linking the representations of the same sample from different views. Furthermore, we use the unified heterogeneous graph as the input of the graph neural network to learn the consensus representation for each instance, effectively integrating complementary information from various views. Extensive experiments on diverse datasets demonstrate the superior effectiveness of our method compared to other state-of-the-art approaches",
    "checked": true,
    "id": "a76f9a3d2d1103bb4ef4e4edaf63c0b50cac84e3",
    "semantic_title": "surer: structure-adaptive unified graph neural network for multi-view clustering",
    "citation_count": 0,
    "authors": [
      "Jing Wang",
      "Songhe Feng",
      "Gengyu Lyu",
      "Jiazheng Yuan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29479": {
    "title": "Rethinking Graph Masked Autoencoders through Alignment and Uniformity",
    "volume": "main",
    "abstract": "Self-supervised learning on graphs can be bifurcated into contrastive and generative methods. Contrastive methods, also known as graph contrastive learning (GCL), have dominated graph self-supervised learning in the past few years, but the recent advent of graph masked autoencoder (GraphMAE) rekindles the momentum behind generative methods. Despite the empirical success of GraphMAE, there is still a dearth of theoretical understanding regarding its efficacy. Moreover, while both generative and contrastive methods have been shown to be effective, their connections and differences have yet to be thoroughly investigated. Therefore, we theoretically build a bridge between GraphMAE and GCL, and prove that the node-level reconstruction objective in GraphMAE implicitly performs context-level GCL. Based on our theoretical analysis, we further identify the limitations of the GraphMAE from the perspectives of alignment and uniformity, which have been considered as two key properties of high-quality representations in GCL. We point out that GraphMAE's alignment performance is restricted by the masking strategy, and the uniformity is not strictly guaranteed. To remedy the aforementioned limitations, we propose an Alignment-Uniformity enhanced Graph Masked AutoEncoder, named AUG-MAE. Specifically, we propose an easy-to-hard adversarial masking strategy to provide hard-to-align samples, which improves the alignment performance. Meanwhile, we introduce an explicit uniformity regularizer to ensure the uniformity of the learned representations. Experimental results on benchmark datasets demonstrate the superiority of our model over existing state-of-the-art methods. The code is available at: https://github.com/AzureLeon1/AUG-MAE",
    "checked": true,
    "id": "782ad7f1459ffdece0636747a5e6e3735d600700",
    "semantic_title": "rethinking graph masked autoencoders through alignment and uniformity",
    "citation_count": 1,
    "authors": [
      "Liang Wang",
      "Xiang Tao",
      "Qiang Liu",
      "Shu Wu",
      "Liang Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29480": {
    "title": "GOODAT: Towards Test-Time Graph Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "Graph neural networks (GNNs) have found widespread application in modeling graph data across diverse domains. While GNNs excel in scenarios where the testing data shares the distribution of their training counterparts (in distribution, ID), they often exhibit incorrect predictions when confronted with samples from an unfamiliar distribution (out-of-distribution, OOD). To identify and reject OOD samples with GNNs, recent studies have explored graph OOD detection, often focusing on training a specific model or modifying the data on top of a well-trained GNN. Despite their effectiveness, these methods come with heavy training resources and costs, as they need to optimize the GNN-based models on training data. Moreover, their reliance on modifying the original GNNs and accessing training data further restricts their universality. To this end, this paper introduces a method to detect Graph Out-of-Distribution At Test-time (namely GOODAT), a data-centric, unsupervised, and plug-and-play solution that operates independently of training data and modifications of GNN architecture. With a lightweight graph masker, GOODAT can learn informative subgraphs from test samples, enabling the capture of distinct graph patterns between OOD and ID samples. To optimize the graph masker, we meticulously design three unsupervised objective functions based on the graph information bottleneck principle, motivating the masker to capture compact yet informative subgraphs for OOD detection. Comprehensive evaluations confirm that our GOODAT method outperforms state-of-the-art benchmarks across a variety of real-world datasets",
    "checked": true,
    "id": "2902a67f9aebb115fc2b6cdf611910e72e896bdd",
    "semantic_title": "goodat: towards test-time graph out-of-distribution detection",
    "citation_count": 3,
    "authors": [
      "Luzhi Wang",
      "Dongxiao He",
      "He Zhang",
      "Yixin Liu",
      "Wenjie Wang",
      "Shirui Pan",
      "Di Jin",
      "Tat-Seng Chua"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29481": {
    "title": "TurboSVM-FL: Boosting Federated Learning through SVM Aggregation for Lazy Clients",
    "volume": "main",
    "abstract": "Federated learning is a distributed collaborative machine learning paradigm that has gained strong momentum in recent years. In federated learning, a central server periodically coordinates models with clients and aggregates the models trained locally by clients without necessitating access to local data. Despite its potential, the implementation of federated learning continues to encounter several challenges, predominantly the slow convergence that is largely due to data heterogeneity. The slow convergence becomes particularly problematic in cross-device federated learning scenarios where clients may be strongly limited by computing power and storage space, and hence counteracting methods that induce additional computation or memory cost on the client side such as auxiliary objective terms and larger training iterations can be impractical. In this paper, we propose a novel federated aggregation strategy, TurboSVM-FL, that poses no additional computation burden on the client side and can significantly accelerate convergence for federated classification task, especially when clients are \"lazy\" and train their models solely for few epochs for next global aggregation. TurboSVM-FL extensively utilizes support vector machine to conduct selective aggregation and max-margin spread-out regularization on class embeddings. We evaluate TurboSVM-FL on multiple datasets including FEMNIST, CelebA, and Shakespeare using user-independent validation with non-iid data distribution. Our results show that TurboSVM-FL can significantly outperform existing popular algorithms on convergence rate and reduce communication rounds while delivering better test metrics including accuracy, F1 score, and MCC",
    "checked": true,
    "id": "a8bf36679d9af998e37ee6b7685646d117aeec1f",
    "semantic_title": "turbosvm-fl: boosting federated learning through svm aggregation for lazy clients",
    "citation_count": 0,
    "authors": [
      "Mengdi Wang",
      "Anna Bodonhelyi",
      "Efe Bozkir",
      "Enkelejda Kasneci"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29482": {
    "title": "MetaCARD: Meta-Reinforcement Learning with Task Uncertainty Feedback via Decoupled Context-Aware Reward and Dynamics Components",
    "volume": "main",
    "abstract": "Meta-Reinforcement Learning (Meta-RL) aims to reveal shared characteristics in dynamics and reward functions across diverse training tasks. This objective is achieved by meta-learning a policy that is conditioned on task representations with encoded trajectory data or context, thus allowing rapid adaptation to new tasks from a known task distribution. However, since the trajectory data generated by the policy may be biased, the task inference module tends to form spurious correlations between trajectory data and specific tasks, thereby leading to poor adaptation to new tasks. To address this issue, we propose the Meta-RL with task unCertAinty feedback through decoupled context-aware Reward and Dynamics components (MetaCARD). MetaCARD distinctly decouples the dynamics and rewards when inferring tasks and integrates task uncertainty feedback from policy evaluation into the task inference module. This design effectively reduces uncertainty in tasks with changes in dynamics or/and reward functions, thereby enabling accurate task identification and adaptation. The experiment results on both Meta-World and classical MuJoCo benchmarks show that MetaCARD significantly outperforms prevailing Meta-RL baselines, demonstrating its remarkable adaptation ability in sophisticated environments that involve changes in both reward functions and dynamics",
    "checked": true,
    "id": "3410b08ec0d23e89390669bae23471555846660a",
    "semantic_title": "metacard: meta-reinforcement learning with task uncertainty feedback via decoupled context-aware reward and dynamics components",
    "citation_count": 0,
    "authors": [
      "Min Wang",
      "Xin Li",
      "Leiji Zhang",
      "Mingzhong Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29483": {
    "title": "Considering Nonstationary within Multivariate Time Series with Variational Hierarchical Transformer for Forecasting",
    "volume": "main",
    "abstract": "The forecasting of Multivariate Time Series (MTS) has long been an important but challenging task. Due to the non-stationary problem across long-distance time steps, previous studies primarily adopt stationarization method to attenuate the non-stationary problem of original series for better predictability. However, existed methods always adopt the stationarized series, which ignore the inherent non-stationarity, and have difficulty in modeling MTS with complex distributions due to the lack of stochasticity. To tackle these problems, we first develop a powerful hierarchical probabilistic generative module to consider the non-stationarity and stochastity characteristics within MTS, and then combine it with transformer for a well-defined variational generative dynamic model named Hierarchical Time series Variational Transformer (HTV-Trans), which recovers the intrinsic non-stationary information into temporal dependencies. Being an powerful probabilistic model, HTV-Trans is utilized to learn expressive representations of MTS and applied to the forecasting tasks. Extensive experiments on diverse datasets show the efficiency of HTV-Trans on MTS forecasting tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muyao Wang",
      "Wenchao Chen",
      "Bo Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29484": {
    "title": "Controller-Guided Partial Label Consistency Regularization with Unlabeled Data",
    "volume": "main",
    "abstract": "Partial label learning (PLL) learns from training examples each associated with multiple candidate labels, among which only one is valid. In recent years, benefiting from the strong capability of dealing with ambiguous supervision and the impetus of modern data augmentation methods, consistency regularization-based PLL methods have achieved a series of successes and become mainstream. However, as the partial annotation becomes insufficient, their performances drop significantly. In this paper, we leverage easily accessible unlabeled examples to facilitate the partial label consistency regularization. In addition to a partial supervised loss, our method performs a controller-guided consistency regularization at both the label-level and representation-level with the help of unlabeled data. To minimize the disadvantages of insufficient capabilities of the initial supervised model, we use the controller to estimate the confidence of each current prediction to guide the subsequent consistency regularization. Furthermore, we dynamically adjust the confidence thresholds so that the number of samples of each class participating in consistency regularization remains roughly equal to alleviate the problem of class-imbalance. Experiments show that our method achieves satisfactory performances in more practical situations, and its modules can be applied to existing PLL methods to enhance their capabilities",
    "checked": true,
    "id": "819a006ee909d0ef2e9b5fe4c75d80ea2bde7604",
    "semantic_title": "controller-guided partial label consistency regularization with unlabeled data",
    "citation_count": 1,
    "authors": [
      "Qian-Wei Wang",
      "Bowen Zhao",
      "Mingyan Zhu",
      "Tianxiang Li",
      "Zimo Liu",
      "Shu-Tao Xia"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29485": {
    "title": "A Bregman Proximal Stochastic Gradient Method with Extrapolation for Nonconvex Nonsmooth Problems",
    "volume": "main",
    "abstract": "In this paper, we explore a specific optimization problem that involves the combination of a differentiable nonconvex function and a nondifferentiable function. The differentiable component lacks a global Lipschitz continuous gradient, posing challenges for optimization. To address this issue and accelerate the convergence, we propose a Bregman proximal stochastic gradient method with extrapolation (BPSGE), which only requires smooth adaptivity of the differentiable part. Under variance reduction framework, we not only analyze the subsequential and global convergence of the proposed algorithm under certain conditions, but also analyze the sublinear convergence rate of the subsequence, and the complexity of the algorithm, revealing that the BPSGE algorithm requires at most O(epsilon\\^\\,(-2)) iterations in expectation to attain an epsilon-stationary point. To validate the effectiveness of our proposed algorithm, we conduct numerical experiments on three real-world applications: graph regularized nonnegative matrix factorization (NMF), matrix factorization with weakly-convex regularization, and NMF with nonconvex sparsity constraints. These experiments demonstrate that BPSGE is faster than the baselines without extrapolation. The code is available at: https://github.com/nothing2wang/BPSGE-Algorithm",
    "checked": true,
    "id": "9af84dccbd1d5e4c28609c43eb3216e5eae50602",
    "semantic_title": "a bregman proximal stochastic gradient method with extrapolation for nonconvex nonsmooth problems",
    "citation_count": 2,
    "authors": [
      "Qingsong Wang",
      "Zehui Liu",
      "Chunfeng Cui",
      "Deren Han"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29486": {
    "title": "ND-MRM: Neuronal Diversity Inspired Multisensory Recognition Model",
    "volume": "main",
    "abstract": "Cross-sensory interaction is a key aspect for multisensory recognition. Without cross-sensory interaction, artificial neural networks show inferior performance in multisensory recognition. On the contrary, the human brain has an inherently remarkable ability in multisensory recognition, which stems from the diverse neurons that exhibit distinct responses to sensory inputs, especially the multisensory neurons with multisensory responses hence enabling cross-sensory interaction. Based on this neuronal diversity, we propose a Neuronal Diversity inspired Multisensory Recognition Model (ND-MRM), which, similar to the brain, comprises unisensory neurons and multisensory neurons. To reflect the different responses characteristics of diverse neurons in the brain, special connection constraints are innovatively designed to regulate the features transmission in the ND-MRM. Leveraging this novel concept of neuronal diversity, our model is biologically plausible, enabling more effective recognition of multisensory information. To validate the performance of the proposed ND-MRM, we employ a multisensory emotion recognition task as a case study. The results demonstrate that our model surpasses state-of-the-art brain-inspired baselines on two datasets, proving the potential of brain-inspired methods for advancing multisensory interaction and recognition",
    "checked": true,
    "id": "d4add7eead83cba77ee7b40b71e037e5ae6929db",
    "semantic_title": "nd-mrm: neuronal diversity inspired multisensory recognition model",
    "citation_count": 0,
    "authors": [
      "Qixin Wang",
      "Chaoqiong Fan",
      "Tianyuan Jia",
      "Yuyang Han",
      "Xia Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29487": {
    "title": "AQ-DETR: Low-Bit Quantized Detection Transformer with Auxiliary Queries",
    "volume": "main",
    "abstract": "DEtection TRansformer (DETR)-based models have achieved remarkable performance. However, they are accompanied by a large computation overhead cost, which significantly prevents their applications on resource-limited devices. Prior arts attempt to reduce the computational burden of DETR using low-bit quantization, while these methods sacrifice a severe significant performance on weight-activation-attention low-bit quantization. We observe that the number of matching queries and positive samples affect much on the representation capacity of queries in DETR, while quantifying queries of DETR further reduces its representational capacity, thus leading to a severe performance drop. We introduce a new quantization strategy based on Auxiliary Queries for DETR (AQ-DETR), aiming to enhance the capacity of quantized queries. In addition, a layer-by-layer distillation is proposed to reduce the quantization error between quantized attention and full-precision counterpart. Through our extensive experiments on large-scale open datasets, the performance of the 4-bit quantization of DETR and Deformable DETR models is comparable to full-precision counterparts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runqi Wang",
      "Huixin Sun",
      "Linlin Yang",
      "Shaohui Lin",
      "Chuanjian Liu",
      "Yan Gao",
      "Yao Hu",
      "Baochang Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29488": {
    "title": "Constrained Bayesian Optimization under Partial Observations: Balanced Improvements and Provable Convergence",
    "volume": "main",
    "abstract": "The partially observable constrained optimization problems (POCOPs) impede data-driven optimization techniques since an infeasible solution of POCOPs can provide little information about the objective as well as the constraints. We endeavor to design an efficient and provable method for expensive POCOPs under the framework of constrained Bayesian optimization. Our method consists of two key components. Firstly, we present an improved design of the acquisition functions that introduce balanced exploration during optimization. We rigorously study the convergence properties of this design to demonstrate its effectiveness. Secondly, we propose Gaussian processes embedding different likelihoods as the surrogate model for partially observable constraints. This model leads to a more accurate representation of the feasible regions compared to traditional classification-based models. Our proposed method is empirically studied on both synthetic and real-world problems. The results demonstrate the competitiveness of our method for solving POCOPs",
    "checked": true,
    "id": "4995ca015bbf56a4ce7b4520d058a17967c7ac9c",
    "semantic_title": "constrained bayesian optimization under partial observations: balanced improvements and provable convergence",
    "citation_count": 3,
    "authors": [
      "Shengbo Wang",
      "Ke Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29489": {
    "title": "Online Restless Multi-Armed Bandits with Long-Term Fairness Constraints",
    "volume": "main",
    "abstract": "Restless multi-armed bandits (RMAB) have been widely used to model sequential decision making problems with constraints. The decision maker (DM) aims to maximize the expected total reward over an infinite horizon under an \"instantaneous activation constraint\" that at most B arms can be activated at any decision epoch, where the state of each arm evolves stochastically according to a Markov decision process (MDP). However, this basic model fails to provide any fairness guarantee among arms. In this paper, we introduce RMAB-F, a new RMAB model with \"long-term fairness constraints\", where the objective now is to maximize the longterm reward while a minimum long-term activation fraction for each arm must be satisfied. For the online RMAB-F setting (i.e., the underlying MDPs associated with each arm are unknown to the DM), we develop a novel reinforcement learning (RL) algorithm named Fair-UCRL. We prove that Fair-UCRL ensures probabilistic sublinear bounds on both the reward regret and the fairness violation regret. Compared with off-the-shelf RL methods, our Fair-UCRL is much more computationally efficient since it contains a novel exploitation that leverages a low-complexity index policy for making decisions. Experimental results further demonstrate the effectiveness of our Fair-UCRL",
    "checked": true,
    "id": "ade8d32ab7308c6e31e8db57a0d2e5231776fa80",
    "semantic_title": "online restless multi-armed bandits with long-term fairness constraints",
    "citation_count": 2,
    "authors": [
      "Shufan  Wang",
      "Guojun Xiong",
      "Jian Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29490": {
    "title": "Exploring Gradient Explosion in Generative Adversarial Imitation Learning: A Probabilistic Perspective",
    "volume": "main",
    "abstract": "Generative Adversarial Imitation Learning (GAIL) stands as a cornerstone approach in imitation learning. This paper investigates the gradient explosion in two types of GAIL: GAIL with deterministic policy (DE-GAIL) and GAIL with stochastic policy (ST-GAIL). We begin with the observation that the training can be highly unstable for DE-GAIL at the beginning of the training phase and end up divergence. Conversely, the ST-GAIL training trajectory remains consistent, reliably converging. To shed light on these disparities, we provide an explanation from a theoretical perspective. By establishing a probabilistic lower bound for GAIL, we demonstrate that gradient explosion is an inevitable outcome for DE-GAIL due to occasionally large expert-imitator policy disparity, whereas ST-GAIL does not have the issue with it. To substantiate our assertion, we illustrate how modifications in the reward function can mitigate the gradient explosion challenge. Finally, we propose CREDO, a simple yet effective strategy that clips the reward function during the training phase, allowing the GAIL to enjoy high data efficiency and stable trainability",
    "checked": true,
    "id": "9c54cf2d040a731dea391fd5e94c5b02ac9fa484",
    "semantic_title": "exploring gradient explosion in generative adversarial imitation learning: a probabilistic perspective",
    "citation_count": 1,
    "authors": [
      "Wanying Wang",
      "Yichen Zhu",
      "Yirui Zhou",
      "Chaomin Shen",
      "Jian Tang",
      "Zhiyuan Xu",
      "Yaxin Peng",
      "Yangchun Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29491": {
    "title": "IGAMT: Privacy-Preserving Electronic Health Record Synthesization with Heterogeneity and Irregularity",
    "volume": "main",
    "abstract": "Integrating electronic health records (EHR) into machine learning-driven clinical research and hospital applications is important, as it harnesses extensive and high-quality patient data to enhance outcome predictions and treatment personalization. Nonetheless, due to privacy and security concerns, the secondary purpose of EHR data is consistently governed and regulated, primarily for research intentions, thereby constraining researchers' access to EHR data. Generating synthetic EHR data with deep learning methods is a viable and promising approach to mitigate privacy concerns, offering not only a supplementary resource for downstream applications but also sidestepping the confidentiality risks associated with real patient data. While prior efforts have concentrated on EHR data synthesis, significant challenges persist in the domain of generating synthetic EHR data: balancing the heterogeneity of real EHR including temporal and non-temporal features, addressing the missing values and irregular measures, and ensuring the privacy of the real data used for model training. Existing works in this domain only focused on solving one or two aforementioned challenges. In this work, we propose IGAMT, an innovative framework to generate privacy-preserved synthetic EHR data that not only maintain high quality with heterogeneous features, missing values, and irregular measures but also balances the privacy-utility trade-off. Extensive experiments prove that IGAMT significantly outperforms baseline architectures in terms of visual resemblance and comparable performance in downstream applications. Ablation case studies also prove the effectiveness of the techniques applied in IGAMT",
    "checked": true,
    "id": "1ab4b85af8960084550bc31f68b88f5ca832c5ed",
    "semantic_title": "igamt: privacy-preserving electronic health record synthesization with heterogeneity and irregularity",
    "citation_count": 1,
    "authors": [
      "Wenjie Wang",
      "Pengfei Tang",
      "Jian Lou",
      "Yuanming Shao",
      "Lance Waller",
      "Yi-an Ko",
      "Li Xiong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29492": {
    "title": "Decoupled Training: Return of Frustratingly Easy Multi-Domain Learning",
    "volume": "main",
    "abstract": "Multi-domain learning (MDL) aims to train a model with minimal average risk across multiple overlapping but non-identical domains. To tackle the challenges of dataset bias and domain domination, numerous MDL approaches have been proposed from the perspectives of seeking commonalities by aligning distributions to reduce domain gap or reserving differences by implementing domain-specific towers, gates, and even experts. MDL models are becoming more and more complex with sophisticated network architectures or loss functions, introducing extra parameters and enlarging computation costs. In this paper, we propose a frustratingly easy and hyperparameter-free multi-domain learning method named Decoupled Training (D-Train). D-Train is a tri-phase general-to-specific training strategy that first pre-trains on all domains to warm up a root model, then post-trains on each domain by splitting into multi-heads, and finally fine-tunes the heads by fixing the backbone, enabling decouple training to achieve domain independence. Despite its extraordinary simplicity and efficiency, D-Train performs remarkably well in extensive evaluations of various datasets from standard benchmarks to applications of satellite imagery and recommender systems",
    "checked": true,
    "id": "e3ed18bd48aab391a4ce76352828c8fd456152e2",
    "semantic_title": "decoupled training: return of frustratingly easy multi-domain learning",
    "citation_count": 1,
    "authors": [
      "Ximei Wang",
      "Junwei Pan",
      "Xingzhuo Guo",
      "Dapeng Liu",
      "Jie Jiang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29493": {
    "title": "Probability-Polarized Optimal Transport for Unsupervised Domain Adaptation",
    "volume": "main",
    "abstract": "Optimal transport (OT) is an important methodology to measure distribution discrepancy, which has achieved promising performance in artificial intelligence applications, e.g., unsupervised domain adaptation. However, from the view of transportation, there are still limitations: 1) the local discriminative structures for downstream tasks, e.g., cluster structure for classification, cannot be explicitly admitted by the learned OT plan; 2) the entropy regularization induces a dense OT plan with increasing uncertainty. To tackle these issues, we propose a novel Probability-Polarized OT (PPOT) framework, which can characterize the structure of OT plan explicitly. Specifically, the probability polarization mechanism is proposed to guide the optimization direction of OT plan, which generates a clear margin between similar and dissimilar transport pairs and reduces the uncertainty. Further, a dynamic mechanism for margin is developed by incorporating task-related information into the polarization, which directly captures the intra/inter class correspondence for knowledge transportation. A mathematical understanding for PPOT is provided from the view of gradient, which ensures interpretability. Extensive experiments on several datasets validate the effectiveness and empirical efficiency of PPOT",
    "checked": true,
    "id": "5b0061aab096174e374ead748028156da1c9de10",
    "semantic_title": "probability-polarized optimal transport for unsupervised domain adaptation",
    "citation_count": 0,
    "authors": [
      "Yan Wang",
      "Chuan-Xian Ren",
      "Yi-Ming Zhai",
      "You-Wei Luo",
      "Hong Yan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29494": {
    "title": "Limited-Supervised Multi-Label Learning with Dependency Noise",
    "volume": "main",
    "abstract": "Limited-supervised multi-label learning (LML) leverages weak or noisy supervision for multi-label classification model training over data with label noise, which contain missing labels and/or redundant labels. Existing studies usually solve LML problems by assuming that label noise is independent of the input features and class labels, while ignoring the fact that noisy labels may depend on the input features (instance-dependent) and the classes (label-dependent) in many real-world applications. In this paper, we propose limited-supervised Multi-label Learning with Dependency Noise (MLDN) to simultaneously identify the instance-dependent and label-dependent label noise by factorizing the noise matrix as the outputs of a mapping from the feature and label representations. Meanwhile, we regularize the problem with the manifold constraint on noise matrix to preserve local relationships and uncover the manifold structure. Theoretically, we bound noise recover error for the resulting problem. We solve the problem by using a first-order scheme based on proximal operator, and the convergence rate of it is at least sub-linear. Extensive experiments conducted on various datasets demonstrate the superiority of our proposed method",
    "checked": true,
    "id": "3b1a170e47e60aac04958f394bf81f942214e6f1",
    "semantic_title": "limited-supervised multi-label learning with dependency noise",
    "citation_count": 0,
    "authors": [
      "Yejiang Wang",
      "Yuhai Zhao",
      "Zhengkui Wang",
      "Wen Shan",
      "Xingwei Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29495": {
    "title": "Non-stationary Projection-Free Online Learning with Dynamic and Adaptive Regret Guarantees",
    "volume": "main",
    "abstract": "Projection-free online learning has drawn increasing interest due to its efficiency in solving high-dimensional problems with complicated constraints. However, most existing projection-free online methods focus on minimizing the static regret, which unfortunately fails to capture the challenge of changing environments. In this paper, we investigate non-stationary projection-free online learning, and choose dynamic regret and adaptive regret to measure the performance. Specifically, we first provide a novel dynamic regret analysis for an existing projection-free method named BOGD_IP, and establish an O(T^¾ (1+P_T)) dynamic regret bound, where P_T denotes the path-length of the comparator sequence. Then, we improve the upper bound to O(T^¾ (1+P_T)^¼) by running multiple BOGD_IP algorithms with different step sizes in parallel, and tracking the best one on the fly. Our results are the first general-case dynamic regret bounds for projection-free online learning, and can recover the existing O(T^¾) static regret by setting P_T = 0. Furthermore, we propose a projection-free method to attain an O(?^¾) adaptive regret bound for any interval with length ?, which nearly matches the static regret over that interval. The essential idea is to maintain a set of BOGD_IP algorithms dynamically, and combine them by a meta algorithm. Moreover, we demonstrate that it is also equipped with an O(T^¾ (1+P_T)^¼) dynamic regret bound. Finally, empirical studies verify our theoretical findings",
    "checked": true,
    "id": "d6898cf1c95fa0d6cf69a8be6c3eb02b653357e6",
    "semantic_title": "non-stationary projection-free online learning with dynamic and adaptive regret guarantees",
    "citation_count": 5,
    "authors": [
      "Yibo Wang",
      "Wenhao Yang",
      "Wei Jiang",
      "Shiyin Lu",
      "Bing Wang",
      "Haihong Tang",
      "Yuanyu Wan",
      "Lijun Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29496": {
    "title": "Wavelet Dynamic Selection Network for Inertial Sensor Signal Enhancement",
    "volume": "main",
    "abstract": "As attitude and motion sensing components, inertial sensors are widely used in various portable devices, covering consumer electronics, sports health, aerospace, etc. But the severe intrinsic errors of inertial sensors heavily restrain their function implementation, especially the advanced functionality, including motion trajectory recovery and motion semantic recognition, which attracts considerable attention. As a mainstream signal processing method, wavelet is hailed as the mathematical microscope of signal due to the plentiful and diverse wavelet basis functions. However, complicated noise types and application scenarios of inertial sensors make selecting wavelet basis perplexing. To this end, we propose a wavelet dynamic selection network (WDSNet), which intelligently selects the appropriate wavelet basis for variable inertial signals. In addition, existing deep learning architectures excel at extracting features from input data but neglect to learn the characteristics of target categories, which is essential to enhance the category awareness capability, thereby improving the selection of wavelet basis. Therefore, we propose a category representation mechanism (CRM), which enables the network to extract and represent category features without increasing trainable parameters. Furthermore, CRM transforms the common fully connected network into category representations, which provide closer supervision to the feature extractor than the far and trivial one-hot classification labels. We call this process of imposing interpretability on a network and using it to supervise the feature extractor the feature supervision mechanism, and its effectiveness is demonstrated experimentally and theoretically in this paper. The enhanced inertial signal can perform impracticable tasks with regard to the original signal, such as trajectory reconstruction. Both quantitative and visual results show that WDSNet outperforms the existing methods. Remarkably, WDSNet, as a weakly-supervised method, achieves the state-of-the-art performance of all the compared fully-supervised methods",
    "checked": true,
    "id": "39af48ea700dd0387147a85303a4def12d6345fa",
    "semantic_title": "wavelet dynamic selection network for inertial sensor signal enhancement",
    "citation_count": 0,
    "authors": [
      "Yifeng Wang",
      "Yi Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29497": {
    "title": "Lost Domain Generalization Is a Natural Consequence of Lack of Training Domains",
    "volume": "main",
    "abstract": "We show a hardness result for the number of training domains required to achieve a small population error in the test domain. Although many domain generalization algorithms have been developed under various domain-invariance assumptions, there is significant evidence to indicate that out-of-distribution (o.o.d.) test accuracy of state-of-the-art o.o.d. algorithms is on par with empirical risk minimization and random guess on the domain generalization benchmarks such as DomainBed. In this work, we analyze its cause and attribute the lost domain generalization to the lack of training domains. We show that, in a minimax lower bound fashion, any learning algorithm that outputs a classifier with an ε excess error to the Bayes optimal classifier requires at least poly(1/ε) number of training domains, even though the number of training data sampled from each training domain is large. Experiments on the DomainBed benchmark demonstrate that o.o.d. test accuracy is monotonically increasing as the number of training domains increases. Our result sheds light on the intrinsic hardness of domain generalization and suggests benchmarking o.o.d. algorithms by the datasets with a sufficient number of training domains",
    "checked": true,
    "id": "56ca51fdce18e23ce65d858805a847b450ebbc55",
    "semantic_title": "lost domain generalization is a natural consequence of lack of training domains",
    "citation_count": 1,
    "authors": [
      "Yimu Wang",
      "Yihan Wu",
      "Hongyang Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29498": {
    "title": "Semi-supervised Learning of Dynamical Systems with Neural Ordinary Differential Equations: A Teacher-Student Model Approach",
    "volume": "main",
    "abstract": "Modeling dynamical systems is crucial for a wide range of tasks, but it remains challenging due to complex nonlinear dynamics, limited observations, or lack of prior knowledge. Recently, data-driven approaches such as Neural Ordinary Differential Equations (NODE) have shown promising results by leveraging the expressive power of neural networks to model unknown dynamics. However, these approaches often suffer from limited labeled training data, leading to poor generalization and suboptimal predictions. On the other hand, semi-supervised algorithms can utilize abundant unlabeled data and have demonstrated good performance in classification and regression tasks. We propose TS-NODE, the first semi-supervised approach to modeling dynamical systems with NODE. TS-NODE explores cheaply generated synthetic pseudo rollouts to broaden exploration in the state space and to tackle the challenges brought by lack of ground-truth system data under a teacher-student model. TS-NODE employs an unified optimization framework that corrects the teacher model based on the student's feedback while mitigating the potential false system dynamics present in pseudo rollouts. TS-NODE demonstrates significant performance improvements over a baseline Neural ODE model on multiple dynamical system modeling tasks",
    "checked": true,
    "id": "45c5b54d2bdb0a694fae923ad367cb39ae6571e3",
    "semantic_title": "semi-supervised learning of dynamical systems with neural ordinary differential equations: a teacher-student model approach",
    "citation_count": 0,
    "authors": [
      "Yu Wang",
      "Yuxuan Yin",
      "Karthik Somayaji NS",
      "Ján Drgoňa",
      "Malachi Schram",
      "Mahantesh Halappanavar",
      "Frank Liu",
      "Peng Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29499": {
    "title": "Critic-Guided Decision Transformer for Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "Recent advancements in offline reinforcement learning (RL) have underscored the capabilities of Return-Conditioned Supervised Learning (RCSL), a paradigm that learns the action distribution based on target returns for each state in a supervised manner. However, prevailing RCSL methods largely focus on deterministic trajectory modeling, disregarding stochastic state transitions and the diversity of future trajectory distributions. A fundamental challenge arises from the inconsistency between the sampled returns within individual trajectories and the expected returns across multiple trajectories. Fortunately, value-based methods offer a solution by leveraging a value function to approximate the expected returns, thereby addressing the inconsistency effectively. Building upon these insights, we propose a novel approach, termed the Critic-Guided Decision Transformer (CGDT), which combines the predictability of long-term returns from value-based methods with the trajectory modeling capability of the Decision Transformer. By incorporating a learned value function, known as the critic, CGDT ensures a direct alignment between the specified target returns and the expected returns of actions. This integration bridges the gap between the deterministic nature of RCSL and the probabilistic characteristics of value-based methods. Empirical evaluations on stochastic environments and D4RL benchmark datasets demonstrate the superiority of CGDT over traditional RCSL methods. These results highlight the potential of CGDT to advance the state of the art in offline RL and extend the applicability of RCSL to a wide range of RL tasks",
    "checked": true,
    "id": "73cb437af59d76e860f91e00dbf95cd3ad0908fd",
    "semantic_title": "critic-guided decision transformer for offline reinforcement learning",
    "citation_count": 4,
    "authors": [
      "Yuanfu Wang",
      "Chao Yang",
      "Ying Wen",
      "Yu Liu",
      "Yu Qiao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29500": {
    "title": "Fully-Connected Spatial-Temporal Graph for Multivariate Time-Series Data",
    "volume": "main",
    "abstract": "Multivariate Time-Series (MTS) data is crucial in various application fields. With its sequential and multi-source (multiple sensors) properties, MTS data inherently exhibits Spatial-Temporal (ST) dependencies, involving temporal correlations between timestamps and spatial correlations between sensors in each timestamp. To effectively leverage this information, Graph Neural Network-based methods (GNNs) have been widely adopted. However, existing approaches separately capture spatial dependency and temporal dependency and fail to capture the correlations between Different sEnsors at Different Timestamps (DEDT). Overlooking such correlations hinders the comprehensive modelling of ST dependencies within MTS data, thus restricting existing GNNs from learning effective representations. To address this limitation, we propose a novel method called Fully-Connected Spatial-Temporal Graph Neural Network (FC-STGNN), including two key components namely FC graph construction and FC graph convolution. For graph construction, we design a decay graph to connect sensors across all timestamps based on their temporal distances, enabling us to fully model the ST dependencies by considering the correlations between DEDT. Further, we devise FC graph convolution with a moving-pooling GNN layer to effectively capture the ST dependencies for learning effective representations. Extensive experiments show the effectiveness of FC-STGNN on multiple MTS datasets compared to SOTA methods. The code is available at https://github.com/Frank-Wang-oss/FCSTGNN",
    "checked": false,
    "id": "22c0e041e205364a67b318a00f949b9d60c3da11",
    "semantic_title": "fully-connected spatial-temporal graph for multivariate time series data",
    "citation_count": 1,
    "authors": [
      "Yucheng Wang",
      "Yuecong Xu",
      "Jianfei Yang",
      "Min Wu",
      "Xiaoli Li",
      "Lihua Xie",
      "Zhenghua Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29501": {
    "title": "Graph-Aware Contrasting for Multivariate Time-Series Classification",
    "volume": "main",
    "abstract": "Contrastive learning, as a self-supervised learning paradigm, becomes popular for Multivariate Time-Series (MTS) classification. It ensures the consistency across different views of unlabeled samples and then learns effective representations for these samples. Existing contrastive learning methods mainly focus on achieving temporal consistency with temporal augmentation and contrasting techniques, aiming to preserve temporal patterns against perturbations for MTS data. However, they overlook spatial consistency that requires the stability of individual sensors and their correlations. As MTS data typically originate from multiple sensors, ensuring spatial consistency becomes essential for the overall performance of contrastive learning on MTS data. Thus, we propose Graph-Aware Contrasting for spatial consistency across MTS data. Specifically, we propose graph augmentations including node and edge augmentations to preserve the stability of sensors and their correlations, followed by graph contrasting with both node- and graph-level contrasting to extract robust sensor- and global-level features. We further introduce multi-window temporal contrasting to ensure temporal consistency in the data for each sensor. Extensive experiments demonstrate that our proposed method achieves state-of-the-art performance on various MTS classification tasks. The code is available at https://github.com/Frank-Wang-oss/TS-GAC",
    "checked": true,
    "id": "0b4ecb5a107cbb2ddb736f50b8bd040f0abbe89d",
    "semantic_title": "graph-aware contrasting for multivariate time-series classification",
    "citation_count": 2,
    "authors": [
      "Yucheng Wang",
      "Yuecong Xu",
      "Jianfei Yang",
      "Min Wu",
      "Xiaoli Li",
      "Lihua Xie",
      "Zhenghua Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29502": {
    "title": "Superposed Atomic Representation for Robust High-Dimensional Data Recovery of Multiple Low-Dimensional Structures",
    "volume": "main",
    "abstract": "This paper proposes a unified Superposed Atomic Representation (SAR) framework for high-dimensional data recovery with multiple low-dimensional structures. The data can be in various forms ranging from vectors to tensors. The goal of SAR is to recover different components from their sum, where each component has a low-dimensional structure, such as sparsity, low-rankness or be lying a low-dimensional subspace. Examples of SAR include, but not limited to, Robust Sparse Representation (RSR), Robust Principal Component Analysis (RPCA), Tensor RPCA (TRPCA), and Outlier Pursuit (OP). We establish the theoretical guarantee for SAR. To further improve SAR, we also develop a Weighted SAR (WSAR) framework by paying more attention and penalizing less on significant atoms of each component. An effective optimization algorithm is devised for WSAR and the convergence of the algorithm is rigorously proved. By leveraging WSAR as a general platform, several new methods are proposed for high-dimensional data recovery. The experiments on real data demonstrate the superiority of WSAR for various data recovery problems",
    "checked": true,
    "id": "cef1156d8345fb81855a0aff5db9475b47d04fc8",
    "semantic_title": "superposed atomic representation for robust high-dimensional data recovery of multiple low-dimensional structures",
    "citation_count": 0,
    "authors": [
      "Yulong Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29503": {
    "title": "Consistency-GAN: Training GANs with Consistency Model",
    "volume": "main",
    "abstract": "For generative learning tasks, there are three crucial criteria for generating samples from the models: quality, coverage/diversity, and sampling speed. Among the existing generative models, Generative adversarial networks (GANs) and diffusion models demonstrate outstanding quality performance while suffering from notable limitations. GANs can generate high-quality results and enable fast sampling, their drawbacks, however, lie in the limited diversity of the generated samples. On the other hand, diffusion models excel at generating high-quality results with a commendable diversity. Yet, its iterative generation process necessitates hundreds to thousands of sampling steps, leading to slow speeds that are impractical for real-time scenarios. To address the aforementioned problem, this paper proposes a novel Consistency-GAN model. In particular, to aid in the training of the GAN, we introduce instance noise, which employs consistency models using only a few steps compared to the conventional diffusion process. Our evaluations on various datasets indicate that our approach significantly accelerates sampling speeds compared to traditional diffusion models, while preserving sample quality and diversity. Furthermore, our approach also has better model coverage than traditional adversarial training methods",
    "checked": true,
    "id": "f3ce0a068f34a2d2a0d8301e06347f0cde0df5ea",
    "semantic_title": "consistency-gan: training gans with consistency model",
    "citation_count": 1,
    "authors": [
      "Yunpeng Wang",
      "Meng Pang",
      "Shengbo Chen",
      "Hong Rao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29504": {
    "title": "DRF: Improving Certified Robustness via Distributional Robustness Framework",
    "volume": "main",
    "abstract": "Randomized smoothing (RS) has provided state-of-the-art (SOTA) certified robustness against adversarial perturbations for large neural networks. Among studies in this field, methods based on adversarial training (AT) achieve remarkably robust performance by applying adversarial examples to construct the smoothed classifier. These AT-based RS methods typically seek a pointwise adversary that generates the worst-case adversarial examples by perturbing each input independently. However, there are unexplored benefits to considering such adversarial robustness across the entire data distribution. To this end, we provide a novel framework called DRF, which connects AT-based RS methods with distributional robustness (DR), and show that these methods are special cases of their counterparts in our framework. Due to the advantages conferred by DR, our framework can control the trade-off between the clean accuracy and certified robustness of smoothed classifiers to a significant extent. Our experiments demonstrate that DRF can substantially improve the certified robustness of AT-based RS",
    "checked": true,
    "id": "5718861a2b5b8a40879f96a550dd132267b49886",
    "semantic_title": "drf: improving certified robustness via distributional robustness framework",
    "citation_count": 1,
    "authors": [
      "Zekai Wang",
      "Zhengyu Zhou",
      "Weiwei Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29505": {
    "title": "BVT-IMA: Binary Vision Transformer with Information-Modified Attention",
    "volume": "main",
    "abstract": "As a compression method that can significantly reduce the cost of calculations and memories, model binarization has been extensively studied in convolutional neural networks. However, the recently popular vision transformer models pose new challenges to such a technique, in which the binarized models suffer from serious performance drops. In this paper, an attention shifting is observed in the binary multi-head self-attention module, which can influence the information fusion between tokens and thus hurts the model performance. From the perspective of information theory, we find a correlation between attention scores and the information quantity, further indicating that a reason for such a phenomenon may be the loss of the information quantity induced by constant moduli of binarized tokens. Finally, we reveal the information quantity hidden in the attention maps of binary vision transformers and propose a simple approach to modify the attention values with look-up information tables so that improve the model performance. Extensive experiments on CIFAR-100/TinyImageNet/ImageNet-1k demonstrate the effectiveness of the proposed information-modified attention on binary vision transformers",
    "checked": true,
    "id": "a3e7cbba2dcfca089e96afb9b31e91a0ba019e07",
    "semantic_title": "bvt-ima: binary vision transformer with information-modified attention",
    "citation_count": 0,
    "authors": [
      "Zhenyu Wang",
      "Hao Luo",
      "Xuemei Xie",
      "Fan Wang",
      "Guangming Shi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29506": {
    "title": "Stealthy Adversarial Attacks on Stochastic Multi-Armed Bandits",
    "volume": "main",
    "abstract": "Adversarial attacks against stochastic multi-armed bandit (MAB) algorithms have been extensively studied in the literature. In this work, we focus on reward poisoning attacks and find most existing attacks can be easily detected by our proposed detection method based on the test of homogeneity, due to their aggressive nature in reward manipulations. This motivates us to study the notion of stealthy attack against stochastic MABs and investigate the resulting attackability. Our analysis shows that against two popularly employed MAB algorithms, UCB1 and $\\epsilon$-greedy, the success of a stealthy attack depends on the environmental conditions and the realized reward of the arm pulled in the first round. We also analyze the situation for general MAB algorithms equipped with our attack detection method and find that it is possible to have a stealthy attack that almost always succeeds. This brings new insights into the security risks of MAB algorithms",
    "checked": true,
    "id": "1f89f64aa95ce5f404bd7f77505aaff9ce80c359",
    "semantic_title": "stealthy adversarial attacks on stochastic multi-armed bandits",
    "citation_count": 0,
    "authors": [
      "Zhiwei Wang",
      "Huazheng Wang",
      "Hongning Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29507": {
    "title": "Building Minimal and Reusable Causal State Abstractions for Reinforcement Learning",
    "volume": "main",
    "abstract": "Two desiderata of reinforcement learning (RL) algorithms are the ability to learn from relatively little experience and the ability to learn policies that generalize to a range of problem specifications. In factored state spaces, one approach towards achieving both goals is to learn state abstractions, which only keep the necessary variables for learning the tasks at hand. This paper introduces Causal Bisimulation Modeling (CBM), a method that learns the causal relationships in the dynamics and reward functions for each task to derive a minimal, task-specific abstraction. CBM leverages and improves implicit modeling to train a high-fidelity causal dynamics model that can be reused for all tasks in the same environment. Empirical validation on two manipulation environments and four tasks reveals that CBM's learned implicit dynamics models identify the underlying causal relationships and state abstractions more accurately than explicit ones. Furthermore, the derived state abstractions allow a task learner to achieve near-oracle levels of sample efficiency and outperform baselines on all tasks",
    "checked": true,
    "id": "1aa4d9eb8aa15b22a8d75b0bfa569c09b0c73443",
    "semantic_title": "building minimal and reusable causal state abstractions for reinforcement learning",
    "citation_count": 2,
    "authors": [
      "Zizhao Wang",
      "Caroline Wang",
      "Xuesu Xiao",
      "Yuke Zhu",
      "Peter Stone"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29508": {
    "title": "EAT: Towards Long-Tailed Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "Despite recent advancements in out-of-distribution (OOD) detection, most current studies assume a class-balanced in-distribution training dataset, which is rarely the case in real-world scenarios. This paper addresses the challenging task of long-tailed OOD detection, where the in-distribution data follows a long-tailed class distribution. The main difficulty lies in distinguishing OOD data from samples belonging to the tail classes, as the ability of a classifier to detect OOD instances is not strongly correlated with its accuracy on the in-distribution classes. To overcome this issue, we propose two simple ideas: (1) Expanding the in-distribution class space by introducing multiple abstention classes. This approach allows us to build a detector with clear decision boundaries by training on OOD data using virtual labels. (2) Augmenting the context-limited tail classes by overlaying images onto the context-rich OOD data. This technique encourages the model to pay more attention to the discriminative features of the tail classes. We provide a clue for separating in-distribution and OOD data by analyzing gradient noise. Through extensive experiments, we demonstrate that our method outperforms the current state-of-the-art on various benchmark datasets. Moreover, our method can be used as an add-on for existing long-tail learning approaches, significantly enhancing their OOD detection performance. Code is available at: https://github.com/Stomach-ache/Long-Tailed-OOD-Detection",
    "checked": true,
    "id": "7ea7ff3ab79705d3b7336ef9243b7c81d3b003ba",
    "semantic_title": "eat: towards long-tailed out-of-distribution detection",
    "citation_count": 1,
    "authors": [
      "Tong Wei",
      "Bo-Lin Wang",
      "Min-Ling Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29509": {
    "title": "Levenshtein Distance Embedding with Poisson Regression for DNA Storage",
    "volume": "main",
    "abstract": "Efficient computation or approximation of Levenshtein distance, a widely-used metric for evaluating sequence similarity, has attracted significant attention with the emergence of DNA storage and other biological applications. Sequence embedding, which maps Levenshtein distance to a conventional distance between embedding vectors, has emerged as a promising solution. In this paper, a novel neural network-based sequence embedding technique using Poisson regression is proposed. We first provide a theoretical analysis of the impact of embedding dimension on model performance and present a criterion for selecting an appropriate embedding dimension. Under this embedding dimension, the Poisson regression is introduced by assuming the Levenshtein distance between sequences of fixed length following a Poisson distribution, which naturally aligns with the definition of Levenshtein distance. Moreover, from the perspective of the distribution of embedding distances, Poisson regression approximates the negative log likelihood of the chi-squared distribution and offers advancements in removing the skewness. Through comprehensive experiments on real DNA storage data, we demonstrate the superior performance of the proposed method compared to state-of-the-art approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Wei",
      "Alan J.X. Guo",
      "Sihan Sun",
      "Mengyi Wei",
      "Wei Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29510": {
    "title": "Multi-Source Collaborative Gradient Discrepancy Minimization for Federated Domain Generalization",
    "volume": "main",
    "abstract": "Federated Domain Generalization aims to learn a domain-invariant model from multiple decentralized source domains for deployment on unseen target domain. Due to privacy concerns, the data from different source domains are kept isolated, which poses challenges in bridging the domain gap. To address this issue, we propose a Multi-source Collaborative Gradient Discrepancy Minimization (MCGDM) method for federated domain generalization. Specifically, we propose intra-domain gradient matching between the original images and augmented images to avoid overfitting the domain-specific information within isolated domains. Additionally, we propose inter-domain gradient matching with the collaboration of other domains, which can further reduce the domain shift across decentralized domains. Combining intra-domain and inter-domain gradient matching, our method enables the learned model to generalize well on unseen domains. Furthermore, our method can be extended to the federated domain adaptation task by fine-tuning the target model on the pseudo-labeled target domain. The extensive experiments on federated domain generalization and adaptation indicate that our method outperforms the state-of-the-art methods significantly",
    "checked": true,
    "id": "718517690113db22107f2d072598c66cd1a92175",
    "semantic_title": "multi-source collaborative gradient discrepancy minimization for federated domain generalization",
    "citation_count": 2,
    "authors": [
      "Yikang Wei",
      "Yahong Han"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29511": {
    "title": "Auto-Prox: Training-Free Vision Transformer Architecture Search via Automatic Proxy Discovery",
    "volume": "main",
    "abstract": "The substantial success of Vision Transformer (ViT) in computer vision tasks is largely attributed to the architecture design. This underscores the necessity of efficient architecture search for designing better ViTs automatically. As training-based architecture search methods are computationally intensive, there's a growing interest in training-free methods that use zero-cost proxies to score ViTs. However, existing training-free approaches require expert knowledge to manually design specific zero-cost proxies. Moreover, these zero-cost proxies exhibit limitations to generalize across diverse domains. In this paper, we introduce Auto-Prox, an automatic proxy discovery framework, to address the problem. First, we build the ViT-Bench-101, which involves different ViT candidates and their actual performance on multiple datasets. Utilizing ViT-Bench-101, we can evaluate zero-cost proxies based on their score-accuracy correlation. Then, we represent zero-cost proxies with computation graphs and organize the zero-cost proxy search space with ViT statistics and primitive operations. To discover generic zero-cost proxies, we propose a joint correlation metric to evolve and mutate different zero-cost proxy candidates. We introduce an elitism-preserve strategy for search efficiency to achieve a better trade-off between exploitation and exploration. Based on the discovered zero-cost proxy, we conduct a ViT architecture search in a training-free manner. Extensive experiments demonstrate that our method generalizes well to different datasets and achieves state-of-the-art results both in ranking correlation and final accuracy. Codes can be found at https://github.com/lilujunai/Auto-Prox-AAAI24",
    "checked": true,
    "id": "c6da4aa9674076d17f343eb5a6713eef35a9764f",
    "semantic_title": "auto-prox: training-free vision transformer architecture search via automatic proxy discovery",
    "citation_count": 1,
    "authors": [
      "Zimian Wei",
      "Peijie Dong",
      "Zheng Hui",
      "Anggeng Li",
      "Lujun Li",
      "Menglong Lu",
      "Hengyue  Pan",
      "Dongsheng Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29512": {
    "title": "Hyperbolic Graph Diffusion Model",
    "volume": "main",
    "abstract": "Diffusion generative models (DMs) have achieved promising results in image and graph generation. However, real-world graphs, such as social networks, molecular graphs, and traffic graphs, generally share non-Euclidean topologies and hidden hierarchies. For example, the degree distributions of graphs are mostly power-law distributions. The current latent diffusion model embeds the hierarchical data in a Euclidean space, which leads to distortions and interferes with modeling the distribution. Instead, hyperbolic space has been found to be more suitable for capturing complex hierarchical structures due to its exponential growth property. In order to simultaneously utilize the data generation capabilities of diffusion models and the ability of hyperbolic embeddings to extract latent hierarchical distributions, we propose a novel graph generation method called, Hyperbolic Graph Diffusion Model (HGDM), which consists of an auto-encoder to encode nodes into successive hyperbolic embeddings, and a DM that operates in the hyperbolic latent space. HGDM captures the crucial graph structure distributions by constructing a hyperbolic potential node space that incorporates edge information. Extensive experiments show that HGDM achieves better performance in generic graph and molecule generation benchmarks, with a 48% improvement in the quality of graph generation with highly hierarchical structures",
    "checked": true,
    "id": "9f97f6beeb8d3c3330dffd661a6e5c6f8995245f",
    "semantic_title": "hyperbolic graph diffusion model",
    "citation_count": 1,
    "authors": [
      "Lingfeng Wen",
      "Xuan Tang",
      "Mingjie Ouyang",
      "Xiangxiang Shen",
      "Jian Yang",
      "Daxin Zhu",
      "Mingsong Chen",
      "Xian Wei"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29513": {
    "title": "Communication Efficient Distributed Newton Method over Unreliable Networks",
    "volume": "main",
    "abstract": "Distributed optimization in resource constrained devices demands both communication efficiency and fast convergence rates. Newton-type methods are getting preferable due to their superior convergence rates compared to the first-order methods. In this paper, we study a new problem in regard to the second-order distributed optimization over unreliable networks. The working devices are power-limited or operate in unfavorable wireless channels, experiencing packet losses during their uplink transmission to the server. Our scenario is very common in real-world and leads to instability of classical distributed optimization methods especially the second-order methods because of their sensitivity to the imprecision of local Hessian matrices. To achieve robustness to high packet loss, communication efficiency and fast convergence rates, we propose a novel distributed second-order method, called RED-New (Packet loss Resilient Distributed Approximate Newton). Each iteration of RED-New comprises two rounds of light-weight and lossy transmissions, in which the server aggregates the local information with a new developed scaling strategy. We prove the linear-quadratic convergence rate of RED-New. Experimental results demonstrate its advantage over first-order and second-order baselines, and its tolerance to packet loss rate ranging from 5% to 40%",
    "checked": true,
    "id": "017cb35ed83adc1b2b8b6a585cf15ce18a2d2b43",
    "semantic_title": "communication efficient distributed newton method over unreliable networks",
    "citation_count": 0,
    "authors": [
      "Ming Wen",
      "Chengchang Liu",
      "Yuedong Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29514": {
    "title": "Homophily-Related: Adaptive Hybrid Graph Filter for Multi-View Graph Clustering",
    "volume": "main",
    "abstract": "Recently there is a growing focus on graph data, and multi-view graph clustering has become a popular area of research interest. Most of the existing methods are only applicable to homophilous graphs, yet the extensive real-world graph data can hardly fulfill the homophily assumption, where the connected nodes tend to belong to the same class. Several studies have pointed out that the poor performance on heterophilous graphs is actually due to the fact that conventional graph neural networks (GNNs), which are essentially low-pass filters, discard information other than the low-frequency information on the graph. Nevertheless, on certain graphs, particularly heterophilous ones, neglecting high-frequency information and focusing solely on low-frequency information impedes the learning of node representations. To break this limitation, our motivation is to perform graph filtering that is closely related to the homophily degree of the given graph, with the aim of fully leveraging both low-frequency and high-frequency signals to learn distinguishable node embedding. In this work, we propose Adaptive Hybrid Graph Filter for Multi-View Graph Clustering (AHGFC). Specifically, a graph joint process and graph joint aggregation matrix are first designed by using the intrinsic node features and adjacency relationship, which makes the low and high-frequency signals on the graph more distinguishable. Then we design an adaptive hybrid graph filter that is related to the homophily degree, which learns the node embedding based on the graph joint aggregation matrix. After that, the node embedding of each view is weighted and fused into a consensus embedding for the downstream task. Experimental results show that our proposed model performs well on six datasets containing homophilous and heterophilous graphs",
    "checked": true,
    "id": "d271548071b8acd57def499bcc86cb9329d71e65",
    "semantic_title": "homophily-related: adaptive hybrid graph filter for multi-view graph clustering",
    "citation_count": 1,
    "authors": [
      "Zichen Wen",
      "Yawen Ling",
      "Yazhou Ren",
      "Tianyi Wu",
      "Jianpeng Chen",
      "Xiaorong Pu",
      "Zhifeng Hao",
      "Lifang He"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29515": {
    "title": "Reproduce, Replicate, Reevaluate. The Long but Safe Way to Extend Machine Learning Methods",
    "volume": "main",
    "abstract": "Reproducibility is a desirable property of scientific research. On the one hand, it increases confidence in results. On the other hand, reproducible results can be extended on a solid basis. In rapidly developing fields such as machine learning, the latter is particularly important to ensure the reliability of research. In this paper, we present a systematic approach to reproducing (using the available implementation), replicating (using an alternative implementation) and reevaluating (using different datasets) state-of-the-art experiments. This approach enables the early detection and correction of deficiencies and thus the development of more robust and transparent machine learning methods. We detail the independent reproduction, replication, and reevaluation of the initially published experiments with a method that we want to extend. For each step, we identify issues and draw lessons learned. We further discuss solutions that have proven effective in overcoming the encountered problems. This work can serve as a guide for further reproducibility studies and generally improve reproducibility in machine learning",
    "checked": true,
    "id": "628bbbebceb6a8bd1705e4228f6761e3e4340836",
    "semantic_title": "reproduce, replicate, reevaluate. the long but safe way to extend machine learning methods",
    "citation_count": 0,
    "authors": [
      "Luisa Werner",
      "Nabil Layaïda",
      "Pierre Genevès",
      "Jérôme Euzenat",
      "Damien Graux"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29516": {
    "title": "Robust Loss Functions for Training Decision Trees with Noisy Labels",
    "volume": "main",
    "abstract": "We consider training decision trees using noisily labeled data, focusing on loss functions that can lead to robust learning algorithms. Our contributions are threefold. First, we offer novel theoretical insights on the robustness of many existing loss functions in the context of decision tree learning. We show that some of the losses belong to a class of what we call conservative losses, and the conservative losses lead to an early stopping behavior during training and noise-tolerant predictions during testing. Second, we introduce a framework for constructing robust loss functions, called distribution losses. These losses apply percentile-based penalties based on an assumed margin distribution, and they naturally allow adapting to different noise rates via a robustness parameter. In particular, we introduce a new loss called the negative exponential loss, which leads to an efficient greedy impurity-reduction learning algorithm. Lastly, our experiments on multiple datasets and noise settings validate our theoretical insight and the effectiveness of our adaptive negative exponential loss",
    "checked": true,
    "id": "eb3c6ecb269e39ea0f53e1402de88fb75e6a4d8c",
    "semantic_title": "robust loss functions for training decision trees with noisy labels",
    "citation_count": 0,
    "authors": [
      "Jonathan Wilton",
      "Nan Ye"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29517": {
    "title": "Neural Network Approximation for Pessimistic Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "Deep reinforcement learning (RL) has shown remarkable success in specific offline decision-making scenarios, yet its theoretical guarantees are still under development. Existing works on offline RL theory primarily emphasize a few trivial settings, such as linear MDP or general function approximation with strong assumptions and independent data, which lack guidance for practical use. The coupling of deep learning and Bellman residuals makes this problem challenging, in addition to the difficulty of data dependence. In this paper, we establish a non-asymptotic estimation error of pessimistic offline RL using general neural network approximation with C-mixing data regarding the structure of networks, the dimension of datasets, and the concentrability of data coverage, under mild assumptions. Our result shows that the estimation error consists of two parts: the first converges to zero at a desired rate on the sample size with partially controllable concentrability, and the second becomes negligible if the residual constraint is tight. This result demonstrates the explicit efficiency of deep adversarial offline RL frameworks. We utilize the empirical process tool for C-mixing sequences and the neural network approximation theory for the Holder class to achieve this. We also develop methods to bound the Bellman estimation error caused by function approximation with empirical Bellman constraint perturbations. Additionally, we present a result that lessens the curse of dimensionality using data with low intrinsic dimensionality and function classes with low complexity. Our estimation provides valuable insights into the development of deep offline RL and guidance for algorithm model design",
    "checked": true,
    "id": "d37a1b401a8d5a24431d1e1ef0cbb7b25f05704f",
    "semantic_title": "neural network approximation for pessimistic offline reinforcement learning",
    "citation_count": 0,
    "authors": [
      "Di Wu",
      "Yuling Jiao",
      "Li Shen",
      "Haizhao Yang",
      "Xiliang Lu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29518": {
    "title": "Solving Spectrum Unmixing as a Multi-Task Bayesian Inverse Problem with Latent Factors for Endmember Variability",
    "volume": "main",
    "abstract": "With the increasing customization of spectrometers, spectral unmixing has become a widely used technique in fields such as remote sensing, textiles, and environmental protection. However, endmember variability is a common issue for unmixing, where changes in lighting, atmospheric, temporal conditions, or the intrinsic spectral characteristics of materials, can all result in variations in the measured spectrum. Recent studies have employed deep neural networks to tackle endmember variability. However, these approaches rely on generic networks to implicitly resolve the issue, which struggles with the ill-posed nature and lack of effective convergence constraints for endmember variability. This paper proposes a streamlined multi-task learning model to rectify this problem, incorporating abundance regression and multi-label classification with Unmixing as a Bayesian Inverse Problem, denoted as BIPU. To address the issue of the ill-posed nature, the uncertainty of unmixing is quantified and minimized through the Laplace approximation in a Bayesian inverse solver. In addition, to improve convergence under the influence of endmember variability, the paper introduces two types of constraints. The first separates background factors of variants from the initial factors for each endmember, while the second identifies and eliminates the influence of non-existent endmembers via multi-label classification during convergence. The effectiveness of this model is demonstrated not only on a self-collected near-infrared spectral textile dataset (FENIR), but also on three commonly used remote sensing hyperspectral image datasets, where it achieves state-of-the-art unmixing performance and exhibits strong generalization capabilities",
    "checked": true,
    "id": "129bb2e7c46da1b837fb16432b7f977e34d188ab",
    "semantic_title": "solving spectrum unmixing as a multi-task bayesian inverse problem with latent factors for endmember variability",
    "citation_count": 0,
    "authors": [
      "Dong Wu",
      "Mingmin Chi",
      "Xuan Zang",
      "Bo Peng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29519": {
    "title": "Distilling Reliable Knowledge for Instance-Dependent Partial Label Learning",
    "volume": "main",
    "abstract": "Partial label learning (PLL) refers to the classification task where each training instance is ambiguously annotated with a set of candidate labels. Despite substantial advancements in tackling this challenge, limited attention has been devoted to a more specific and realistic setting, denoted as instance-dependent partial label learning (IDPLL). Within this contex, the assignment of partial labels depends on the distinct features of individual instances, rather than being random. In this paper, we initiate an exploration into a self-distillation framework for this problem, driven by the proven effectiveness and stability of this framework. Nonetheless, a crucial shortfall is identified: the foundational assumption central to IDPLL, involving what we term as partial label knowledge stipulating that candidate labels should exhibit superior confidence compared to non-candidates, is not fully upheld within the distillation process. To address this challenge, we introduce DIRK, a novel distillation approach that leverages a rectification process to DIstill Reliable Knowledge, while concurrently preserves informative fine-grained label confidence. In addition, to harness the rectified confidence to its fullest potential, we propose a knowledge-based representation refinement module, seamlessly integrated into the DIRK framework. This module effectively transmits the essence of similarity knowledge from the label space to the feature space, thereby amplifying representation learning and subsequently engendering marked improvements in model performance. Experiments and analysis on multiple datasets validate the rationality and superiority of our proposed approach",
    "checked": true,
    "id": "870552b15681a726a6e54165b427635d3557c653",
    "semantic_title": "distilling reliable knowledge for instance-dependent partial label learning",
    "citation_count": 0,
    "authors": [
      "Dong-Dong Wu",
      "Deng-Bao Wang",
      "Min-Ling Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29520": {
    "title": "OCEAN-MBRL: Offline Conservative Exploration for Model-Based Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "Model-based offline reinforcement learning (RL) algorithms have emerged as a promising paradigm for offline RL. These algorithms usually learn a dynamics model from a static dataset of transitions, use the model to generate synthetic trajectories, and perform conservative policy optimization within these trajectories. However, our observations indicate that policy optimization methods used in these model-based offline RL algorithms are not effective at exploring the learned model and induce biased exploration, which ultimately impairs the performance of the algorithm. To address this issue, we propose Offline Conservative ExplorAtioN (OCEAN), a novel rollout approach to model-based offline RL. In our method, we incorporate additional exploration techniques and introduce three conservative constraints based on uncertainty estimation to mitigate the potential impact of significant dynamic errors resulting from exploratory transitions. Our work is a plug-in method and can be combined with classical model-based RL algorithms, such as MOPO, COMBO, and RAMBO. Experiment results of our method on the D4RL MuJoCo benchmark show that OCEAN significantly improves the performance of existing algorithms",
    "checked": true,
    "id": "b44b0e8222021b51783c62b0bce071ef6fb3f12c",
    "semantic_title": "ocean-mbrl: offline conservative exploration for model-based offline reinforcement learning",
    "citation_count": 0,
    "authors": [
      "Fan Wu",
      "Rui Zhang",
      "Qi Yi",
      "Yunkai Gao",
      "Jiaming Guo",
      "Shaohui Peng",
      "Siming Lan",
      "Husheng Han",
      "Yansong Pan",
      "Kaizhao Yuan",
      "Pengwei Jin",
      "Ruizhi Chen",
      "Yunji Chen",
      "Ling Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29521": {
    "title": "Earthfarsser: Versatile Spatio-Temporal Dynamical Systems Modeling in One Model",
    "volume": "main",
    "abstract": "Efficiently modeling spatio-temporal (ST) physical processes and observations presents a challenging problem for the deep learning community. Many recent studies have concentrated on meticulously reconciling various advantages, leading to designed models that are neither simple nor practical. To address this issue, this paper presents a systematic study on existing shortcomings faced by off-the-shelf models, including lack of local fidelity, poor prediction performance over long time-steps, low scalability, and inefficiency. To systematically address the aforementioned problems, we propose an EarthFarseer, a concise framework that combines parallel local convolutions and global Fourier-based transformer architectures, enabling dynamically capture the local-global spatial interactions and dependencies. EarthFarseer also incorporates a multi-scale fully convolutional and Fourier architectures to efficiently and effectively capture the temporal evolution. Our proposal demonstrates strong adaptability across various tasks and datasets, with fast convergence and better local fidelity in long time-steps predictions. Extensive experiments and visualizations over eight human society physical and natural physical datasets demonstrates the state-of-the-art performance of EarthFarseer. We release our code at https://github.com/easylearningscores/EarthFarseer",
    "checked": true,
    "id": "64a046a9175276e6860a548f090efde51193e84f",
    "semantic_title": "earthfarsser: versatile spatio-temporal dynamical systems modeling in one model",
    "citation_count": 4,
    "authors": [
      "Hao Wu",
      "Yuxuan Liang",
      "Wei Xiong",
      "Zhengyang Zhou",
      "Wei Huang",
      "Shilong Wang",
      "Kun Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29522": {
    "title": "SafeAR: Safe Algorithmic Recourse by Risk-Aware Policies",
    "volume": "main",
    "abstract": "With the growing use of machine learning (ML) models in critical domains such as finance and healthcare, the need to offer recourse for those adversely affected by the decisions of ML models has become more important; individuals ought to be provided with recommendations on actions to take for improving their situation and thus receiving a favorable decision. Prior work on sequential algorithmic recourse---which recommends a series of changes---focuses on action feasibility and uses the proximity of feature changes to determine action costs. However, the uncertainties of feature changes and the risk of higher than average costs in recourse have not been considered. It is undesirable if a recourse could (with some probability) result in a worse situation from which recovery requires an extremely high cost. It is essential to incorporate risks when computing and evaluating recourse. We call the recourse computed with such risk considerations as Safe Algorithmic Recourse (SafeAR). The objective is to empower people to choose a recourse based on their risk tolerance. In this work, we discuss and show how existing recourse desiderata can fail to capture the risk of higher costs. We present a method to compute recourse policies that consider variability in cost and connect algorithmic recourse literature with risk-sensitive reinforcement learning. We also adopt measures \"Value at Risk\" and \"Conditional Value at Risk\" from the financial literature to summarize risk concisely. We apply our method to two real-world datasets and compare policies with different risk-aversion levels using risk measures and recourse desiderata (sparsity and proximity)",
    "checked": true,
    "id": "d8f3619fcaf1dea7907d6fe4aed39d8d8d266f97",
    "semantic_title": "safear: safe algorithmic recourse by risk-aware policies",
    "citation_count": 0,
    "authors": [
      "Haochen Wu",
      "Shubham Sharma",
      "Sunandita Patra",
      "Sriram Gopalakrishnan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29523": {
    "title": "SwitchTab: Switched Autoencoders Are Effective Tabular Learners",
    "volume": "main",
    "abstract": "Self-supervised representation learning methods have achieved significant success in computer vision and natural language processing (NLP), where data samples exhibit explicit spatial or semantic dependencies. However, applying these methods to tabular data is challenging due to the less pronounced dependencies among data samples. In this paper, we address this limitation by introducing SwitchTab, a novel self-supervised method specifically designed to capture latent dependencies in tabular data. SwitchTab leverages an asymmetric encoder-decoder framework to decouple mutual and salient features among data pairs, resulting in more representative embeddings. These embeddings, in turn, contribute to better decision boundaries and lead to improved results in downstream tasks. To validate the effectiveness of SwitchTab, we conduct extensive experiments across various domains involving tabular data. The results showcase superior performance in end-to-end prediction tasks with fine-tuning. Moreover, we demonstrate that pre-trained salient embeddings can be utilized as plug-and-play features to enhance the performance of various traditional classification methods (e.g., Logistic Regression, XGBoost, etc.). Lastly, we highlight the capability of SwitchTab to create explainable representations through visualization of decoupled mutual and salient features in the latent space",
    "checked": true,
    "id": "146efd9ed4ce53d9da48d302f95a8f597bc22b8b",
    "semantic_title": "switchtab: switched autoencoders are effective tabular learners",
    "citation_count": 14,
    "authors": [
      "Jing Wu",
      "Suiyao Chen",
      "Qi Zhao",
      "Renat Sergazinov",
      "Chen Li",
      "Shengjie Liu",
      "Chongchao Zhao",
      "Tianpei Xie",
      "Hanqing Guo",
      "Cheng Ji",
      "Daniel Cociorva",
      "Hakan Brunzell"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29524": {
    "title": "PORTAL: Automatic Curricula Generation for Multiagent Reinforcement Learning",
    "volume": "main",
    "abstract": "Despite many breakthroughs in recent years, it is still hard for MultiAgent Reinforcement Learning (MARL) algorithms to directly solve complex tasks in MultiAgent Systems (MASs) from scratch. In this work, we study how to use Automatic Curriculum Learning (ACL) to reduce the number of environmental interactions required to learn a good policy. In order to solve a difficult task, ACL methods automatically select a sequence of tasks (i.e., curricula). The idea is to obtain maximum learning progress towards the final task by continuously learning on tasks that match the current capabilities of the learners. The key question is how to measure the learning progress of the learner for better curriculum selection. We propose a novel ACL framework, PrOgRessive mulTiagent Automatic curricuLum (PORTAL), for MASs. PORTAL selects curricula according to two critera: 1) How difficult is a task, relative to the learners' current abilities? 2) How similar is a task, relative to the final task? By learning a shared feature space between tasks, PORTAL is able to characterize different tasks based on the distribution of features and select those that are similar to the final task. Also, the shared feature space can effectively facilitate the policy transfer between curricula. Experimental results show that PORTAL can train agents to master extremely hard cooperative tasks, which can not be achieved with previous state-of-the-art MARL algorithms",
    "checked": true,
    "id": "8121a4dc2daec112be196801a9e4ecb697a8d7dc",
    "semantic_title": "portal: automatic curricula generation for multiagent reinforcement learning",
    "citation_count": 2,
    "authors": [
      "Jizhou Wu",
      "Jianye Hao",
      "Tianpei Yang",
      "Xiaotian Hao",
      "Yan Zheng",
      "Weixun Wang",
      "Matthew E. Taylor"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29525": {
    "title": "FedA3I: Annotation Quality-Aware Aggregation for Federated Medical Image Segmentation against Heterogeneous Annotation Noise",
    "volume": "main",
    "abstract": "Federated learning (FL) has emerged as a promising paradigm for training segmentation models on decentralized medical data, owing to its privacy-preserving property. However, existing research overlooks the prevalent annotation noise encountered in real-world medical datasets, which limits the performance ceilings of FL. In this paper, we, for the first time, identify and tackle this problem. For problem formulation, we propose a contour evolution for modeling non-independent and identically distributed (Non-IID) noise across pixels within each client and then extend it to the case of multi-source data to form a heterogeneous noise model (i.e., Non-IID annotation noise across clients). For robust learning from annotations with such two-level Non-IID noise, we emphasize the importance of data quality in model aggregation, allowing high-quality clients to have a greater impact on FL. To achieve this, we propose Federated learning with Annotation quAlity-aware AggregatIon, named FedA3I, by introducing a quality factor based on client-wise noise estimation. Specifically, noise estimation at each client is accomplished through the Gaussian mixture model and then incorporated into model aggregation in a layer-wise manner to up-weight high-quality clients. Extensive experiments on two real-world medical image segmentation datasets demonstrate the superior performance of FedA3I against the state-of-the-art approaches in dealing with cross-client annotation noise. The code is available at https://github.com/wnn2000/FedAAAI",
    "checked": true,
    "id": "ca887f1a480d297d1a434c87096d951173e51341",
    "semantic_title": "feda3i: annotation quality-aware aggregation for federated medical image segmentation against heterogeneous annotation noise",
    "citation_count": 3,
    "authors": [
      "Nannan Wu",
      "Zhaobin Sun",
      "Zengqiang Yan",
      "Li Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29526": {
    "title": "Low-Rank Kernel Tensor Learning for Incomplete Multi-View Clustering",
    "volume": "main",
    "abstract": "Incomplete Multiple Kernel Clustering algorithms, which aim to learn a common latent representation from pre-constructed incomplete multiple kernels from the original data, followed by k-means for clustering. They have attracted intensive attention due to their high computational efficiency. However, our observation reveals that the imputation of these approaches for each kernel ignores the influence of other incomplete kernels. In light of this, we present a novel method called Low-Rank Kernel Tensor Learning for Incomplete Multiple Views Clustering (LRKT-IMVC) to address the above issue. Specifically, LRKT-IMVC first introduces the concept of kernel tensor to explore the inter-view correlations, and then the low-rank kernel tensor constraint is used to further capture the consistency information to impute missing kernel elements, thereby improving the quality of clustering. Moreover, we carefully design an alternative optimization method with promising convergence to solve the resulting optimization problem. The proposed method is compared with recent advances in experiments with different missing ratios on seven well-known datasets, demonstrating its effectiveness and the advantages of the proposed interpolation method",
    "checked": true,
    "id": "7ee8ede97059482cfa111ddd80e92a7794febdcb",
    "semantic_title": "low-rank kernel tensor learning for incomplete multi-view clustering",
    "citation_count": 0,
    "authors": [
      "Tingting Wu",
      "Songhe Feng",
      "Jiazheng Yuan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29527": {
    "title": "Test-Time Domain Adaptation by Learning Domain-Aware Batch Normalization",
    "volume": "main",
    "abstract": "Test-time domain adaptation aims to adapt the model trained on source domains to unseen target domains using a few unlabeled images. Emerging research has shown that the label and domain information is separately embedded in the weight matrix and batch normalization (BN) layer. Previous works normally update the whole network naively without explicitly decoupling the knowledge between label and domain. As a result, it leads to knowledge interference and defective distribution adaptation. In this work, we propose to reduce such learning interference and elevate the domain knowledge learning by only manipulating the BN layer. However, the normalization step in BN is intrinsically unstable when the statistics are re-estimated from a few samples. We find that ambiguities can be greatly reduced when only updating the two affine parameters in BN while keeping the source domain statistics. To further enhance the domain knowledge extraction from unlabeled data, we construct an auxiliary branch with label-independent self-supervised learning (SSL) to provide supervision. Moreover, we propose a bi-level optimization based on meta-learning to enforce the alignment of two learning objectives of auxiliary and main branches. The goal is to use the auxiliary branch to adapt the domain and benefit main task for subsequent inference. Our method keeps the same computational cost at inference as the auxiliary branch can be thoroughly discarded after adaptation. Extensive experiments show that our method outperforms the prior works on five WILDS real-world domain shift datasets. Our method can also be integrated with methods with label-dependent optimization to further push the performance boundary. Our code is available at https://github.com/ynanwu/MABN",
    "checked": true,
    "id": "ea1dc43aefc69d4a80323eb9e0bbfe96d0857023",
    "semantic_title": "test-time domain adaptation by learning domain-aware batch normalization",
    "citation_count": 4,
    "authors": [
      "Yanan Wu",
      "Zhixiang Chi",
      "Yang Wang",
      "Konstantinos N. Plataniotis",
      "Songhe Feng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29528": {
    "title": "H-ensemble: An Information Theoretic Approach to Reliable Few-Shot Multi-Source-Free Transfer",
    "volume": "main",
    "abstract": "Multi-source transfer learning is an effective solution to data scarcity by utilizing multiple source tasks for the learning of the target task. However, access to source data and model details is limited in the era of commercial models, giving rise to the setting of multi-source-free (MSF) transfer learning that aims to leverage source domain knowledge without such access. As a newly defined problem paradigm, MSF transfer learning remains largely underexplored and not clearly formulated. In this work, we adopt an information theoretic perspective on it and propose a framework named H-ensemble, which dynamically learns the optimal linear combination, or ensemble, of source models for the target task, using a generalization of maximal correlation regression. The ensemble weights are optimized by maximizing an information theoretic metric for transferability. Compared to previous works, H-ensemble is characterized by: 1) its adaptability to a novel and realistic MSF setting for few-shot target tasks, 2) theoretical reliability, 3) a lightweight structure easy to interpret and adapt. Our method is empirically validated by ablation studies, along with extensive comparative analysis with other task ensemble and transfer learning methods. We show that the H-ensemble can successfully learn the optimal task ensemble, as well as outperform prior arts",
    "checked": true,
    "id": "44c0f0a32780016046715da113352659f3844806",
    "semantic_title": "h-ensemble: an information theoretic approach to reliable few-shot multi-source-free transfer",
    "citation_count": 0,
    "authors": [
      "Yanru Wu",
      "Jianning Wang",
      "Weida Wang",
      "Yang Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29529": {
    "title": "Data Poisoning to Fake a Nash Equilibria for Markov Games",
    "volume": "main",
    "abstract": "We characterize offline data poisoning attacks on Multi-Agent Reinforcement Learning (MARL), where an attacker may change a data set in an attempt to install a (potentially fictitious) unique Markov-perfect Nash equilibrium for a two-player zero-sum Markov game. We propose the unique Nash set, namely the set of games, specified by their Q functions, with a specific joint policy being the unique Nash equilibrium. The unique Nash set is central to poisoning attacks because the attack is successful if and only if data poisoning pushes all plausible games inside it. The unique Nash set generalizes the reward polytope commonly used in inverse reinforcement learning to MARL. For zero-sum Markov games, both the inverse Nash set and the set of plausible games induced by data are polytopes in the Q function space. We exhibit a linear program to efficiently compute the optimal poisoning attack. Our work sheds light on the structure of data poisoning attacks on offline MARL, a necessary step before one can design more robust MARL algorithms",
    "checked": true,
    "id": "6d81df514ec45a9525c8e0e42d8e2f7b5452a326",
    "semantic_title": "data poisoning to fake a nash equilibria for markov games",
    "citation_count": 0,
    "authors": [
      "Young Wu",
      "Jeremy McMahan",
      "Xiaojin Zhu",
      "Qiaomin Xie"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29530": {
    "title": "Self-Training Based Few-Shot Node Classification by Knowledge Distillation",
    "volume": "main",
    "abstract": "Self-training based few-shot node classification (FSNC) methods have shown excellent performance in real applications, but they cannot make the full use of the information in the base set and are easily affected by the quality of pseudo-labels. To address these issues, this paper proposes a new self-training FSNC method by involving the representation distillation and the pseudo-label distillation. Specifically, the representation distillation includes two knowledge distillation methods (i.e., the local representation distillation and the global representation distillation) to transfer the information in the base set to the novel set. The pseudo-label distillation is designed to conduct knowledge distillation on the pseudo-labels to improve their quality. Experimental results showed that our method achieves supreme performance, compared with state-of-the-art methods. Our code and a comprehensive theoretical version are available at https://github.com/zongqianwu/KD-FSNC",
    "checked": true,
    "id": "5201ea84b9c7aa314afca76344e41daac462346b",
    "semantic_title": "self-training based few-shot node classification by knowledge distillation",
    "citation_count": 0,
    "authors": [
      "Zongqian Wu",
      "Yujie Mo",
      "Peng Zhou",
      "Shangbo Yuan",
      "Xiaofeng Zhu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29531": {
    "title": "Market-GAN: Adding Control to Financial Market Data Generation with Semantic Context",
    "volume": "main",
    "abstract": "Financial simulators play an important role in enhancing forecasting accuracy, managing risks, and fostering strategic financial decision-making. Despite the development of financial market simulation methodologies, existing frameworks often struggle with adapting to specialized simulation context. We pinpoint the challenges as i) current financial datasets do not contain context labels; ii) current techniques are not designed to generate financial data with context as control, which demands greater precision compared to other modalities; iii) the inherent difficulties in generating context-aligned, high-fidelity data given the non-stationary, noisy nature of financial data. To address these challenges, our contributions are: i) we proposed the Contextual Market Dataset with market dynamics, stock ticker, and history state as context, leveraging a market dynamics modeling method that combines linear regression and clustering to extract market dynamics; ii) we present Market-GAN, a novel architecture incorporating a Generative Adversarial Networks (GAN) for the controllable generation with context, an autoencoder for learning low-dimension features, and supervisors for knowledge transfer; iii) we introduce a two-stage training scheme to ensure that Market-GAN captures the intrinsic market distribution with multiple objectives. In the pertaining stage, with the use of the autoencoder and supervisors, we prepare the generator with a better initialization for the adversarial training stage. We propose a set of holistic evaluation metrics that consider alignment, fidelity, data usability on downstream tasks, and market facts. We evaluate Market-GAN with the Dow Jones Industrial Average data from 2000 to 2023 and showcase superior performance in comparison to 4 state-of-the-art time-series generative models",
    "checked": true,
    "id": "2670beab82947439edd1b88d95da38ecadb0235f",
    "semantic_title": "market-gan: adding control to financial market data generation with semantic context",
    "citation_count": 0,
    "authors": [
      "Haochong Xia ",
      "Shuo Sun",
      "Xinrun Wang",
      "Bo An"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29532": {
    "title": "A Separation and Alignment Framework for Black-Box Domain Adaptation",
    "volume": "main",
    "abstract": "Black-box domain adaptation (BDA) targets to learn a classifier on an unsupervised target domain while assuming only access to black-box predictors trained from unseen source data. Although a few BDA approaches have demonstrated promise by manipulating the transferred labels, they largely overlook the rich underlying structure in the target domain. To address this problem, we introduce a novel separation and alignment framework for BDA. Firstly, we locate those well-adapted samples via loss ranking and a flexible confidence-thresholding procedure. Then, we introduce a novel graph contrastive learning objective that aligns under-adapted samples to their local neighbors and well-adapted samples. Lastly, the adaptation is finally achieved by a nearest-centroid-augmented objective that exploits the clustering effect in the feature space. Extensive experiments demonstrate that our proposed method outperforms best baselines on benchmark datasets, e.g. improving the averaged per-class accuracy by 4.1% on the VisDA dataset. The source code is available at: https://github.com/MingxuanXia/SEAL",
    "checked": true,
    "id": "8eddeee6f4d34795244decfe8534b3a6f2f2c477",
    "semantic_title": "a separation and alignment framework for black-box domain adaptation",
    "citation_count": 0,
    "authors": [
      "Mingxuan Xia",
      "Junbo Zhao",
      "Gengyu Lyu",
      "Zenan Huang",
      "Tianlei Hu",
      "Gang Chen",
      "Haobo Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29533": {
    "title": "Transformer as Linear Expansion of Learngene",
    "volume": "main",
    "abstract": "We propose expanding the shared Transformer module to produce and initialize Transformers of varying depths, enabling adaptation to diverse resource constraints. Drawing an analogy to genetic expansibility, we term such module as learngene. To identify the expansion mechanism, we delve into the relationship between the layer's position and its corresponding weight value, and find that linear function appropriately approximates this relationship. Building on this insight, we present Transformer as Linear Expansion of learnGene (TLEG), a novel approach for flexibly producing and initializing Transformers of diverse depths. Specifically, to learn learngene, we firstly construct an auxiliary Transformer linearly expanded from learngene, after which we train it through employing soft distillation. Subsequently, we can produce and initialize Transformers of varying depths via linearly expanding the well-trained learngene, thereby supporting diverse downstream scenarios. Extensive experiments on ImageNet-1K demonstrate that TLEG achieves comparable or better performance in contrast to many individual models trained from scratch, while reducing around 2× training cost. When transferring to several downstream classification datasets, TLEG surpasses existing initialization methods by a large margin (e.g., +6.87% on iNat 2019 and +7.66% on CIFAR-100). Under the situation where we need to produce models of varying depths adapting for different resource constraints, TLEG achieves comparable results while reducing around 19× parameters stored to initialize these models and around 5× pre-training costs, in contrast to the pre-training and fine-tuning approach. When transferring a fixed set of parameters to initialize different models, TLEG presents better flexibility and competitive performance while reducing around 2.9× parameters stored to initialize, compared to the pre-training approach",
    "checked": true,
    "id": "66b19b2fd01ebcce2118dc707e18202b4ba1f39a",
    "semantic_title": "transformer as linear expansion of learngene",
    "citation_count": 2,
    "authors": [
      "Shiyu Xia",
      "Miaosen Zhang",
      "Xu Yang",
      "Ruiming Chen",
      "Haokun Chen",
      "Xin Geng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29534": {
    "title": "IVP-VAE: Modeling EHR Time Series with Initial Value Problem Solvers",
    "volume": "main",
    "abstract": "Continuous-time models such as Neural ODEs and Neural Flows have shown promising results in analyzing irregularly sampled time series frequently encountered in electronic health records. Based on these models, time series are typically processed with a hybrid of an initial value problem (IVP) solver and a recurrent neural network within the variational autoencoder architecture. Sequentially solving IVPs makes such models computationally less efficient. In this paper, we propose to model time series purely with continuous processes whose state evolution can be approximated directly by IVPs. This eliminates the need for recurrent computation and enables multiple states to evolve in parallel. We further fuse the encoder and decoder with one IVP solver utilizing its invertibility, which leads to fewer parameters and faster convergence. Experiments on three real-world datasets show that the proposed method can systematically outperform its predecessors, achieve state-of-the-art results, and have significant advantages in terms of data efficiency",
    "checked": true,
    "id": "c78ba058385d74374a79a6beea09a2a3e76b4696",
    "semantic_title": "ivp-vae: modeling ehr time series with initial value problem solvers",
    "citation_count": 1,
    "authors": [
      "Jingge Xiao",
      "Leonie Basso",
      "Wolfgang Nejdl",
      "Niloy Ganguly",
      "Sandipan Sikdar"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29535": {
    "title": "SHoP: A Deep Learning Framework for Solving High-Order Partial Differential Equations",
    "volume": "main",
    "abstract": "Solving partial differential equations (PDEs) has been a fundamental problem in computational science and of wide applications for both scientific and engineering research. Due to its universal approximation property, neural network is widely used to approximate the solutions of PDEs. However, existing works are incapable of solving high-order PDEs due to insufficient calculation accuracy of higher-order derivatives, and the final network is a black box without explicit explanation. To address these issues, we propose a deep learning framework to solve high-order PDEs, named SHoP. Specifically, we derive the high-order derivative rule for neural network, to get the derivatives quickly and accurately; moreover, we expand the network into a Taylor series, providing an explicit solution for the PDEs. We conduct experimental validations four high-order PDEs with different dimensions, showing that we can solve high-order PDEs efficiently and accurately. The source code can be found at https://github.com/HarryPotterXTX/SHoP.git",
    "checked": true,
    "id": "5ad34a92464e2b988151641a65275cf941d6f099",
    "semantic_title": "shop: a deep learning framework for solving high-order partial differential equations",
    "citation_count": 1,
    "authors": [
      "Tingxiong Xiao",
      "Runzhao Yang",
      "Yuxiao Cheng",
      "Jinli Suo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29536": {
    "title": "Enhancing Evolving Domain Generalization through Dynamic Latent Representations",
    "volume": "main",
    "abstract": "Domain generalization is a critical challenge for machine learning systems. Prior domain generalization methods focus on extracting domain-invariant features across several stationary domains to enable generalization to new domains. However, in non-stationary tasks where new domains evolve in an underlying continuous structure, such as time, merely extracting the invariant features is insufficient for generalization to the evolving new domains. Nevertheless, it is non-trivial to learn both evolving and invariant features within a single model due to their conflicts. To bridge this gap, we build causal models to characterize the distribution shifts concerning the two patterns, and propose to learn both dynamic and invariant features via a new framework called Mutual Information-Based Sequential Autoencoders (MISTS). MISTS adopts information theoretic constraints onto sequential autoencoders to disentangle the dynamic and invariant features, and leverage an adaptive classifier to make predictions based on both evolving and invariant information. Our experimental results on both synthetic and real-world datasets demonstrate that MISTS succeeds in capturing both evolving and invariant information, and present promising results in evolving domain generalization tasks",
    "checked": true,
    "id": "2c13d93d03dc2c4510f654f705430605af5739b3",
    "semantic_title": "enhancing evolving domain generalization through dynamic latent representations",
    "citation_count": 1,
    "authors": [
      "Binghui Xie",
      "Yongqiang Chen",
      "Jiaqi Wang",
      "Kaiwen Zhou",
      "Bo Han",
      "Wei Meng",
      "James Cheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29537": {
    "title": "Trust Region Methods for Nonconvex Stochastic Optimization beyond Lipschitz Smoothness",
    "volume": "main",
    "abstract": "In many important machine learning applications, the standard assumption of having a globally Lipschitz continuous gradient may fail to hold. This paper delves into a more general (L0, L1)-smoothness setting, which gains particular significance within the realms of deep neural networks and distributionally robust optimization (DRO). We demonstrate the significant advantage of trust region methods for stochastic nonconvex optimization under such generalized smoothness assumption. We show that first-order trust region methods can recover the normalized and clipped stochastic gradient as special cases and then provide a unified analysis to show their convergence to first-order stationary conditions. Motivated by the important application of DRO, we propose a generalized high-order smoothness condition, under which second-order trust region methods can achieve a complexity of O(epsilon(-3.5)) for convergence to second-order stationary points. By incorporating variance reduction, the second-order trust region method obtains an even better complexity of O(epsilon(-3)), matching the optimal bound for standard smooth optimization. To our best knowledge, this is the first work to show convergence beyond the first-order stationary condition for generalized smooth optimization. Preliminary experiments show that our proposed algorithms perform favorably compared with existing methods",
    "checked": true,
    "id": "01b57f38e4e50b25ac3350d6881b7a4bc16c2203",
    "semantic_title": "trust region methods for nonconvex stochastic optimization beyond lipschitz smoothness",
    "citation_count": 3,
    "authors": [
      "Chenghan Xie",
      "Chenxi Li",
      "Chuwen Zhang",
      "Qi Deng",
      "Dongdong Ge",
      "Yinyu Ye"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29538": {
    "title": "AUC Optimization from Multiple Unlabeled Datasets",
    "volume": "main",
    "abstract": "Weakly supervised learning aims to make machine learning more powerful when the perfect supervision is unavailable, and has attracted much attention from researchers. Among the various scenarios of weak supervision, one of the most challenging cases is learning from multiple unlabeled (U) datasets with only a little knowledge of the class priors, or U^m learning for short. In this paper, we study the problem of building an AUC (area under ROC curve) optimal model from multiple unlabeled datasets, which maximizes the pairwise ranking ability of the classifier. We propose U^m-AUC, an AUC optimization approach that converts the U^m data into a multi-label AUC optimization problem, and can be trained efficiently. We show that the proposed U^m-AUC is effective theoretically and empirically",
    "checked": true,
    "id": "fb28db58e1cd6f01e28a2929b20816a45cacbf47",
    "semantic_title": "auc optimization from multiple unlabeled datasets",
    "citation_count": 0,
    "authors": [
      "Zheng Xie",
      "Yu Liu",
      "Ming Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29539": {
    "title": "MoDE: A Mixture-of-Experts Model with Mutual Distillation among the Experts",
    "volume": "main",
    "abstract": "The application of mixture-of-experts (MoE) is gaining popularity due to its ability to improve model's performance. In an MoE structure, the gate layer plays a significant role in distinguishing and routing input features to different experts. This enables each expert to specialize in processing their corresponding sub-tasks. However, the gate's routing mechanism also gives rise to \"narrow vision\": the individual MoE's expert fails to use more samples in learning the allocated subtask, which in turn limits the MoE to further improve its generalization ability. To effectively address this, we propose a method called Mixture-of-Distilled-Expert (MoDE), which applies moderate mutual distillation among experts to enable each expert to pick up more features learned by other experts and gain more accurate perceptions on their allocated sub-tasks. We conduct plenty experiments including tabular, NLP and CV datasets, which shows MoDE's effectiveness, universality and robustness. Furthermore, we develop a parallel study through innovatively constructing \"expert probing\", to experimentally prove why MoDE works: moderate distilling knowledge from other experts can improve each individual expert's test performances on their assigned tasks, leading to MoE's overall performance improvement",
    "checked": true,
    "id": "6b189b37c0a323ae2126daa5af73c76c986a8265",
    "semantic_title": "mode: a mixture-of-experts model with mutual distillation among the experts",
    "citation_count": 0,
    "authors": [
      "Zhitian Xie",
      "Yinger Zhang",
      "Chenyi Zhuang",
      "Qitao Shi",
      "Zhining Liu",
      "Jinjie Gu",
      "Guannan Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29540": {
    "title": "MmAP: Multi-Modal Alignment Prompt for Cross-Domain Multi-Task Learning",
    "volume": "main",
    "abstract": "Multi-Task Learning (MTL) is designed to train multiple correlated tasks simultaneously, thereby enhancing the performance of individual tasks. Typically, a multi-task network structure consists of a shared backbone and task-specific decoders. However, the complexity of the decoders increases with the number of tasks. To tackle this challenge, we integrate the decoder-free vision-language model CLIP, which exhibits robust zero-shot generalization capability. Recently, parameter-efficient transfer learning methods have been extensively explored with CLIP for adapting to downstream tasks, where prompt tuning showcases strong potential. Nevertheless, these methods solely fine-tune a single modality (text or visual), disrupting the modality structure of CLIP. In this paper, we first propose Multi-modal Alignment Prompt (MmAP) for CLIP, which aligns text and visual modalities during fine-tuning process. Building upon MmAP, we develop an innovative multi-task prompt learning framework. On the one hand, to maximize the complementarity of tasks with high similarity, we utilize a gradient-driven task grouping method that partitions tasks into several disjoint groups and assign a group-shared MmAP to each group. On the other hand, to preserve the unique characteristics of each task, we assign an task-specific MmAP to each task. Comprehensive experiments on two large multi-task learning datasets demonstrate that our method achieves significant performance improvements compared to full fine-tuning while only utilizing approximately ~ 0.09% of trainable parameters",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Xin",
      "Junlong Du",
      "Qiang Wang",
      "Ke Yan",
      "Shouhong Ding"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29541": {
    "title": "VMT-Adapter: Parameter-Efficient Transfer Learning for Multi-Task Dense Scene Understanding",
    "volume": "main",
    "abstract": "Large-scale pre-trained models have achieved remarkable success in various computer vision tasks. A standard approach to leverage these models is to fine-tune all model parameters for downstream tasks, which poses challenges in terms of computational and storage costs. Recently, inspired by Natural Language Processing (NLP), parameter-efficient transfer learning has been successfully applied to vision tasks. However, most existing techniques primarily focus on single-task adaptation, and despite limited research on multi-task adaptation, these methods often exhibit suboptimal training/inference efficiency. In this paper, we first propose an once-for-all Vision Multi-Task Adapter (VMT-Adapter), which strikes approximately O(1) training and inference efficiency w.r.t task number. Concretely, VMT-Adapter shares the knowledge from multiple tasks to enhance cross-task interaction while preserves task-specific knowledge via independent knowledge extraction modules. Notably, since task-specific modules require few parameters, VMT-Adapter can handle an arbitrary number of tasks with a negligible increase of trainable parameters. We also propose VMT-Adapter-Lite, which further reduces the trainable parameters by learning shared parameters between down- and up-projections. Extensive experiments on four dense scene understanding tasks demonstrate the superiority of VMT-Adapter(-Lite), achieving a 3.96% (1.34%) relative improvement compared to single-task full fine-tuning, while utilizing merely ～1% (0.36%) trainable parameters of the pre-trained model",
    "checked": true,
    "id": "7c3652f7bcfcd0e8cbf67ef325e686b67adc6de1",
    "semantic_title": "vmt-adapter: parameter-efficient transfer learning for multi-task dense scene understanding",
    "citation_count": 15,
    "authors": [
      "Yi Xin",
      "Junlong Du",
      "Qiang Wang",
      "Zhiwen Lin",
      "Ke Yan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29542": {
    "title": "BiPFT: Binary Pre-trained Foundation Transformer with Low-Rank Estimation of Binarization Residual Polynomials",
    "volume": "main",
    "abstract": "Pretrained foundation models offer substantial benefits for a wide range of downstream tasks, which can be one of the most potential techniques to access artificial general intelligence. However, scaling up foundation transformers for maximal task-agnostic knowledge has brought about computational challenges, especially on resource-limited devices such as mobiles. This work proposes the first Binary Pretrained Foundation Transformer (BiPFT) for natural language understanding (NLU) tasks, which remarkably saves 56 times operations and 28 times memory. In contrast to previous task-specific binary transformers, BiPFT exhibits a substantial enhancement in the learning capabilities of binary neural networks (BNNs), promoting BNNs into the era of pre-training. Benefiting from extensive pretraining data, we further propose a data-driven binarization method. Specifically, we first analyze the binarization error in self-attention operations and derive the polynomials of binarization error. To simulate full-precision self-attention, we define binarization error as binarization residual polynomials, and then introduce low-rank estimators to model these polynomials. Extensive experiments validate the effectiveness of BiPFTs, surpassing task-specific baseline by 15.4% average performance on the GLUE benchmark. BiPFT also demonstrates improved robustness to hyperparameter changes, improved optimization efficiency, and reduced reliance on downstream distillation, which consequently generalize on various NLU tasks and simplify the downstream pipeline of BNNs. Our code and pretrained models are publicly available at https://github.com/Xingrun-Xing/BiPFT",
    "checked": true,
    "id": "0f8c15448db683116bfe7bafb1a36274bbce3f59",
    "semantic_title": "bipft: binary pre-trained foundation transformer with low-rank estimation of binarization residual polynomials",
    "citation_count": 2,
    "authors": [
      "Xingrun Xing",
      "Li Du",
      "Xinyuan Wang",
      "Xianlin Zeng",
      "Yequan Wang",
      "Zheng Zhang",
      "Jiajun Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29543": {
    "title": "DePRL: Achieving Linear Convergence Speedup in Personalized Decentralized Learning with Shared Representations",
    "volume": "main",
    "abstract": "Decentralized learning has emerged as an alternative method to the popular parameter-server framework which suffers from high communication burden, single-point failure and scalability issues due to the need of a central server. However, most existing works focus on a single shared model for all workers regardless of the data heterogeneity problem, rendering the resulting model performing poorly on individual workers. In this work, we propose a novel personalized decentralized learning algorithm named DePRL via shared representations. Our algorithm relies on ideas from representation learning theory to learn a low-dimensional global representation collaboratively among all workers in a fully decentralized manner, as well as a user-specific low-dimensional local head leading to a personalized solution for each worker. We show that DePRL achieves, for the first time, a provable \\textit{linear speedup for convergence} with general non-linear representations (i.e., the convergence rate is improved linearly with respect to the number of workers). Experimental results support our theoretical findings showing the superiority of our method in data heterogeneous environments",
    "checked": true,
    "id": "4eb4a117efcccbc2cd0ea079d6d69f6b78cec945",
    "semantic_title": "deprl: achieving linear convergence speedup in personalized decentralized learning with shared representations",
    "citation_count": 0,
    "authors": [
      "Guojun Xiong",
      "Gang Yan",
      "Shiqiang Wang",
      "Jian Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29544": {
    "title": "TEILP: Time Prediction over Knowledge Graphs via Logical Reasoning",
    "volume": "main",
    "abstract": "Conventional embedding-based models approach event time prediction in temporal knowledge graphs (TKGs) as a ranking problem. However, they often fall short in capturing essential temporal relationships such as order and distance. In this paper, we propose TEILP, a logical reasoning framework that naturaly integrates such temporal elements into knowledge graph predictions. We first convert TKGs into a temporal event knowledge graph (TEKG) which has a more explicit representation of time in term of nodes of the graph. The TEKG equips us to develop a differentiable random walk approach to time prediction. Finally, we introduce conditional probability density functions, associated with the logical rules involving the query interval, using which we arrive at the time prediction. We compare TEILP with state-of-the-art methods on five benchmark datasets. We show that our model achieves a significant improvement over baselines while providing interpretable explanations. In particular, we consider several scenarios where training samples are limited, event types are imbalanced, and forecasting the time of future events based on only past events is desired. In all these cases, TEILP outperforms state-of-the-art methods in terms of robustness",
    "checked": true,
    "id": "4399670ed40fc486ce1446a846e790786469f949",
    "semantic_title": "teilp: time prediction over knowledge graphs via logical reasoning",
    "citation_count": 5,
    "authors": [
      "Siheng Xiong",
      "Yuan Yang",
      "Ali Payani",
      "James C Kerce",
      "Faramarz Fekri"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29545": {
    "title": "FairWASP: Fast and Optimal Fair Wasserstein Pre-processing",
    "volume": "main",
    "abstract": "Recent years have seen a surge of machine learning approaches aimed at reducing disparities in model outputs across different subgroups. In many settings, training data may be used in multiple downstream applications by different users, which means it may be most effective to intervene on the training data itself. In this work, we present FairWASP, a novel pre-processing approach designed to reduce disparities in classification datasets without modifying the original data. FairWASP returns sample-level weights such that the reweighted dataset minimizes the Wasserstein distance to the original dataset while satisfying (an empirical version of) demographic parity, a popular fairness criterion. We show theoretically that integer weights are optimal, which means our method can be equivalently understood as duplicating or eliminating samples. FairWASP can therefore be used to construct datasets which can be fed into any classification method, not just methods which accept sample weights. Our work is based on reformulating the pre-processing task as a large-scale mixed-integer program (MIP), for which we propose a highly efficient algorithm based on the cutting plane method. Experiments demonstrate that our proposed optimization algorithm significantly outperforms state-of-the-art commercial solvers in solving both the MIP and its linear program relaxation. Further experiments highlight the competitive performance of FairWASP in reducing disparities while preserving accuracy in downstream classification settings",
    "checked": true,
    "id": "c14572cbcca22f4257ea9fb9f85b1eac8cb6ab88",
    "semantic_title": "fairwasp: fast and optimal fair wasserstein pre-processing",
    "citation_count": 1,
    "authors": [
      "Zikai Xiong",
      "Niccolò Dalmasso",
      "Alan Mishler",
      "Vamsi K. Potluru",
      "Tucker Balch",
      "Manuela Veloso"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29546": {
    "title": "Reliable Conflictive Multi-View Learning",
    "volume": "main",
    "abstract": "Multi-view learning aims to combine multiple features to achieve more comprehensive descriptions of data. Most previous works assume that multiple views are strictly aligned. However, real-world multi-view data may contain low-quality conflictive instances, which show conflictive information in different views. Previous methods for this problem mainly focus on eliminating the conflictive data instances by removing them or replacing conflictive views. Nevertheless, real-world applications usually require making decisions for conflictive instances rather than only eliminating them. To solve this, we point out a new Reliable Conflictive Multi-view Learning (RCML) problem, which requires the model to provide decision results and attached reliabilities for conflictive multi-view data. We develop an Evidential Conflictive Multi-view Learning (ECML) method for this problem. ECML first learns view-specific evidence, which could be termed as the amount of support to each category collected from data. Then, we can construct view-specific opinions consisting of decision results and reliability. In the multi-view fusion stage, we propose a conflictive opinion aggregation strategy and theoretically prove this strategy can exactly model the relation of multi-view common and view-specific reliabilities. Experiments performed on 6 datasets verify the effectiveness of ECML. The code is released at https://github.com/jiajunsi/RCML",
    "checked": true,
    "id": "bb74a51d4cc121408b84890975cd86cad03238b8",
    "semantic_title": "reliable conflictive multi-view learning",
    "citation_count": 7,
    "authors": [
      "Cai Xu",
      "Jiajun Si",
      "Ziyu Guan",
      "Wei Zhao",
      "Yue Wu",
      "Xiyue Gao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29547": {
    "title": "A Label Disambiguation-Based Multimodal Massive Multiple Instance Learning Approach for Immune Repertoire Classification",
    "volume": "main",
    "abstract": "One individual human's immune repertoire consists of a huge set of adaptive immune receptors at a certain time point, representing the individual's adaptive immune state. Immune repertoire classification and associated receptor identification have the potential to make a transformative contribution to the development of novel vaccines and therapies. The vast number of instances and exceedingly low witness rate pose a great challenge to the immune repertoire classification, which can be formulated as a Massive Multiple Instance Learning (MMIL) problem. Traditional MIL methods, at both bag-level and instance-level, confront the issues of substantial computational burden or supervision ambiguity when handling massive instances. To address these issues, we propose a novel label disambiguation-based multimodal massive multiple instance learning approach (LaDM³IL) for immune repertoire classification. LaDM³IL adapts the instance-level MIL paradigm to deal with the issue of high computational cost and employs a specially-designed label disambiguation module for label correction, mitigating the impact of misleading supervision. To achieve a more comprehensive representation of each receptor, LaDM³IL leverages a multimodal fusion module with gating-based attention and tensor-fusion to integrate the information from gene segments and amino acid (AA) sequences of each immune receptor. Extensive experiments on the Cytomegalovirus (CMV) and Cancer datasets demonstrate the superior performance of the proposed LaDM³IL for both immune repertoire classification and associated receptor identification tasks. The code is publicly available at https://github.com/Josie-xufan/LaDM3IL",
    "checked": true,
    "id": "e2c10ec23ee7852b5ebb322b6aff248fbb58cc58",
    "semantic_title": "a label disambiguation-based multimodal massive multiple instance learning approach for immune repertoire classification",
    "citation_count": 1,
    "authors": [
      "Fan Xu",
      "Yu Zhao",
      "Bingzhe Wu",
      "Yueshan Huang",
      "Qin Ren",
      "Yang Xiao",
      "Bing He",
      "Jie Zheng",
      "Jianhua Yao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29548": {
    "title": "Deep Variational Incomplete Multi-View Clustering: Exploring Shared Clustering Structures",
    "volume": "main",
    "abstract": "Incomplete multi-view clustering (IMVC) aims to reveal shared clustering structures within multi-view data, where only partial views of the samples are available. Existing IMVC methods primarily suffer from two issues: 1) Imputation-based methods inevitably introduce inaccurate imputations, which in turn degrade clustering performance; 2) Imputation-free methods are susceptible to unbalanced information among views and fail to fully exploit shared information. To address these issues, we propose a novel method based on variational autoencoders. Specifically, we adopt multiple view-specific encoders to extract information from each view and utilize the Product-of-Experts approach to efficiently aggregate information to obtain the common representation. To enhance the shared information in the common representation, we introduce a coherence objective to mitigate the influence of information imbalance. By incorporating the Mixture-of-Gaussians prior information into the latent representation, our proposed method is able to learn the common representation with clustering-friendly structures. Extensive experiments on four datasets show that our method achieves competitive clustering performance compared with state-of-the-art methods",
    "checked": true,
    "id": "46139650aa9aea8bd17de3df1f7e2368c7265709",
    "semantic_title": "deep variational incomplete multi-view clustering: exploring shared clustering structures",
    "citation_count": 0,
    "authors": [
      "Gehui Xu",
      "Jie Wen",
      "Chengliang Liu",
      "Bing Hu",
      "Yicheng Liu",
      "Lunke Fei",
      "Wei Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29549": {
    "title": "Sparse Variational Student-t Processes",
    "volume": "main",
    "abstract": "The theory of Bayesian learning incorporates the use of Student-t Processes to model heavy-tailed distributions and datasets with outliers. However, despite Student-t Processes having a similar computational complexity as Gaussian Processes, there has been limited emphasis on the sparse representation of this model. This is mainly due to the increased difficulty in modeling and computation compared to previous sparse Gaussian Processes. Our motivation is to address the need for a sparse representation framework that reduces computational complexity, allowing Student-t Processes to be more flexible for real-world datasets. To achieve this, we leverage the conditional distribution of Student-t Processes to introduce sparse inducing points. Bayesian methods and variational inference are then utilized to derive a well-defined lower bound, facilitating more efficient optimization of our model through stochastic gradient descent. We propose two methods for computing the variational lower bound, one utilizing Monte Carlo sampling and the other employing Jensen's inequality to compute the KL regularization term in the loss function. We propose adopting these approaches as viable alternatives to Gaussian processes when the data might contain outliers or exhibit heavy-tailed behavior, and we provide specific recommendations for their applicability. We evaluate the two proposed approaches on various synthetic and real-world datasets from UCI and Kaggle, demonstrating their effectiveness compared to baseline methods in terms of computational complexity and accuracy, as well as their robustness to outliers",
    "checked": true,
    "id": "7517812a539c05431e78d3b8211aee1b3f2d7ca5",
    "semantic_title": "sparse variational student-t processes",
    "citation_count": 0,
    "authors": [
      "Jian Xu",
      "Delu Zeng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29550": {
    "title": "Relative Policy-Transition Optimization for Fast Policy Transfer",
    "volume": "main",
    "abstract": "We consider the problem of policy transfer between two Markov Decision Processes (MDPs). We introduce a lemma based on existing theoretical results in reinforcement learning to measure the relativity gap between two arbitrary MDPs, that is the difference between any two cumulative expected returns defined on different policies and environment dynamics. Based on this lemma, we propose two new algorithms referred to as Relative Policy Optimization (RPO) and Relative Transition Optimization (RTO), which offer fast policy transfer and dynamics modelling, respectively. RPO transfers the policy evaluated in one environment to maximize the return in another, while RTO updates the parameterized dynamics model to reduce the gap between the dynamics of the two environments. Integrating the two algorithms results in the complete Relative Policy-Transition Optimization (RPTO) algorithm, in which the policy interacts with the two environments simultaneously, such that data collections from two environments, policy and transition updates are completed in one closed loop to form a principled learning framework for policy transfer. We demonstrate the effectiveness of RPTO on a set of MuJoCo continuous control tasks by creating policy transfer problems via variant dynamics",
    "checked": true,
    "id": "478c1c3b3b344c22bc7bc6cd41f21ecd4ea6f261",
    "semantic_title": "relative policy-transition optimization for fast policy transfer",
    "citation_count": 0,
    "authors": [
      "Jiawei Xu",
      "Cheng Zhou",
      "Yizheng Zhang",
      "Baoxiang Wang",
      "Lei Han"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29551": {
    "title": "Union Subgraph Neural Networks",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs) are widely used for graph representation learning in many application domains. The expressiveness of vanilla GNNs is upper-bounded by 1-dimensional Weisfeiler-Leman (1-WL) test as they operate on rooted subtrees through iterative message passing. In this paper, we empower GNNs by injecting neighbor-connectivity information extracted from a new type of substructure. We first investigate different kinds of connectivities existing in a local neighborhood and identify a substructure called union subgraph, which is able to capture the complete picture of the 1-hop neighborhood of an edge. We then design a shortest-path-based substructure descriptor that possesses three nice properties and can effectively encode the high-order connectivities in union subgraphs. By infusing the encoded neighbor connectivities, we propose a novel model, namely Union Subgraph Neural Network (UnionSNN), which is proven to be strictly more powerful than 1-WL in distinguishing non-isomorphic graphs. Additionally, the local encoding from union subgraphs can also be injected into arbitrary message-passing neural networks (MPNNs) and Transformer-based models as a plugin. Extensive experiments on 18 benchmarks of both graph-level and node-level tasks demonstrate that UnionSNN outperforms state-of-the-art baseline models, with competitive computational efficiency. The injection of our local encoding to existing models is able to boost the performance by up to 11.09%. Our code is available at https://github.com/AngusMonroe/UnionSNN",
    "checked": true,
    "id": "bab6e4c6bc0c7edf1f7d34b95f810c77c51a652c",
    "semantic_title": "union subgraph neural networks",
    "citation_count": 1,
    "authors": [
      "Jiaxing Xu",
      "Aihu Zhang",
      "Qingtian Bian",
      "Vijay Prakash Dwivedi",
      "Yiping Ke"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29552": {
    "title": "Enhancing Ensemble Clustering with Adaptive High-Order Topological Weights",
    "volume": "main",
    "abstract": "Ensemble clustering learns more accurate consensus results from a set of weak base clustering results. This technique is more challenging than other clustering algorithms due to the base clustering result set's randomness and the inaccessibility of data features. Existing ensemble clustering methods rely on the Co-association (CA) matrix quality but lack the capability to handle missing connections in base clustering. Inspired by the neighborhood high-order and topological similarity theories, this paper proposes a topological ensemble model based on high-order information. Specifically, this paper compensates for missing connections by mining neighborhood high-order connection information in the CA matrix and learning optimal connections with adaptive weights. Afterward, the learned excellent connections are embedded into topology learning to capture the topology of the base clustering. Finally, we incorporate adaptive high-order connection representation and topology learning into a unified learning framework. To our knowledge, this is the first ensemble clustering work based on topological similarity and high-order connectivity relations. Extensive experiments on multiple datasets demonstrate the effectiveness of the proposed method. The source code of the proposed approach is available at https://github.com/ltyong/awec",
    "checked": true,
    "id": "d14bbb723dc9bb5e0a39c00c50d4d88290e5f941",
    "semantic_title": "enhancing ensemble clustering with adaptive high-order topological weights",
    "citation_count": 1,
    "authors": [
      "Jiaxuan Xu",
      "Taiyong Li",
      "Lei Duan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29553": {
    "title": "PTMQ: Post-training Multi-Bit Quantization of Neural Networks",
    "volume": "main",
    "abstract": "The ability of model quantization with arbitrary bit-width to dynamically meet diverse bit-width requirements during runtime has attracted significant attention. Recent research has focused on optimizing large-scale training methods to achieve robust bit-width adaptation, which is a time-consuming process requiring hundreds of GPU hours. Furthermore, converting bit-widths requires recalculating statistical parameters of the norm layers, thereby impeding real-time switching of the bit-width. To overcome these challenges, we propose an efficient Post-Training Multi-bit Quantization (PTMQ) scheme that requires only a small amount of calibration data to perform block-wise reconstruction of multi-bit quantization errors. It eliminates the influence of statistical parameters by fusing norm layers, and supports real-time switching bit-widths in uniform quantization and mixed-precision quantization. To improve quantization accuracy and robustness, we propose a Multi-bit Feature Mixer technique (MFM) for fusing features of different bit-widths to enhance robustness across varying bit-widths. Moreover, we introduced the Group-wise Distillation Loss (GD-Loss) to enhance the correlation between different bit-width groups and further improve the overall performance of PTMQ. Extensive experiments demonstrate that PTMQ achieves comparable performance to existing state-of-the-art post-training quantization methods, while optimizing it speeds up by 100$\\times$ compared to recent multi-bit quantization works. Code can be available at https://github.com/xuke225/PTMQ",
    "checked": true,
    "id": "1bb0225f3bad00e4eb088cfbfe755af5e1f73c4e",
    "semantic_title": "ptmq: post-training multi-bit quantization of neural networks",
    "citation_count": 0,
    "authors": [
      "Ke Xu",
      "Zhongcheng Li",
      "Shanshan Wang",
      "Xingyi Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29554": {
    "title": "LSTKC: Long Short-Term Knowledge Consolidation for Lifelong Person Re-identification",
    "volume": "main",
    "abstract": "Lifelong person re-identification (LReID) aims to train a unified model from diverse data sources step by step. The severe domain gaps between different training steps result in catastrophic forgetting in LReID, and existing methods mainly rely on data replay and knowledge distillation techniques to handle this issue. However, the former solution needs to store historical exemplars which inevitably impedes data privacy. The existing knowledge distillation-based models usually retain all the knowledge of the learned old models without any selections, which will inevitably include erroneous and detrimental knowledge that severely impacts the learning performance of the new model. To address these issues, we propose an exemplar-free LReID method named LongShort Term Knowledge Consolidation (LSTKC) that contains a Rectification-based Short-Term Knowledge Transfer module (R-STKT) and an Estimation-based Long-Term Knowledge Consolidation module (E-LTKC). For each learning iteration within one training step, R-STKT aims to filter and rectify the erroneous knowledge contained in the old model and transfer the rectified knowledge to facilitate the short-term learning of the new model. Meanwhile, once one training step is finished, E-LTKC proposes to further consolidate the learned long-term knowledge via adaptively fusing the parameters of models from different steps. Consequently, experimental results show that our LSTKC exceeds the state-of-the-art methods by 6.3%/9.4% and 7.9%/4.5%, 6.4%/8.0% and 9.0%/5.5% average mAP/R@1 on seen and unseen domains under two different training orders of the challenging LReID benchmark respectively",
    "checked": true,
    "id": "e5a6765fb413b866b4fe0d9c420ecc5cdc0eb360",
    "semantic_title": "lstkc: long short-term knowledge consolidation for lifelong person re-identification",
    "citation_count": 1,
    "authors": [
      "Kunlun Xu",
      "Xu Zou",
      "Jiahuan Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29555": {
    "title": "Defying Imbalanced Forgetting in Class Incremental Learning",
    "volume": "main",
    "abstract": "We observe a high level of imbalance in the accuracy of different learned classes in the same old task for the first time. This intriguing phenomenon, discovered in replay-based Class Incremental Learning (CIL), highlights the imbalanced forgetting of learned classes, as their accuracy is similar before the occurrence of catastrophic forgetting. This discovery remains previously unidentified due to the reliance on average incremental accuracy as the measurement for CIL, which assumes that the accuracy of classes within the same task is similar. However, this assumption is invalid in the face of catastrophic forgetting. Further empirical studies indicate that this imbalanced forgetting is caused by conflicts in representation between semantically similar old and new classes. These conflicts are rooted in the data imbalance present in replay-based CIL methods. Building on these insights, we propose CLass-Aware Disentanglement (CLAD) as a means to predict the old classes that are more likely to be forgotten and enhance their accuracy. Importantly, CLAD can be seamlessly integrated into existing CIL methods. Extensive experiments demonstrate that CLAD consistently improves current replay-based methods, resulting in performance gains of up to 2.56%",
    "checked": true,
    "id": "25a916d1fe1a0782a17b989f485083c764a5658e",
    "semantic_title": "defying imbalanced forgetting in class incremental learning",
    "citation_count": 0,
    "authors": [
      "Shixiong Xu",
      "Gaofeng Meng",
      "Xing Nie",
      "Bolin Ni",
      "Bin Fan",
      "Shiming Xiang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29556": {
    "title": "E2E-AT: A Unified Framework for Tackling Uncertainty in Task-Aware End-to-End Learning",
    "volume": "main",
    "abstract": "Successful machine learning involves a complete pipeline of data, model, and downstream applications. Instead of treating them separately, there has been a prominent increase of attention within the constrained optimization (CO) and machine learning (ML) communities towards combining prediction and optimization models. The so-called end-to-end (E2E) learning captures the task-based objective for which they will be used for decision making. Although a large variety of E2E algorithms have been presented, it has not been fully investigated how to systematically address uncertainties involved in such models. Most of the existing work considers the uncertainties of ML in the input space and improves robustness through adversarial training. We extend this idea to E2E learning and prove that there is a robustness certification procedure by solving augmented integer programming. Furthermore, we show that neglecting the uncertainty of COs during training causes a new trigger for generalization errors. To include all these components, we propose a unified framework that covers the uncertainties emerging in both the input feature space of the ML models and the COs. The framework is described as a robust optimization problem and is practically solved via end-to-end adversarial training (E2E-AT). Finally, the performance of E2E-AT is evaluated by a real-world end-to-end power system operation problem, including load forecasting and sequential scheduling tasks",
    "checked": true,
    "id": "8cd7c43d77c205c5339aea44a663636d9fde94ad",
    "semantic_title": "e2e-at: a unified framework for tackling uncertainty in task-aware end-to-end learning",
    "citation_count": 1,
    "authors": [
      "Wangkun Xu",
      "Jianhong Wang",
      "Fei Teng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29557": {
    "title": "LERE: Learning-Based Low-Rank Matrix Recovery with Rank Estimation",
    "volume": "main",
    "abstract": "A fundamental task in the realms of computer vision, Low-Rank Matrix Recovery (LRMR) focuses on the inherent low-rank structure precise recovery from incomplete data and/or corrupted measurements given that the rank is a known prior or accurately estimated. However, it remains challenging for existing rank estimation methods to accurately estimate the rank of an ill-conditioned matrix. Also, existing LRMR optimization methods are heavily dependent on the chosen parameters, and are therefore difficult to adapt to different situations. Addressing these issues, A novel LEarning-based low-rank matrix recovery with Rank Estimation (LERE) is proposed. More specifically, considering the characteristics of the Gerschgorin disk's center and radius, a new heuristic decision rule in the Gerschgorin Disk Theorem is significantly enhanced and the low-rank boundary can be exactly located, which leads to a marked improvement in the accuracy of rank estimation. According to the estimated rank, we select row and column sub-matrices from the observation matrix by uniformly random sampling. A 17-iteration feedforward-recurrent-mixed neural network is then adapted to learn the parameters in the sub-matrix recovery processing. Finally, by the correlation of the row sub-matrix and column sub-matrix, LERE successfully recovers the underlying low-rank matrix. Overall, LERE is more efficient and robust than existing LRMR methods. Experimental results demonstrate that LERE surpasses state-of-the-art (SOTA) methods. The code for this work is accessible at https://github.com/zhengqinxu/LERE",
    "checked": true,
    "id": "192907845888958f3356d8d6a992564c8012fd14",
    "semantic_title": "lere: learning-based low-rank matrix recovery with rank estimation",
    "citation_count": 0,
    "authors": [
      "Zhengqin Xu",
      "Yulun Zhang",
      "Chao Ma",
      "Yichao Yan",
      "Zelin Peng",
      "Shoulie Xie",
      "Shiqian Wu",
      "Xiaokang Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29558": {
    "title": "Multiobjective Lipschitz Bandits under Lexicographic Ordering",
    "volume": "main",
    "abstract": "This paper studies the multiobjective bandit problem under lexicographic ordering, wherein the learner aims to simultaneously maximize ? objectives hierarchically. The only existing algorithm for this problem considers the multi-armed bandit model, and its regret bound is O((KT)^(2/3)) under a metric called priority-based regret. However, this bound is suboptimal, as the lower bound for single objective multi-armed bandits is Omega(KlogT). Moreover, this bound becomes vacuous when the arm number K is infinite. To address these limitations, we investigate the multiobjective Lipschitz bandit model, which allows for an infinite arm set. Utilizing a newly designed multi-stage decision-making strategy, we develop an improved algorithm that achieves a general regret bound of O(T^((d_z^i+1)/(d_z^i+2))) for the i-th objective, where d_z^i is the zooming dimension for the i-th objective, with i in {1,2,...,m}. This bound matches the lower bound of the single objective Lipschitz bandit problem in terms of T, indicating that our algorithm is almost optimal. Numerical experiments confirm the effectiveness of our algorithm",
    "checked": true,
    "id": "b29491bcd447b3f9de9fb9d4ab4bd24855aa9d73",
    "semantic_title": "multiobjective lipschitz bandits under lexicographic ordering",
    "citation_count": 0,
    "authors": [
      "Bo Xue",
      "Ji Cheng",
      "Fei Liu",
      "Yimu Wang",
      "Qingfu Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29559": {
    "title": "Residual Hyperbolic Graph Convolution Networks",
    "volume": "main",
    "abstract": "Hyperbolic graph convolutional networks (HGCNs) have demonstrated representational capabilities of modeling hierarchical-structured graphs. However, as in general GCNs, over-smoothing may occur as the number of model layers increases, limiting the representation capabilities of most current HGCN models. In this paper, we propose residual hyperbolic graph convolutional networks (R-HGCNs) to address the over-smoothing problem. We introduce a hyperbolic residual connection function to overcome the over-smoothing problem, and also theoretically prove the effectiveness of the hyperbolic residual function. Moreover, we use product manifolds and HyperDrop to facilitate the R-HGCNs. The distinctive features of the R-HGCNs are as follows: (1) The hyperbolic residual connection preserves the initial node information in each layer and adds a hyperbolic identity mapping to prevent node features from being indistinguishable. (2) Product manifolds in R-HGCNs have been set up with different origin points in different components to facilitate the extraction of feature information from a wider range of perspectives, which enhances the representing capability of R-HGCNs. (3) HyperDrop adds multiplicative Gaussian noise into hyperbolic representations, such that perturbations can be added to alleviate the over-fitting problem without deconstructing the hyperbolic geometry. Experiment results demonstrate the effectiveness of R-HGCNs under various graph convolution layers and different structures of product manifolds",
    "checked": true,
    "id": "e80d742332bc9a28ce588bb0337bc545506494fe",
    "semantic_title": "residual hyperbolic graph convolution networks",
    "citation_count": 0,
    "authors": [
      "Yangkai Xue",
      "Jindou Dai",
      "Zhipeng Lu",
      "Yuwei Wu",
      "Yunde Jia"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29560": {
    "title": "GraFITi: Graphs for Forecasting Irregularly Sampled Time Series",
    "volume": "main",
    "abstract": "Forecasting irregularly sampled time series with missing values is a crucial task for numerous real-world applications such as healthcare, astronomy, and climate sciences. State-of-the-art approaches to this problem rely on Ordinary Differential Equations (ODEs) which are known to be slow and often require additional features to handle missing values. To address this issue, we propose a novel model using Graphs for Forecasting Irregularly Sampled Time Series with missing values which we call GraFITi. GraFITi first converts the time series to a Sparsity Structure Graph which is a sparse bipartite graph, and then reformulates the forecasting problem as the edge weight prediction task in the graph. It uses the power of Graph Neural Networks to learn the graph and predict the target edge weights. GraFITi has been tested on 3 real-world and 1 synthetic irregularly sampled time series dataset with missing values and compared with various state-of-the-art models. The experimental results demonstrate that GraFITi improves the forecasting accuracy by up to 17% and reduces the run time up to 5 times compared to the state-of-the-art forecasting models",
    "checked": false,
    "id": "dde3f44a50ea9bbe9e60e2d171cb667ffa24b3cf",
    "semantic_title": "forecasting irregularly sampled time series using graphs",
    "citation_count": 1,
    "authors": [
      "Vijaya Krishna Yalavarthi",
      "Kiran Madhusudhanan",
      "Randolf Scholz",
      "Nourhan Ahmed",
      "Johannes Burchert",
      "Shayan Jawed",
      "Stefan Born",
      "Lars Schmidt-Thieme"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29561": {
    "title": "Live and Learn: Continual Action Clustering with Incremental Views",
    "volume": "main",
    "abstract": "Multi-view action clustering leverages the complementary information from different camera views to enhance the clustering performance. Although existing approaches have achieved significant progress, they assume all camera views are available in advance, which is impractical when the camera view is incremental over time. Besides, learning the invariant information among multiple camera views is still a challenging issue, especially in continual learning scenario. Aiming at these problems, we propose a novel continual action clustering (CAC) method, which is capable of learning action categories in a continual learning manner. To be specific, we first devise a category memory library, which captures and stores the learned categories from historical views. Then, as a new camera view arrives, we only need to maintain a consensus partition matrix, which can be updated by leveraging the incoming new camera view rather than keeping all of them. Finally, a three-step alternate optimization is proposed, in which the category memory library and consensus partition matrix are optimized. The empirical experimental results on 6 realistic multi-view action collections demonstrate the excellent clustering performance and time/space efficiency of the CAC compared with 15 state-of-the-art baselines",
    "checked": true,
    "id": "48949898aa843c7234177c287eef544b0592b1e8",
    "semantic_title": "live and learn: continual action clustering with incremental views",
    "citation_count": 0,
    "authors": [
      "Xiaoqiang Yan",
      "Yingtao Gan",
      "Yiqiao Mao",
      "Yangdong Ye",
      "Hui Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29562": {
    "title": "Federated Partial Label Learning with Local-Adaptive Augmentation and Regularization",
    "volume": "main",
    "abstract": "Partial label learning (PLL) expands the applicability of supervised machine learning models by enabling effective learning from weakly annotated overcomplete labels. Existing PLL methods however focus on the standard centralized learning scenarios. In this paper, we expand PLL into the distributed computation setting by formalizing a new learning scenario named as federated partial label learning (FedPLL), where the training data with partial labels are distributed across multiple local clients with privacy constraints. To address this challenging problem, we propose a novel Federated PLL method with Local-Adaptive Augmentation and Regularization (FedPLL-LAAR). In addition to alleviating the partial label noise with moving-average label disambiguation, the proposed method performs MixUp-based local-adaptive data augmentation to mitigate the challenge posed by insufficient and imprecisely annotated local data, and dynamically incorporates the guidance of global model to minimize client drift through adaptive gradient alignment regularization between the global and local models. Extensive experiments conducted on multiple datasets under the FedPLL setting demonstrate the effectiveness of the proposed FedPLL-LAAR method for federated partial label learning",
    "checked": true,
    "id": "b55994878bdedc06d24bd77795c642105a5ded8d",
    "semantic_title": "federated partial label learning with local-adaptive augmentation and regularization",
    "citation_count": 0,
    "authors": [
      "Yan Yan",
      "Yuhong Guo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29563": {
    "title": "An Optimal Transport View for Subspace Clustering and Spectral Clustering",
    "volume": "main",
    "abstract": "Clustering is one of the most fundamental problems in machine learning and data mining, and many algorithms have been proposed in the past decades. Among them, subspace clustering and spectral clustering are the most famous approaches. In this paper, we provide an explanation for subspace clustering and spectral clustering from the perspective of optimal transport. Optimal transport studies how to move samples from one distribution to another distribution with minimal transport cost, and has shown a powerful ability to extract geometric information. By considering a self optimal transport model with only one group of samples, we observe that both subspace clustering and spectral clustering can be explained in the framework of optimal transport, and the optimal transport matrix bridges the spaces of features and spectral embeddings. Inspired by this connection, we propose a spectral optimal transport barycenter model, which learns spectral embeddings by solving a barycenter problem equipped with an optimal transport discrepancy and guidance of data. Based on our proposed model, we take advantage of optimal transport to exploit both feature and metric information involved in data for learning coupled spectral embeddings and affinity matrix in a unified model. We develop an alternating optimization algorithm to solve the resultant problems, and conduct experiments in different settings to evaluate the performance of our proposed methods",
    "checked": true,
    "id": "3d64f8f61c0c6ad61fa5cf48bc36f9a065355bb1",
    "semantic_title": "an optimal transport view for subspace clustering and spectral clustering",
    "citation_count": 0,
    "authors": [
      "Yuguang Yan",
      "Zhihao Xu",
      "Canlin Yang",
      "Jie Zhang",
      "Ruichu Cai",
      "Michael Kwok-Po Ng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29564": {
    "title": "Exploiting Geometry for Treatment Effect Estimation via Optimal Transport",
    "volume": "main",
    "abstract": "Estimating treatment effects from observational data suffers from the issue of confounding bias, which is induced by the imbalanced confounder distributions between the treated and control groups. As an effective approach, re-weighting learns a group of sample weights to balance the confounder distributions. Existing methods of re-weighting highly rely on a propensity score model or moment alignment. However, for complex real-world data, it is difficult to obtain an accurate propensity score prediction. Although moment alignment is free of learning a propensity score model, accurate estimation for high-order moments is computationally difficult and still remains an open challenge, and first and second-order moments are insufficient to align the distributions and easy to be misled by outliers. In this paper, we exploit geometry to capture the intrinsic structure involved in data for balancing the confounder distributions, so that confounding bias can be reduced even with outliers. To achieve this, we construct a connection between treatment effect estimation and optimal transport, a powerful tool to capture geometric information. After that, we propose an optimal transport model to learn sample weights by extracting geometry from confounders, in which geometric information between groups and within groups is leveraged for better confounder balancing. A projected mirror descent algorithm is employed to solve the derived optimization problem. Experimental studies on both synthetic and real-world datasets demonstrate the effectiveness of our proposed method",
    "checked": true,
    "id": "a95fb898bb6fb950cce96cbeb644b469a319ba7a",
    "semantic_title": "exploiting geometry for treatment effect estimation via optimal transport",
    "citation_count": 2,
    "authors": [
      "Yuguang Yan",
      "Zeqin Yang",
      "Weilin Chen",
      "Ruichu Cai",
      "Zhifeng Hao",
      "Michael Kwok-Po Ng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29565": {
    "title": "Wasserstein Differential Privacy",
    "volume": "main",
    "abstract": "Differential privacy (DP) has achieved remarkable results in the field of privacy-preserving machine learning. However, existing DP frameworks do not satisfy all the conditions for becoming metrics, which prevents them from deriving better basic private properties and leads to exaggerated values on privacy budgets. We propose Wasserstein differential privacy (WDP), an alternative DP framework to measure the risk of privacy leakage, which satisfies the properties of symmetry and triangle inequality. We show and prove that WDP has 13 excellent properties, which can be theoretical supports for the better performance of WDP than other DP frameworks. In addition, we derive a general privacy accounting method called Wasserstein accountant, which enables WDP to be applied in stochastic gradient descent (SGD) scenarios containing subsampling. Experiments on basic mechanisms, compositions and deep learning show that the privacy budgets obtained by Wasserstein accountant are relatively stable and less influenced by order. Moreover, the overestimation on privacy budgets can be effectively alleviated. The code is available at https://github.com/Hifipsysta/WDP",
    "checked": true,
    "id": "28c8a779715560a63be45acb05a655509201ba4e",
    "semantic_title": "wasserstein differential privacy",
    "citation_count": 1,
    "authors": [
      "Chengyi Yang",
      "Jiayin Qi",
      "Aimin Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29566": {
    "title": "Federated Causality Learning with Explainable Adaptive Optimization",
    "volume": "main",
    "abstract": "Discovering the causality from observational data is a crucial task in various scientific domains. With increasing awareness of privacy, data are not allowed to be exposed, and it is very hard to learn causal graphs from dispersed data, since these data may have different distributions. In this paper, we propose a federated causal discovery strategy (FedCausal) to learn the unified global causal graph from decentralized heterogeneous data. We design a global optimization formula to naturally aggregate the causal graphs from client data and constrain the acyclicity of the global graph without exposing local data. Unlike other federated causal learning algorithms, FedCausal unifies the local and global optimizations into a complete directed acyclic graph (DAG) learning process with a flexible optimization objective. We prove that this optimization objective has a high interpretability and can adaptively handle homogeneous and heterogeneous data. Experimental results on synthetic and real datasets show that FedCausal can effectively deal with non-independently and identically distributed (non-iid) data and has a superior performance",
    "checked": true,
    "id": "5e0505d271645d1ed0c8cd388df8baca0540b4ab",
    "semantic_title": "federated causality learning with explainable adaptive optimization",
    "citation_count": 0,
    "authors": [
      "Dezhi Yang",
      "Xintong He",
      "Jun Wang",
      "Guoxian Yu",
      "Carlotta Domeniconi",
      "Jinglin Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29567": {
    "title": "Multi-Modal Disordered Representation Learning Network for Description-Based Person Search",
    "volume": "main",
    "abstract": "Description-based person search aims to retrieve images of the target identity via textual descriptions. One of the challenges for this task is to extract discriminative representation from images and descriptions. Most existing methods apply the part-based split method or external models to explore the fine-grained details of local features, which ignore the global relationship between partial information and cause network instability. To overcome these issues, we propose a Multi-modal Disordered Representation Learning Network (MDRL) for description-based person search to fully extract the visual and textual representations. Specifically, we design a Cross-modality Global Feature Learning Architecture to learn the global features from the two modalities and meet the demand of the task. Based on our global network, we introduce a Disorder Local Learning Module to explore local features by a disordered reorganization strategy from both visual and textual aspects and enhance the robustness of the whole network. Besides, we introduce a Cross-modality Interaction Module to guide the two streams to extract visual or textual representations considering the correlation between modalities. Extensive experiments are conducted on two public datasets, and the results show that our method outperforms the state-of-the-art methods on CUHK-PEDES and ICFG-PEDES datasets and achieves superior performance",
    "checked": true,
    "id": "c884bd5daea4e96c7ff9a93cedf87ed4ce399c33",
    "semantic_title": "multi-modal disordered representation learning network for description-based person search",
    "citation_count": 0,
    "authors": [
      "Fan Yang",
      "Wei Li",
      "Menglong Yang",
      "Binbin Liang",
      "Jianwei Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29568": {
    "title": "Exploring One-Shot Semi-supervised Federated Learning with Pre-trained Diffusion Models",
    "volume": "main",
    "abstract": "Recently, semi-supervised federated learning (semi-FL) has been proposed to handle the commonly seen real-world scenarios with labeled data on the server and unlabeled data on the clients. However, existing methods face several challenges such as communication costs, data heterogeneity, and training pressure on client devices. To address these challenges, we introduce the powerful diffusion models (DM) into semi-FL and propose FedDISC, a Federated Diffusion-Inspired Semi-supervised Co-training method. Specifically, we first extract prototypes of the labeled server data and use these prototypes to predict pseudo-labels of the client data. For each category, we compute the cluster centroids and domain-specific representations to signify the semantic and stylistic information of their distributions. After adding noise, these representations are sent back to the server, which uses the pre-trained DM to generate synthetic datasets complying with the client distributions and train a global model on it. With the assistance of vast knowledge within DM, the synthetic datasets have comparable quality and diversity to the client images, subsequently enabling the training of global models that achieve performance equivalent to or even surpassing the ceiling of supervised centralized training. FedDISC works within one communication round, does not require any local training, and involves very minimal information uploading, greatly enhancing its practicality. Extensive experiments on three large-scale datasets demonstrate that FedDISC effectively addresses the semi-FL problem on non-IID clients and outperforms the compared SOTA methods. Sufficient visualization experiments also illustrate that the synthetic dataset generated by FedDISC exhibits comparable diversity and quality to the original client dataset, with a neglectable possibility of leaking privacy-sensitive information of the clients",
    "checked": false,
    "id": "b5a915a67cf51db50bafb7ea912a6fc64304b0c2",
    "semantic_title": "exploring one-shot semi-supervised federated learning with a pre-trained diffusion model",
    "citation_count": 13,
    "authors": [
      "Mingzhao Yang",
      "Shangchao Su",
      "Bin Li",
      "Xiangyang Xue"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29569": {
    "title": "Exploring Sparse Visual Prompt for Domain Adaptive Dense Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Senqiao Yang",
      "Jiarui Wu",
      "Jiaming Liu",
      "Xiaoqi Li",
      "Qizhe Zhang",
      "Mingjie Pan",
      "Yulu Gan",
      "Zehui Chen",
      "Shanghang Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29570": {
    "title": "A Variational Autoencoder for Neural Temporal Point Processes with Dynamic Latent Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sikun Yang",
      "Hongyuan Zha"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29571": {
    "title": "A Transfer Approach Using Graph Neural Networks in Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianpei Yang",
      "Heng You",
      "Jianye Hao",
      "Yan Zheng",
      "Matthew E. Taylor"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29572": {
    "title": "Safe Abductive Learning in the Presence of Inaccurate Rules",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao-Wen Yang",
      "Jie-Jing Shao",
      "Wei-Wei Tu",
      "Yu-Feng Li",
      "Wang-Zhou Dai",
      "Zhi-Hua  Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29573": {
    "title": "Leveraging Normalization Layer in Adapters with Progressive Learning and Adaptive Distillation for Cross-Domain Few-Shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "YongJin Yang",
      "Taehyeon Kim",
      "Se-Young Yun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29574": {
    "title": "Adversarial Purification with the Manifold Hypothesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoyuan Yang",
      "Zhiwei Xu",
      "Jing Zhang",
      "Richard Hartley",
      "Peter Tu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29575": {
    "title": "Dynamic Knowledge Injection for AIXI Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel Yang-Zhao",
      "Kee Siong Ng",
      "Marcus Hutter"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29576": {
    "title": "PerFedRLNAS: One-for-All Personalized Federated Neural Architecture Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dixi Yao",
      "Baochun Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29577": {
    "title": "VQ-FONT: Few-Shot Font Generation with Structure-Aware Enhancement and Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingshuai Yao",
      "Yabo Zhang",
      "Xianhui Lin",
      "Xiaoming Li",
      "Wangmeng Zuo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29578": {
    "title": "DrFuse: Learning Disentangled Representation for Clinical Multi-Modal Fusion with Missing Modality and Modal Inconsistency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenfang Yao",
      "Kejing Yin",
      "William K. Cheung",
      "Jia Liu",
      "Jing Qin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29579": {
    "title": "Progressively Knowledge Distillation via Re-parameterizing Diffusion Reverse Process",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xufeng Yao",
      "Fanbin Lu",
      "Yuechen Zhang",
      "Xinyun Zhang",
      "Wenqian Zhao",
      "Bei Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29580": {
    "title": "Data-Augmented Curriculum Graph Neural Architecture Search under Distribution Shifts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Yao",
      "Xin Wang",
      "Yijian Qin",
      "Ziwei Zhang",
      "Wenwu Zhu",
      "Hong Mei"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29581": {
    "title": "Task-Free Dynamic Sparse Vision Transformer for Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fei Ye",
      "Adrian G. Bors"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29582": {
    "title": "Task-Free Continual Generation and Representation Learning via Dynamic Expansionable Memory Cluster",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fei Ye",
      "Adrian G. Bors"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29583": {
    "title": "Uncertainty Regularized Evidential Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Ye",
      "Tiejin Chen",
      "Hua Wei",
      "Liang Zhan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29584": {
    "title": "Near-Optimal Resilient Aggregation Rules for Distributed Learning Using 1-Center and 1-Mean Clustering with Outliers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhao Yi",
      "Ronghui You",
      "Hong  Liu",
      "Changxin Liu",
      "Yuan Wang",
      "Jiancheng Lv"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29585": {
    "title": "Discriminatively Fuzzy Multi-View K-means Clustering with Local Structure Preserving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Yin",
      "Shiliang Sun",
      "Lai Wei",
      "Pei Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29586": {
    "title": "Effective Causal Discovery under Identifiable Heteroscedastic Noise Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Naiyu Yin",
      "Tian Gao",
      "Yue Yu",
      "Qiang Ji"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29587": {
    "title": "Dynamic Spiking Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nan Yin",
      "Mengzhu Wang",
      "Zhenghan Chen",
      "Giulia De Masi",
      "Huan Xiong",
      "Bin Gu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29588": {
    "title": "Asymmetric Mutual Alignment for Unsupervised Zero-Shot Sketch-Based Image Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihui Yin",
      "Jiexi Yan",
      "Chenghao Xu",
      "Cheng Deng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29589": {
    "title": "Risk-Conditioned Reinforcement Learning: A Generalized Approach for Adapting to Varying Risk Measures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gwangpyo Yoo",
      "Jinwoo Park",
      "Honguk Woo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29590": {
    "title": "Online Boosting Adaptive Learning under Concept Drift for Multistream Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "En Yu",
      "Jie Lu",
      "Bin Zhang",
      "Guangquan Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29591": {
    "title": "Chronic Poisoning: Backdoor Attack against Split Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangchao Yu",
      "Bo Zeng",
      "Kai Zhao",
      "Zhi Pang",
      "Lina Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29592": {
    "title": "Cheaper and Faster: Distributed Deep Reinforcement Learning with Serverless Computing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanfei Yu",
      "Jian Li",
      "Yang Hua",
      "Xu Yuan",
      "Hao Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29593": {
    "title": "Barely Supervised Learning for Graph-Based Fraud Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hang Yu",
      "Zhengyang Liu",
      "Xiangfeng Luo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29594": {
    "title": "A Non-parametric Graph Clustering Framework for Multi-View Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengju Yu",
      "Siwei Wang",
      "Zhibin Dong",
      "Wenxuan Tu",
      "Suyuan Liu",
      "Zhao Lv",
      "Pan Li",
      "Miao Wang",
      "En Zhu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29595": {
    "title": "DVSAI: Diverse View-Shared Anchors Based Incomplete Multi-View Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengju Yu",
      "Siwei Wang",
      "Pei Zhang",
      "Miao Wang",
      "Ziming Wang",
      "Zhe Liu",
      "Liming Fang",
      "En Zhu",
      "Xinwang Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29596": {
    "title": "HGPrompt: Bridging Homogeneous and Heterogeneous Graphs for Few-Shot Prompt Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingtong Yu",
      "Yuan Fang",
      "Zemin Liu",
      "Xinming Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29597": {
    "title": "ANEDL: Adaptive Negative Evidential Deep Learning for Open-Set Semi-supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Yu",
      "Danruo Deng",
      "Furui Liu",
      "Qi Dou",
      "Yueming Jin",
      "Guangyong Chen",
      "Pheng Ann Heng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29598": {
    "title": "TIKP: Text-to-Image Knowledge Preservation for Continual Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhidong Yu",
      "Wei Yang",
      "Xike Xie",
      "Zhenbo Shi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29599": {
    "title": "Accelerating Text-to-Image Editing via Cache-Enabled Sparse Diffusion Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihao Yu",
      "Haoyang Li",
      "Fangcheng Fu",
      "Xupeng Miao",
      "Bin Cui"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29600": {
    "title": "PDE+: Enhancing Generalization via PDE with Adaptive Distributional Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yige Yuan",
      "Bingbing Xu",
      "Bo Lin",
      "Liang Hou",
      "Fei Sun",
      "Huawei Shen",
      "Xueqi Cheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29601": {
    "title": "Self-Paced Unified Representation Learning for Hierarchical Multi-Label Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixuan Yuan",
      "Hao Liu",
      "Haoyi Zhou",
      "Denghui Zhang",
      "Xiao Zhang",
      "Hao Wang",
      "Hui  Xiong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29602": {
    "title": "A Plug-and-Play Quaternion Message-Passing Module for Molecular Conformation Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Angxiao Yue",
      "Dixin Luo",
      "Hongteng Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29603": {
    "title": "Efficient Asynchronous Federated Learning with Prospective Momentum Aggregation and Fine-Grained Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Zang",
      "Zhe Xue",
      "Shilong Ou",
      "Lingyang Chu",
      "Junping Du",
      "Yunfei Long"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29604": {
    "title": "Generalizing across Temporal Domains with Koopman Operators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiuhao Zeng",
      "Wei Wang",
      "Fan Zhou",
      "Gezheng Xu",
      "Ruizhi Pu",
      "Changjian Shui",
      "Christian Gagné",
      "Shichun Yang",
      "Charles X. Ling",
      "Boyu Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29605": {
    "title": "Hierarchical Multi-Marginal Optimal Transport for Network Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhichen Zeng",
      "Boxin Du",
      "Si Zhang",
      "Yinglong Xia",
      "Zhining Liu",
      "Hanghang Tong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29606": {
    "title": "Harnessing the Power of SVD: An SVA Module for Enhanced Signal Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Zhai",
      "Shuyuan Yang",
      "Yitong Li",
      "Zhixi Feng",
      "Zhihao Chang",
      "Quanwei Gao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29607": {
    "title": "Optimistic Model Rollouts for Pessimistic Offline Policy Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanzhao Zhai",
      "Yiying Li",
      "Zijian Gao",
      "Xudong Gong",
      "Kele Xu",
      "Dawei Feng",
      "Ding Bo",
      "Huaimin Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29608": {
    "title": "MetaDiff: Meta-Learning with Conditional Diffusion for Few-Shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baoquan Zhang",
      "Chuyao Luo",
      "Demin Yu",
      "Xutao Li",
      "Huiwei Lin",
      "Yunming Ye",
      "Bowen Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29609": {
    "title": "Learning Cluster-Wise Anchors for Multi-View Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Zhang",
      "Xiuyi Jia",
      "Zechao Li",
      "Chunlin Chen",
      "Huaxiong Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29610": {
    "title": "Targeted Activation Penalties Help CNNs Ignore Spurious Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dekai Zhang",
      "Matt Williams",
      "Francesca Toni"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29611": {
    "title": "Robust Test-Time Adaptation for Zero-Shot Prompt Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ding-Chu Zhang",
      "Zhi Zhou",
      "Yu-Feng Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29612": {
    "title": "FM-OV3D: Foundation Model-Based Cross-Modal Knowledge Blending for Open-Vocabulary 3D Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongmei Zhang",
      "Chang Li",
      "Renrui Zhang",
      "Shenghao Xie",
      "Wei Xue",
      "Xiaodong Xie",
      "Shanghang Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29613": {
    "title": "Coupled Confusion Correction: Learning from Crowds with Sparse Annotations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hansong Zhang",
      "Shikun Li",
      "Dan Zeng",
      "Chenggang Yan",
      "Shiming Ge"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29614": {
    "title": "Exponential Hardness of Optimization from the Locality in Quantum Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao-Kai Zhang",
      "Chengkai Zhu",
      "Geng Liu",
      "Xin Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29615": {
    "title": "HONGAT: Graph Attention Networks in the Presence of High-Order Neighbors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heng-Kai Zhang",
      "Yi-Ge Zhang",
      "Zhi Zhou",
      "Yu-Feng Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29616": {
    "title": "Memory-Efficient Reversible Spiking Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hong Zhang",
      "Yu Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29617": {
    "title": "FedTGP: Trainable Global Prototypes with Adaptive-Margin-Enhanced Contrastive Learning for Data and Model Heterogeneity in Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianqing Zhang",
      "Yang Liu",
      "Yang Hua",
      "Jian Cao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29618": {
    "title": "Reinforced Adaptive Knowledge Learning for Multimodal Fake News Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Litian Zhang",
      "Xiaoming Zhang",
      "Ziyi Zhou",
      "Feiran Huang",
      "Chaozhuo Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29619": {
    "title": "Multi-Label Supervised Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pingyue Zhang",
      "Mengyue Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29620": {
    "title": "United We Stand: Accelerating Privacy-Preserving Neural Inference by Conjunctive Optimization with Interleaved Nexus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiao Zhang",
      "Tao Xiang",
      "Chunsheng Xin",
      "Hongyi Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29621": {
    "title": "A Learnable Discrete-Prior Fusion Autoencoder with Contrastive Learning for Tabular Data Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rongchao Zhang",
      "Yiwei Lou",
      "Dexuan Xu",
      "Yongzhi Cao",
      "Hanpin Wang",
      "Yu Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29622": {
    "title": "Efficient Deweahter Mixture-of-Experts with Uncertainty-Aware Feature-Wise Linear Modulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rongyu Zhang",
      "Yulin Luo",
      "Jiaming Liu",
      "Huanrui Yang",
      "Zhen Dong",
      "Denis Gudovskiy",
      "Tomoyuki Okuno",
      "Yohei Nakata",
      "Kurt Keutzer",
      "Yuan Du",
      "Shanghang Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29623": {
    "title": "Analyzing Generalization in Policy Networks: A Case Study with the Double-Integrator System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruining Zhang",
      "Haoran Han",
      "Maolong Lv",
      "Qisong Yang",
      "Jian Cheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29624": {
    "title": "Reviewing the Forgotten Classes for Domain Adaptation of Black-Box Predictors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaojie Zhang",
      "Chun Shen",
      "Shuai Lü",
      "Zeyu Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29625": {
    "title": "TC-LIF: A Two-Compartment Spiking Neuron Model for Long-Term Sequential Modelling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shimin Zhang",
      "Qu Yang",
      "Chenxiang Ma",
      "Jibin Wu",
      "Haizhou Li",
      "Kay Chen Tan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29626": {
    "title": "Learning with Noisy Labels Using Hyperspherical Margin Weighting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuo Zhang",
      "Yuwen Li",
      "Zhongyu Wang",
      "Jianqing Li",
      "Chengyu Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29627": {
    "title": "One Step Closer to Unbiased Aleatoric Uncertainty Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wang Zhang",
      "Ziwen Martin Ma",
      "Subhro Das",
      "Tsui-Wei Lily Weng",
      "Alexandre Megretski",
      "Luca Daniel",
      "Lam M. Nguyen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29628": {
    "title": "Gaussian Process Neural Additive Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Zhang",
      "Brian Barr",
      "John Paisley"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29629": {
    "title": "From Toxic to Trustworthy: Using Self-Distillation and Semi-supervised Methods to Refine Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xianda Zhang",
      "Baolin Zheng",
      "Jianbao Hu",
      "Chengyang Li",
      "Xiaoying Bai"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29630": {
    "title": "Low Category Uncertainty and High Training Potential Instance Learning for Unsupervised Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyu Zhang",
      "Meng Kang",
      "Shuai Lü"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29631": {
    "title": "Class-Attribute Priors: Adapting Optimization to Heterogeneity and Fairness Objective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuechen Zhang",
      "Mingchen Li",
      "Jiasi Chen",
      "Christos Thrampoulidis",
      "Samet Oymak"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29632": {
    "title": "Learning Multi-Task Sparse Representation Based on Fisher Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yayu Zhang",
      "Yuhua Qian",
      "Guoshuai Ma",
      "Keyin Zheng",
      "Guoqing Liu",
      "Qingfu Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29633": {
    "title": "A Perspective of Q-value Estimation on Offline-to-Online Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinmin Zhang",
      "Jie Liu",
      "Chuming Li",
      "Yazhe Niu",
      "Yaodong Yang",
      "Yu Liu",
      "Wanli Ouyang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29634": {
    "title": "Mitigating Label Bias in Machine Learning: Fairness through Confident Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixuan Zhang",
      "Boyu Li",
      "Zenan Ling",
      "Feng Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29635": {
    "title": "Enhancing Representation of Spiking Neural Networks via Similarity-Sensitive Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhan Zhang",
      "Xiaode Liu",
      "Yuanpei Chen",
      "Weihang Peng",
      "Yufei Guo",
      "Xuhui Huang",
      "Zhe Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29636": {
    "title": "Cached Transformers: Improving Transformers with Differentiable Memory Cachde",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoyang Zhang",
      "Wenqi Shao",
      "Yixiao Ge",
      "Xiaogang Wang",
      "Jinwei Gu",
      "Ping Luo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29637": {
    "title": "An Implicit Trust Region Approach to Behavior Regularized Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhe Zhang",
      "Xiaoyang Tan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29638": {
    "title": "Towards a Theoretical Understanding of Why Local Search Works for Clustering with Fair-Center Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Zhang",
      "Junfeng Yang",
      "Limei Liu",
      "Xuesong Xu",
      "Guozhen Rong",
      "Qilong Feng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29639": {
    "title": "Symmetric Self-Paced Learning for Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Di Zhao",
      "Yun Sing Koh",
      "Gillian Dobbie",
      "Hongsheng Hu",
      "Philippe Fournier-Viger"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29640": {
    "title": "Dynamic Reactive Spiking Graph Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Zhao",
      "Xu Yang",
      "Cheng Deng",
      "Junchi Yan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29641": {
    "title": "Learning Visual Abstract Reasoning through Dual-Stream Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Zhao",
      "Chang Xu",
      "Bailu Si"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29642": {
    "title": "Robust Visual Recognition with Class-Imbalanced Open-World Noisy Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Na Zhao",
      "Gim Hee Lee"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29643": {
    "title": "From GARCH to Neural Network for Volatility Forecast",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengfei Zhao",
      "Haoren Zhu",
      "Wilfred Siu Hung NG",
      "Dik Lun Lee"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29644": {
    "title": "Robust Nonparametric Regression under Poisoning Attack",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Puning Zhao",
      "Zhiguo Wan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29645": {
    "title": "Embedded Feature Selection on Graph-Based Multi-View Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhui Zhao",
      "Guangfei Li",
      "Haizhou Yang",
      "Quanxue Gao",
      "Qianqian Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29646": {
    "title": "Domain Invariant Learning for Gaussian Processes and Bayesian Exploration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xilong Zhao",
      "Siyuan Bian",
      "Yaoyun Zhang",
      "Yuliang Zhang",
      "Qinying Gu",
      "Xinbing Wang",
      "Chenghu Zhou",
      "Nanyang Ye"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29647": {
    "title": "CcDPM: A Continuous Conditional Diffusion Probabilistic Model for Inverse Design",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanxuan Zhao",
      "Peng Zhang",
      "Guopeng Sun",
      "Zhigong Yang",
      "Jianqiang Chen",
      "Yueqing Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29648": {
    "title": "A Twist for Graph Classification: Optimizing Causal Information Flow in Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhe Zhao",
      "Pengkun Wang",
      "Haibin Wen",
      "Yudong Zhang",
      "Zhengyang Zhou",
      "Yang Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29649": {
    "title": "DCLP: Neural Architecture Predictor with Curriculum Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shenghe Zheng",
      "Hongzhi Wang",
      "Tianyu Mu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29650": {
    "title": "Confusing Pair Correction Based on Category Prototype for Domain Adaptation under Noisy Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Churan Zhi",
      "Junbao Zhuo",
      "Shuhui Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29651": {
    "title": "Knowledge-Aware Parameter Coaching for Personalized Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingjian Zhi",
      "Yuanguo Bi",
      "Wenchao Xu",
      "Haozhao Wang",
      "Tianao Xiang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29652": {
    "title": "No Prior Mask: Eliminate Redundant Action for Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dianyu Zhong",
      "Yiqin Yang",
      "Qianchuan Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29653": {
    "title": "PreRoutGNN for Timing Prediction with Order Preserving Partition: Global Circuit Pre-training, Local Delay Learning and Attentional Cell Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruizhe Zhong",
      "Junjie Ye",
      "Zhentao Tang",
      "Shixiong Kai",
      "Mingxuan Yuan",
      "Jianye Hao",
      "Junchi Yan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29654": {
    "title": "Cycle Self-Refinement for Multi-Source Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaoyang Zhou",
      "Zengmao Wang",
      "Bo Du",
      "Yong Luo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29655": {
    "title": "Explaining Generalization Power of a DNN Using Interactive Concepts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huilin Zhou",
      "Hao Zhang",
      "Huiqi Deng",
      "Dongrui Liu",
      "Wen Shen",
      "Shih-Han Chan",
      "Quanshi Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29656": {
    "title": "Token-Level Contrastive Learning with Modality-Aware Prompting for Multimodal Intent Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qianrui Zhou",
      "Hua Xu",
      "Hao Li",
      "Hanlei Zhang",
      "Xiaohan Zhang",
      "Yifan Wang",
      "Kai Gao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29657": {
    "title": "On the Robustness of Neural-Enhanced Video Streaming against Adversarial Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qihua Zhou",
      "Jingcai Guo",
      "Song Guo",
      "Ruibin Li",
      "Jie Zhang",
      "Bingjie Wang",
      "Zhenda Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29658": {
    "title": "Generalizable Task Representation Learning for Offline Meta-Reinforcement Learning with Data Limitations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Renzhe Zhou",
      "Chen-Xiao Gao",
      "Zongzhang Zhang",
      "Yang Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29659": {
    "title": "Federated Label-Noise Learning with Local Diversity Product Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaochen Zhou",
      "Xudong Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29660": {
    "title": "Abstract and Explore: A Novel Behavioral Metric with Cyclic Dynamics in Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anjie Zhu",
      "Peng-Fei Zhang",
      "Ruihong Qiu",
      "Zetao Zheng",
      "Zi Huang",
      "Jie Shao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29661": {
    "title": "Adaptive Meta-Learning Probabilistic Inference Framework for Long Sequence Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianping Zhu",
      "Xin Guo",
      "Yang Chen",
      "Yao Yang",
      "Wenbo Li",
      "Bo Jin",
      "Fei Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29662": {
    "title": "Towards the Disappearing Truth: Fine-Grained Joint Causal Influences Learning with Hidden Variable-Driven Causal Hypergraphs in Time Series",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kun Zhu",
      "Chunhui Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29663": {
    "title": "Contrastive Balancing Representation Learning for Heterogeneous Dose-Response Curves Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minqin Zhu",
      "Anpeng Wu",
      "Haoxuan Li",
      "Ruoxuan Xiong",
      "Bo Li",
      "Xiaoqing Yang",
      "Xuan Qin",
      "Peng Zhen",
      "Jiecheng Guo",
      "Fei Wu",
      "Kun Kuang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29664": {
    "title": "Every Node Is Different: Dynamically Fusing Self-Supervised Tasks for Attributed Graph Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengfei Zhu",
      "Qian Wang",
      "Yu Wang",
      "Jialu Li",
      "Qinghua Hu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29665": {
    "title": "Double Buffers CEM-TD3: More Efficient Evolution and Richer Exploration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng Zhu",
      "Chun Shen",
      "Shuai Lü",
      "Junhong Wu",
      "Daolong An"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29666": {
    "title": "Decoding Global Preferences: Temporal and Cooperative Dependency Modeling in Multi-Agent Preference-Based Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianchen Zhu",
      "Yue Qiu",
      "Haoyi Zhou",
      "Jianxin Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29667": {
    "title": "Detection and Defense of Unlearnable Examples",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Zhu",
      "Lijia Yu",
      "Xiao-Shan Gao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29668": {
    "title": "Robust Node Classification on Graph Data with Graph and Label Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yonghua Zhu",
      "Lei Feng",
      "Zhenyun Deng",
      "Yang Chen",
      "Robert Amor",
      "Michael Witbrock"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29669": {
    "title": "MFABA: A More Faithful and Accelerated Boundary-Based Attribution Method for Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyu Zhu",
      "Huaming Chen",
      "Jiayu Zhang",
      "Xinyi Wang",
      "Zhibo Jin",
      "Minhui Xue",
      "Dongxiao Zhu",
      "Kim-Kwang Raymond Choo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29670": {
    "title": "DS-AL: A Dual-Stream Analytic Learning for Exemplar-Free Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huiping Zhuang",
      "Run He",
      "Kai Tong",
      "Ziqian Zeng",
      "Cen Chen",
      "Zhiping Lin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29671": {
    "title": "Patch-Aware Sample Selection for Efficient Masked Image Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengyang Zhuge",
      "Jiaxing Wang",
      "Yong Li",
      "Yongjun Bao",
      "Peisong  Wang",
      "Jian Cheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29672": {
    "title": "Dirichlet-Based Prediction Calibration for Learning with Noisy Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen-Chen Zong",
      "Ye-Wen Wang",
      "Ming-Kun Xie",
      "Sheng-Jun Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29673": {
    "title": "Coverage-Guaranteed Prediction Sets for Out-of-Distribution Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Zou",
      "Weiwei Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29674": {
    "title": "Generalization Analysis of Machine Learning Algorithms via the Worst-Case Data-Generating Probability Measure",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinying Zou",
      "Samir M. Perlaza",
      "Iñaki Esnaola",
      "Eitan Altman"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29675": {
    "title": "Probabilistic Neural Circuits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pedro Zuidberg Dos Martires"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29676": {
    "title": "Improved Anonymous Multi-Agent Path Finding Algorithm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zain Alabedeen Ali",
      "Konstantin Yakovlev"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29677": {
    "title": "Cautiously-Optimistic Knowledge Sharing for Cooperative Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanwen Ba",
      "Xuan Liu",
      "Xinning Chen",
      "Hao Wang",
      "Yang Xu",
      "Kenli Li",
      "Shigeng Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29678": {
    "title": "Natural Strategic Ability in Stochastic Multi-Agent Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raphaël Berthon",
      "Joost-Pieter Katoen",
      "Munyque Mittelmann",
      "Aniello Murano"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29679": {
    "title": "On Alternating-Time Temporal Logic, Hyperproperties, and Strategy Sharing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raven Beutner",
      "Bernd Finkbeiner"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29680": {
    "title": "RGMComm: Return Gap Minimization via Discrete Communications in Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingdi Chen",
      "Tian Lan",
      "Carlee Joe-Wong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29681": {
    "title": "STAS: Spatial-Temporal Return Decomposition for Solving Sparse Rewards Problems in Multi-agent Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sirui Chen",
      "Zhaowei Zhang",
      "Yaodong Yang",
      "Yali Du"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29682": {
    "title": "Learning Efficient and Robust Multi-Agent Communication via Graph Information Bottleneck",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shifei Ding",
      "Wei Du",
      "Ling Ding",
      "Lili Guo",
      "Jian Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29683": {
    "title": "Expressive Multi-Agent Communication via Identity-Aware Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Du",
      "Shifei Ding",
      "Lili Guo",
      "Jian Zhang",
      "Ling Ding"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29684": {
    "title": "Situation-Dependent Causal Influence-Based Cooperative Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Du",
      "Yutong Ye",
      "Pengyu Zhang",
      "Yaning Yang",
      "Mingsong Chen",
      "Ting Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29685": {
    "title": "Learning Multi-Object Positional Relationships via Emergent Communication",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yicheng Feng",
      "Boshi An",
      "Zongqing Lu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29686": {
    "title": "Exact Algorithms and Lowerbounds for Multiagent Path Finding: Power of Treelike Topology",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Foivos Fioravantes",
      "Dušan Knop",
      "Jan Matyáš Křištan",
      "Nikolaos Melissinos",
      "Michal Opler"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29687": {
    "title": "The Irrelevance of Influencers: Information Diffusion with Re-Activation and Immunity Lasts Exponentially Long on Social Network Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tobias Friedrich",
      "Andreas Göbel",
      "Nicolas Klodt",
      "Martin S. Krejca",
      "Marcus Pappik"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29688": {
    "title": "Memory Asymmetry Creates Heteroclinic Orbits to Nash Equilibrium in Learning in Zero-Sum Games",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuma Fujimoto",
      "Kaito Ariu",
      "Kenshi Abe"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29689": {
    "title": "Factored Online Planning in Many-Agent POMDPs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maris F. L. Galesloot",
      "Thiago D. Simão",
      "Sebastian Junges",
      "Nils Jansen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29690": {
    "title": "Foundations of Reactive Synthesis for Declarative Process Specifications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luca Geatti",
      "Marco Montali",
      "Andrey Rivkin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29691": {
    "title": "Learning in Online Principal-Agent Interactions: The Power of Menus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minbiao Han",
      "Michael Albert",
      "Haifeng Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29692": {
    "title": "Stability of Multi-Agent Learning in Competitive Networks: Delaying the Onset of Chaos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aamal Hussain",
      "Francesco Belardinelli"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29693": {
    "title": "Settling Decentralized Multi-Agent Coordinated Exploration by Novelty Sharing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haobin Jiang",
      "Ziluo Ding",
      "Zongqing Lu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29694": {
    "title": "Optimistic Value Instructors for Cooperative Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Li",
      "Yupeng Zhang",
      "Jianqi Wang",
      "Yujing Hu",
      "Shaokang Dong",
      "Wenbin Li",
      "Tangjie Lv",
      "Changjie Fan",
      "Yang Gao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29695": {
    "title": "ConcaveQ: Non-monotonic Value Function Factorization via Concave Representations in Deep Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huiqun Li",
      "Hanhan Zhou",
      "Yifei Zou",
      "Dongxiao Yu",
      "Tian Lan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29696": {
    "title": "Transition-Informed Reinforcement Learning for Large-Scale Stackelberg Mean-Field Games",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengdeng Li",
      "Runsheng Yu",
      "Xinrun Wang",
      "Bo An"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29697": {
    "title": "Decentralized Gradient-Free Methods for Stochastic Non-smooth Non-convex Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenwei Lin",
      "Jingfan Xia",
      "Qi Deng",
      "Luo Luo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29698": {
    "title": "Imagine, Initialize, and Explore: An Effective Exploration Method in Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyang Liu",
      "Lipeng Wan",
      "Xinrui Yang",
      "Zhuoran Chen",
      "Xingyu Chen",
      "Xuguang Lan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29699": {
    "title": "TAPE: Leveraging Agent Topology for Cooperative Multi-Agent Policy Gradient",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingzhou Lou",
      "Junge Zhang",
      "Timothy J. Norman",
      "Kaiqi Huang",
      "Yali Du"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29700": {
    "title": "PMAC: Personalized Multi-Agent Communication",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangrui Meng",
      "Ying Tan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29701": {
    "title": "Adaptive Anytime Multi-Agent Path Finding Using Bandit-Based Large Neighborhood Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomy Phan",
      "Taoan Huang",
      "Bistra Dilkina",
      "Sven Koenig"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29702": {
    "title": "Minimum Coverage Sets for Training Robust Ad Hoc Teamwork Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Rahman",
      "Jiaxun Cui",
      "Peter Stone"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29703": {
    "title": "Decentralized Monte Carlo Tree Search for Partially Observable Multi-Agent Pathfinding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexey Skrynnik",
      "Anton Andreychuk",
      "Konstantin Yakovlev",
      "Aleksandr Panov"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29704": {
    "title": "Learn to Follow: Decentralized Lifelong Multi-Agent Pathfinding via Planning and Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexey Skrynnik",
      "Anton Andreychuk",
      "Maria Nesterova",
      "Konstantin Yakovlev",
      "Aleksandr Panov"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29705": {
    "title": "What Makes Good Collaborative Views? Contrastive Mutual Information Maximization for Multi-Agent Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wanfang Su",
      "Lixing Chen",
      "Yang Bai",
      "Xi Lin",
      "Gaolei Li",
      "Zhe Qu",
      "Pan Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29706": {
    "title": "Bidirectional Temporal Plan Graph: Enabling Switchable Passing Orders for More Efficient Multi-Agent Path Finding Plan Execution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Su",
      "Rishi Veerapaneni",
      "Jiaoyang Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29707": {
    "title": "Large-Scale Multi-Robot Coverage Path Planning via Local Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingtao Tang",
      "Hang Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29708": {
    "title": "Robust Communicative Multi-Agent Reinforcement Learning with Active Defense",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lebin Yu",
      "Yunbo Qiu",
      "Quanming Yao",
      "Yuan Shen",
      "Xudong Zhang",
      "Jian Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29709": {
    "title": "Leveraging Partial Symmetry for Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Yu",
      "Rongye Shi",
      "Pu Feng",
      "Yongkai Tian",
      "Simin Li",
      "Shuhao Liao",
      "Wenjun Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29710": {
    "title": "ProAgent: Building Proactive Cooperative Agents with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ceyao Zhang",
      "Kaijie Yang",
      "Siyi Hu",
      "Zihao Wang",
      "Guanghe Li",
      "Yihang Sun",
      "Cheng Zhang",
      "Zhaowei Zhang",
      "Anji Liu",
      "Song-Chun Zhu",
      "Xiaojun Chang",
      "Junge Zhang",
      "Feng Yin",
      "Yitao Liang",
      "Yaodong Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29711": {
    "title": "Intrinsic Action Tendency Consistency for Cooperative Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junkai Zhang",
      "Yifan Zhang",
      "Xi Sheryl Zhang",
      "Yifan Zang",
      "Jian Cheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29712": {
    "title": "Emergent Communication for Numerical Concepts Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Enshuai Zhou",
      "Yifan Hao",
      "Rui Zhang",
      "Yuxuan Guo",
      "Zidong Du",
      "Xishan Zhang",
      "Xinkai Song",
      "Chao Wang",
      "Xuehai Zhou",
      "Jiaming Guo",
      "Qi Yi",
      "Shaohui Peng",
      "Di Huang",
      "Ruizhi Chen",
      "Qi Guo",
      "Yunji Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29713": {
    "title": "Decomposing Temporal Equilibrium Strategy for Coordinated Distributed Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenyang Zhu",
      "Wen Si",
      "Jinyu Zhu",
      "Zhihao Jiang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29714": {
    "title": "Balancing Humans and Machines: A Study on Integration Scale and Its Impact on Collaborative Performance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Zou",
      "Sannyuya Liu",
      "Yawei Luo",
      "Yaqi Liu",
      "Jintian Feng",
      "Mengqi Wei",
      "Jianwen Sun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29715": {
    "title": "Frame Semantic Role Labeling Using Arbitrary-Order Conditional Random Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaoyi Ai",
      "Kewei Tu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29716": {
    "title": "DTF-AT: Decoupled Time-Frequency Audio Transformer for Event Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tony Alex",
      "Sara Ahmed",
      "Armin Mustafa",
      "Muhammad Awais",
      "Philip JB Jackson"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29717": {
    "title": "WikiSQE: A Large-Scale Dataset for Sentence Quality Estimation in Wikipedia",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kenichiro Ando",
      "Satoshi Sekine",
      "Mamoru Komachi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29718": {
    "title": "Beyond Grounding: Extracting Fine-Grained Event Hierarchies across Modalities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hammad Ayyubi",
      "Christopher Thomas",
      "Lovish Chum",
      "Rahul Lokesh",
      "Long Chen",
      "Yulei Niu",
      "Xudong Lin",
      "Xuande Feng",
      "Jaywon Koo",
      "Sounak Ray",
      "Shih-Fu Chang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29719": {
    "title": "All Should Be Equal in the Eyes of LMs: Counterfactually Aware Fair Text Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pragyan Banerjee",
      "Abhinav Java",
      "Surgan Jandial",
      "Simra Shahid",
      "Shaz Furniturewala",
      "Balaji Krishnamurthy",
      "Sumit Bhatia"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29720": {
    "title": "Graph of Thoughts: Solving Elaborate Problems with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maciej Besta",
      "Nils Blach",
      "Ales Kubicek",
      "Robert Gerstenberger",
      "Michal Podstawski",
      "Lukas Gianinazzi",
      "Joanna Gajda",
      "Tomasz Lehmann",
      "Hubert Niewiadomski",
      "Piotr Nyczyk",
      "Torsten Hoefler"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29721": {
    "title": "When Do Program-of-Thought Works for Reasoning?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Bi",
      "Ningyu Zhang",
      "Yinuo Jiang",
      "Shumin Deng",
      "Guozhou Zheng",
      "Huajun Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29722": {
    "title": "Beyond Attention: Breaking the Limits of Transformer Context Length with Recurrent Memory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aydar Bulatov",
      "Yuri Kuratov",
      "Yermek Kapushev",
      "Mikhail Burtsev"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29723": {
    "title": "MedBench: A Large-Scale Chinese Benchmark for Evaluating Medical Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Cai",
      "Linlin Wang",
      "Ye Wang",
      "Gerard de Melo",
      "Ya Zhang",
      "Yanfeng Wang",
      "Liang He"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29724": {
    "title": "CAR-Transformer: Cross-Attention Reinforcement Transformer for Cross-Lingual Summarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuang Cai",
      "Yuyu Yuan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29725": {
    "title": "Compositional Generalization for Multi-Label Text Classification: A Data-Augmentation Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuyang Chai",
      "Zhuang Li",
      "Jiahui Liu",
      "Lei Chen",
      "Fei Li",
      "Donghong Ji",
      "Chong Teng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29726": {
    "title": "Counterfactual-Enhanced Information Bottleneck for Aspect-Based Sentiment Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingshan Chang",
      "Min Yang",
      "Qingshan Jiang",
      "Ruifeng Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29727": {
    "title": "Visual Instruction Tuning with Polite Flamingo",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Delong Chen",
      "Jianfeng Liu",
      "Wenliang Dai",
      "Baoyuan Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29728": {
    "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawei Chen",
      "Hongyu Lin",
      "Xianpei Han",
      "Le Sun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29729": {
    "title": "CIDR: A Cooperative Integrated Dynamic Refining Method for Minimal Feature Removal Problem",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qian Chen",
      "Taolin Zhang",
      "Dongyang Li",
      "Xiaofeng He"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29730": {
    "title": "Is a Large Language Model a Good Annotator for Event Extraction?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruirui Chen",
      "Chengwei Qin",
      "Weifeng Jiang",
      "Dongkyu Choi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29731": {
    "title": "Modeling Adaptive Inter-Task Feature Interactions via Sentiment-Aware Contrastive Learning for Joint Aspect-Sentiment Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Chen",
      "Yuxuan Liu",
      "Zhao Zhang",
      "Fuzhen Zhuang",
      "Jiang Zhong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29732": {
    "title": "From Coarse to Fine: A Distillation Method for Fine-Grained Emotion-Causal Span Pair Extraction in Conversation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinhao Chen",
      "Chong Yang",
      "Changzhi Sun",
      "Man Lan",
      "Aimin Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29733": {
    "title": "Divergence-Guided Simultaneous Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinjie Chen",
      "Kai Fan",
      "Wei Luo",
      "Linlin Zhang",
      "Libo Zhao",
      "Xinggao Liu",
      "Zhongqiang Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29734": {
    "title": "Benchmarking Large Language Models on Controllable Generation under Diversified Instructions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihan Chen",
      "Benfeng Xu",
      "Quan Wang",
      "Yi Liu",
      "Zhendong Mao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29735": {
    "title": "Journey to the Center of the Knowledge Neurons: Discoveries of Language-Independent Knowledge Neurons and Degenerate Knowledge Neurons",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuheng Chen",
      "Pengfei Cao",
      "Yubo Chen",
      "Kang Liu",
      "Jun Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29736": {
    "title": "Talk Funny! A Large-Scale Humor Response Dataset with Chain-of-Humor Interpretation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuyan Chen",
      "Yichen Yuan",
      "Panjun Liu",
      "Dayiheng Liu",
      "Qinghao Guan",
      "Mengfei Guo",
      "Haiming Peng",
      "Bang Liu",
      "Zhixu Li",
      "Yanghua Xiao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29737": {
    "title": "Editing Language Model-Based Knowledge Graph Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyuan Cheng",
      "Ningyu Zhang",
      "Bozhong Tian",
      "Xi Chen",
      "Qingbin Liu",
      "Huajun Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29738": {
    "title": "Towards Multi-Intent Spoken Language Understanding via Hierarchical Attention and Optimal Transport",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuxin Cheng",
      "Zhihong Zhu",
      "Hongxiang Li",
      "Yaowei Li",
      "Xianwei Zhuang",
      "Yuexian Zou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29739": {
    "title": "Cooper: Coordinating Specialized Agents towards a Complex Dialogue Goal",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Cheng",
      "Wenge Liu",
      "Jian Wang",
      "Chak Tou Leong",
      "Yi Ouyang",
      "Wenjie Li",
      "Xian Wu",
      "Yefeng Zheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29740": {
    "title": "DDDM-VC: Decoupled Denoising Diffusion Models with Disentangled Representation and Prior Mixup for Verified Robust Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ha-Yeong Choi",
      "Sang-Hoon Lee",
      "Seong-Whan Lee"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29741": {
    "title": "How to Protect Copyright Data in Optimization of Large Language Models?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Timothy Chu",
      "Zhao Song",
      "Chiwun Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29742": {
    "title": "Unsupervised Layer-Wise Score Aggregation for Textual OOD Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maxime Darrin",
      "Guillaume Staerman",
      "Eduardo Dadalto Camara Gomes",
      "Jackie C. K. Cheung",
      "Pablo Piantanida",
      "Pierre Colombo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29743": {
    "title": "Spanning the Spectrum of Hatred Detection: A Persian Multi-Label Hate Speech Dataset with Annotator Rationales",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zahra  Delbari",
      "Nafise Sadat Moosavi",
      "Mohammad Taher Pilehvar"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29744": {
    "title": "Enhancing Bilingual Lexicon Induction via Bi-directional Translation Pair Retrieving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiuyu Ding",
      "Hailong Cao",
      "Tiejun Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29745": {
    "title": "From Retrieval to Generation: A Simple and Unified Generative Model for End-to-End Task-Oriented Dialogue",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyuan Ding",
      "Zhihao Yang",
      "Ling Luo",
      "Yuanyuan Sun",
      "Hongfei Lin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29746": {
    "title": "How to Trade Off the Quantity and Capacity of Teacher Ensemble: Learning Categorical Distribution to Stochastically Employ a Teacher for Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixiang Ding",
      "Guoqing Jiang",
      "Shuai Zhang",
      "Lin Guo",
      "Wei Lin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29747": {
    "title": "UniCATS: A Unified Context-Aware Text-to-Speech Framework with Contextual VQ-Diffusion and Vocoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenpeng Du",
      "Yiwei Guo",
      "Feiyu Shen",
      "Zhijun Liu",
      "Zheng Liang",
      "Xie Chen",
      "Shuai Wang",
      "Hui Zhang",
      "Kai Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29748": {
    "title": "DocMSU: A Comprehensive Benchmark for Document-Level Multimodal Sarcasm Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hang Du",
      "Guoshun Nan",
      "Sicheng Zhang",
      "Binzhu Xie",
      "Junrui Xu",
      "Hehe Fan",
      "Qimei Cui",
      "Xiaofeng Tao",
      "Xudong Jiang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29749": {
    "title": "AdaCCD: Adaptive Semantic Contrasts Discovery Based Cross Lingual Adaptation for Code Clone Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangkai Du",
      "Tengfei Ma",
      "Lingfei Wu",
      "Xuhong Zhang",
      "Shouling Ji"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29750": {
    "title": "Frugal LMs Trained to Invoke Symbolic Solvers Achieve Parameter-Efficient Arithmetic Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Subhabrata Dutta",
      "Ishan Pandey",
      "Joykirat Singh",
      "Sunny Manchanda",
      "Soumen Chakrabarti",
      "Tanmoy Chakraborty"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29751": {
    "title": "Can Large Language Models Serve as Rational Players in Game Theory? A Systematic Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Caoyun Fan",
      "Jindou Chen",
      "Yaohui Jin",
      "Hao He"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29752": {
    "title": "Enhancing Low-Resource Relation Representations through Multi-View Decoupling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenghao Fan",
      "Wei Wei",
      "Xiaoye Qu",
      "Zhenyi Lu",
      "Wenfeng Xie",
      "Yu Cheng",
      "Dangyang Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29753": {
    "title": "Quantum-Inspired Neural Network with Runge-Kutta Method",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zipeng Fan",
      "Jing Zhang",
      "Peng Zhang",
      "Qianxi Lin",
      "Hui Gao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29754": {
    "title": "Large Language Models Are Neurosymbolic Reasoners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meng Fang",
      "Shilong Deng",
      "Yudi Zhang",
      "Zijing Shi",
      "Ling Chen",
      "Mykola Pechenizkiy",
      "Jun Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29755": {
    "title": "Combining Multiple Supervision for Robust Zero-Shot Dense Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Fang",
      "Qingyao Ai",
      "Jingtao Zhan",
      "Yiqun Liu",
      "Xiaolong Wu",
      "Zhao Cao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29756": {
    "title": "Watermarking Conditional Text Generation for AI Detection: Unveiling Challenges and a Semantic-Aware Watermark Remedy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Fu",
      "Deyi Xiong",
      "Yue Dong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29757": {
    "title": "BAND: Biomedical Alert News Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihao Fu",
      "Meiru Zhang",
      "Zaiqiao Meng",
      "Yannan Shen",
      "David Buckeridge",
      "Nigel Collier"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29758": {
    "title": "Winnie: Task-Oriented Dialog System with Structure-Aware Contrastive Learning and Enhanced Policy Planning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaizhi Gao",
      "Tianyu Wang",
      "Zhongjing Ma",
      "Suli Zou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29759": {
    "title": "Confucius: Iterative Tool Learning from Introspection Feedback by Easy-to-Difficult Curriculum",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shen Gao",
      "Zhengliang Shi",
      "Minghang Zhu",
      "Bowen Fang",
      "Xin Xin",
      "Pengjie Ren",
      "Zhumin Chen",
      "Jun Ma",
      "Zhaochun Ren"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29760": {
    "title": "Customizing Language Model Responses with Contrastive In-Context Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Gao",
      "Kamalika Das"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29761": {
    "title": "DA-Net: A Disentangled and Adaptive Network for Multi-Source Cross-Lingual Transfer Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ling Ge",
      "Chunming Hu",
      "Guanghui Ma",
      "Jihong Liu",
      "Hong Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29762": {
    "title": "Discrepancy and Uncertainty Aware Denoising Knowledge Distillation for Zero-Shot Cross-Lingual Named Entity Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ling Ge",
      "Chunming Hu",
      "Guanghui Ma",
      "Jihong Liu",
      "Hong Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29763": {
    "title": "Who Knows the Answer? Finding the Best Model and Prompt for Each Query Using Confidence-Based Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Walter Gerych",
      "Yara Rizk",
      "Vatche Isahagian",
      "Vinod Muthusamy",
      "Evelyn Duesterwald",
      "Praveen Venkateswaran"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29764": {
    "title": "A General Search-Based Framework for Generating Textual Counterfactual Explanations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Gilo",
      "Shaul Markovitch"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29765": {
    "title": "What Makes Quantization for Large Language Model Hard? An Empirical Study from the Lens of Perturbation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuocheng Gong",
      "Jiahao Liu",
      "Jingang Wang",
      "Xunliang Cai",
      "Dongyan Zhao",
      "Rui Yan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29766": {
    "title": "CoPL: Contextual Prompt Learning for Vision-Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Koustava Goswami",
      "Srikrishna Karanam",
      "Prateksha Udhayanan",
      "K J Joseph",
      "Balaji Vasan Srinivasan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29767": {
    "title": "Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhouhong Gu",
      "Xiaoxuan Zhu",
      "Haoning Ye",
      "Lin Zhang",
      "Jianchen Wang",
      "Yixin Zhu",
      "Sihang Jiang",
      "Zhuozhi Xiong",
      "Zihan Li",
      "Weijie Wu",
      "Qianyu He",
      "Rui Xu",
      "Wenhao Huang",
      "Jingping Liu",
      "Zili Wang",
      "Shusen Wang",
      "Weiguo Zheng",
      "Hongwei Feng",
      "Yanghua Xiao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29768": {
    "title": "DINGO: Towards Diverse and Fine-Grained Instruction-Following Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihui Gu",
      "Xingwu Sun",
      "Fengzong Lian",
      "Zhanhui Kang",
      "Chengzhong Xu",
      "Ju Fan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29769": {
    "title": "MM-TTS: Multi-Modal Prompt Based Style Transfer for Expressive Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhao Guan",
      "Yishuang Li",
      "Tao Li",
      "Hukai Huang",
      "Feng Wang",
      "Jiayan Lin",
      "Lingyan Huang",
      "Lin Li",
      "Qingyang Hong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29770": {
    "title": "Mitigating Large Language Model Hallucinations via Autonomous Knowledge Graph-Based Retrofitting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyan Guan",
      "Yanjiang Liu",
      "Hongyu Lin",
      "Yaojie Lu",
      "Ben He",
      "Xianpei Han",
      "Le Sun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29771": {
    "title": "Detecting and Preventing Hallucinations in Large Vision Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anisha Gunjal",
      "Jihan Yin",
      "Erhan Bas"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29772": {
    "title": "MolTailor: Tailoring Chemical Molecular Representation to Specific Tasks via Text Prompts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoqiang Guo",
      "Sendong Zhao",
      "Haochun Wang",
      "Yanrui Du",
      "Bing Qin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29773": {
    "title": "Audio Generation with Multiple Conditional Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhifang Guo",
      "Jianguo Mao",
      "Rui Tao",
      "Long Yan",
      "Kazushige Ouchi",
      "Hong Liu",
      "Xiangdong Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29774": {
    "title": "Small Language Model Can Self-Correct",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haixia Han",
      "Jiaqing Liang",
      "Jie Shi",
      "Qianyu He",
      "Yanghua Xiao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29775": {
    "title": "Decoupling Representation and Knowledge for Few-Shot Intent Classification and Slot Filling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Han",
      "Yixiong Zou",
      "Haozhao Wang",
      "Jun Wang",
      "Wei Liu",
      "Yao Wu",
      "Tao Zhang",
      "Ruixuan Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29776": {
    "title": "Multi-Modal Latent Space Learning for Chain-of-Thought Reasoning in Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liqi He",
      "Zuchao Li",
      "Xiantao Cai",
      "Ping Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29777": {
    "title": "Can Large Language Models Understand Real-World Complex Instructions?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qianyu He",
      "Jie Zeng",
      "Wenhao Huang",
      "Lina Chen",
      "Jin Xiao",
      "Qianxi He",
      "Xunzhe Zhou",
      "Jiaqing Liang",
      "Yanghua Xiao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29778": {
    "title": "Improving Factual Error Correction by Learning to Inject Factual Errors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingwei He",
      "Qianru Zhang",
      "A-Long Jin",
      "Jun Ma",
      "Yuan Yuan",
      "Siu Ming Yiu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29779": {
    "title": "Text2Analysis: A Benchmark of Table Question Answering with Advanced Data Analysis and Unclear Queries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyi He",
      "Mengyu Zhou",
      "Xinrun Xu",
      "Xiaojun Ma",
      "Rui Ding",
      "Lun Du",
      "Yan Gao",
      "Ran Jia",
      "Xu Chen",
      "Shi Han",
      "Zejian Yuan",
      "Dongmei Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29780": {
    "title": "ParaGuide: Guided Diffusion Paraphrasers for Plug-and-Play Textual Style Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zachary Horvitz",
      "Ajay Patel",
      "Chris Callison-Burch",
      "Zhou Yu",
      "Kathleen McKeown"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29781": {
    "title": "ShareBERT: Embeddings Are Capable of Learning Hidden Layers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jia Cheng Hu",
      "Roberto Cavicchioli",
      "Giulia Berardinelli",
      "Alessandro Capotondi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29782": {
    "title": "LLM vs Small Model? Large Language Model Based Text Augmentation Enhanced Personality Detection Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linmei Hu",
      "Hongyu He",
      "Duokang Wang",
      "Ziwang Zhao",
      "Yingxia Shao",
      "Liqiang Nie"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29783": {
    "title": "Learning Robust Rationales for Model Explainability: A Guidance-Based Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuaibo Hu",
      "Kui Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29784": {
    "title": "Separate the Wheat from the Chaff: Model Deficiency Unlearning via Parameter-Efficient Module Operation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinshuo Hu",
      "Dongfang Li",
      "Baotian Hu",
      "Zihao Zheng",
      "Zhenyu Liu",
      "Min Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29785": {
    "title": "Three Heads Are Better than One: Improving Cross-Domain NER with Progressive Decomposed Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuming Hu",
      "Zhaochen Hong",
      "Yong Jiang",
      "Zhichao Lin",
      "Xiaobin Wang",
      "Pengjun Xie",
      "Philip S. Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29786": {
    "title": "Uncovering and Mitigating the Hidden Chasm: A Study on the Text-Text Domain Gap in Euphemism Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxue Hu",
      "Junsong Li",
      "Mingmin Wu",
      "Zhongqiang Huang",
      "Gang Chen",
      "Ying Sha"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29787": {
    "title": "PoetryDiffusion: Towards Joint Semantic and Metrical Manipulation in Poetry Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyuan Hu",
      "Chumin Liu",
      "Yue Feng",
      "Anh Tuan Luu",
      "Bryan Hooi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29788": {
    "title": "Towards Equipping Transformer with the Ability of Systematic Compositionality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Huang",
      "Peixin Qin",
      "Wenqiang Lei",
      "Jiancheng Lv"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29789": {
    "title": "Cross-Modal and Uni-Modal Soft-Label Alignment for Image-Text Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hailang Huang",
      "Zhijie Nie",
      "Ziqiao Wang",
      "Ziyu Shang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29790": {
    "title": "Response Enhanced Semi-supervised Dialogue Query Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianheng Huang",
      "Ante Wang",
      "Linfeng Gao",
      "Linfeng Song",
      "Jinsong Su"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29791": {
    "title": "PMRC: Prompt-Based Machine Reading Comprehension for Few-Shot Named Entity Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jin Huang",
      "Danfeng Yan",
      "Yuanqiang Cai"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29792": {
    "title": "Revisiting Document-Level Relation Extraction with Context-Guided Link Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Monika Jain",
      "Raghava Mutharaju",
      "Ramakanth Kavuluru",
      "Kuldeep Singh"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29793": {
    "title": "Enhancing Zero-Shot Multi-Speaker TTS with Negated Speaker Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yejin Jeon",
      "Yunsu Kim",
      "Gary Geunbae  Lee"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29794": {
    "title": "Chain-of-Thought Improves Text Generation with Citations in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bin Ji",
      "Huijun Liu",
      "Mingzhe Du",
      "See-Kiong Ng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29795": {
    "title": "Debiasing Multimodal Sarcasm Detection with Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengzhao Jia",
      "Can Xie",
      "Liqiang Jing"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29796": {
    "title": "ZO-AdaMU Optimizer: Adapting Perturbation by the Momentum and Uncertainty in Zeroth-Order Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuoran Jiang",
      "Qingcai Chen",
      "Youcheng Pan",
      "Yang Xiang",
      "Yukang Lin",
      "Xiangping Wu",
      "Chuanyi Liu",
      "Xiaobao Song"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29797": {
    "title": "Unsupervised Extractive Summarization with Learnable Length Control Strategies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Renlong Jie",
      "Xiaojun Meng",
      "Xin Jiang",
      "Qun Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29798": {
    "title": "BOK-VQA: Bilingual outside Knowledge-Based Visual Question Answering via Graph Representation Pretraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "MinJun Kim",
      "SeungWoo Song",
      "YouHan Lee",
      "Haneol Jang",
      "KyungTae Lim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29799": {
    "title": "Improving Knowledge Extraction from LLMs for Task Learning through Agent Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "James R. Kirk",
      "Robert E. Wray",
      "Peter Lindes",
      "John E. Laird"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29800": {
    "title": "On Unsupervised Domain Adaptation: Pseudo Label Guided Mixup for Adversarial Prompt Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fanshuang Kong",
      "Richong Zhang",
      "Ziqiao Wang",
      "Yongyi Mao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29801": {
    "title": "A Hierarchical Network for Multimodal Document-Level Relation Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingxing Kong",
      "Jiuliang Wang",
      "Zheng Ma",
      "Qifeng Zhou",
      "Jianbing Zhang",
      "Liang He",
      "Jiajun Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29802": {
    "title": "Large Language Models Are Clinical Reasoners: Reasoning-Aware Diagnosis Framework with Prompt-Generated Rationales",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taeyoon Kwon",
      "Kai Tzu-iunn Ong",
      "Dongjin Kang",
      "Seungjun Moon",
      "Jeong Ryong Lee",
      "Dosik Hwang",
      "Beomseok Sohn",
      "Yongsik Sim",
      "Dongha Lee",
      "Jinyoung Yeo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29803": {
    "title": "Frequency Spectrum Is More Effective for Multimodal Representation and Fusion: A Multimodal Spectrum Rumor Detector",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "An Lao",
      "Qi Zhang",
      "Chongyang Shi",
      "Longbing Cao",
      "Kun Yi",
      "Liang Hu",
      "Duoqian Miao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29804": {
    "title": "LAMPAT: Low-Rank Adaption for Multilingual Paraphrasing Using Adversarial Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Khoi M. Le",
      "Trinh Pham",
      "Tho Quan",
      "Anh Tuan Luu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29805": {
    "title": "Continual Relation Extraction via Sequential Multi-Task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thanh-Thien Le",
      "Manh Nguyen",
      "Tung Thanh Nguyen",
      "Linh Ngo Van",
      "Thien Huu Nguyen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29806": {
    "title": "Labels Need Prompts Too: Mask Matching for Natural Language Understanding Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Li",
      "Wei Ye",
      "Quansen Wang",
      "Wen Zhao",
      "Shikun Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29807": {
    "title": "Harnessing Holistic Discourse Features and Triadic Interaction for Sentiment Quadruple Extraction in Dialogues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bobo Li",
      "Hao Fei",
      "Lizi Liao",
      "Yu Zhao",
      "Fangfang Su",
      "Fei Li",
      "Donghong Ji"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29808": {
    "title": "Task Contamination: Language Models May Not Be Few-Shot Anymore",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changmao Li",
      "Jeffrey Flanigan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29809": {
    "title": "Dialogue for Prompting: A Policy-Gradient-Based Discrete Prompt Generation for Few-Shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengzhengxu Li",
      "Xiaoming Liu",
      "Yichen Wang",
      "Duyi Li",
      "Yu Lan",
      "Chao Shen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29810": {
    "title": "DeepSpeed Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Conglong Li",
      "Zhewei Yao",
      "Xiaoxia Wu",
      "Minjia Zhang",
      "Connor Holmes",
      "Cheng Li",
      "Yuxiong He"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29811": {
    "title": "Advancing Spatial Reasoning in Large Language Models: An In-Depth Evaluation and Enhancement Using the StepGame Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangjun Li",
      "David C. Hogg",
      "Anthony G. Cohn"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29812": {
    "title": "Exploiting Auxiliary Caption for Video Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongxiang Li",
      "Meng Cao",
      "Xuxin Cheng",
      "Yaowei Li",
      "Zhihong Zhu",
      "Yuexian Zou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29813": {
    "title": "VLN-Video: Utilizing Driving Videos for Outdoor Vision-and-Language Navigation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jialu Li",
      "Aishwarya Padmakumar",
      "Gaurav Sukhatme",
      "Mohit Bansal"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29814": {
    "title": "Enhancing Multi-Label Classification via Dynamic Label-Order Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiangnan Li",
      "Yice Zhang",
      "Shiwei Chen",
      "Ruifeng Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29815": {
    "title": "Norm Tweaking: High-Performance Low-Bit Quantization of Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liang Li",
      "Qingyuan Li",
      "Bo Zhang",
      "Xiangxiang Chu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29816": {
    "title": "Object Attribute Matters in Visual Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peize Li",
      "Qingyi Si",
      "Peng Fu",
      "Zheng Lin",
      "Yan Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29817": {
    "title": "Translate Meanings, Not Just Words: IdiomKB's Role in Optimizing Idiomatic Translation with Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuang Li",
      "Jiangjie Chen",
      "Siyu Yuan",
      "Xinyi Wu",
      "Hao Yang",
      "Shimin Tao",
      "Yanghua Xiao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29818": {
    "title": "PMET: Precise Model Editing in a Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaopeng Li",
      "Shasha Li",
      "Shezheng Song",
      "Jing Yang",
      "Jun Ma",
      "Jie Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29819": {
    "title": "Dialogues Are Not Just Text: Modeling Cognition for Dialogue Coherence Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xue Li",
      "Jia Su",
      "Yang Yang",
      "Zipeng Gao",
      "Xinyu Duan",
      "Yi Guan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29820": {
    "title": "EcomGPT: Instruction-Tuning Large Language Models with Chain-of-Task Tasks for E-commerce",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangning Li",
      "Shirong Ma",
      "Xiaobin Wang",
      "Shen Huang",
      "Chengyue Jiang",
      "Hai-Tao Zheng",
      "Pengjun Xie",
      "Fei Huang",
      "Yong Jiang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29821": {
    "title": "Turning Dust into Gold: Distilling Complex Reasoning Capabilities from LLMs by Leveraging Negative Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiwei Li",
      "Peiwen Yuan",
      "Shaoxiong Feng",
      "Boyuan Pan",
      "Bin Sun",
      "Xinglin Wang",
      "Heda Wang",
      "Kan Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29822": {
    "title": "LatestEval: Addressing Data Contamination in Language Model Evaluation through Dynamic and Time-Sensitive Test Construction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yucheng Li",
      "Frank Guerin",
      "Chenghua Lin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29823": {
    "title": "FlexKBQA: A Flexible LLM-Powered Framework for Few-Shot Knowledge Base Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenyu Li",
      "Sunqi Fan",
      "Yu Gu",
      "Xiuxing Li",
      "Zhichao Duan",
      "Bowen Dong",
      "Ning Liu",
      "Jianyong Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29824": {
    "title": "Machine-Created Universal Language for Cross-Lingual Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaobo Liang",
      "Quanzhi Zhu",
      "Junhe Zhao",
      "Nan Duan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29825": {
    "title": "CFEVER: A Chinese Fact Extraction and VERification Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ying-Jia Lin",
      "Chun-Yi Lin",
      "Chia-Jen Yeh",
      "Yi-Ting Li",
      "Yun-Yu Hu",
      "Chih-Hao Hsu",
      "Mei-Feng Lee",
      "Hung-Yu Kao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29826": {
    "title": "Bootstrapping Large Language Models for Radiology Report Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chang Liu",
      "Yuanhe Tian",
      "Weidong Chen",
      "Yan Song",
      "Yongdong Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29827": {
    "title": "Liberating Seen Classes: Boosting Few-Shot and Zero-Shot Text Classification via Anchor Generation and Classification Reframing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Liu",
      "Siyang Zhao",
      "Xiaotong Zhang",
      "Feng Zhang",
      "Wei Wang",
      "Fenglong Ma",
      "Hongyang Chen",
      "Hong Yu",
      "Xianchao Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29828": {
    "title": "Beyond Entities: A Large-Scale Multi-Modal Knowledge Graph with Triplet Fact Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingping Liu",
      "Mingchuan Zhang",
      "Weichen Li",
      "Chao Wang",
      "Shuang Li",
      "Haiyun Jiang",
      "Sihang Jiang",
      "Yanghua Xiao",
      "Yunwen Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29829": {
    "title": "Chinese Spelling Correction as Rephrasing Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linfeng Liu",
      "Hongqiu Wu",
      "Hai Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29830": {
    "title": "TA&AT: Enhancing Task-Oriented Dialog with Turn-Level Auxiliary Tasks and Action-Tree Based Scheduled Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Longxiang Liu",
      "Xiuxing Li",
      "Yang Feng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29831": {
    "title": "Hierarchical Aligned Multimodal Learning for NER on Tweet Posts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peipei Liu",
      "Hong Li",
      "Yimo Ren",
      "Jie Liu",
      "Shuaizong Si",
      "Hongsong Zhu",
      "Limin Sun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29832": {
    "title": "Adaptive Prompt Routing for Arbitrary Text Style Transfer with Pre-trained Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingyi Liu",
      "Jinghui Qin",
      "Wenxuan Ye",
      "Hao Mou",
      "Yuxuan He",
      "Keze Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29833": {
    "title": "Emotion Rendering for Conversational Speech Synthesis with Heterogeneous Graph-Based Context Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Liu",
      "Yifan Hu",
      "Yi Ren",
      "Xiang Yin",
      "Haizhou Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29834": {
    "title": "Robust Evaluation Measures for Evaluating Social Biases in Masked Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29835": {
    "title": "Improved Graph Contrastive Learning for Short Text Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yonghao Liu",
      "Lan Huang",
      "Fausto Giunchiglia",
      "Xiaoyue Feng",
      "Renchu Guan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29836": {
    "title": "QuerySum: A Multi-Document Query-Focused Summarization Dataset Augmented with Similar Query Clusters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yushan Liu",
      "Zili Wang",
      "Ruifeng Yuan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29837": {
    "title": "Generative Multi-Modal Knowledge Retrieval with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinwei Long",
      "Jiali Zeng",
      "Fandong Meng",
      "Zhiyuan Ma",
      "Kaiyan Zhang",
      "Bowen Zhou",
      "Jie Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29838": {
    "title": "Synergistic Anchored Contrastive Pre-training for Few-Shot Relation Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Da Luo",
      "Yanglei Gan",
      "Rui Hou",
      "Run Lin",
      "Qiao Liu",
      "Yuxiang Cai",
      "Wannian Gao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29839": {
    "title": "STAR: Boosting Low-Resource Information Extraction by Structure-to-Text Data Generation with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingyu Derek Ma",
      "Xiaoxuan Wang",
      "Po-Nien Kung",
      "P. Jeffrey Brantingham",
      "Nanyun Peng",
      "Wei Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29840": {
    "title": "Mastering Context-to-Label Representation Transformation for Event Causality Identification with Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hieu Man",
      "Franck Dernoncourt",
      "Thien Huu Nguyen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29841": {
    "title": "Span Graph Transformer for Document-Level Named Entity Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongli Mao",
      "Xian-Ling Mao",
      "Hanlin Tang",
      "Yu-Ming Shang",
      "Heyan Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29842": {
    "title": "Underspecification in Language Modeling Tasks: A Causality-Informed Study of Gendered Pronoun Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emily McMilin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29843": {
    "title": "MCL-NER: Cross-Lingual Named Entity Recognition via Multi-View Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ying Mo",
      "Jian Yang",
      "Jiahao Liu",
      "Qifan Wang",
      "Ruoyu Chen",
      "Jingang Wang",
      "Zhoujun Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29844": {
    "title": "KAM-CoT: Knowledge Augmented Multimodal Chain-of-Thoughts Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Debjyoti Mondal",
      "Suraj Modi",
      "Subhadarshi Panda",
      "Rituraj Singh",
      "Godawari Sudhakar Rao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29845": {
    "title": "Accelerating the Global Aggregation of Local Explanations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alon Mor",
      "Yonatan Belinkov",
      "Benny Kimelfeld"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29846": {
    "title": "Self-Supervised Disentangled Representation Learning for Robust Target Speech Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoxi Mu",
      "Xinyu Yang",
      "Sining Sun",
      "Qing Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29847": {
    "title": "READ-PVLA: Recurrent Adapter with Partial Video-Language Alignment for Parameter-Efficient Transfer Learning in Low-Resource Video-Language Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thong Nguyen",
      "Xiaobao Wu",
      "Xinshuai Dong",
      "Khoi M. Le",
      "Zhiyuan Hu",
      "Cong-Duy Nguyen",
      "See-Kiong Ng",
      "Anh Tuan Luu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29848": {
    "title": "Code-Style In-Context Learning for Knowledge-Based Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhijie Nie",
      "Richong Zhang",
      "Zhongyuan Wang",
      "Xudong Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29849": {
    "title": "Aspect-Based Sentiment Analysis with Explicit Sentiment Augmentations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jihong Ouyang",
      "Zhiyao Yang",
      "Silong Liang",
      "Bing Wang",
      "Yimeng Wang",
      "Ximing Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29850": {
    "title": "Fact-Driven Logical Reasoning for Machine Reading Comprehension",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siru Ouyang",
      "Zhuosheng Zhang",
      "Hai Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29851": {
    "title": "Preparing Lessons for Progressive Training on Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Pan",
      "Ye Yuan",
      "Yichun Yin",
      "Jiaxin Shi",
      "Zenglin Xu",
      "Ming Zhang",
      "Lifeng Shang",
      "Xin Jiang",
      "Qun Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29852": {
    "title": "A Novel Energy Based Model Mechanism for Multi-Modal Aspect-Based Sentiment Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianshuo Peng",
      "Zuchao Li",
      "Ping Wang",
      "Lefei Zhang",
      "Hai Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29853": {
    "title": "A Joint Framework with Heterogeneous-Relation-Aware Graph and Multi-Channel Label Enhancing Strategy for Event Causality Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruili Pu",
      "Yang Li",
      "Jun Zhao",
      "Suge Wang",
      "Deyu Li",
      "Jian Liao",
      "Jianxing Zheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29854": {
    "title": "MULTISCRIPT: Multimodal Script Learning for Supporting Open Domain Everyday Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyuan Qi",
      "Minqian Liu",
      "Ying Shen",
      "Zhiyang Xu",
      "Lifu Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29855": {
    "title": "Exploring Transformer Extrapolation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Qin",
      "Yiran Zhong",
      "Hui Deng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29856": {
    "title": "Using Artificial Populations to Study Psychological Phenomena in Neural Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jesse Roberts",
      "Kyle Moore",
      "Drew Wilenzick",
      "Douglas Fisher"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29857": {
    "title": "Better than Random: Reliable NLG Human Evaluation with Constrained Active Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Ruan",
      "Xiao Pu",
      "Mingqi Gao",
      "Xiaojun Wan",
      "Yuesheng Zhu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29858": {
    "title": "VELMA: Verbalization Embodiment of LLM Agents for Vision and Language Navigation in Street View",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raphael Schumann",
      "Wanrong Zhu",
      "Weixi Feng",
      "Tsu-Jui Fu",
      "Stefan Riezler",
      "William Yang Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29859": {
    "title": "OntoFact: Unveiling Fantastic Fact-Skeleton of LLMs via Ontology-Driven Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyu Shang",
      "Wenjun Ke",
      "Nana Xiu",
      "Peng Wang",
      "Jiajun Liu",
      "Yanhui Li",
      "Zhizhao Luo",
      "Ke Ji"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29860": {
    "title": "Agile-Quant: Activation-Guided Quantization for Faster Inference of LLMs on the Edge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan Shen",
      "Peiyan Dong",
      "Lei Lu",
      "Zhenglun Kong",
      "Zhengang Li",
      "Ming Lin",
      "Chao Wu",
      "Yanzhi Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29861": {
    "title": "CORECODE: A Common Sense Annotated Dialogue Dataset with Benchmark Tasks for Chinese Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dan Shi",
      "Chaobin You",
      "Jiantao Huang",
      "Taihao Li",
      "Deyi Xiong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29862": {
    "title": "A Unified Knowledge Transfer Network for Generalized Category Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenkai Shi",
      "Wenbin An",
      "Feng Tian",
      "Yan Chen",
      "Yaqiang Wu",
      "Qianying Wang",
      "Ping Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29863": {
    "title": "RewriteLM: An Instruction-Tuned Large Language Model for Text Rewriting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Shu",
      "Liangchen Luo",
      "Jayakumar Hoskere",
      "Yun Zhu",
      "Yinxiao Liu",
      "Simon Tong",
      "Jindong Chen",
      "Lei Meng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29864": {
    "title": "Well, Now We Know! Unveiling Sarcasm: Initiating and Exploring Multimodal Conversations with Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gopendra Vikram Singh",
      "Mauajama Firdaus",
      "Dushyant Singh Chauhan",
      "Asif Ekbal",
      "Pushpak Bhattacharyya"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29865": {
    "title": "Preference Ranking Optimization for Human Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feifan Song",
      "Bowen Yu",
      "Minghao Li",
      "Haiyang Yu",
      "Fei Huang",
      "Yongbin Li",
      "Houfeng Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29866": {
    "title": "TACIT: A Target-Agnostic Feature Disentanglement Framework for Cross-Domain Text Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Song",
      "Fausto Giunchiglia",
      "Yingji Li",
      "Mingjie Tian",
      "Hao Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29867": {
    "title": "A Dual-Way Enhanced Framework from Text Matching Point of View for Multimodal Entity Linking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shezheng Song",
      "Shan Zhao",
      "ChengYu Wang",
      "Tianwei Yan",
      "Shasha Li",
      "Xiaoguang Mao",
      "Meng Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29868": {
    "title": "RoPDA: Robust Prompt-Based Data Augmentation for Low-Resource Named Entity Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sihan Song",
      "Furao Shen",
      "Jian Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29869": {
    "title": "Wikiformer: Pre-training with Structured Information of Wikipedia for Ad-Hoc Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weihang Su",
      "Qingyao Ai",
      "Xiangsheng Li",
      "Jia Chen",
      "Yiqun Liu",
      "Xiaolong Wu",
      "Shengluan Hou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29870": {
    "title": "SIG: Speaker Identification in Literature via Prompt-Based Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenlin Su",
      "Liyan Xu",
      "Jin Xu",
      "Jiangnan Li",
      "Mingdu Huangfu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29871": {
    "title": "Collaborative Synthesis of Patient Records through Multi-Visit Health State Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongda Sun",
      "Hongzhan Lin",
      "Rui Yan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29872": {
    "title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liangtai Sun",
      "Yang Han",
      "Zihan Zhao",
      "Da Ma",
      "Zhennan Shen",
      "Baocai Chen",
      "Lu Chen",
      "Kai Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29873": {
    "title": "UMIE: Unified Multimodal Information Extraction with Instruction Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lin  Sun",
      "Kai Zhang",
      "Qingyuan Li",
      "Renze Lou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29874": {
    "title": "InstructDoc: A Dataset for Zero-Shot Generalization of Visual Document Understanding with Instructions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryota Tanaka",
      "Taichi Iki",
      "Kyosuke Nishida",
      "Kuniko Saito",
      "Jun Suzuki"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29875": {
    "title": "Graph Neural Prompting with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yijun Tian",
      "Huan Song",
      "Zichen Wang",
      "Haozhu Wang",
      "Ziqing Hu",
      "Fang Wang",
      "Nitesh V. Chawla",
      "Panpan Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29876": {
    "title": "Adaptive Graph Learning for Multimodal Conversational Emotion Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Geng Tu",
      "Tian Xie",
      "Bin Liang",
      "Hongpeng Wang",
      "Ruifeng Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29877": {
    "title": "Dependency Structure-Enhanced Graph Attention Networks for Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qizhi Wan",
      "Changxuan Wan",
      "Keli Xiao",
      "Kun Lu",
      "Chenliang Li",
      "Xiping Liu",
      "Dexi Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29878": {
    "title": "ESRL: Efficient Sampling-Based Reinforcement Learning for Sequence Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenglong Wang",
      "Hang Zhou",
      "Yimin Hu",
      "Yifu Huo",
      "Bei Li",
      "Tongran Liu",
      "Tong Xiao",
      "Jingbo Zhu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29879": {
    "title": "Exploring Equation as a Better Intermediate Meaning Representation for Numerical Reasoning of Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dingzirui Wang",
      "Longxu Dou",
      "Wenbin Zhang",
      "Junyu Zeng",
      "Wanxiang Che"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29880": {
    "title": "Manifold-Based Verbalizer Space Re-embedding for Tuning-Free Prompt-Based Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haochun Wang",
      "Sendong Zhao",
      "Chi Liu",
      "Nuwa Xi",
      "MuZhen Cai",
      "Bing Qin ",
      "Ting Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29881": {
    "title": "Improving the Robustness of Knowledge-Grounded Dialogue via Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaan Wang",
      "JIanfeng Qu",
      "Kexin Wang",
      "Zhixu Li",
      "Wen Hua",
      "Ximing Li",
      "An Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29882": {
    "title": "Restoring Speaking Lips from Occlusion for Audio-Visual Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiadong Wang",
      "Zexu Pan",
      "Malu Zhang",
      "Robby T. Tan",
      "Haizhou Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29883": {
    "title": "Learning from Failure: Improving Meeting Summarization without Good Samples",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ke Wang",
      "Xiutian Zhao",
      "Wei Peng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29884": {
    "title": "T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Large Language Model Signals for Science Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Wang",
      "Yi Hu",
      "Jiabang He",
      "Xing Xu",
      "Ning Liu",
      "Hui Liu",
      "Heng Tao Shen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29885": {
    "title": "Mitigating the Impact of False Negative in Dense Retrieval with Contrastive Confidence Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiqi Wang",
      "Yeqin Zhang",
      "Cam-Tu Nguyen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29886": {
    "title": "DenoSent: A Denoising Objective for Self-Supervised Sentence Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinghao Wang",
      "Junliang He",
      "Pengyu Wang",
      "Yunhua Zhou",
      "Tianxiang Sun",
      "Xipeng Qiu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29887": {
    "title": "LLMRG: Improving Recommendations through Large Language Model Reasoning Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Wang",
      "Zhixuan Chu",
      "Xin Ouyang",
      "Simeng Wang",
      "Hongyan Hao",
      "Yue Shen",
      "Jinjie Gu",
      "Siqiao Xue",
      "James Zhang",
      "Qing Cui",
      "Longfei Li",
      "Jun Zhou",
      "Sheng Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29888": {
    "title": "A Positive-Unlabeled Metric Learning Framework for Document-Level Relation Extraction with Incomplete Labeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ye Wang",
      "Huazheng Pan",
      "Tao Zhang",
      "Wen Wu",
      "Wenxin Hu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29889": {
    "title": "Knowledge Graph Prompting for Multi-Document Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Wang",
      "Nedim Lipka",
      "Ryan A. Rossi",
      "Alexa Siu",
      "Ruiyi Zhang",
      "Tyler Derr"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29890": {
    "title": "STAIR: Spatial-Temporal Reasoning with Auditable Intermediate Results for Video Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yueqian Wang",
      "Yuxuan Wang",
      "Kai  Chen",
      "Dongyan Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29891": {
    "title": "Video Event Extraction with Multi-View Interaction Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaiwen Wei",
      "Runyan Du",
      "Li Jin",
      "Jian Liu",
      "Jianhua Yin",
      "Linhao Zhang",
      "Jintao Liu",
      "Nayu Liu",
      "Jingyuan Zhang",
      "Zhi Guo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29892": {
    "title": "ConsistNER: Towards Instructive NER Demonstrations for LLMs with the Consistency of Ontology and Context",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenxiao Wu",
      "Wenjun Ke",
      "Peng Wang",
      "Zhizhao Luo",
      "Guozheng Li",
      "Wanyi Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29893": {
    "title": "Mitigating Idiom Inconsistency: A Multi-Semantic Contrastive Learning Method for Chinese Idiom Reading Comprehension",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingmin Wu",
      "Yuxue Hu",
      "Yongcheng Zhang",
      "Zeng Zhi",
      "Guixin Su",
      "Ying Sha"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29894": {
    "title": "Improving Open-Domain Dialogue Response Generation with Multi-Source Multilingual Commonsense Knowledge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sixing Wu",
      "Jiong Yu",
      "Jiahao Chen",
      "Xiaofan Deng",
      "Wei Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29895": {
    "title": "On the Affinity, Rationality, and Diversity of Hierarchical Topic Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaobao Wu",
      "Fengjun Pan",
      "Thong Nguyen",
      "Yichao Feng",
      "Chaoqun Liu",
      "Cong-Duy Nguyen",
      "Anh Tuan Luu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29896": {
    "title": "MindMap: Constructing Evidence Chains for Multi-Step Reasoning in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangyu Wu",
      "Xu Han",
      "Wei Song",
      "Miaomiao Cheng",
      "Fei Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29897": {
    "title": "De-biased Attention Supervision for Text Classification with Causality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiquan Wu",
      "Yifei Liu",
      "Ziyu Zhao",
      "Weiming Lu",
      "Yating Zhang",
      "Changlong Sun",
      "Fei Wu",
      "Kun Kuang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29898": {
    "title": "Get an A in Math: Progressive Rectification Prompting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenyu Wu",
      "Meng Jiang",
      "Chao Shen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29899": {
    "title": "DIUSum: Dynamic Image Utilization for Multimodal Summarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Min Xiao",
      "Junnan Zhu",
      "Feifei Zhai",
      "Yu Zhou",
      "Chengqing Zong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29900": {
    "title": "Automated Defect Report Generation for Enhanced Industrial Quality Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayuan Xie",
      "Zhiping Zhou",
      "Zihan Wu",
      "Xinting Zhang",
      "Jiexin Wang",
      "Yi Cai",
      "Qing Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29901": {
    "title": "ALISON: Fast and Effective Stylometric Authorship Obfuscation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eric Xing",
      "Saranya Venkatraman",
      "Thai Le",
      "Dongwon Lee"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29902": {
    "title": "SECap: Speech Emotion Captioning with Large Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaoxun Xu",
      "Hangting Chen",
      "Jianwei Yu",
      "Qiaochu Huang",
      "Zhiyong Wu",
      "Shi-Xiong Zhang",
      "Guangzhi Li",
      "Yi Luo",
      "Rongzhi Gu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29903": {
    "title": "Question Calibration and Multi-Hop Modeling for Temporal Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Xue",
      "Di Liang",
      "Pengfei Wang",
      "Jing Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29904": {
    "title": "Robust Few-Shot Named Entity Recognition with Boundary Discrimination and Correlation Purification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaojun Xue",
      "Chunxia Zhang",
      "Tianxiang Xu",
      "Zhendong Niu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29905": {
    "title": "Tackling Vision Language Tasks through Learning Inner Monologues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Diji Yang",
      "Kezhen Chen",
      "Jinmeng Rao",
      "Xiaoyuan Guo",
      "Yawen Zhang",
      "Jie Yang",
      "Yi Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29906": {
    "title": "YTCommentQA: Video Question Answerability in Instructional Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saelyne Yang",
      "Sunghyun Park",
      "Yunseok Jang",
      "Moontae Lee"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29907": {
    "title": "Zhongjing: Enhancing the Chinese Medical Capabilities of Large Language Model through Expert Feedback and Real-World Multi-Turn Dialogue",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Songhua Yang",
      "Hanjie Zhao",
      "Senbin Zhu",
      "Guangyu Zhou",
      "Hongfei Xu",
      "Yuxiang Jia",
      "Hongying Zan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29908": {
    "title": "Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhewei Yao",
      "Xiaoxia Wu",
      "Cheng Li",
      "Stephen Youn",
      "Yuxiong He"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29909": {
    "title": "Investigating the Effectiveness of Task-Agnostic Prefix Prompt for Instruction Following",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seonghyeon Ye",
      "Hyeonbin Hwang",
      "Sohee Yang",
      "Hyeongu Yun",
      "Yireun Kim",
      "Minjoon Seo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29910": {
    "title": "Uni-MIS: United Multiple Intent Spoken Language Understanding via Multi-View Intent-Slot Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shangjian Yin",
      "Peijie Huang",
      "Yuhong Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29911": {
    "title": "TextGT: A Double-View Graph Transformer on Text for Aspect-Based Sentiment Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuo Yin",
      "Guoqiang Zhong"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29912": {
    "title": "History Matters: Temporal Knowledge Editing in Large Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xunjian Yin",
      "Jin Jiang",
      "Liming Yang",
      "Xiaojun Wan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29913": {
    "title": "Topic-VQ-VAE: Leveraging Latent Codebooks for Flexible Topic-Guided Document Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "YoungJoon Yoo",
      "JongWon Choi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29914": {
    "title": "CK12: A Rounded K12 Knowledge Graph Based Benchmark for Chinese Holistic Cognition Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weihao You",
      "Pengcheng Wang",
      "Changlong Li",
      "Zhilong Ji",
      "Jinfeng Bai"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29915": {
    "title": "Reliable Data Generation and Selection for Low-Resource Relation Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junjie Yu",
      "Xing Wang",
      "Wenliang Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29916": {
    "title": "MELO: Enhancing Model Editing with Neuron-Indexed Dynamic LoRA",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lang Yu",
      "Qin Chen",
      "Jie Zhou",
      "Liang He"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29917": {
    "title": "SeqGPT: An Out-of-the-Box Large Language Model for Open Domain Sequence Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyu Yu",
      "Chengyue Jiang",
      "Chao Lou",
      "Shen Huang",
      "Xiaobin Wang",
      "Wei Liu",
      "Jiong Cai",
      "Yangning Li",
      "Yinghui Li",
      "Kewei Tu",
      "Hai-Tao Zheng",
      "Ningyu Zhang",
      "Pengjun Xie",
      "Fei Huang",
      "Yong Jiang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29918": {
    "title": "TaskLAMA: Probing the Complex Task Understanding of Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quan Yuan",
      "Mehran Kazemi",
      "Xin Xu",
      "Isaac Noble",
      "Vaiva Imbrasaite",
      "Deepak Ramachandran"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29919": {
    "title": "An Autoregressive Text-to-Graph Framework for Joint Entity and Relation Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Urchade Zaratiana",
      "Nadi Tomeh",
      "Pierre Holat",
      "Thierry Charnois"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29920": {
    "title": "Teaching Large Language Models to Translate with Comparison",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiali Zeng",
      "Fandong Meng",
      "Yongjing Yin",
      "Jie Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29921": {
    "title": "InterpretARA: Enhancing Hybrid Automatic Readability Assessment with Linguistic Feature Interpreter and Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinshan Zeng",
      "Xianchao Tong",
      "Xianglong Yu",
      "Wenyan Xiao",
      "Qing Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29922": {
    "title": "ConsistentEE: A Consistent and Hardness-Guided Early Exiting Method for Accelerating Language Models Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziqian Zeng",
      "Yihuai Hong",
      "Hongliang Dai",
      "Huiping Zhuang",
      "Cen Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29923": {
    "title": "A Comprehensive Analysis of the Effectiveness of Large Language Models as Automatic Dialogue Evaluators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Zhang",
      "Luis Fernando D'Haro",
      "Yiming Chen",
      "Malu Zhang",
      "Haizhou Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29924": {
    "title": "PREFER: Prompt Ensemble Learning via Feedback-Reflect-Refine",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenrui Zhang",
      "Lin Liu",
      "Chuyuan Wang",
      "Xiao Sun",
      "Hongyu Wang",
      "Jinpeng Wang",
      "Mingchen Cai"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29925": {
    "title": "Causal Walk: Debiasing Multi-Hop Fact Verification with Front-Door Adjustment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Congzhi Zhang",
      "Linhai Zhang",
      "Deyu Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29926": {
    "title": "Visual Hallucination Elevates Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fang Zhang",
      "Yongxin Zhu",
      "Xiangxiang Wang",
      "Huang Chen",
      "Xing Sun",
      "Linli Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29927": {
    "title": "Quantum Interference Model for Semantic Biases of Glosses in Word Sense Disambiguation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junwei Zhang",
      "Ruifang He",
      "Fengyu Guo",
      "Chang Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29928": {
    "title": "Tree-of-Reasoning Question Decomposition for Complex Question Answering with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kun Zhang",
      "Jiali Zeng",
      "Fandong Meng",
      "Yuanzhuo Wang",
      "Shiqi Sun",
      "Long Bai",
      "Huawei Shen",
      "Jie Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29929": {
    "title": "What to Remember: Self-Adaptive Continual Learning for Audio Deepfake Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "XiaoHui Zhang",
      "Jiangyan Yi",
      "Chenglong Wang",
      "Chu Yuan Zhang",
      "Siding Zeng",
      "Jianhua Tao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29930": {
    "title": "A Goal Interaction Graph Planning Framework for Conversational Recommendation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaotong Zhang",
      "Xuefang Jia",
      "Han Liu",
      "Xinyue Liu",
      "Xianchao Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29931": {
    "title": "Personalized LoRA for Human-Centered Text Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "You Zhang",
      "Jin Wang",
      "Liang-Chih Yu",
      "Dan Xu",
      "Xuejie Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29932": {
    "title": "StyleSinger: Style Transfer for Out-of-Domain Singing Voice Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Zhang",
      "Rongjie Huang",
      "Ruiqi Li",
      "JinZheng He",
      "Yan Xia",
      "Feiyang Chen",
      "Xinyu Duan",
      "Baoxing Huai",
      "Zhou Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29933": {
    "title": "Seed-Guided Fine-Grained Entity Typing in Science and Engineering Domains",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Zhang",
      "Yunyi Zhang",
      "Yanzhen Shen",
      "Yu Deng",
      "Lucian Popa",
      "Larisa  Shwartz",
      "ChengXiang  Zhai",
      "Jiawei Han"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29934": {
    "title": "LLMEval: A Preliminary Study on How to Evaluate Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Zhang",
      "Ming Zhang",
      "Haipeng Yuan",
      "Shichun Liu",
      "Yongyao Shi",
      "Tao Gui",
      "Qi Zhang",
      "Xuanjing Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29935": {
    "title": "Coreference Graph Guidance for Mind-Map Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuowei Zhang",
      "Mengting Hu",
      "Yinhao Bai",
      "Zhen Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29936": {
    "title": "ExpeL: LLM Agents Are Experiential Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrew Zhao",
      "Daniel Huang",
      "Quentin Xu",
      "Matthieu Lin",
      "Yong-Jin Liu",
      "Gao Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29937": {
    "title": "Conditional Variational Autoencoder for Sign Language Translation with Cross-Modal Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Zhao",
      "Liang Zhang",
      "Biao Fu",
      "Cong Hu",
      "Jinsong Su",
      "Yidong Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29938": {
    "title": "Graph Reasoning Transformers for Knowledge-Aware Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruilin Zhao",
      "Feng Zhao",
      "Liang Hu",
      "Guandong Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29939": {
    "title": "MultiSum: A Multi-Facet Approach for Extractive Social Summarization Utilizing Semantic and Sociological Relationships",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tanglong Zhao",
      "Ruifang He",
      "Jing Xu",
      "Bo Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29940": {
    "title": "QPEN: Quantum Projection and Quantum Entanglement Enhanced Network for Cross-Lingual Aspect-Based Sentiment Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingqiang Zhao",
      "Hai Wan",
      "Kunxun Qi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29941": {
    "title": "SENCR: A Span Enhanced Two-Stage Network with Counterfactual Rethinking for Chinese NER",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hang Zheng",
      "Qingsong Li",
      "Shen Chen",
      "Yuxuan Liang",
      "Li Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29942": {
    "title": "Reverse Multi-Choice Dialogue Commonsense Inference with Graph-of-Thought",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li Zheng",
      "Hao Fei",
      "Fei Li",
      "Bobo Li",
      "Lizi Liao",
      "Donghong Ji",
      "Chong Teng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29943": {
    "title": "FT-GAN: Fine-Grained Tune Modeling for Chinese Opera Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meizhen Zheng",
      "Peng Bai",
      "Xiaodong Shi",
      "Xun Zhou",
      "Yiting Yan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29944": {
    "title": "Layer-Wise Representation Fusion for Compositional Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yafang Zheng",
      "Lei Lin",
      "Shuangtao Li",
      "Yuxuan Yuan",
      "Zhaohong Lai",
      "Shan Liu",
      "Biao Fu",
      "Yidong Chen",
      "Xiaodong Shi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29945": {
    "title": "You Only Read Once: Constituency-Oriented Relational Graph Convolutional Network for Multi-Aspect Multi-Sentiment Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongqiang Zheng",
      "Xia Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29946": {
    "title": "MemoryBank: Enhancing Large Language Models with Long-Term Memory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wanjun Zhong",
      "Lianghong Guo",
      "Qiqi Gao",
      "He Ye",
      "Yanlin Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29947": {
    "title": "Fine-Grained Distillation for Long Document Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yucheng Zhou",
      "Tao Shen",
      "Xiubo Geng",
      "Chongyang Tao",
      "Jianbing Shen",
      "Guodong Long",
      "Can Xu",
      "Daxin Jiang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29948": {
    "title": "Quantifying and Analyzing Entity-Level Memorization in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenhong Zhou",
      "Jiuyang Xiang",
      "Chaomeng Chen",
      "Sen Su"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29949": {
    "title": "MathAttack: Attacking Large Language Models towards Math Solving Ability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihao Zhou",
      "Qiufeng Wang",
      "Mingyu Jin",
      "Jie Yao",
      "Jianan Ye",
      "Wei Liu",
      "Wei Wang",
      "Xiaowei Huang",
      "Kaizhu Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29950": {
    "title": "LimeAttack: Local Explainable Method for Textual Hard-Label Adversarial Attack",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hai Zhu",
      "Qingyang Zhao",
      "Weiwei Shang",
      "Yuren Wu",
      "Kai Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29951": {
    "title": "Multichannel AV-wav2vec2: A Framework for Learning Multichannel Multi-Modal Speech Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiushi Zhu",
      "Jie Zhang",
      "Yu Gu",
      "Yuchen Hu",
      "Lirong Dai"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29952": {
    "title": "Aligner²: Enhancing Joint Multiple Intent Detection and Slot Filling via Adjustive and Forced Cross-Task Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihong Zhu",
      "Xuxin Cheng",
      "Yaowei Li",
      "Hongxiang Li",
      "Yuexian Zou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29953": {
    "title": "Towards Explainable Joint Models via Information Theory for Multiple Intent Detection and Slot Filling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xianwei Zhuang",
      "Xuxin Cheng",
      "Yuexian Zou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29954": {
    "title": "Video-Context Aligned Transformer for Video Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linlin Zong",
      "Jiahui Wan",
      "Xianchao Zhang",
      "Xinyue Liu",
      "Wenxin Liang",
      "Bo Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29955": {
    "title": "Quality-Diversity Generative Sampling for Learning with Synthetic Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Allen Chang",
      "Matthew C. Fontaine",
      "Serena Booth",
      "Maja J. Matarić",
      "Stefanos Nikolaidis"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29956": {
    "title": "A Cross-View Hierarchical Graph Learning Hypernetwork for Skill Demand-Supply Joint Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenshuo Chao",
      "Zhaopeng Qiu",
      "Likang Wu",
      "Zhuoning Guo",
      "Zhi Zheng",
      "Hengshu Zhu",
      "Hao Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29957": {
    "title": "Conditional Backdoor Attack via JPEG Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiuyu Duan",
      "Zhongyun Hua",
      "Qing Liao",
      "Yushu Zhang",
      "Leo Yu Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29958": {
    "title": "Complementary Knowledge Distillation for Robust and Privacy-Preserving Model Serving in Vertical Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dashan Gao",
      "Sheng Wan",
      "Lixin Fan",
      "Xin Yao",
      "Qiang Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29959": {
    "title": "Resource Democratization: Is Compute the Binding Constraint on AI Research?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rebecca Gelles",
      "Veronica Kinoshita",
      "Micah Musser",
      "James Dunham"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29960": {
    "title": "How to Overcome Curse-of-Dimensionality for Out-of-Distribution Detection?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soumya Suvra Ghosal",
      "Yiyou Sun",
      "Yixuan Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29961": {
    "title": "Exploiting Discrepancy in Feature Statistic for Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyuan Guan",
      "Jiankang Chen",
      "Shenshen Bu",
      "Yuren Zhou",
      "Wei-Shi Zheng",
      "Ruixuan Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29962": {
    "title": "Reward Penalties on Augmented States for Solving Richly Constrained RL Effectively",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Jiang",
      "Tien Mai",
      "Pradeep Varakantham",
      "Huy Hoang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29963": {
    "title": "The Logic of Doxastic Strategies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junli Jiang",
      "Pavel Naumov"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29964": {
    "title": "MERGE: Fast Private Text Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zi Liang",
      "Pinghui Wang",
      "Ruofei Zhang",
      "Nuo Xu",
      "Shuo Zhang",
      "Lifeng Xing",
      "Haitao Bai",
      "Ziyang Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29965": {
    "title": "Does Few-Shot Learning Suffer from Backdoor Attacks?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinwei Liu",
      "Xiaojun Jia",
      "Jindong Gu",
      "Yuan Xun",
      "Siyuan Liang",
      "Xiaochun Cao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29966": {
    "title": "Towards Model Extraction Attacks in GAN-Based Image Translation via Domain Shift Mitigation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Di Mi",
      "Yanjun Zhang",
      "Leo Yu Zhang",
      "Shengshan Hu",
      "Qi Zhong",
      "Haizhuan Yuan",
      "Shirui Pan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29967": {
    "title": "Towards the Robustness of Differentially Private Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Qi",
      "Huili Wang",
      "Yongfeng Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29968": {
    "title": "Responsibility in Extensive Form Games",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi SHI"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29969": {
    "title": "Towards Fairness in Online Service with K Servers and Its Application on Fair Food Delivery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daman Deep Singh",
      "Amit Kumar",
      "Abhijnan Chakraborty"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29970": {
    "title": "Value Kaleidoscope: Engaging AI with Pluralistic Human Values, Rights, and Duties",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taylor Sorensen",
      "Liwei Jiang",
      "Jena D. Hwang",
      "Sydney Levine",
      "Valentina Pyatkin",
      "Peter West",
      "Nouha Dziri",
      "Ximing Lu",
      "Kavel Rao",
      "Chandra Bhagavatula",
      "Maarten Sap",
      "John Tasioulas",
      "Yejin Choi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29971": {
    "title": "Moral Uncertainty and the Problem of Fanaticism",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jazon Szabo",
      "Natalia Criado",
      "Jose Such",
      "Sanjay Modgil"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29972": {
    "title": "U-trustworthy Models. Reliability, Competence, and Confidence in Decision-Making",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ritwik Vashistha",
      "Arya Farahi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29973": {
    "title": "TraceEvader: Making DeepFakes More Untraceable via Evading the Forgery Model Attribution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengjie Wu",
      "Jingui Ma",
      "Run Wang",
      "Sidan Zhang",
      "Ziyou Liang",
      "Boheng Li",
      "Chenhao Lin",
      "Liming Fang",
      "Lina Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29974": {
    "title": "SAME: Sample Reconstruction against Model Extraction Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Xie",
      "Jie Zhang",
      "Shiqian Zhao",
      "Tianwei Zhang",
      "Xiaofeng Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29975": {
    "title": "High-Fidelity Gradient Inversion in Distributed Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zipeng Ye",
      "Wenjian Luo",
      "Qi Zhou",
      "Yubo Tang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29976": {
    "title": "Robustness Verification of Deep Reinforcement Learning Based Control Systems Using Reward Martingales",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dapeng Zhi",
      "Peixin Wang",
      "Cheng Chen",
      "Min Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29977": {
    "title": "Regulating AI: Applying Insights from Behavioural Economics and Psychology to the Application of Article 5 of the EU AI Act",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huixin Zhong",
      "Eamonn O'Neill",
      "Janina A. Hoffmann"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29978": {
    "title": "Batch Normalization Is Blind to the First and Second Derivatives of the Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhanpeng Zhou",
      "Wen Shen",
      "Huixin Chen",
      "Ling Tang",
      "Yuefeng Chen",
      "Quanshi Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29979": {
    "title": "Block-Level Goal Recognition Design",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tsz-Chiu Au"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29980": {
    "title": "Learning Planning Domains from Non-redundant Fully-Observed Traces: Theoretical Foundations and Complexity Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pascal Bachor",
      "Gregor Behnke"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29981": {
    "title": "Dealing with Numeric and Metric Time Constraints in PDDL3 via Compilation to Numeric Planning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luigi Bonassi",
      "Alfonso Emilio Gerevini",
      "Enrico Scala"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29982": {
    "title": "The Complexity of Optimizing Atomic Congestion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cornelius Brand",
      "Robert Ganian",
      "Subrahmanyam Kalyanasundaram",
      "Fionn Mc Inerney"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29983": {
    "title": "Stop! Planner Time: Metareasoning for Probabilistic Planning Using Learned Performance Profiles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew Budd",
      "Bruno Lacerda",
      "Nick Hawes"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29984": {
    "title": "Can LLMs Fix Issues with Reasoning Models? Towards More Likely Models for AI Planning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Turgay Caglar",
      "Sirine Belhaj",
      "Tathagata Chakraborty",
      "Michael Katz",
      "Sarath Sreedharan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29985": {
    "title": "Symbolic Numeric Planning with Patterns",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matteo Cardellini",
      "Enrico Giunchiglia",
      "Marco Maratea"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29986": {
    "title": "Learning Domain-Independent Heuristics for Grounded and Lifted Planning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dillon Z. Chen",
      "Sylvie Thiébaux",
      "Felipe Trevizan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29987": {
    "title": "Approximate Distance Oracle for Fault-Tolerant Geometric Spanners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyungjin Cho",
      "Jihun Shin",
      "Eunjin Oh"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29988": {
    "title": "Optimizing the Optimization of Planning Domains by Automatic Action Schema Splitting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mojtaba Elahi",
      "Jussi Rintanen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29989": {
    "title": "An Effective Polynomial Technique for Compiling Conditional Effects Away",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alfonso Emilio Gerevini",
      "Francesco Percassi",
      "Enrico Scala"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29990": {
    "title": "GOALNET: Interleaving Neural Goal Predicate Inference with Classical Planning for Generalization in Robot Instruction Following",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jigyasa Gupta",
      "Shreya Sharma",
      "Shreshth Tuli",
      "Rohan Paul",
      "Mausam"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29991": {
    "title": "SayCanPay: Heuristic Planning with Large Language Models Using Learnable Domain Knowledge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rishi Hazra",
      "Pedro Zuidberg Dos Martires",
      "Luc De Raedt"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29992": {
    "title": "A Surprisingly Simple Continuous-Action POMDP Solver: Lazy Cross-Entropy Search Over Policy Trees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marcus Hoerger",
      "Hanna Kurniawati",
      "Dirk Kroese",
      "Nan Ye"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29993": {
    "title": "Optimizing Local Satisfaction of Long-Run Average Objectives in Markov Decision Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Klaška",
      "Antonín Kučera",
      "Vojtěch Kůr",
      "Vít Musil",
      "Vojtěch Řehák"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29994": {
    "title": "Monte Carlo Tree Search in the Presence of Transition Uncertainty",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Farnaz Kohankhaki",
      "Kiarash Aghakasiri",
      "Hongming Zhang",
      "Ting-Han Wei",
      "Chao Gao",
      "Martin Müller"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29995": {
    "title": "Learning Safe Action Models with Partial Observability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hai S. Le",
      "Brendan Juba",
      "Roni Stern"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29996": {
    "title": "Generalized Planning for the Abstraction and Reasoning Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Lei",
      "Nir Lipovetzky",
      "Krista A. Ehinger"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29997": {
    "title": "Simplifying Complex Observation Models in Continuous POMDP Planning with Probabilistic Guarantees and Practice",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Idan Lev-Yehudi",
      "Moran Barenboim",
      "Vadim Indelman"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29998": {
    "title": "Learning to Optimize Permutation Flow Shop Scheduling via Graph-Based Imitation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Longkang Li",
      "Siyuan Liang",
      "Zihao Zhu",
      "Chris Ding",
      "Hongyuan Zha",
      "Baoyuan Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/29999": {
    "title": "NaRuto: Automatically Acquiring Planning Models from Narrative Texts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruiqi Li",
      "Leyang Cui",
      "Songtuan Lin",
      "Patrik Haslum"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30000": {
    "title": "On the Computational Complexity of Plan Verification, (Bounded) Plan-Optimality Verification, and Bounded Plan Existence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Songtuan Lin",
      "Conny Olz",
      "Malte Helmert",
      "Pascal Bercher"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30001": {
    "title": "PRP Rebooted: Advancing the State of the Art in FOND Planning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christian Muise",
      "Sheila A. McIlraith",
      "J. Christopher Beck"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30002": {
    "title": "Abstract Action Scheduling for Optimal Temporal Planning via OMT",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefan Panjkovic",
      "Andrea Micheli"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30003": {
    "title": "Generalising Planning Environment Redesign",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alberto Pozanco",
      "Ramon Fraga Pereira",
      "Daniel Borrajo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30004": {
    "title": "When CEGAR Meets Regression: A Love Story in Optimal Classical Planning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Martín Pozo",
      "Alvaro Torralba",
      "Carlos Linares Lopez"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30005": {
    "title": "Efficient Constraint Generation for Stochastic Shortest Path Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Johannes Schmalz",
      "Felipe Trevizan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30006": {
    "title": "Generalized Planning in PDDL Domains with Pretrained Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tom Silver",
      "Soham Dan",
      "Kavitha Srinivas",
      "Joshua B. Tenenbaum",
      "Leslie Kaelbling",
      "Michael Katz"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30007": {
    "title": "Equity-Transformer: Solving NP-Hard Min-Max Routing Problems as Sequential Generation with Equity Context",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiwoo Son",
      "Minsu Kim",
      "Sanghyeok Choi",
      "Hyeonah Kim",
      "Jinkyoo Park"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30008": {
    "title": "Distilling Autoregressive Models to Obtain High-Performance Non-autoregressive Solvers for Vehicle Routing Problems with Faster Inference Speed",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yubin Xiao",
      "Di Wang",
      "Boyang Li",
      "Mingzhao Wang",
      "Xuan Wu",
      "Changliang Zhou",
      "You Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30009": {
    "title": "GLOP: Learning Global Partition and Local Construction for Solving Large-Scale Routing Problems in Real-Time",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoran Ye",
      "Jiarui Wang",
      "Helan Liang",
      "Zhiguang Cao",
      "Yong Li",
      "Fanzhang Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30010": {
    "title": "Learning-Augmented Online Algorithm for Two-Level Ski-Rental Problem",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keyuan Zhang",
      "Zhongdong Liu",
      "Nakjung Choi",
      "Bo Ji"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30011": {
    "title": "s-ID: Causal Effect Identification in a Sub-population",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amir Mohammad Abouei",
      "Ehsan Mokhtarian",
      "Negar Kiyavash"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30012": {
    "title": "On Estimating the Gradient of the Expected Information Gain in Bayesian Experimental Design",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziqiao Ao",
      "Jinglai  Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30013": {
    "title": "Backward Responsibility in Transition Systems Using General Power Indices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christel Baier",
      "Roxane van den Bossche",
      "Sascha Klüppelholz",
      "Johannes Lehmann",
      "Jakob Piribauer"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30014": {
    "title": "The Expected Loss of Preconditioned Langevin Dynamics Reveals the Hessian Rank",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amitay Bar",
      "Rotem Mulayoff",
      "Tomer Michaeli",
      "Ronen Talmon"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30015": {
    "title": "Pandora's Problem with Deadlines",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ben Berger",
      "Tomer Ezra",
      "Michal Feldman",
      "Federico Fusco"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30016": {
    "title": "Minibatch Stochastic Three Points Method for Unconstrained Smooth Minimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soumia Boucherouite",
      "Grigory Malinovsky",
      "Peter Richtárik",
      "El Houcine Bergou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30017": {
    "title": "Identification of Causal Structure with Latent Variables Based on Higher Order Cumulants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Chen",
      "Zhiyi Huang",
      "Ruichu Cai",
      "Zhifeng Hao",
      "Kun Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30018": {
    "title": "Direct Amortized Likelihood Ratio Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adam D. Cobb",
      "Brian Matejek",
      "Daniel Elenius",
      "Anirban Roy",
      "Susmit Jha"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30019": {
    "title": "Probabilistic Offline Policy Ranking with Approximate Bayesian Computation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Longchao Da",
      "Porter Jenkins",
      "Trevor Schwantes",
      "Jeffrey Dotson",
      "Hua Wei"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30020": {
    "title": "Generalized Bradley-Terry Models for Score Estimation from Paired Comparisons",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julien Fageot",
      "Sadegh Farhadkhani",
      "Lê-Nguyên Hoang",
      "Oscar Villemaud"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30021": {
    "title": "Identifiability of Direct Effects from Summary Causal Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simon Ferreira",
      "Charles K. Assaad"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30022": {
    "title": "Model Counting and Sampling via Semiring Extensions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andreas Goral",
      "Joachim Giesen",
      "Mark Blacher",
      "Christoph Staudt",
      "Julien Klaus"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30023": {
    "title": "Identification for Tree-Shaped Structural Causal Models in Polynomial Time",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aaryan Gupta",
      "Markus Bläser"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30024": {
    "title": "Learning GAI-Decomposable Utility Models for Multiattribute Decision Making",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Margot Herin",
      "Patrice Perny",
      "Nataliya Sokolovska"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30025": {
    "title": "Uncertainty Quantification in Heterogeneous Treatment Effect Estimation with Gaussian-Process-Based Partially Linear Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shunsuke Horii",
      "Yoichi Chikahara"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30026": {
    "title": "Learning Diffusions under Uncertainty",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Huang",
      "Qian Yan",
      "Keqi Han",
      "Ting Gan",
      "Jiawei Jiang",
      "Quanqing Xu",
      "Chuanhui Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30027": {
    "title": "Robustly Improving Bandit Algorithms with Confounded and Selection Biased Offline Data: A Causal Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wen Huang",
      "Xintao Wu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30028": {
    "title": "Effectiveness of Constant Stepsize in Markovian LSA and Statistical Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongyan (Lucy) Huo",
      "Yudong Chen",
      "Qiaomin Xie"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30029": {
    "title": "Piecewise Linear Transformation – Propagating Aleatoric Uncertainty in Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Krapf",
      "Michael Hagn",
      "Paul Miethaner",
      "Alexander Schiller",
      "Lucas Luttner",
      "Bernd Heinrich"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30030": {
    "title": "Probabilities of Causation with Nonbinary Treatment and Effect",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ang Li",
      "Judea Pearl"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30031": {
    "title": "Unit Selection with Nonbinary Treatment and Effect",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ang Li",
      "Judea Pearl"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30032": {
    "title": "Solving Satisfiability Modulo Counting for Symbolic and Statistical AI Integration with Provable Guarantees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinzhao Li",
      "Nan Jiang",
      "Yexiang Xue"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30033": {
    "title": "TNPAR: Topological Neural Poisson Auto-Regressive Model for Learning Granger Causal Structure from Event Sequences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuequn Liu",
      "Ruichu Cai",
      "Wei Chen",
      "Jie Qiao",
      "Yuguang Yan",
      "Zijian Li",
      "Keli Zhang",
      "Zhifeng Hao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30034": {
    "title": "Colour Passing Revisited: Lifted Model Construction with Commutative Factors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Malte Luttermann",
      "Tanya Braun",
      "Ralf Möller",
      "Marcel Gehrke"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30035": {
    "title": "Root Cause Explanation of Outliers under Noisy Mechanisms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Phuoc Nguyen",
      "Truyen Tran",
      "Sunil Gupta",
      "Thin Nguyen",
      "Svetha Venkatesh"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30036": {
    "title": "Identification of Causal Structure in the Presence of Missing Data with Additive Noise Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Qiao",
      "Zhengming Chen",
      "Jianhua Yu",
      "Ruichu Cai",
      "Zhifeng Hao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30037": {
    "title": "Causal Discovery from Poisson Branching Structural Causal Model Using High-Order Cumulant with Path Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Qiao",
      "Yu Xiang",
      "Zhengming Chen",
      "Ruichu Cai",
      "Zhifeng Hao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30038": {
    "title": "A Fixed-Parameter Tractable Algorithm for Counting Markov Equivalence Classes with the Same Skeleton",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vidya Sagar Sharma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30039": {
    "title": "Learning Bayesian Network Classifiers to Minimize the Class Variable Parameters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shouta Sugahara",
      "Koya Kato",
      "Maomi Ueno"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30040": {
    "title": "Bayesian Inference with Complex Knowledge Graph Evidence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Armin Toroghi",
      "Scott Sanner"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30041": {
    "title": "Exact, Fast and Expressive Poisson Point Processes via Squared Neural Families",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Russell Tsuchida",
      "Cheng Soon Ong",
      "Dino Sejdinovic"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30042": {
    "title": "Inference and Learning in Dynamic Decision Networks Using Knowledge Compilation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gabriele Venturato",
      "Vincent Derkinderen",
      "Pedro Zuidberg Dos Martires",
      "Luc De Raedt"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30043": {
    "title": "Linear-Time Algorithms for Front-Door Adjustment in Causal Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marcel Wienöbst",
      "Benito van der Zander",
      "Maciej Liśkiewicz"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30044": {
    "title": "Neural Causal Abstractions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kevin Xia",
      "Elias Bareinboim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30045": {
    "title": "Federated Contextual Cascading Bandits with Asynchronous Communication and Heterogeneous Users",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hantao Yang",
      "Xutong Liu",
      "Zhiyong Wang",
      "Hong Xie",
      "John C. S. Lui",
      "Defu Lian",
      "Enhong Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30046": {
    "title": "Causal-Driven Skill Prerequisite Structure Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shenbao Yu",
      "Yifeng Zeng",
      "Fan Yang",
      "Yinghui Pan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30047": {
    "title": "Deep Copula-Based Survival Analysis for Dependent Censoring with Identifiability Guarantees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weijia Zhang",
      "Chun Kai Ling",
      "Xuanhui Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30048": {
    "title": "DOGE-Train: Discrete Optimization on GPU with End-to-End Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ahmed Abbas",
      "Paul Swoboda"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30049": {
    "title": "Delegation-Relegation for Boolean Matrix Factorization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Florent Avellaneda",
      "Roger Villemaire"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30050": {
    "title": "An Interpretable Approach to the Solutions of High-Dimensional Partial Differential Equations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lulu Cao",
      "Yufei Liu",
      "Zhenzhong Wang",
      "Dejun Xu",
      "Kai Ye",
      "Kay Chen Tan",
      "Min Jiang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30051": {
    "title": "Sampling for Beyond-Worst-Case Online Ranking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingyun Chen",
      "Sungjin Im",
      "Benjamin Moseley",
      "Chenyang Xu",
      "Ruilong Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30052": {
    "title": "Learning Ultrametric Trees for Optimal Transport Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samantha Chen",
      "Puoya Tabaghi",
      "Yusu Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30053": {
    "title": "Parameterized Approximation Algorithms for Sum of Radii Clustering and Variants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xianrun Chen",
      "Dachuan Xu",
      "Yicheng Xu",
      "Yong Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30054": {
    "title": "Traffic Flow Optimisation for Lifelong Multi-Agent Path Finding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhe Chen",
      "Daniel Harabor",
      "Jiaoyang Li",
      "Peter J. Stuckey"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30055": {
    "title": "Runtime Analysis of the (μ + 1) GA: Provable Speed-Ups from Strong Drift towards Diverse Populations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Doerr",
      "Aymen Echarghaoui",
      "Mohammed Jamal",
      "Martin S. Krejca"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30056": {
    "title": "Novelty vs. Potential Heuristics: A Comparison of Hardness Measures for Satisficing Planning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simon Dold",
      "Malte Helmert"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30057": {
    "title": "Cumulative Regret Analysis of the Piyavskii–Shubert Algorithm and Its Variants for Global Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaan Gokcesu",
      "Hakan Gökcesu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30058": {
    "title": "Efficient Constrained K-center Clustering with Background Knowledge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Longkun Guo",
      "Chaoqi Jia",
      "Kewen Liao",
      "Zhigang Lu",
      "Minhui Xue"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30059": {
    "title": "Limited Query Graph Connectivity Test",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingyu Guo",
      "Jialiang Li",
      "Aneta Neumann",
      "Frank Neumann",
      "Hung Nguyen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30060": {
    "title": "Theoretical Aspects of Generating Instances with Unique Solutions: Pre-assignment Models for Unique Vertex Cover",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Takashi Horiyama",
      "Yasuaki Kobayashi",
      "Hirotaka Ono",
      "Kazuhisa Seto",
      "Ryu Suzuki"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30061": {
    "title": "KD-Club: An Efficient Exact Algorithm with New Coloring-Based Upper Bound for the Maximum k-Defective Clique Problem",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingming Jin",
      "Jiongzhi Zheng",
      "Kun He"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30062": {
    "title": "Parallel Beam Search Algorithms for Domain-Independent Dynamic Programming",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryo Kuroiwa",
      "J. Christopher Beck"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30063": {
    "title": "Rectangle Search: An Anytime Beam Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sofia Lemons",
      "Wheeler Ruml",
      "Rob Holte",
      "Carlos Linares Lopez"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30064": {
    "title": "Learning to Stop Cut Generation for Efficient Mixed-Integer Linear Programming",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haotian Ling",
      "Zhihai Wang",
      "Jie Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30065": {
    "title": "A Fast Exact Solver with Theoretical Analysis for the Maximum Edge-Weighted Clique Problem",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lu Liu",
      "Mingyu Xiao",
      "Yi Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30066": {
    "title": "Towards Running Time Analysis of Interactive Multi-Objective Evolutionary Algorithms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianhao Lu",
      "Chao Bian",
      "Chao Qian"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30067": {
    "title": "Accelerating Cutting-Plane Algorithms via Reinforcement Learning Surrogates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyle Mana",
      "Fernando Acero",
      "Stephen Mak",
      "Parisa Zehtabi",
      "Michael Cashmore",
      "Daniele Magazzeni",
      "Manuela Veloso"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30068": {
    "title": "Paths, Proofs, and Perfection: Developing a Human-Interpretable Proof System for Constrained Shortest Paths",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Konstantin Sidorov",
      "Gonçalo Homem de Almeida Correia",
      "Mathijs de Weerdt",
      "Emir Demirović"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30069": {
    "title": "Learning Encodings for Constructive Neural Combinatorial Optimization Needs to Regret",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Sun",
      "Zhi Zheng",
      "Zhenkun Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30070": {
    "title": "COMBHelper: A Neural Approach to Reduce Search Space for Graph Combinatorial Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Tian",
      "Sourav Medya",
      "Wei Ye"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30071": {
    "title": "Improving Neural Network Generalization on Data-Limited Regression with Doubly-Robust Boosting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30072": {
    "title": "Inertial Algorithm with Dry Fraction and Convolutional Sparse Coding for 3D Localization with Light Field Microscopy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaofan Wang",
      "Zhiyuan Deng",
      "Changle Wang",
      "Jinjia Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30073": {
    "title": "A Novel Skip Orthogonal List for Dynamic Optimal Transport Problem",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyang Xu",
      "Hu Ding"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30074": {
    "title": "Sample-and-Bound for Non-convex Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaoguang Zhai",
      "Zhizhen Qin",
      "Sicun Gao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30075": {
    "title": "Threshold-Based Responsive Simulated Annealing for Directed Feedback Vertex Set Problem",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingyun Zhang",
      "Yuming Du",
      "Zhouxing Su",
      "Chu-Min Li",
      "Junzhou Xu",
      "Zhihuai Chen",
      "Zhipeng Lü"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30076": {
    "title": "Jointly Improving the Sample and Communication Complexities in Decentralized Stochastic Minimax Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan Zhang",
      "Gabriel Mancino-Ball",
      "Necdet Serhat Aybat",
      "Yangyang Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30077": {
    "title": "Runtime Analysis of the SMS-EMOA for Many-Objective Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weijie Zheng",
      "Benjamin Doerr"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30078": {
    "title": "How to Use the Metropolis Algorithm for Multi-Objective Optimization?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weijie Zheng",
      "Mingfeng Li",
      "Renzhong Deng",
      "Benjamin Doerr"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30079": {
    "title": "Two-Stage Evolutionary Reinforcement Learning for Enhancing Exploration and Exploitation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingling Zhu",
      "Xiaoqiang Wu",
      "Qiuzhen Lin",
      "Wei-Neng Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30080": {
    "title": "ImageCaptioner2: Image Captioner for Image Captioning Bias Amplification Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eslam Abdelrahman",
      "Pengzhan Sun",
      "Li Erran Li",
      "Mohamed Elhoseiny"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30081": {
    "title": "A Framework for Data-Driven Explainability in Mathematical Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kevin-Martin Aigner",
      "Marc Goerigk",
      "Michael Hartisch",
      "Frauke Liers",
      "Arthur Miehlich"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30082": {
    "title": "On the Importance of Application-Grounded Experimental Design for Evaluating Explainable ML Methods",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kasun Amarasinghe",
      "Kit T. Rodolfa",
      "Sérgio Jesus",
      "Valerie Chen",
      "Vladimir Balayan",
      "Pedro Saleiro",
      "Pedro Bizarro",
      "Ameet Talwalkar",
      "Rayid Ghani"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30083": {
    "title": "Risk-Aware Continuous Control with Neural Contextual Bandits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jose A. Ayala-Romero",
      "Andres Garcia-Saavedra",
      "Xavier Costa-Perez"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30084": {
    "title": "Robust Uncertainty Quantification Using Conformalised Monte Carlo Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Bethell",
      "Simos Gerasimou",
      "Radu Calinescu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30085": {
    "title": "CCTR: Calibrating Trajectory Prediction for Uncertainty-Aware Motion Planning in Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengtai Cao",
      "Xinhong Chen",
      "Jianping Wang",
      "Qun Song",
      "Rui Tan",
      "Yung-Hui Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30086": {
    "title": "Rethinking the Development of Large Language Models from the Causal Perspective: A Legal Text Prediction Case Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haotian Chen",
      "Lingwei Zhang",
      "Yiran Liu",
      "Yang Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30087": {
    "title": "Truth Forest: Toward Multi-Scale Truthfulness in Large Language Models through Intervention without Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongzhi Chen",
      "Xingwu Sun",
      "Xianfeng Jiao",
      "Fengzong Lian",
      "Zhanhui Kang",
      "Di Wang",
      "Chengzhong Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30088": {
    "title": "Constrained Meta-Reinforcement Learning for Adaptable Safety Guarantee with Differentiable Convex Programming",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minjae Cho",
      "Chuangchuang Sun"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30089": {
    "title": "Conformal Prediction Regions for Time Series Using Linear Complementarity Programming",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew Cleaveland",
      "Insup Lee",
      "George J. Pappas",
      "Lars Lindemann"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30090": {
    "title": "TTTS: Tree Test Time Simulation for Enhancing Decision Tree Robustness against Adversarial Examples",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seffi Cohen",
      "Ofir Arbili",
      "Yisroel Mirsky",
      "Lior Rokach"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30091": {
    "title": "Find the Lady: Permutation and Re-synchronization of Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Carl De Sousa Trias",
      "Mihai Petru Mitrea",
      "Attilio Fiandrotti",
      "Marco Cagnazzo",
      "Sumanta Chaudhuri",
      "Enzo Tartaglione"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30092": {
    "title": "Stability Analysis of Switched Linear Systems with Neural Lyapunov Functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Virginie Debauche",
      "Alec Edwards",
      "Raphaël M. Jungers",
      "Alessandro Abate"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30093": {
    "title": "Robustness Verification of Multi-Class Tree Ensembles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Laurens Devos",
      "Lorenzo Cascioli",
      "Jesse Davis"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30094": {
    "title": "P2BPO: Permeable Penalty Barrier-Based Policy Optimization for Safe RL",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sumanta Dey",
      "Pallab Dasgupta",
      "Soumyajit Dey"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30095": {
    "title": "Trade-Offs in Fine-Tuned Diffusion Models between Accuracy and Interpretability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mischa Dombrowski",
      "Hadrien Reynaud",
      "Johanna P. Müller",
      "Matthew Baugh",
      "Bernhard Kainz"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30096": {
    "title": "From Hope to Safety: Unlearning Biases of Deep Models via Gradient Penalization in Latent Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maximilian Dreyer",
      "Frederik Pahde",
      "Christopher J. Anders",
      "Wojciech Samek",
      "Sebastian Lapuschkin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30097": {
    "title": "Automatically Testing Functional Properties of Code Translation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hasan Ferit Eniser",
      "Valentin Wüstholz",
      "Maria Christakis"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30098": {
    "title": "A Simple and Yet Fairly Effective Defense for Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sofiane Ennadir",
      "Yassine Abbahaddou",
      "Johannes F. Lutzeyer",
      "Michalis Vazirgiannis",
      "Henrik Boström"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30099": {
    "title": "Invisible Backdoor Attack against 3D Point Cloud Classifier in Graph Spectral Domain",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linkun Fan",
      "Fazhi He",
      "Tongzhen Si",
      "Wei Tang",
      "Bing Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30100": {
    "title": "CASE: Exploiting Intra-class Compactness and Inter-class Separability of Feature Embeddings for Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuai Feng",
      "Pengsheng Jin",
      "Chongjun Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30101": {
    "title": "Solving Non-rectangular Reward-Robust MDPs via Frequency Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Uri Gadot",
      "Esther Derman",
      "Navdeep Kumar",
      "Maxence Mohamed Elfatihi",
      "Kfir Levy",
      "Shie Mannor"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30102": {
    "title": "Balance Reward and Safety Optimization for Safe Reinforcement Learning: A Perspective of Gradient Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shangding Gu",
      "Bilgehan Sel",
      "Yuhao Ding",
      "Lu Wang",
      "Qingwei Lin",
      "Ming Jin",
      "Alois Knoll"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30103": {
    "title": "π-Light: Programmatic Interpretable Reinforcement Learning for Resource-Limited Traffic Signal Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yin Gu",
      "Kai Zhang",
      "Qi Liu",
      "Weibo Gao",
      "Longfei Li",
      "Jun Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30104": {
    "title": "Generative Model for Decision Trees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Riccardo Guidotti",
      "Anna Monreale",
      "Mattia Setzu",
      "Giulia Volpi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30105": {
    "title": "Omega-Regular Decision Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ernst Moritz Hahn",
      "Mateo Perez",
      "Sven Schewe",
      "Fabio Somenzi",
      "Ashutosh Trivedi",
      "Dominik Wojtczak"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30106": {
    "title": "Provable Robustness against a Union of L_0 Adversarial Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zayd Hammoudeh",
      "Daniel Lowd"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30107": {
    "title": "All but One: Surgical Concept Erasing with Model Preservation in Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "SeungHoo Hong",
      "Juhun Lee",
      "Simon S. Woo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30108": {
    "title": "Towards Efficient Verification of Quantized Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pei Huang",
      "Haoze Wu",
      "Yuting Yang",
      "Ieva Daukantas",
      "Min Wu",
      "Yedi Zhang",
      "Clark Barrett"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30109": {
    "title": "On the Concept Trustworthiness in Concept Bottleneck Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qihan Huang",
      "Jie Song",
      "Jingwen Hu",
      "Haofei Zhang",
      "Yong Wang",
      "Mingli Song"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30110": {
    "title": "Personalization as a Shortcut for Few-Shot Backdoor Attack against Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihao Huang",
      "Felix Juefei-Xu",
      "Qing Guo",
      "Jie Zhang",
      "Yutong Wu",
      "Ming Hu",
      "Tianlin Li",
      "Geguang Pu",
      "Yang Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30111": {
    "title": "Stronger and Transferable Node Injection Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samyak Jain",
      "Tanima Dutta"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30112": {
    "title": "Learning Fair Policies for Multi-Stage Selection Problems from Observational Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuangzhuang Jia",
      "Grani A. Hanasusanto",
      "Phebe Vayanos",
      "Weijun Xie"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30113": {
    "title": "NeRFail: Neural Radiance Fields-Based Multiview Adversarial Attack",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenxiang Jiang",
      "Hanwei Zhang",
      "Xi Wang",
      "Zhongwen Guo",
      "Hao Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30114": {
    "title": "Analysis of Differentially Private Synthetic Data: A Measurement Error Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangdi Jiang",
      "Yi Liu",
      "Xiaodong Yan",
      "Anne-Sophie Charest",
      "Linglong Kong",
      "Bei Jiang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30115": {
    "title": "Chasing Fairness in Graphs: A GNN Architecture Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhimeng Jiang",
      "Xiaotian Han",
      "Chao Fan",
      "Zirui Liu",
      "Na Zou",
      "Ali Mostafavi",
      "Xia Hu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30116": {
    "title": "Assume-Guarantee Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Milad Kazemi",
      "Mateo Perez",
      "Fabio Somenzi",
      "Sadegh Soudjani",
      "Ashutosh Trivedi",
      "Alvaro Velasquez"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30117": {
    "title": "DeepBern-Nets: Taming the Complexity of Certifying Neural Networks Using Bernstein Polynomial Activations and Precise Bound Propagation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haitham Khedr",
      "Yasser Shoukry"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30118": {
    "title": "Layer Attack Unlearning: Fast and Accurate Machine Unlearning via Layer Level Attack and Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyunjune Kim",
      "Sangyong Lee",
      "Simon S. Woo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30119": {
    "title": "Quilt: Robust Data Segment Selection against Concept Drifts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minsu Kim",
      "Seong-Hyeon Hwang",
      "Steven Euijong Whang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30120": {
    "title": "OUTFOX: LLM-Generated Essay Detection Through In-Context Learning with Adversarially Generated Examples",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryuto Koike",
      "Masahiro Kaneko",
      "Naoaki Okazaki"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30121": {
    "title": "Accelerating Adversarially Robust Model Selection for Deep Neural Networks via Racing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthias König",
      "Holger H. Hoos",
      "Jan N. van Rijn"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30122": {
    "title": "Robust Active Measuring under Model Uncertainty",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Merlijn Krale",
      "Thiago D. Simão",
      "Jana Tumova",
      "Nils Jansen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30123": {
    "title": "Towards Large Certified Radius in Randomized Smoothing Using Quasiconcave Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo-Han Kung",
      "Shang-Tse Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30124": {
    "title": "Contrastive Credibility Propagation for Reliable Semi-supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brody Kutt",
      "Pralay Ramteke",
      "Xavier Mignot",
      "Pamela Toman",
      "Nandini Ramanan",
      "Sujit Rokka Chhetri",
      "Shan Huang",
      "Min Du",
      "William Hewlett"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30125": {
    "title": "Exponent Relaxation of Polynomial Zonotopes and Its Applications in Formal Neural Network Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tobias Ladner",
      "Matthias Althoff"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30126": {
    "title": "I Prefer Not to Say: Protecting User Consent in Models with Optional Personal Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tobias Leemann",
      "Martin Pawelczyk",
      "Christian Thomas Eberle",
      "Gjergji Kasneci"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30127": {
    "title": "Promoting Counterfactual Robustness through Diversity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesco Leofante",
      "Nico Potyka"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30128": {
    "title": "Revisiting the Information Capacity of Neural Network Watermarks: Upper Bound Estimation and Beyond",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangqi Li",
      "Haodong Zhao",
      "Wei Du",
      "Shilin Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30129": {
    "title": "PointCVaR: Risk-Optimized Outlier Removal for Robust 3D Point Cloud Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinke Li",
      "Junchi Lu",
      "Henghui Ding",
      "Changsheng Sun",
      "Joey Tianyi Zhou",
      "Yeow Meng Chee"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30130": {
    "title": "Game-Theoretic Unlearnable Example Generator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuang Liu",
      "Yihan Wang",
      "Xiao-Shan Gao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30131": {
    "title": "Beyond Traditional Threats: A Persistent Backdoor Attack on Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Liu",
      "Yuhang Zhang",
      "Zhu Feng",
      "Zhiqin Yang",
      "Chen Xu",
      "Dapeng Man",
      "Wu Yang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30132": {
    "title": "Handling Long and Richly Constrained Tasks through Constrained Hierarchical Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxiao Lu",
      "Arunesh Sinha",
      "Pradeep Varakantham"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30133": {
    "title": "Combining Graph Transformers Based Multi-Label Active Learning and Informative Data Augmentation for Chest Xray Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dwarikanath Mahapatra",
      "Behzad Bozorgtabar",
      "Zongyuan Ge",
      "Mauricio Reyes",
      "Jean-Philippe Thiran"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30134": {
    "title": "Enumerating Safe Regions in Deep Neural Networks with Provable Probabilistic Guarantees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luca Marzari",
      "Davide Corsi",
      "Enrico Marchesini",
      "Alessandro Farinelli",
      "Ferdinando Cicalese"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30135": {
    "title": "Divide-and-Aggregate Learning for Evaluating Performance on Unlabeled Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuyu Miao",
      "Jian Liu",
      "Lin Zheng",
      "Hong Jin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30136": {
    "title": "SentinelLMs: Encrypted Input Adaptation and Fine-Tuning of Language Models for Private and Secure Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhijit Mishra",
      "Mingda Li",
      "Soham Deo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30137": {
    "title": "Safeguarded Progress in Reinforcement Learning: Safe Bayesian Exploration for Control Policy Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rohan Mitta",
      "Hosein Hasanbeig",
      "Jun Wang",
      "Daniel Kroening",
      "Yiannis Kantaros",
      "Alessandro Abate"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30138": {
    "title": "Feature Unlearning for Pre-trained GANs and VAEs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saemi Moon",
      "Seunghyuk Cho",
      "Dongwoo Kim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30139": {
    "title": "Reward Certification for Policy Smoothed Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ronghui Mu",
      "Leandro Soriano Marcolino",
      "Yanghao Zhang",
      "Tianle Zhang",
      "Xiaowei Huang",
      "Wenjie Ruan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30140": {
    "title": "EncryIP: A Practical Encryption-Based Framework for Model Intellectual Property Protection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Mu",
      "Yu Wang",
      "Zhengan Huang",
      "Junzuo Lai",
      "Yehong Zhang",
      "Hui Wang",
      "Yue Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30141": {
    "title": "Neural Closure Certificates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alireza Nadali",
      "Vishnu Murali",
      "Ashutosh Trivedi",
      "Majid Zamani"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30142": {
    "title": "SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in Generative Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manish Nagireddy",
      "Lamogha Chiazor",
      "Moninder Singh",
      "Ioana Baldini"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30143": {
    "title": "MaxEnt Loss: Constrained Maximum Entropy for Calibration under Out-of-Distribution Shift",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dexter Neo",
      "Stefan Winkler",
      "Tsuhan Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30144": {
    "title": "ORES: Open-Vocabulary Responsible Visual Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minheng Ni",
      "Chenfei Wu",
      "Xiaodong Wang",
      "Shengming Yin",
      "Lijuan Wang",
      "Zicheng Liu",
      "Nan Duan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30145": {
    "title": "Q-SENN: Quantized Self-Explaining Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Norrenbrock",
      "Marco Rudolph",
      "Bodo Rosenhahn"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30146": {
    "title": "Understanding Likelihood of Normalizing Flow and Image Complexity through the Lens of Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Genki Osada",
      "Tsubasa Takahashi",
      "Takashi Nishide"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30147": {
    "title": "Adversarial Initialization with Universal Adversarial Perturbation: A New Approach to Fast Adversarial Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Pan",
      "Qing Li",
      "Xin Yao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30148": {
    "title": "A PAC Learning Algorithm for LTL and Omega-Regular Objectives in MDPs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mateo Perez",
      "Fabio Somenzi",
      "Ashutosh Trivedi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30149": {
    "title": "Robust Stochastic Graph Generator for Counterfactual Explanations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mario Alfonso Prado-Romero",
      "Bardh Prenkaj",
      "Giovanni Stilo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30150": {
    "title": "Visual Adversarial Examples Jailbreak Aligned Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangyu Qi",
      "Kaixuan Huang",
      "Ashwinee Panda",
      "Peter Henderson",
      "Mengdi Wang",
      "Prateek Mittal"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30151": {
    "title": "Dissenting Explanations: Leveraging Disagreement to Reduce Model Overreliance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Omer Reingold",
      "Judy Hanwen Shen",
      "Aditi Talati"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30152": {
    "title": "I-CEE: Tailoring Explanations of Image Classification Models to User Expertise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yao Rong",
      "Peizhu Qian",
      "Vaibhav Unhelkar",
      "Enkelejda Kasneci"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30153": {
    "title": "A Simple and Practical Method for Reducing the Disparate Impact of Differential Privacy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucas Rosenblatt",
      "Julia Stoyanovich",
      "Christopher Musco"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30154": {
    "title": "Interpretability Benchmark for Evaluating Spatial Misalignment of Prototypical Parts Explanations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mikołaj Sacha",
      "Bartosz Jura",
      "Dawid Rymarczyk",
      "Łukasz Struski",
      "Jacek Tabor",
      "Bartosz Zieliński"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30155": {
    "title": "Human-Guided Moral Decision Making in Text-Based Games",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijing Shi",
      "Meng Fang",
      "Ling Chen",
      "Yali Du",
      "Jun Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30156": {
    "title": "Towards Fairer Centroids in K-means Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stanley Simoes",
      "Deepak P",
      "Muiris MacCarthaigh"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30157": {
    "title": "Toward Robustness in Multi-Label Classification: A Data Augmentation Strategy against Imbalance and Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hwanjun Song",
      "Minseok Kim",
      "Jae-Gil Lee"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30158": {
    "title": "Bidirectional Contrastive Split Learning for Visual Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuwei Sun",
      "Hideya Ochiai"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30159": {
    "title": "Quantile-Based Maximum Likelihood Training for Outlier Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Masoud Taghikhah",
      "Nishant Kumar",
      "Siniša Šegvić",
      "Abouzar Eslami",
      "Stefan Gumhold"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30160": {
    "title": "Sparsity-Guided Holistic Explanation for LLMs with Interpretable Inference-Time Intervention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Tan",
      "Tianlong Chen",
      "Zhenyu Zhang",
      "Huan Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30161": {
    "title": "Toward More Generalized Malicious URL Detection Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yun-Da Tsai",
      "Cayon Liow",
      "Yin Sheng Siang",
      "Shou-De Lin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30162": {
    "title": "Self-Supervised Likelihood Estimation with Energy Guidance for Anomaly Segmentation in Urban Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanpeng Tu",
      "Yuxi Li",
      "Boshen Zhang",
      "Liang Liu",
      "Jiangning Zhang",
      "Yabiao Wang",
      "Cairong Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30163": {
    "title": "Pure-Past Action Masking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giovanni Varricchione",
      "Natasha Alechina",
      "Mehdi Dastani",
      "Giuseppe De Giacomo",
      "Brian Logan",
      "Giuseppe Perelli"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30164": {
    "title": "Long-Term Safe Reinforcement Learning with Binary Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akifumi Wachi",
      "Wataru Hashimoto",
      "Kazumune Hashimoto"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30165": {
    "title": "Identifying Reasons for Bias: An Argumentation-Based Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Madeleine Waller",
      "Odinaldo Rodrigues",
      "Oana Cocarascu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30166": {
    "title": "Would You Like Your Data to Be Trained? A User Controllable Recommendation Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Wang",
      "Xu Chen",
      "Zhenhua Dong",
      "Quanyu Dai"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30167": {
    "title": "Moderate Message Passing Improves Calibration: A Universal Way to Mitigate Confidence Bias in Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Min Wang",
      "Hao Yang",
      "Jincai Huang",
      "Qing Cheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30168": {
    "title": "Generating Diagnostic and Actionable Explanations for Fair Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenzhong Wang",
      "Qingyuan Zeng",
      "Wanyu Lin",
      "Min Jiang",
      "Kay Chen Tan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30169": {
    "title": "Physics-Informed Representation and Learning: Control and Risk Quantification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuoyuan Wang",
      "Reece Keller",
      "Xiyu Deng",
      "Kenta Hoshino",
      "Takashi Tanaka",
      "Yorie Nakahira"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30170": {
    "title": "Safe Reinforcement Learning with Instantaneous Constraints: The Role of Aggressive Exploration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Honghao Wei",
      "Xin Liu",
      "Lei Ying"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30171": {
    "title": "Concealing Sensitive Samples against Gradient Leakage in Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Wu",
      "Munawar Hayat",
      "Mingyi  Zhou",
      "Mehrtash Harandi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30172": {
    "title": "The Evidence Contraction Issue in Deep Evidential Regression: Discussion and Solution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuefei Wu",
      "Bin Shi",
      "Bo Dong",
      "Qinghua Zheng",
      "Hua Wei"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30173": {
    "title": "Byzantine-Robust Decentralized Learning via Remove-then-Clip Aggregation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Caiyi Yang",
      "Javad Ghaderi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30174": {
    "title": "Hypothesis Testing for Class-Conditional Noise Using Local Maximum Likelihood",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weisong Yang",
      "Rafael Poyiadzi",
      "Niall Twomey",
      "Raul Santos-Rodriguez"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30175": {
    "title": "Providing Fair Recourse over Plausible Groups",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jayanth Yetukuri",
      "Ian Hardy",
      "Yevgeniy Vorobeychik",
      "Berk Ustun",
      "Yang Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30176": {
    "title": "Representation-Based Robustness in Goal-Conditioned Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangyu Yin",
      "Sihao Wu",
      "Jiaxu Liu",
      "Meng Fang",
      "Xingyu Zhao",
      "Xiaowei Huang",
      "Wenjie Ruan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30177": {
    "title": "Enhancing Off-Policy Constrained Reinforcement Learning through Adaptive Ensemble C Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hengrui Zhang",
      "Youfang Lin",
      "Shuo Shen",
      "Sheng Han",
      "Kai Lv"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30178": {
    "title": "Efficient Toxic Content Detection by Bootstrapping and Distilling Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiang Zhang",
      "Qiong Wu",
      "Yiming Xu",
      "Cheng Cao",
      "Zheng Du",
      "Konstantinos Psounis"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30179": {
    "title": "LR-XFL: Logical Reasoning-Based Explainable Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanci Zhang",
      "Han Yu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30180": {
    "title": "GaLileo: General Linear Relaxation Framework for Tightening Robustness Certification of Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunruo Zhang",
      "Lujia Shen",
      "Shanqing Guo",
      "Shouling Ji"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30181": {
    "title": "A Huber Loss Minimization Approach to Byzantine Robust Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Puning Zhao",
      "Fei Yu",
      "Zhiguo Wan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30182": {
    "title": "Responsible Bandit Learning via Privacy-Protected Mean-Volatility Utility",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shanshan Zhao",
      "Wenhai Cui",
      "Bei Jiang",
      "Linglong Kong",
      "Xiaodong Yan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30183": {
    "title": "UMA: Facilitating Backdoor Scanning via Unlearning-Based Model Ablation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Zhao",
      "Congyi Li",
      "Kai Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30184": {
    "title": "AdvST: Revisiting Data Augmentations for Single Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangtao Zheng",
      "Mengdi Huai",
      "Aidong Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30185": {
    "title": "Can LLM Replace Stack Overflow? A Study on Robustness and Reliability of Large Language Model Code Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li Zhong",
      "Zilong Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30186": {
    "title": "DataElixir: Purifying Poisoned Dataset to Mitigate Backdoor Attacks via Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiachen Zhou",
      "Peizhuo Lv",
      "Yibing Lan",
      "Guozhu Meng",
      "Kai Chen",
      "Hualong Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30187": {
    "title": "Closing the Gap: Achieving Better Accuracy-Robustness Tradeoffs against Query-Based Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pascal Zimmer",
      "Sébastien Andreina",
      "Giorgia Azzurra Marson",
      "Ghassan Karame"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30188": {
    "title": "Coevolutionary Algorithm for Building Robust Decision Trees under Minimax Regret",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adam Żychowski",
      "Andrew Perrault",
      "Jacek Mańdziuk"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30189": {
    "title": "BirdCollect: A Comprehensive Benchmark for Analyzing Dense Bird Flock Attributes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kshitiz .",
      "Sonu Shreshtha",
      "Bikash Dutta",
      "Muskan Dosi",
      "Mayank Vatsa",
      "Richa Singh",
      "Saket Anand",
      "Sudeep Sarkar",
      "Sevaram Mali Parihar"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30190": {
    "title": "A Bayesian Spatial Model to Correct Under-Reporting in Urban Crowdsourcing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gabriel Agostini",
      "Emma Pierson",
      "Nikhil Garg"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30191": {
    "title": "Automatic Interpretation of Line Probe Assay Test for Tuberculosis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jatin Agrawal",
      "Mukul Kumar",
      "Avtansh Tiwari",
      "Sachin Danisetty",
      "Soma Dhavala",
      "Nakul Jain",
      "Prasaanth Balraj",
      "Niket Singh",
      "Siddhant Shingi",
      "Jayakrishna Kurada",
      "Raghuram Rao",
      "S Anand",
      "Nishant Kumar"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30192": {
    "title": "Physics-Informed Graph Neural Networks for Water Distribution Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Inaam Ashraf",
      "Janine Strotherm",
      "Luca Hermes",
      "Barbara Hammer"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30193": {
    "title": "Quantile-Regression-Ensemble: A Deep Learning Algorithm for Downscaling Extreme Precipitation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Bailie",
      "Yun Sing Koh",
      "Neelesh Rampal",
      "Peter B. Gibson"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30194": {
    "title": "Early Detection of Extreme Storm Tide Events Using Multimodal Data Processing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marcel Barros",
      "Andressa Pinto",
      "Andres Monroy",
      "Felipe Moreno",
      "Jefferson Coelho",
      "Aldomar Pietro Silva",
      "Caio Fabricio Deberaldini Netto",
      "José Roberto Leite",
      "Marlon Mathias",
      "Eduardo Tannuri",
      "Artur Jordao",
      "Edson Gomi",
      "Fabio Cozman",
      "Marcelo Dottori",
      "Anna Helena Reali Costa"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30195": {
    "title": "Decision-Making for Land Conservation: A Derivative-Free Optimization Framework with Nonlinear Inputs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cassidy K. Buhler",
      "Hande Y. Benson"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30196": {
    "title": "CariesXrays: Enhancing Caries Detection in Hospital-Scale Panoramic Dental X-rays via Feature Pyramid Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bingzhi Chen",
      "Sisi Fu",
      "Yishu Liu",
      "Jiahui Pan",
      "Guangming Lu",
      "Zheng Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30197": {
    "title": "Referee-Meta-Learning for Fast Adaptation of Locational Fairness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiye Chen",
      "Yiqun Xie",
      "Xiaowei Jia",
      "Erhu He",
      "Han Bao",
      "Bang An",
      "Xun Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30198": {
    "title": "From Artificially Real to Real: Leveraging Pseudo Data from Large Language Models for Low-Resource Molecule Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhan Chen",
      "Nuwa Xi",
      "Yanrui Du",
      "Haochun Wang",
      "Jianyu Chen",
      "Sendong Zhao",
      "Bing Qin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30199": {
    "title": "Auto311: A Confidence-Guided Automated System for Non-emergency Calls",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zirong Chen",
      "Xutong Sun",
      "Yuanhe Li",
      "Meiyi Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30200": {
    "title": "Blind-Touch: Homomorphic Encryption-Based Distributed Neural Network Inference for Privacy-Preserving Fingerprint Authentication",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyunmin Choi",
      "Simon S. Woo",
      "Hyoungshick Kim"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30201": {
    "title": "Identifying Guarantors of War Veterans Using Robust-SEAL: A Case of the Korean War",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jong in Choi",
      "Won Kyung Lee",
      "Jae Hwan Lee",
      "So Young Sohn"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30202": {
    "title": "Fair Sampling in Diffusion Models through Switching Mechanism",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujin Choi",
      "Jinseong Park",
      "Hoki Kim",
      "Jaewook Lee",
      "Saerom Park"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30203": {
    "title": "Arbitrariness and Social Prediction: The Confounding Role of Variance in Fair Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "A. Feder Cooper",
      "Katherine Lee",
      "Madiha Zahrah Choksi",
      "Solon Barocas",
      "Christopher De Sa",
      "James Grimmelmann",
      "Jon Kleinberg",
      "Siddhartha Sen",
      "Baobao Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30204": {
    "title": "Finding ε and δ of Traditional Disclosure Control Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saswat Das",
      "Keyu Zhu",
      "Christine Task",
      "Pascal Van Hentenryck",
      "Ferdinando Fioretto"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30205": {
    "title": "MedAlign: A Clinician-Generated Dataset for Instruction Following with Electronic Medical Records",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Scott L. Fleming",
      "Alejandro Lozano",
      "William J. Haberkorn",
      "Jenelle A. Jindal",
      "Eduardo Reis",
      "Rahul Thapa",
      "Louis Blankemeier",
      "Julian Z. Genkins",
      "Ethan Steinberg",
      "Ashwin Nayak",
      "Birju Patel",
      "Chia-Chun Chiang",
      "Alison Callahan",
      "Zepeng Huo",
      "Sergios Gatidis",
      "Scott Adams",
      "Oluseyi Fayanju",
      "Shreya J. Shah",
      "Thomas Savage",
      "Ethan Goh",
      "Akshay S. Chaudhari",
      "Nima Aghaeepour",
      "Christopher Sharp",
      "Michael A. Pfeffer",
      "Percy Liang",
      "Jonathan H. Chen",
      "Keith E. Morse",
      "Emma P. Brunskill",
      "Jason A. Fries",
      "Nigam H. Shah"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30206": {
    "title": "CLIPSyntel: CLIP and LLM Synergy for Multimodal Question Summarization in Healthcare",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akash Ghosh",
      "Arkadeep Acharya",
      "Raghav Jain",
      "Sriparna Saha",
      "Aman Chadha",
      "Setu Sinha"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30207": {
    "title": "Benchmarking Cyber Harassment Dialogue Comprehension through Emotion-Informed Manifestations-Determinants Demarcation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soumitra Ghosh",
      "Gopendra Vikram Singh",
      "Jashn Arora",
      "Asif Ekbal"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30208": {
    "title": "Grey-Box Bayesian Optimization for Sensor Placement in Assisted Living Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shadan Golestan",
      "Omid Ardakanian",
      "Pierre Boulanger"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30209": {
    "title": "Federated Learning via Input-Output Collaborative Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan Gong",
      "Shanglin Li",
      "Yuxiang Bao",
      "Barry Yao",
      "Yawen Huang",
      "Ziyan Wu",
      "Baochang Zhang",
      "Yefeng Zheng",
      "David Doermann"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30210": {
    "title": "Scaling Up Pareto Optimization for Tree Structures with Affine Transformations: Evaluating Hybrid Floating Solar-Hydropower Systems in the Amazon",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marc Grimson",
      "Rafael Almeida",
      "Qinru Shi",
      "Yiwei Bai",
      "Héctor Angarita",
      "Felipe Siqueira Pacheco",
      "Rafael Schmitt",
      "Alexander Flecker",
      "Carla P. Gomes"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30211": {
    "title": "Fair Multivariate Adaptive Regression Splines for Ensuring Equity and Transparency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Parian Haghighat",
      "Denisa Gándara",
      "Lulu Kang",
      "Hadis Anahideh"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30212": {
    "title": "Fair Graph Learning Using Constraint-Aware Priority Adjustment and Graph Masking in River Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Erhu He",
      "Yiqun Xie",
      "Alexander Sun",
      "Jacob Zwart",
      "Jie Yang",
      "Zhenong Jin",
      "Yang Wang",
      "Hassan Karimi",
      "Xiaowei Jia"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30213": {
    "title": "Multi-Modal Discussion Transformer: Integrating Text, Images and Graph Transformers to Detect Hate Speech on Social Media",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liam Hebert",
      "Gaurav Sahu",
      "Yuxuan Guo",
      "Nanda Kishore Sreenivas",
      "Lukasz Golab",
      "Robin Cohen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30214": {
    "title": "Bad Actor, Good Advisor: Exploring the Role of Large Language Models in Fake News Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Beizhe Hu",
      "Qiang Sheng",
      "Juan Cao",
      "Yuhui Shi",
      "Yang Li",
      "Danding Wang",
      "Peng Qi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30215": {
    "title": "Long-Term Fair Decision Making through Deep Generative Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaowei Hu",
      "Yongkai Wu",
      "Lu Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30216": {
    "title": "CityPulse: Fine-Grained Assessment of Urban Change with Street View Time Series",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyuan Huang",
      "Zejia Wu",
      "Jiajun Wu",
      "Jackelyn Hwang",
      "Ram Rajagopal"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30217": {
    "title": "iTrendRNN: An Interpretable Trend-Aware RNN for Meteorological Spatiotemporal Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xu Huang",
      "Chuyao Luo",
      "Bowen Zhang",
      "Huiwei Lin",
      "Xutao Li",
      "Yunming Ye"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30218": {
    "title": "Where It Really Matters: Few-Shot Environmental Conservation Media Monitoring for Low-Resource Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sameer Jain",
      "Sedrick Scott Keh",
      "Shova Chhetri",
      "Karun Dewan",
      "Pablo  Izquierdo",
      "Johanna   Prussmann",
      "Pooja Shrestha",
      "César Suárez",
      "Zheyuan Ryan Shi",
      "Lei Li",
      "Fei Fang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30219": {
    "title": "Active Reinforcement Learning for Robust Building Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Doseok Jang",
      "Larry Yan",
      "Lucas Spangher",
      "Costas J. Spanos"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30220": {
    "title": "Adversarial Fairness Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taeuk Jang",
      "Xiaoqian Wang",
      "Heng Huang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30221": {
    "title": "Unraveling Pain Levels: A Data-Uncertainty Guided Approach for Effective Pain Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinwei Ji",
      "Xiaomin Chang",
      "Wei Li",
      "Albert Y. Zomaya"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30222": {
    "title": "Outlier Ranking for Large-Scale Public Health Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ananya Joshi",
      "Tina Townes",
      "Nolan Gormley",
      "Luke  Neureiter",
      "Roni Rosenfeld",
      "Bryan Wilder"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30223": {
    "title": "Deploying ADVISER: Impact and Lessons from Using Artificial Intelligence for Child Vaccination Uptake in Nigeria",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Opadele Kehinde ",
      "Ruth Abdul",
      "Bose Afolabi",
      "Parminder Vir",
      "Corinne Namblard",
      "Ayan Mukhopadhyay",
      "Abiodun Adereni"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30224": {
    "title": "Vector Field Oriented Diffusion Model for Crystal Material Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Astrid Klipfel",
      "Yaël Fregier",
      "Adlane Sayede",
      "Zied Bouraoui"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30225": {
    "title": "Combining Deep Learning and Street View Imagery to Map Smallholder Crop Types",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jordi Laguarta Soler",
      "Thomas Friedel",
      "Sherrie Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30226": {
    "title": "GLH-Water: A Large-Scale Dataset for Global Surface Water Detection in Large-Size Very-High-Resolution Satellite Imagery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yansheng Li",
      "Bo Dang",
      "Wanchun Li",
      "Yongjun Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30227": {
    "title": "AutoLTS: Automating Cycling Stress Assessment via Contrastive Learning and Spatial Post-processing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Lin",
      "Shoshanna Saxe",
      "Timothy C. Y. Chan"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30228": {
    "title": "Depression Detection via Capsule Networks with Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Liu",
      "Changya Li",
      "Xiaotong Zhang",
      "Feng Zhang",
      "Wei Wang",
      "Fenglong Ma",
      "Hongyang Chen",
      "Hong Yu",
      "Xianchao Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30229": {
    "title": "On the Actionability of Outcome Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lydia T. Liu",
      "Solon Barocas",
      "Jon Kleinberg",
      "Karen Levy"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30230": {
    "title": "Hear You Say You: An Efficient Framework for Marine Mammal Sounds' Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangrui  Liu",
      "Xiaoou Liu",
      "Shan Du",
      "Julian  Cheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30231": {
    "title": "Identifying and Addressing Disparities in Public Libraries with Bayesian Latent Variable Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhi Liu",
      "Sarah Rankin",
      "Nikhil Garg"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30232": {
    "title": "Interpretable Long-Form Legal Question Answering with Retrieval-Augmented Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antoine Louis",
      "Gijs van Dijck",
      "Gerasimos Spanakis"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30233": {
    "title": "T-NET: Weakly Supervised Graph Learning for Combatting Human Trafficking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pratheeksha Nair",
      "Javin Liu",
      "Catalina Vajiac",
      "Andreas Olligschlaeger",
      "Duen Horng Chau",
      "Mirela Cazzolato",
      "Cara Jones",
      "Christos Faloutsos",
      "Reihaneh Rabbany"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30234": {
    "title": "Promoting Fair Vaccination Strategies through Influence Maximization: A Case Study on COVID-19 Spread",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicola Neophytou",
      "Afaf Taik",
      "Golnoosh Farnadi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30235": {
    "title": "DISCount: Counting in Large Image Collections with Detector-Based Importance Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gustavo Perez",
      "Subhransu Maji",
      "Daniel Sheldon"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30236": {
    "title": "Discretionary Trees: Understanding Street-Level Bureaucracy via Machine Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gaurab Pokharel",
      "Sanmay Das",
      "Patrick Fowler"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30237": {
    "title": "IndicCONAN: A Multilingual Dataset for Combating Hate Speech in Indian Context",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nihar Ranja Sahoo",
      "Gyana Prakash Beria",
      "Pushpak Bhattacharyya"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30238": {
    "title": "Carbon Footprint Reduction for Sustainable Data Centers in Real-Time",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soumyendu Sarkar",
      "Avisek Naug",
      "Ricardo Luna",
      "Antonio Guillen",
      "Vineet Gundecha",
      "Sahand Ghorbanpour",
      "Sajad Mousavi",
      "Dejan Markovikj",
      "Ashwin Ramesh Babu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30239": {
    "title": "Evaluating Pre-trial Programs Using Interpretable Machine Learning Matching Algorithms for Causal Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Travis Seale-Carlisle",
      "Saksham Jain",
      "Courtney Lee",
      "Caroline Levenson",
      "Swathi Ramprasad",
      "Brandon Garrett",
      "Sudeepa Roy",
      "Cynthia Rudin",
      "Alexander Volfovsky"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30240": {
    "title": "Self-Supervised Framework Based on Subject-Wise Clustering for Human Subject Time Series Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eunseon Seong",
      "Harim Lee",
      "Dong-Kyu Chae"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30241": {
    "title": "Characterizing Information Seeking Events in Health-Related Social Discourse",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Omar Sharif",
      "Madhusudan Basak",
      "Tanzia Parvin",
      "Ava Scharfstein",
      "Alphonso Bradham",
      "Jacob T. Borodovsky",
      "Sarah E. Lord",
      "Sarah M. Preum"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30242": {
    "title": "Nowcasting Temporal Trends Using Indirect Surveys",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ajitesh Srivastava",
      "Juan Marcos Ramirez",
      "Sergio Díaz-Aranda",
      "Jose Aguilar",
      "Antonio Fernández Anta",
      "Antonio Ortega",
      "Rosa Elvira Lillo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30243": {
    "title": "FairPlay: A Multi-Sided Fair Dynamic Pricing Policy for Hotels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Errikos Streviniotis",
      "Athina Georgara",
      "Filippo Bistaffa",
      "Georgios Chalkiadakis"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30244": {
    "title": "Stable Matchings in Practice: A Constraint Programming Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaohong Sun",
      "Naoyuki Yamada",
      "Yoshihiro Takenami",
      "Daisuke Moriwaki",
      "Makoto Yokoo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30245": {
    "title": "Social, Legal, Ethical, Empathetic, and Cultural Rules: Compilation and Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicolas Troquard",
      "Martina De Sanctis",
      "Paola Inverardi",
      "Patrizio Pelliccione",
      "Gian Luca Scoccia"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30246": {
    "title": "Preventing Eviction-Caused Homelessness through ML-Informed Distribution of Rental Assistance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Catalina Vajiac",
      "Arun Frey",
      "Joachim Baumann",
      "Abigail Smith",
      "Kasun Amarasinghe",
      "Alice Lai",
      "Kit T. Rodolfa",
      "Rayid Ghani"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30247": {
    "title": "RLPeri: Accelerating Visual Perimetry Test with Reinforcement Learning and Convolutional Feature Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tanvi Verma",
      "Linh Le Dinh",
      "Nicholas Tan",
      "Xinxing Xu",
      "Chingyu Cheng",
      "Yong Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30248": {
    "title": "Deep Reinforcement Learning for Early Diagnosis of Lung Cancer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Wang",
      "Qining Zhang",
      "Lei Ying",
      "Chuan Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30249": {
    "title": "SimFair: Physics-Guided Fairness-Aware Learning with Simulation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihao Wang",
      "Yiqun Xie",
      "Zhili Li",
      "Xiaowei Jia",
      "Zhe Jiang",
      "Aolin Jia",
      "Shuo Xu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30250": {
    "title": "I Open at the Close: A Deep Reinforcement Learning Evaluation of Open Streets Initiatives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "R. Teal Witter",
      "Lucas Rosenblatt"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30251": {
    "title": "HarvestNet: A Dataset for Detecting Smallholder Farming Activity Using Harvest Piles and Remote Sensing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonathan Xu",
      "Amna Elmustafa",
      "Liya Weldegebriel",
      "Emnet Negash",
      "Richard Lee",
      "Chenlin Meng",
      "Stefano Ermon",
      "David Lobell"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30252": {
    "title": "Harnessing Network Effect for Fake News Mitigation: Selecting Debunkers via Self-Imitation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaofei Xu",
      "Ke Deng",
      "Michael Dann",
      "Xiuzhen Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30253": {
    "title": "Spatial-Logic-Aware Weakly Supervised Learning for Flood Mapping on Earth Imagery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zelin Xu",
      "Tingsong Xiao",
      "Wenchong He",
      "Yu Wang",
      "Zhe Jiang",
      "Shigang  Chen",
      "Yiqun Xie",
      "Xiaowei Jia",
      "Da Yan",
      "Yang Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30254": {
    "title": "Unveiling the Tapestry of Automated Essay Scoring: A Comprehensive Investigation of Accuracy, Fairness, and Generalizability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaixun Yang",
      "Mladen Raković",
      "Yuyang Li",
      "Quanlong Guan",
      "Dragan Gašević",
      "Guangliang Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30255": {
    "title": "Graph Bayesian Optimization for Multiplex Influence Maximization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zirui Yuan",
      "Minglai Shao",
      "Zhiqian Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30256": {
    "title": "Fairness-Aware Structured Pruning in Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abdelrahman Zayed",
      "Gonçalo Mordido",
      "Samira Shabanian",
      "Ioana Baldini",
      "Sarath Chandar"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30257": {
    "title": "Estimating On-Road Transportation Carbon Emissions from Open Data of Road Network and Origin-Destination Flow Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinwei Zeng",
      "Yu  Liu",
      "Jingtao Ding",
      "Jian Yuan",
      "Yong Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30258": {
    "title": "Towards Automatic Boundary Detection for Human-AI Collaborative Hybrid Essay in Education",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijie Zeng",
      "Lele Sha",
      "Yuheng Li",
      "Kaixun Yang",
      "Dragan Gašević",
      "Guangliang Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30259": {
    "title": "Pre-trained Online Contrastive Learning for Insurance Fraud Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Zhang",
      "Dawei Cheng",
      "Jie Yang",
      "Yi Ouyang",
      "Xian Wu",
      "Yefeng Zheng",
      "Changjun Jiang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30260": {
    "title": "UV-SAM: Adapting Segment Anything Model for Urban Village Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Zhang",
      "Yu  Liu",
      "Yuming Lin",
      "Qingmin Liao",
      "Yong Li"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30261": {
    "title": "Causally Aware Generative Adversarial Networks for Light Pollution Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuyao Zhang",
      "Ke Guo",
      "Xiao Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30262": {
    "title": "Multiple-Source Localization from a Single-Snapshot Observation Using Graph Bayesian Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zonghan Zhang",
      "Zijian Zhang",
      "Zhiqian Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30263": {
    "title": "Leveraging Opposite Gender Interaction Ratio as a Path towards Fairness in Online Dating Recommendations Based on User Sexual Orientation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuying Zhao",
      "Yu Wang",
      "Yi Zhang",
      "Pamela Wisniewski",
      "Charu Aggarwal",
      "Tyler Derr"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30264": {
    "title": "AI-Based Energy Transportation Safety: Pipeline Radial Threat Estimation Using Intelligent Sensing System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengyuan Zhu",
      "Yiyuan Yang",
      "Kaixiang Yang",
      "Haifeng Zhang",
      "Qinmin Yang",
      "C. L. Philip Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30265": {
    "title": "TAU: Trajectory Data Augmentation with Uncertainty for Next POI Recommendation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuang Zhuang",
      "Tianxin Wei",
      "Lingbo Liu",
      "Heng Qi",
      "Yanming Shen",
      "Baocai Yin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30266": {
    "title": "Recommender Ecosystems: A Mechanism Design Perspective on Holistic Modeling and Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Craig Boutilier",
      "Martin Mladenov",
      "Guy Tennenholtz"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30267": {
    "title": "Model Reprogramming: Resource-Efficient Cross-Domain Machine Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pin-Yu Chen"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30268": {
    "title": "Conversational Modeling for Constraint Satisfaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eugene C. Freuder"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30269": {
    "title": "Integrated Systems for Computational Scientific Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pat Langley"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30270": {
    "title": "Towards a More Burkean Approach to Computational Social Choice",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Omer Lev"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30271": {
    "title": "Regeneration Learning: A Learning Paradigm for Data Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xu Tan",
      "Tao Qin",
      "Jiang Bian",
      "Tie-Yan Liu",
      "Yoshua Bengio"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30272": {
    "title": "The Fairness Fair: Bringing Human Perception into Collective Decision-Making",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hadi Hosseini"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30273": {
    "title": "Temporal Fairness in Multiwinner Voting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Edith Elkind",
      "Svetlana Obraztsova",
      "Nicholas Teh"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30274": {
    "title": "Mixed Fair Division: A Survey",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengxin Liu",
      "Xinhang Lu",
      "Mashbat Suzuki",
      "Toby Walsh"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30275": {
    "title": "Adventures of Trustworthy Vision-Language Models: A Survey",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mayank Vatsa",
      "Anubhooti Jain",
      "Richa Singh"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30276": {
    "title": "Interactive Theorem Provers: Applications in AI, Opportunities, and Challenges",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Abdulaziz"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30277": {
    "title": "Symbolic Reasoning Methods for AI Planning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gregor Behnke"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30278": {
    "title": "Demystifying Algorithmic Fairness in an Uncertain World",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lu Cheng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30279": {
    "title": "Data-Efficient Graph Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaize Ding"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30280": {
    "title": "Making Natural Language Reasoning Explainable and Faithful",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinya Du"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30281": {
    "title": "Towards Robust Visual Understanding: from Recognition to Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tejas Gokhale"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30282": {
    "title": "Continual Learning in an Open and Dynamic World",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunhui Guo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30283": {
    "title": "Scaling Offline Evaluation of Reinforcement Learning Agents through Abstraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Josiah P. Hanna"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30284": {
    "title": "Collaborative Learning across Heterogeneous Systems with Pre-Trained Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Trong Nghia Hoang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30285": {
    "title": "Understanding Surprising Generalization Phenomena in Deep Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Hu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30286": {
    "title": "Fostering Trustworthiness in Machine Learning Algorithms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengdi Huai"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30287": {
    "title": "Deep Learning on Graphs: A Data-Centric Exploration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Jin"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30288": {
    "title": "Quantifying Political Polarization through the Lens of Machine Translation and Vicarious Offense",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashiqur R. KhudaBukhsh"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30289": {
    "title": "Learning Representations for Robust Human-Robot Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yen-Ling Kuo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30290": {
    "title": "The Role of Over-Parameterization in Machine Learning – the Good, the Bad, the Ugly",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fanghui Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30291": {
    "title": "Algorithmic Foundation of Federated Learning with Sequential Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingrui Liu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30292": {
    "title": "When Causal Inference Meets Graph Machine Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Ma"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30293": {
    "title": "Towards Holistic, Pragmatic and Multimodal Conversational Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pranava Madhyastha"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30294": {
    "title": "From Statistical Relational to Neuro-Symbolic Artificial Intelligence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giuseppe Marra"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30295": {
    "title": "Harmonious Mobility for Robots that Work with and around People",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christoforos Mavrogiannis"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30296": {
    "title": "Recent Advancements in Inverse Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alberto Maria Metelli"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30297": {
    "title": "Exploiting Data Geometry in Machine Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Melanie Weber"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30298": {
    "title": "Towards Trustworthy Deep Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tsui-Wei (Lily) Weng"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30299": {
    "title": "Towards Reliable Learning in the Wild: Generalization and Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huaxiu Yao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30300": {
    "title": "Towards Human-like Learning from Relational Structured Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quanming Yao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30301": {
    "title": "Fairness with Censorship: Bridging the Gap between Fairness Research and Real-World Deployment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenbin Zhang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30302": {
    "title": "Fair and Optimal Prediction via Post-Processing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30303": {
    "title": "Towards Reproducible, Automated, and Scalable Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Zhao"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30304": {
    "title": "Combating Insider Threat in the Open-World Environments: Identification, Monitoring, and Data Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dawei Zhou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30590": {
    "title": "Select and Augment: Enhanced Dense Retrieval Knowledge Graph Augmentation (Abstract Reprint)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Micheal Abaho",
      "Yousef H. Alfaifi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30591": {
    "title": "Program Synthesis with Best-First Bottom-Up Search (Abstract Reprint)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saqib Ameen",
      "Levi H. S. Lelis"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30592": {
    "title": "Monitoring of Perception Systems: Deterministic, Probabilistic, and Learning-Based Fault Detection and Identification (Abstract Reprint)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pasquale Antonante",
      "Heath Nilsen",
      "Luca Carlone"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30593": {
    "title": "A General Model for Aggregating Annotations AcrossSimple, Complex, and Multi-object Annotation Tasks (Abstract Reprint)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Braylan",
      "Madalyn Marabella",
      "Omar Alonso",
      "Matthew Lease"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30594": {
    "title": "Temporal Logic Explanations for Dynamic Decision Systems Using Anchors and Monte Carlo Tree Search (Abstract Reprint)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tzu-Yi Chiu",
      "Jerome Le Ny",
      "Jean-Pierre David"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30595": {
    "title": "Mimicking Behaviors in Separated Domains (Abstract Reprint)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giuseppe De Giacomo",
      "Dror Fried",
      "Fabio Patrizi",
      "Shufang Zhu"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30596": {
    "title": "Counterfactual Explanations for Misclassified Images: How Human and Machine Explanations Differ (Abstract Reprint)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eoin Delaney",
      "Arjun Pakrashi",
      "Derek Greene",
      "Mark T. Keane"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30597": {
    "title": "Reasoning about Causality in Games (Abstract Reprint)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lewis Hammond",
      "James Fox",
      "Tom Everitt",
      "Ryan Carey",
      "Alessandro Abate",
      "Michael Wooldridge"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30598": {
    "title": "A Survey of Learning Criteria Going beyond the Usual Risk (Abstract Reprint)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew J. Holland",
      "Kazuki Tanabe"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30599": {
    "title": "Sim-to-Lab-to-Real: Safe Reinforcement Learning with Shielding and Generalization Guarantees (Abstract Reprint)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai-Chieh Hsu",
      "Allen Z. Ren",
      "Duy P. Nguyen",
      "Anirudha Majumdar",
      "Jaime F. Fisac"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30600": {
    "title": "FlexiBO: A Decoupled Cost-Aware Multi-objective Optimization Approach for Deep Neural Networks (Abstract Reprint)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Shahriar Iqbal",
      "Jianhai Su",
      "Lars Kotthoff",
      "Pooyan Jamshidi"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30601": {
    "title": "Discovering Agents (Abstract Reprint)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zachary Kenton",
      "Ramana Kumar",
      "Sebastian Farquhar",
      "Jonathan Richens",
      "Matt MacDermott",
      "Tom Everitt"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30602": {
    "title": "Reward (Mis)design for Autonomous Driving (Abstract Reprint)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "W. Bradley Knox",
      "Alessandro Allievi",
      "Holger Banzhaf",
      "Felix Schmitt",
      "Peter Stone"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30603": {
    "title": "The Defeat of the Winograd Schema Challenge (Abstract Reprint)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vid Kocijan",
      "Ernest Davis",
      "Thomas Lukasiewicz",
      "Gary Marcus",
      "Leora Morgenstern"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30604": {
    "title": "Convolutional Spectral Kernel Learning with Generalization Guarantees (Abstract Reprint)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Li",
      "Yong Liu",
      "Weiping Wang"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30605": {
    "title": "G–LIME: Statistical Learning for Local Interpretations of Deep Neural Networks Using Global Priors (Abstract Reprint)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuhong Li",
      "Haoyi Xiong",
      "Xingjian Li",
      "Xiao Zhang",
      "Ji Liu",
      "Haiyan Jiang",
      "Zeyu Chen",
      "Dejing Dou"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30606": {
    "title": "Exploiting Action Impact Regularity and Exogenous State Variables for Offline Reinforcement Learning (Abstract Reprint)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vincent Liu",
      "James R. Wright",
      "Mrtha White"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30607": {
    "title": "Introduction to the Special Track on Artificial Intelligence and COVID-19 (Abstract Reprint)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Martin Michalowski",
      "Robert Moskovitch",
      "Nitesh V. Chawla"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30608": {
    "title": "TEAMSTER: Model-Based Reinforcement Learning for Ad Hoc Teamwork (Abstract Reprint)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "João G. Ribeiro",
      "Gonçalo Rodrigues",
      "Alberto Sardinha",
      "Francisco S. Melo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30609": {
    "title": "Sequential Model-Based Diagnosis by Systematic Search (Abstract Reprint)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Patrick Rodler"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30610": {
    "title": "Actor Prioritized Experience Replay (Abstract Reprint)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baturay Saglam",
      "Furkan Mutlu",
      "Dogan Cicek",
      "Suleyman Kozat"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30611": {
    "title": "Accurate Parameter Estimation for Safety-Critical Systems with Unmodeled Dynamics (Abstract Reprint)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arnab Sarker",
      "Peter Fisher",
      "Joseph Gaudio",
      "Anuradha Annaswamy"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30612": {
    "title": "Your Prompt Is My Command: On Assessing the Human-Centred Generality of Multimodal Models (Abstract Reprint)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wout Schellaert",
      "Fernando Martínez-Plumed",
      "Karina Vold",
      "John Burden",
      "Pablo A. M. Casares",
      "Bao Sheng Loe",
      "Roi Reichart",
      "Sean Ó hÉigeartaigh",
      "Anna Korhonen",
      "José Hernández-Orallo"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30613": {
    "title": "Reward-Respecting Subtasks for Model-Based Reinforcement Learning (Abstract Reprint)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Richard S. Sutton",
      "Marlos C. Machado",
      "G. Zacharias Holland",
      "David Szepesvari",
      "Finbarr Timbers",
      "Brian Tanner",
      "Adam White"
    ]
  },
  "https://ojs.aaai.org/index.php/AAAI/article/view/30614": {
    "title": "Post-trained Convolution Networks for Single Image Super-resolution (Abstract Reprint)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seid Miad Zandavi"
    ]
  }
}