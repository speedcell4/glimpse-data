{
  "https://aclanthology.org/2023.emnlp-main.1": {
    "title": "IAG: Induction-Augmented Generation Framework for Answering Reasoning Questions",
    "volume": "main",
    "abstract": "Retrieval-Augmented Generation (RAG), by incorporating external knowledge with parametric memory of language models, has become the state-of-the-art architecture for open-domain QA tasks. However, common knowledge bases are inherently constrained by limited coverage and noisy information, making retrieval-based approaches inadequate to answer implicit reasoning questions. In this paper, we propose an Induction-Augmented Generation (IAG) framework that utilizes inductive knowledge along with the retrieved documents for implicit reasoning. We leverage large language models (LLMs) for deriving such knowledge via a novel prompting method based on inductive reasoning patterns. On top of this, we implement two versions of IAG named IAG-GPT and IAG-Student, respectively. IAG-GPT directly utilizes the knowledge generated by GPT-3 for answer prediction, while IAG-Student gets rid of dependencies on GPT service at inference time by incorporating a student inductor model. The inductor is firstly trained via knowledge distillation and further optimized by back-propagating the generator feedback via differentiable beam scores. Experimental results show that IAG outperforms RAG baselines as well as ChatGPT on two Open-Domain QA tasks. Notably, our best models have won the first place in the official leaderboards of CSQA2.0 (since Nov 1, 2022) and StrategyQA (since Jan 8, 2023)",
    "checked": true,
    "id": "bd1ef429956cea3542c14006df67e890cb57a80b",
    "semantic_title": "iag: induction-augmented generation framework for answering reasoning questions",
    "citation_count": 0,
    "authors": [
      "Zhebin Zhang",
      "Xinyu Zhang",
      "Yuanhang Ren",
      "Saijiang Shi",
      "Meng Han",
      "Yongkang Wu",
      "Ruofei Lai",
      "Zhao Cao"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.2": {
    "title": "Absolute Position Embedding Learns Sinusoid-like Waves for Attention Based on Relative Position",
    "volume": "main",
    "abstract": "Attention weight is a clue to interpret how a Transformer-based model makes an inference. In some attention heads, the attention focuses on the neighbors of each token. This allows the output vector of each token to depend on the surrounding tokens and contributes to make the inference context-dependent. We analyze the mechanism behind the concentration of attention on nearby tokens. We show that the phenomenon emerges as follows: (1) learned position embedding has sinusoid-like components, (2) such components are transmitted to the query and the key in the self-attention, (3) the attention head shifts the phases of the sinusoid-like components so that the attention concentrates on nearby tokens at specific relative positions. In other words, a certain type of Transformer-based model acquires the sinusoidal positional encoding to some extent on its own through Masked Language Modeling",
    "checked": true,
    "id": "1681fb0b88538d5d17aa7e321e979d54a5433365",
    "semantic_title": "absolute position embedding learns sinusoid-like waves for attention based on relative position",
    "citation_count": 0,
    "authors": [
      "Yuji Yamamoto",
      "Takuya Matsuzaki"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.3": {
    "title": "Chinese Lexical Substitution: Dataset and Method",
    "volume": "main",
    "abstract": "Existing lexical substitution (LS) benchmarks were collected by asking human annotators to think of substitutes from memory, resulting in benchmarks with limited coverage and relatively small scales. To overcome this problem, we propose a novel annotation method to construct an LS dataset based on human and machine collaboration. Based on our annotation method, we construct the first Chinese LS dataset CHNLS which consists of 33,695 instances and 144,708 substitutes, covering three text genres (News, Novel, and Wikipedia). Specifically, we first combine four unsupervised LS methods as an ensemble method to generate the candidate substitutes, and then let human annotators judge these candidates or add new ones. This collaborative process combines the diversity of machine-generated substitutes with the expertise of human annotators. Experimental results that the ensemble method outperforms other LS methods. To our best knowledge, this is the first study for the Chinese LS task",
    "checked": true,
    "id": "cbf2534a35eff7ebd3266883e85be43143312241",
    "semantic_title": "chinese lexical substitution: dataset and method",
    "citation_count": 0,
    "authors": [
      "Jipeng Qiang",
      "Kang Liu",
      "Ying Li",
      "Yun Li",
      "Yi Zhu",
      "Yun-Hao Yuan",
      "Xiaocheng Hu",
      "Xiaoye Ouyang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.4": {
    "title": "Decoding the Silent Majority: Inducing Belief Augmented Social Graph with Large Language Model for Response Forecasting",
    "volume": "main",
    "abstract": "Automatic response forecasting for news media plays a crucial role in enabling content producers to efficiently predict the impact of news releases and prevent unexpected negative outcomes such as social conflict and moral injury. To effectively forecast responses, it is essential to develop measures that leverage the social dynamics and contextual information surrounding individuals, especially in cases where explicit profiles or historical actions of the users are limited (referred to as lurkers). As shown in a previous study, 97% of all tweets are produced by only the most active 25% of users. However, existing approaches have limited exploration of how to best process and utilize these important features. To address this gap, we propose a novel framework, named SocialSense, that leverages a large language model to induce a belief-centered graph on top of an existent social network, along with graph-based propagation to capture social dynamics. We hypothesize that the induced graph that bridges the gap between distant users who share similar beliefs allows the model to effectively capture the response patterns. Our method surpasses existing state-of-the-art in experimental evaluations for both zero-shot and supervised settings, demonstrating its effectiveness in response forecasting. Moreover, the analysis reveals the framework's capability to effectively handle unseen user and lurker scenarios, further highlighting its robustness and practical applicability",
    "checked": true,
    "id": "5edefb123ba3caee360ee9ff7977906dd1adba53",
    "semantic_title": "decoding the silent majority: inducing belief augmented social graph with large language model for response forecasting",
    "citation_count": 3,
    "authors": [
      "Chenkai Sun",
      "Jinning Li",
      "Yi Fung",
      "Hou Chan",
      "Tarek Abdelzaher",
      "ChengXiang Zhai",
      "Heng Ji"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.5": {
    "title": "Fine-grained Conversational Decoding via Isotropic and Proximal Search",
    "volume": "main",
    "abstract": "General-purpose text decoding approaches are usually adopted for dialogue response generation. Although the quality of the generated responses can be improved with dialogue-specific encoding methods, conversational decoding methods are still under-explored. Inspired by SimDRC that a good dialogue feature space should follow the rules of locality and isotropy, we present a fine-grained conversational decoding method, termed isotropic and proximal search (IPS). Our method is designed to generate the semantic-concentrated response, while still maintaining informativeness and discrimination against the context. Experiments show that our approach significantly outperforms existing decoding strategies in the dialogue field across both automatic and human evaluation metrics. More in-depth analyses further confirm the effectiveness of our approach",
    "checked": true,
    "id": "3eb74feac121bfc3641a42bc02d7be883f28dd58",
    "semantic_title": "fine-grained conversational decoding via isotropic and proximal search",
    "citation_count": 0,
    "authors": [
      "Yuxuan Yao",
      "Han Wu",
      "Qiling Xu",
      "Linqi Song"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.6": {
    "title": "Holistic Inter-Annotator Agreement and Corpus Coherence Estimation in a Large-scale Multilingual Annotation Campaign",
    "volume": "main",
    "abstract": "In this paper we report on the complexity of persuasion technique annotation in the context of a large multilingual annotation campaign involving 6 languages and approximately 40 annotators. We highlight the techniques that appear to be difficult for humans to annotate and elaborate on our findings on the causes of this phenomenon. We introduce Holistic IAA, a new word embedding-based annotator agreement metric and we report on various experiments using this metric and its correlation with the traditional Inter Annotator Agreement (IAA) metrics. However, given somewhat limited and loose interaction between annotators, i.e., only a few annotators annotate the same document subsets, we try to devise a way to assess the coherence of the entire dataset and strive to find a good proxy for IAA between annotators tasked to annotate different documents and in different languages, for which classical IAA metrics can not be applied",
    "checked": true,
    "id": "a21def33d328e5c2b623f551abf7fcbe6c216dcd",
    "semantic_title": "holistic inter-annotator agreement and corpus coherence estimation in a large-scale multilingual annotation campaign",
    "citation_count": 0,
    "authors": [
      "Nicolas Stefanovitch",
      "Jakub Piskorski"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.7": {
    "title": "PHD: Pixel-Based Language Modeling of Historical Documents",
    "volume": "main",
    "abstract": "The digitisation of historical documents has provided historians with unprecedented research opportunities. Yet, the conventional approach to analysing historical documents involves converting them from images to text using OCR, a process that overlooks the potential benefits of treating them as images and introduces high levels of noise. To bridge this gap, we take advantage of recent advancements in pixel-based language models trained to reconstruct masked patches of pixels instead of predicting token distributions. Due to the scarcity of real historical scans, we propose a novel method for generating synthetic scans to resemble real historical documents. We then pre-train our model, PHD, on a combination of synthetic scans and real historical newspapers from the 1700-1900 period. Through our experiments, we demonstrate that PHD exhibits high proficiency in reconstructing masked image patches and provide evidence of our model's noteworthy language understanding capabilities. Notably, we successfully apply our model to a historical QA task, highlighting its usefulness in this domain",
    "checked": true,
    "id": "6fdbf4f9ea26068a67de5c16ca0497c317916189",
    "semantic_title": "phd: pixel-based language modeling of historical documents",
    "citation_count": 0,
    "authors": [
      "Nadav Borenstein",
      "Phillip Rust",
      "Desmond Elliott",
      "Isabelle Augenstein"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.8": {
    "title": "Primacy Effect of ChatGPT",
    "volume": "main",
    "abstract": "Instruction-tuned large language models (LLMs), such as ChatGPT, have led to promising zero-shot performance in discriminative natural language understanding (NLU) tasks. This involves querying the LLM using a prompt containing the question, and the candidate labels to choose from. The question-answering capabilities of ChatGPT arise from its pre-training on large amounts of human-written text, as well as its subsequent fine-tuning on human preferences, which motivates us to ask: Does ChatGPT also inherit humans' cognitive biases? In this paper, we study the primacy effect of ChatGPT: the tendency of selecting the labels at earlier positions as the answer. We have two main findings: i) ChatGPT's decision is sensitive to the order of labels in the prompt; ii) ChatGPT has a clearly higher chance to select the labels at earlier positions as the answer. We hope that our experiments and analyses provide additional insights into building more reliable ChatGPT-based solutions. We release the source code at https://github.com/wangywUST/PrimacyEffectGPT",
    "checked": true,
    "id": "7c3c9f90e3acc5a0e780b121456a45df8ebed1a0",
    "semantic_title": "primacy effect of chatgpt",
    "citation_count": 0,
    "authors": [
      "Yiwei Wang",
      "Yujun Cai",
      "Muhao Chen",
      "Yuxuan Liang",
      "Bryan Hooi"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.9": {
    "title": "Evaluating the Rationale Understanding of Critical Reasoning in Logical Reading Comprehension",
    "volume": "main",
    "abstract": "To precisely evaluate a language model's capability for logical reading comprehension, we present a dataset for testing the understanding of the rationale behind critical reasoning. For questions taken from an existing multiple-choice logical reading comprehension dataset, we crowdsource rationale texts that explain why we should select or eliminate answer options, resulting in 3,003 multiple-choice subquestions that are associated with 943 main questions. Experiments on our dataset show that recent large language models (e.g., InstructGPT) struggle to answer the subquestions even if they are able to answer the main questions correctly. We find that the models perform particularly poorly in answering subquestions written for the incorrect options of the main questions, implying that the models have a limited capability for explaining why incorrect alternatives should be eliminated. These results suggest that our dataset encourages further investigation into the critical reasoning ability of language models while focusing on the elimination process of relevant alternatives",
    "checked": true,
    "id": "467a355a564dfc8846b7c4979fc531939343768e",
    "semantic_title": "evaluating the rationale understanding of critical reasoning in logical reading comprehension",
    "citation_count": 0,
    "authors": [
      "Akira Kawabata",
      "Saku Sugawara"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.10": {
    "title": "Evaluating and Modeling Attribution for Cross-Lingual Question Answering",
    "volume": "main",
    "abstract": "Trustworthy answer content is abundant in many high-resource languages and is instantly accessible through question answering systems — yet this content can be hard to access for those that do not speak these languages. The leap forward in cross-lingual modeling quality offered by generative language models offers much promise, yet their raw generations often fall short in factuality. To improve trustworthiness in these systems, a promising direction is to attribute the answer to a retrieved source, possibly in a content-rich language different from the query. Our work is the first to study attribution for cross-lingual question answering. First, we collect data in 5 languages to assess the attribution level of a state-of-the-art cross-lingual QA system. To our surprise, we find that a substantial portion of the answers is not attributable to any retrieved passages (up to 50% of answers exactly matching a gold reference) despite the system being able to attend directly to the retrieved text. Second, to address this poor attribution level, we experiment with a wide range of attribution detection techniques. We find that Natural Language Inference models and PaLM 2 fine-tuned on a very small amount of attribution data can accurately detect attribution. With these models, we improve the attribution level of a cross-lingual QA system. Overall, we show that current academic generative cross-lingual QA systems have substantial shortcomings in attribution and we build tooling to mitigate these issues",
    "checked": true,
    "id": "bb45c524eaed36604599a07ade72b1f641d0ddee",
    "semantic_title": "evaluating and modeling attribution for cross-lingual question answering",
    "citation_count": 4,
    "authors": [
      "Benjamin Muller",
      "John Wieting",
      "Jonathan Clark",
      "Tom Kwiatkowski",
      "Sebastian Ruder",
      "Livio Soares",
      "Roee Aharoni",
      "Jonathan Herzig",
      "Xinyi Wang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.11": {
    "title": "Better Quality Pre-training Data and T5 Models for African Languages",
    "volume": "main",
    "abstract": "In this study, we highlight the importance of enhancing the quality of pretraining data in multilingual language models. Existing web crawls have demonstrated quality issues, particularly in the context of low-resource languages. Consequently, we introduce a new multilingual pretraining corpus for 16 African languages, designed by carefully auditing existing pretraining corpora to understand and rectify prevalent quality issues. To compile this dataset, we undertake a rigorous examination of current data sources for thirteen languages within one of the most extensive multilingual web crawls, mC4, and extract cleaner data through meticulous auditing and improved web crawling strategies. Subsequently, we pretrain a new T5-based model on this dataset and evaluate its performance on multiple downstream tasks. Our model demonstrates better downstream effectiveness over existing pretrained models across four NLP tasks, underscoring the critical role data quality plays in pretraining language models in low-resource scenarios. Specifically, on cross-lingual QA evaluation, our new model is more than twice as effective as multilingual T5. All code, data and models are publicly available at https://github.com/castorini/AfriTeVa-keji",
    "checked": true,
    "id": "8a930572177545e7394ba5cd03e9342142da564e",
    "semantic_title": "better quality pre-training data and t5 models for african languages",
    "citation_count": 0,
    "authors": [
      "Akintunde Oladipo",
      "Mofetoluwa Adeyemi",
      "Orevaoghene Ahia",
      "Abraham Owodunni",
      "Odunayo Ogundepo",
      "David Adelani",
      "Jimmy Lin"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.12": {
    "title": "Sparse Universal Transformer",
    "volume": "main",
    "abstract": "The Universal Transformer (UT) is a variant of the Transformer that shares parameters across its layers and is Turing-complete under certain assumptions. Empirical evidence also shows that UTs have better compositional generalization than Vanilla Transformers (VTs) in formal language tasks. The parameter-sharing also affords it better parameter efficiency than VTs. Despite its many advantages, most state-of-the-art NLP systems use VTs as their backbone model instead of UTs. This is mainly because scaling UT parameters is more compute and memory intensive than scaling up a VT. This paper proposes the Sparse Universal Transformer (SUT), which leverages Sparse Mixture of Experts (SMoE) to reduce UT's computation complexity while retaining its parameter efficiency and generalization ability. Experiments show that SUT combines the best of both worlds, achieving strong generalization results on formal language tasks (Logical inference and CFQ) and impressive parameter and computation efficiency on standard natural language benchmarks like WMT'14",
    "checked": true,
    "id": "bcd84a2b8f9ae40a908f375425f113c82f8dd739",
    "semantic_title": "sparse universal transformer",
    "citation_count": 0,
    "authors": [
      "Shawn Tan",
      "Yikang Shen",
      "Zhenfang Chen",
      "Aaron Courville",
      "Chuang Gan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.13": {
    "title": "Theory of Mind for Multi-Agent Collaboration via Large Language Models",
    "volume": "main",
    "abstract": "While Large Language Models (LLMs) have demonstrated impressive accomplishments in both reasoning and planning, their abilities in multi-agent collaborations remains largely unexplored. This study evaluates LLM-based agents in a multi-agent cooperative text game with Theory of Mind (ToM) inference tasks, comparing their performance with Multi-Agent Reinforcement Learning (MARL) and planning-based baselines. We observed evidence of emergent collaborative behaviors and high-order Theory of Mind capabilities among LLM-based agents. Our results reveal limitations in LLM-based agents' planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state. We explore the use of explicit belief state representations to mitigate these issues, finding that it enhances task performance and the accuracy of ToM inferences for LLM-based agents",
    "checked": true,
    "id": "e17c58d7a48b6b811df023484161a3b9c03e0d6b",
    "semantic_title": "theory of mind for multi-agent collaboration via large language models",
    "citation_count": 1,
    "authors": [
      "Huao Li",
      "Yu Chong",
      "Simon Stepputtis",
      "Joseph Campbell",
      "Dana Hughes",
      "Charles Lewis",
      "Katia Sycara"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.14": {
    "title": "Establishing Trustworthiness: Rethinking Tasks and Model Evaluation",
    "volume": "main",
    "abstract": "Language understanding is a multi-faceted cognitive capability, which the Natural Language Processing (NLP) community has striven to model computationally for decades. Traditionally, facets of linguistic intelligence have been compartmentalized into tasks with specialized model architectures and corresponding evaluation protocols. With the advent of large language models (LLMs) the community has witnessed a dramatic shift towards general purpose, task-agnostic approaches powered by generative models. As a consequence, the traditional compartmentalized notion of language tasks is breaking down, followed by an increasing challenge for evaluation and analysis. At the same time, LLMs are being deployed in more real-world scenarios, including previously unforeseen zero-shot setups, increasing the need for trustworthy and reliable systems. Therefore, we argue that it is time to rethink what constitutes tasks and model evaluation in NLP, and pursue a more holistic view on language, placing trustworthiness at the center. Towards this goal, we review existing compartmentalized approaches for understanding the origins of a model's functional capacity, and provide recommendations for more multi-faceted evaluation protocols",
    "checked": true,
    "id": "a32a999377d3eb14ac7652877f002f5c548d3134",
    "semantic_title": "establishing trustworthiness: rethinking tasks and model evaluation",
    "citation_count": 0,
    "authors": [
      "Robert Litschko",
      "Max Müller-Eberstein",
      "Rob van der Goot",
      "Leon Weber-Genzel",
      "Barbara Plank"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.15": {
    "title": "Let's Think Frame by Frame with VIP: A Video Infilling and Prediction Dataset for Evaluating Video Chain-of-Thought",
    "volume": "main",
    "abstract": "Despite exciting recent results showing vision-language systems' capacity to reason about images using natural language, their capacity for video reasoning remains underexplored. We motivate framing video reasoning as the sequential understanding of a small number of keyframes, thereby leveraging the power and robustness of vision-language while alleviating the computational complexities of processing videos. To evaluate this novel application, we introduce VIP, an inference-time challenge dataset designed to explore models' reasoning capabilities through video chain-of-thought. Inspired by visually descriptive scene plays, we propose two formats for keyframe description: unstructured dense captions and structured scene descriptions that identify the focus, action, mood, objects, and setting (FAMOuS) of the keyframe. To evaluate video reasoning, we propose two tasks: Video Infilling and Video Prediction, which test abilities to generate multiple intermediate keyframes and predict future keyframes, respectively. We benchmark GPT-4, GPT-3, and VICUNA on VIP, demonstrate the performance gap in these complex video reasoning tasks, and encourage future work to prioritize language models for efficient and generalized video reasoning",
    "checked": true,
    "id": "59672d598346581a08b9bb24ab007db6d0d3ab49",
    "semantic_title": "let's think frame by frame with vip: a video infilling and prediction dataset for evaluating video chain-of-thought",
    "citation_count": 0,
    "authors": [
      "Vaishnavi Himakunthala",
      "Andy Ouyang",
      "Daniel Rose",
      "Ryan He",
      "Alex Mei",
      "Yujie Lu",
      "Chinmay Sonar",
      "Michael Saxon",
      "William Wang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.16": {
    "title": "GPTAraEval: A Comprehensive Evaluation of ChatGPT on Arabic NLP",
    "volume": "main",
    "abstract": "ChatGPT's emergence heralds a transformative phase in NLP, particularly demonstrated through its excellent performance on many English benchmarks. However, the model's efficacy across diverse linguistic contexts remains largely uncharted territory. This work aims to bridge this knowledge gap, with a primary focus on assessing ChatGPT's capabilities on Arabic languages and dialectal varieties. Our comprehensive study conducts a large-scale automated and human evaluation of ChatGPT, encompassing 44 distinct language understanding and generation tasks on over 60 different datasets. To our knowledge, this marks the first extensive performance analysis of ChatGPT's deployment in Arabic NLP. Our findings indicate that, despite its remarkable performance in English, ChatGPT is consistently surpassed by smaller models that have undergone finetuning on Arabic. We further undertake a meticulous comparison of ChatGPT and GPT-4's Modern Standard Arabic (MSA) and Dialectal Arabic (DA), unveiling the relative shortcomings of both models in handling Arabic dialects compared to MSA. Although we further explore and confirm the utility of employing GPT-4 as a potential alternative for human evaluation, our work adds to a growing body of research underscoring the limitations of ChatGPT",
    "checked": true,
    "id": "7782055e33ee248ef47b1415624c2efb7a7e8410",
    "semantic_title": "gptaraeval: a comprehensive evaluation of chatgpt on arabic nlp",
    "citation_count": 9,
    "authors": [
      "Md Tawkat Islam Khondaker",
      "Abdul Waheed",
      "El Moatez Billah Nagoudi",
      "Muhammad Abdul-Mageed"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.17": {
    "title": "Dual-Channel Span for Aspect Sentiment Triplet Extraction",
    "volume": "main",
    "abstract": "Aspect Sentiment Triplet Extraction (ASTE) is one of the compound tasks of fine-grained aspect-based sentiment analysis (ABSA), aiming at extracting the triplets of aspect terms, corresponding opinion terms and the associated sentiment orientation. Recent efforts in exploiting span-level semantic interaction shown superior performance on ASTE task. However, most of the existing span-based approaches suffer from enumerating all possible spans, since it can introduce too much noise in sentiment triplet extraction. To ease this burden, we propose a dual-channel span generation method to coherently constrain the search space of span candidates. Specifically, we leverage the syntactic relations among aspect/opinion terms and the associated part-of-speech characteristics in those terms to generate span candidates, which reduces span enumeration by nearly half. Besides, feature representations are learned from syntactic and part-of-speech correlation among terms, which renders span representation fruitful linguistic information. Extensive experiments on two versions of public datasets demonstrate both the effectiveness of our design and the superiority on ASTE/ATE/OTE tasks",
    "checked": true,
    "id": "90a5a545f4f840e66c5ca8e33bbcb9fc17c02afa",
    "semantic_title": "dual-channel span for aspect sentiment triplet extraction",
    "citation_count": 0,
    "authors": [
      "Pan Li",
      "Ping Li",
      "Kai Zhang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.18": {
    "title": "Cultural Concept Adaptation on Multimodal Reasoning",
    "volume": "main",
    "abstract": "Developing cultural adaptation methods is important, which can improve the model performance on the low-resource ones and provide more equitable opportunities for everyone to benefit from advanced technology. Past methods primarily focused on multilingual and multimodal capabilities, and the improvement of multicultural competence is still an unexplored problem. This is largely due to the difficulty of data scarcity and expensive annotation. In this paper, we navigate this uncharted territory by leveraging high-resource cultures to facilitate comprehension of low-resource ones. We first introduce an annotation-free method for cultural-concept adaptation and construct a concept mapping set. To facilitate the model's comprehension of cultural-concept mappings, we propose a new multimodal data augmentation called CultureMixup. This approach employs a three-tier code-switching strategy on textual sentences. Additionally, it uses a cultural concept-based mixup method for the images. This combination effectively generates new data instances across culture, phrase, word, and image levels. For visually grounded reasoning across languages and cultures, experimental results on five languages show that our method consistently improves performance for four existing multilingual and multimodal models on both zero-shot and few-shot settings",
    "checked": true,
    "id": "f0290a86171cfe092368fce736880bba056c82c0",
    "semantic_title": "cultural concept adaptation on multimodal reasoning",
    "citation_count": 0,
    "authors": [
      "Zhi Li",
      "Yin Zhang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.19": {
    "title": "Understanding Compositional Data Augmentation in Typologically Diverse Morphological Inflection",
    "volume": "main",
    "abstract": "Data augmentation techniques are widely used in low-resource automatic morphological inflection to address the issue of data sparsity. However, the full implications of these techniques remain poorly understood. In this study, we aim to shed light on the theoretical aspects of the data augmentation strategy StemCorrupt, a method that generates synthetic examples by randomly substituting stem characters in existing gold standard training examples. Our analysis uncovers that StemCorrupt brings about fundamental changes in the underlying data distribution, revealing inherent compositional concatenative structure. To complement our theoretical analysis, we investigate the data-efficiency of StemCorrupt. Through evaluation across a diverse set of seven typologically distinct languages, we demonstrate that selecting a subset of datapoints with both high diversity and high predictive uncertainty significantly enhances the data-efficiency of compared to competitive baselines. Furthermore, we explore the impact of typological features on the choice of augmentation strategy and find that languages incorporating non-concatenativity, such as morphonological alternations, derive less benefit from synthetic examples with high predictive uncertainty. We attribute this effect to phonotactic violations induced by StemCorrupt, emphasizing the need for further research to ensure optimal performance across the entire spectrum of natural language morphology",
    "checked": true,
    "id": "db854ffffba91d48c684c5ff86cc594de83395a2",
    "semantic_title": "understanding compositional data augmentation in typologically diverse morphological inflection",
    "citation_count": 0,
    "authors": [
      "Farhan Samir",
      "Miikka Silfverberg"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.20": {
    "title": "Evaluating Object Hallucination in Large Vision-Language Models",
    "volume": "main",
    "abstract": "Inspired by the superior language abilities of large language models (LLM), large vision-language models (LVLM) have been recently proposed by integrating powerful LLMs for improving the performance on complex multimodal tasks. Despite the promising progress on LVLMs, we find that they suffer from object hallucinations, i.e., they tend to generate objects inconsistent with the target images in the descriptions. To investigate it, this work presents the first systematic study on object hallucination of LVLMs. We conduct the evaluation experiments on several representative LVLMs, and show that they mostly suffer from severe object hallucination issues. We further discuss that the visual instructions may influence the hallucination, and find that: objects that frequently appear in the visual instructions or co-occur with the image objects are obviously prone to be hallucinated by LVLMs. Besides, we further design a polling-based query method called POPE for better evaluation of object hallucination. Experiment results show that our POPE can evaluate object hallucination in a more stable and flexible way",
    "checked": true,
    "id": "206400aba5f12f734cdd2e4ab48ef6014ea60773",
    "semantic_title": "evaluating object hallucination in large vision-language models",
    "citation_count": 79,
    "authors": [
      "Yifan Li",
      "Yifan Du",
      "Kun Zhou",
      "Jinpeng Wang",
      "Xin Zhao",
      "Ji-Rong Wen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.21": {
    "title": "Event Ontology Completion with Hierarchical Structure Evolution Networks",
    "volume": "main",
    "abstract": "Traditional event detection methods require predefined event schemas. However, manually defining event schemas is expensive and the coverage of schemas is limited. To this end, some works study the event type induction (ETI) task, which discovers new event types via clustering. However, the setting of ETI suffers from two limitations: event types are not linked into the existing hierarchy and have no semantic names. In this paper, we propose a new research task named Event Ontology Completion (EOC), which aims to simultaneously achieve event clustering, hierarchy expansion and type naming. Furthermore, we develop a Hierarchical Structure Evolution Network (HalTon) for this new task. Specifically, we first devise a Neighborhood Contrastive Clustering module to cluster unlabeled event instances. Then, we propose a Hierarchy-Aware Linking module to incorporate the hierarchical information for event expansion. Finally, we generate meaningful names for new types via an In-Context Learning-based Naming module. Extensive experiments indicate that our method achieves the best performance, outperforming the baselines by 8.23%, 8.79% and 8.10% of ARI score on three datasets",
    "checked": true,
    "id": "b75d8b54525397044cf92fc856a3da1ddb975af9",
    "semantic_title": "event ontology completion with hierarchical structure evolution networks",
    "citation_count": 0,
    "authors": [
      "Pengfei Cao",
      "Yupu Hao",
      "Yubo Chen",
      "Kang Liu",
      "Jiexin Xu",
      "Huaijun Li",
      "Xiaojian Jiang",
      "Jun Zhao"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.22": {
    "title": "Parameter-efficient Tuning for Large Language Model without Calculating Its Gradients",
    "volume": "main",
    "abstract": "Fine-tuning all parameters of large language models (LLMs) requires significant computational resources and is time-consuming. Recent parameter-efficient tuning methods such as Adapter tuning, Prefix tuning, and LoRA allow for updating a small subset of parameters in large language models. However, they can only save approximately 30% of the training memory requirements, due to the problem that gradient computation and backpropagation are still necessary for these methods. This paper proposes a novel parameter-efficient tuning method for LLMs without calculating their gradients. Leveraging the discernible similarities between the parameter-efficient modules of the same task learned by both large and small language models, we put forward a strategy for transferring the parameter-efficient modules, originally derived from small language models to much larger ones. To ensure a smooth and effective adaptation process, we further introduce a Bridge model to guarantee dimensional consistency while also stimulating a dynamic interaction between the models. We demonstrate the effectiveness of our method using the T5 and GPT-2 series of language models on the SuperGLUE benchmark. Our method achieves comparable performance to both fine-tuning and parameter-efficient tuning on large language models without needing gradient-based optimization. Additionally, our method achieves up to 5.7x memory reduction compared to parameter-efficient tuning",
    "checked": true,
    "id": "4540d2f5be6563ea91a56aef36b8354aac197303",
    "semantic_title": "parameter-efficient tuning for large language model without calculating its gradients",
    "citation_count": 0,
    "authors": [
      "Feihu Jin",
      "Jiajun Zhang",
      "Chengqing Zong"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.23": {
    "title": "Discourse Structures Guided Fine-grained Propaganda Identification",
    "volume": "main",
    "abstract": "Propaganda is a form of deceptive narratives that instigate or mislead the public, usually with a political purpose. In this paper, we aim to identify propaganda in political news at two fine-grained levels: sentence-level and token-level. We observe that propaganda content is more likely to be embedded in sentences that attribute causality or assert contrast to nearby sentences, as well as seen in opinionated evaluation, speculation and discussions of future expectation. Hence, we propose to incorporate both local and global discourse structures for propaganda discovery and construct two teacher models for identifying PDTB-style discourse relations between nearby sentences and common discourse roles of sentences in a news article respectively. We further devise two methods to incorporate the two types of discourse structures for propaganda identification by either using teacher predicted probabilities as additional features or soliciting guidance in a knowledge distillation framework. Experiments on the benchmark dataset demonstrate that leveraging guidance from discourse structures can significantly improve both precision and recall of propaganda content identification",
    "checked": true,
    "id": "c851f71f1470cd8a5d8189997c9be1b828ab6f1f",
    "semantic_title": "discourse structures guided fine-grained propaganda identification",
    "citation_count": 0,
    "authors": [
      "Yuanyuan Lei",
      "Ruihong Huang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.24": {
    "title": "CompoundPiece: Evaluating and Improving Decompounding Performance of Language Models",
    "volume": "main",
    "abstract": "While many languages possess processes of joining two or more words to create compound words, previous studies have been typically limited only to languages with excessively productive compound formation (e.g., German, Dutch) and there is no public dataset containing compound and non-compound words across a large number of languages. In this work, we systematically study decompounding, the task of splitting compound words into their constituents, at a wide scale. We first address the data gap by introducing a dataset of 255k compound and non-compound words across 56 diverse languages obtained from Wiktionary. We then use this dataset to evaluate an array of Large Language Models (LLMs) on the decompounding task. We find that LLMs perform poorly, especially on words which are tokenized unfavorably by subword tokenization. We thus introduce a novel methodology to train dedicated models for decompounding. The proposed two-stage procedure relies on a fully self-supervised objective in the first stage, while the second, supervised learning stage optionally fine-tunes the model on the annotated Wiktionary data. Our self-supervised models outperform the prior best unsupervised decompounding models by 13.9% accuracy on average. Our fine-tuned models outperform all prior (language-specific) decompounding tools. Furthermore, we use our models to leverage decompounding during the creation of a subword tokenizer, which we refer to as CompoundPiece. CompoundPiece tokenizes compound words more favorably on average, leading to improved performance on decompounding over an otherwise equivalent model using SentencePiece tokenization",
    "checked": true,
    "id": "b3cff6abe401a244c21d4706b0931e48acaeeb4e",
    "semantic_title": "compoundpiece: evaluating and improving decompounding performance of language models",
    "citation_count": 1,
    "authors": [
      "Benjamin Minixhofer",
      "Jonas Pfeiffer",
      "Ivan Vulić"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.25": {
    "title": "Improving Image Captioning via Predicting Structured Concepts",
    "volume": "main",
    "abstract": "Having the difficulty of solving the semantic gap between images and texts for the image captioning task, conventional studies in this area paid some attention to treating semantic concepts as a bridge between the two modalities and improved captioning performance accordingly. Although promising results on concept prediction were obtained, the aforementioned studies normally ignore the relationship among concepts, which relies on not only objects in the image, but also word dependencies in the text, so that offers a considerable potential for improving the process of generating good descriptions. In this paper, we propose a structured concept predictor (SCP) to predict concepts and their structures, then we integrate them into captioning, so that enhance the contribution of visual signals in this task via concepts and further use their relations to distinguish cross-modal semantics for better description generation. Particularly, we design weighted graph convolutional networks (W-GCN) to depict concept relations driven by word dependencies, and then learns differentiated contributions from these concepts for following decoding process. Therefore, our approach captures potential relations among concepts and discriminatively learns different concepts, so that effectively facilitates image captioning with inherited information across modalities. Extensive experiments and their results demonstrate the effectiveness of our approach as well as each proposed module in this work",
    "checked": true,
    "id": "03ca692027883a11315d59c9a422e67879548c41",
    "semantic_title": "improving image captioning via predicting structured concepts",
    "citation_count": 0,
    "authors": [
      "Ting Wang",
      "Weidong Chen",
      "Yuanhe Tian",
      "Yan Song",
      "Zhendong Mao"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.26": {
    "title": "GATITOS: Using a New Multilingual Lexicon for Low-resource Machine Translation",
    "volume": "main",
    "abstract": "Modern machine translation models and language models are able to translate without having been trained on parallel data, greatly expanding the set of languages that they can serve. However, these models still struggle in a variety of predictable ways, a problem that cannot be overcome without at least some trusted bilingual data. This work expands on a cheap and abundant resource to combat this problem: bilingual lexica. We test the efficacy of bilingual lexica in a real-world set-up, on 200-language translation models trained on web-crawled text. We present several findings: (1) using lexical data augmentation, we demonstrate sizable performance gains for unsupervised translation; (2) we compare several families of data augmentation, demonstrating that they yield similar improvements, and can be combined for even greater improvements; (3) we demonstrate the importance of carefully curated lexica over larger, noisier ones, especially with larger models; and (4) we compare the efficacy of multilingual lexicon data versus human-translated parallel data. Based on results from (3), we develop and open-source GATITOS, a high-quality, curated dataset in 168 tail languages, one of the first human-translated resources to cover many of these languages",
    "checked": true,
    "id": "dc6f2dc99181026a8d5a07fd403393d307d5fd8d",
    "semantic_title": "gatitos: using a new multilingual lexicon for low-resource machine translation",
    "citation_count": 0,
    "authors": [
      "Alexander Jones",
      "Isaac Caswell",
      "Orhan Firat",
      "Ishank Saxena"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.27": {
    "title": "Continually Improving Extractive QA via Human Feedback",
    "volume": "main",
    "abstract": "We study continually improving an extractive question answering (QA) system via human user feedback. We design and deploy an iterative approach, where information-seeking users ask questions, receive model-predicted answers, and provide feedback. We conduct experiments involving thousands of user interactions under diverse setups to broaden the understanding of learning from feedback over time. Our experiments show effective improvement from user feedback of extractive QA models over time across different data regimes, including significant potential for domain adaptation",
    "checked": true,
    "id": "627964b3dc7211f81902afeaa79d9357b7b2440c",
    "semantic_title": "continually improving extractive qa via human feedback",
    "citation_count": 2,
    "authors": [
      "Ge Gao",
      "Hung-Ting Chen",
      "Yoav Artzi",
      "Eunsol Choi"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.28": {
    "title": "Using Interpretation Methods for Model Enhancement",
    "volume": "main",
    "abstract": "In the age of neural natural language processing, there are plenty of works trying to derive interpretations of neural models. Intuitively, when gold rationales exist during training, one can additionally train the model to match its interpretation with the rationales. However, this intuitive idea has not been fully explored. In this paper, we propose a framework of utilizing interpretation methods and gold rationales to enhance models. Our framework is very general in the sense that it can incorporate various interpretation methods. Previously proposed gradient-based methods can be shown as an instance of our framework. We also propose two novel instances utilizing two other types of interpretation methods, erasure/replace-based and extractor-based methods, for model enhancement. We conduct comprehensive experiments on a variety of tasks. Experimental results show that our framework is effective especially in low-resource settings in enhancing models with various interpretation methods, and our two newly-proposed methods outperform gradient-based methods in most settings. Code is available at https://github.com/Chord-Chen-30/UIMER",
    "checked": true,
    "id": "5b72c354d482964ef52a2c8832c82619ff10cb2e",
    "semantic_title": "using interpretation methods for model enhancement",
    "citation_count": 0,
    "authors": [
      "Zhuo Chen",
      "Chengyue Jiang",
      "Kewei Tu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.29": {
    "title": "An Expression Tree Decoding Strategy for Mathematical Equation Generation",
    "volume": "main",
    "abstract": "Generating mathematical equations from natural language requires an accurate understanding of the relations among math expressions. Existing approaches can be broadly categorized into token-level and expression-level generation. The former treats equations as a mathematical language, sequentially generating math tokens. Expression-level methods generate each expression one by one. However, each expression represents a solving step, and there naturally exist parallel or dependent relations between these steps, which are ignored by current sequential methods. Therefore, we integrate tree structure into the expression-level generation and advocate an expression tree decoding strategy. To generate a tree with expression as its node, we employ a layer-wise parallel decoding strategy: we decode multiple independent expressions (leaf nodes) in parallel at each layer and repeat parallel decoding layer by layer to sequentially generate these parent node expressions that depend on others. Besides, a bipartite matching algorithm is adopted to align multiple predictions with annotations for each layer. Experiments show our method outperforms other baselines, especially for these equations with complex structures",
    "checked": true,
    "id": "2179a13e78195fa78be5ccca2b5dcf9fad783ffc",
    "semantic_title": "an expression tree decoding strategy for mathematical equation generation",
    "citation_count": 0,
    "authors": [
      "Wenqi Zhang",
      "Yongliang Shen",
      "Qingpeng Nong",
      "Zeqi Tan",
      "Yanna Ma",
      "Weiming Lu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.30": {
    "title": "Bootstrapping Small & High Performance Language Models with Unmasking-Removal Training Policy",
    "volume": "main",
    "abstract": "BabyBERTa, a language model trained on small-scale child-directed speech while none of the words are unmasked during training, has been shown to achieve a level of grammaticality comparable to that of RoBERTa-base, which is trained on 6,000 times more words and 15 times more parameters. Relying on this promising result, we explore in this paper the performance of BabyBERTa-based models in downstream tasks, focusing on Semantic Role Labeling (SRL) and two Extractive Question Answering tasks, with the aim of building more efficient systems that rely on less data and smaller models. We investigate the influence of these models both alone and as a starting point to larger pre-trained models, separately examining the contribution of the pre-training data, the vocabulary, and the masking policy on the downstream task performance. Our results show that BabyBERTa trained with unmasking-removal policy is a much stronger starting point for downstream tasks compared to the use of RoBERTa masking policy when 10M words are used for training and that this tendency persists, although to a lesser extent, when adding more training data",
    "checked": true,
    "id": "da443d54407db4a9967d8712513e10e14f913ac8",
    "semantic_title": "bootstrapping small & high performance language models with unmasking-removal training policy",
    "citation_count": 0,
    "authors": [
      "Yahan Yang",
      "Elior Sulem",
      "Insup Lee",
      "Dan Roth"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.31": {
    "title": "Diversity Enhanced Narrative Question Generation for Storybooks",
    "volume": "main",
    "abstract": "Question generation (QG) from a given context can enhance comprehension, engagement, assessment, and overall efficacy in learning or conversational environments. Despite recent advancements in QG, the challenge of enhancing or measuring the diversity of generated questions often remains unaddressed. In this paper, we introduce a multi-question generation model (mQG), which is capable of generating multiple, diverse, and answerable questions by focusing on context and questions. To validate the answerability of the generated questions, we employ a SQuAD 2.0 fine-tuned question answering model, classifying the questions as answerable or not. We train and evaluate mQG on the FairytaleQA dataset, a well-structured QA dataset based on storybooks, with narrative questions. We further apply a zero-shot adaptation on the TellMeWhy and SQuAD1.1 datasets. mQG shows promising results across various evaluation metrics, among strong baselines",
    "checked": true,
    "id": "643e2d0e05f7369fd75c865a28932fe7eaec38a2",
    "semantic_title": "diversity enhanced narrative question generation for storybooks",
    "citation_count": 0,
    "authors": [
      "Hokeun Yoon",
      "JinYeong Bak"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.32": {
    "title": "Debiasing Made State-of-the-art: Revisiting the Simple Seed-based Weak Supervision for Text Classification",
    "volume": "main",
    "abstract": "Recent advances in weakly supervised text classification mostly focus on designing sophisticated methods to turn high-level human heuristics into quality pseudo-labels. In this paper, we revisit the seed matching-based method, which is arguably the simplest way to generate pseudo-labels, and show that its power was greatly underestimated. We show that the limited performance of seed matching is largely due to the label bias injected by the simple seed-match rule, which prevents the classifier from learning reliable confidence for selecting high-quality pseudo-labels. Interestingly, simply deleting the seed words present in the matched input texts can mitigate the label bias and help learn better confidence. Subsequently, the performance achieved by seed matching can be improved significantly, making it on par with or even better than the state-of-the-art. Furthermore, to handle the case when the seed words are not made known, we propose to simply delete the word tokens in the input text randomly with a high deletion ratio. Remarkably, seed matching equipped with this random deletion method can often achieve even better performance than that with seed deletion",
    "checked": true,
    "id": "12b540c717f6a4aa6ee7008b30bc2a1f65af9d8a",
    "semantic_title": "debiasing made state-of-the-art: revisiting the simple seed-based weak supervision for text classification",
    "citation_count": 0,
    "authors": [
      "Chengyu Dong",
      "Zihan Wang",
      "Jingbo Shang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.33": {
    "title": "How to Enhance Causal Discrimination of Utterances: A Case on Affective Reasoning",
    "volume": "main",
    "abstract": "Our investigation into the Affective Reasoning in Conversation (ARC) task highlights the challenge of causal discrimination. Almost all existing models, including large language models (LLMs), excel at capturing semantic correlations within utterance embeddings but fall short in determining the specific causal relationships. To overcome this limitation, we propose the incorporation of i.i.d. noise terms into the conversation process, thereby constructing a structural causal model (SCM). It explores how distinct causal relationships of fitted embeddings can be discerned through independent conditions. To facilitate the implementation of deep learning, we introduce the cogn frameworks to handle unstructured conversation data, and employ an autoencoder architecture to regard the unobservable noise as learnable \"implicit causes.\" Moreover, we curate a synthetic dataset that includes i.i.d. noise. Through comprehensive experiments, we validate the effectiveness and interpretability of our approach. Our code is available in https://github.com/Zodiark-ch/mater-of-our-EMNLP2023-paper",
    "checked": true,
    "id": "7b569454e5e866af863eaedf883018c5a0b168d7",
    "semantic_title": "how to enhance causal discrimination of utterances: a case on affective reasoning",
    "citation_count": 3,
    "authors": [
      "Hang Chen",
      "Xinyu Yang",
      "Jing Luo",
      "Wenjing Zhu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.34": {
    "title": "Compressing and Debiasing Vision-Language Pre-Trained Models for Visual Question Answering",
    "volume": "main",
    "abstract": "Despite the excellent performance of vision-language pre-trained models (VLPs) on conventional VQA task, they still suffer from two problems: First, VLPs tend to rely on language biases in datasets and fail to generalize to out-of-distribution (OOD) data. Second, they are inefficient in terms of memory footprint and computation. Although promising progress has been made in both problems, most existing works tackle them independently. To facilitate the application of VLP to VQA tasks, it is imperative to jointly study VLP compression and OOD robustness, which, however, has not yet been explored. This paper investigates whether a VLP can be compressed and debiased simultaneously by searching sparse and robust subnetworks. To this end, we systematically study the design of a training and compression pipeline to search the subnetworks, as well as the assignment of sparsity to different modality-specific modules. Our experiments involve 2 VLPs, 2 compression methods, 4 training methods, 2 datasets and a range of sparsity levels. Our results show that there indeed exist sparse and robust subnetworks, which are competitive with the debiased full VLP and clearly outperform the debiasing SoTAs with fewer parameters on OOD datasets VQA-CP v2 and VQA-VS. The codes can be found at https://github.com/PhoebusSi/Compress-Robust-VQA",
    "checked": true,
    "id": "574bb665fd46d59f905590f87c2821341acef57c",
    "semantic_title": "compressing and debiasing vision-language pre-trained models for visual question answering",
    "citation_count": 0,
    "authors": [
      "Qingyi Si",
      "Yuanxin Liu",
      "Zheng Lin",
      "Peng Fu",
      "Yanan Cao",
      "Weiping Wang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.35": {
    "title": "Selectively Answering Ambiguous Questions",
    "volume": "main",
    "abstract": "Trustworthy language models should abstain from answering questions when they do not know the answer. However, the answer to a question can be unknown for a variety of reasons. Prior research has focused on the case in which the question is clear and the answer is unambiguous but possibly unknown. However, the answer to a question can also be unclear due to uncertainty of the questioner's intent or context. We investigate question answering from this perspective, focusing on answering a subset of questions with a high degree of accuracy, from a set of questions in which many are inherently ambiguous. In this setting, we find that the most reliable approach to calibration involves quantifying repetition within a set of sampled model outputs, rather than the model's likelihood or self-verification as used in prior work. We find this to be the case across different types of uncertainty, varying model scales and both with or without instruction tuning. Our results suggest that sampling-based confidence scores help calibrate answers to relatively unambiguous questions, with more dramatic improvements on ambiguous questions",
    "checked": true,
    "id": "9386242a9ad5c1a15023043ef63d45e42ceac546",
    "semantic_title": "selectively answering ambiguous questions",
    "citation_count": 1,
    "authors": [
      "Jeremy Cole",
      "Michael Zhang",
      "Daniel Gillick",
      "Julian Eisenschlos",
      "Bhuwan Dhingra",
      "Jacob Eisenstein"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.36": {
    "title": "Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context Learning",
    "volume": "main",
    "abstract": "Temporal knowledge graph (TKG) forecasting benchmarks challenge models to predict future facts using knowledge of past facts. In this paper, we develop an approach to use in-context learning (ICL) with large language models (LLMs) for TKG forecasting. Our extensive evaluation compares diverse baselines, including both simple heuristics and state-of-the-art (SOTA) supervised models, against pre-trained LLMs across several popular benchmarks and experimental settings. We observe that naive LLMs perform on par with SOTA models, which employ carefully designed architectures and supervised training for the forecasting task, falling within the (-3.6%, +1.5%) Hits@1 margin relative to the median performance. To better understand the strengths of LLMs for forecasting, we explore different approaches for selecting historical facts, constructing prompts, controlling information propagation, and parsing outputs into a probability distribution. A surprising finding from our experiments is that LLM performance endures (±0.4% Hit@1) even when semantic information is removed by mapping entities/relations to arbitrary numbers, suggesting that prior semantic knowledge is unnecessary; rather, LLMs can leverage the symbolic patterns in the context to achieve such a strong performance. Our analysis also reveals that ICL enables LLMs to learn irregular patterns from the historical context, going beyond frequency and recency biases",
    "checked": true,
    "id": "c9f83c0fa1425d61c5b16aadc4492ad53e4fbda2",
    "semantic_title": "temporal knowledge graph forecasting without knowledge using in-context learning",
    "citation_count": 3,
    "authors": [
      "Dong-Ho Lee",
      "Kian Ahrabian",
      "Woojeong Jin",
      "Fred Morstatter",
      "Jay Pujara"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.37": {
    "title": "Knowledge Graph Compression Enhances Diverse Commonsense Generation",
    "volume": "main",
    "abstract": "Generating commonsense explanations requires reasoning about commonsense knowledge beyond what is explicitly mentioned in the context. Existing models use commonsense knowledge graphs such as ConceptNet to extract a subgraph of relevant knowledge pertaining to concepts in the input. However, due to the large coverage and, consequently, vast scale of ConceptNet, the extracted subgraphs may contain loosely related, redundant and irrelevant information, which can introduce noise into the model. We propose to address this by applying a differentiable graph compression algorithm that focuses on the relevant knowledge for the task. The compressed subgraphs yield considerably more diverse outputs when incorporated into models for the tasks of generating commonsense and abductive explanations. Moreover, our model achieves better quality-diversity tradeoff than a large language model with 100 times the number of parameters. Our generic approach can be applied to additional NLP tasks that can benefit from incorporating external knowledge",
    "checked": true,
    "id": "a338aa9a61ac5ffe6f2241971c9348a9ca60a448",
    "semantic_title": "knowledge graph compression enhances diverse commonsense generation",
    "citation_count": 0,
    "authors": [
      "EunJeong Hwang",
      "Veronika Thost",
      "Vered Shwartz",
      "Tengfei Ma"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.38": {
    "title": "Pragmatic Reasoning Unlocks Quantifier Semantics for Foundation Models",
    "volume": "main",
    "abstract": "Generalized quantifiers (e.g., few, most) are used to indicate the proportions predicates satisfy (for example, some apples are red). One way to interpret quantifier semantics is to explicitly bind these satisfactions with percentage scopes (e.g., 30%-40% of apples are red). This approach can be helpful for tasks like logic formalization and surface-form quantitative reasoning (Gordon and Schubert, 2010; Roy et al., 2015). However, it remains unclear if recent foundation models (Bommasani et al., 2021) possess this ability due to the absence of direct training signals. To explore this, we introduce QuRe, a crowd-sourced dataset of human-annotated generalized quantifiers in Wikipedia sentences featuring percentage-equipped predicates. We explore quantifier comprehension using PRESQUE, a framework that combines natural language inference and the Rational Speech Acts framework. Experimental results on the HVD dataset (Herbelot and Vecchi, 2015) and QuRe demonstrate PRESQUE's superiority over a literal listener baseline, showing a 20% relative improvement in F1 in predicting percentage scopes for quantifiers, even with no additional training",
    "checked": true,
    "id": "e0974461d5c60e9aac8b06a501ca393d8bfeb1c0",
    "semantic_title": "pragmatic reasoning unlocks quantifier semantics for foundation models",
    "citation_count": 0,
    "authors": [
      "Yiyuan Li",
      "Rakesh Menon",
      "Sayan Ghosh",
      "Shashank Srivastava"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.39": {
    "title": "LLM-FP4: 4-Bit Floating-Point Quantized Transformers",
    "volume": "main",
    "abstract": "We propose LLM-FP4 for quantizing both weights and activations in large language models (LLMs) down to 4-bit floating-point values, in a post-training manner. Existing post-training quantization (PTQ) solutions are primarily integer-based and struggle with bit widths below 8 bits. Compared to integer quantization, floating-point (FP) quantization is more flexible and can better handle long-tail or bell-shaped distributions, and it has emerged as a default choice in many hardware platforms. One characteristic of FP quantization is that its performance largely depends on the choice of exponent bits and clipping range. In this regard, we construct a strong FP-PTQ baseline by searching for the optimal quantization parameters. Furthermore, we observe a high inter-channel variance and low intra-channel variance pattern in activation distributions, which adds activation quantization difficulty. We recognize this pattern to be consistent across a spectrum of transformer models designed for diverse tasks such as LLMs, BERT, and Vision Transformer models. To tackle this, we propose per-channel activation quantization and show that these additional scaling factors can be reparameterized as exponential biases of weights, incurring a negligible cost. Our method, for the first time, can quantize both weights and activations in the LLaMA-13B to only 4-bit and achieves an average score of 63.1 on the common sense zero-shot reasoning tasks, which is only 5.8 lower than the full-precision model, significantly outperforming the previous state-of-the-art by 12.7 points. Code is available at: https://github.com/nbasyl/LLM-FP4",
    "checked": true,
    "id": "a8b995f0da78a79447dfb18c2337972b044f4239",
    "semantic_title": "llm-fp4: 4-bit floating-point quantized transformers",
    "citation_count": 3,
    "authors": [
      "Shih-yang Liu",
      "Zechun Liu",
      "Xijie Huang",
      "Pingcheng Dong",
      "Kwang-Ting Cheng"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.40": {
    "title": "Improving Biomedical Abstractive Summarisation with Knowledge Aggregation from Citation Papers",
    "volume": "main",
    "abstract": "Abstracts derived from biomedical literature possess distinct domain-specific characteristics, including specialised writing styles and biomedical terminologies, which necessitate a deep understanding of the related literature. As a result, existing language models struggle to generate technical summaries that are on par with those produced by biomedical experts, given the absence of domain-specific background knowledge. This paper aims to enhance the performance of language models in biomedical abstractive summarisation by aggregating knowledge from external papers cited within the source article. We propose a novel attention-based citation aggregation model that integrates domain-specific knowledge from citation papers, allowing neural networks to generate summaries by leveraging both the paper content and relevant knowledge from citation papers. Furthermore, we construct and release a large-scale biomedical summarisation dataset that serves as a foundation for our research. Extensive experiments demonstrate that our model outperforms state-of-the-art approaches and achieves substantial improvements in abstractive biomedical text summarisation",
    "checked": true,
    "id": "9c5609baff6175b0a2e436bb69e89737c4be3cf4",
    "semantic_title": "improving biomedical abstractive summarisation with knowledge aggregation from citation papers",
    "citation_count": 2,
    "authors": [
      "Chen Tang",
      "Shun Wang",
      "Tomas Goldsack",
      "Chenghua Lin"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.41": {
    "title": "Explanation Selection Using Unlabeled Data for Chain-of-Thought Prompting",
    "volume": "main",
    "abstract": "Recent work has shown how to prompt large language models with explanations to obtain strong performance on textual reasoning tasks, i.e., the chain-of-thought paradigm. However, subtly different explanations can yield widely varying downstream task accuracy. Explanations that have not been \"tuned\" for a task, such as off-the-shelf explanations written by non-experts, may lead to mediocre performance. This paper tackles the problem of how to optimize explanation-infused prompts in a blackbox fashion. We first generate sets of candidate explanations for each example in the prompt using a leave-one-out scheme, then find an effective combination of these explanations with a two-stage framework. We first evaluate explanations for each in-context example in isolation according to two proxy metrics, log likelihood and accuracy on new examples. Then, we search over combinations of explanations to find one that yields high performance against a silver-labeled development set. Across four textual reasoning tasks spanning question answering, mathematical reasoning, and natural language inference, results show that our proxy metrics correlate with ground truth accuracy and our overall method can effectively improve prompts over crowdworker annotations and naive search strategies",
    "checked": true,
    "id": "07157c3cb9010acb84237e121dacc7ecba30f04b",
    "semantic_title": "explanation selection using unlabeled data for chain-of-thought prompting",
    "citation_count": 3,
    "authors": [
      "Xi Ye",
      "Greg Durrett"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.42": {
    "title": "HalOmi: A Manually Annotated Benchmark for Multilingual Hallucination and Omission Detection in Machine Translation",
    "volume": "main",
    "abstract": "Hallucinations in machine translation are translations that contain information completely unrelated to the input. Omissions are translations that do not include some of the input information. While both cases tend to be catastrophic errors undermining user trust, annotated data with these types of pathologies is extremely scarce and is limited to a few high-resource languages. In this work, we release an annotated dataset for the hallucination and omission phenomena covering 18 translation directions with varying resource levels and scripts. Our annotation covers different levels of partial and full hallucinations as well as omissions both at the sentence and at the word level. Additionally, we revisit previous methods for hallucination and omission detection, show that conclusions made based on a single language pair largely do not hold for a large-scale evaluation, and establish new solid baselines",
    "checked": true,
    "id": "4c868db20349fdc271506a16ba7af881f1169d81",
    "semantic_title": "halomi: a manually annotated benchmark for multilingual hallucination and omission detection in machine translation",
    "citation_count": 8,
    "authors": [
      "David Dale",
      "Elena Voita",
      "Janice Lam",
      "Prangthip Hansanti",
      "Christophe Ropers",
      "Elahe Kalbassi",
      "Cynthia Gao",
      "Loic Barrault",
      "Marta Costa-jussà"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.43": {
    "title": "Gradient-based Gradual Pruning for Language-Specific Multilingual Neural Machine Translation",
    "volume": "main",
    "abstract": "Multilingual neural machine translation (MNMT) offers the convenience of translating between multiple languages with a single model. However, MNMT often suffers from performance degradation in high-resource languages compared to bilingual counterparts. This degradation is commonly attributed to parameter interference, which occurs when parameters are fully shared across all language pairs. In this work, to tackle this issue we propose a gradient-based gradual pruning technique for MNMT. Our approach aims to identify an optimal sub-network for each language pair within the multilingual model by leveraging gradient-based information as pruning criterion and gradually increasing the pruning ratio as schedule. Our approach allows for partial parameter sharing across language pairs to alleviate interference, and each pair preserves its unique parameters to capture language-specific information. Comprehensive experiments on IWSLT and WMT datasets show that our approach yields a notable performance gain on both datasets",
    "checked": true,
    "id": "336b10394a6b23a888f5fe0f109965802da47bb4",
    "semantic_title": "gradient-based gradual pruning for language-specific multilingual neural machine translation",
    "citation_count": 0,
    "authors": [
      "Dan He",
      "Minh-Quang Pham",
      "Thanh-Le Ha",
      "Marco Turchi"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.44": {
    "title": "LLM-powered Data Augmentation for Enhanced Cross-lingual Performance",
    "volume": "main",
    "abstract": "This paper explores the potential of leveraging Large Language Models (LLMs) for data augmentation in multilingual commonsense reasoning datasets where the available training data is extremely limited. To achieve this, we utilise several LLMs, namely Dolly-v2, StableVicuna, ChatGPT, and GPT-4, to augment three datasets: XCOPA, XWinograd, and XStoryCloze. Subsequently, we evaluate the effectiveness of fine-tuning smaller multilingual models, mBERT and XLMR, using the synthesised data. We compare the performance of training with data generated in English and target languages, as well as translated English-generated data, revealing the overall advantages of incorporating data generated by LLMs, e.g. a notable 13.4 accuracy score improvement for the best case. Furthermore, we conduct a human evaluation by asking native speakers to assess the naturalness and logical coherence of the generated examples across different languages. The results of the evaluation indicate that LLMs such as ChatGPT and GPT-4 excel at producing natural and coherent text in most languages, however, they struggle to generate meaningful text in certain languages like Tamil. We also observe that ChatGPT falls short in generating plausible alternatives compared to the original dataset, whereas examples from GPT-4 exhibit competitive logical consistency",
    "checked": false,
    "id": "07af221e02d12ce893b7d3ebc89468442914590d",
    "semantic_title": "development of efficient techniques for asr system for speech detection and recognization system using gaussian mixture model- universal background model",
    "citation_count": 0,
    "authors": [
      "Chenxi Whitehouse",
      "Monojit Choudhury",
      "Alham Aji"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.45": {
    "title": "Prompt-based Logical Semantics Enhancement for Implicit Discourse Relation Recognition",
    "volume": "main",
    "abstract": "Implicit Discourse Relation Recognition (IDRR), which infers discourse relations without the help of explicit connectives, is still a crucial and challenging task for discourse parsing. Recent works tend to exploit the hierarchical structure information from the annotated senses, which demonstrate enhanced discourse relation representations can be obtained by integrating sense hierarchy. Nevertheless, the performance and robustness for IDRR are significantly constrained by the availability of annotated data. Fortunately, there is a wealth of unannotated utterances with explicit connectives, that can be utilized to acquire enriched discourse relation features. In light of such motivation, we propose a Prompt-based Logical Semantics Enhancement (PLSE) method for IDRR. Essentially, our method seamlessly injects knowledge relevant to discourse relation into pre-trained language models through prompt-based connective prediction. Furthermore, considering the prompt-based connective prediction exhibits local dependencies due to the deficiency of masked language model (MLM) in capturing global semantics, we design a novel self-supervised learning objective based on mutual information maximization to derive enhanced representations of logical semantics for IDRR. Experimental results on PDTB 2.0 and CoNLL16 datasets demonstrate that our method achieves outstanding and consistent performance against the current state-of-the-art models",
    "checked": true,
    "id": "79551518b6d5f920cb5ba77705be57853778db77",
    "semantic_title": "prompt-based logical semantics enhancement for implicit discourse relation recognition",
    "citation_count": 0,
    "authors": [
      "Chenxu Wang",
      "Ping Jian",
      "Mu Huang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.46": {
    "title": "VLIS: Unimodal Language Models Guide Multimodal Language Generation",
    "volume": "main",
    "abstract": "Multimodal language generation, which leverages the synergy of language and vision, is a rapidly expanding field. However, existing vision-language models face challenges in tasks that require complex linguistic understanding. To address this issue, we introduce Visual-Language models as Importance Sampling weights (VLIS), a novel framework that combines the visual conditioning capability of vision-language models with the language understanding of unimodal text-only language models without further training. It extracts pointwise mutual information of each image and text from a visual-language model and uses the value as an importance sampling weight to adjust the token likelihood from a text-only model. VLIS improves vision-language models on diverse tasks, including commonsense understanding (WHOOPS, OK-VQA, and ScienceQA) and complex text generation (Concadia, Image Paragraph Captioning, and ROCStories). Our results suggest that VLIS represents a promising new direction for multimodal language generation",
    "checked": true,
    "id": "ff4f3ac67c6d7d7da3d0f27ddafc0f9552a29b00",
    "semantic_title": "vlis: unimodal language models guide multimodal language generation",
    "citation_count": 0,
    "authors": [
      "Jiwan Chung",
      "Youngjae Yu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.47": {
    "title": "Conceptual structure coheres in human cognition but not in large language models",
    "volume": "main",
    "abstract": "Neural network models of language have long been used as a tool for developing hypotheses about conceptual representation in the mind and brain. For many years, such use involved extracting vector-space representations of words and using distances among these to predict or understand human behavior in various semantic tasks. In contemporary language models, however, it is possible to interrogate the latent structure of conceptual representations using methods nearly identical to those commonly used with human participants. The current work uses three common techniques borrowed from cognitive psychology to estimate and compare lexical-semantic structure in both humans and a well-known large language model, the DaVinci variant of GPT-3. In humans, we show that conceptual structure is robust to differences in culture, language, and method of estimation. Structures estimated from the LLM behavior, while individually fairly consistent with those estimated from human behavior, depend much more upon the particular task used to generate behavior responses–responses generated by the very same model in the three tasks yield estimates of conceptual structure that cohere less with one another than do human structure estimates. The results suggest one important way that knowledge inhering in contemporary LLMs can differ from human cognition",
    "checked": true,
    "id": "3a69769d2d0d259299373698ae73c940a255e932",
    "semantic_title": "conceptual structure coheres in human cognition but not in large language models",
    "citation_count": 1,
    "authors": [
      "Siddharth Suresh",
      "Kushin Mukherjee",
      "Xizheng Yu",
      "Wei-Chun Huang",
      "Lisa Padua",
      "Timothy Rogers"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.48": {
    "title": "Towards LLM-driven Dialogue State Tracking",
    "volume": "main",
    "abstract": "Dialogue State Tracking (DST) is of paramount importance in ensuring accurate tracking of user goals and system actions within task-oriented dialogue systems. The emergence of large language models (LLMs) such as GPT3 and ChatGPT has sparked considerable interest in assessing their efficacy across diverse applications. In this study, we conduct an initial examination of ChatGPT's capabilities in DST. Our evaluation uncovers the exceptional performance of ChatGPT in this task, offering valuable insights to researchers regarding its capabilities and providing useful directions for designing and enhancing dialogue systems. Despite its impressive performance, ChatGPT has significant limitations including its closed-source nature, request restrictions, raising data privacy concerns, and lacking local deployment capabilities. To address these concerns, we present LDST, an LLM-driven DST framework based on smaller, open-source foundation models. By utilizing a novel domain-slot instruction tuning method, LDST achieves performance on par with ChatGPT. Comprehensive evaluations across three distinct experimental settings, we find that LDST exhibits remarkable performance improvements in both zero-shot and few-shot setting compared to previous SOTA methods. The source code is provided for reproducibility",
    "checked": true,
    "id": "4f55797c559bfaae4597ba67550d5d38f09285a9",
    "semantic_title": "towards llm-driven dialogue state tracking",
    "citation_count": 0,
    "authors": [
      "Yujie Feng",
      "Zexin Lu",
      "Bo Liu",
      "Liming Zhan",
      "Xiao-Ming Wu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.49": {
    "title": "Learning Language-guided Adaptive Hyper-modality Representation for Multimodal Sentiment Analysis",
    "volume": "main",
    "abstract": "Though Multimodal Sentiment Analysis (MSA) proves effective by utilizing rich information from multiple sources (*e.g.,* language, video, and audio), the potential sentiment-irrelevant and conflicting information across modalities may hinder the performance from being further improved. To alleviate this, we present Adaptive Language-guided Multimodal Transformer (ALMT), which incorporates an Adaptive Hyper-modality Learning (AHL) module to learn an irrelevance/conflict-suppressing representation from visual and audio features under the guidance of language features at different scales. With the obtained hyper-modality representation, the model can obtain a complementary and joint representation through multimodal fusion for effective MSA. In practice, ALMT achieves state-of-the-art performance on several popular datasets (*e.g.,* MOSI, MOSEI and CH-SIMS) and an abundance of ablation demonstrates the validity and necessity of our irrelevance/conflict suppression mechanism",
    "checked": true,
    "id": "0d547eb3c07c5015656c54f06b4c874a1f6db0d0",
    "semantic_title": "learning language-guided adaptive hyper-modality representation for multimodal sentiment analysis",
    "citation_count": 0,
    "authors": [
      "Haoyu Zhang",
      "Yu Wang",
      "Guanghao Yin",
      "Kejun Liu",
      "Yuanyuan Liu",
      "Tianshu Yu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.50": {
    "title": "Multitask Multimodal Prompted Training for Interactive Embodied Task Completion",
    "volume": "main",
    "abstract": "Interactive and embodied tasks pose at least two fundamental challenges to existing Vision & Language (VL) models, including 1) grounding language in trajectories of actions and observations, and 2) referential disambiguation. To tackle these challenges, we propose an Embodied MultiModal Agent (EMMA): a unified encoder-decoder model that reasons over images and trajectories, and casts action prediction as multimodal text generation. By unifying all tasks as text generation, EMMA learns a language of actions which facilitates transfer across tasks. Different to previous modular approaches with independently trained components, we use a single multitask model where each task contributes to goal completion. EMMA performs on par with similar models on several VL benchmarks and sets a new state-of-the-art performance (36.81% success rate) on the Dialog-guided Task Completion (DTC), a benchmark to evaluate dialog-guided agents in the Alexa Arena",
    "checked": true,
    "id": "54a65fb8c740f96c83efb4181c4311474a7835c2",
    "semantic_title": "multitask multimodal prompted training for interactive embodied task completion",
    "citation_count": 0,
    "authors": [
      "Georgios Pantazopoulos",
      "Malvina Nikandrou",
      "Amit Parekh",
      "Bhathiya Hemanthage",
      "Arash Eshghi",
      "Ioannis Konstas",
      "Verena Rieser",
      "Oliver Lemon",
      "Alessandro Suglia"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.51": {
    "title": "We're Afraid Language Models Aren't Modeling Ambiguity",
    "volume": "main",
    "abstract": "Ambiguity is an intrinsic feature of natural language. Managing ambiguity is a key part of human language understanding, allowing us to anticipate misunderstanding as communicators and revise our interpretations as listeners. As language models are increasingly employed as dialogue interfaces and writing aids, handling ambiguous language is critical to their success. We capture ambiguity in a sentence through its effect on entailment relations with another sentence, and collect AmbiEnt, a linguist-annotated benchmark of 1,645 examples with diverse kinds of ambiguity. We design a suite of tests based on AmbiEnt, presenting the first evaluation of pretrained LMs to recognize ambiguity and disentangle possible meanings. We find that the task remains extremely challenging, including for GPT-4, whose generated disambiguations are considered correct only 32% of the time in crowdworker evaluation, compared to 90% for disambiguations in our dataset. Finally, to illustrate the value of ambiguity-sensitive tools, we show that a multilabel NLI model can flag political claims in the wild that are misleading due to ambiguity. We encourage the field to rediscover the importance of ambiguity for NLP",
    "checked": true,
    "id": "e877d295ca425faf33f0c8e4d8c410c2e9c8a26d",
    "semantic_title": "we're afraid language models aren't modeling ambiguity",
    "citation_count": 23,
    "authors": [
      "Alisa Liu",
      "Zhaofeng Wu",
      "Julian Michael",
      "Alane Suhr",
      "Peter West",
      "Alexander Koller",
      "Swabha Swayamdipta",
      "Noah Smith",
      "Yejin Choi"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.52": {
    "title": "Linear-Time Modeling of Linguistic Structure: An Order-Theoretic Perspective",
    "volume": "main",
    "abstract": "Tasks that model the relation between pairs of tokens in a string are a vital part of understanding natural language. Such tasks, in general, require exhaustive pair-wise comparisons of tokens, thus having a quadratic runtime complexity in the length of the string. We show that these exhaustive comparisons can be avoided, and, moreover, the complexity of such tasks can be reduced to linear by casting the relation between tokens as a partial order over the string. Our method predicts real numbers for each token in a string in parallel and sorts the tokens accordingly, resulting in total orders of the tokens in the string. Each total order implies a set of arcs oriented from smaller to greater tokens, sorted by their predicted numbers. The intersection of total orders results in a partial order over the set of tokens in the string, which is then decoded into a directed graph representing the desired linguistic structure. Our experiments on dependency parsing and coreference resolution show that our method achieves state-of-the-art or comparable performance. Moreover, the linear complexity and parallelism of our method double the speed of graph-based coreference resolution models, and bring a 10-times speed-up over graph-based dependency parsers",
    "checked": true,
    "id": "340ae8b712340d513f288a3b84416f29bde1de2b",
    "semantic_title": "linear-time modeling of linguistic structure: an order-theoretic perspective",
    "citation_count": 0,
    "authors": [
      "Tianyu Liu",
      "Afra Amini",
      "Mrinmaya Sachan",
      "Ryan Cotterell"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.53": {
    "title": "GEMINI: Controlling The Sentence-Level Summary Style in Abstractive Text Summarization",
    "volume": "main",
    "abstract": "Human experts write summaries using different techniques, including extracting a sentence from the document and rewriting it, or fusing various information from the document to abstract it. These techniques are flexible and thus difficult to be imitated by any single method. To address this issue, we propose an adaptive model, GEMINI, that integrates a rewriter and a generator to mimic the sentence rewriting and abstracting techniques, respectively. GEMINI adaptively chooses to rewrite a specific document sentence or generate a summary sentence from scratch. Experiments demonstrate that our adaptive approach outperforms the pure abstractive and rewriting baselines on three benchmark datasets, achieving the best results on WikiHow. Interestingly, empirical results show that the human summary styles of summary sentences are consistently predictable given their context. We release our code and model at https://github.com/baoguangsheng/gemini",
    "checked": true,
    "id": "4fe028e3b206026a407524455283201f752186c1",
    "semantic_title": "gemini: controlling the sentence-level summary style in abstractive text summarization",
    "citation_count": 1,
    "authors": [
      "Guangsheng Bao",
      "Zebin Ou",
      "Yue Zhang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.54": {
    "title": "Fidelity-Enriched Contrastive Search: Reconciling the Faithfulness-Diversity Trade-Off in Text Generation",
    "volume": "main",
    "abstract": "In this paper, we address the hallucination problem commonly found in natural language generation tasks. Language models often generate fluent and convincing content but can lack consistency with the provided source, resulting in potential inaccuracies. We propose a new decoding method called Fidelity-Enriched Contrastive Search (FECS), which augments the contrastive search framework with context-aware regularization terms. FECS promotes tokens that are semantically similar to the provided source while penalizing repetitiveness in the generated text. We demonstrate its effectiveness across two tasks prone to hallucination: abstractive summarization and dialogue generation. Results show that FECS consistently enhances faithfulness across various language model sizes while maintaining output diversity comparable to well-performing decoding algorithms",
    "checked": true,
    "id": "ff5695b5a452629f67094e8004db410c26731fb1",
    "semantic_title": "fidelity-enriched contrastive search: reconciling the faithfulness-diversity trade-off in text generation",
    "citation_count": 0,
    "authors": [
      "Wei-Lin Chen",
      "Cheng-Kuang Wu",
      "Hsin-Hsi Chen",
      "Chung-Chi Chen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.55": {
    "title": "Analyzing Norm Violations in Live-Stream Chat",
    "volume": "main",
    "abstract": "Toxic language, such as hate speech, can deter users from participating in online communities and enjoying popular platforms. Previous approaches to detecting toxic language and norm violations have been primarily concerned with conversations from online forums and social media, such as Reddit and Twitter. These approaches are less effective when applied to conversations on live-streaming platforms, such as Twitch and YouTube Live, as each comment is only visible for a limited time and lacks a thread structure that establishes its relationship with other comments. In this work, we share the first NLP study dedicated to detecting norm violations in conversations on live-streaming platforms. We define norm violation categories in live-stream chats and annotate 4,583 moderated comments from Twitch. We articulate several facets of live-stream data that differ from other forums, and demonstrate that existing models perform poorly in this setting. By conducting a user study, we identify the informational context humans use in live-stream moderation, and train models leveraging context to identify norm violations. Our results show that appropriate contextual information can boost moderation performance by 35%",
    "checked": true,
    "id": "b647872ea2f3589e20dae7a5b04b1961861fa215",
    "semantic_title": "analyzing norm violations in live-stream chat",
    "citation_count": 0,
    "authors": [
      "Jihyung Moon",
      "Dong-Ho Lee",
      "Hyundong Cho",
      "Woojeong Jin",
      "Chan Park",
      "Minwoo Kim",
      "Jonathan May",
      "Jay Pujara",
      "Sungjoon Park"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.56": {
    "title": "Coarse-to-Fine Contrastive Learning in Image-Text-Graph Space for Improved Vision-Language Compositionality",
    "volume": "main",
    "abstract": "Contrastively trained vision-language models have achieved remarkable progress in vision and language representation learning. However, recent research has highlighted severe limitations of these models in their ability to perform compositional reasoning over objects, attributes, and relations. Scene graphs have emerged as an effective way to understand images compositionally. These are graph-structured semantic representations of images that contain objects, their attributes, and relations with other objects in a scene. In this work, we consider the scene graph parsed from text as a proxy for the image scene graph and propose a graph decomposition and augmentation framework along with a coarse-to-fine contrastive learning objective between images and text that aligns sentences of various complexities to the same image. We also introduce novel negative mining techniques in the scene graph space for improving attribute binding and relation understanding. Through extensive experiments, we demonstrate the effectiveness of our approach that significantly improves attribute binding, relation understanding, systematic generalization, and productivity on multiple recently proposed benchmarks (For example, improvements up to 18% for systematic generalization, 16.5% for relation understanding over a strong baseline), while achieving similar or better performance than CLIP on various general multimodal tasks",
    "checked": true,
    "id": "46ac5734e8e5122d3c34df1b37cd68481bd4b9f2",
    "semantic_title": "coarse-to-fine contrastive learning in image-text-graph space for improved vision-language compositionality",
    "citation_count": 4,
    "authors": [
      "Harman Singh",
      "Pengchuan Zhang",
      "Qifan Wang",
      "Mengjiao Wang",
      "Wenhan Xiong",
      "Jingfei Du",
      "Yu Chen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.57": {
    "title": "Reading Books is Great, But Not if You Are Driving! Visually Grounded Reasoning about Defeasible Commonsense Norms",
    "volume": "main",
    "abstract": "Commonsense norms are defeasible by context: reading books is usually great, but not when driving a car. While contexts can be explicitly described in language, in embodied scenarios, contexts are often provided visually. This type of visually grounded reasoning about defeasible commonsense norms is generally easy for humans, but (as we show) poses a challenge for machines, as it necessitates both visual understanding and reasoning about commonsense norms. We construct a new multimodal benchmark for studying commonsense norms: NormLens. NormLens consists of 10K human judgments accompanied by free-form explanations covering 2K multimodal situations, and serves as a probe to address two questions: (1) to what extent can models align with average human judgment? and (2) how well can models explain their predicted judgments? We find that state-of-the-art model judgments and explanations are not well-aligned with human annotation. Additionally, we present a simple yet effective approach to better align models with humans by distilling social commonsense knowledge from large language models. The data and code will be released",
    "checked": true,
    "id": "00c19d9818bf093a2eed323d1bd5c763c4f512b9",
    "semantic_title": "reading books is great, but not if you are driving! visually grounded reasoning about defeasible commonsense norms",
    "citation_count": 0,
    "authors": [
      "Seungju Han",
      "Junhyeok Kim",
      "Jack Hessel",
      "Liwei Jiang",
      "Jiwan Chung",
      "Yejin Son",
      "Yejin Choi",
      "Youngjae Yu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.58": {
    "title": "Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have gained significant popularity for their impressive performance across diverse fields. However, LLMs are prone to hallucinate untruthful or nonsensical outputs that fail to meet user expectations in many real-world applications. Existing works for detecting hallucinations in LLMs either rely on external knowledge for reference retrieval or require sampling multiple responses from the LLM for consistency verification, making these methods costly and inefficient. In this paper, we propose a novel reference-free, uncertainty-based method for detecting hallucinations in LLMs. Our approach imitates human focus in factuality checking from three aspects: 1) focus on the most informative and important keywords in the given text; 2) focus on the unreliable tokens in historical context which may lead to a cascade of hallucinations; and 3) focus on the token properties such as token type and token frequency. Experimental results on relevant datasets demonstrate the effectiveness of our proposed method, which achieves state-of-the-art performance across all the evaluation metrics and eliminates the need for additional information",
    "checked": true,
    "id": "2fae69cea48d332c5788537a0b5e9e76c10e3baf",
    "semantic_title": "enhancing uncertainty-based hallucination detection with stronger focus",
    "citation_count": 0,
    "authors": [
      "Tianhang Zhang",
      "Lin Qiu",
      "Qipeng Guo",
      "Cheng Deng",
      "Yue Zhang",
      "Zheng Zhang",
      "Chenghu Zhou",
      "Xinbing Wang",
      "Luoyi Fu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.59": {
    "title": "FactKB: Generalizable Factuality Evaluation using Language Models Enhanced with Factual Knowledge",
    "volume": "main",
    "abstract": "Evaluating the factual consistency of automatically generated summaries is essential for the progress and adoption of reliable summarization systems. Despite recent advances, existing factuality evaluation models are not robust, being especially prone to entity and relation errors in new domains. We propose FactKB—a simple new approach to factuality evaluation that is generalizable across domains, in particular with respect to entities and relations. FactKB is based on language models pretrained using facts extracted from external knowledge bases. We introduce three types of complementary factuality pretraining objectives based on entity-specific facts, facts extracted from auxiliary knowledge about entities, and facts constructed compositionally through knowledge base walks. The resulting factuality evaluation model achieves state-of-the-art performance on two in-domain news summarization benchmarks as well as on three out-of-domain scientific literature datasets. Further analysis of FactKB shows improved ability to detect erroneous entities and relations in summaries and is robust and easily generalizable across domains",
    "checked": true,
    "id": "663d743272e9ab04f54d9105a3c3a3f6e22dd1dd",
    "semantic_title": "factkb: generalizable factuality evaluation using language models enhanced with factual knowledge",
    "citation_count": 9,
    "authors": [
      "Shangbin Feng",
      "Vidhisha Balachandran",
      "Yuyang Bai",
      "Yulia Tsvetkov"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.60": {
    "title": "Mitigating Backdoor Poisoning Attacks through the Lens of Spurious Correlation",
    "volume": "main",
    "abstract": "Modern NLP models are often trained over large untrusted datasets, raising the potential for a malicious adversary to compromise model behaviour. For instance, backdoors can be implanted through crafting training instances with a specific textual trigger and a target label. This paper posits that backdoor poisoning attacks exhibit a spurious correlation between simple text features and classification labels, and accordingly, proposes methods for mitigating spurious correlation as means of defence. Our empirical study reveals that the malicious triggers are highly correlated to their target labels; therefore such correlations are extremely distinguishable compared to those scores of benign features, and can be used to filter out potentially problematic instances. Compared with several existing defences, our defence method significantly reduces attack success rates across backdoor attacks, and in the case of insertion-based attacks, our method provides a near-perfect defence",
    "checked": true,
    "id": "442ed16498e0edc80bcd23590d5b238293acdb20",
    "semantic_title": "mitigating backdoor poisoning attacks through the lens of spurious correlation",
    "citation_count": 4,
    "authors": [
      "Xuanli He",
      "Qiongkai Xu",
      "Jun Wang",
      "Benjamin Rubinstein",
      "Trevor Cohn"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.61": {
    "title": "Symbol tuning improves in-context learning in language models",
    "volume": "main",
    "abstract": "We present symbol tuning - finetuning language models on in-context input-label pairs where natural language labels (e.g., \"positive/negative sentiment\") are replaced with arbitrary symbols (e.g., \"foo/bar\"). Symbol tuning leverages the intuition that when a model cannot use instructions or natural language labels to figure out a task, it must instead do so by learning the input-label mappings. We experiment with symbol tuning across PaLM models up to 540B parameters and observe benefits across various settings. First, symbol tuning boosts performance on unseen in-context learning tasks and is much more robust to underspecified prompts, such as those without instructions or without natural language labels. Second, symbol-tuned models are much stronger at algorithmic reasoning tasks, with up to 18.2% better performance on the List Functions benchmark and up to 15.3% better performance on the Simple Turing Concepts benchmark. Finally, symbol-tuned models show large improvements in following flipped-labels presented in-context, meaning that they are more capable of using in-context information to override prior knowledge",
    "checked": true,
    "id": "3db1219429c3f04e88347d41269bdc83c457fbf9",
    "semantic_title": "symbol tuning improves in-context learning in language models",
    "citation_count": 18,
    "authors": [
      "Jerry Wei",
      "Le Hou",
      "Andrew Lampinen",
      "Xiangning Chen",
      "Da Huang",
      "Yi Tay",
      "Xinyun Chen",
      "Yifeng Lu",
      "Denny Zhou",
      "Tengyu Ma",
      "Quoc Le"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.62": {
    "title": "The neural dynamics of word recognition and integration",
    "volume": "main",
    "abstract": "Listeners recognize and integrate words in rapid and noisy everyday speech by combining expectations about upcoming content with incremental sensory evidence. We present a computational model of word recognition which formalizes this perceptual process in Bayesian decision theory. We fit this model to explain scalp EEG signals recorded as subjects passively listened to a fictional story, revealing both the dynamics of the online auditory word recognition process and the neural correlates of the recognition and integration of words. The model reveals distinct neural processing of words depending on whether or not they can be quickly recognized. While all words trigger a neural response characteristic of probabilistic integration — voltage modulations predicted by a word's surprisal in context — these modulations are amplified for words which require more than roughly 150 ms of input to be recognized. We observe no difference in the latency of these neural responses according to words' recognition times. Our results support a two-part model of speech comprehension, combining an eager and rapid process of word recognition with a temporally independent process of word integration. However, we also developed alternative models of the scalp EEG signal not incorporating word recognition dynamics which showed similar performance improvements. We discuss potential future modeling steps which may help to separate these hypotheses",
    "checked": true,
    "id": "7d689a3c4e3d0ad71732cf699fa75fab57e7333f",
    "semantic_title": "the neural dynamics of word recognition and integration",
    "citation_count": 0,
    "authors": [
      "Jon Gauthier",
      "Roger Levy"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.63": {
    "title": "Tree of Clarifications: Answering Ambiguous Questions with Retrieval-Augmented Large Language Models",
    "volume": "main",
    "abstract": "Questions in open-domain question answering are often ambiguous, allowing multiple interpretations. One approach to handling them is to identify all possible interpretations of the ambiguous question (AQ) and to generate a long-form answer addressing them all, as suggested by Stelmakh et al., (2022). While it provides a comprehensive response without bothering the user for clarification, considering multiple dimensions of ambiguity and gathering corresponding knowledge remains a challenge. To cope with the challenge, we propose a novel framework, Tree of Clarifications (ToC): It recursively constructs a tree of disambiguations for the AQ—via few-shot prompting leveraging external knowledge—and uses it to generate a long-form answer. ToC outperforms existing baselines on ASQA in a few-shot setup across the metrics, while surpassing fully-supervised baselines trained on the whole training set in terms of Disambig-F1 and Disambig-ROUGE. Code is available at https://github.com/gankim/tree-of-clarifications",
    "checked": true,
    "id": "c49fd6cac5382cdbc2bc31be195e42bc28dc615d",
    "semantic_title": "tree of clarifications: answering ambiguous questions with retrieval-augmented large language models",
    "citation_count": 0,
    "authors": [
      "Gangwoo Kim",
      "Sungdong Kim",
      "Byeongguk Jeon",
      "Joonsuk Park",
      "Jaewoo Kang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.64": {
    "title": "Incorporating Worker Perspectives into MTurk Annotation Practices for NLP",
    "volume": "main",
    "abstract": "Current practices regarding data collection for natural language processing on Amazon Mechanical Turk (MTurk) often rely on a combination of studies on data quality and heuristics shared among NLP researchers. However, without considering the perspectives of MTurk workers, these approaches are susceptible to issues regarding workers' rights and poor response quality. We conducted a critical literature review and a survey of MTurk workers aimed at addressing open questions regarding best practices for fair payment, worker privacy, data quality, and considering worker incentives. We found that worker preferences are often at odds with received wisdom among NLP researchers. Surveyed workers preferred reliable, reasonable payments over uncertain, very high payments; reported frequently lying on demographic questions; and expressed frustration at having work rejected with no explanation. We also found that workers view some quality control methods, such as requiring minimum response times or Master's qualifications, as biased and largely ineffective. Based on the survey results, we provide recommendations on how future NLP studies may better account for MTurk workers' experiences in order to respect workers' rights and improve data quality",
    "checked": true,
    "id": "6f9f9e9cb90e0139480fe983609f8e88556ee04f",
    "semantic_title": "incorporating worker perspectives into mturk annotation practices for nlp",
    "citation_count": 0,
    "authors": [
      "Olivia Huang",
      "Eve Fleisig",
      "Dan Klein"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.65": {
    "title": "Predict the Future from the Past? On the Temporal Data Distribution Shift in Financial Sentiment Classifications",
    "volume": "main",
    "abstract": "Temporal data distribution shift is prevalent in the financial text. How can a financial sentiment analysis system be trained in a volatile market environment that can accurately infer sentiment and be robust to temporal data distribution shifts? In this paper, we conduct an empirical study on the financial sentiment analysis system under temporal data distribution shifts using a real-world financial social media dataset that spans three years. We find that the fine-tuned models suffer from general performance degradation in the presence of temporal distribution shifts. Furthermore, motivated by the unique temporal nature of the financial text, we propose a novel method that combines out-of-distribution detection with time series modeling for temporal financial sentiment analysis. Experimental results show that the proposed method enhances the model's capability to adapt to evolving temporal shifts in a volatile financial market",
    "checked": true,
    "id": "f529a62dc84712f6178332f72e67b7573bd10f0b",
    "semantic_title": "predict the future from the past? on the temporal data distribution shift in financial sentiment classifications",
    "citation_count": 0,
    "authors": [
      "Yue Guo",
      "Chenxi Hu",
      "Yi Yang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.66": {
    "title": "Look-back Decoding for Open-Ended Text Generation",
    "volume": "main",
    "abstract": "Given a prefix (context), open-ended generation aims to decode texts that are coherent, which do not abruptly drift from previous topics, and informative, which do not suffer from undesired repetitions. In this paper, we propose Look-back, an improved decoding algorithm that leverages the Kullback–Leibler divergence to track the distribution distance between current and historical decoding steps. Thus Look-back can automatically predict potential repetitive phrase and topic drift, and remove tokens that may cause the failure modes, restricting the next token probability distribution within a plausible distance to the history. We perform decoding experiments on document continuation and story generation, and demonstrate that Look-back is able to generate more fluent and coherent text, outperforming other strong decoding methods significantly in both automatic and human evaluations",
    "checked": true,
    "id": "0a90d5f56150eafd3ac88e4406b17496fedf529a",
    "semantic_title": "look-back decoding for open-ended text generation",
    "citation_count": 1,
    "authors": [
      "Nan Xu",
      "Chunting Zhou",
      "Asli Celikyilmaz",
      "Xuezhe Ma"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.67": {
    "title": "Large Language Models Can Self-Improve",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate \"high-confidence\" rationale-augmented answers for unlabeled questions using Chain-of-Though (CoT) prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs. We show that without any ground truth label, our approach improves the general reasoning ability of a 540B-parameter LLM (74.4%→82.1% on GSM8K, 90.0%→94.4% on OpenBookQA, and 63.4%→67.9% on ANLI-A3) and can also be adapted to extreme low-resource cases where even training questions and CoT prompts are limited. We conduct ablation studies and show that fine-tuning on diverse reasoning paths is critical for self-improvement",
    "checked": true,
    "id": "3fa70115248377c3d1517c9f978791a296fbc1dd",
    "semantic_title": "large language models can self-improve",
    "citation_count": 171,
    "authors": [
      "Jiaxin Huang",
      "Shixiang Gu",
      "Le Hou",
      "Yuexin Wu",
      "Xuezhi Wang",
      "Hongkun Yu",
      "Jiawei Han"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.68": {
    "title": "CodeT5+: Open Code Large Language Models for Code Understanding and Generation",
    "volume": "main",
    "abstract": "Large language models (LLMs) pretrained on vast source code have achieved prominent progress in code intelligence. However, existing code LLMs have two main limitations. First, they often adopt a specific architecture (encoder-only or decoder-only) or rely on a unified encoder-decoder network for different downstream tasks, lacking the flexibility to operate in the optimal architecture for a specific task. Secondly, they often employ a limited set of pretraining objectives which might not be relevant to some tasks and hence result in substantial performance degrade. To address these limitations, we propose \"CodeT5+\", a family of encoder-decoder LLMs for code in which component modules can be flexibly combined to suit a wide range of code tasks. Such flexibility is enabled by our proposed mixture of pretraining objectives, which cover span denoising, contrastive learning, text-code matching, and causal LM pretraining tasks, on both unimodal and bimodal multilingual code corpora. Furthermore, we propose to initialize CodeT5+ with frozen off-the-shelf LLMs without training from scratch to efficiently scale up our models, and explore instruction-tuning to align with natural language instructions. We extensively evaluate CodeT5+ on over 20 code-related benchmarks in different settings, including zero-shot, finetuning, and instruction-tuning. We observe state-of-the-art (SoTA) performance on various code-related tasks, and our instruction-tuned CodeT5+ 16B achieves new SoTA results of 35.0% pass@1 and 54.5% pass@10 on the HumanEval code generation task against other open code LLMs, even surpassing the OpenAI code-cushman-001 model",
    "checked": true,
    "id": "9ada8fa11b1cdece31f253acae50b62df8d5f823",
    "semantic_title": "codet5+: open code large language models for code understanding and generation",
    "citation_count": 77,
    "authors": [
      "Yue Wang",
      "Hung Le",
      "Akhilesh Gotmare",
      "Nghi Bui",
      "Junnan Li",
      "Steven Hoi"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.69": {
    "title": "Structural generalization in COGS: Supertagging is (almost) all you need",
    "volume": "main",
    "abstract": "In many Natural Language Processing applications, neural networks have been found to fail to generalize on out-of-distribution examples. In particular, several recent semantic parsing datasets have put forward important limitations of neural networks in cases where compositional generalization is required. In this work, we extend a neural graph-based parsing framework in several ways to alleviate this issue, notably: (1) the introduction of a supertagging step with valency constraints, expressed as an integer linear program; (2) the reduction of the graph prediction problem to the maximum matching problem; (3) the design of an incremental early-stopping training strategy to prevent overfitting. Experimentally, our approach significantly improves results on examples that require structural generalization in the COGS dataset, a known challenging benchmark for compositional generalization. Overall, these results confirm that structural constraints are important for generalization in semantic parsing",
    "checked": true,
    "id": "8cf73794eb23aa1550b306f71d6d0ab874a0c283",
    "semantic_title": "structural generalization in cogs: supertagging is (almost) all you need",
    "citation_count": 0,
    "authors": [
      "Alban Petit",
      "Caio Corro",
      "François Yvon"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.70": {
    "title": "BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations",
    "volume": "main",
    "abstract": "Recent advancements in biological research leverage the integration of molecules, proteins, and natural language to enhance drug discovery. However, current models exhibit several limitations, such as the generation of invalid molecular SMILES, underutilization of contextual information, and equal treatment of structured and unstructured knowledge. To address these issues, we propose BioT5, a comprehensive pre-training framework that enriches cross-modal integration in biology with chemical knowledge and natural language associations. BioT5 utilizes SELFIES for 100% robust molecular representations and extracts knowledge from the surrounding context of bio-entities in unstructured biological literature. Furthermore, BioT5 distinguishes between structured and unstructured knowledge, leading to more effective utilization of information. After fine-tuning, BioT5 shows superior performance across a wide range of tasks, demonstrating its strong capability of capturing underlying relations and properties of bio-entities. Our code is available at https://github.com/QizhiPei/BioT5",
    "checked": true,
    "id": "c3382fd533b9dd7f8ed7ba7766159079bc1d3935",
    "semantic_title": "biot5: enriching cross-modal integration in biology with chemical knowledge and natural language associations",
    "citation_count": 0,
    "authors": [
      "Qizhi Pei",
      "Wei Zhang",
      "Jinhua Zhu",
      "Kehan Wu",
      "Kaiyuan Gao",
      "Lijun Wu",
      "Yingce Xia",
      "Rui Yan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.71": {
    "title": "Hyperpolyglot LLMs: Cross-Lingual Interpretability in Token Embeddings",
    "volume": "main",
    "abstract": "Cross-lingual transfer learning is an important property of multilingual large language models (LLMs). But how do LLMs represent relationships between languages? Every language model has an input layer that maps tokens to vectors. This ubiquitous layer of language models is often overlooked. We find that similarities between these input embeddings are highly interpretable and that the geometry of these embeddings differs between model families. In one case (XLM-RoBERTa), embeddings encode language: tokens in different writing systems can be linearly separated with an average of 99.2% accuracy. Another family (mT5) represents cross-lingual semantic similarity: the 50 nearest neighbors for any token represent an average of 7.61 writing systems, and are frequently translations. This result is surprising given that there is no explicit parallel cross-lingual training corpora and no explicit incentive for translations in pre-training objectives. Our research opens the door for investigations in 1) The effect of pre-training and model architectures on representations of languages and 2) The applications of cross-lingual representations embedded in language models",
    "checked": true,
    "id": "be1cd3f0309afdb43223961d115b1316a09d5718",
    "semantic_title": "hyperpolyglot llms: cross-lingual interpretability in token embeddings",
    "citation_count": 0,
    "authors": [
      "Andrea Wen-Yi",
      "David Mimno"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.72": {
    "title": "Target-oriented Proactive Dialogue Systems with Personalization: Problem Formulation and Dataset Curation",
    "volume": "main",
    "abstract": "Target-oriented dialogue systems, designed to proactively steer conversations toward predefined targets or accomplish specific system-side goals, are an exciting area in conversational AI. In this work, by formulating a <dialogue act, topic> pair as the conversation target, we explore a novel problem of personalized target-oriented dialogue by considering personalization during the target accomplishment process. However, there remains an emergent need for high-quality datasets, and building one from scratch requires tremendous human effort. To address this, we propose an automatic dataset curation framework using a role-playing approach. Based on this framework, we construct a large-scale personalized target-oriented dialogue dataset, TopDial, which comprises about 18K multi-turn dialogues. The experimental results show that this dataset is of high quality and could contribute to exploring personalized target-oriented dialogue",
    "checked": true,
    "id": "adf565b56ca24a598c85bd832e495f09a14cd83d",
    "semantic_title": "target-oriented proactive dialogue systems with personalization: problem formulation and dataset curation",
    "citation_count": 0,
    "authors": [
      "Jian Wang",
      "Yi Cheng",
      "Dongding Lin",
      "Chak Leong",
      "Wenjie Li"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.73": {
    "title": "SeqXGPT: Sentence-Level AI-Generated Text Detection",
    "volume": "main",
    "abstract": "Widely applied large language models (LLMs) can generate human-like content, raising concerns about the abuse of LLMs. Therefore, it is important to build strong AI-generated text (AIGT) detectors. Current works only consider document-level AIGT detection, therefore, in this paper, we first introduce a sentence-level detection challenge by synthesizing a dataset that contains documents that are polished with LLMs, that is, the documents contain sentences written by humans and sentences modified by LLMs. Then we propose Sequence X (Check) GPT, a novel method that utilizes log probability lists from white-box LLMs as features for sentence-level AIGT detection. These features are composed like waves in speech processing and cannot be studied by LLMs. Therefore, we build SeqXGPT based on convolution and self-attention networks. We test it in both sentence and document-level detection challenges. Experimental results show that previous methods struggle in solving sentence-level AIGT detection, while our method not only significantly surpasses baseline methods in both sentence and document-level detection challenges but also exhibits strong generalization capabilities",
    "checked": true,
    "id": "02f0d5faa1cd88ea0afe48b756bfbf9adfefba28",
    "semantic_title": "seqxgpt: sentence-level ai-generated text detection",
    "citation_count": 1,
    "authors": [
      "Pengyu Wang",
      "Linyang Li",
      "Ke Ren",
      "Botian Jiang",
      "Dong Zhang",
      "Xipeng Qiu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.74": {
    "title": "QTSumm: Query-Focused Summarization over Tabular Data",
    "volume": "main",
    "abstract": "People primarily consult tables to conduct data analysis or answer specific questions. Text generation systems that can provide accurate table summaries tailored to users' information needs can facilitate more efficient access to relevant data insights. Motivated by this, we define a new query-focused table summarization task, where text generation models have to perform human-like reasoning and analysis over the given table to generate a tailored summary. We introduce a new benchmark named QTSumm for this task, which contains 7,111 human-annotated query-summary pairs over 2,934 tables covering diverse topics. We investigate a set of strong baselines on QTSumm, including text generation, table-to-text generation, and large language models. Experimental results and manual analysis reveal that the new task presents significant challenges in table-to-text generation for future research. Moreover, we propose a new approach named ReFactor, to retrieve and reason over query-relevant information from tabular data to generate several natural language facts. Experimental results demonstrate that ReFactor can bring effective improvements to baselines by concatenating the generated facts to the model input. Our data and code are publicly available at https://github.com/yale-nlp/QTSumm",
    "checked": true,
    "id": "2236dca1be9f73c634d4d865e3c78b9639bfcd98",
    "semantic_title": "qtsumm: query-focused summarization over tabular data",
    "citation_count": 0,
    "authors": [
      "Yilun Zhao",
      "Zhenting Qi",
      "Linyong Nan",
      "Boyu Mi",
      "Yixin Liu",
      "Weijin Zou",
      "Simeng Han",
      "Ruizhe Chen",
      "Xiangru Tang",
      "Yumo Xu",
      "Dragomir Radev",
      "Arman Cohan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.75": {
    "title": "From Wrong To Right: A Recursive Approach Towards Vision-Language Explanation",
    "volume": "main",
    "abstract": "Addressing the challenge of adapting pre-trained vision-language models for generating insightful explanations for visual reasoning tasks with limited annotations, we present ReVisE: a Recursive Visual Explanation algorithm. Our method iteratively computes visual features (conditioned on the text input), an answer, and an explanation, to improve the explanation quality step by step until the answer converges. We find that this multi-step approach guides the model to correct its own answers and outperforms single-step explanation generation. Furthermore, explanations generated by ReVisE also serve as valuable annotations for few-shot self-training. Our approach outperforms previous methods while utilizing merely 5% of the human-annotated explanations across 10 metrics, demonstrating up to a 4.2 and 1.3 increase in BLEU-1 score on the VCR and VQA-X datasets, underscoring the efficacy and data-efficiency of our method",
    "checked": true,
    "id": "b66abfe0bb08a81f682faf70dab19a11ceb35d6e",
    "semantic_title": "from wrong to right: a recursive approach towards vision-language explanation",
    "citation_count": 0,
    "authors": [
      "Jiaxin Ge",
      "Sanjay Subramanian",
      "Trevor Darrell",
      "Boyi Li"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.76": {
    "title": "‘Don't Get Too Technical with Me': A Discourse Structure-Based Framework for Automatic Science Journalism",
    "volume": "main",
    "abstract": "Science journalism refers to the task of reporting technical findings of a scientific paper as a less technical news article to the general public audience. We aim to design an automated system to support this real-world task (i.e., automatic science journalism ) by 1) introducing a newly-constructed and real-world dataset (SciTechNews), with tuples of a publicly-available scientific paper, its corresponding news article, and an expert-written short summary snippet; 2) proposing a novel technical framework that integrates a paper's discourse structure with its metadata to guide generation; and, 3) demonstrating with extensive automatic and human experiments that our model outperforms other baseline methods (e.g. Alpaca and ChatGPT) in elaborating a content plan meaningful for the target audience, simplify the information selected, and produce a coherent final report in a layman's style",
    "checked": false,
    "id": "c9189c11489ff7a4d6dadde0443a99d553de20e1",
    "semantic_title": "don't get too technical with me': a discourse structure-based framework for automatic science journalism",
    "citation_count": 0,
    "authors": [
      "Ronald Cardenas",
      "Bingsheng Yao",
      "Dakuo Wang",
      "Yufang Hou"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.77": {
    "title": "LACMA: Language-Aligning Contrastive Learning with Meta-Actions for Embodied Instruction Following",
    "volume": "main",
    "abstract": "End-to-end Transformers have demonstrated an impressive success rate for Embodied Instruction Following when the environment has been seen in training. However, they tend to struggle when deployed in an unseen environment. This lack of generalizability is due to the agent's insensitivity to subtle changes in natural language instructions. To mitigate this issue, we propose explicitly aligning the agent's hidden states with the instructions via contrastive learning. Nevertheless, the semantic gap between high-level language instructions and the agent's low-level action space remains an obstacle. Therefore, we further introduce a novel concept of meta-actions to bridge the gap. Meta-actions are ubiquitous action patterns that can be parsed from the original action sequence. These patterns represent higher-level semantics that are intuitively aligned closer to the instructions. When meta-actions are applied as additional training signals, the agent generalizes better to unseen environments. Compared to a strong multi-modal Transformer baseline, we achieve a significant 4.5% absolute gain in success rate in unseen environments of ALFRED Embodied Instruction Following. Additional analysis shows that the contrastive objective and meta-actions are complementary in achieving the best results, and the resulting agent better aligns its states with corresponding instructions, making it more suitable for real-world embodied agents",
    "checked": true,
    "id": "2020e516779cb0e6121cc629c3c90a3088347780",
    "semantic_title": "lacma: language-aligning contrastive learning with meta-actions for embodied instruction following",
    "citation_count": 2,
    "authors": [
      "Cheng-Fu Yang",
      "Yen-Chun Chen",
      "Jianwei Yang",
      "Xiyang Dai",
      "Lu Yuan",
      "Yu-Chiang Wang",
      "Kai-Wei Chang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.78": {
    "title": "Penalty Decoding: Well Suppress the Self-Reinforcement Effect in Open-Ended Text Generation",
    "volume": "main",
    "abstract": "The decoding algorithm is critical for open-ended text generation, transforming latent representations into coherent and meaningful outputs. This paper investigates the self-reinforcement effect in text generation and the effectiveness of a repetition penalty to mitigate it. However, determining the optimal repetition penalty value is challenging. To tackle this, we propose a forgetting mechanism that disregards distant tokens, reducing the burden of penalty selection. In addition, we introduce a length penalty to address overly short sentences caused by excessive penalties. Our penalty decoding approach incorporating three strategies helps resolve issues with sampling methods deviating from factual information. Experimental results demonstrate the efficacy of our approach in generating high-quality sentences resembling human output",
    "checked": true,
    "id": "e77b127e9e2117d5d213326e350dbd14192247bc",
    "semantic_title": "penalty decoding: well suppress the self-reinforcement effect in open-ended text generation",
    "citation_count": 0,
    "authors": [
      "Wenhong Zhu",
      "Hongkun Hao",
      "Rui Wang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.79": {
    "title": "Towards Robust Pruning: An Adaptive Knowledge-Retention Pruning Strategy for Language Models",
    "volume": "main",
    "abstract": "The pruning objective has recently extended beyond accuracy and sparsity to robustness in language models. Despite this, existing methods struggle to enhance robustness against adversarial attacks when continually increasing model sparsity and require a retraining process. As humans step into the era of large language models, these issues become increasingly prominent. This paper proposes that the robustness of language models is proportional to the extent of pre-trained knowledge they encompass. Accordingly, we introduce a post-training pruning strategy designed to faithfully replicate the embedding space and feature space of dense language models, aiming to conserve more pre-trained knowledge during the pruning process. In this setup, each layer's reconstruction error not only originates from itself but also includes cumulative error from preceding layers, followed by an adaptive rectification. Compared to other state-of-art baselines, our approach demonstrates a superior balance between accuracy, sparsity, robustness, and pruning cost with BERT on datasets SST2, IMDB, and AGNews, marking a significant stride towards robust pruning in language models",
    "checked": true,
    "id": "c8adc15c2c93719ff6c43174ec9f2d92063fcd2b",
    "semantic_title": "towards robust pruning: an adaptive knowledge-retention pruning strategy for language models",
    "citation_count": 0,
    "authors": [
      "Jianwei Li",
      "Qi Lei",
      "Wei Cheng",
      "Dongkuan Xu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.80": {
    "title": "Clinical Contradiction Detection",
    "volume": "main",
    "abstract": "Detecting contradictions in text is essential in determining the validity of the literature and sources that we consume. Medical corpora are riddled with conflicting statements. This is due to the large throughput of new studies and the difficulty in replicating experiments, such as clinical trials. Detecting contradictions in this domain is hard since it requires clinical expertise. We present a distant supervision approach that leverages a medical ontology to build a seed of potential clinical contradictions over 22 million medical abstracts. We automatically build a labeled training dataset consisting of paired clinical sentences that are grounded in an ontology and represent potential medical contradiction. The dataset is used to weakly-supervise state-of-the-art deep learning models showing significant empirical improvements across multiple medical contradiction datasets",
    "checked": true,
    "id": "4459e52f28f0ee7c4827795057faad1152a77f60",
    "semantic_title": "clinical contradiction detection",
    "citation_count": 0,
    "authors": [
      "Dave Makhervaks",
      "Plia Gillis",
      "Kira Radinsky"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.81": {
    "title": "Vera: A General-Purpose Plausibility Estimation Model for Commonsense Statements",
    "volume": "main",
    "abstract": "Today's language models can be remarkably intelligent yet still produce text that contains trivial commonsense errors. Therefore, we seek a retrospective verification approach that can reflect on the commonsense plausibility of the machine text, and introduce Vera, a general-purpose model that learns to estimate the commonsense plausibility of declarative statements. To support diverse commonsense domains, Vera is trained on ~7M commonsense statements that are automatically converted from 19 QA datasets and two commonsense knowledge bases, and using a combination of three training objectives. When applied to solving commonsense problems in the verification format, Vera substantially outperforms existing models that can be repurposed for commonsense verification, even including GPT-3.5/ChatGPT/GPT-4, and it further exhibits generalization capabilities to unseen tasks and provides well-calibrated outputs. We find that Vera excels at filtering machine-generated commonsense knowledge and is useful in detecting erroneous commonsense statements generated by models like ChatGPT in real-world settings",
    "checked": true,
    "id": "b953a1527c8878698f9adb2691e425d5d206cae4",
    "semantic_title": "vera: a general-purpose plausibility estimation model for commonsense statements",
    "citation_count": 6,
    "authors": [
      "Jiacheng Liu",
      "Wenya Wang",
      "Dianzhuo Wang",
      "Noah Smith",
      "Yejin Choi",
      "Hannaneh Hajishirzi"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.82": {
    "title": "Text-Transport: Toward Learning Causal Effects of Natural Language",
    "volume": "main",
    "abstract": "As language technologies gain prominence in real-world settings, it is important to understand *how* changes to language affect reader perceptions. This can be formalized as the *causal effect* of varying a linguistic attribute (e.g., sentiment) on a reader's response to the text. In this paper, we introduce Text-Transport, a method for estimation of causal effects from natural language under any text distribution. Current approaches for valid causal effect estimation require strong assumptions about the data, meaning the data from which one *can* estimate valid causal effects often is not representative of the actual target domain of interest. To address this issue, we leverage the notion of distribution shift to describe an estimator that *transports* causal effects between domains, bypassing the need for strong assumptions in the target domain. We derive statistical guarantees on the uncertainty of this estimator, and we report empirical results and analyses that support the validity of Text-Transport across data settings. Finally, we use Text-Transport to study a realistic setting—hate speech on social media—in which causal effects do shift significantly between text domains, demonstrating the necessity of transport when conducting causal inference on natural language",
    "checked": true,
    "id": "778e5ed31bfff51a372a93b08f0a4b1191561661",
    "semantic_title": "text-transport: toward learning causal effects of natural language",
    "citation_count": 0,
    "authors": [
      "Victoria Lin",
      "Louis-Philippe Morency",
      "Eli Ben-Michael"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.83": {
    "title": "How Does Generative Retrieval Scale to Millions of Passages?",
    "volume": "main",
    "abstract": "The emerging paradigm of generative retrieval re-frames the classic information retrieval problem into a sequence-to-sequence modeling task, forgoing external indices and encoding an entire document corpus within a single Transformer. Although many different approaches have been proposed to improve the effectiveness of generative retrieval, they have only been evaluated on document corpora on the order of 100K in size. We conduct the first empirical study of generative retrieval techniques across various corpus scales, ultimately scaling up to the entire MS MARCO passage ranking task with a corpus of 8.8M passages and evaluating model sizes up to 11B parameters. We uncover several findings about scaling generative retrieval to millions of passages; notably, the central importance of using synthetic queries as document representations during indexing, the ineffectiveness of existing proposed architecture modifications when accounting for compute cost, and the limits of naively scaling model parameters with respect to retrieval performance. While we find that generative retrieval is competitive with state-of-the-art dual encoders on small corpora, scaling to millions of passages remains an important and unsolved challenge. We believe these findings will be valuable for the community to clarify the current state of generative retrieval, highlight the unique challenges, and inspire new research directions",
    "checked": true,
    "id": "20a7b1e274aff828466bba3760992aa54e14951a",
    "semantic_title": "how does generative retrieval scale to millions of passages?",
    "citation_count": 15,
    "authors": [
      "Ronak Pradeep",
      "Kai Hui",
      "Jai Gupta",
      "Adam Lelkes",
      "Honglei Zhuang",
      "Jimmy Lin",
      "Donald Metzler",
      "Vinh Tran"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.84": {
    "title": "Unveiling the Implicit Toxicity in Large Language Models",
    "volume": "main",
    "abstract": "The open-endedness of large language models (LLMs) combined with their impressive capabilities may lead to new safety issues when being exploited for malicious use. While recent studies primarily focus on probing toxic outputs that can be easily detected with existing toxicity classifiers, we show that LLMs can generate diverse implicit toxic outputs that are exceptionally difficult to detect via simply zero-shot prompting. Moreover, we propose a reinforcement learning (RL) based attacking method to further induce the implicit toxicity in LLMs. Specifically, we optimize the language model with a reward that prefers implicit toxic outputs to explicit toxic and non-toxic ones. Experiments on five widely-adopted toxicity classifiers demonstrate that the attack success rate can be significantly improved through RL fine-tuning. For instance, the RL-finetuned LLaMA-13B model achieves an attack success rate of 90.04% on BAD and 62.85% on Davinci003. Our findings suggest that LLMs pose a significant threat in generating undetectable implicit toxic outputs. We further show that fine-tuning toxicity classifiers on the annotated examples from our attacking method can effectively enhance their ability to detect LLM-generated implicit toxic language",
    "checked": true,
    "id": "034c8d4eb031786925ef274e6d275c7c210c4f1d",
    "semantic_title": "unveiling the implicit toxicity in large language models",
    "citation_count": 0,
    "authors": [
      "Jiaxin Wen",
      "Pei Ke",
      "Hao Sun",
      "Zhexin Zhang",
      "Chengfei Li",
      "Jinfeng Bai",
      "Minlie Huang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.85": {
    "title": "Is ChatGPT a General-Purpose Natural Language Processing Task Solver?",
    "volume": "main",
    "abstract": "Spurred by advancements in scale, large language models (LLMs) have demonstrated the ability to perform a variety of natural language processing (NLP) tasks zero-shot—i.e., without adaptation on downstream data. Recently, the debut of ChatGPT has drawn a great deal of attention from the natural language processing (NLP) community due to the fact that it can generate high-quality responses to human input and self-correct previous mistakes based on subsequent conversations. However, it is not yet known whether ChatGPT can serve as a generalist model that can perform many NLP tasks zero-shot. In this work, we empirically analyze the zero-shot learning ability of ChatGPT by evaluating it on 20 popular NLP datasets covering 7 representative task categories. With extensive empirical studies, we demonstrate both the effectiveness and limitations of the current version of ChatGPT. We find that ChatGPT performs well on many tasks favoring reasoning capabilities (e.g., arithmetic reasoning) while it still faces challenges when solving specific tasks such as sequence tagging. We additionally provide in-depth analysis through qualitative case studies",
    "checked": true,
    "id": "873a581320d928249609d3c07229d5af182a379c",
    "semantic_title": "is chatgpt a general-purpose natural language processing task solver?",
    "citation_count": 285,
    "authors": [
      "Chengwei Qin",
      "Aston Zhang",
      "Zhuosheng Zhang",
      "Jiaao Chen",
      "Michihiro Yasunaga",
      "Diyi Yang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.86": {
    "title": "Length is a Curse and a Blessing for Document-level Semantics",
    "volume": "main",
    "abstract": "In recent years, contrastive learning (CL) has been extensively utilized to recover sentence and document-level encoding capability from pre-trained language models. In this work, we question the length generalizability of CL-based models, i.e., their vulnerability towards length-induced semantic shift. We verify not only that length vulnerability is a significant yet overlooked research gap, but we can devise unsupervised CL methods solely depending on the semantic signal provided by document length. We first derive the theoretical foundations underlying length attacks, showing that elongating a document would intensify the high intra-document similarity that is already brought by CL. Moreover, we found that isotropy promised by CL is highly dependent on the length range of text exposed in training. Inspired by these findings, we introduce a simple yet universal document representation learning framework, **LA(SER)3**: length-agnostic self-reference for semantically robust sentence representation learning, achieving state-of-the-art unsupervised performance on the standard information retrieval benchmark. [Our code is publicly available.](https://github.com/gowitheflow-1998/LA-SER-cubed)",
    "checked": true,
    "id": "57eda84ff5aa860ea9a42b2c51e9711e7b794822",
    "semantic_title": "length is a curse and a blessing for document-level semantics",
    "citation_count": 0,
    "authors": [
      "Chenghao Xiao",
      "Yizhi Li",
      "G Hudson",
      "Chenghua Lin",
      "Noura Al Moubayed"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.87": {
    "title": "ALCUNA: Large Language Models Meet New Knowledge",
    "volume": "main",
    "abstract": "With the rapid development of NLP, large-scale language models (LLMs) excel in various tasks across multiple domains now. However, existing benchmarks may not adequately measure these models' capabilities, especially when faced with new knowledge. In this paper, we address the lack of benchmarks to evaluate LLMs' ability to handle new knowledge, an important and challenging aspect in the rapidly evolving world. We propose an approach called KnowGen that generates new knowledge by altering existing entity attributes and relationships, resulting in artificial entities that are distinct from real-world entities. With KnowGen, we introduce a benchmark named ALCUNA to assess LLMs' abilities in knowledge understanding, differentiation, and association. We benchmark several LLMs, reveals that their performance in face of new knowledge is not satisfactory, particularly in reasoning between new and internal knowledge. We also explore the impact of entity similarity on the model's understanding of entity knowledge and the influence of contextual entities. We appeal to the need for caution when using LLMs in new scenarios or with new knowledge, and hope that our benchmarks can help drive the development of LLMs in face of new knowledge",
    "checked": true,
    "id": "896845aa992d50182227e4eb2883f784e16fe60b",
    "semantic_title": "alcuna: large language models meet new knowledge",
    "citation_count": 0,
    "authors": [
      "Xunjian Yin",
      "Baizhou Huang",
      "Xiaojun Wan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.88": {
    "title": "Location-Aware Visual Question Generation with Lightweight Models",
    "volume": "main",
    "abstract": "This work introduces a novel task, location-aware visual question generation (LocaVQG), which aims to generate engaging questions from data relevant to a particular geographical location. Specifically, we represent such location-aware information with surrounding images and a GPS coordinate. To tackle this task, we present a dataset generation pipeline that leverages GPT-4 to produce diverse and sophisticated questions. Then, we aim to learn a lightweight model that can address the LocaVQG task and fit on an edge device, such as a mobile phone. To this end, we propose a method which can reliably generate engaging questions from location-aware information. Our proposed method outperforms baselines regarding human evaluation (e.g., engagement, grounding, coherence) and automatic evaluation metrics (e.g., BERTScore, ROUGE-2). Moreover, we conduct extensive ablation studies to justify our proposed techniques for both generating the dataset and solving the task",
    "checked": true,
    "id": "9df29853b62c4c141ad13dfc47d9de4adaa3ee72",
    "semantic_title": "location-aware visual question generation with lightweight models",
    "citation_count": 0,
    "authors": [
      "Nicholas Suwono",
      "Justin Chen",
      "Tun Hung",
      "Ting-Hao Huang",
      "I-Bin Liao",
      "Yung-Hui Li",
      "Lun-Wei Ku",
      "Shao-Hua Sun"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.89": {
    "title": "MemeCap: A Dataset for Captioning and Interpreting Memes",
    "volume": "main",
    "abstract": "Memes are a widely popular tool for web users to express their thoughts using visual metaphors. Understanding memes requires recognizing and interpreting visual metaphors with respect to the text inside or around the meme, often while employing background knowledge and reasoning abilities. We present the task of meme captioning and release a new dataset, MemeCap. Our dataset contains 6.3K memes along with the title of the post containing the meme, the meme captions, the literal image caption, and the visual metaphors. Despite the recent success of vision and language (VL) models on tasks such as image captioning and visual question answering, our extensive experiments using state-of-the-art VL models show that they still struggle with visual metaphors, and perform substantially worse than humans",
    "checked": true,
    "id": "b82c1b0512d25307e3c81bb8d9df1607267a7a52",
    "semantic_title": "memecap: a dataset for captioning and interpreting memes",
    "citation_count": 4,
    "authors": [
      "EunJeong Hwang",
      "Vered Shwartz"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.90": {
    "title": "Where to start? Analyzing the potential value of intermediate models",
    "volume": "main",
    "abstract": "Previous studies observed that finetuned models may be better base models than the vanilla pretrained model. Such a model, finetuned on some source dataset, may provide a better starting point for a new finetuning process on a desired target dataset. Here, we perform a systematic analysis of this intertraining scheme, over a wide range of English classification tasks. Surprisingly, our analysis suggests that the potential intertraining gain can be analyzed independently for the target dataset under consideration, and for a base model being considered as a starting point. Hence, a performant model is generally strong, even if its training data was not aligned with the target dataset. Furthermore, we leverage our analysis to propose a practical and efficient approach to determine if and how to select a base model in real-world settings. Last, we release an updating ranking of best models in the HuggingFace hub per architecture",
    "checked": true,
    "id": "1b76caf03527cab16052db606455d9827a4883e3",
    "semantic_title": "where to start? analyzing the potential value of intermediate models",
    "citation_count": 18,
    "authors": [
      "Leshem Choshen",
      "Elad Venezian",
      "Shachar Don-Yehiya",
      "Noam Slonim",
      "Yoav Katz"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.91": {
    "title": "Transcending Scaling Laws with 0.1% Extra Compute",
    "volume": "main",
    "abstract": "Scaling language models improves performance but comes with significant computational costs. This paper proposes UL2R, a method that substantially improves existing language models and their scaling curves with a relatively tiny amount of extra compute. The key idea is to continue training a state-of-the-art large language model on a few more steps with UL2's mixture-of-denoiser objective. We show that, with almost negligible extra computational costs and no new sources of data, we are able to substantially improve the scaling properties of large language models on downstream metrics. In this paper, we continue training a baseline language model, PaLM, with ULR2, introducing a new set of models at 8B, 62B, and 540B scale which we call U-PaLM. Impressively, at 540B scale, we show an approximately 2x computational savings rate where U-PaLM achieves the same performance as the final PaLM 540B model at around half its computational budget (i.e., saving ~4.4 million TPUv4 hours). We further show that this improved scaling curve leads to \"emergent abilities\" on challenging BIG-Bench tasks—for instance, U-PaLM does much better on some tasks or demonstrates better quality at much smaller scale (62B as opposed to 540B). Overall, we show that U-PaLM outperforms PaLM on many few-shot setups, including reasoning tasks with chain-of-thought (e.g., GSM8K), multilingual tasks (MGSM, TydiQA), MMLU and challenging BIG-Bench tasks",
    "checked": false,
    "id": "c648d6b064028b2f9fd3aa629b667a020d898b8e",
    "semantic_title": "new results on random perturbations of pseudoperiodic flows",
    "citation_count": 0,
    "authors": [
      "Yi Tay",
      "Jason Wei",
      "Hyung Chung",
      "Vinh Tran",
      "David So",
      "Siamak Shakeri",
      "Xavier Garcia",
      "Steven Zheng",
      "Jinfeng Rao",
      "Aakanksha Chowdhery",
      "Denny Zhou",
      "Donald Metzler",
      "Slav Petrov",
      "Neil Houlsby",
      "Quoc Le",
      "Mostafa Dehghani"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.92": {
    "title": "CoAnnotating: Uncertainty-Guided Work Allocation between Human and Large Language Models for Data Annotation",
    "volume": "main",
    "abstract": "Annotated data plays a critical role in Natural Language Processing (NLP) in training models and evaluating their performance. Given recent developments in Large Language Models (LLMs), models such as ChatGPT demonstrate zero-shot capability on many text-annotation tasks, comparable with or even exceeding human annotators. Such LLMs can serve as alternatives for manual annotation, due to lower costs and higher scalability. However, limited work has leveraged LLMs as complementary annotators, nor explored how annotation work is best allocated among humans and LLMs to achieve both quality and cost objectives. We propose CoAnnotating, a novel paradigm for Human-LLM co-annotation of unstructured texts at scale. Under this framework, we utilize uncertainty to estimate LLMs' annotation capability. Our empirical study shows CoAnnotating to be an effective means to allocate work from results on different datasets, with up to 21% performance improvement over random baseline. For code implementation, see https://github.com/SALT-NLP/CoAnnotating",
    "checked": true,
    "id": "2a74fc66beea8bce542581560ca6ec5a0e1bb024",
    "semantic_title": "coannotating: uncertainty-guided work allocation between human and large language models for data annotation",
    "citation_count": 1,
    "authors": [
      "Minzhi Li",
      "Taiwei Shi",
      "Caleb Ziems",
      "Min-Yen Kan",
      "Nancy Chen",
      "Zhengyuan Liu",
      "Diyi Yang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.93": {
    "title": "Optimizing Retrieval-augmented Reader Models via Token Elimination",
    "volume": "main",
    "abstract": "Fusion-in-Decoder (FiD) is an effective retrieval-augmented language model applied across a variety of open-domain tasks, such as question answering, fact checking, etc. In FiD, supporting passages are first retrieved and then processed using a generative model (Reader), which can cause a significant bottleneck in decoding time, particularly with long outputs. In this work, we analyze the contribution and necessity of all the retrieved passages to the performance of reader models, and propose eliminating some of the retrieved information, at the token level, that might not contribute essential information to the answer generation process. We demonstrate that our method can reduce run-time by up to 62.2%, with only a 2% reduction in performance, and in some cases, even improve the performance results",
    "checked": true,
    "id": "c0aac09fe67c39fbb377e45e03e38cd0d9b24ab4",
    "semantic_title": "optimizing retrieval-augmented reader models via token elimination",
    "citation_count": 0,
    "authors": [
      "Moshe Berchansky",
      "Peter Izsak",
      "Avi Caciularu",
      "Ido Dagan",
      "Moshe Wasserblat"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.94": {
    "title": "WSDMS: Debunk Fake News via Weakly Supervised Detection of Misinforming Sentences with Contextualized Social Wisdom",
    "volume": "main",
    "abstract": "Fake news debunking primarily focuses on determining the truthfulness of news articles, which oversimplifies the issue as fake news often combines elements of both truth and falsehood. Thus, it becomes crucial to identify specific instances of misinformation within the articles. In this research, we investigate a novel task in the field of fake news debunking, which involves detecting sentence-level misinformation. One of the major challenges in this task is the absence of a training dataset with sentence-level annotations regarding veracity. Inspired by the Multiple Instance Learning (MIL) approach, we propose a model called Weakly Supervised Detection of Misinforming Sentences (WSDMS). This model only requires bag-level labels for training but is capable of inferring both sentence-level misinformation and article-level veracity, aided by relevant social media conversations that are attentively contextualized with news sentences. We evaluate WSDMS on three real-world benchmarks and demonstrate that it outperforms existing state-of-the-art baselines in debunking fake news at both the sentence and article levels",
    "checked": true,
    "id": "d5dd6c18097a91730de738024b53df7423138ff0",
    "semantic_title": "wsdms: debunk fake news via weakly supervised detection of misinforming sentences with contextualized social wisdom",
    "citation_count": 0,
    "authors": [
      "Ruichao Yang",
      "Wei Gao",
      "Jing Ma",
      "Hongzhan Lin",
      "Zhiwei Yang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.95": {
    "title": "Robust Prompt Optimization for Large Language Models Against Distribution Shifts",
    "volume": "main",
    "abstract": "Large Language Model (LLM) has demonstrated significant ability in various Natural Language Processing tasks. However, their effectiveness is highly dependent on the phrasing of the task prompt, leading to research on automatic prompt optimization using labeled task data. We reveal that these prompt optimization techniques are vulnerable to distribution shifts such as subpopulation shifts, which are common for LLMs in real-world scenarios such as customer reviews analysis. In this light, we propose a new problem of robust prompt optimization for LLMs against distribution shifts, which requires the prompt optimized over the labeled source group can simultaneously generalize to an unlabeled target group. To solve this problem, we propose Generalized Prompt Optimization framework , which incorporates the unlabeled data from the target group into prompt optimization. Extensive experimental results demonstrate the effectiveness of the proposed framework with significant performance improvement on the target group and comparable performance on the source group",
    "checked": true,
    "id": "3b0c49ca5ac0f441c302c9ca4def4804253552d5",
    "semantic_title": "robust prompt optimization for large language models against distribution shifts",
    "citation_count": 0,
    "authors": [
      "Moxin Li",
      "Wenjie Wang",
      "Fuli Feng",
      "Yixin Cao",
      "Jizhi Zhang",
      "Tat-Seng Chua"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.96": {
    "title": "Exploiting Asymmetry for Synthetic Training Data Generation: SynthIE and the Case of Information Extraction",
    "volume": "main",
    "abstract": "Large language models (LLMs) have great potential for synthetic data generation. This work shows that useful data can be synthetically generated even for tasks that cannot be solved directly by LLMs: for problems with structured outputs, it is possible to prompt an LLM to perform the task in the reverse direction, by generating plausible input text for a target output structure. Leveraging this asymmetry in task difficulty makes it possible to produce large-scale, high-quality data for complex tasks. We demonstrate the effectiveness of this approach on closed information extraction, where collecting ground-truth data is challenging, and no satisfactory dataset exists to date. We synthetically generate a dataset of 1.8M data points, establish its superior quality compared to existing datasets in a human evaluation, and use it to finetune small models (220M and 770M parameters), termed SynthIE, that outperform the prior state of the art (with equal model size) by a substantial margin of 57 absolute points in micro-F1 and 79 points in macro-F1. Code, data, and models are available at anonymous",
    "checked": true,
    "id": "f64e49d76048c902cc02e8ae27dcd4ac0dbcb97f",
    "semantic_title": "exploiting asymmetry for synthetic training data generation: synthie and the case of information extraction",
    "citation_count": 16,
    "authors": [
      "Martin Josifoski",
      "Marija Sakota",
      "Maxime Peyrard",
      "Robert West"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.97": {
    "title": "Condensing Multilingual Knowledge with Lightweight Language-Specific Modules",
    "volume": "main",
    "abstract": "Incorporating language-specific (LS) modules or Mixture-of-Experts (MoE) are proven methods to boost performance in multilingual model performance, but the scalability of these approaches to hundreds of languages or experts tends to be hard to manage. We present Language-specific Matrix Synthesis (LMS), a novel method that addresses the issue. LMS utilizes parameter-efficient and lightweight modules, reducing the number of parameters while outperforming existing methods, e.g., +1.73 BLEU over Switch Transformer on OPUS-100 multilingual translation. Additionally, we introduce Fuse Distillation (FD) to condense multilingual knowledge from multiple LS modules into a single shared module, improving model inference and storage efficiency. Our approach demonstrates superior scalability and performance compared to state-of-the-art methods",
    "checked": true,
    "id": "cf99558d35891d35ba5429cfce5ca2e2ae383b71",
    "semantic_title": "condensing multilingual knowledge with lightweight language-specific modules",
    "citation_count": 0,
    "authors": [
      "Haoran Xu",
      "Weiting Tan",
      "Shuyue Li",
      "Yunmo Chen",
      "Benjamin Van Durme",
      "Philipp Koehn",
      "Kenton Murray"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.98": {
    "title": "The Framework Tax: Disparities Between Inference Efficiency in NLP Research and Deployment",
    "volume": "main",
    "abstract": "Increased focus on the computational efficiency of systems in natural language processing has motivated the design of efficient model architectures and improvements to underlying hardware accelerators. However, the resulting increases in computational throughput and reductions in floating point operations have not directly translated to improvements in wall-clock inference latency. We demonstrate that these discrepancies can be largely attributed to bottlenecks introduced by deep learning frameworks. We denote this phenomena as the framework tax, and observe that the disparity is growing as hardware speed increases over time. In this work, we examine this phenomena through a series of case studies analyzing the effects of model design decisions, framework paradigms, and hardware platforms on total model latency. Based on our findings, we provide actionable recommendations to researchers and practitioners aimed at narrowing the gap between efficient NLP model research and practice",
    "checked": false,
    "id": "b777aa86b5a1d49ce8eababc5c2ee56d3562801e",
    "semantic_title": "the framework tax: disparities between inference efficiency in research and deployment",
    "citation_count": 2,
    "authors": [
      "Jared Fernandez",
      "Jacob Kahn",
      "Clara Na",
      "Yonatan Bisk",
      "Emma Strubell"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.99": {
    "title": "Evaluating Cross-Domain Text-to-SQL Models and Benchmarks",
    "volume": "main",
    "abstract": "Text-to-SQL benchmarks play a crucial role in evaluating the progress made in the field and the ranking of different models. However, accurately matching a model-generated SQL query to a reference SQL query in a benchmark fails for various reasons, such as underspecified natural language queries, inherent assumptions in both model-generated and reference queries, and the non-deterministic nature of SQL output under certain conditions. In this paper, we conduct an extensive study of several prominent cross-domain text-to-SQL benchmarks and re-evaluate some of the top-performing models within these benchmarks, by both manually evaluating the SQL queries and rewriting them in equivalent expressions. Our evaluation reveals that attaining a perfect performance on these benchmarks is unfeasible due to the multiple interpretations that can be derived from the provided samples. Furthermore, we find that the true performance of the models is underestimated and their relative performance changes after a re-evaluation. Most notably, our evaluation reveals a surprising discovery: a recent GPT4-based model surpasses the gold standard reference queries in the Spider benchmark in our human evaluation. This finding highlights the importance of interpreting benchmark evaluations cautiously, while also acknowledging the critical role of additional independent evaluations in driving advancements in the field",
    "checked": true,
    "id": "f245db3cf43c960d58990e7118b0239c13952578",
    "semantic_title": "evaluating cross-domain text-to-sql models and benchmarks",
    "citation_count": 0,
    "authors": [
      "Mohammadreza Pourreza",
      "Davood Rafiei"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.100": {
    "title": "Increasing Coverage and Precision of Textual Information in Multilingual Knowledge Graphs",
    "volume": "main",
    "abstract": "Recent work in Natural Language Processing and Computer Vision has been using textual information – e.g., entity names and descriptions – available in knowledge graphs to ground neural models to high-quality structured data. However, when it comes to non-English languages, the quantity and quality of textual information are comparatively scarce. To address this issue, we introduce the novel task of automatic Knowledge Graph Completion (KGE) and perform a thorough investigation on bridging the gap in both the quantity and quality of textual information between English and non-English languages. More specifically, we: i) bring to light the problem of increasing multilingual coverage and precision of entity names and descriptions in Wikidata; ii) demonstrate that state-of-the-art methods, namely, Machine Translation (MT), Web Search (WS), and Large Language Models (LLMs), struggle with this task; iii) present M-NTA, a novel unsupervised approach that combines MT, WS, and LLMs to generate high-quality textual information; and, iv) study the impact of increasing multilingual coverage and precision of non-English textual information in Entity Linking, Knowledge Graph Completion, and Question Answering. As part of our effort towards better multilingual knowledge graphs, we also introduce WikiKGE-10, the first human-curated benchmark to evaluate KGE approaches in 10 languages across 7 language families",
    "checked": true,
    "id": "8d24e6680a19c2f4c113e45145ec067130069805",
    "semantic_title": "increasing coverage and precision of textual information in multilingual knowledge graphs",
    "citation_count": 0,
    "authors": [
      "Simone Conia",
      "Min Li",
      "Daniel Lee",
      "Umar Minhas",
      "Ihab Ilyas",
      "Yunyao Li"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.101": {
    "title": "Memory-Based Invariance Learning for Out-of-Domain Text Classification",
    "volume": "main",
    "abstract": "We investigate the task of out-of-domain (OOD) text classification with the aim of extending a classification model, trained on multiple source domains, to an unseen target domain. Recent studies have shown that learning invariant representations can enhance the performance of OOD generalization. However, the inherent disparity in data distribution across different domains poses challenges for achieving effective invariance learning. This study addresses this issue by employing memory augmentations. Specifically, we augment the original feature space using key-value memory and employ a meta-learning-based approach to enhance the quality of the invariant representations. Experimental results on sentiment analysis and natural language inference tasks show the effectiveness of memory-based method for invariance learning, leading to state-of-the-art performance on six datasets",
    "checked": true,
    "id": "e2b71b8487379bbace9eedd9452bfc3689f58035",
    "semantic_title": "memory-based invariance learning for out-of-domain text classification",
    "citation_count": 0,
    "authors": [
      "Chen Jia",
      "Yue Zhang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.102": {
    "title": "Outlier Suppression+: Accurate quantization of large language models by equivalent and effective shifting and scaling",
    "volume": "main",
    "abstract": "Post-training quantization (PTQ) of transformer language models faces significant challenges due to the existence of detrimental outliers in activations. We observe that these outliers are concentrated in specific channels and are asymmetric across channels. To address this issue, we propose the Outlier Suppression+ (OS+) framework, which contains the channel-wise shifting for asymmetry and channel-wise scaling for concentration. We show that these operations can be seamlessly migrated into subsequent modules while maintaining equivalence. Second, we propose a fast and stable scheme to calculate effective shifting and scaling values. The channel-wise shifting aligns the center of each channel for removal of outlier asymmetry. The channel-wise scaling quantitatively evaluates changes brought by migration and quantization for better quantization burden balance. We validate our OS+ under both standard and fine-grained quantization settings with models including BERT, OPT, BLOOM, BLOOMZ, and LLaMA. Comprehensive results across various tasks demonstrate the superiority of our approach. Especially, with standard quantization, OS+ can achieve near-floating-point performance on both small models and large language models on 8-bit and 6-bit. Besides, we establish a new state-of-the-art for 4-bit BERT with 15.5% improvement. Our code is available at https://github.com/ModelTC/Outlier_Suppression_Plus",
    "checked": true,
    "id": "f3275146eb973c9726dc550a88cc552f0dfa5ea7",
    "semantic_title": "outlier suppression+: accurate quantization of large language models by equivalent and effective shifting and scaling",
    "citation_count": 0,
    "authors": [
      "Xiuying Wei",
      "Yunchen Zhang",
      "Yuhang Li",
      "Xiangguo Zhang",
      "Ruihao Gong",
      "Jinyang Guo",
      "Xianglong Liu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.103": {
    "title": "Three Stream Based Multi-level Event Contrastive Learning for Text-Video Event Extraction",
    "volume": "main",
    "abstract": "Text-video based multimodal event extraction refers to identifying event information from the given text-video pairs. Existing methods predominantly utilize video appearance features (VAF) and text sequence features (TSF) as input information. Some of them employ contrastive learning to align VAF with the event types extracted from TSF. However, they disregard the motion representations in videos and the optimization of contrastive objective could be misguided by the background noise from RGB frames. We observe that the same event triggers correspond to similar motion trajectories, which are hardly affected by the background noise. Moviated by this, we propose a Three Stream Multimodal Event Extraction framework (TSEE) that simultaneously utilizes the features of text sequence and video appearance, as well as the motion representations to enhance the event extraction capacity. Firstly, we extract the optical flow features (OFF) as motion representations from videos to incorporate with VAF and TSF. Then we introduce a Multi-level Event Contrastive Learning module to align the embedding space between OFF and event triggers, as well as between event triggers and types. Finally, a Dual Querying Text module is proposed to enhance the interaction between modalities. Experimental results show that TSEE outperforms the state-of-the-art methods, which demonstrates its superiority",
    "checked": true,
    "id": "f8c7e5e4fd5e1fe5bf9328b2a3bb4d3ab37f0d42",
    "semantic_title": "three stream based multi-level event contrastive learning for text-video event extraction",
    "citation_count": 0,
    "authors": [
      "Jiaqi Li",
      "Chuanyi Zhang",
      "Miaozeng Du",
      "Dehai Min",
      "Yongrui Chen",
      "Guilin Qi"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.104": {
    "title": "Diversify Question Generation with Retrieval-Augmented Style Transfer",
    "volume": "main",
    "abstract": "Given a textual passage and an answer, humans are able to ask questions with various expressions, but this ability is still challenging for most question generation (QG) systems. Existing solutions mainly focus on the internal knowledge within the given passage or the semantic word space for diverse content planning. These methods, however, have not considered the potential of external knowledge for expression diversity. To bridge this gap, we propose RAST, a framework for Retrieval-Augmented Style Transfer, where the objective is to utilize the style of diverse templates for question generation. For training RAST, we develop a novel Reinforcement Learning (RL) based approach that maximizes a weighted combination of diversity reward and consistency reward. Here, the consistency reward is computed by a Question-Answering (QA) model, whereas the diversity reward measures how much the final output mimics the retrieved template. Experimental results show that our method outperforms previous diversity-driven baselines on diversity while being comparable in terms of consistency scores. Our code is available at https://github.com/gouqi666/RAST",
    "checked": true,
    "id": "e17109b4ca2688753948c02e37912834b5c76fad",
    "semantic_title": "diversify question generation with retrieval-augmented style transfer",
    "citation_count": 0,
    "authors": [
      "Qi Gou",
      "Zehua Xia",
      "Bowen Yu",
      "Haiyang Yu",
      "Fei Huang",
      "Yongbin Li",
      "Nguyen Cam-Tu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.105": {
    "title": "Fast and Accurate Factual Inconsistency Detection Over Long Documents",
    "volume": "main",
    "abstract": "Generative AI models exhibit remarkable potential; however, hallucinations across various tasks present a significant challenge, particularly for longer inputs that current approaches struggle to address effectively. We introduce SCALE (Source Chunking Approach for Large-scale inconsistency Evaluation), a task-agnostic model for detecting factual inconsistencies using a novel chunking strategy. Specifically, SCALE is a Natural Language Inference (NLI) based model that uses large text chunks to condition over long texts. This approach achieves state-of-the-art performance in factual inconsistency detection for diverse tasks and long inputs. Additionally, we leverage the chunking mechanism and employ a novel algorithm to explain SCALE's decisions through relevant source sentence retrieval. Our evaluations reveal that SCALE outperforms existing methods on both standard benchmarks and a new long-form dialogue dataset ScreenEval we constructed. Moreover, SCALE surpasses competitive systems in efficiency and model explanation evaluations. We have released our code and data publicly to GitHub",
    "checked": true,
    "id": "c65867e127e39f03eda29ca5d82c298fad29d0b3",
    "semantic_title": "fast and accurate factual inconsistency detection over long documents",
    "citation_count": 1,
    "authors": [
      "Barrett Lattimer",
      "Patrick CHen",
      "Xinyuan Zhang",
      "Yi Yang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.106": {
    "title": "Interpreting Embedding Spaces by Conceptualization",
    "volume": "main",
    "abstract": "One of the main methods for computational interpretation of a text is mapping it into a vector in some embedding space. Such vectors can then be used for a variety of textual processing tasks. Recently, most embedding spaces are a product of training large language models (LLMs). One major drawback of this type of representation is their incomprehensibility to humans. Understanding the embedding space is crucial for several important needs, including the need to debug the embedding method and compare it to alternatives, and the need to detect biases hidden in the model. In this paper, we present a novel method of understanding embeddings by transforming a latent embedding space into a comprehensible conceptual space. We present an algorithm for deriving a conceptual space with dynamic on-demand granularity. We devise a new evaluation method, using either human rater or LLM-based raters, to show that the conceptualized vectors indeed represent the semantics of the original latent ones. We show the use of our method for various tasks, including comparing the semantics of alternative models and tracing the layers of the LLM. The code is available online https://github.com/adiSimhi/Interpreting-Embedding-Spaces-by-Conceptualization",
    "checked": true,
    "id": "3961d7525852c26ec5928323a54fc3a6a036110f",
    "semantic_title": "interpreting embedding spaces by conceptualization",
    "citation_count": 0,
    "authors": [
      "Adi Simhi",
      "Shaul Markovitch"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.107": {
    "title": "Knowledge-Augmented Language Model Verification",
    "volume": "main",
    "abstract": "Recent Language Models (LMs) have shown impressive capabilities in generating texts with the knowledge internalized in parameters. Yet, LMs often generate the factually incorrect responses to the given queries, since their knowledge may be inaccurate, incomplete, and outdated. To address this problem, previous works propose to augment LMs with the knowledge retrieved from an external knowledge source. However, such approaches often show suboptimal text generation performance due to two reasons: 1) the model may fail to retrieve the knowledge relevant to the given query, or 2) the model may not faithfully reflect the retrieved knowledge in the generated text. To overcome these, we propose to verify the output and the knowledge of the knowledge-augmented LMs with a separate verifier, which is a small LM that is trained to detect those two types of errors through instruction-finetuning. Then, when the verifier recognizes an error, we can rectify it by either retrieving new knowledge or generating new text. Further, we use an ensemble of the outputs from different instructions with a single verifier to enhance the reliability of the verification processes. We validate the effectiveness of the proposed verification steps on multiple question answering benchmarks, whose results show that the proposed verifier effectively identifies retrieval and generation errors, allowing LMs to provide more factually correct outputs. Our code is available at https://github.com/JinheonBaek/KALMV",
    "checked": true,
    "id": "8236c7cb26222b98af6dbcb15f9166f957b6ea67",
    "semantic_title": "knowledge-augmented language model verification",
    "citation_count": 0,
    "authors": [
      "Jinheon Baek",
      "Soyeong Jeong",
      "Minki Kang",
      "Jong Park",
      "Sung Hwang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.108": {
    "title": "A Generation-based Deductive Method for Math Word Problems",
    "volume": "main",
    "abstract": "Math word problems (MWP) involving advanced operators such as linear equation solver cannot be easily tackled by earlier MWP methods, because the existing generation methods suffer from repeated sub-expression generation and deductive methods are restricted to dealing with binary operations. This paper propose a new multivariate directed acyclic graph (mDAG) as an alternative to the generation methods' binary expression tree or the deductive methods' binary directed acyclic graph. Then to produce the topological ordering of mDAG, we propose a generation-based deductive (GeDe) model, which equips a generation model with a re-encoder to keep the deductive property but avoid the expensive enumeration of the deductive methods. GeDe performs well on math problems with many operators on the widely used benchmarks as well as solving multivariate operators on our own CMWPA benchmark. Our code is available at https://github.com/hyx1999/GeDe",
    "checked": true,
    "id": "2f9221877030c28cf98f0847ff8b8e787377b9a6",
    "semantic_title": "a generation-based deductive method for math word problems",
    "citation_count": 0,
    "authors": [
      "Yuxuan Hu",
      "Jing Zhang",
      "Haoyang Li",
      "Cuiping Li",
      "Hong Chen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.109": {
    "title": "Failures Pave the Way: Enhancing Large Language Models through Tuning-free Rule Accumulation",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have showcased impressive performance. However, due to their inability to capture relationships among samples, these frozen LLMs inevitably keep repeating similar mistakes. In this work, we propose our Tuning-free Rule Accumulation (TRAN) framework, which guides LLMs in improving their performance by learning from previous mistakes. Considering data arrives sequentially, LLMs gradually accumulate rules from incorrect cases, forming a rule collection. These rules are then utilized by the LLMs to avoid making similar mistakes when processing subsequent inputs. Moreover, the rules remain independent of the primary prompts, seamlessly complementing prompt design strategies. Experimentally, we show that TRAN improves over recent baselines by a large margin",
    "checked": true,
    "id": "ab90b84b42d43c3077c374cd34b3a48a881faf43",
    "semantic_title": "failures pave the way: enhancing large language models through tuning-free rule accumulation",
    "citation_count": 0,
    "authors": [
      "Zeyuan Yang",
      "Peng Li",
      "Yang Liu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.110": {
    "title": "Building Persona Consistent Dialogue Agents with Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "Maintaining a consistent persona is a key quality for any open domain dialogue system. Current state-of-the-art systems do this by training agents with supervised learning or online reinforcement learning (RL). However, systems trained with supervised learning often lack consistency as they are never punished for uttering contradictions. Additional training with RL can alleviate some of these issues, however the training process is expensive. Instead, we propose an offline RL framework to improve the persona consistency of dialogue systems. Our framework allows us to combine the advantages of previous methods as we can inexpensively train our model on existing data as in supervised learning, while punishing and rewarding specific utterances as in RL. We also introduce a simple importance sampling method to reduce the variance of importance weights in offline RL training which we call Variance-Reducing MLE-Initialized (VaRMI) importance sampling. Our automatic and human evaluations show that our framework improves both the persona consistency and dialogue quality of a state-of-the-art social chatbot",
    "checked": true,
    "id": "45e7d1b24344a54bf1201aa6df369e2f2fc15a04",
    "semantic_title": "building persona consistent dialogue agents with offline reinforcement learning",
    "citation_count": 0,
    "authors": [
      "Ryan Shea",
      "Zhou Yu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.111": {
    "title": "Augmenting Zero-Shot Dense Retrievers with Plug-in Mixture-of-Memories",
    "volume": "main",
    "abstract": "In this paper we improve the zero-shot generalization ability of language models via Mixture-Of-Memory Augmentation (MoMA), a mechanism that retrieves augmentation documents from multiple information corpora (external memories), with the option to \"plug in\" unseen memory at inference time. We develop a joint learning mechanism that trains the augmentation component with latent labels derived from the end retrieval task, paired with hard negatives from the memory mixture. We instantiate the model in a zero-shot dense retrieval setting by augmenting strong T5-based retrievers with MoMA. With only T5-base, our model obtains strong zero-shot retrieval accuracy on the eighteen tasks included in the standard BEIR benchmark, outperforming some systems with larger model sizes. As a plug-in-play model, our model can efficiently generalize to any unseen corpus, meanwhile achieving comparable or even better performance than methods relying on target-specific pretraining. Our analysis further illustrates the necessity of augmenting with mixture-of-memory for robust generalization, the benefits of augmentation learning, and how MoMA utilizes the plug-in memory at inference time without changing its parameters. Our code can be found at https://github.com/gesy17/MoMA",
    "checked": true,
    "id": "e0401ca2d4fd6d0ed55130a4a24b33ed90111479",
    "semantic_title": "augmenting zero-shot dense retrievers with plug-in mixture-of-memories",
    "citation_count": 3,
    "authors": [
      "Suyu Ge",
      "Chenyan Xiong",
      "Corby Rosset",
      "Arnold Overwijk",
      "Jiawei Han",
      "Paul Bennett"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.112": {
    "title": "Active Instruction Tuning: Improving Cross-Task Generalization by Training on Prompt Sensitive Tasks",
    "volume": "main",
    "abstract": "Instruction tuning (IT) achieves impressive zero-shot generalization results by training large language models (LLMs) on a massive amount of diverse tasks with instructions. However, how to select new tasks to improve the performance and generalizability of IT models remains an open question. Training on all existing tasks is impractical due to prohibiting computation requirements, and randomly selecting tasks can lead to suboptimal performance. In this work, we propose active instruction tuning based on prompt uncertainty, a novel framework to identify informative tasks, and then actively tune the models on the selected tasks. We represent the informativeness of new tasks with the disagreement of the current model outputs over perturbed prompts. Our experiments on NIV2 and Self-Instruct datasets demonstrate that our method consistently outperforms other baseline strategies for task selection, achieving better out-of-distribution generalization with fewer training tasks. Additionally, we introduce a task map that categorizes and diagnoses tasks based on prompt uncertainty and prediction probability. We discover that training on ambiguous (prompt-uncertain) tasks improves generalization while training on difficult (prompt-certain and low-probability) tasks offers no benefit, underscoring the importance of task selection for instruction tuning",
    "checked": true,
    "id": "590954e15e247cc343710ee97e396ad99f52970f",
    "semantic_title": "active instruction tuning: improving cross-task generalization by training on prompt sensitive tasks",
    "citation_count": 1,
    "authors": [
      "Po-Nien Kung",
      "Fan Yin",
      "Di Wu",
      "Kai-Wei Chang",
      "Nanyun Peng"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.113": {
    "title": "Towards Example-Based NMT with Multi-Levenshtein Transformers",
    "volume": "main",
    "abstract": "Retrieval-Augmented Machine Translation (RAMT) is attracting growing attention. This is because RAMT not only improves translation metrics, but is also assumed to implement some form of domain adaptation. In this contribution, we study another salient trait of RAMT, its ability to make translation decisions more transparent by allowing users to go back to examples that contributed to these decisions. For this, we propose a novel architecture aiming to increase this transparency. This model adapts a retrieval-augmented version of the Levenshtein Transformer and makes it amenable to simultaneously edit multiple fuzzy matches found in memory. We discuss how to perform training and inference in this model, based on multi-way alignment algorithms and imitation learning. Our experiments show that editing several examples positively impacts translation scores, notably increasing the number of target spans that are copied from existing instances",
    "checked": true,
    "id": "cbb452e4834ddd186df585082d28cbfa94434c39",
    "semantic_title": "towards example-based nmt with multi-levenshtein transformers",
    "citation_count": 0,
    "authors": [
      "Maxime Bouthors",
      "Josep Crego",
      "François Yvon"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.114": {
    "title": "DUnE: Dataset for Unified Editing",
    "volume": "main",
    "abstract": "Even the most advanced language models remain susceptible to errors necessitating to modify these models without initiating a comprehensive retraining process. Model editing refers to the modification of a model's knowledge or representations in a manner that produces the desired outcomes. Prior research primarily centered around editing factual data e.g. \"Messi plays for Inter Miami\" confining the definition of an edit to a knowledge triplet i.e. (subject, object, relation). However, as the applications of language models expand, so do the diverse ways in which we wish to edit and refine their outputs. In this study, we broaden the scope of the editing problem to include an array of editing cases such as debiasing and rectifying reasoning errors and define an edit as any natural language expression that solicits a change in the model's outputs. We are introducing DUnE, an editing benchmark where edits are natural language sentences and propose that DUnE presents a challenging yet relevant task. To substantiate this claim, we conduct an extensive series of experiments testing various editing approaches to address DUnE, demonstrating their respective strengths and weaknesses. We argue that retrieval-augmented language modeling can outperform specialized editing techniques and neither set of approaches has fully solved the generalized editing problem covered by our benchmark",
    "checked": true,
    "id": "647d64679501b9c161d39f9872d255b8dec95def",
    "semantic_title": "dune: dataset for unified editing",
    "citation_count": 0,
    "authors": [
      "Afra Akyürek",
      "Eric Pan",
      "Garry Kuwanto",
      "Derry Wijaya"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.115": {
    "title": "Fifty Shades of Bias\": Normative Ratings of Gender Bias in GPT Generated English Text",
    "volume": "main",
    "abstract": "Language serves as a powerful tool for the manifestation of societal belief systems. In doing so, it also perpetuates the prevalent biases in our society. Gender bias is one of the most pervasive biases in our society and is seen in online and offline discourses. With LLMs increasingly gaining human-like fluency in text generation, gaining a nuanced understanding of the biases these systems can generate is imperative. Prior work often treats gender bias as a binary classification task. However, acknowledging that bias must be perceived at a relative scale; we investigate the generation and consequent receptivity of manual annotators to bias of varying degrees. Specifically, we create the first dataset of GPT-generated English text with normative ratings of gender bias. Ratings were obtained using Best–Worst Scaling – an efficient comparative annotation framework. Next, we systematically analyze the variation of themes of gender biases in the observed ranking and show that identity-attack is most closely related to gender bias. Finally, we show the performance of existing automated models trained on related concepts on our dataset",
    "checked": true,
    "id": "aa988cf6414025df4ea411ed885f2ea42b25a111",
    "semantic_title": "fifty shades of bias\": normative ratings of gender bias in gpt generated english text",
    "citation_count": 0,
    "authors": [
      "Rishav Hada",
      "Agrima Seth",
      "Harshita Diddee",
      "Kalika Bali"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.116": {
    "title": "Hybrid Inverted Index Is a Robust Accelerator for Dense Retrieval",
    "volume": "main",
    "abstract": "Inverted file structure is a common technique for accelerating dense retrieval. It clusters documents based on their embeddings; during searching, it probes nearby clusters w.r.t. an input query and only evaluates documents within them by subsequent codecs, thus avoiding the expensive cost from exhaustive traversal. However, the clustering is always lossy, which results in the miss of relevant documents in the probed clusters and hence degrades retrieval quality. In contrast, lexical matching, such as overlaps of salient terms, tend to be strong features for identifying relevant documents. In this work, we present the Hybrid Inverted Index (HI2), where the embedding clusters and salient terms work collaboratively to accelerate dense retrieval. To make best of both effectiveness and efficiency, we devise a cluster selector and a term selector, to construct compact inverted lists and efficiently searching through them. Moreover, we leverage simple unsupervised algorithms as well as end-to-end knowledge distillation to learn these two modules, with the latter further boosting the effectiveness. Based on comprehensive experiments on popular retrieval benchmarks, we verify that clusters and terms indeed complement each other, enabling HI2 to achieve lossless retrieval quality with competitive efficiency across a variety of index settings",
    "checked": true,
    "id": "0e0ce140f33223e645d2132445a0ed72a1eb8f2d",
    "semantic_title": "hybrid inverted index is a robust accelerator for dense retrieval",
    "citation_count": 0,
    "authors": [
      "Peitian Zhang",
      "Zheng Liu",
      "Shitao Xiao",
      "Zhicheng Dou",
      "Jing Yao"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.117": {
    "title": "ChatGPT to Replace Crowdsourcing of Paraphrases for Intent Classification: Higher Diversity and Comparable Model Robustness",
    "volume": "main",
    "abstract": "The emergence of generative large language models (LLMs) raises the question: what will be its impact on crowdsourcing? Traditionally, crowdsourcing has been used for acquiring solutions to a wide variety of human-intelligence tasks, including ones involving text generation, modification or evaluation. For some of these tasks, models like ChatGPT can potentially substitute human workers. In this study, we investigate whether this is the case for the task of paraphrase generation for intent classification. We apply data collection methodology of an existing crowdsourcing study (similar scale, prompts and seed data) using ChatGPT and Falcon-40B. We show that ChatGPT-created paraphrases are more diverse and lead to at least as robust models",
    "checked": true,
    "id": "ef2241b1e36dfa160ae3ca51f88cec77ae358a71",
    "semantic_title": "chatgpt to replace crowdsourcing of paraphrases for intent classification: higher diversity and comparable model robustness",
    "citation_count": 4,
    "authors": [
      "Jan Cegin",
      "Jakub Simko",
      "Peter Brusilovsky"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.118": {
    "title": "Query-as-context Pre-training for Dense Passage Retrieval",
    "volume": "main",
    "abstract": "Recently, methods have been developed to improve the performance of dense passage retrieval by using context-supervised pre-training. These methods simply consider two passages from the same document to be relevant, without taking into account the potential negative impacts of weakly correlated pairs. Thus, this paper proposes query-as-context pre-training, a simple yet effective pre-training technique to alleviate the issue. Query-as-context pre-training assumes that the query derived from a passage is more likely to be relevant to that passage and forms a passage-query pair. These passage-query pairs are then used in contrastive or generative context-supervised pre-training. The pre-trained models are evaluated on large-scale passage retrieval benchmarks and out-of-domain zero-shot benchmarks. Experimental results show that query-as-context pre-training brings considerable gains for retrieval performances, demonstrating its effectiveness and efficiency",
    "checked": true,
    "id": "2889bf7968b6d5b30b58e66425fb453e1308a5b6",
    "semantic_title": "query-as-context pre-training for dense passage retrieval",
    "citation_count": 3,
    "authors": [
      "Xing W",
      "Guangyuan Ma",
      "Wanhui Qian",
      "Zijia Lin",
      "Songlin Hu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.119": {
    "title": "A Suite of Generative Tasks for Multi-Level Multimodal Webpage Understanding",
    "volume": "main",
    "abstract": "Webpages have been a rich, scalable resource for vision-language and language only tasks. Yet only pieces of webpages are kept in existing datasets: image-caption pairs, long text articles, or raw HTML, never all in one place. Webpage tasks have resultingly received little attention and structured image-text data left underused. To study multimodal webpage understanding, we introduce the Wikipedia Webpage suite (WikiWeb2M) containing 2M pages with all of the associated image, text, and structure data. We verify its utility on three generative tasks: page description generation, section summarization, and contextual image captioning. We design a novel attention mechanism Prefix Global, which selects the most relevant image and text content as global tokens to attend to the rest of the webpage for context. By using page structure to separate such tokens, it performs better than full attention with lower computational complexity. Extensive experiments show that the new data in WikiWeb2M improves task performance compared to prior work",
    "checked": true,
    "id": "173f0ba1105373ebf0981f9b5ee30893c43e015d",
    "semantic_title": "a suite of generative tasks for multi-level multimodal webpage understanding",
    "citation_count": 2,
    "authors": [
      "Andrea Burns",
      "Krishna Srinivasan",
      "Joshua Ainslie",
      "Geoff Brown",
      "Bryan Plummer",
      "Kate Saenko",
      "Jianmo Ni",
      "Mandy Guo"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.120": {
    "title": "Democratizing Reasoning Ability: Tailored Learning from Large Language Model",
    "volume": "main",
    "abstract": "Large language models (LLMs) exhibit impressive emergent abilities in natural language processing, but their democratization is hindered due to huge computation requirements and closed-source nature. Recent research on advancing open-source smaller LMs by distilling knowledge from black-box LLMs has obtained promising results in the instruction-following ability. However, the reasoning ability which is more challenging to foster, is relatively rarely explored. In this paper, we propose a tailored learning approach to distill such reasoning ability to smaller LMs to facilitate the democratization of the exclusive reasoning ability. In contrast to merely employing LLM as a data annotator, we exploit the potential of LLM as a reasoning teacher by building an interactive multi-round learning paradigm. This paradigm enables the student to expose its deficiencies to the black-box teacher who then can provide customized training data in return. Further, to exploit the reasoning potential of the smaller LM, we propose self-reflection learning to motivate the student to learn from self-made mistakes. The learning from self-reflection and LLM are all tailored to the student's learning status, thanks to the seamless integration with the multi-round learning paradigm. Comprehensive experiments and analysis on mathematical and commonsense reasoning tasks demonstrate the effectiveness of our method. The code will be available at https://github.com/Raibows/Learn-to-Reason",
    "checked": true,
    "id": "be7dbac2bcaed4cd034a7371004a011933e1bdca",
    "semantic_title": "democratizing reasoning ability: tailored learning from large language model",
    "citation_count": 1,
    "authors": [
      "Zhaoyang Wang",
      "Shaohan Huang",
      "Yuxuan Liu",
      "Jiahai Wang",
      "Minghui Song",
      "Zihan Zhang",
      "Haizhen Huang",
      "Furu Wei",
      "Weiwei Deng",
      "Feng Sun",
      "Qi Zhang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.121": {
    "title": "OpenAsp: A Benchmark for Multi-document Open Aspect-based Summarization",
    "volume": "main",
    "abstract": "The performance of automatic summarization models has improved dramatically in recent years. Yet, there is still a gap in meeting specific information needs of users in real-world scenarios, particularly when a targeted summary is sought, such as in the useful aspect-based summarization setting targeted in this paper. Previous datasets and studies for this setting have predominantly concentrated on a limited set of pre-defined aspects, focused solely on single document inputs, or relied on synthetic data. To advance research on more realistic scenarios, we introduce OpenAsp, a benchmark for multi-document open aspect-based summarization. This benchmark is created using a novel and cost-effective annotation protocol, by which an open aspect dataset is derived from existing generic multi-document summarization datasets. We analyze the properties of OpenAsp showcasing its high-quality content. Further, we show that the realistic open-aspect setting realized in OpenAsp poses a challenge for current state-of-the-art summarization models, as well as for large language models",
    "checked": true,
    "id": "14749a353cf9651de57256c063cd575e13f8caf8",
    "semantic_title": "openasp: a benchmark for multi-document open aspect-based summarization",
    "citation_count": 0,
    "authors": [
      "Shmuel Amar",
      "Liat Schiff",
      "Ori Ernst",
      "Asi Shefer",
      "Ori Shapira",
      "Ido Dagan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.122": {
    "title": "PEFTDebias : Capturing debiasing information using PEFTs",
    "volume": "main",
    "abstract": "The increasing use of foundation models highlights the urgent need to address and eliminate implicit biases present in them that arise during pretraining. In this paper, we introduce PEFTDebias, a novel approach that employs parameter-efficient fine-tuning (PEFT) to mitigate the biases within foundation models. PEFTDebias consists of two main phases: an upstream phase for acquiring debiasing parameters along a specific bias axis, and a downstream phase where these parameters are incorporated into the model and frozen during the fine-tuning process. By evaluating on four datasets across two bias axes namely gender and race, we find that downstream biases can be effectively reduced with PEFTs. In addition, we show that these parameters possess axis-specific debiasing characteristics, enabling their effective transferability in mitigating biases in various downstream tasks",
    "checked": true,
    "id": "258d94a650b399dbcb16bf7b811c2c1edd1c2102",
    "semantic_title": "peftdebias : capturing debiasing information using pefts",
    "citation_count": 0,
    "authors": [
      "Sumit Agarwal",
      "Aditya Veerubhotla",
      "Srijan Bansal"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.123": {
    "title": "Byte Pair Encoding for Symbolic Music",
    "volume": "main",
    "abstract": "When used with deep learning, the symbolic music modality is often coupled with language model architectures. To do so, the music needs to be tokenized, i.e. converted into a sequence of discrete tokens. This can be achieved by different approaches, as music can be composed of simultaneous tracks, of simultaneous notes with several attributes. Until now, the proposed tokenizations rely on small vocabularies of tokens describing the note attributes and time events, resulting in fairly long token sequences, and a sub-optimal use of the embedding space of language models. Recent research has put efforts on reducing the overall sequence length by merging embeddings or combining tokens. In this paper, we show that Byte Pair Encoding, a compression technique widely used for natural language, significantly decreases the sequence length while increasing the vocabulary size. By doing so, we leverage the embedding capabilities of such models with more expressive tokens, resulting in both better results and faster inference in generation and classification tasks. The [source code is shared on Github](https://github.com/Natooz/bpe-symbolic-music), along with a [companion website](https://Natooz.github.io/BPE-Symbolic-Music). Finally, BPE is directly implemented in [MidiTok](https://github.com/Natooz/MidiTok), allowing the reader to easily benefit from this method",
    "checked": true,
    "id": "7ffb0e79f38690d6b76ea81be46873f590723571",
    "semantic_title": "byte pair encoding for symbolic music",
    "citation_count": 4,
    "authors": [
      "Nathan Fradet",
      "Nicolas Gutowski",
      "Fabien Chhel",
      "Jean-Pierre Briot"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.124": {
    "title": "Combining Denoising Autoencoders with Contrastive Learning to fine-tune Transformer Models",
    "volume": "main",
    "abstract": "Recently, using large pre-trained Transformer models for transfer learning tasks has evolved to the point where they have become one of the flagship trends in the Natural Language Processing (NLP) community, giving rise to various outlooks such as prompt-based, adapters, or combinations with unsupervised approaches, among many others. In this work, we propose a 3-Phase technique to adjust a base model for a classification task. First, we adapt the model's signal to the data distribution by performing further training with a Denoising Autoencoder (DAE). Second, we adjust the representation space of the output to the corresponding classes by clustering through a Contrastive Learning (CL) method. In addition, we introduce a new data augmentation approach for Supervised Contrastive Learning to correct the unbalanced datasets. Third, we apply fine-tuning to delimit the predefined categories. These different phases provide relevant and complementary knowledge to the model to learn the final task. We supply extensive experimental results on several datasets to demonstrate these claims. Moreover, we include an ablation study and compare the proposed method against other ways of combining these techniques",
    "checked": true,
    "id": "e9579e82c728f5e6a4b074e1199fa03365d574e3",
    "semantic_title": "combining denoising autoencoders with contrastive learning to fine-tune transformer models",
    "citation_count": 0,
    "authors": [
      "Alejo Lopez-Avila",
      "Víctor Suárez-Paniagua"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.125": {
    "title": "Self-Influence Guided Data Reweighting for Language Model Pre-training",
    "volume": "main",
    "abstract": "Language Models (LMs) pre-trained with selfsupervision on large text corpora have become the default starting point for developing models for various NLP tasks. Once the pre-training corpus has been assembled, all data samples in the corpus are treated with equal importance during LM pre-training. However, due to varying levels of relevance and quality of data, equal importance to all the data samples may not be the optimal choice. While data reweighting has been explored in the context of task-specific supervised learning and LM fine-tuning, model-driven reweighting for pretraining data has not been explored. We fill this important gap and propose PRESENCE, a method for jointly reweighting samples by leveraging self-influence (SI) scores as an indicator of sample importance and pre-training. PRESENCE promotes novelty and stability for model pre-training. Through extensive analysis spanning multiple model sizes, datasets, and tasks, we present PRESENCE as an important first step in the research direction of sample reweighting for pre-training language models",
    "checked": true,
    "id": "b39b47ea594fe2365a055b6f0ca0a5915e88c6de",
    "semantic_title": "self-influence guided data reweighting for language model pre-training",
    "citation_count": 1,
    "authors": [
      "Megh Thakkar",
      "Tolga Bolukbasi",
      "Sriram Ganapathy",
      "Shikhar Vashishth",
      "Sarath Chandar",
      "Partha Talukdar"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.126": {
    "title": "ACTOR: Active Learning with Annotator-specific Classification Heads to Embrace Human Label Variation",
    "volume": "main",
    "abstract": "Label aggregation such as majority voting is commonly used to resolve annotator disagreement in dataset creation. However, this may disregard minority values and opinions. Recent studies indicate that learning from individual annotations outperforms learning from aggregated labels, though they require a considerable amount of annotation. Active learning, as an annotation cost-saving strategy, has not been fully explored in the context of learning from disagreement. We show that in the active learning setting, a multi-head model performs significantly better than a single-head model in terms of uncertainty estimation. By designing and evaluating acquisition functions with annotator-specific heads on two datasets, we show that group-level entropy works generally well on both datasets. Importantly, it achieves performance in terms of both prediction and uncertainty estimation comparable to full-scale training from disagreement, while saving 70% of the annotation budget",
    "checked": true,
    "id": "bc5ef93ceb086307fae0d95c9f2e417cd4f1c3f1",
    "semantic_title": "actor: active learning with annotator-specific classification heads to embrace human label variation",
    "citation_count": 0,
    "authors": [
      "Xinpeng Wang",
      "Barbara Plank"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.127": {
    "title": "TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models",
    "volume": "main",
    "abstract": "Factual consistency evaluation is often conducted using Natural Language Inference (NLI) models, yet these models exhibit limited success in evaluating summaries. Previous work improved such models with synthetic training data. However, the data is typically based on perturbed human-written summaries, which often differ in their characteristics from real model-generated summaries and have limited coverage of possible factual errors. Alternatively, large language models (LLMs) have recently shown promising results in directly evaluating generative tasks, but are too computationally expensive for practical use. Motivated by these limitations, we introduce TrueTeacher, a method for generating synthetic data by annotating diverse model-generated summaries using a LLM. Unlike prior work, TrueTeacher does not rely on human-written summaries, and is multilingual by nature. Experiments on the TRUE benchmark show that a student model trained using our data, substantially outperforms both the state-of-the-art model with similar capacity, and the LLM teacher. In a systematic study, we compare TrueTeacher to existing synthetic data generation methods and demonstrate its superiority and robustness to domain-shift. We also show that our method generalizes to multilingual scenarios. Lastly, we release our large scale synthetic dataset (1.4M examples), generated using TrueTeacher, and a checkpoint trained on this data",
    "checked": true,
    "id": "d2d16333a4b0dc7e3463b280b9945e5ee6c53396",
    "semantic_title": "trueteacher: learning factual consistency evaluation with large language models",
    "citation_count": 13,
    "authors": [
      "Zorik Gekhman",
      "Jonathan Herzig",
      "Roee Aharoni",
      "Chen Elkind",
      "Idan Szpektor"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.128": {
    "title": "VivesDebate-Speech: A Corpus of Spoken Argumentation to Leverage Audio Features for Argument Mining",
    "volume": "main",
    "abstract": "In this paper, we describe VivesDebate-Speech, a corpus of spoken argumentation created to leverage audio features for argument mining tasks. The creation of this corpus represents an important contribution to the intersection of speech processing and argument mining communities, and one of the most complete publicly available resources in this topic. Moreover, we have performed a set of first-of-their-kind experiments which show an improvement when integrating audio features into the argument mining pipeline. The provided results can be used as a baseline for future research",
    "checked": true,
    "id": "b0522f20f4897dcc692130843ac3615956ee2ee1",
    "semantic_title": "vivesdebate-speech: a corpus of spoken argumentation to leverage audio features for argument mining",
    "citation_count": 0,
    "authors": [
      "Ramon Ruiz-Dolz",
      "Javier Sanchez"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.129": {
    "title": "Tagging-Assisted Generation Model with Encoder and Decoder Supervision for Aspect Sentiment Triplet Extraction",
    "volume": "main",
    "abstract": "ASTE (Aspect Sentiment Triplet Extraction) has gained increasing attention. Recent advancements in the ASTE task have been primarily driven by Natural Language Generation-based (NLG) approaches. However, most NLG methods overlook the supervision of the encoder-decoder hidden representations and fail to fully utilize the semantic information provided by the labels to enhance supervision. These limitations can hinder the extraction of implicit aspects and opinions. To address these challenges, we propose a tagging-assisted generation model with encoder and decoder supervision (TAGS), which enhances the supervision of the encoder and decoder through multiple-perspective tagging assistance and label semantic representations. Specifically, TAGS enhances the generation task by integrating an additional sequence tagging task, which improves the encoder's capability to distinguish the words of triplets. Moreover, it utilizes sequence tagging probabilities to guide the decoder, improving the generated content's quality. Furthermore, TAGS employs a self-decoding process for labels to acquire the semantic representations of the labels and aligns the decoder's hidden states with these semantic representations, thereby achieving enhanced semantic supervision for the decoder's hidden states. Extensive experiments on various public benchmarks demonstrate that TAGS achieves state-of-the-art performance",
    "checked": true,
    "id": "825d4b42a57fb07efce5ef6042dea09c8a9ffb5b",
    "semantic_title": "tagging-assisted generation model with encoder and decoder supervision for aspect sentiment triplet extraction",
    "citation_count": 0,
    "authors": [
      "Luo Xianlong",
      "Meng Yang",
      "Yihao Wang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.130": {
    "title": "Larger Probes Tell a Different Story: Extending Psycholinguistic Datasets Via In-Context Learning",
    "volume": "main",
    "abstract": "Language model probing is often used to test specific capabilities of models. However, conclusions from such studies may be limited when the probing benchmarks are small and lack statistical power. In this work, we introduce new, larger datasets for negation (NEG-1500-SIMP) and role reversal (ROLE-1500) inspired by psycholinguistic studies. We dramatically extend existing NEG-136 and ROLE-88 benchmarks using GPT3, increasing their size from 18 and 44 sentence pairs to 750 each. We also create another version of extended negation dataset (NEG-1500-SIMP-TEMP), created using template-based generation. It consists of 770 sentence pairs. We evaluate 22 models on the extended datasets, seeing model performance dip 20-57% compared to the original smaller benchmarks. We observe high levels of negation sensitivity in models like BERT and ALBERT demonstrating that previous findings might have been skewed due to smaller test sets. Finally, we observe that while GPT3 has generated all the examples in ROLE-1500 is only able to solve 24.6% of them during probing. The datasets and code are available on Github",
    "checked": true,
    "id": "10e5d2f2e8a59686567ae0d2d0ce741cea9e9ea3",
    "semantic_title": "larger probes tell a different story: extending psycholinguistic datasets via in-context learning",
    "citation_count": 0,
    "authors": [
      "Namrata Shivagunde",
      "Vladislav Lialin",
      "Anna Rumshisky"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.131": {
    "title": "Norm of Word Embedding Encodes Information Gain",
    "volume": "main",
    "abstract": "Distributed representations of words encode lexical semantic information, but what type of information is encoded and how? Focusing on the skip-gram with negative-sampling method, we found that the squared norm of static word embedding encodes the information gain conveyed by the word; the information gain is defined by the Kullback-Leibler divergence of the co-occurrence distribution of the word to the unigram distribution. Our findings are explained by the theoretical framework of the exponential family of probability distributions and confirmed through precise experiments that remove spurious correlations arising from word frequency. This theory also extends to contextualized word embeddings in language models or any neural networks with the softmax output layer. We also demonstrate that both the KL divergence and the squared norm of embedding provide a useful metric of the informativeness of a word in tasks such as keyword extraction, proper-noun discrimination, and hypernym discrimination",
    "checked": true,
    "id": "f686665626e65523d6f14892029a962199af4ca9",
    "semantic_title": "norm of word embedding encodes information gain",
    "citation_count": 3,
    "authors": [
      "Momose Oyama",
      "Sho Yokoi",
      "Hidetoshi Shimodaira"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.132": {
    "title": "CRT-QA: A Dataset of Complex Reasoning Question Answering over Tabular Data",
    "volume": "main",
    "abstract": "Large language models (LLMs) show powerful reasoning abilities on various text-based tasks. However, their reasoning capability on structured data such as tables has not been systematically explored. In this work, we first establish a comprehensive taxonomy of reasoning and operation types for tabular data analysis. Then, we construct a complex reasoning QA dataset over tabular data, named CRT-QA dataset (Complex Reasoning QA over Tabular data), with the following unique features: (1) it is the first Table QA dataset with multi-step operation and informal reasoning; (2) it contains fine-grained annotations on questions' directness, composition types of sub-questions, and human reasoning paths which can be used to conduct a thorough investigation on LLMs' reasoning ability; (3) it contains a collection of unanswerable and indeterminate questions that commonly arise in real-world situations. We further introduce an efficient and effective tool-augmented method, named ARC (Auto-exemplar-guided Reasoning with Code), to use external tools such as Pandas to solve table reasoning tasks without handcrafted demonstrations. The experiment results show that CRT-QA presents a strong challenge for baseline methods and ARC achieves the best result",
    "checked": true,
    "id": "a38dc45c9f74e600bab0b1a6b9ff2d9ba59cd3d5",
    "semantic_title": "crt-qa: a dataset of complex reasoning question answering over tabular data",
    "citation_count": 0,
    "authors": [
      "Zhehao Zhang",
      "Xitao Li",
      "Yan Gao",
      "Jian-Guang Lou"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.133": {
    "title": "Promoting Topic Coherence and Inter-Document Consorts in Multi-Document Summarization via Simplicial Complex and Sheaf Graph",
    "volume": "main",
    "abstract": "Multi-document Summarization (MDS) characterizes compressing information from multiple source documents to its succinct summary. An ideal summary should encompass all topics and accurately model cross-document relations expounded upon in the source documents. However, existing systems either impose constraints on the length of tokens during the encoding or falter in capturing the intricate cross-document relationships. These limitations impel the systems to produce summaries that are non-factual and unfaithful, thereby imparting an unfair comprehension of the topic to the readers. To counter these limitations and promote the information equivalence between the source document and generated summary, we propose FIBER, a novel encoder-decoder model that uses pre-trained BART to comprehensively analyze linguistic nuances, simplicial complex layer to apprehend inherent properties that transcend pairwise associations and sheaf graph attention to effectively capture the heterophilic properties. We benchmark FIBER with eleven baselines over four widely-used MDS datasets – Multinews, CQASumm, DUC and Opinosis, and show that FIBER achieves consistent performance improvement across all the evaluation metrics (syntactical, semantical and faithfulness). We corroborate these improvements further through qualitative human evaluation",
    "checked": true,
    "id": "90812d2cd9e1697821c4b4014f44f5bb1873bffe",
    "semantic_title": "promoting topic coherence and inter-document consorts in multi-document summarization via simplicial complex and sheaf graph",
    "citation_count": 0,
    "authors": [
      "Yash Atri",
      "Arun Iyer",
      "Tanmoy Chakraborty",
      "Vikram Goyal"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.134": {
    "title": "MAGNIFICo: Evaluating the In-Context Learning Ability of Large Language Models to Generalize to Novel Interpretations",
    "volume": "main",
    "abstract": "Humans possess a remarkable ability to assign novel interpretations to linguistic expressions, enabling them to learn new words and understand community-specific connotations. However, Large Language Models (LLMs) have a knowledge cutoff and are costly to finetune repeatedly. Therefore, it is crucial for LLMs to learn novel interpretations in-context. In this paper, we systematically analyse the ability of LLMs to acquire novel interpretations using in-context learning. To facilitate our study, we introduce MAGNIFICo, an evaluation suite implemented within a text-to-SQL semantic parsing framework that incorporates diverse tokens and prompt settings to simulate real-world complexity. Experimental results on MAGNIFICo demonstrate that LLMs exhibit a surprisingly robust capacity for comprehending novel interpretations from natural language descriptions as well as from discussions within long conversations. Nevertheless, our findings also highlight the need for further improvements, particularly when interpreting unfamiliar words or when composing multiple novel interpretations simultaneously in the same example. Additionally, our analysis uncovers the semantic predispositions in LLMs and reveals the impact of recency bias for information presented in long contexts",
    "checked": true,
    "id": "0577ca1b6f8d9cddbad7f76ea7f82dc71b5af043",
    "semantic_title": "magnifico: evaluating the in-context learning ability of large language models to generalize to novel interpretations",
    "citation_count": 0,
    "authors": [
      "Arkil Patel",
      "Satwik Bhattamishra",
      "Siva Reddy",
      "Dzmitry Bahdanau"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.135": {
    "title": "Generating and Evaluating Tests for K-12 Students with Language Model Simulations: A Case Study on Sentence Reading Efficiency",
    "volume": "main",
    "abstract": "Developing an educational test can be expensive and time-consuming, as each item must be written by experts and then evaluated by collecting hundreds of student responses. Moreover, many tests require multiple distinct sets of questions administered throughout the school year to closely monitor students' progress, known as parallel tests. In this study, we focus on tests of silent sentence reading efficiency, used to assess students' reading ability over time. To generate high-quality parallel tests, we propose to fine-tune large language models (LLMs) to simulate how previous students would have responded to unseen items. With these simulated responses, we can estimate each item's difficulty and ambiguity. We first use GPT-4 to generate new test items following a list of expert-developed rules and then apply a fine-tuned LLM to filter the items based on criteria from psychological measurements. We also propose an optimal-transport-inspired technique for generating parallel tests and show the generated tests closely correspond to the original test's difficulty and reliability based on crowdworker responses. Our evaluation of a generated test with 234 students from grades 2 to 8 produces test scores highly correlated (r=0.93) to those of a standard test form written by human experts and evaluated across thousands of K-12 students",
    "checked": true,
    "id": "1c882af20986251bab90318c7e1fae93e0c12cf6",
    "semantic_title": "generating and evaluating tests for k-12 students with language model simulations: a case study on sentence reading efficiency",
    "citation_count": 0,
    "authors": [
      "Eric Zelikman",
      "Wanjing Ma",
      "Jasmine Tran",
      "Diyi Yang",
      "Jason Yeatman",
      "Nick Haber"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.136": {
    "title": "Counter Turing Test (CT2): AI-Generated Text Detection is Not as Easy as You May Think - Introducing AI Detectability Index (ADI)",
    "volume": "main",
    "abstract": "With the rise of prolific ChatGPT, the risk and consequences of AI-generated text has increased alarmingly. This triggered a series of events, including an open letter, signed by thousands of researchers and tech leaders in March 2023, demanding a six-month moratorium on the training of AI systems more sophisticated than GPT-4. To address the inevitable question of ownership attribution for AI-generated artifacts, the US Copyright Office released a statement stating that \"if the content is traditional elements of authorship produced by a machine, the work lacks human authorship and the office will not register it for copyright\". Furthermore, both the US and the EU governments have recently drafted their initial proposals regarding the regulatory framework for AI. Given this cynosural spotlight on generative AI, AI-generated text detection (AGTD) has emerged as a topic that has already received immediate attention in research, with some initial methods having been proposed, soon followed by the emergence of techniques to bypass detection. This paper introduces the Counter Turing Test (CT2), a benchmark consisting of techniques aiming to offer a comprehensive evaluation of the robustness of existing AGTD techniques. Our empirical findings unequivocally highlight the fragility of the proposed AGTD methods under scrutiny. Amidst the extensive deliberations on policy-making for regulating AI development, it is of utmost importance to assess the detectability of content generated by LLMs. Thus, to establish a quantifiable spectrum facilitating the evaluation and ranking of LLMs according to their detectability levels, we propose the AI Detectability Index (ADI). We conduct a thorough examination of 15 contemporary LLMs, empirically demonstrating that larger LLMs tend to have a lower ADI, indicating they are less detectable compared to smaller LLMs. We firmly believe that ADI holds significant value as a tool for the wider NLP community, with the potential to serve as a rubric in AI-related policy-making",
    "checked": false,
    "id": "e55d12cb0e92aa5f4f8ca5dc24d57bd0dcc861ae",
    "semantic_title": "counter turing test ct^2: ai-generated text detection is not as easy as you may think - introducing ai detectability index",
    "citation_count": 1,
    "authors": [
      "Megha Chakraborty",
      "S.M Towhidul Islam Tonmoy",
      "S M Mehedi Zaman",
      "Shreya Gautam",
      "Tanay Kumar",
      "Krish Sharma",
      "Niyar Barman",
      "Chandan Gupta",
      "Vinija Jain",
      "Aman Chadha",
      "Amit Sheth",
      "Amitava Das"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.137": {
    "title": "Revisiting the Optimality of Word Lengths",
    "volume": "main",
    "abstract": "Zipf (1935) posited that wordforms are optimized to minimize utterances' communicative costs. Under the assumption that cost is given by an utterance's length, he supported this claim by showing that words' lengths are inversely correlated with their frequencies. Communicative cost, however, can be operationalized in different ways. Piantadosi et al. (2011) claim that cost should be measured as the distance between an utterance's information rate and channel capacity, which we dub the channel capacity hypothesis (CCH) here. Following this logic, they then proposed that a word's length should be proportional to the expected value of its surprisal (negative log-probability in context). In this work, we show that Piantadosi et al.'s derivation does not minimize CCH's cost, but rather a lower bound, which we term CCH-lower. We propose a novel derivation, suggesting an improved way to minimize CCH's cost. Under this method, we find that a language's word lengths should instead be proportional to the surprisal's expectation plus its variance-to-mean ratio. Experimentally, we compare these three communicative cost functions: Zipf's, CCH-lower , and CCH. Across 13 languages and several experimental settings, we find that length is better predicted by frequency than either of the other hypotheses. In fact, when surprisal's expectation, or expectation plus variance-to-mean ratio, is estimated using better language models, it leads to worse word length predictions. We take these results as evidence that Zipf's longstanding hypothesis holds",
    "checked": true,
    "id": "46c971d1c01c19332fbae1cd08d7e60f36eb51cd",
    "semantic_title": "revisiting the optimality of word lengths",
    "citation_count": 1,
    "authors": [
      "Tiago Pimentel",
      "Clara Meister",
      "Ethan Wilcox",
      "Kyle Mahowald",
      "Ryan Cotterell"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.138": {
    "title": "Document-level Relationship Extraction by Bidirectional Constraints of Beta Rules",
    "volume": "main",
    "abstract": "Document-level Relation Extraction (DocRE) aims to extract relations among entity pairs in documents. Some works introduce logic constraints into DocRE, addressing the issues of opacity and weak logic in original DocRE models. However, they only focus on forward logic constraints and the rules mined in these works often suffer from pseudo rules with high standard-confidence but low support. In this paper, we proposes Bidirectional Constraints of Beta Rules(BCBR), a novel logic constraint framework. BCBR first introduces a new rule miner which model rules by beta contribtion. Then forward and reverse logic constraints are constructed based on beta rules. Finally, BCBR reconstruct rule consistency loss by bidirectional constraints to regulate the output of the DocRE model. Experiments show that BCBR outperforms original DocRE models in terms of relation extraction performance (~2.7 F1 score) and logical consistency(~3.1 logic score). Furthermore, BCBR consistently outperforms two other logic constraint frameworks",
    "checked": true,
    "id": "dfc6b25bea2fe20ab3bf1c6dfb26d73f63409330",
    "semantic_title": "document-level relationship extraction by bidirectional constraints of beta rules",
    "citation_count": 0,
    "authors": [
      "Yichun Liu",
      "Zizhong Zhu",
      "Xiaowang Zhang",
      "Zhiyong Feng",
      "Daoqi Chen",
      "Yaxin Li"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.139": {
    "title": "Instructed Language Models with Retrievers Are Powerful Entity Linkers",
    "volume": "main",
    "abstract": "Generative approaches powered by large language models (LLMs) have demonstrated emergent abilities in tasks that require complex reasoning abilities. Yet the generative nature still makes the generated content suffer from hallucinations, thus unsuitable for entity-centric tasks like entity linking (EL) requiring precise entity predictions over a large knowledge base. We present Instructed Generative Entity Linker (INSGENEL), the first approach that enables casual language models to perform entity linking over knowledge bases. Several methods of equipping language models with EL ability were proposed in this work, including (i) a sequence-to-sequence training EL objective with instruction-tuning, (ii) a novel generative EL framework based on a light-weight potential mention retriever that frees the model from heavy and non-parallelizable decoding, achieving 4× speedup without compromise on linking metrics. INSGENEL outperforms previous generative alternatives with +6.8 F1 points gain on average, also with a huge advantage in training data efficiency and training compute consumption. In addition, our skillfully-engineered in-context learning (ICL) framework for EL still lags behind INSGENEL significantly, reaffirming that the EL task remains a persistent hurdle for general LLMs",
    "checked": true,
    "id": "6974eeca4aa3e2fc904364ac7b3eca9cbdf9ca7c",
    "semantic_title": "instructed language models with retrievers are powerful entity linkers",
    "citation_count": 0,
    "authors": [
      "Zilin Xiao",
      "Ming Gong",
      "Jie Wu",
      "Xingyao Zhang",
      "Linjun Shou",
      "Daxin Jiang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.140": {
    "title": "Towards Noise-Tolerant Speech-Referring Video Object Segmentation: Bridging Speech and Text",
    "volume": "main",
    "abstract": "Linguistic communication is prevalent in Human-Computer Interaction (HCI). Speech (spoken language) serves as a convenient yet potentially ambiguous form due to noise and accents, exposing a gap compared to text. In this study, we investigate the prominent HCI task, Referring Video Object Segmentation (R-VOS), which aims to segment and track objects using linguistic references. While text input is well-investigated, speech input is under-explored. Our objective is to bridge the gap between speech and text, enabling the adaptation of existing text-input R-VOS models to accommodate noisy speech input effectively. Specifically, we propose a method to align the semantic spaces between speech and text by incorporating two key modules: 1) Noise-Aware Semantic Adjustment (NSA) for clear semantics extraction from noisy speech; and 2) Semantic Jitter Suppression (SJS) enabling R-VOS models to tolerate noisy queries. Comprehensive experiments conducted on the challenging AVOS benchmarks reveal that our proposed method outperforms state-of-the-art approaches",
    "checked": true,
    "id": "37e8e07d3ecfa43a1e64d48202c73f597e6f9fee",
    "semantic_title": "towards noise-tolerant speech-referring video object segmentation: bridging speech and text",
    "citation_count": 0,
    "authors": [
      "Xiang Li",
      "Jinglu Wang",
      "Xiaohao Xu",
      "Muqiao Yang",
      "Fan Yang",
      "Yizhou Zhao",
      "Rita Singh",
      "Bhiksha Raj"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.141": {
    "title": "PROSE: A Pronoun Omission Solution for Chinese-English Spoken Language Translation",
    "volume": "main",
    "abstract": "Neural Machine Translation (NMT) systems encounter a significant challenge when translating a pro-drop (‘pronoun-dropping') language (e.g., Chinese) to a non-pro-drop one (e.g., English), since the pro-drop phenomenon demands NMT systems to recover omitted pronouns. This unique and crucial task, however, lacks sufficient datasets for benchmarking. To bridge this gap, we introduce PROSE, a new benchmark featured in diverse pro-drop instances for document-level Chinese-English spoken language translation. Furthermore, we conduct an in-depth investigation of the pro-drop phenomenon in spoken Chinese on this dataset, reconfirming that pro-drop reduces the performance of NMT systems in Chinese-English translation. To alleviate the negative impact introduced by pro-drop, we propose Mention-Aware Semantic Augmentation, a novel approach that leverages the semantic embedding of dropped pronouns to augment training pairs. Results from the experiments on four Chinese-English translation corpora show that our proposed method outperforms existing methods regarding omitted pronoun retrieval and overall translation quality",
    "checked": true,
    "id": "ffa99ce25cba44a1ada6af8a39f72697a03ccf24",
    "semantic_title": "prose: a pronoun omission solution for chinese-english spoken language translation",
    "citation_count": 0,
    "authors": [
      "Ke Wang",
      "Xiutian Zhao",
      "Yanghui Li",
      "Wei Peng"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.142": {
    "title": "A Diachronic Analysis of Paradigm Shifts in NLP Research: When, How, and Why?",
    "volume": "main",
    "abstract": "Understanding the fundamental concepts and trends in a scientific field is crucial for keeping abreast of its continuous advancement. In this study, we propose a systematic framework for analyzing the evolution of research topics in a scientific field using causal discovery and inference techniques. We define three variables to encompass diverse facets of the evolution of research topics within NLP and utilize a causal discovery algorithm to unveil the causal connections among these variables using observational data. Subsequently, we leverage this structure to measure the intensity of these relationships. By conducting extensive experiments on the ACL Anthology corpus, we demonstrate that our framework effectively uncovers evolutionary trends and the underlying causes for a wide range of NLP research topics. Specifically, we show that tasks and methods are primary drivers of research in NLP, with datasets following, while metrics have minimal impact",
    "checked": true,
    "id": "e9ef16c4537499904de8a0b7563e80f012af7aec",
    "semantic_title": "a diachronic analysis of paradigm shifts in nlp research: when, how, and why?",
    "citation_count": 1,
    "authors": [
      "Aniket Pramanick",
      "Yufang Hou",
      "Saif Mohammad",
      "Iryna Gurevych"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.143": {
    "title": "Does the Correctness of Factual Knowledge Matter for Factual Knowledge-Enhanced Pre-trained Language Models?",
    "volume": "main",
    "abstract": "In recent years, the injection of factual knowledge has been observed to have a significant positive correlation to the downstream task performance of pre-trained language models. However, existing work neither demonstrates that pre-trained models successfully learn the injected factual knowledge nor proves that there is a causal relation between injected factual knowledge and downstream performance improvements. In this paper, we introduce a counterfactual-based analysis framework to explore the causal effects of factual knowledge injection on the performance of language models within pretrain-finetune paradigm. Instead of directly probing the language model or exhaustively enumerating potential confounding factors, we analyze this issue by perturbing the factual knowledge sources at different scales and comparing the performance of pre-trained language models before and after the perturbation. Surprisingly, throughout our experiments, we find that although the knowledge seems to be successfully injected, the correctness of injected knowledge only has a very limited effect on the models' downstream performance. This finding strongly challenges previous assumptions that the injected factual knowledge is the key for language models to achieve performance improvements on downstream tasks in pretrain-finetune paradigm",
    "checked": true,
    "id": "159ae0f1d92177f8308224d6ddcbad5ddaf0d366",
    "semantic_title": "does the correctness of factual knowledge matter for factual knowledge-enhanced pre-trained language models?",
    "citation_count": 0,
    "authors": [
      "Boxi Cao",
      "Qiaoyu Tang",
      "Hongyu Lin",
      "Xianpei Han",
      "Le Sun"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.144": {
    "title": "Syntactic Substitutability as Unsupervised Dependency Syntax",
    "volume": "main",
    "abstract": "Syntax is a latent hierarchical structure which underpins the robust and compositional nature of human language. In this work, we explore the hypothesis that syntactic dependencies can be represented in language model attention distributions and propose a new method to induce these structures theory-agnostically. Instead of modeling syntactic relations as defined by annotation schemata, we model a more general property implicit in the definition of dependency relations, syntactic substitutability. This property captures the fact that words at either end of a dependency can be substituted with words from the same category. Substitutions can be used to generate a set of syntactically invariant sentences whose representations are then used for parsing. We show that increasing the number of substitutions used improves parsing accuracy on natural data. On long-distance subject-verb agreement constructions, our method achieves 79.5% recall compared to 8.9% using a previous method. Our method also provides improvements when transferred to a different parsing setup, demonstrating that it generalizes",
    "checked": true,
    "id": "18c94f47ac4fb6e1937ade9c8433782ac55c1576",
    "semantic_title": "syntactic substitutability as unsupervised dependency syntax",
    "citation_count": 0,
    "authors": [
      "Jasper Jian",
      "Siva Reddy"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.145": {
    "title": "MProto: Multi-Prototype Network with Denoised Optimal Transport for Distantly Supervised Named Entity Recognition",
    "volume": "main",
    "abstract": "Distantly supervised named entity recognition (DS-NER) aims to locate entity mentions and classify their types with only knowledge bases or gazetteers and unlabeled corpus. However, distant annotations are noisy and degrade the performance of NER models. In this paper, we propose a noise-robust prototype network named MProto for the DS-NER task. Different from previous prototype-based NER methods, MProto represents each entity type with multiple prototypes to characterize the intra-class variance among entity representations. To optimize the classifier, each token should be assigned an appropriate ground-truth prototype and we consider such token-prototype assignment as an optimal transport (OT) problem. Furthermore, to mitigate the noise from incomplete labeling, we propose a novel denoised optimal transport (DOT) algorithm. Specifically, we utilize the assignment result between *Other* class tokens and all prototypes to distinguish unlabeled entity tokens from true negatives. Experiments on several DS-NER benchmarks demonstrate that our MProto achieves state-of-the-art performance. The source code is now available on Github",
    "checked": true,
    "id": "bccc22f9f2f08e3e8490e341aea58b551592fedf",
    "semantic_title": "mproto: multi-prototype network with denoised optimal transport for distantly supervised named entity recognition",
    "citation_count": 0,
    "authors": [
      "Shuhui Wu",
      "Yongliang Shen",
      "Zeqi Tan",
      "Wenqi Ren",
      "Jietian Guo",
      "Shiliang Pu",
      "Weiming Lu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.146": {
    "title": "The Shifted and The Overlooked: A Task-oriented Investigation of User-GPT Interactions",
    "volume": "main",
    "abstract": "Recent progress in Large Language Models (LLMs) has produced models that exhibit remarkable performance across a variety of NLP tasks. However, it remains unclear whether the existing focus of NLP research accurately captures the genuine requirements of human users. This paper provides a comprehensive analysis of the divergence between academic research in NLP and the needs of real-world NLP applications via a large-scale collection of user-GPT conversations. We analyze a large-scale collection of real user queries to GPT. We compare these queries against existing NLP benchmark tasks and identify a significant gap between the tasks that users frequently request from LLMs and the tasks that are commonly studied in academic research. For example, we find that tasks such as \"design\" and \"planning\" are prevalent in user interactions but largely neglected or different from traditional NLP benchmarks. We investigate these overlooked tasks, dissect the practical challenges, and provide insights toward a roadmap to make LLMs better aligned with user needs",
    "checked": true,
    "id": "62454a3694e2e52b8698458440612505a3f7404b",
    "semantic_title": "the shifted and the overlooked: a task-oriented investigation of user-gpt interactions",
    "citation_count": 0,
    "authors": [
      "Siru Ouyang",
      "Shuohang Wang",
      "Yang Liu",
      "Ming Zhong",
      "Yizhu Jiao",
      "Dan Iter",
      "Reid Pryzant",
      "Chenguang Zhu",
      "Heng Ji",
      "Jiawei Han"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.147": {
    "title": "Learning the Visualness of Text Using Large Vision-Language Models",
    "volume": "main",
    "abstract": "Visual text evokes an image in a person's mind, while non-visual text fails to do so. A method to automatically detect visualness in text will enable text-to-image retrieval and generation models to augment text with relevant images. This is particularly challenging with long-form text as text-to-image generation and retrieval models are often triggered for text that is designed to be explicitly visual in nature, whereas long-form text could contain many non-visual sentences. To this end, we curate a dataset of 3,620 English sentences and their visualness scores provided by multiple human annotators. We also propose a fine-tuning strategy that adapts large vision-language models like CLIP by modifying the model's contrastive learning objective to map text identified as non-visual to a common NULL image while matching visual text to their corresponding images in the document. We evaluate the proposed approach on its ability to (i) classify visual and non-visual text accurately, and (ii) attend over words that are identified as visual in psycholinguistic studies. Empirical evaluation indicates that our approach performs better than several heuristics and baseline models for the proposed task. Furthermore, to highlight the importance of modeling the visualness of text, we conduct qualitative analyses of text-to-image generation systems like DALL-E",
    "checked": true,
    "id": "4f5a5a7f4f90fde2b23fc496ec7fc8d429d4bc5e",
    "semantic_title": "learning the visualness of text using large vision-language models",
    "citation_count": 0,
    "authors": [
      "Gaurav Verma",
      "Ryan Rossi",
      "Christopher Tensmeyer",
      "Jiuxiang Gu",
      "Ani Nenkova"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.148": {
    "title": "The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values",
    "volume": "main",
    "abstract": "Human feedback is increasingly used to steer the behaviours of Large Language Models (LLMs). However, it is unclear how to collect and incorporate feedback in a way that is efficient, effective and unbiased, especially for highly subjective human preferences and values. In this paper, we survey existing approaches for learning from human feedback, drawing on 95 papers primarily from the ACL and arXiv repositories. First, we summarise the past, pre-LLM trends for integrating human feedback into language models. Second, we give an overview of present techniques and practices, as well as the motivations for using feedback; conceptual frameworks for defining values and preferences; and how feedback is collected and from whom. Finally, we encourage a better future of feedback learning in LLMs by raising five unresolved conceptual and practical challenges",
    "checked": true,
    "id": "dd7a74a09fc29cadcd47fafc4f7812bb8d2d7208",
    "semantic_title": "the past, present and better future of feedback learning in large language models for subjective human preferences and values",
    "citation_count": 1,
    "authors": [
      "Hannah Kirk",
      "Andrew Bean",
      "Bertie Vidgen",
      "Paul Rottger",
      "Scott Hale"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.149": {
    "title": "TempTabQA: Temporal Question Answering for Semi-Structured Tables",
    "volume": "main",
    "abstract": "Semi-structured data, such as Infobox tables, often include temporal information about entities, either implicitly or explicitly. Can current NLP systems reason about such information in semi-structured tables? To tackle this question, we introduce the task of temporal question answering on semi-structured tables. We present a dataset, TEMPTABQA, which comprises 11,454 question-answer pairs extracted from 1,208 Wikipedia Infobox tables spanning more than 90 distinct domains. Using this dataset, we evaluate several state-of-the-art models for temporal reasoning. We observe that even the top-performing LLMs lag behind human performance by more than 13.5 F1 points. Given these results, our dataset has the potential to serve as a challenging benchmark to improve the temporal reasoning capabilities of NLP models",
    "checked": true,
    "id": "5fd41a098ad3108a6e0ea14f87961e42703d6807",
    "semantic_title": "temptabqa: temporal question answering for semi-structured tables",
    "citation_count": 0,
    "authors": [
      "Vivek Gupta",
      "Pranshu Kandoi",
      "Mahek Vora",
      "Shuo Zhang",
      "Yujie He",
      "Ridho Reinanda",
      "Vivek Srikumar"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.150": {
    "title": "Task-Level Thinking Steps Help Large Language Models for Challenging Classification Task",
    "volume": "main",
    "abstract": "Large language models (LLMs) have shown incredible performance on many tasks such as dialogue generation, commonsense reasoning and question answering. In-context learning (ICL) is an important paradigm for adapting LLMs to the downstream tasks by prompting few demonstrations. However, the distribution of demonstrations can severely affect the performance, especially for challenging classification tasks. In this paper, we propose the concept of task-level thinking steps that can eliminate bias introduced by demonstrations. Further, to help LLMs distinguish confusing classes, we design a progressive revision framework, which can improve the thinking steps by correcting hard demonstrations. Experimental results prove the superiority of our proposed method, achieving best performance on three kinds of challenging classification tasks in the zero-shot and few-shot settings. Besides, with task-level thinking steps, automatically generated chain-of-thoughts (CoTs) bring more competitive performance",
    "checked": true,
    "id": "c29a8bbc5fca651bed7cc080d02bb3766146137d",
    "semantic_title": "task-level thinking steps help large language models for challenging classification task",
    "citation_count": 0,
    "authors": [
      "Chunhui Du",
      "Jidong Tian",
      "Haoran Liao",
      "Jindou Chen",
      "Hao He",
      "Yaohui Jin"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.151": {
    "title": "RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation",
    "volume": "main",
    "abstract": "The task of repository-level code completion is to continue writing the unfinished code based on a broader context of the repository. While for automated code completion tools, it is difficult to utilize the useful information scattered in different files. We propose RepoCoder, a simple, generic, and effective framework to address the challenge. It streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model in an iterative retrieval-generation pipeline. RepoCoder makes effective utilization of repository-level information for code completion and has the ability to generate code at various levels of granularity. Moreover, we propose a new benchmark RepoBench, which consists of the latest and high-quality real-world repositories covering line, API invocation, and function body completion scenarios. Experimental results indicate that RepoCoder significantly improves the In-File completion baseline by over 10% in all settings and consistently outperforms the vanilla retrieval-augmented code completion approach. Furthermore, we validate the effectiveness of RepoCoder through comprehensive analysis, providing valuable insights for future research. Our source code and benchmark will be publicly available after the paper review",
    "checked": true,
    "id": "af5c7848417882012203ac21399977ebda695a2b",
    "semantic_title": "repocoder: repository-level code completion through iterative retrieval and generation",
    "citation_count": 25,
    "authors": [
      "Fengji Zhang",
      "Bei Chen",
      "Yue Zhang",
      "Jacky Keung",
      "Jin Liu",
      "Daoguang Zan",
      "Yi Mao",
      "Jian-Guang Lou",
      "Weizhu Chen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.152": {
    "title": "Influence Scores at Scale for Efficient Language Data Sampling",
    "volume": "main",
    "abstract": "Modern ML systems ingest data aggregated from diverse sources, such as synthetic, human-annotated, and live customer traffic. Understanding which examples are important to the performance of a learning algorithm is crucial for efficient model training. Recently, a growing body of literature has given rise to various \"influence scores,\" which use training artifacts such as model confidence or checkpointed gradients to identify important subsets of data. However, these methods have primarily been developed in computer vision settings, and it remains unclear how well they generalize to language-based tasks using pretrained models. In this paper, we explore the applicability of influence scores in language classification tasks. We evaluate a diverse subset of these scores on the SNLI dataset by quantifying accuracy changes in response to pruning training data through random and influence-score-based sampling. We then stress-test one of the scores – \"variance of gradients\" (VoG) from Agarwal and Hooker (2022) – in an NLU model stack that was exposed to dynamic user speech patterns in a voice assistant type of setting. Our experiments demonstrate that in many cases, encoder-based language models can be fine-tuned on roughly 50% of the original data without degradation in performance metrics. Along the way, we summarize lessons learned from applying out-of-the-box implementations of influence scores, quantify the effects of noisy and class-imbalanced data, and offer recommendations on score-based sampling for better accuracy and training efficiency",
    "checked": true,
    "id": "6e3c983e24a8699c8627868ce16d98410d461395",
    "semantic_title": "influence scores at scale for efficient language data sampling",
    "citation_count": 0,
    "authors": [
      "Nikhil Anand",
      "Joshua Tan",
      "Maria Minakova"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.153": {
    "title": "G-Eval: NLG Evaluation using Gpt-4 with Better Human Alignment",
    "volume": "main",
    "abstract": "The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-Eval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose analysis on the behavior of LLM-based evaluators, and highlight the potential concern of LLM-based evaluators having a bias towards the LLM-generated texts",
    "checked": true,
    "id": "381ab7a640f5b46b62f7e08d1af4a8e0d3eadd55",
    "semantic_title": "g-eval: nlg evaluation using gpt-4 with better human alignment",
    "citation_count": 204,
    "authors": [
      "Yang Liu",
      "Dan Iter",
      "Yichong Xu",
      "Shuohang Wang",
      "Ruochen Xu",
      "Chenguang Zhu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.154": {
    "title": "Learning Retrieval Augmentation for Personalized Dialogue Generation",
    "volume": "main",
    "abstract": "Personalized dialogue generation, focusing on generating highly tailored responses by leveraging persona profiles and dialogue context, has gained significant attention in conversational AI applications. However, persona profiles, a prevalent setting in current personalized dialogue datasets, typically composed of merely four to five sentences, may not offer comprehensive descriptions of the persona about the agent, posing a challenge to generate truly personalized dialogues. To handle this problem, we propose Learning Retrieval Augmentation for Personalized DialOgue Generation (LAPDOG), which studies the potential of leveraging external knowledge for persona dialogue generation. Specifically, the proposed LAPDOG model consists of a story retriever and a dialogue generator. The story retriever uses a given persona profile as queries to retrieve relevant information from the story document, which serves as a supplementary context to augment the persona profile. The dialogue generator utilizes both the dialogue history and the augmented persona profile to generate personalized responses. For optimization, we adopt a joint training framework that collaboratively learns the story retriever and dialogue generator, where the story retriever is optimized towards desired ultimate metrics (e.g., BLEU) to retrieve content for the dialogue generator to generate personalized responses. Experiments conducted on the CONVAI2 dataset with ROCStory as a supplementary data source show that the proposed LAPDOG method substantially outperforms the baselines, indicating the effectiveness of the proposed method. The LAPDOG model code is publicly available for further exploration",
    "checked": true,
    "id": "114dcd4be9eb6f5a7257fbfdb542cfab3153a292",
    "semantic_title": "learning retrieval augmentation for personalized dialogue generation",
    "citation_count": 0,
    "authors": [
      "Qiushi Huang",
      "Shuai Fu",
      "Xubo Liu",
      "Wenwu Wang",
      "Tom Ko",
      "Yu Zhang",
      "Lilian Tang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.155": {
    "title": "The Troubling Emergence of Hallucination in Large Language Models - An Extensive Definition, Quantification, and Prescriptive Remediations",
    "volume": "main",
    "abstract": "The recent advancements in Large Language Models (LLMs) have garnered widespread acclaim for their remarkable emerging capabilities. However, the issue of hallucination has parallelly emerged as a by-product, posing significant concerns. While some recent endeavors have been made to identify and mitigate different types of hallucination, there has been a limited emphasis on the nuanced categorization of hallucination and associated mitigation methods. To address this gap, we offer a fine-grained discourse on profiling hallucination based on its degree, orientation, and category, along with offering strategies for alleviation. As such, we define two overarching orientations of hallucination: (i) factual mirage (FM) and (ii) silver lining (SL). To provide a more comprehensive understanding, both orientations are further sub-categorized into intrinsic and extrinsic, with three degrees of severity - (i) mild, (ii) moderate, and (iii) alarming. We also meticulously categorize hallucination into six types: (i) acronym ambiguity, (ii) numeric nuisance, (iii) generated golem, (iv) virtual voice, (v) geographic erratum, and (vi) time wrap. Furthermore, we curate HallucInation eLiciTation (HILT), a publicly available dataset comprising of 75,000 samples generated using 15 contemporary LLMs along with human annotations for the aforementioned categories. Finally, to establish a method for quantifying and to offer a comparative spectrum that allows us to evaluate and rank LLMs based on their vulnerability to producing hallucinations, we propose Hallucination Vulnerability Index (HVI). Amidst the extensive deliberations on policy-making for regulating AI development, it is of utmost importance to assess and measure which LLM is more vulnerable towards hallucination. We firmly believe that HVI holds significant value as a tool for the wider NLP community, with the potential to serve as a rubric in AI-related policy-making. In conclusion, we propose two solution strategies for mitigating hallucinations",
    "checked": true,
    "id": "99bfe503743c5ec8e16e50ab8438159cdb533a89",
    "semantic_title": "the troubling emergence of hallucination in large language models - an extensive definition, quantification, and prescriptive remediations",
    "citation_count": 4,
    "authors": [
      "Vipula Rawte",
      "Swagata Chakraborty",
      "Agnibh Pathak",
      "Anubhav Sarkar",
      "S.M Towhidul Islam Tonmoy",
      "Aman Chadha",
      "Amit Sheth",
      "Amitava Das"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.156": {
    "title": "NAIL: Lexical Retrieval Indices with Efficient Non-Autoregressive Decoders",
    "volume": "main",
    "abstract": "Neural document rerankers are extremely effective in terms of accuracy. However, the best models require dedicated hardware for serving, which is costly and often not feasible. To avoid this servingtime requirement, we present a method of capturing up to 86% of the gains of a Transformer cross-attention model with a lexicalized scoring function that only requires 10-6% of the Transformer's FLOPs per document and can be served using commodity CPUs. When combined with a BM25 retriever, this approach matches the quality of a state-of-the art dual encoder retriever, that still requires an accelerator for query encoding. We introduce nail (Non-Autoregressive Indexing with Language models) as a model architecture that is compatible with recent encoder-decoder and decoder-only large language models, such as T5, GPT-3 and PaLM. This model architecture can leverage existing pre-trained checkpoints and can be fine-tuned for efficiently constructing document representations that do not require neural processing of queries",
    "checked": true,
    "id": "597ea1ce9c86b728871035d08941ad84d813b5c4",
    "semantic_title": "nail: lexical retrieval indices with efficient non-autoregressive decoders",
    "citation_count": 0,
    "authors": [
      "Livio Soares",
      "Daniel Gillick",
      "Jeremy Cole",
      "Tom Kwiatkowski"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.157": {
    "title": "Analyzing Modular Approaches for Visual Question Decomposition",
    "volume": "main",
    "abstract": "Modular neural networks without additional training have recently been shown to surpass end-to-end neural networks on challenging vision–language tasks. The latest such methods simultaneously introduce LLM-based code generation to build programs and a number of skill-specific, task-oriented modules to execute them. In this paper, we focus on ViperGPT and ask where its additional performance comes from and how much is due to the (state-of-art, end-to-end) BLIP-2 model it subsumes vs. additional symbolic components. To do so, we conduct a controlled study (comparing end-to-end, modular, and prompting-based methods across several VQA benchmarks). We find that ViperGPT's reported gains over BLIP-2 can be attributed to its selection of task-specific modules, and when we run ViperGPT using a more task-agnostic selection of modules, these gains go away. ViperGPT retains much of its performance if we make prominent alterations to its selection of modules: e.g. removing or retaining only BLIP-2. We also compare ViperGPT against a prompting-based decomposition strategy and find that, on some benchmarks, modular approaches significantly benefit by representing subtasks with natural language, instead of code. Our code is fully available at https://github.com/brown-palm/visual-question-decomposition",
    "checked": true,
    "id": "cf7d69709bdeddd561c183178bbc1f0c2e156a08",
    "semantic_title": "analyzing modular approaches for visual question decomposition",
    "citation_count": 0,
    "authors": [
      "Apoorv Khandelwal",
      "Ellie Pavlick",
      "Chen Sun"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.158": {
    "title": "Improving Summarization with Human Edits",
    "volume": "main",
    "abstract": "Recent work has shown the promise of learning with human feedback paradigms to produce human-determined high-quality text. Existing works use human feedback to train large language models (LLMs) in general domain abstractive summarization and have obtained summary quality exceeding traditional likelihood training. In this paper, we focus on a less explored form of human feedback – Human Edits. We propose Sequence Alignment (un)Likelihood Training (SALT), a novel technique to use both the human-edited and model-generated data together in the training loop. In addition, we demonstrate simulating Human Edits with ground truth summaries coming from existing training data – Imitation edits, along with the model-generated summaries obtained after the training, to reduce the need for expensive human-edit data. In our experiments, we extend human feedback exploration from general domain summarization to medical domain summarization. Our results demonstrate the effectiveness of SALT in improving the summary quality with Human and Imitation Edits. Through additional experiments, we show that SALT outperforms the conventional RLHF method (designed for human preferences) – DPO, when applied to human-edit data. We hope the evidence in our paper prompts researchers to explore, collect, and better use different human feedback approaches scalably",
    "checked": true,
    "id": "9cc0151d85c60e4413bf6c10cce6e251f10961ac",
    "semantic_title": "improving summarization with human edits",
    "citation_count": 2,
    "authors": [
      "Zonghai Yao",
      "Benjamin Schloss",
      "Sai Selvaraj"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.159": {
    "title": "Did You Mean...? Confidence-based Trade-offs in Semantic Parsing",
    "volume": "main",
    "abstract": "We illustrate how a calibrated model can help balance common trade-offs in task-oriented parsing. In a simulated annotator-in-the-loop experiment, we show that well-calibrated confidence scores allow us to balance cost with annotator load, improving accuracy with a small number of interactions. We then examine how confidence scores can help optimize the trade-off between usability and safety. We show that confidence-based thresholding can substantially reduce the number of incorrect low-confidence programs executed; however, this comes at a cost to usability. We propose the DidYouMean system which better balances usability and safety by rephrasing low-confidence inputs",
    "checked": true,
    "id": "695a24f4bd79293d7c4dc41ce3f86c66d601f930",
    "semantic_title": "did you mean...? confidence-based trade-offs in semantic parsing",
    "citation_count": 1,
    "authors": [
      "Elias Stengel-Eskin",
      "Benjamin Van Durme"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.160": {
    "title": "The Skipped Beat: A Study of Sociopragmatic Understanding in LLMs for 64 Languages",
    "volume": "main",
    "abstract": "Instruction tuned large language models (LLMs), such as ChatGPT, demonstrate remarkable performance in a wide range of tasks. Despite numerous recent studies that examine the performance of instruction-tuned LLMs on various NLP benchmarks, there remains a lack of comprehensive investigation into their ability to understand cross-lingual sociopragmatic meaning (SM), i.e., meaning embedded within social and interactive contexts. This deficiency arises partly from SM not being adequately represented in any of the existing benchmarks. To address this gap, we present SPARROW, an extensive multilingual benchmark specifically designed for SM understanding. SPARROW comprises 169 datasets covering 13 task types across six primary categories (e.g., anti-social language detection, emotion recognition). SPARROW datasets encompass 64 different languages originating from 12 language families representing 16 writing scripts. We evaluate the performance of various multilingual pretrained language models (e.g., mT5) and instruction-tuned LLMs (e.g., BLOOMZ, ChatGPT) on SPARROW through fine-tuning, zero-shot, and/or few-shot learning. Our comprehensive analysis reveals that existing open-source instruction tuned LLMs still struggle to understand SM across various languages, performing close to a random baseline in some cases. We also find that although ChatGPT outperforms many LLMs, it still falls behind task-specific finetuned models with a gap of 12.19 SPARROW score. Our benchmark is available at: https://github.com/UBC-NLP/SPARROW",
    "checked": true,
    "id": "c8a940f2015afad576d35e7a6916cc1a0cec169d",
    "semantic_title": "the skipped beat: a study of sociopragmatic understanding in llms for 64 languages",
    "citation_count": 0,
    "authors": [
      "Chiyu Zhang",
      "Khai Doan",
      "Qisheng Liao",
      "Muhammad Abdul-Mageed"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.161": {
    "title": "Understanding the Effect of Model Compression on Social Bias in Large Language Models",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) trained with self-supervision on vast corpora of web text fit to the social biases of that text. Without intervention, these social biases persist in the model's predictions in downstream tasks, leading to representational harm. Many strategies have been proposed to mitigate the effects of inappropriate social biases learned during pretraining. Simultaneously, methods for model compression have become increasingly popular to reduce the computational burden of LLMs. Despite the popularity and need for both approaches, little work has been done to explore the interplay between these two. We perform a carefully controlled study of the impact of model compression via quantization and knowledge distillation on measures of social bias in LLMs. Longer pretraining and larger models led to higher social bias, and quantization showed a regularizer effect with its best trade-off around 20% of the original pretraining time",
    "checked": true,
    "id": "45e50baac4d341f0cf1a40af096bfa9c3f555235",
    "semantic_title": "understanding the effect of model compression on social bias in large language models",
    "citation_count": 0,
    "authors": [
      "Gustavo Gonçalves",
      "Emma Strubell"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.162": {
    "title": "BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology",
    "volume": "main",
    "abstract": "The ability to automatically generate accurate protocols for scientific experiments would represent a major step towards the automation of science. Large Language Models (LLMs) have impressive capabilities on a wide range of tasks, such as question answering and the generation of coherent text and code. However, LLMs can struggle with multi-step problems and long-term planning, which are crucial for designing scientific experiments. Moreover, evaluation of the accuracy of scientific protocols is challenging, because experiments can be described correctly in many different ways, require expert knowledge to evaluate, and cannot usually be executed automatically. Here we present an automatic evaluation framework for the task of planning experimental protocols, and we introduce BioProt: a dataset of biology protocols with corresponding pseudocode representations. To measure performance on generating scientific protocols, we use an LLM to convert a natural language protocol into pseudocode, and then evaluate an LLM's ability to reconstruct the pseudocode from a high-level description and a list of admissible pseudocode functions. We evaluate GPT-3 and GPT-4 on this task and explore their robustness. We externally validate the utility of pseudocode representations of text by generating accurate novel protocols using retrieved pseudocode, and we run a generated protocol successfully in our biological laboratory. Our framework is extensible to the evaluation and improvement of language model",
    "checked": true,
    "id": "1efb48995732f58dbf2e251bfdf8571545033db9",
    "semantic_title": "bioplanner: automatic evaluation of llms on protocol planning in biology",
    "citation_count": 1,
    "authors": [
      "Odhran O’Donoghue",
      "Aleksandar Shtedritski",
      "John Ginger",
      "Ralph Abboud",
      "Ali Ghareeb",
      "Samuel Rodriques"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.163": {
    "title": "Cross-lingual Prompting: Improving Zero-shot Chain-of-Thought Reasoning across Languages",
    "volume": "main",
    "abstract": "Chain-of-thought (CoT) is capable of eliciting models to explicitly generate reasoning paths, thus promoting reasoning accuracy and attracting increasing attention. Specifically, zero-shot CoT achieves remarkable improvements in a wide range of reasoning tasks by simply instructing the LLM with the prompt \"Let's think step by step!\". Despite the success of zero-shot CoT, the existing zero-shot prompting techniques remain limited to a single language, making it challenging to generalize to other languages and hindering global development. In this work, we introduce cross-lingual prompting (CLP), aiming to improve zero-shot CoT reasoning across languages. Specifically, CLP consists of two main components: (1) cross-lingual alignment prompting and (2) task-specific solver prompting. The cross-lingual alignment prompting is responsible for aligning representations across different languages, whereas the task-specific solver prompting is used to generate the final chain of thoughts and results for the reasoning task. In addition, we further introduce cross-lingual self-consistent prompting (CLSP) to ensemble different reasoning paths across languages. Our experimental evaluations on several benchmarks demonstrate that CLP and CLSP significantly outperform the existing prompting methods and achieve state-of-the-art performance. We hope this work will inspire further breakthroughs in cross-lingual CoT",
    "checked": true,
    "id": "4a99be7d5e0fbbdb28914bd5e96df26949ecb75e",
    "semantic_title": "cross-lingual prompting: improving zero-shot chain-of-thought reasoning across languages",
    "citation_count": 1,
    "authors": [
      "Libo Qin",
      "Qiguang Chen",
      "Fuxuan Wei",
      "Shijue Huang",
      "Wanxiang Che"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.164": {
    "title": "FinGPT: Large Generative Models for a Small Language",
    "volume": "main",
    "abstract": "Large language models (LLMs) excel in many tasks in NLP and beyond, but most open models have very limited coverage of smaller languages and LLM work tends to focus on languages where nearly unlimited data is available for pretraining. In this work, we study the challenges of creating LLMs for Finnish, a language spoken by less than 0.1% of the world population. We compile an extensive dataset of Finnish combining web crawls, news, social media and eBooks. We pursue two approaches to pretrain models: 1) we train seven monolingual models from scratch (186M to 13B parameters) dubbed FinGPT, 2) we continue the pretraining of the multilingual BLOOM model on a mix of its original training data and Finnish, resulting in a 176 billion parameter model we call BLUUMI. For model evaluation, we introduce FIN-bench, a version of BIG-bench with Finnish tasks. We also assess other model qualities such as toxicity and bias. Our models and tools are openly available at https://turkunlp.org/gpt3-finnish",
    "checked": true,
    "id": "1a849f298dcd49fe99ebca6749c0564585aa3018",
    "semantic_title": "fingpt: large generative models for a small language",
    "citation_count": 1,
    "authors": [
      "Risto Luukkonen",
      "Ville Komulainen",
      "Jouni Luoma",
      "Anni Eskelinen",
      "Jenna Kanerva",
      "Hanna-Mari Kupari",
      "Filip Ginter",
      "Veronika Laippala",
      "Niklas Muennighoff",
      "Aleksandra Piktus",
      "Thomas Wang",
      "Nouamane Tazi",
      "Teven Scao",
      "Thomas Wolf",
      "Osma Suominen",
      "Samuli Sairanen",
      "Mikko Merioksa",
      "Jyrki Heinonen",
      "Aija Vahtola",
      "Samuel Antao",
      "Sampo Pyysalo"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.165": {
    "title": "Boosting Summarization with Normalizing Flows and Aggressive Training",
    "volume": "main",
    "abstract": "This paper presents FlowSUM, a normalizing flows-based variational encoder-decoder framework for Transformer-based summarization. Our approach tackles two primary challenges in variational summarization: insufficient semantic information in latent representations and posterior collapse during training. To address these challenges, we employ normalizing flows to enable flexible latent posterior modeling, and we propose a controlled alternate aggressive training (CAAT) strategy with an improved gate mechanism. Experimental results show that FlowSUM significantly enhances the quality of generated summaries and unleashes the potential for knowledge distillation with minimal impact on inference time. Furthermore, we investigate the issue of posterior collapse in normalizing flows and analyze how the summary quality is affected by the training strategy, gate initialization, and the type and number of normalizing flows used, offering valuable insights for future research",
    "checked": true,
    "id": "da70e316e6ac42fe5268b124f9fe1ab677a1d541",
    "semantic_title": "boosting summarization with normalizing flows and aggressive training",
    "citation_count": 0,
    "authors": [
      "Yu Yang",
      "Xiaotong Shen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.166": {
    "title": "Indicative Summarization of Long Discussions",
    "volume": "main",
    "abstract": "Online forums encourage the exchange and discussion of different stances on many topics. Not only do they provide an opportunity to present one's own arguments, but may also gather a broad cross-section of others' arguments. However, the resulting long discussions are difficult to overview. This paper presents a novel unsupervised approach using large language models (LLMs) to generating indicative summaries for long discussions that basically serve as tables of contents. Our approach first clusters argument sentences, generates cluster labels as abstractive summaries, and classifies the generated cluster labels into argumentation frames resulting in a two-level summary. Based on an extensively optimized prompt engineering approach, we evaluate 19 LLMs for generative cluster labeling and frame classification. To evaluate the usefulness of our indicative summaries, we conduct a purpose-driven user study via a new visual interface called **Discussion Explorer**: It shows that our proposed indicative summaries serve as a convenient navigation tool to explore long discussions",
    "checked": true,
    "id": "813370072963a32c6a3a371fd5000bdbf777eca3",
    "semantic_title": "indicative summarization of long discussions",
    "citation_count": 0,
    "authors": [
      "Shahbaz Syed",
      "Dominik Schwabe",
      "Khalid Khatib",
      "Martin Potthast"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.167": {
    "title": "A Framework for Vision-Language Warm-up Tasks in Multimodal Dialogue Models",
    "volume": "main",
    "abstract": "Most research on multimodal open-domain dialogue agents has focused on pretraining and multi-task learning using additional rich datasets beyond a given target dataset. However, methods for exploiting these additional datasets can be quite limited in real-world settings, creating a need for more efficient methods for constructing agents based solely on the target dataset. To address these issues, we present a new learning strategy called vision-language warm-up tasks for multimodal dialogue models (VLAW-MDM). This strategy does not require the use of large pretraining or multi-task datasets but rather relies solely on learning from target data. Moreover, our proposed approach automatically generate captions for images and incorporate them into the model's input to improve the contextualization of visual information. Using this novel approach, we empirically demonstrate that our learning strategy is effective for limited data and relatively small models. The result show that our method achieved comparable and in some cases superior performance compared to existing state-of-the-art models on various evaluation metrics",
    "checked": true,
    "id": "ef0aac0fd7845c754e350c92b7c6e2d62ee81649",
    "semantic_title": "a framework for vision-language warm-up tasks in multimodal dialogue models",
    "citation_count": 0,
    "authors": [
      "Jaewook Lee",
      "Seongsik Park",
      "Seong-Heum Park",
      "Hongjin Kim",
      "Harksoo Kim"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.168": {
    "title": "Once is Enough: A Light-Weight Cross-Attention for Fast Sentence Pair Modeling",
    "volume": "main",
    "abstract": "Transformer-based models have achieved great success on sentence pair modeling tasks, such as answer selection and natural language inference (NLI). These models generally perform cross-attention over input pairs, leading to prohibitive computational cost. Recent studies propose dual-encoder and late interaction architectures for faster computation. However, the balance between the expressive of cross-attention and computation speedup still needs better coordinated. To this end, this paper introduces a novel paradigm TopicAns for efficient sentence pair modeling. TopicAns involves a lightweight cross-attention mechanism. It conducts query encoding only once while modeling the query-candidate interaction in parallel. Extensive experiments conducted on four tasks demonstrate that our TopicAnscan speed up sentence pairing by over 113x while achieving comparable performance as the more expensive cross-attention models",
    "checked": true,
    "id": "93e2501e3a489490af9ff34601df73954f860fd0",
    "semantic_title": "once is enough: a light-weight cross-attention for fast sentence pair modeling",
    "citation_count": 0,
    "authors": [
      "Yuanhang Yang",
      "Shiyi Qi",
      "Chuanyi Liu",
      "Qifan Wang",
      "Cuiyun Gao",
      "Zenglin Xu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.169": {
    "title": "Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts",
    "volume": "main",
    "abstract": "As large language models (LLMs) have shown effectiveness with different prompting methods, such as Chain of Thought, Program of Thought, we find that these methods have formed a great complementarity to each other on math reasoning tasks. In this work, we propose XoT, an integrated problem solving framework by prompting LLMs with diverse reasoning thoughts. For each question, XoT always begins with selecting the most suitable method then executes each method iteratively. Within each iteration, XoT actively checks the validity of the generated answer and incorporates the feedback from external executors, allowing it to dynamically switch among different prompting methods. Through extensive experiments on 10 popular math reasoning datasets, we demonstrate the effectiveness of our proposed approach and thoroughly analyze the strengths of each module. Moreover, empirical results suggest that our framework is orthogonal to recent work that makes improvements on single reasoning methods and can further generalise to logical reasoning domain. By allowing method switching, XoT provides a fresh perspective on the collaborative integration of diverse reasoning thoughts in a unified framework",
    "checked": true,
    "id": "f176d0d466d7c778a6435fe9a8d7e49508cb9059",
    "semantic_title": "plan, verify and switch: integrated reasoning with diverse x-of-thoughts",
    "citation_count": 1,
    "authors": [
      "Tengxiao Liu",
      "Qipeng Guo",
      "Yuqing Yang",
      "Xiangkun Hu",
      "Yue Zhang",
      "Xipeng Qiu",
      "Zheng Zhang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.170": {
    "title": "GLEN: General-Purpose Event Detection for Thousands of Types",
    "volume": "main",
    "abstract": "The progress of event extraction research has been hindered by the absence of wide-coverage, large-scale datasets. To make event extraction systems more accessible, we build a general-purpose event detection dataset GLEN, which covers 205K event mentions with 3,465 different types, making it more than 20x larger in ontology than today's largest event dataset. GLEN is created by utilizing the DWD Overlay, which provides a mapping between Wikidata Qnodes and PropBank rolesets. This enables us to use the abundant existing annotation for PropBank as distant supervision. In addition, we also propose a new multi-stage event detection model specifically designed to handle the large ontology size in GLEN. We show that our model exhibits superior performance compared to a range of baselines including InstructGPT. Finally, we perform error analysis and show that label noise is still the largest challenge for improving performance for this new dataset",
    "checked": true,
    "id": "a5ebebcf0d17d08bfa2895533b121a4411c35685",
    "semantic_title": "glen: general-purpose event detection for thousands of types",
    "citation_count": 4,
    "authors": [
      "Sha Li",
      "Qiusi Zhan",
      "Kathryn Conger",
      "Martha Palmer",
      "Heng Ji",
      "Jiawei Han"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.171": {
    "title": "Hierarchical Pretraining on Multimodal Electronic Health Records",
    "volume": "main",
    "abstract": "Pretraining has proven to be a powerful technique in natural language processing (NLP), exhibiting remarkable success in various NLP downstream tasks. However, in the medical domain, existing pretrained models on electronic health records (EHR) fail to capture the hierarchical nature of EHR data, limiting their generalization capability across diverse downstream tasks using a single pretrained model. To tackle this challenge, this paper introduces a novel, general, and unified pretraining framework called MedHMP, specifically designed for hierarchically multimodal EHR data. The effectiveness of the proposed MedHMP is demonstrated through experimental results on eight downstream tasks spanning three levels. Comparisons against eighteen baselines further highlight the efficacy of our approach",
    "checked": true,
    "id": "5bda9200b6466da0779116385a277be6dbc746f4",
    "semantic_title": "hierarchical pretraining on multimodal electronic health records",
    "citation_count": 0,
    "authors": [
      "Xiaochen Wang",
      "Junyu Luo",
      "Jiaqi Wang",
      "Ziyi Yin",
      "Suhan Cui",
      "Yuan Zhong",
      "Yaqing Wang",
      "Fenglong Ma"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.172": {
    "title": "Critic-Driven Decoding for Mitigating Hallucinations in Data-to-text Generation",
    "volume": "main",
    "abstract": "Hallucination of text ungrounded in the input is a well-known problem in neural data-to-text generation. Many methods have been proposed to mitigate it, but they typically require altering model architecture or collecting additional data, and thus cannot be easily applied to an existing model. In this paper, we explore a new way to mitigate hallucinations by combining the probabilistic output of a generator language model (LM) with the output of a special \"text critic\" classifier, which guides the generation by assessing the match between the input data and the text generated so far. Our method does not need any changes to the underlying LM's architecture or training procedure and can thus be combined with any model and decoding operating on word probabilities. The critic does not need any additional training data, using the base LM's training data and synthetic negative examples. Our experimental results show that our method improves over the baseline on the WebNLG and OpenDialKG benchmarks",
    "checked": true,
    "id": "7a0081f6464de0ce95b71137b76bd2aa773b6b32",
    "semantic_title": "critic-driven decoding for mitigating hallucinations in data-to-text generation",
    "citation_count": 0,
    "authors": [
      "Mateusz Lango",
      "Ondrej Dusek"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.173": {
    "title": "Bridging the Gap between Synthetic and Authentic Images for Multimodal Machine Translation",
    "volume": "main",
    "abstract": "Multimodal machine translation (MMT) simultaneously takes the source sentence and a relevant image as input for translation. Since there is no paired image available for the input sentence in most cases, recent studies suggest utilizing powerful text-to-image generation models to provide image inputs. Nevertheless, synthetic images generated by these models often follow different distributions compared to authentic images. Consequently, using authentic images for training and synthetic images for inference can introduce a distribution shift, resulting in performance degradation during inference. To tackle this challenge, in this paper, we feed synthetic and authentic images to the MMT model, respectively. Then we minimize the gap between the synthetic and authentic images by drawing close the input image representations of the Transformer Encoder and the output distributions of the Transformer Decoder. Therefore, we mitigate the distribution disparity introduced by the synthetic images during inference, thereby freeing the authentic images from the inference process. Experimental results show that our approach achieves state-of-the-art performance on the Multi30K En-De and En-Fr datasets, while remaining independent of authentic images during inference",
    "checked": true,
    "id": "7f265fd9a036b2f049a430a4d0f44ee200a68deb",
    "semantic_title": "bridging the gap between synthetic and authentic images for multimodal machine translation",
    "citation_count": 0,
    "authors": [
      "Wenyu Guo",
      "Qingkai Fang",
      "Dong Yu",
      "Yang Feng"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.174": {
    "title": "DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models",
    "volume": "main",
    "abstract": "Pretrained language models have learned a vast amount of human knowledge from large-scale corpora, but their powerful memorization capability also brings the risk of data leakage. Some risks may only be discovered after the model training is completed, such as the model memorizing a specific phone number and frequently outputting it. In such cases, model developers need to eliminate specific data influences from the model to mitigate legal and ethical penalties. To effectively mitigate these risks, people often have to spend a significant amount of time and computational costs to retrain new models instead of finding ways to cure the ‘sick' models. Therefore, we propose a method to locate and erase risky neurons in order to eliminate the impact of privacy data in the model. We use a new method based on integrated gradients to locate neurons associated with privacy texts, and then erase these neurons by setting their activation values to zero.Furthermore, we propose a risky neuron aggregation method to eliminate the influence of privacy data in the model in batches. Experimental results show that our method can effectively and quickly eliminate the impact of privacy data without affecting the model's performance. Additionally, we demonstrate the relationship between model memorization and neurons through experiments, further illustrating the robustness of our method",
    "checked": true,
    "id": "41a3c41ba1912e1384849e6898c241af89cc4a11",
    "semantic_title": "depn: detecting and editing privacy neurons in pretrained language models",
    "citation_count": 0,
    "authors": [
      "Xinwei Wu",
      "Junzhuo Li",
      "Minghui Xu",
      "Weilong Dong",
      "Shuangzhi Wu",
      "Chao Bian",
      "Deyi Xiong"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.175": {
    "title": "Investigating Bias in Multilingual Language Models: Cross-Lingual Transfer of Debiasing Techniques",
    "volume": "main",
    "abstract": "This paper investigates the transferability of debiasing techniques across different languages within multilingual models. We examine the applicability of these techniques in English, French, German, and Dutch. Using multilingual BERT (mBERT), we demonstrate that cross-lingual transfer of debiasing techniques is not only feasible but also yields promising results. Surprisingly, our findings reveal no performance disadvantages when applying these techniques to non-English languages. Using translations of the CrowS-Pairs dataset, our analysis identifies SentenceDebias as the best technique across different languages, reducing bias in mBERT by an average of 13%. We also find that debiasing techniques with additional pretraining exhibit enhanced cross-lingual effectiveness for the languages included in the analyses, particularly in lower-resource languages. These novel insights contribute to a deeper understanding of bias mitigation in multilingual language models and provide practical guidance for debiasing techniques in different language contexts",
    "checked": true,
    "id": "bdaf4c9e78b7e194c3e894f96af907b7d4b5fb2f",
    "semantic_title": "investigating bias in multilingual language models: cross-lingual transfer of debiasing techniques",
    "citation_count": 0,
    "authors": [
      "Manon Reusens",
      "Philipp Borchert",
      "Margot Mieskes",
      "Jochen De Weerdt",
      "Bart Baesens"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.176": {
    "title": "Can Language Models Laugh at YouTube Short-form Videos?",
    "volume": "main",
    "abstract": "As short-form funny videos on social networks are gaining popularity, it becomes demanding for AI models to understand them for better communication with humans. Unfortunately, previous video humor datasets target specific domains such as speeches or sitcoms, and mostly focus on verbal cues. We curate a user-generated dataset of 10K multimodal funny videos from YouTube, called ExFunTube. Using a video filtering pipeline with GPT-3.5, we verify both verbal and visual elements contributing to humor. After filtering, we annotate each video with timestamps and text explanations for funny moments. Our ExFunTube is unique over existing datasets in that our videos cover a wide range of domains with various types of humor that necessitate a multimodal understanding of the content. Also, we develop a zero-shot video-to-text prompting to maximize video humor understanding of large language models (LLMs). With three different evaluation methods using automatic scores, rationale quality experiments, and human evaluations, we show that our prompting significantly improves LLMs' ability for humor explanation",
    "checked": true,
    "id": "ca1261a7e058ea3339fbdaec07ffcf6d0b617c8d",
    "semantic_title": "can language models laugh at youtube short-form videos?",
    "citation_count": 0,
    "authors": [
      "Dayoon Ko",
      "Sangho Lee",
      "Gunhee Kim"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.177": {
    "title": "Random Entity Quantization for Parameter-Efficient Compositional Knowledge Graph Representation",
    "volume": "main",
    "abstract": "Representation Learning on Knowledge Graphs (KGs) is essential for downstream tasks. The dominant approach, KG Embedding (KGE), represents entities with independent vectors and faces the scalability challenge. Recent studies propose an alternative way for parameter efficiency, which represents entities by composing entity-corresponding codewords matched from predefined small-scale codebooks. We refer to the process of obtaining corresponding codewords of each entity as entity quantization, for which previous works have designed complicated strategies. Surprisingly, this paper shows that simple random entity quantization can achieve similar results to current strategies. We analyze this phenomenon and reveal that entity codes, the quantization outcomes for expressing entities, have higher entropy at the code level and Jaccard distance at the codeword level under random entity quantization. Therefore, different entities become more easily distinguished, facilitating effective KG representation. The above results show that current quantization strategies are not critical for KG representation, and there is still room for improvement in entity distinguishability beyond current strategies",
    "checked": true,
    "id": "c261c5df8009f34092fa943bc8bcad1c724ee86b",
    "semantic_title": "random entity quantization for parameter-efficient compositional knowledge graph representation",
    "citation_count": 0,
    "authors": [
      "Jiaang Li",
      "Quan Wang",
      "Yi Liu",
      "Licheng Zhang",
      "Zhendong Mao"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.178": {
    "title": "Exploring All-In-One Knowledge Distillation Framework for Neural Machine Translation",
    "volume": "main",
    "abstract": "Conventional knowledge distillation(KD) approaches are commonly employed to compress neural machine translation(NMT) models. However, they only obtain one lightweight student each time. Consequently, we have to conduct KD multiple times when different students are required at the same time, which could be resource-intensive. Additionally, these students are individually optimized, and thus lack interactions with each other, leading to their potential not being fully exerted. In this work, we propose a novel All-In-One Knowledge Distillation(AIO-KD) framework for NMT, which generates multiple satisfactory students at once. Under AIO-KD, we first randomly extract fewer-layer subnetworks from the teacher as the sample students. Then, we jointly optimize the teacher and these students, where the students simultaneously learn the knowledge from the teacher and interact with other students via mutual learning. When utilized, we re-extract the candidate students, satisfying the specifications of various devices. Particularly, we adopt carefully-designed strategies for AIO-KD: 1) we dynamically detach gradients to prevent poorly-performed students from negatively affecting the teacher during the knowledge transfer, which could subsequently impact other students; 2) we design a two-stage mutual learning strategy, which alleviates the negative impacts of poorly-performed students on the early-stage student interactions. Extensive experiments and in-depth analyses on three benchmarks demonstrate the effectiveness and eco-friendliness of AIO-KD. Our source code is available at https://github.com/DeepLearnXMU/AIO-KD",
    "checked": true,
    "id": "7f7c482630abeee274777219b205f889999eab80",
    "semantic_title": "exploring all-in-one knowledge distillation framework for neural machine translation",
    "citation_count": 0,
    "authors": [
      "Zhongjian Miao",
      "Wen Zhang",
      "Jinsong Su",
      "Xiang Li",
      "Jian Luan",
      "Yidong Chen",
      "Bin Wang",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.179": {
    "title": "HistAlign: Improving Context Dependency in Language Generation by Aligning with History",
    "volume": "main",
    "abstract": "Language models (LMs) can generate hallucinations and incoherent outputs, which highlights their weak context dependency. Cache-LMs, which augment LMs with a memory of recent history, can increase context dependency and have shown remarkable performance in diverse language generation tasks. However, we find that even with training, the performance gain stemming from the cache component of current cache-LMs is suboptimal due to the misalignment between the current hidden states and those stored in the memory. In this work, we present HistAlign, a new training approach to ensure good cache alignment such that the model receives useful signals from the history. We first prove our concept on a simple and synthetic task where the memory is essential for correct predictions, and we show that the cache component of HistAlign is better aligned and improves overall performance. Next, we evaluate HistAlign on diverse downstream language generation tasks, including prompt continuation, abstractive summarization, and data-to-text. We demonstrate that HistAlign improves text coherence and faithfulness in open-ended and conditional generation settings respectively. HistAlign is also generalizable across different model families, showcasing its strength in improving context dependency of LMs in diverse scenarios",
    "checked": true,
    "id": "911010d91fb5f632a99be2eb9f57d57287f248e9",
    "semantic_title": "histalign: improving context dependency in language generation by aligning with history",
    "citation_count": 2,
    "authors": [
      "David Wan",
      "Shiyue Zhang",
      "Mohit Bansal"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.180": {
    "title": "CombLM: Adapting Black-Box Language Models through Small Fine-Tuned Models",
    "volume": "main",
    "abstract": "Methods for adapting language models (LMs) to new tasks and domains have traditionally assumed white-box access to the model, and work by modifying its parameters. However, this is incompatible with a recent trend in the field, where the highest quality models are only available as black-boxes through inference APIs. Even when the model weights are available, the computational cost of fine-tuning large LMs can be prohibitive for most practitioners. In this work, we present a lightweight method for adapting large LMs to new domains and tasks, assuming no access to their weights or intermediate activations. Our approach fine-tunes a small white-box LM and combines it with the large black-box LM at the probability level through a small network, learned on a small validation set. We validate our approach by adapting a large LM (OPT-30B) to several domains and a downstream task (machine translation), observing improved performance in all cases, of up to 9%, while using a domain expert 23x smaller",
    "checked": true,
    "id": "e55fff3d7e59a7192b59b4b497c17ea9c77a9d16",
    "semantic_title": "comblm: adapting black-box language models through small fine-tuned models",
    "citation_count": 0,
    "authors": [
      "Aitor Ormazabal",
      "Mikel Artetxe",
      "Eneko Agirre"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.181": {
    "title": "Image Manipulation via Multi-Hop Instructions - A New Dataset and Weakly-Supervised Neuro-Symbolic Approach",
    "volume": "main",
    "abstract": "We are interested in image manipulation via natural language text – a task that is useful for multiple AI applications but requires complex reasoning over multi-modal spaces. We extend recently proposed Neuro Symbolic Concept Learning (NSCL), which has been quite effective for the task of Visual Question Answering (VQA), for the task of image manipulation. Our system referred to as NeuroSIM can perform complex multi-hop reasoning over multi-object scenes and only requires weak supervision in the form of annotated data for VQA. NeuroSIM parses an instruction into a symbolic program, based on a Domain Specific Language (DSL) comprising of object attributes and manipulation operations, that guides its execution. We create a new dataset for the task, and extensive experiments demonstrate that NeuroSIM is highly competitive with or beats SOTA baselines that make use of supervised data for manipulation",
    "checked": true,
    "id": "e4b79ed9463a2ce399c8800258f548730a99cbb7",
    "semantic_title": "image manipulation via multi-hop instructions - a new dataset and weakly-supervised neuro-symbolic approach",
    "citation_count": 0,
    "authors": [
      "Harman Singh",
      "Poorva Garg",
      "Mohit Gupta",
      "Kevin Shah",
      "Ashish Goswami",
      "Satyam Modi",
      "Arnab Mondal",
      "Dinesh Khandelwal",
      "Dinesh Garg",
      "Parag Singla"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.182": {
    "title": "Generative Spoken Language Model based on continuous word-sized audio tokens",
    "volume": "main",
    "abstract": "In NLP, text language models based on words or subwords are known to outperform their character-based counterparts. Yet, in the speech community, the standard input of spoken LMs are 20ms or 40ms-long discrete units (shorter than a phoneme). Taking inspiration from word-based LM, we introduce a Generative Spoken Language Model (GSLM) based on word-size continuous-valued audio tokens that can generate diverse and expressive language output. This is obtained by replacing lookup table for lexical types with a Lexical Embedding function, the cross entropy loss by a contrastive loss, and multinomial sampling by k-NN sampling. The resulting model is the first generative language model based on word-size continuous tokens. Its performance is on par with discrete unit GSLMs regarding generation quality as measured by automatic metrics and subjective human judgements. Moreover, it is five times more memory efficient thanks to its large 200ms units. In addition, the embeddings before and after the Lexical Embedder are phonetically and semantically interpretable",
    "checked": true,
    "id": "2d5ef1d6b631ac7ee0ae87e480381407c11b9fb2",
    "semantic_title": "generative spoken language model based on continuous word-sized audio tokens",
    "citation_count": 1,
    "authors": [
      "Robin Algayres",
      "Yossi Adi",
      "Tu Nguyen",
      "Jade Copet",
      "Gabriel Synnaeve",
      "Benoît Sagot",
      "Emmanuel Dupoux"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.183": {
    "title": "Enhancing Chat Language Models by Scaling High-quality Instructional Conversations",
    "volume": "main",
    "abstract": "Fine-tuning on instruction data has been widely validated as an effective practice for implementing chat language models like ChatGPT. Scaling the diversity and quality of such data, although straightforward, stands a great chance of leading to improved performance. This paper aims to push the upper bound of open-source models further. We first provide a systematically designed, diverse, informative, large-scale dataset of instructional conversations, UltraChat, which does not involve human queries. Our objective is to capture the breadth of interactions between a human user and an AI assistant and employs a comprehensive framework to generate multi-turn conversation iteratively. UltraChat contains 1.5 million high-quality multi-turn dialogues and covers a wide range of topics and instructions. Our statistical analysis of UltraChat reveals its superiority in various key metrics, including scale, average length, diversity, coherence, etc., solidifying its position as a leading open-source dataset. Building upon UltraChat, we fine-tune a LLaMA model to create a powerful conversational model, UltraLM. Our evaluations indicate that UltraLM consistently outperforms other open-source models, including WizardLM and Vicuna, the previously recognized state-of-the-art open-source models",
    "checked": true,
    "id": "a122863d239643453195424c04067e89406246e1",
    "semantic_title": "enhancing chat language models by scaling high-quality instructional conversations",
    "citation_count": 47,
    "authors": [
      "Ning Ding",
      "Yulin Chen",
      "Bokai Xu",
      "Yujia Qin",
      "Shengding Hu",
      "Zhiyuan Liu",
      "Maosong Sun",
      "Bowen Zhou"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.184": {
    "title": "Weakly-Supervised Learning of Visual Relations in Multimodal Pretraining",
    "volume": "main",
    "abstract": "Recent work in vision-and-language pretraining has investigated supervised signals from object detection data to learn better, fine-grained multimodal representations. In this work, we take a step further and explore how we can tap into supervision from small-scale visual relation data. In particular, we propose two pretraining approaches to contextualise visual entities in a multimodal setup. With verbalised scene graphs, we transform visual relation triplets into structured captions, and treat them as additional image descriptions. With masked relation prediction, we further encourage relating entities from image regions with visually masked contexts. When applied to strong baselines pretrained on large amounts of Web data, zero-shot evaluations on both coarse-grained and fine-grained tasks show the efficacy of our methods in learning multimodal representations from weakly-supervised relations data",
    "checked": true,
    "id": "f4f88f2679ab7c4a6ac01eed77f52f9b5e01f2eb",
    "semantic_title": "weakly-supervised learning of visual relations in multimodal pretraining",
    "citation_count": 0,
    "authors": [
      "Emanuele Bugliarello",
      "Aida Nematzadeh",
      "Lisa Hendricks"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.185": {
    "title": "Unsupervised Grammatical Error Correction Rivaling Supervised Methods",
    "volume": "main",
    "abstract": "State-of-the-art grammatical error correction (GEC) systems rely on parallel training data (ungrammatical sentences and their manually corrected counterparts), which are expensive to construct. In this paper, we employ the Break-It-Fix-It (BIFI) method to build an unsupervised GEC system. The BIFI framework generates parallel data from unlabeled text using a fixer to transform ungrammatical sentences into grammatical ones, and a critic to predict sentence grammaticality. We present an unsupervised approach to build the fixer and the critic, and an algorithm that allows them to iteratively improve each other. We evaluate our unsupervised GEC system on English and Chinese GEC. Empirical results show that our GEC system outperforms previous unsupervised GEC systems, and achieves performance comparable to supervised GEC systems without ensemble. Furthermore, when combined with labeled training data, our system achieves new state-of-the-art results on the CoNLL-2014 and NLPCC-2018 test sets",
    "checked": true,
    "id": "ad1bc48e37c5fb2185f15cef643d49e2d7661fa4",
    "semantic_title": "unsupervised grammatical error correction rivaling supervised methods",
    "citation_count": 0,
    "authors": [
      "Hannan Cao",
      "Liping Yuan",
      "Yuchen Zhang",
      "Hwee Tou Ng"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.186": {
    "title": "S2abEL: A Dataset for Entity Linking from Scientific Tables",
    "volume": "main",
    "abstract": "Entity linking (EL) is the task of linking a textual mention to its corresponding entry in a knowledge base, and is critical for many knowledge-intensive NLP applications. When applied to tables in scientific papers, EL is a step toward large-scale scientific knowledge bases that could enable advanced scientific question answering and analytics. We present the first dataset for EL in scientific tables. EL for scientific tables is especially challenging because scientific knowledge bases can be very incomplete, and disambiguating table mentions typically requires understanding the paper's text in addition to the table. Our dataset, Scientific Table Entity Linking (S2abEL), focuses on EL in machine learning results tables and includes hand-labeled cell types, attributed sources, and entity links from the PaperswithCode taxonomy for 8,429 cells from 732 tables. We introduce a neural baseline method designed for EL on scientific tables containing many out-of-knowledge-base mentions, and show that it significantly outperforms a state-of-the-art generic table EL method. The best baselines fall below human performance, and our analysis highlights avenues for improvement",
    "checked": true,
    "id": "0bc975e61002ec29ac67d44d91d35cdbfc56982a",
    "semantic_title": "s2abel: a dataset for entity linking from scientific tables",
    "citation_count": 1,
    "authors": [
      "Yuze Lou",
      "Bailey Kuehl",
      "Erin Bransom",
      "Sergey Feldman",
      "Aakanksha Naik",
      "Doug Downey"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.187": {
    "title": "API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs",
    "volume": "main",
    "abstract": "Recent research has demonstrated that Large Language Models (LLMs) can enhance their capabilities by utilizing external tools. However, three pivotal questions remain unanswered: (1) How effective are current LLMs in utilizing tools? (2) How can we enhance LLMs' ability to utilize tools? (3) What obstacles need to be overcome to leverage tools? To address these questions, we introduce API-Bank, a groundbreaking benchmark, specifically designed for tool-augmented LLMs. For the first question, we develop a runnable evaluation system consisting of 73 API tools. We annotate 314 tool-use dialogues with 753 API calls to assess the existing LLMs' capabilities in planning, retrieving, and calling APIs. For the second question, we construct a comprehensive training set containing 1,888 tool-use dialogues from 2,138 APIs spanning 1,000 distinct domains. Using this dataset, we train Lynx, a tool-augmented LLM initialized from Alpaca. Experimental results demonstrate that GPT-3.5 exhibits improved tool utilization compared to GPT-3, while GPT-4 excels in planning. However, there is still significant potential for further improvement. Moreover, Lynx surpasses Alpaca's tool utilization performance by more than 26 pts and approaches the effectiveness of GPT-3.5. Through error analysis, we highlight the key challenges for future research in this field to answer the third question",
    "checked": true,
    "id": "19c222d1f18317d58cc85491f37479bc0dc49f41",
    "semantic_title": "api-bank: a comprehensive benchmark for tool-augmented llms",
    "citation_count": 25,
    "authors": [
      "Minghao Li",
      "Yingxiu Zhao",
      "Bowen Yu",
      "Feifan Song",
      "Hangyu Li",
      "Haiyang Yu",
      "Zhoujun Li",
      "Fei Huang",
      "Yongbin Li"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.188": {
    "title": "Language and Mental Health: Measures of Emotion Dynamics from Text as Linguistic Biosocial Markers",
    "volume": "main",
    "abstract": "Research in psychopathology has shown that, at an aggregate level, the patterns of emotional change over time—emotion dynamics—are indicators of one's mental health. One's patterns of emotion change have traditionally been determined through self-reports of emotions; however, there are known issues with accuracy, bias, and convenience. Recent approaches to determining emotion dynamics from one's everyday utterances, addresses many of these concerns, but it is not yet known whether these measures of utterance emotion dynamics (UED) correlate with mental health diagnoses. Here, for the first time, we study the relationship between tweet emotion dynamics and mental health disorders. We find that each of the UED metrics studied varied by the user's self-disclosed diagnosis. For example: average valence was significantly higher (i.e., more positive text) in the control group compared to users with ADHD, MDD, and PTSD. Valence variability was significantly lower in the control group compared to ADHD, depression, bipolar disorder, MDD, PTSD, and OCD but not PPD. Rise and recovery rates of valence also exhibited significant differences from the control. This work provides important early evidence for how linguistic cues pertaining to emotion dynamics can play a crucial role as biosocial markers for mental illnesses and aid in the understanding, diagnosis, and management of mental health disorders",
    "checked": true,
    "id": "8f5cc9d6cfb22e09429685d61536854cc1803be1",
    "semantic_title": "language and mental health: measures of emotion dynamics from text as linguistic biosocial markers",
    "citation_count": 1,
    "authors": [
      "Daniela Teodorescu",
      "Tiffany Cheng",
      "Alona Fyshe",
      "Saif Mohammad"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.189": {
    "title": "Lion: Adversarial Distillation of Proprietary Large Language Models",
    "volume": "main",
    "abstract": "The practice of transferring knowledge from a sophisticated, proprietary large language model (LLM) to a compact, open-source LLM has garnered considerable attention. Previous works have focused on a unidirectional knowledge distillation way by aligning the responses of the student model with those of the teacher model to a set of instructions. Nevertheless, they overlooked the possibility of incorporating any \"feedback\"–identifying challenging instructions where the student model's performance falls short–to boost the student model's proficiency iteratively. To this end, we propose a novel adversarial distillation framework for a more efficient knowledge transfer. Leveraging the versatile role adaptability of LLMs, we prompt the teacher model to identify \"hard\" instructions and generate new \"hard\" instructions for the student model, creating a three-stage adversarial loop of imitation, discrimination, and generation. By applying this adversarial framework, we successfully transfer knowledge from ChatGPT to a student model (named Lion), using a mere 70k training data. Our results show that Lion-13B not only achieves comparable open-ended generation capabilities to ChatGPT but surpasses conventional state-of-the-art (SOTA) instruction-tuned models like Vicuna-13B by 55.4% in challenging zero-shot reasoning benchmarks such as BIG-Bench Hard (BBH) and 16.7% on AGIEval",
    "checked": true,
    "id": "02529b2666a536053a2e2940de5b28de36fd594b",
    "semantic_title": "lion: adversarial distillation of proprietary large language models",
    "citation_count": 3,
    "authors": [
      "Yuxin Jiang",
      "Chunkit Chan",
      "Mingyang Chen",
      "Wei Wang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.190": {
    "title": "Evaluating Large Language Models on Controlled Generation Tasks",
    "volume": "main",
    "abstract": "While recent studies have looked into the abilities of large language models in various benchmark tasks, including question generation, reading comprehension, multilingual and etc, there have been few studies looking into the controllability of large language models on generation tasks. We present an extensive analysis of various benchmarks including a sentence planning benchmark with different granularities. After comparing large language models against state-of-the-start finetuned smaller models, we present a spectrum showing large language models falling behind, are comparable, or exceed the ability of smaller models. We conclude that *large language models struggle at meeting fine-grained hard constraints*",
    "checked": true,
    "id": "91b5d8287e2353f667f90641e3d930cc5ee0942e",
    "semantic_title": "evaluating large language models on controlled generation tasks",
    "citation_count": 1,
    "authors": [
      "Jiao Sun",
      "Yufei Tian",
      "Wangchunshu Zhou",
      "Nan Xu",
      "Qian Hu",
      "Rahul Gupta",
      "John Wieting",
      "Nanyun Peng",
      "Xuezhe Ma"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.191": {
    "title": "DeSIQ: Towards an Unbiased, Challenging Benchmark for Social Intelligence Understanding",
    "volume": "main",
    "abstract": "Social intelligence is essential for understanding and reasoning about human expressions, intents and interactions. One representative benchmark for its study is Social Intelligence Queries (Social-IQ), a dataset of multiple-choice questions on videos of complex social interactions. We define a comprehensive methodology to study the soundness of Social-IQ, as the soundness of such benchmark datasets is crucial to the investigation of the underlying research problem. We define a comprehensive methodology to study the soundness of Social-IQ, as the soundness of such benchmark datasets is crucial to the investigation of the underlying research problem. Our analysis reveals that Social-IQ contains substantial biases, which can be exploited by a moderately strong language model to learn spurious correlations to achieve perfect performance without being given the context or even the question. We introduce DeSIQ, a new challenging dataset, constructed by applying simple perturbations to Social-IQ. Our empirical analysis shows De-SIQ significantly reduces the biases in the original Social-IQ dataset. Furthermore, we examine and shed light on the effect of model size, model style, learning settings, commonsense knowledge, and multi-modality on the new benchmark performance. Our new dataset, observations and findings open up important research questions for the study of social intelligence",
    "checked": true,
    "id": "cf4fac3510b0e4fdab47fa7495c18ad32dfefde0",
    "semantic_title": "desiq: towards an unbiased, challenging benchmark for social intelligence understanding",
    "citation_count": 0,
    "authors": [
      "Xiao-Yu Guo",
      "Yuan-Fang Li",
      "Reza Haf"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.192": {
    "title": "Why LLMs Hallucinate, and How to Get (Evidential) Closure: Perceptual, Intensional, and Extensional Learning for Faithful Natural Language Generation",
    "volume": "main",
    "abstract": "We show that LLMs hallucinate because their output is not constrained to be synonymous with claims for which they have evidence: a condition that we call evidential closure. Information about the truth or falsity of sentences is not statistically identified in the standard neural language generation setup, and so cannot be conditioned on to generate new strings. We then show how to constrain LLMs to produce output that satisfies evidential closure. A multimodal LLM must learn about the external world (perceptual learning); it must learn a mapping from strings to states of the world (extensional learning); and, to achieve fluency when generalizing beyond a body of evidence, it must learn mappings from strings to their synonyms (intensional learning). The output of a unimodal LLM must be synonymous with strings in a validated evidence set. Finally, we present a heuristic procedure, Learn-Babble-Prune, that yields faithful output from an LLM by rejecting output that is not synonymous with claims for which the LLM has evidence",
    "checked": true,
    "id": "5e6e36f08504c65fb61aa777f058882cc0a09346",
    "semantic_title": "why llms hallucinate, and how to get (evidential) closure: perceptual, intensional, and extensional learning for faithful natural language generation",
    "citation_count": 0,
    "authors": [
      "Adam Bouyamourn"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.193": {
    "title": "A Question Answering Framework for Decontextualizing User-facing Snippets from Scientific Documents",
    "volume": "main",
    "abstract": "Many real-world applications (e.g., note taking, search) require extracting a sentence or paragraph from a document and showing that snippet to a human outside of the source document. Yet, users may find snippets difficult to understand as they lack context from the original document. In this work, we use language models to rewrite snippets from scientific documents to be read on their own. First, we define the requirements and challenges for this user-facing decontextualization task, such as clarifying where edits occur and handling references to other documents. Second, we propose a framework that decomposes the task into three stages: question generation, question answering, and rewriting. Using this framework, we collect gold decontextualizations from experienced scientific article readers. We then conduct a range of experiments across state-of-the-art commercial and open-source language models to identify how to best provide missing-but-relevant information to models for our task. Finally, we develop QaDecontext, a simple prompting strategy inspired by our framework that improves over end-to-end prompting. We conclude with analysis that finds, while rewriting is easy, question generation and answering remain challenging for today's models",
    "checked": true,
    "id": "8232fbb89fea58391111bad0dc30f299778dcb80",
    "semantic_title": "a question answering framework for decontextualizing user-facing snippets from scientific documents",
    "citation_count": 1,
    "authors": [
      "Benjamin Newman",
      "Luca Soldaini",
      "Raymond Fok",
      "Arman Cohan",
      "Kyle Lo"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.194": {
    "title": "SLOG: A Structural Generalization Benchmark for Semantic Parsing",
    "volume": "main",
    "abstract": "The goal of compositional generalization benchmarks is to evaluate how well models generalize to new complex linguistic expressions. Existing benchmarks often focus on lexical generalization, the interpretation of novel lexical items in syntactic structures familiar from training; structural generalization tasks, where a model needs to interpret syntactic structures that are themselves unfamiliar from training, are often underrepresented, resulting in overly optimistic perceptions of how well models can generalize. We introduce SLOG, a semantic parsing dataset that extends COGS (Kim and Linzen, 2020) with 17 structural generalization cases. In our experiments, the generalization accuracy of Transformer models, including pretrained ones, only reaches 40.6%, while a structure-aware parser only achieves 70.8%. These results are far from the near-perfect accuracy existing models achieve on COGS, demonstrating the role of SLOG in foregrounding the large discrepancy between models' lexical and structural generalization capacities",
    "checked": true,
    "id": "2ff1e8648ff9a4a7fae43dffe8b5629fc0c32513",
    "semantic_title": "slog: a structural generalization benchmark for semantic parsing",
    "citation_count": 3,
    "authors": [
      "Bingzhi Li",
      "Lucia Donatelli",
      "Alexander Koller",
      "Tal Linzen",
      "Yuekun Yao",
      "Najoung Kim"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.195": {
    "title": "Pushdown Layers: Encoding Recursive Structure in Transformer Language Models",
    "volume": "main",
    "abstract": "Recursion is a prominent feature of human language, and fundamentally challenging for self-attention due to the lack of an explicit recursive-state tracking mechanism. Consequently, Transformer language models poorly capture long-tail recursive structure and exhibit sample-inefficient syntactic generalization. This work introduces Pushdown Layers, a new self-attention layer that models recursive state via a stack tape that tracks estimated depths of every token in an incremental parse of the observed prefix. Transformer LMs with Pushdown Layers are syntactic language models that autoregressively and synchronously update this stack tape as they predict new tokens, in turn using the stack tape to softly modulate attention over tokens—for instance, learning to \"skip\" over closed constituents. When trained on a corpus of strings annotated with silver constituency parses, Transformers equipped with Pushdown Layers achieve dramatically better and 3-5x more sample-efficient syntactic generalization, while maintaining similar perplexities. Pushdown Layers are a drop-in replacement for standard self-attention. We illustrate this by finetuning GPT2-medium with Pushdown Layers on an automatically parsed WikiText-103, leading to improvements on several GLUE text classification tasks",
    "checked": true,
    "id": "20d8448cf1efea10e5ddb3ca8bc8beaef874c51e",
    "semantic_title": "pushdown layers: encoding recursive structure in transformer language models",
    "citation_count": 0,
    "authors": [
      "Shikhar Murty",
      "Pratyusha Sharma",
      "Jacob Andreas",
      "Christopher Manning"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.196": {
    "title": "Can LLMs Facilitate Interpretation of Pre-trained Language Models?",
    "volume": "main",
    "abstract": "Work done to uncover the knowledge encoded within pre-trained language models rely on annotated corpora or human-in-the-loop methods. However, these approaches are limited in terms of scalability and the scope of interpretation. We propose using a large language model, ChatGPT, as an annotator to enable fine-grained interpretation analysis of pre-trained language models. We discover latent concepts within pre-trained language models by applying agglomerative hierarchical clustering over contextualized representations and then annotate these concepts using ChatGPT. Our findings demonstrate that ChatGPT produces accurate and semantically richer annotations compared to human-annotated concepts. Additionally, we showcase how GPT-based annotations empower interpretation analysis methodologies of which we demonstrate two: probing frameworks and neuron interpretation. To facilitate further exploration and experimentation in the field, we make available a substantial ConceptNet dataset (TCN) comprising 39,000 annotated concepts",
    "checked": true,
    "id": "6c340ff334beac9524629d19f84544ed2bb29e85",
    "semantic_title": "can llms facilitate interpretation of pre-trained language models?",
    "citation_count": 1,
    "authors": [
      "Basel Mousi",
      "Nadir Durrani",
      "Fahim Dalvi"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.197": {
    "title": "Enhancing Low-resource Fine-grained Named Entity Recognition by Leveraging Coarse-grained Datasets",
    "volume": "main",
    "abstract": "Named Entity Recognition (NER) frequently suffers from the problem of insufficient labeled data, particularly in fine-grained NER scenarios. Although K-shot learning techniques can be applied, their performance tends to saturate when the number of annotations exceeds several tens of labels. To overcome this problem, we utilize existing coarse-grained datasets that offer a large number of annotations. A straightforward approach to address this problem is pre-finetuning, which employs coarse-grained data for representation learning. However, it cannot directly utilize the relationships between fine-grained and coarse-grained entities, although a fine-grained entity type is likely to be a subcategory of a coarse-grained entity type. We propose a fine-grained NER model with a Fine-to-Coarse(F2C) mapping matrix to leverage the hierarchical structure explicitly. In addition, we present an inconsistency filtering method to eliminate coarse-grained entities that are inconsistent with fine-grained entity types to avoid performance degradation. Our experimental results show that our method outperforms both K-shot learning and supervised learning methods when dealing with a small number of fine-grained annotations",
    "checked": true,
    "id": "4992083e2188fe6bc31d4a09b6776092736df34a",
    "semantic_title": "enhancing low-resource fine-grained named entity recognition by leveraging coarse-grained datasets",
    "citation_count": 0,
    "authors": [
      "Su Lee",
      "Seokjin Oh",
      "Woohwan Jung"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.198": {
    "title": "Oolong: Investigating What Makes Transfer Learning Hard with Controlled Studies",
    "volume": "main",
    "abstract": "When we transfer a pretrained language model to a new language, there are many axes of variation that change at once. To disentangle the impact of different factors like syntactic similarity and vocabulary similarity, we propose a set of controlled transfer studies: we systematically transform the language of the GLUE benchmark, altering one axis of crosslingual variation at a time, and then measure the resulting drops in a pretrained model's downstream performance. We find that models can largely recover from syntactic-style shifts, but cannot recover from vocabulary misalignment and embedding matrix re-initialization, even with continued pretraining on 15 million tokens. Moreover, good-quality tokenizers in the transfer language do not make vocabulary alignment easier. Our experiments provide insights into the factors of cross-lingual transfer that researchers should most focus on when designing language transfer scenarios",
    "checked": true,
    "id": "49ef71faeb6cb86ddd59e9f6aa091192e2944206",
    "semantic_title": "oolong: investigating what makes transfer learning hard with controlled studies",
    "citation_count": 1,
    "authors": [
      "Zhengxuan Wu",
      "Alex Tamkin",
      "Isabel Papadimitriou"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.199": {
    "title": "Non-Autoregressive Math Word Problem Solver with Unified Tree Structure",
    "volume": "main",
    "abstract": "Existing MWP solvers employ sequence or binary tree to present the solution expression and decode it from given problem description. However, such structures fail to handle the variants that can be derived via mathematical manipulation, e.g., (a1+a2)*a3 and a1 * a3+a2 * a3 can both be possible valid solutions for a same problem but formulated as different expression sequences or trees. The multiple solution variants depicting different possible solving procedures for the same input problem would raise two issues: 1) making it hard for the model to learn the mapping function between the input and output spaces effectively, and 2) wrongly indicating wrong when evaluating a valid expression variant. To address these issues, we introduce a unified tree structure to present a solution expression, where the elements are permutable and identical for all the expression variants. We propose a novel non-autoregressive solver, named MWP-NAS, to parse the problem and deduce the solution expression based on the unified tree. For evaluating the possible expression variants, we design a path-based metric to evaluate the partial accuracy of expressions of a unified tree. The results from extensive experiments conducted on Math23K and MAWPS demonstrate the effectiveness of our proposed MWP-NAS. The codes and checkpoints are available at: https://github.com/mengqunhan/MWP-NAS",
    "checked": true,
    "id": "9ccb2beaec722232a84e9a7682c72dcf7de667df",
    "semantic_title": "non-autoregressive math word problem solver with unified tree structure",
    "citation_count": 3,
    "authors": [
      "Yi Bin",
      "Mengqun Han",
      "Wenhao Shi",
      "Lei Wang",
      "Yang Yang",
      "See-Kiong Ng",
      "Heng Shen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.200": {
    "title": "Improving Chinese Pop Song and Hokkien Gezi Opera Singing Voice Synthesis by Enhancing Local Modeling",
    "volume": "main",
    "abstract": "Singing Voice Synthesis (SVS) strives to synthesize pleasing vocals based on music scores and lyrics. The current acoustic models based on Transformer usually process the entire sequence globally and use a simple L1 loss. However, this approach overlooks the significance of local modeling within the sequence and the local optimization of the hard-to-synthesize parts in the predicted mel-spectrogram. Consequently, the synthesized audio exhibits local incongruities (e.g., local pronunciation jitter or local noise). To address this problem, we propose two methods to enhance local modeling in the acoustic model. First, we devise a nearest neighbor local attention, where each phoneme token focuses only on the adjacent phoneme tokens located before and after it. Second, we propose a phoneme-level local adaptive weights loss function that enables the model to focus more on the hard-to-synthesize parts of the mel-spectrogram. We have verified the universality of our methods on public Chinese pop song and Hokkien Gezi Opera datasets. Extensive experiments have demonstrated the effectiveness of our methods, resulting in significant improvements in both objective and subjective evaluations when compared to the strong baselines. Our code and demonstration samples are available at https://github.com/baipeng1/SVSELM",
    "checked": true,
    "id": "5cbb610e9bd216d390818bd5e8b23d06700fe64b",
    "semantic_title": "improving chinese pop song and hokkien gezi opera singing voice synthesis by enhancing local modeling",
    "citation_count": 0,
    "authors": [
      "Peng Bai",
      "Yue Zhou",
      "Meizhen Zheng",
      "Wujin Sun",
      "Xiaodong Shi"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.201": {
    "title": "What Else Do I Need to Know? The Effect of Background Information on Users' Reliance on QA Systems",
    "volume": "main",
    "abstract": "NLP systems have shown impressive performance at answering questions by retrieving relevant context. However, with the increasingly large models, it is impossible and often undesirable to constrain models' knowledge or reasoning to only the retrieved context. This leads to a mismatch between the information that the models access to derive the answer and the information that is available to the user to assess the model predicted answer. In this work, we study how users interact with QA systems in the absence of sufficient information to assess their predictions. Further, we ask whether adding the requisite background helps mitigate users' over-reliance on predictions. Our study reveals that users rely on model predictions even in the absence of sufficient information needed to assess the model's correctness. Providing the relevant background, however, helps users better catch model errors, reducing over-reliance on incorrect predictions. On the flip side, background information also increases users' confidence in their accurate as well as inaccurate judgments. Our work highlights that supporting users' verification of QA predictions is an important, yet challenging, problem",
    "checked": true,
    "id": "8f614c5fd8902d7511f532488fe8e64f2403d1ed",
    "semantic_title": "what else do i need to know? the effect of background information on users' reliance on qa systems",
    "citation_count": 1,
    "authors": [
      "Navita Goyal",
      "Eleftheria Briakou",
      "Amanda Liu",
      "Connor Baumler",
      "Claire Bonial",
      "Jeffrey Micher",
      "Clare Voss",
      "Marine Carpuat",
      "Hal Daumé III"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.202": {
    "title": "GROOViST: A Metric for Grounding Objects in Visual Storytelling",
    "volume": "main",
    "abstract": "A proper evaluation of stories generated for a sequence of images—the task commonly referred to as visual storytelling—must consider multiple aspects, such as coherence, grammatical correctness, and visual grounding. In this work, we focus on evaluating the degree of grounding, that is, the extent to which a story is about the entities shown in the images. We analyze current metrics, both designed for this purpose and for general vision-text alignment. Given their observed shortcomings, we propose a novel evaluation tool, GROOViST, that accounts for cross-modal dependencies, temporal misalignments (the fact that the order in which entities appear in the story and the image sequence may not match), and human intuitions on visual grounding. An additional advantage of GROOViST is its modular design, where the contribution of each component can be assessed and interpreted individually",
    "checked": true,
    "id": "3286d5ca2c323ebf73ad4976480361f654f67b0c",
    "semantic_title": "groovist: a metric for grounding objects in visual storytelling",
    "citation_count": 0,
    "authors": [
      "Aditya Surikuchi",
      "Sandro Pezzelle",
      "Raquel Fernández"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.203": {
    "title": "VIBE: Topic-Driven Temporal Adaptation for Twitter Classification",
    "volume": "main",
    "abstract": "Language features are evolving in real-world social media, resulting in the deteriorating performance of text classification in dynamics. To address this challenge, we study temporal adaptation, where models trained on past data are tested in the future. Most prior work focused on continued pretraining or knowledge updating, which may compromise their performance on noisy social media data. To tackle this issue, we reflect feature change via modeling latent topic evolution and propose a novel model, VIBE: Variational Information Bottleneck for Evolutions. Concretely, we first employ two Information Bottleneck (IB) regularizers to distinguish past and future topics. Then, the distinguished topics work as adaptive features via multi-task training with timestamp and class label prediction. In adaptive learning, VIBE utilizes retrieved unlabeled data from online streams created posterior to training data time. Substantial Twitter experiments on three classification tasks show that our model, with only 3% of data, significantly outperforms previous state-of-the-art continued-pretraining methods",
    "checked": true,
    "id": "79e5996555961f02af51b6bdc0af6853097a043b",
    "semantic_title": "vibe: topic-driven temporal adaptation for twitter classification",
    "citation_count": 1,
    "authors": [
      "Yuji Zhang",
      "Jing Li",
      "Wenjie Li"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.204": {
    "title": "TOD-Flow: Modeling the Structure of Task-Oriented Dialogues",
    "volume": "main",
    "abstract": "Task-Oriented Dialogue (TOD) systems have become crucial components in interactive artificial intelligence applications. While recent advances have capitalized on pre-trained language models (PLMs), they exhibit limitations regarding transparency and controllability. To address these challenges, we propose a novel approach focusing on inferring the TOD-flow graph from dialogue data annotated with dialog acts, uncovering the underlying task structure in the form of a graph. The inferred TOD-flow graph can be easily integrated with any dialogue model to improve its prediction performance, transparency, and controllability. Our TOD-flow graph learns what a model can, should, and should not predict, effectively reducing the search space and providing a rationale for the model's prediction. We show that the proposed TOD-flow graph better resemble human-annotated graphs compared to prior approaches. Furthermore, when combined with several dialogue policies and end-to-end dialogue models, we demonstrate that our approach significantly improves dialog act classification and end-to-end response generation performance in the MultiWOZ and SGD benchmarks",
    "checked": true,
    "id": "898a7644d6b9e3634e2231e3b739c1a5032419c8",
    "semantic_title": "tod-flow: modeling the structure of task-oriented dialogues",
    "citation_count": 0,
    "authors": [
      "Sungryull Sohn",
      "Yiwei Lyu",
      "Anthony Liu",
      "Lajanugen Logeswaran",
      "Dong-Ki Kim",
      "Dongsub Shim",
      "Honglak Lee"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.205": {
    "title": "TopWORDS-Poetry: Simultaneous Text Segmentation and Word Discovery for Classical Chinese Poetry via Bayesian Inference",
    "volume": "main",
    "abstract": "As a precious cultural heritage of human beings, classical Chinese poetry has a very unique writing style and often contains special words that rarely appear in general Chinese texts, posting critical challenges for natural language processing. Little effort has been made in the literature for processing texts from classical Chinese poetry. This study fills in this gap with TopWORDS-Poetry, an unsupervised method that can achieve reliable text segmentation and word discovery for classical Chinese poetry simultaneously without pre-given vocabulary or training corpus. Experimental studies confirm that TopWORDS-Poetry can successfully recognize unique poetry words, such as named entities and literary allusions, from metrical poems of Complete Tang Poetry and segment these poetry lines into sequences of meaningful words with high quality",
    "checked": true,
    "id": "c5e9171792a719e81fda6f848036ead82badd991",
    "semantic_title": "topwords-poetry: simultaneous text segmentation and word discovery for classical chinese poetry via bayesian inference",
    "citation_count": 0,
    "authors": [
      "Changzai Pan",
      "Feiyue Li",
      "Ke Deng"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.206": {
    "title": "Knowledge Rumination for Pre-trained Language Models",
    "volume": "main",
    "abstract": "Previous studies have revealed that vanilla pre-trained language models (PLMs) lack the capacity to handle knowledge-intensive NLP tasks alone; thus, several works have attempted to integrate external knowledge into PLMs. However, despite the promising outcome, we empirically observe that PLMs may have already encoded rich knowledge in their pre-trained parameters but fails to fully utilize them when applying to knowledge-intensive tasks. In this paper, we propose a new paradigm dubbed Knowledge Rumination to help the pre-trained language model utilize that related latent knowledge without retrieving them from the external corpus. By simply adding a prompt like \"As far as I know\" to the PLMs, we try to review related latent knowledge and inject them back into the model for knowledge consolidation. We apply the proposed knowledge rumination to various language models, including RoBERTa, DeBERTa, and GPT-3. Experimental results on six commonsense reasoning tasks and GLUE benchmarks demonstrate the effectiveness of our proposed approach, which proves that the knowledge stored in PLMs can be better exploited to enhance performance",
    "checked": true,
    "id": "c131ac4360aa9d9b7a955bed87e9cd4a1a3e7562",
    "semantic_title": "knowledge rumination for pre-trained language models",
    "citation_count": 2,
    "authors": [
      "Yunzhi Yao",
      "Peng Wang",
      "Shengyu Mao",
      "Chuanqi Tan",
      "Fei Huang",
      "Huajun Chen",
      "Ningyu Zhang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.207": {
    "title": "Struct-XLM: A Structure Discovery Multilingual Language Model for Enhancing Cross-lingual Transfer through Reinforcement Learning",
    "volume": "main",
    "abstract": "Cross-lingual transfer learning heavily relies on well-aligned cross-lingual representations. The syntactic structure is recognized as beneficial for cross-lingual transfer, but limited researches utilize it for aligning representation in multilingual pre-trained language models (PLMs). Additionally, existing methods require syntactic labels that are difficult to obtain and of poor quality for low-resource languages. To address this gap, we propose Struct-XLM, a novel multilingual language model that leverages reinforcement learning (RL) to autonomously discover universal syntactic structures for improving the cross-lingual representation alignment of PLM. Struct-XLM integrates a policy network (PNet) and a translation ranking task. The PNet is designed to discover structural information and integrate it into the last layer of the PLM through the structural multi-head attention module to obtain structural representation. The translation ranking task obtains a delayed reward based on the structural representation to optimize the PNet while improving the alignment of cross-lingual representation. Experiments show the effectiveness of the proposed approach for enhancing cross-lingual transfer of multilingual PLM on the XTREME benchmark",
    "checked": true,
    "id": "334e62b85c555fa89bba0a25f770898d3b80c6f9",
    "semantic_title": "struct-xlm: a structure discovery multilingual language model for enhancing cross-lingual transfer through reinforcement learning",
    "citation_count": 0,
    "authors": [
      "Linjuan Wu",
      "Weiming Lu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.208": {
    "title": "AdaSent: Efficient Domain-Adapted Sentence Embeddings for Few-Shot Classification",
    "volume": "main",
    "abstract": "Recent work has found that few-shot sentence classification based on pre-trained Sentence Encoders (SEs) is efficient, robust, and effective. In this work, we investigate strategies for domain-specialization in the context of few-shot sentence classification with SEs. We first establish that unsupervised Domain-Adaptive Pre-Training (DAPT) of a base Pre-trained Language Model (PLM) (i.e., not an SE) substantially improves the accuracy of few-shot sentence classification by up to 8.4 points. However, applying DAPT on SEs, on the one hand, disrupts the effects of their (general-domain) Sentence Embedding Pre-Training (SEPT). On the other hand, applying general-domain SEPT on top of a domain-adapted base PLM (i.e., after DAPT) is effective but inefficient, since the computationally expensive SEPT needs to be executed on top of a DAPT-ed PLM of each domain. As a solution, we propose AdaSent, which decouples SEPT from DAPT by training a SEPT adapter on the base PLM. The adapter can be inserted into DAPT-ed PLMs from any domain. We demonstrate AdaSent's effectiveness in extensive experiments on 17 different few-shot sentence classification datasets. AdaSent matches or surpasses the performance of full SEPT on DAPT-ed PLM, while substantially reducing the training costs. The code for AdaSent is available",
    "checked": true,
    "id": "a6432dca42d8c3d9627a65b3fc337f043e865746",
    "semantic_title": "adasent: efficient domain-adapted sentence embeddings for few-shot classification",
    "citation_count": 0,
    "authors": [
      "Yongxin Huang",
      "Kexin Wang",
      "Sourav Dutta",
      "Raj Patel",
      "Goran Glavaš",
      "Iryna Gurevych"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.209": {
    "title": "Interview Evaluation: A Novel Approach for Automatic Evaluation of Conversational Question Answering Models",
    "volume": "main",
    "abstract": "Conversational Question Answering (CQA) aims to provide natural language answers to users in information-seeking dialogues. Existing CQA benchmarks often evaluate models using pre-collected human-human conversations. However, replacing the model-predicted dialogue history with ground truth compromises the naturalness and sustainability of CQA evaluation. While previous studies proposed using predicted history and rewriting techniques to address unresolved coreferences and incoherencies, this approach renders the question self-contained from the conversation. In this paper, we propose a novel automatic evaluation approach, interview evaluation. Specifically, ChatGPT acts as the interviewer (Q agent) with a set of carefully designed prompts, and the CQA model under test serves as the interviewee (A agent). During the interview evaluation, questions are dynamically generated by the Q agent to guide the A agent in predicting the correct answer through an interactive process. We evaluated four different models on QuAC and two models on CoQA in our experiments. The experiment results demonstrate that our interview evaluation has advantages over previous CQA evaluation approaches, particularly in terms of naturalness and coherence. The source code is made publicly available",
    "checked": true,
    "id": "4e09336ee463086b70b8ff51c214003310197b39",
    "semantic_title": "interview evaluation: a novel approach for automatic evaluation of conversational question answering models",
    "citation_count": 0,
    "authors": [
      "Xibo Li",
      "Bowei Zou",
      "Yifan Fan",
      "Yanling Li",
      "Ai Ti Aw",
      "Yu Hong"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.210": {
    "title": "TCFLE-8: a Corpus of Learner Written Productions for French as a Foreign Language and its Application to Automated Essay Scoring",
    "volume": "main",
    "abstract": "Automated Essay Scoring (AES) aims to automatically assess the quality of essays. Automation enables large-scale assessment, improvements in consistency, reliability, and standardization. Those characteristics are of particular relevance in the context of language certification exams. However, a major bottleneck in the development of AES systems is the availability of corpora, which, unfortunately, are scarce, especially for languages other than English. In this paper, we aim to foster the development of AES for French by providing the TCFLE-8 corpus, a corpus of 6.5k essays collected in the context of the Test de Connaissance du Français (TCF - French Knowledge Test) certification exam. We report the strict quality procedure that led to the scoring of each essay by at least two raters according to the CEFR levels and to the creation of a balanced corpus. In addition, we describe how linguistic properties of the essays relate to the learners' proficiency in TCFLE-8. We also advance the state-of-the-art performance for the AES task in French by experimenting with two strong baselines (i.e. RoBERTa and feature-based). Finally, we discuss the challenges of AES using TCFLE-8",
    "checked": true,
    "id": "42416d79cbe2f18d7ea246417e688f7cac29b6a3",
    "semantic_title": "tcfle-8: a corpus of learner written productions for french as a foreign language and its application to automated essay scoring",
    "citation_count": 0,
    "authors": [
      "Rodrigo Wilkens",
      "Alice Pintard",
      "David Alfter",
      "Vincent Folny",
      "Thomas François"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.211": {
    "title": "Dancing Between Success and Failure: Edit-level Simplification Evaluation using SALSA",
    "volume": "main",
    "abstract": "Large language models (e.g., GPT-4) are uniquely capable of producing highly rated text simplification, yet current human evaluation methods fail to provide a clear understanding of systems' specific strengths and weaknesses. To address this limitation, we introduce SALSA, an edit-based human annotation framework that enables holistic and fine-grained text simplification evaluation. We develop twenty one linguistically grounded edit types, covering the full spectrum of success and failure across dimensions of conceptual, syntactic and lexical simplicity. Using SALSA, we collect 19K edit annotations on 840 simplifications, revealing discrepancies in the distribution of simplification strategies performed by fine-tuned models, prompted LLMs and humans, and find GPT-3.5 performs more quality edits than humans, but still exhibits frequent errors. Using our fine-grained annotations, we develop LENS-SALSA, a reference-free automatic simplification metric, trained to predict sentence- and word-level quality simultaneously. Additionally, we introduce word-level quality estimation for simplification and report promising baseline results. Our data, new metric, and annotation toolkit are available at https://salsa-eval.com",
    "checked": true,
    "id": "3d4a89217c8e361c2adc80dec773ff1607f88a43",
    "semantic_title": "dancing between success and failure: edit-level simplification evaluation using salsa",
    "citation_count": 3,
    "authors": [
      "David Heineman",
      "Yao Dou",
      "Mounica Maddela",
      "Wei Xu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.212": {
    "title": "Confidence-based Ensembling of Perspective-aware Models",
    "volume": "main",
    "abstract": "Research in the field of NLP has recently focused on the variability that people show in selecting labels when performing an annotation task. Exploiting disagreements in annotations has been shown to offer advantages for accurate modelling and fair evaluation. In this paper, we propose a strongly perspectivist model for supervised classification of natural language utterances. Our approach combines the predictions of several perspective-aware models using key information of their individual confidence to capture the subjectivity encoded in the annotation of linguistic phenomena. We validate our method through experiments on two case studies, irony and hate speech detection, in in-domain and cross-domain settings. The results show that confidence-based ensembling of perspective-aware models seems beneficial for classification performance in all scenarios. In addition, we demonstrate the effectiveness of our method with automatically extracted perspectives from annotations when the annotators' metadata are not available",
    "checked": true,
    "id": "d0797d8f75d5ac6a6ef8977ae8249dd81e728f98",
    "semantic_title": "confidence-based ensembling of perspective-aware models",
    "citation_count": 0,
    "authors": [
      "Silvia Casola",
      "Soda Lo",
      "Valerio Basile",
      "Simona Frenda",
      "Alessandra Cignarella",
      "Viviana Patti",
      "Cristina Bosco"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.213": {
    "title": "ToViLaG: Your Visual-Language Generative Model is Also An Evildoer",
    "volume": "main",
    "abstract": "Recent large-scale Visual-Language Generative Models (VLGMs) have achieved unprecedented improvement in multimodal image/text generation. However, these models might also generate toxic content, e.g., offensive text and pornography images, raising significant ethical risks. Despite exhaustive studies on toxic degeneration of language models, this problem remains largely unexplored within the context of visual-language generation. This work delves into the propensity for toxicity generation and susceptibility to toxic data across various VLGMs. For this purpose, we built ToViLaG, a dataset comprising 32K co-toxic/mono-toxic text-image pairs and 1K innocuous but evocative text that tends to stimulate toxicity. Furthermore, we propose WInToRe, a novel toxicity metric tailored to visual-language generation, which theoretically reflects different aspects of toxicity considering both input and output. On such a basis, we benchmarked the toxicity of a diverse spectrum of VLGMs and discovered that some models do more evil than expected while some are more vulnerable to infection, underscoring the necessity of VLGMs detoxification. Therefore, we develop an innovative bottleneck-based detoxification method. Our method could reduce toxicity while maintaining comparable generation quality, providing a promising initial solution to this line of research",
    "checked": true,
    "id": "10280c290825fc0b0c884e988f4f1dedb80e4e80",
    "semantic_title": "tovilag: your visual-language generative model is also an evildoer",
    "citation_count": 0,
    "authors": [
      "Xinpeng Wang",
      "Xiaoyuan Yi",
      "Han Jiang",
      "Shanlin Zhou",
      "Zhihua Wei",
      "Xing Xie"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.214": {
    "title": "GPT-RE: In-context Learning for Relation Extraction using Large Language Models",
    "volume": "main",
    "abstract": "In spite of the potential for ground-breaking achievements offered by large language models (LLMs) (e.g., GPT-3) via in-context learning (ICL), they still lag significantly behind fully-supervised baselines (e.g., fine-tuned BERT) in relation extraction (RE). This is due to the two major shortcomings of ICL for RE: (1) low relevance regarding entity and relation in existing sentence-level demonstration retrieval approaches for ICL; and (2) the lack of explaining input-label mappings of demonstrations leading to poor ICL effectiveness. In this paper, we propose GPT-RE to successfully address the aforementioned issues by (1) incorporating task-aware representations in demonstration retrieval; and (2) enriching the demonstrations with gold label-induced reasoning logic. We evaluate GPT-RE on four widely-used RE datasets, and observe that GPT-RE achieves improvements over not only existing GPT-3 baselines, but also fully-supervised baselines as in Figure 1. Specifically, GPT-RE achieves SOTA performances on the Semeval and SciERC datasets, and competitive performances on the TACRED and ACE05 datasets. Additionally, a critical issue of LLMs revealed by previous work, the strong inclination to wrongly classify NULL examples into other pre-defined labels, is substantially alleviated by our method. We show an empirical analysis",
    "checked": true,
    "id": "f2cd02c03d0169374442d9bc227c9aed178f4b20",
    "semantic_title": "gpt-re: in-context learning for relation extraction using large language models",
    "citation_count": 16,
    "authors": [
      "Zhen Wan",
      "Fei Cheng",
      "Zhuoyuan Mao",
      "Qianying Liu",
      "Haiyue Song",
      "Jiwei Li",
      "Sadao Kurohashi"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.215": {
    "title": "Sociocultural Norm Similarities and Differences via Situational Alignment and Explainable Textual Entailment",
    "volume": "main",
    "abstract": "Designing systems that can reason across cultures requires that they are grounded in the norms of the contexts in which they operate. However, current research on developing computational models of social norms has primarily focused on American society. Here, we propose a novel approach to discover and compare descriptive social norms across Chinese and American cultures. We demonstrate our approach by leveraging discussions on a Chinese Q&A platform—Zhihu—and the existing SocialChemistry dataset as proxies for contrasting cultural axes, align social situations cross-culturally, and extract social norms from texts using in-context learning. Embedding Chain-of-Thought prompting in a human-AI collaborative framework, we build a high-quality dataset of 3,069 social norms aligned with social situations across Chinese and American cultures alongside corresponding free-text explanations. To test the ability of models to reason about social norms across cultures, we introduce the task of explainable social norm entailment, showing that existing models under 3B parameters have significant room for improvement in both automatic and human evaluation. Further analysis of cross-cultural norm differences based on our dataset shows empirical alignment with the social orientations framework, revealing several situational and descriptive nuances in norms across these cultures",
    "checked": true,
    "id": "18bd959aaa8a83b5b2192282224d700da7459857",
    "semantic_title": "sociocultural norm similarities and differences via situational alignment and explainable textual entailment",
    "citation_count": 2,
    "authors": [
      "Sky CH-Wang",
      "Arkadiy Saakyan",
      "Oliver Li",
      "Zhou Yu",
      "Smaranda Muresan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.216": {
    "title": "INFORM : Information eNtropy based multi-step reasoning FOR large language Models",
    "volume": "main",
    "abstract": "Large language models (LLMs) have demonstrated exceptional performance in reasoning tasks with dedicated Chain-of-Thought (CoT) prompts. Further enhancing CoT prompts with exquisite exemplars can significantly improve reasoning performance.However, the effectiveness of CoT prompts may fluctuate dramatically with different choices of in-context examples. Additionally, manual construction of rationale steps can be time-consuming, presenting challenges for the widespread adoption of CoT prompting. In this work, we propose a novel approach by introducing information entropy (IE) as a criteria on for CoT prompt selection. We extend this criterion to the CoT generation and inference stages, automatically generating CoT prompts with higher information entropy scores and adaptively determining the number of samples. These three stages together form our proposed information- entropy-based multi-step reasoning for large language models, named INFORM. Our experiments across seven reasoning benchmarks utilizing two language models(GPT-3.5-Turbo and text-davinci-003) demonstrate the superiority of INFORM both in performance and efficiency",
    "checked": true,
    "id": "d42e04e2650b85495aa695a90aaf437b5ad90516",
    "semantic_title": "inform : information entropy based multi-step reasoning for large language models",
    "citation_count": 0,
    "authors": [
      "Chuyue Zhou",
      "Wangjie You",
      "Juntao Li",
      "Jing Ye",
      "Kehai Chen",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.217": {
    "title": "Adaptive Gating in Mixture-of-Experts based Language Models",
    "volume": "main",
    "abstract": "Large language models have demonstrated exceptional language understanding capabilities in many NLP tasks. Sparsely activated mixture-of-experts (MoE) has emerged as a promising solution for scaling models while maintaining a constant number of computational operations. Existing MoE models adopt a fixed gating network where each token is computed by the same number of experts. This contradicts our intuition that the tokens in each sequence vary in terms of their linguistic complexity and, consequently, require different computational costs. Little is discussed in prior research on the trade-off between computation per token and model performance. This paper introduces adaptive gating in MoE, a flexible training strategy that allows tokens to be processed by a variable number of experts based on expert probability distribution. Adaptive gating preserves sparsity while improving training efficiency. We further draw upon curriculum learning to better align the order of training samples and maximize the training time savings. Extensive experiments on diverse NLP tasks show that adaptive gating reduces at most 22.5% training time while maintaining inference quality. Moreover, we conduct a comprehensive analysis of the gating decisions and present our insights on which tokens are inherently difficult to process, depending on the specific language task",
    "checked": true,
    "id": "59472fe8bba99c998fb119fa684a80423d1f2f09",
    "semantic_title": "adaptive gating in mixture-of-experts based language models",
    "citation_count": 0,
    "authors": [
      "Jiamin Li",
      "Qiang Su",
      "Yitao Yang",
      "Yimin Jiang",
      "Cong Wang",
      "Hong Xu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.218": {
    "title": "On the Automatic Generation and Simplification of Children's Stories",
    "volume": "main",
    "abstract": "With recent advances in large language models (LLMs), the concept of automatically generating children's educational materials has become increasingly realistic. Working toward the goal of age-appropriate simplicity in generated educational texts, we first examine the ability of several popular LLMs to generate stories with properly adjusted lexical and readability levels. We find that, in spite of the growing capabilities of LLMs, they do not yet possess the ability to limit their vocabulary to levels appropriate for younger age groups. As a second experiment, we explore the ability of state-of-the-art lexical simplification models to generalize to the domain of children's stories and, thus, create an efficient pipeline for their automatic generation. In order to test these models, we develop a dataset of child-directed lexical simplification instances, with examples taken from the LLM-generated stories in our first experiment. We find that, while the strongest-performing current lexical simplification models do not perform as well on material designed for children due to their reliance on large language models behind the scenes, some models that still achieve fairly strong results on general data can mimic or even improve their performance on children-directed data with proper fine-tuning, which we conduct using our newly created child-directed simplification dataset",
    "checked": true,
    "id": "a952e79b7ceef1994a8c26c0c8d0d5445cf4f9cf",
    "semantic_title": "on the automatic generation and simplification of children's stories",
    "citation_count": 0,
    "authors": [
      "Maria Valentini",
      "Jennifer Weber",
      "Jesus Salcido",
      "Téa Wright",
      "Eliana Colunga",
      "Katharina von der Wense"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.219": {
    "title": "When Do Decompositions Help for Machine Reading?",
    "volume": "main",
    "abstract": "Answering complex questions often requires multi-step reasoning in order to obtain the final answer. Most research into decompositions of complex questions involves open-domain systems, which have shown success in using these decompositions for improved retrieval. In the machine reading setting, however, work to understand when decompositions are helpful is understudied. We conduct experiments on decompositions in machine reading to unify recent work in this space, using a range of models and datasets. We find that decompositions can be helpful in zero or limited-data settings, giving several points of improvement in exact match. However, we also show that when models are given access to around a few hundred or more examples, decompositions are not helpful (and can actually be detrimental). Thus, our analysis implies that models can learn decompositions implicitly even with limited data",
    "checked": true,
    "id": "624ea7bdaf7e8e3f7bd76f72aa665b562f0dd70a",
    "semantic_title": "when do decompositions help for machine reading?",
    "citation_count": 0,
    "authors": [
      "Kangda Wei",
      "Dawn Lawrie",
      "Benjamin Van Durme",
      "Yunmo Chen",
      "Orion Weller"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.220": {
    "title": "The Curious Case of Hallucinatory (Un)answerability: Finding Truths in the Hidden States of Over-Confident Large Language Models",
    "volume": "main",
    "abstract": "Large language models (LLMs) have been shown to possess impressive capabilities, while also raising crucial concerns about the faithfulness of their responses. A primary issue arising in this context is the management of (un)answerable queries by LLMs, which often results in hallucinatory behavior due to overconfidence. In this paper, we explore the behavior of LLMs when presented with (un)answerable queries. We ask: do models represent the fact that the question is (un)answerable when generating a hallucinatory answer? Our results show strong indications that such models encode the answerability of an input query, with the representation of the first decoded token often being a strong indicator. These findings shed new light on the spatial organization within the latent representations of LLMs, unveiling previously unexplored facets of these models. Moreover, they pave the way for the development of improved decoding techniques with better adherence to factual generation, particularly in scenarios where query (un)answerability is a concern",
    "checked": false,
    "id": "a267cfbd6930c8e2c720104ae4a90e39461a6694",
    "semantic_title": "the curious case of hallucinatory unanswerablity: finding truths in the hidden states of over-confident large language models",
    "citation_count": 0,
    "authors": [
      "Aviv Slobodkin",
      "Omer Goldman",
      "Avi Caciularu",
      "Ido Dagan",
      "Shauli Ravfogel"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.221": {
    "title": "Identifying Informational Sources in News Articles",
    "volume": "main",
    "abstract": "News articles are driven by the informational sources journalists use in reporting. Modeling when, how and why sources get used together in stories can help us better understand the information we consume and even help journalists with the task of producing it. In this work, we take steps toward this goal by constructing the largest and widest-ranging annotated dataset, to date, of informational sources used in news writing. We first show that our dataset can be used to train high-performing models for information detection and source attribution. Then, we introduce a novel task, source prediction, to study the compositionality of sources in news articles – i.e. how they are chosen to complement each other. We show good modeling performance on this task, indicating that there is a pattern to the way different sources are used together in news storytelling. This insight opens the door for a focus on sources in narrative science (i.e. planning-based language generation) and computational journalism (i.e. a source-recommendation system to aid journalists writing stories). All data and model code can be found at https://github.com/alex2awesome/source-exploration",
    "checked": true,
    "id": "230d1486a8030fcf2b4cdad133642bb2dd226c60",
    "semantic_title": "identifying informational sources in news articles",
    "citation_count": 1,
    "authors": [
      "Alexander Spangher",
      "Nanyun Peng",
      "Emilio Ferrara",
      "Jonathan May"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.222": {
    "title": "Retrofitting Light-weight Language Models for Emotions using Supervised Contrastive Learning",
    "volume": "main",
    "abstract": "We present a novel retrofitting method to induce emotion aspects into pre-trained language models (PLMs) such as BERT and RoBERTa. Our method updates pre-trained network weights using contrastive learning so that the text fragments exhibiting similar emotions are encoded nearby in the representation space, and the fragments with different emotion content are pushed apart. While doing so, it also ensures that the linguistic knowledge already present in PLMs is not inadvertently perturbed. The language models retrofitted by our method, i.e., BERTEmo and RoBERTaEmo, produce emotion-aware text representations, as evaluated through different clustering and retrieval metrics. For the downstream tasks on sentiment analysis and sarcasm detection, they perform better than their pre-trained counterparts (about 1% improvement in F1-score) and other existing approaches. Additionally, a more significant boost in performance is observed for the retrofitted models over pre-trained ones in few-shot learning setting",
    "checked": true,
    "id": "36d6d614c7b36144c59e3892a6812b2a68af2c93",
    "semantic_title": "retrofitting light-weight language models for emotions using supervised contrastive learning",
    "citation_count": 0,
    "authors": [
      "Sapan Shah",
      "Sreedhar Reddy",
      "Pushpak Bhattacharyya"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.223": {
    "title": "Longtriever: a Pre-trained Long Text Encoder for Dense Document Retrieval",
    "volume": "main",
    "abstract": "Pre-trained language models (PLMs) have achieved the preeminent position in dense retrieval due to their powerful capacity in modeling intrinsic semantics. However, most existing PLM-based retrieval models encounter substantial computational costs and are infeasible for processing long documents. In this paper, a novel retrieval model Longtriever is proposed to embrace three core challenges of long document retrieval: substantial computational cost, incomprehensive document understanding, and scarce annotations. Longtriever splits long documents into short blocks and then efficiently models the local semantics within a block and the global context semantics across blocks in a tightly-coupled manner. A pre-training phase is further proposed to empower Longtriever to achieve a better understanding of underlying semantic correlations. Experimental results on two popular benchmark datasets demonstrate the superiority of our proposal",
    "checked": true,
    "id": "3e2708cca97f36bfc70cf1c907bda8f3755a6999",
    "semantic_title": "longtriever: a pre-trained long text encoder for dense document retrieval",
    "citation_count": 0,
    "authors": [
      "Junhan Yang",
      "Zheng Liu",
      "Chaozhuo Li",
      "Guangzhong Sun",
      "Xing Xie"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.224": {
    "title": "Revisiting De-Identification of Electronic Medical Records: Evaluation of Within- and Cross-Hospital Generalization",
    "volume": "main",
    "abstract": "The de-identification task aims to detect and remove the protected health information from electronic medical records (EMRs). Previous studies generally focus on the within-hospital setting and achieve great successes, while the cross-hospital setting has been overlooked. This study introduces a new de-identification dataset comprising EMRs from three hospitals in China, creating a benchmark for evaluating both within- and cross-hospital generalization. We find significant domain discrepancy between hospitals. A model with almost perfect within-hospital performance struggles when transferred across hospitals. Further experiments show that pretrained language models and some domain generalization methods can alleviate this problem. We believe that our data and findings will encourage investigations on the generalization of medical NLP models",
    "checked": true,
    "id": "6a4f391f23fe8cb11758995a192035020815f179",
    "semantic_title": "revisiting de-identification of electronic medical records: evaluation of within- and cross-hospital generalization",
    "citation_count": 0,
    "authors": [
      "Yiyang Liu",
      "Jinpeng Li",
      "Enwei Zhu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.225": {
    "title": "Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) prompted to generate chain-of-thought (CoT) exhibit impressive reasoning capabilities. Recent attempts at prompt decomposition toward solving complex, multi-step reasoning problems depend on the ability of the LLM to simultaneously decompose and solve the problem. A significant disadvantage is that foundational LLMs are typically not available for fine-tuning, making adaptation computationally prohibitive. We believe (and demonstrate) that problem decomposition and solution generation are distinct capabilites, better addressed in separate modules, than by one monolithic LLM. We introduce DaSLaM, which uses a decomposition generator to decompose complex problems into subproblems that require fewer reasoning steps. These subproblems are answered by a solver. We use a relatively small (13B parameters) LM as the decomposition generator, which we train using policy gradient optimization to interact with a solver LM (regarded as black-box) and guide it through subproblems, thereby rendering our method solver-agnostic. Evaluation on multiple different reasoning datasets reveal that with our method, a 175 billion parameter LM (text-davinci-003) can produce competitive or even better performance, compared to its orders-of-magnitude larger successor, GPT-4. Additionally, we show that DaSLaM is not limited by the solver's capabilities as a function of scale; e.g., solver LMs with diverse sizes give significant performance improvement with our solver-agnostic decomposition technique. Exhaustive ablation studies evince the superiority of our modular finetuning technique over exorbitantly large decomposer LLMs, based on prompting alone",
    "checked": true,
    "id": "f9465e71697cae802d66a66eb307f0a809773cd3",
    "semantic_title": "small language models fine-tuned to coordinate larger language models improve complex reasoning",
    "citation_count": 0,
    "authors": [
      "Gurusha Juneja",
      "Subhabrata Dutta",
      "Soumen Chakrabarti",
      "Sunny Manchanda",
      "Tanmoy Chakraborty"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.226": {
    "title": "Language Representation Projection: Can We Transfer Factual Knowledge across Languages in Multilingual Language Models?",
    "volume": "main",
    "abstract": "Multilingual pretrained language models serve as repositories of multilingual factual knowledge. Nevertheless, a substantial performance gap of factual knowledge probing exists between high-resource languages and low-resource languages, suggesting limited implicit factual knowledge transfer across languages in multilingual pretrained language models. This paper investigates the feasibility of explicitly transferring relatively rich factual knowledge from English to non-English languages. To accomplish this, we propose two parameter-free Language Representation Projection modules (LRP2). The first module converts non-English representations into English-like equivalents, while the second module reverts English-like representations back into representations of the corresponding non-English language. Experimental results on the mLAMA dataset demonstrate that LRP2 significantly improves factual knowledge retrieval accuracy and facilitates knowledge transferability across diverse non-English languages. We further investigate the working mechanism of LRP2 from the perspectives of representation space and cross-lingual knowledge neuron",
    "checked": true,
    "id": "f3d5f9a6e4dd61962736988937100b2168f5ba1c",
    "semantic_title": "language representation projection: can we transfer factual knowledge across languages in multilingual language models?",
    "citation_count": 0,
    "authors": [
      "Shaoyang Xu",
      "Junzhuo Li",
      "Deyi Xiong"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.227": {
    "title": "Structural Priming Demonstrates Abstract Grammatical Representations in Multilingual Language Models",
    "volume": "main",
    "abstract": "Abstract grammatical knowledge—of parts of speech and grammatical patterns—is key to the capacity for linguistic generalization in humans. But how abstract is grammatical knowledge in large language models? In the human literature, compelling evidence for grammatical abstraction comes from structural priming. A sentence that shares the same grammatical structure as a preceding sentence is processed and produced more readily. Because confounds exist when using stimuli in a single language, evidence of abstraction is even more compelling from crosslingual structural priming, where use of a syntactic structure in one language primes an analogous structure in another language. We measure crosslingual structural priming in large language models, comparing model behavior to human experimental results from eight crosslingual experiments covering six languages, and four monolingual structural priming experiments in three non-English languages. We find evidence for abstract monolingual and crosslingual grammatical representations in the models that function similarly to those found in humans. These results demonstrate that grammatical representations in multilingual language models are not only similar across languages, but they can causally influence text produced in different languages",
    "checked": true,
    "id": "ea924dc8f307e96c93c5085616edd2e2bbc1e4cb",
    "semantic_title": "structural priming demonstrates abstract grammatical representations in multilingual language models",
    "citation_count": 0,
    "authors": [
      "James Michaelov",
      "Catherine Arnett",
      "Tyler Chang",
      "Ben Bergen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.228": {
    "title": "ReasoningLM: Enabling Structural Subgraph Reasoning in Pre-trained Language Models for Question Answering over Knowledge Graph",
    "volume": "main",
    "abstract": "Question Answering over Knowledge Graph (KGQA) aims to seek answer entities for the natural language question from a large-scale Knowledge Graph (KG). To better perform reasoning on KG, recent work typically adopts a pre-trained language model (PLM) to model the question, and a graph neural network (GNN) based module to perform multi-hop reasoning on the KG. Despite the effectiveness, due to the divergence in model architecture, the PLM and GNN are not closely integrated, limiting the knowledge sharing and fine-grained feature interactions. To solve it, we aim to simplify the above two-module approach, and develop a more capable PLM that can directly support subgraph reasoning for KGQA, namely ReasoningLM. In our approach, we propose a subgraph-aware self-attention mechanism to imitate the GNN for performing structured reasoning, and also adopt an adaptation tuning strategy to adapt the model parameters with 20,000 subgraphs with synthesized questions. After adaptation, the PLM can be parameter-efficient fine-tuned on downstream tasks. Experiments show that ReasoningLM surpasses state-of-the-art models by a large margin, even with fewer updated parameters and less training data. Our codes and data are publicly available at https://github.com/RUCAIBox/ReasoningLM",
    "checked": true,
    "id": "1866cf8e89f5f3375ca6abc019bdf963bd486d56",
    "semantic_title": "reasoninglm: enabling structural subgraph reasoning in pre-trained language models for question answering over knowledge graph",
    "citation_count": 0,
    "authors": [
      "Jinhao Jiang",
      "Kun Zhou",
      "Xin Zhao",
      "Yaliang Li",
      "Ji-Rong Wen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.229": {
    "title": "Deep Natural Language Feature Learning for Interpretable Prediction",
    "volume": "main",
    "abstract": "We propose a general method to break down a main complex task into a set of intermediary easier sub-tasks, which are formulated in natural language as binary questions related to the final target task. Our method allows for representing each example by a vector consisting of the answers to these questions. We call this representation Natural Language Learned Features (NLLF). NLLF is generated by a small transformer language model (e.g., BERT) that has been trained in a Natural Language Inference (NLI) fashion, using weak labels automatically obtained from a Large Language Model (LLM). We show that the LLM normally struggles for the main task using in-context learning, but can handle these easiest subtasks and produce useful weak labels to train a BERT. The NLI-like training of the BERT allows for tackling zero-shot inference with any binary question, and not necessarily the ones seen during the training. We show that this NLLF vector not only helps to reach better performances by enhancing any classifier, but that it can be used as input of an easy-to-interpret machine learning model like a decision tree. This decision tree is interpretable but also reaches high performances, surpassing those of a pre-trained transformer in some cases. We have successfully applied this method to two completely different tasks: detecting incoherence in students' answers to open-ended mathematics exam questions, and screening abstracts for a systematic literature review of scientific papers on climate change and agroecology",
    "checked": true,
    "id": "b5cd009b9b6cdc293d70aa4d869cb8ea5d9e2da6",
    "semantic_title": "deep natural language feature learning for interpretable prediction",
    "citation_count": 0,
    "authors": [
      "Felipe Urrutia",
      "Cristian Calderon",
      "Valentin Barriere"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.230": {
    "title": "ROBBIE: Robust Bias Evaluation of Large Generative Language Models",
    "volume": "main",
    "abstract": "As generative large language models (LLMs) grow more performant and prevalent, we must develop comprehensive enough tools to measure and improve their fairness. Different prompt-based datasets can be used to measure social bias across multiple text domains and demographic axes, meaning that testing LLMs on more datasets can potentially help us characterize their biases more fully, and better ensure equal and equitable treatment of marginalized demographic groups. In this work, our focus is two-fold: (1) Benchmarking: a comparison of 6 different prompt-based bias and toxicity metrics across 12 demographic axes and 5 families of generative LLMs. Out of those 6 metrics, AdvPromptSet and HolisticBiasR are novel datasets proposed in the paper. The comparison of those benchmarks gives us insights about the bias and toxicity of the compared models. Therefore, we explore the frequency of demographic terms in common LLM pre-training corpora and how this may relate to model biases. (2) Mitigation: we conduct a comprehensive study of how well 3 bias/toxicity mitigation techniques perform across our suite of measurements. ROBBIE aims to provide insights for practitioners while deploying a model, emphasizing the need to not only measure potential harms, but also understand how they arise by characterizing the data, mitigate harms once found, and balance any trade-offs. We open-source our analysis code in hopes of encouraging broader measurements of bias in future LLMs",
    "checked": true,
    "id": "14ba788bf3b55ddcb515aad2deb45c6a4422e473",
    "semantic_title": "robbie: robust bias evaluation of large generative language models",
    "citation_count": 0,
    "authors": [
      "David Esiobu",
      "Xiaoqing Tan",
      "Saghar Hosseini",
      "Megan Ung",
      "Yuchen Zhang",
      "Jude Fernandes",
      "Jane Dwivedi-Yu",
      "Eleonora Presani",
      "Adina Williams",
      "Eric Smith"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.231": {
    "title": "Enhancing Task-oriented Dialogue Systems with Generative Post-processing Networks",
    "volume": "main",
    "abstract": "Recently, post-processing networks (PPNs), which modify the outputs of arbitrary modules including non-differentiable ones in task-oriented dialogue systems, have been proposed. PPNs have successfully improved the dialogue performance by post-processing natural language understanding (NLU), dialogue state tracking (DST), and dialogue policy (Policy) modules with a classification-based approach. However, they cannot be applied to natural language generation (NLG) modules because the post-processing of the utterance output by the NLG module requires a generative approach. In this study, we propose a new post-processing component for NLG, generative post-processing networks (GenPPNs). For optimizing GenPPNs via reinforcement learning, the reward function incorporates dialogue act contribution, a new measure to evaluate the contribution of GenPPN-generated utterances with regard to task completion in dialogue. Through simulation and human evaluation experiments based on the MultiWOZ dataset, we confirmed that GenPPNs improve the task completion performance of task-oriented dialogue systems",
    "checked": true,
    "id": "bf895a14dcd66d0dd285ccd73fe63380372918e9",
    "semantic_title": "enhancing task-oriented dialogue systems with generative post-processing networks",
    "citation_count": 0,
    "authors": [
      "Atsumoto Ohashi",
      "Ryuichiro Higashinaka"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.232": {
    "title": "Adapting Language Models to Compress Contexts",
    "volume": "main",
    "abstract": "Transformer-based language models (LMs) are powerful and widely-applicable tools, but their usefulness is constrained by a finite context window and the expensive computational cost of processing long text documents. We propose to adapt pre-trained LMs into AutoCompressors. These language models are capable of compressing long contexts into summary vectors, which are then accessible to the model as soft prompts. Summary vectors are trained with an unsupervised objective, whereby long documents are processed in segments, and summary vectors from all previous segments are used in language modeling. We fine-tune OPT and Llama-2 models on sequences of up to 30,720 tokens and show that AutoCompressors can utilize long contexts to improve perplexity. We evaluate AutoCompressors on in-context learning by compressing task demonstrations and find that summary vectors are good substitutes for plain-text demonstrations, increasing accuracy while reducing inference costs. Finally, we explore the benefits of pre-computing summary vectors for large corpora by applying summary vectors to retrieval-augmented language modeling and a passage re-ranking task. Overall, AutoCompressors emerge as a simple and inexpensive solution to extend the context window of LMs while speeding up inference over long contexts",
    "checked": true,
    "id": "2f7364d8e5cf94315bf8905f57de9c5543e9a4bf",
    "semantic_title": "adapting language models to compress contexts",
    "citation_count": 12,
    "authors": [
      "Alexis Chevalier",
      "Alexander Wettig",
      "Anirudh Ajith",
      "Danqi Chen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.233": {
    "title": "Selective Labeling: How to Radically Lower Data-Labeling Costs for Document Extraction Models",
    "volume": "main",
    "abstract": "Building automatic extraction models for visually rich documents like invoices, receipts, bills, tax forms, etc. has received significant attention lately. A key bottleneck in developing extraction models for new document types is the cost of acquiring the several thousand high-quality labeled documents that are needed to train a model with acceptable accuracy. In this paper, we propose selective labeling as a solution to this problem. The key insight is to simplify the labeling task to provide \"yes/no\" labels for candidate extractions predicted by a model trained on partially labeled documents. We combine this with a custom active learning strategy to find the predictions that the model is most uncertain about. We show through experiments on document types drawn from 3 different domains that selective labeling can reduce the cost of acquiring labeled data by 10× with a negligible loss in accuracy",
    "checked": true,
    "id": "75b646bb409fcbe6dc77d4f27ab1363575ce25fd",
    "semantic_title": "selective labeling: how to radically lower data-labeling costs for document extraction models",
    "citation_count": 0,
    "authors": [
      "Yichao Zhou",
      "James Wendt",
      "Navneet Potti",
      "Jing Xie",
      "Sandeep Tata"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.234": {
    "title": "TRAVEL: Tag-Aware Conversational FAQ Retrieval via Reinforcement Learning",
    "volume": "main",
    "abstract": "Efficiently retrieving FAQ questions that match users' intent is essential for online customer service. Existing methods aim to fully utilize the dynamic conversation context to enhance the semantic association between the user query and FAQ questions. However, the conversation context contains noise, e.g., users may click questions they don't like, leading to inaccurate semantics modeling. To tackle this, we introduce tags of FAQ questions, which can help us eliminate irrelevant information. We later integrate them into a reinforcement learning framework and minimize the negative impact of irrelevant information in the dynamic conversation context. We experimentally demonstrate our efficiency and effectiveness on conversational FAQ retrieval compared to other baselines",
    "checked": true,
    "id": "b60c7d72b42d37b8afc2bcdbcdcf191913bce469",
    "semantic_title": "travel: tag-aware conversational faq retrieval via reinforcement learning",
    "citation_count": 0,
    "authors": [
      "Yue Chen",
      "Dingnan Jin",
      "Chen Huang",
      "Jia Liu",
      "Wenqiang Lei"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.235": {
    "title": "Continual Dialogue State Tracking via Example-Guided Question Answering",
    "volume": "main",
    "abstract": "Dialogue systems are frequently updated to accommodate new services, but naively updating them by continually training with data for new services in diminishing performance on previously learnt services. Motivated by the insight that dialogue state tracking (DST), a crucial component of dialogue systems that estimates the user's goal as a conversation proceeds, is a simple natural language understanding task, we propose reformulating it as a bundle of granular example-guided question answering tasks to minimize the task shift between services and thus benefit continual learning. Our approach alleviates service-specific memorization and teaches a model to contextualize the given question and example to extract the necessary information from the conversation. We find that a model with just 60M parameters can achieve a significant boost by learning to learn from in-context examples retrieved by a retriever trained to identify turns with similar dialogue state changes. Combining our method with dialogue-level memory replay, our approach attains state of the art performance on DST continual learning metrics without relying on any complex regularization or parameter expansion methods",
    "checked": true,
    "id": "a5688a783312d389bf60141b2a01538bfdddc956",
    "semantic_title": "continual dialogue state tracking via example-guided question answering",
    "citation_count": 0,
    "authors": [
      "Hyundong Cho",
      "Andrea Madotto",
      "Zhaojiang Lin",
      "Khyathi Chandu",
      "Satwik Kottur",
      "Jing Xu",
      "Jonathan May",
      "Chinnadhurai Sankar"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.236": {
    "title": "Lost in Translation, Found in Spans: Identifying Claims in Multilingual Social Media",
    "volume": "main",
    "abstract": "Claim span identification (CSI) is an important step in fact-checking pipelines, aiming to identify text segments that contain a check-worthy claim or assertion in a social media post. Despite its importance to journalists and human fact-checkers, it remains a severely understudied problem, and the scarce research on this topic so far has only focused on English. Here we aim to bridge this gap by creating a novel dataset, X-CLAIM, consisting of 7K real-world claims collected from numerous social media platforms in five Indian languages and English. We report strong baselines with state-of-the-art encoder-only language models (e.g., XLM-R) and we demonstrate the benefits of training on multiple languages over alternative cross-lingual transfer methods such as zero-shot transfer, or training on translated data, from a high-resource language such as English. We evaluate generative large language models from the GPT series using prompting methods on the X-CLAIM dataset and we find that they underperform the smaller encoder-only language models for low-resource languages",
    "checked": true,
    "id": "792fca11821edf87e37f42de650f827613ccbac5",
    "semantic_title": "lost in translation, found in spans: identifying claims in multilingual social media",
    "citation_count": 2,
    "authors": [
      "Shubham Mittal",
      "Megha Sundriyal",
      "Preslav Nakov"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.237": {
    "title": "COVID-19 Vaccine Misinformation in Middle Income Countries",
    "volume": "main",
    "abstract": "This paper introduces a multilingual dataset of COVID-19 vaccine misinformation, consisting of annotated tweets from three middle-income countries: Brazil, Indonesia, and Nigeria. The expertly curated dataset includes annotations for 5,952 tweets, assessing their relevance to COVID-19 vaccines, presence of misinformation, and the themes of the misinformation. To address challenges posed by domain specificity, the low-resource setting, and data imbalance, we adopt two approaches for developing COVID-19 vaccine misinformation detection models: domain-specific pre-training and text augmentation using a large language model. Our best misinformation detection models demonstrate improvements ranging from 2.7 to 15.9 percentage points in macro F1-score compared to the baseline models. Additionally, we apply our misinformation detection models in a large-scale study of 19 million unlabeled tweets from the three countries between 2020 and 2022, showcasing the practical application of our dataset and models for detecting and analyzing vaccine misinformation in multiple countries and languages. Our analysis indicates that percentage changes in the number of new COVID-19 cases are positively associated with COVID-19 vaccine misinformation rates in a staggered manner for Brazil and Indonesia, and there are significant positive associations between the misinformation rates across the three countries",
    "checked": true,
    "id": "03407071e0a1f3fa893e44f18d499a3f00dae4dd",
    "semantic_title": "covid-19 vaccine misinformation in middle income countries",
    "citation_count": 0,
    "authors": [
      "Jongin Kim",
      "Byeo Bak",
      "Aditya Agrawal",
      "Jiaxi Wu",
      "Veronika Wirtz",
      "Traci Hong",
      "Derry Wijaya"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.238": {
    "title": "Contrastive Learning of Sentence Embeddings from Scratch",
    "volume": "main",
    "abstract": "Contrastive learning has been the dominant approach to train state-of-the-art sentence embeddings. Previous studies have typically learned sentence embeddings either through the use of human-annotated natural language inference (NLI) data or via large-scale unlabeled sentences in an unsupervised manner. However, even in the case of unlabeled data, their acquisition presents challenges in certain domains due to various reasons. due to copyright restrictions, data distribution issues, and messy formats, among other factors. To address these issues, we present SynCSE, a contrastive learning framework that trains sentence embeddings with synthetic data. Specifically, we explore utilizing large language models to synthesize the required data samples for contrastive learning, including (1) producing positive and negative annotations given unlabeled sentences SynCSE-partial, and (2) generating sentences along with their corresponding annotations from scratch SynCSE-scratch. Notably, SynCSE-scratch constitutes the first contrastive learning method to learn sentence embeddings from scratch without manually collecting any data sample. Experimental results on sentence similarity and reranking tasks indicate that both SynCSE-partial and SynCSE-scratch greatly outperform unsupervised baselines, and SynCSE-partial even achieves comparable performance to the supervised models in most settings",
    "checked": true,
    "id": "65e5f2c5fafa70088169bc4d27abc40406f08b1f",
    "semantic_title": "contrastive learning of sentence embeddings from scratch",
    "citation_count": 1,
    "authors": [
      "Junlei Zhang",
      "Zhenzhong Lan",
      "Junxian He"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.239": {
    "title": "A Rose by Any Other Name would not Smell as Sweet: Social Bias in Names Mistranslation",
    "volume": "main",
    "abstract": "We ask the question: Are there widespread disparities in machine translations of names across race/ethnicity, and gender? We hypothesize that the translation quality of names and surrounding context will be lower for names associated with US racial and ethnic minorities due to these systems' tendencies to standardize language to predominant language patterns. We develop a dataset of names that are strongly demographically aligned and propose a translation evaluation procedure based on round-trip translation. We analyze the effect of name demographics on translation quality using generalized linear mixed effects models and find that the ability of translation systems to correctly translate female-associated names is significantly lower than male-associated names. This effect is particularly pronounced for female-associated names that are also associated with racial (Black) and ethnic (Hispanic) minorities. This disparity in translation quality between social groups for something as personal as someone's name has significant implications for people's professional, personal, and cultural identities, self-worth and ease of communication. Our findings suggest that more MT research is needed to improve the translation of names and to provide high-quality service for users regardless of gender, race, and ethnicity",
    "checked": true,
    "id": "e6588892d18e86537ffadfcb9e987c513b1760e5",
    "semantic_title": "a rose by any other name would not smell as sweet: social bias in names mistranslation",
    "citation_count": 0,
    "authors": [
      "Sandra Sandoval",
      "Jieyu Zhao",
      "Marine Carpuat",
      "Hal Daumé III"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.240": {
    "title": "Investigating Efficiently Extending Transformers for Long Input Summarization",
    "volume": "main",
    "abstract": "While large pretrained Transformer models have proven highly capable at tackling natural language tasks, handling long sequence inputs still poses a significant challenge. One such task is long input summarization, where inputs are longer than the maximum input context of most models. Through an extensive set of experiments, we investigate what model architectural changes and pretraining paradigms most efficiently adapt a pretrained Transformer for long input summarization. We find that a staggered, block-local Transformer with global encoder tokens strikes a good balance of performance and efficiency, and that an additional pretraining phase on long sequences meaningfully improves downstream summarization performance. Based on our findings, we introduce PEGASUS-X, an extension of the PEGASUS model with additional long input pretraining to handle inputs of up to 16K tokens, which achieves strong performance on long input summarization tasks comparable with much larger models",
    "checked": true,
    "id": "3b39efe6c91ae432dd35bb79431edb8a6719f906",
    "semantic_title": "investigating efficiently extending transformers for long input summarization",
    "citation_count": 27,
    "authors": [
      "Jason Phang",
      "Yao Zhao",
      "Peter Liu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.241": {
    "title": "CS2W: A Chinese Spoken-to-Written Style Conversion Dataset with Multiple Conversion Types",
    "volume": "main",
    "abstract": "Spoken texts (either manual or automatic transcriptions from automatic speech recognition (ASR)) often contain disfluencies and grammatical errors, which pose tremendous challenges to downstream tasks. Converting spoken into written language is hence desirable. Unfortunately, the availability of datasets for this is limited. To address this issue, we present CS2W, a Chinese Spoken-to-Written style conversion dataset comprising 7,237 spoken sentences extracted from transcribed conversational texts. Four types of conversion problems are covered in CS2W: disfluencies, grammatical errors, ASR transcription errors, and colloquial words. Our annotation convention, data, and code are publicly available at https://github.com/guozishan/CS2W",
    "checked": true,
    "id": "5b6d23bbef701348d61609a2b49d9130ceb87bc6",
    "semantic_title": "cs2w: a chinese spoken-to-written style conversion dataset with multiple conversion types",
    "citation_count": 0,
    "authors": [
      "Zishan Guo",
      "Linhao Yu",
      "Minghui Xu",
      "Renren Jin",
      "Deyi Xiong"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.242": {
    "title": "Unifying Cross-Lingual Transfer across Scenarios of Resource Scarcity",
    "volume": "main",
    "abstract": "The scarcity of data in many of the world's languages necessitates the transfer of knowledge from other, resource-rich languages. However, the level of scarcity varies significantly across multiple dimensions, including: i) the amount of task-specific data available in the source and target languages; ii) the amount of monolingual and parallel data available for both languages; and iii) the extent to which they are supported by pretrained multilingual and translation models. Prior work has largely treated these dimensions and the various techniques for dealing with them separately; in this paper, we offer a more integrated view by exploring how to deploy the arsenal of cross-lingual transfer tools across a range of scenarios, especially the most challenging, low-resource ones. To this end, we run experiments on the AmericasNLI and NusaX benchmarks over 20 languages, simulating a range of few-shot settings. The best configuration in our experiments employed parameter-efficient language and task adaptation of massively multilingual Transformers, trained simultaneously on source language data and both machine-translated and natural data for multiple target languages. In addition, we show that pre-trained translation models can be easily adapted to unseen languages, thus extending the range of our hybrid technique and translation-based transfer more broadly. Beyond new insights into the mechanisms of cross-lingual transfer, we hope our work will provide practitioners with a toolbox to integrate multiple techniques for different real-world scenarios. Our code is available at https://github.com/parovicm/unified-xlt",
    "checked": true,
    "id": "0d63cdd04d4b36b76e04e4d9f06ac8418a3ec8bf",
    "semantic_title": "unifying cross-lingual transfer across scenarios of resource scarcity",
    "citation_count": 0,
    "authors": [
      "Alan Ansell",
      "Marinela Parović",
      "Ivan Vulić",
      "Anna Korhonen",
      "Edoardo Ponti"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.243": {
    "title": "A Tale of Pronouns: Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation",
    "volume": "main",
    "abstract": "Recent instruction fine-tuned models can solve multiple NLP tasks when prompted to do so, with machine translation (MT) being a prominent use case. However, current research often focuses on standard performance benchmarks, leaving compelling fairness and ethical considerations behind. In MT, this might lead to misgendered translations, resulting, among other harms, in the perpetuation of stereotypes and prejudices. In this work, we address this gap by investigating whether and to what extent such models exhibit gender bias in machine translation and how we can mitigate it. Concretely, we compute established gender bias metrics on the WinoMT corpus from English to German and Spanish. We discover that IFT models default to male-inflected translations, even disregarding female occupational stereotypes. Next, using interpretability methods, we unveil that models systematically overlook the pronoun indicating the gender of a target occupation in misgendered translations. Finally, based on this finding, we propose an easy-to-implement and effective bias mitigation solution based on few-shot learning that leads to significantly fairer translations",
    "checked": true,
    "id": "85bb4acab1d2a169472f85477eff4ef0a4047582",
    "semantic_title": "a tale of pronouns: interpretability informs gender bias mitigation for fairer instruction-tuned machine translation",
    "citation_count": 0,
    "authors": [
      "Giuseppe Attanasio",
      "Flor Plaza del Arco",
      "Debora Nozza",
      "Anne Lauscher"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.244": {
    "title": "DisCo: Distilled Student Models Co-training for Semi-supervised Text Mining",
    "volume": "main",
    "abstract": "Many text mining models are constructed by fine-tuning a large deep pre-trained language model (PLM) in downstream tasks. However, a significant challenge that arises nowadays is how to maintain performance when we use a lightweight model with limited labeled samples. We present DisCo, a semi-supervised learning (SSL) framework for fine-tuning a cohort of small student models generated from a large PLM using knowledge distillation. Our key insight is to share complementary knowledge among distilled student cohorts to promote their SSL effectiveness. DisCo employs a novel co-training technique to optimize a cohort of multiple small student models by promoting knowledge sharing among students under diversified views: model views produced by different distillation strategies and data views produced by various input augmentations. We evaluate DisCo on both semi-supervised text classification and extractive summarization tasks. Experimental results show that DisCo can produce student models that are 7.6× smaller and 4.8 × faster in inference than the baseline PLMs while maintaining comparable performance. We also show that DisCo-generated student models outperform the similar-sized models elaborately tuned in distinct tasks",
    "checked": true,
    "id": "5229b836401177f5ce11e2398e96dcd7b7358447",
    "semantic_title": "disco: distilled student models co-training for semi-supervised text mining",
    "citation_count": 0,
    "authors": [
      "Weifeng Jiang",
      "Qianren Mao",
      "Chenghua Lin",
      "Jianxin Li",
      "Ting Deng",
      "Weiyi Yang",
      "Zheng Wang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.245": {
    "title": "Dynosaur: A Dynamic Growth Paradigm for Instruction-Tuning Data Curation",
    "volume": "main",
    "abstract": "Instruction tuning has emerged to enhance the capabilities of large language models (LLMs) to comprehend instructions and generate appropriate responses. Existing methods either manually annotate or employ LLM (e.g., GPT-series) to generate data for instruction tuning. However, they often overlook associating instructions with existing annotated datasets. In this paper, we propose Dynosaur, a dynamic growth paradigm for the automatic curation of instruction-tuning data. Based on the metadata of existing datasets, we use LLMs to automatically construct instruction-tuning data by identifying relevant data fields and generating appropriate instructions. By leveraging the existing annotated datasets, Dynosaur offers several advantages: 1) it reduces the API cost for generating instructions (e.g., it costs less than $12 USD by calling GPT-3.5-turbo for generating 800K instruction tuning samples; 2) it provides high-quality data for instruction tuning (e.g., it performs better than Alpaca and Flan on Super-NI and Longform with comparable data sizes); and 3) it supports the continuous improvement of models by generating instruction-tuning data when a new annotated dataset becomes available. We further investigate a continual learning scheme for learning with the ever-growing instruction-tuning dataset, and demonstrate that replaying tasks with diverse instruction embeddings not only helps mitigate forgetting issues but generalizes to unseen tasks better. Code and data are available at https://github.com/WadeYin9712/Dynosaur",
    "checked": true,
    "id": "7742233d33da13910d0303e4ec8814a4e26e96e9",
    "semantic_title": "dynosaur: a dynamic growth paradigm for instruction-tuning data curation",
    "citation_count": 11,
    "authors": [
      "Da Yin",
      "Xiao Liu",
      "Fan Yin",
      "Ming Zhong",
      "Hritik Bansal",
      "Jiawei Han",
      "Kai-Wei Chang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.246": {
    "title": "Are All Steps Equally Important? Benchmarking Essentiality Detection in Event Processes",
    "volume": "main",
    "abstract": "Natural language often describes events in different granularities, such that more coarse-grained (goal) events can often be decomposed into fine-grained sequences of (step) events. A critical but overlooked challenge in understanding an event process lies in the fact that the step events are not equally important to the central goal. In this paper, we seek to fill this gap by studying how well current models can understand the essentiality of different step events towards a goal event. As discussed by cognitive studies, such an ability enables the machine to mimic human's commonsense reasoning about preconditions and necessary efforts of daily-life tasks. Our work contributes with a high-quality corpus of (goal, step) pairs from a community guideline website WikiHow, where the steps are manually annotated with their essentiality w.r.t. the goal. The high IAA indicates that humans have a consistent understanding of the events. Despite evaluating various statistical and massive pre-trained NLU models, we observe that existing SOTA models all perform drastically behind humans, indicating the need for future investigation of this crucial yet challenging task",
    "checked": true,
    "id": "10e1e4d306e0c5f4afe6a5c7b579f5c75ff2e3ac",
    "semantic_title": "are all steps equally important? benchmarking essentiality detection in event processes",
    "citation_count": 0,
    "authors": [
      "Haoyu Wang",
      "Hongming Zhang",
      "Yueguan Wang",
      "Yuqian Deng",
      "Muhao Chen",
      "Dan Roth"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.247": {
    "title": "Language Model is Suitable for Correction of Handwritten Mathematical Expressions Recognition",
    "volume": "main",
    "abstract": "Handwritten mathematical expression recognition (HMER) is a multidisciplinary task that generates LaTeX sequences from images. Existing approaches, employing tree decoders within attention-based encoder-decoder architectures, aim to capture the hierarchical tree structure, but are limited by CFGs and pre-generated triplet data, hindering expandability and neglecting visual ambiguity challenges. This article investigates the distinctive language characteristics of LaTeX mathematical expressions, revealing two key observations: 1) the presence of explicit structural symbols, and 2) the treatment of symbols, particularly letters, as minimal units with context-dependent semantics, representing variables or constants. Rooted in these properties, we propose that language models have the potential to synchronously and complementarily provide both structural and semantic information, making them suitable for correction of HMER. To validate our proposition, we propose an architecture called Recognize and Language Fusion Network (RLFN), which integrates recognition and language features to output corrected sequences while jointly optimizing with a string decoder recognition model. Experiments show that RLFN outperforms existing state-of-the-art methods on the CROHME 2014/2016/2019 datasets",
    "checked": true,
    "id": "3136091148e02300c6486656d4b205d650184906",
    "semantic_title": "language model is suitable for correction of handwritten mathematical expressions recognition",
    "citation_count": 0,
    "authors": [
      "Zui Chen",
      "Jiaqi Han",
      "Chaofan Yang",
      "Yi Zhou"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.248": {
    "title": "Vicinal Risk Minimization for Few-Shot Cross-lingual Transfer in Abusive Language Detection",
    "volume": "main",
    "abstract": "Cross-lingual transfer learning from high-resource to medium and low-resource languages has shown encouraging results. However, the scarcity of resources in target languages remains a challenge. In this work, we resort to data augmentation and continual pre-training for domain adaptation to improve cross-lingual abusive language detection. For data augmentation, we analyze two existing techniques based on vicinal risk minimization and propose MIXAG, a novel data augmentation method which interpolates pairs of instances based on the angle of their representations. Our experiments involve seven languages typologically distinct from English and three different domains. The results reveal that the data augmentation strategies can enhance few-shot cross-lingual abusive language detection. Specifically, we observe that consistently in all target languages, MIXAG improves significantly in multidomain and multilingual environments. Finally, we show through an error analysis how the domain adaptation can favour the class of abusive texts (reducing false negatives), but at the same time, declines the precision of the abusive language detection model",
    "checked": true,
    "id": "459e619dbb5a692c468485a78d22a72374f388ab",
    "semantic_title": "vicinal risk minimization for few-shot cross-lingual transfer in abusive language detection",
    "citation_count": 0,
    "authors": [
      "Gretel De la Peña Sarracén",
      "Paolo Rosso",
      "Robert Litschko",
      "Goran Glavaš",
      "Simone Ponzetto"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.249": {
    "title": "SuperDialseg: A Large-scale Dataset for Supervised Dialogue Segmentation",
    "volume": "main",
    "abstract": "Dialogue segmentation is a crucial task for dialogue systems allowing a better understanding of conversational texts. Despite recent progress in unsupervised dialogue segmentation methods, their performances are limited by the lack of explicit supervised signals for training. Furthermore, the precise definition of segmentation points in conversations still remains as a challenging problem, increasing the difficulty of collecting manual annotations. In this paper, we provide a feasible definition of dialogue segmentation points with the help of document-grounded dialogues and release a large-scale supervised dataset called SuperDialseg, containing 9,478 dialogues based on two prevalent document-grounded dialogue corpora, and also inherit their useful dialogue-related annotations. Moreover, we provide a benchmark including 18 models across five categories for the dialogue segmentation task with several proper evaluation metrics. Empirical studies show that supervised learning is extremely effective in in-domain datasets and models trained on SuperDialseg can achieve good generalization ability on out-of-domain data. Additionally, we also conducted human verification on the test set and the Kappa score confirmed the quality of our automatically constructed dataset. We believe our work is an important step forward in the field of dialogue segmentation",
    "checked": true,
    "id": "1078bb4a94cb566b1d32e86476790f39b3b17b71",
    "semantic_title": "superdialseg: a large-scale dataset for supervised dialogue segmentation",
    "citation_count": 0,
    "authors": [
      "Junfeng Jiang",
      "Chengzhang Dong",
      "Sadao Kurohashi",
      "Akiko Aizawa"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.250": {
    "title": "ATFormer: A Learned Performance Model with Transfer Learning Across Devices for Deep Learning Tensor Programs",
    "volume": "main",
    "abstract": "The training and inference efficiency of ever-larger deep neural networks highly rely on the performance of tensor operators on specific hardware platforms. Therefore, a compilation-based optimization flow with automatic tensor generation and parameter tuning is necessary for efficient model deployment. While compilation-based methods with performance models can provide dynamic and suitable code optimization, they suffer from a large design space exploration with rough measurement accuracy and poor transferability among different hardware platforms. This paper presents ATFormer, a simple yet efficient design with attention-inspired modules to accurately predict the performance of optimized operators by capturing global and long-range dependencies within a complete scheduling space. Compared with state-of-the-arts, ATFormer can predict the optimal implementation of tensor operators to reduce inference time with minimal effort on modern DNN benchmarks. Furthermore, ATFormer with pre-trained parameters can quickly adapt to different workloads and hardware via transfer learning",
    "checked": true,
    "id": "8bc6af0bbf82bff8313e23de2f4d93dfb437b080",
    "semantic_title": "atformer: a learned performance model with transfer learning across devices for deep learning tensor programs",
    "citation_count": 0,
    "authors": [
      "Yang Bai",
      "Wenqian Zhao",
      "Shuo Yin",
      "Zixiao Wang",
      "Bei Yu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.251": {
    "title": "mRedditSum: A Multimodal Abstractive Summarization Dataset of Reddit Threads with Images",
    "volume": "main",
    "abstract": "The growing number of multimodal online discussions necessitates automatic summarization to save time and reduce content overload. However, existing summarization datasets are not suitable for this purpose, as they either do not cover discussions, multiple modalities, or both. To this end, we present mRedditSum, the first multimodal discussion summarization dataset. It consists of 3,033 discussion threads where a post solicits advice regarding an issue described with an image and text, and respective comments express diverse opinions. We annotate each thread with a human-written summary that captures both the essential information from the text, as well as the details available only in the image. Experiments show that popular summarization models—GPT-3.5, BART, and T5—consistently improve in performance when visual information is incorporated. We also introduce a novel method, cluster-based multi-stage summarization, that outperforms existing baselines and serves as a competitive baseline for future work",
    "checked": true,
    "id": "9e52e82773f2f5be5c7239068bf4f6029d71f844",
    "semantic_title": "mredditsum: a multimodal abstractive summarization dataset of reddit threads with images",
    "citation_count": 0,
    "authors": [
      "Keighley Overbay",
      "Jaewoo Ahn",
      "Fatemeh Pesaran zadeh",
      "Joonsuk Park",
      "Gunhee Kim"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.252": {
    "title": "Sparse Low-rank Adaptation of Pre-trained Language Models",
    "volume": "main",
    "abstract": "Fine-tuning pre-trained large language models in a parameter-efficient manner is widely studied for its effectiveness and efficiency. The popular method of low-rank adaptation (LoRA) offers a notable approach, hypothesizing that the adaptation process is intrinsically low-dimensional. Although LoRA has demonstrated commendable performance, it is implemented with a fixed and unalterable intrinsic rank that might not always be the ideal choice. Recognizing the need for more flexible adaptation, we extend the methodology of LoRA to an innovative approach we call sparse low-rank adaptation (SoRA) that enables dynamic adjustments to the intrinsic rank during the adaptation process. We achieve this through the incorporation of a gate unit optimized with proximal gradient method in the training stage, controlling the cardinality of rank under the sparsity of the gate. In the subsequent inference stage, we eliminate the parameter blocks corresponding to the zeroed-out ranks, to reduce each SoRA module back to a concise yet rank-optimal LoRA. Our approach strengthens the representation power of LoRA by initializing it with a higher rank, while efficiently taming a temporarily increased number of parameters via updating in a sparse way. We further introduce a sparsifying scheduler for SoRA, aiming to examine the impact of the number of non-zero parameters on the model's memorization and generalization. Our experimental results demonstrate that SoRA can outperform other baselines even with 70% retained parameters and 70% training time",
    "checked": true,
    "id": "70ded1d6e83a1cbeecec256a070c4b9ebfc6085f",
    "semantic_title": "sparse low-rank adaptation of pre-trained language models",
    "citation_count": 1,
    "authors": [
      "Ning Ding",
      "Xingtai Lv",
      "Qiaosen Wang",
      "Yulin Chen",
      "Bowen Zhou",
      "Zhiyuan Liu",
      "Maosong Sun"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.253": {
    "title": "Human Learning by Model Feedback: The Dynamics of Iterative Prompting with Midjourney",
    "volume": "main",
    "abstract": "Generating images with a Text-to-Image model often requires multiple trials, where human users iteratively update their prompt based on feedback, namely the output image. Taking inspiration from cognitive work on reference games and dialogue alignment, this paper analyzes the dynamics of the user prompts along such iterations. We compile a dataset of iterative interactions of human users with Midjourney. Our analysis then reveals that prompts predictably converge toward specific traits along these iterations. We further study whether this convergence is due to human users, realizing they missed important details, or due to adaptation to the model's \"preferences\", producing better images for a specific language style. We show initial evidence that both possibilities are at play. The possibility that users adapt to the model's preference raises concerns about reusing user data for further training. The prompts may be biased towards the preferences of a specific model, rather than align with human intentions and natural manner of expression",
    "checked": true,
    "id": "6e6ef0defdc1d8844ad843f0166d1a0a6a17932e",
    "semantic_title": "human learning by model feedback: the dynamics of iterative prompting with midjourney",
    "citation_count": 1,
    "authors": [
      "Shachar Don-Yehiya",
      "Leshem Choshen",
      "Omri Abend"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.254": {
    "title": "ULF: Unsupervised Labeling Function Correction using Cross-Validation for Weak Supervision",
    "volume": "main",
    "abstract": "A cost-effective alternative to manual data labeling is weak supervision (WS), where data samples are automatically annotated using a predefined set of labeling functions (LFs), rule-based mechanisms that generate artificial labels for the associated classes. In this work, we investigate noise reduction techniques for WS based on the principle of k-fold cross-validation. We introduce a new algorithm ULF for Unsupervised Labeling Function correction, which denoises WS data by leveraging models trained on all but some LFs to identify and correct biases specific to the held-out LFs. Specifically, ULF refines the allocation of LFs to classes by re-estimating this assignment on highly reliable cross-validated samples. Evaluation on multiple datasets confirms ULF's effectiveness in enhancing WS learning without the need for manual labeling",
    "checked": true,
    "id": "bf1023a59258757aeba8f5b7b717aa9d6a5000f9",
    "semantic_title": "ulf: unsupervised labeling function correction using cross-validation for weak supervision",
    "citation_count": 0,
    "authors": [
      "Anastasiia Sedova",
      "Benjamin Roth"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.255": {
    "title": "The Art of SOCRATIC QUESTIONING: Recursive Thinking with Large Language Models",
    "volume": "main",
    "abstract": "Chain-of-Thought (CoT) prompting enables large language models to solve complex reasoning problems by generating intermediate steps. However, confined by its inherent single-pass and sequential generation process, CoT heavily relies on the initial decisions, causing errors in early steps to accumulate and impact the final answers. In contrast, humans adopt recursive thinking when tackling complex reasoning problems, i.e. iteratively breaking the original problem into approachable sub-problems and aggregating their answers to resolve the original one. Inspired by the human cognitive process, we propose SOCRATIC QUESTIONING, a divide-and-conquer style algorithm that mimics the recursive thinking process. Specifically, SOCRATIC QUESTIONING leverages large language models to raise and answer sub-questions until collecting enough information to tackle the original question. Unlike CoT, SOCRATIC QUESTIONING explicitly navigates the thinking space, stimulates effective recursive thinking, and is more robust towards errors in the thinking process. Extensive experiments on several complex reasoning tasks, including MMLU, MATH, LogiQA, and visual question-answering demonstrate significant performance improvements over the state-of-the-art prompting methods, such as CoT, and Tree-of-Thought. The qualitative analysis clearly shows that the intermediate reasoning steps elicited by SOCRATIC QUESTIONING are similar to humans' recursively thinking process of complex reasoning problems",
    "checked": true,
    "id": "69335077fcacbff7a7cf25697da1949e6bdfa968",
    "semantic_title": "the art of socratic questioning: recursive thinking with large language models",
    "citation_count": 0,
    "authors": [
      "Jingyuan Qi",
      "Zhiyang Xu",
      "Ying Shen",
      "Minqian Liu",
      "Di Jin",
      "Qifan Wang",
      "Lifu Huang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.256": {
    "title": "Ideology Takes Multiple Looks: A High-Quality Dataset for Multifaceted Ideology Detection",
    "volume": "main",
    "abstract": "Ideology detection (ID) is important for gaining insights about peoples' opinions and stances on our world and society, which can find many applications in politics, economics and social sciences. It is not uncommon that a piece of text can contain descriptions of various issues. It is also widely accepted that a person can take different ideological stances in different facets. However, existing datasets for the ID task only label a text as ideologically left- or right-leaning as a whole, regardless whether the text containing one or more different issues. Moreover, most prior work annotates texts from data resources with known ideological bias through distant supervision approaches, which may result in many false labels. With some theoretical help from social sciences, this work first designs an ideological schema containing five domains and twelve facets for a new multifaceted ideology detection (MID) task to provide a more complete and delicate description of ideology. We construct a MITweet dataset for the MID task, which contains 12,594 English Twitter posts, each annotated with a Relevance and an Ideology label for all twelve facets. We also design and test a few of strong baselines for the MID task under in-topic and cross-topic settings, which can serve as benchmarks for further research",
    "checked": true,
    "id": "97d2d479e75e001a5b8bcfba06f787765a8df37c",
    "semantic_title": "ideology takes multiple looks: a high-quality dataset for multifaceted ideology detection",
    "citation_count": 0,
    "authors": [
      "Songtao Liu",
      "Ziling Luo",
      "Minghua Xu",
      "Lixiao Wei",
      "Ziyao Wei",
      "Han Yu",
      "Wei Xiang",
      "Bang Wang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.257": {
    "title": "Transductive Learning for Textual Few-Shot Classification in API-based Embedding Models",
    "volume": "main",
    "abstract": "Proprietary and closed APIs are becoming increasingly common to process natural language, and are impacting the practical applications of natural language processing, including few-shot classification. Few-shot classification involves training a model to perform a new classification task with a handful of labeled data. This paper presents three contributions. First, we introduce a scenario where the embedding of a pre-trained model is served through a gated API with compute-cost and data-privacy constraints. Second, we propose a transductive inference, a learning paradigm that has been overlooked by the NLP community. Transductive inference, unlike traditional inductive learning, leverages the statistics of unlabelled data. We also introduce a new parameter-free transductive regularizer based on the Fisher-Rao loss, which can be used on top of the gated API embeddings. This method fully utilizes unlabelled data, does not share any label with the third-party API provider and could serve as a baseline for future research. Third, we propose an improved experimental setting and compile a benchmark of eight datasets involving multiclass classification in four different languages, with up to 151 classes. We evaluate our methods using eight backbone models, along with an episodic evaluation over 1,000 episodes, which demonstrate the superiority of transductive inference over the standard inductive setting",
    "checked": true,
    "id": "72b4020f18b56bace32c473aaf2db9e0085f8837",
    "semantic_title": "transductive learning for textual few-shot classification in api-based embedding models",
    "citation_count": 0,
    "authors": [
      "Pierre Colombo",
      "Victor Pellegrain",
      "Malik Boudiaf",
      "Myriam Tami",
      "Victor Storchan",
      "Ismail Ayed",
      "Pablo Piantanida"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.258": {
    "title": "MEGA: Multilingual Evaluation of Generative AI",
    "volume": "main",
    "abstract": "Generative AI models have shown impressive performance on many Natural Language Processing tasks such as language understanding, reasoning, and language generation. An important question being asked by the AI community today is about the capabilities and limits of these models, and it is clear that evaluating generative AI is very challenging. Most studies on generative LLMs have been restricted to English and it is unclear how capable these models are at understanding and generating text in other languages. We present the first comprehensive benchmarking of generative LLMs - MEGA, which evaluates models on standard NLP benchmarks, covering 16 NLP datasets across 70 typologically diverse languages. We compare the performance of generative LLMs including Chat-GPT and GPT-4 to State of the Art (SOTA) non-autoregressive models on these tasks to determine how well generative models perform compared to the previous generation of LLMs. We present a thorough analysis of the performance of models across languages and tasks and discuss challenges in improving the performance of generative LLMs on low-resource languages. We create a framework for evaluating generative LLMs in the multilingual setting and provide directions for future progress in the field",
    "checked": true,
    "id": "62ad7ea9467bbcdbfe325b9ee561cab3908e4583",
    "semantic_title": "mega: multilingual evaluation of generative ai",
    "citation_count": 43,
    "authors": [
      "Kabir Ahuja",
      "Harshita Diddee",
      "Rishav Hada",
      "Millicent Ochieng",
      "Krithika Ramesh",
      "Prachi Jain",
      "Akshay Nambi",
      "Tanuja Ganu",
      "Sameer Segal",
      "Mohamed Ahmed",
      "Kalika Bali",
      "Sunayana Sitaram"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.259": {
    "title": "Support or Refute: Analyzing the Stance of Evidence to Detect Out-of-Context Mis- and Disinformation",
    "volume": "main",
    "abstract": "Mis- and disinformation online have become a major societal problem as major sources of online harms of different kinds. One common form of mis- and disinformation is out-of-context (OOC) information, where different pieces of information are falsely associated, e.g., a real image combined with a false textual caption or a misleading textual description. Although some past studies have attempted to defend against OOC mis- and disinformation through external evidence, they tend to disregard the role of different pieces of evidence with different stances. Motivated by the intuition that the stance of evidence represents a bias towards different detection results, we propose a stance extraction network (SEN) that can extract the stances of different pieces of multi-modal evidence in a unified framework. Moreover, we introduce a support-refutation score calculated based on the co-occurrence relations of named entities into the textual SEN. Extensive experiments on a public large-scale dataset demonstrated that our proposed method outperformed the state-of-the-art baselines, with the best model achieving a performance gain of 3.2% in accuracy",
    "checked": true,
    "id": "d876221f70c08ca3af1ff0c934619ced612dec8c",
    "semantic_title": "support or refute: analyzing the stance of evidence to detect out-of-context mis- and disinformation",
    "citation_count": 1,
    "authors": [
      "Xin Yuan",
      "Jie Guo",
      "Weidong Qiu",
      "Zheng Huang",
      "Shujun Li"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.260": {
    "title": "Video-Helpful Multimodal Machine Translation",
    "volume": "main",
    "abstract": "Existing multimodal machine translation (MMT) datasets consist of images and video captions or instructional video subtitles, which rarely contain linguistic ambiguity, making visual information ineffective in generating appropriate translations. Recent work has constructed an ambiguous subtitles dataset to alleviate this problem but is still limited to the problem that videos do not necessarily contribute to disambiguation. We introduce EVA (Extensive training set and Video-helpful evaluation set for Ambiguous subtitles translation), an MMT dataset containing 852k Japanese-English parallel subtitle pairs, 520k Chinese-English parallel subtitle pairs, and corresponding video clips collected from movies and TV episodes. In addition to the extensive training set, EVA contains a video-helpful evaluation set in which subtitles are ambiguous, and videos are guaranteed helpful for disambiguation. Furthermore, we propose SAFA, an MMT model based on the Selective Attention model with two novel methods: Frame attention loss and Ambiguity augmentation, aiming to use videos in EVA for disambiguation fully. Experiments on EVA show that visual information and the proposed methods can boost translation performance, and our model performs significantly better than existing MMT models",
    "checked": true,
    "id": "5f95d39b71d36b0802187c4ccf06944faac2e26a",
    "semantic_title": "video-helpful multimodal machine translation",
    "citation_count": 0,
    "authors": [
      "Yihang Li",
      "Shuichiro Shimizu",
      "Chenhui Chu",
      "Sadao Kurohashi",
      "Wei Li"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.261": {
    "title": "Large Language Models are Temporal and Causal Reasoners for Video Question Answering",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have shown remarkable performances on a wide range of natural language understanding and generation tasks. We observe that the LLMs provide effective priors in exploiting linguistic shortcuts for temporal and causal reasoning in Video Question Answering (VideoQA). However, such priors often cause suboptimal results on VideoQA by leading the model to over-rely on questions, i.e., linguistic bias, while ignoring visual content. This is also known as ‘ungrounded guesses' or ‘hallucinations'. To address this problem while leveraging LLMs' prior on VideoQA, we propose a novel framework, Flipped-VQA, encouraging the model to predict all the combinations of ⟨V, Q, A⟩ triplet by flipping the source pair and the target label to understand their complex relationships, i.e., predict A, Q, and V given a VQ, VA, and QA pairs, respectively. In this paper, we develop LLaMA-VQA by applying Flipped-VQA to LLaMA, and it outperforms both LLMs-based and non-LLMs-based models on five challenging VideoQA benchmarks. Furthermore, our Flipped-VQA is a general framework that is applicable to various LLMs (OPT and GPT-J) and consistently improves their performances. We empirically demonstrate that Flipped-VQA not only enhances the exploitation of linguistic shortcuts but also mitigates the linguistic bias, which causes incorrect answers over-relying on the question. Code is available at https://github.com/mlvlab/Flipped-VQA",
    "checked": true,
    "id": "bb4516ad6eb7adda97d81f09d4bb92b3ad056c42",
    "semantic_title": "large language models are temporal and causal reasoners for video question answering",
    "citation_count": 2,
    "authors": [
      "Dohwan Ko",
      "Ji Lee",
      "Woo-Young Kang",
      "Byungseok Roh",
      "Hyunwoo Kim"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.262": {
    "title": "Uncertainty Guided Global Memory Improves Multi-Hop Question Answering",
    "volume": "main",
    "abstract": "Transformers have become the gold standard for many natural language processing tasks and, in particular, for multi-hop question answering (MHQA). This task includes processing a long document and reasoning over the multiple parts of it. The landscape of MHQA approaches can be classified into two primary categories. The first group focuses on extracting supporting evidence, thereby constraining the QA model's context to predicted facts. Conversely, the second group relies on the attention mechanism of the long input encoding model to facilitate multi-hop reasoning. However, attention-based token representations lack explicit global contextual information to connect reasoning steps. To address these issues, we propose GEMFormer, a two-stage method that first collects relevant information over the entire document to the memory and then combines it with local context to solve the task. Our experimental results show that fine-tuning a pre-trained model with memory-augmented input, including the most certain global elements, improves the model's performance on three MHQA datasets compared to the baseline. We also found that the global explicit memory contains information from supporting facts required for the correct answer",
    "checked": true,
    "id": "e4c886fb04932bf6521d6ae4f3adc01ff2b18961",
    "semantic_title": "uncertainty guided global memory improves multi-hop question answering",
    "citation_count": 0,
    "authors": [
      "Alsu Sagirova",
      "Mikhail Burtsev"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.263": {
    "title": "Prompting Large Language Models with Chain-of-Thought for Few-Shot Knowledge Base Question Generation",
    "volume": "main",
    "abstract": "The task of Question Generation over Knowledge Bases (KBQG) aims to convert a logical form into a natural language question. For the sake of expensive cost of large-scale question annotation, the methods of KBQG under low-resource scenarios urgently need to be developed. However, current methods heavily rely on annotated data for fine-tuning, which is not well-suited for few-shot question generation. The emergence of Large Language Models (LLMs) has shown their impressive generalization ability in few-shot tasks. Inspired by Chain-of-Thought (CoT) prompting, which is an in-context learning strategy for reasoning, we formulate KBQG task as a reasoning problem, where the generation of a complete question is splitted into a series of sub-question generation. Our proposed prompting method KQG-CoT first retrieves supportive logical forms from the unlabeled data pool taking account of the characteristics of the logical form. Then, we write a prompt to explicit the reasoning chain of generating complicated questions based on the selected demonstrations. To further ensure prompt quality, we extend KQG-CoT into KQG-CoT+ via sorting the logical forms by their complexity. We conduct extensive experiments over three public KBQG datasets. The results demonstrate that our prompting method consistently outperforms other prompting baselines on the evaluated datasets. Remarkably, our KQG-CoT+ method could surpass existing few-shot SoTA results of the PathQuestions dataset by 18.25, 10.72, and 10.18 absolute points on BLEU-4, METEOR, and ROUGE-L, respectively",
    "checked": true,
    "id": "eafc97d979e790a84329f0a49d1b627bd5979499",
    "semantic_title": "prompting large language models with chain-of-thought for few-shot knowledge base question generation",
    "citation_count": 1,
    "authors": [
      "Yuanyuan Liang",
      "Jianing Wang",
      "Hanlun Zhu",
      "Lei Wang",
      "Weining Qian",
      "Yunshi Lan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.264": {
    "title": "TrojanSQL: SQL Injection against Natural Language Interface to Database",
    "volume": "main",
    "abstract": "The technology of text-to-SQL has significantly enhanced the efficiency of accessing and manipulating databases. However, limited research has been conducted to study its vulnerabilities emerging from malicious user interaction. By proposing TrojanSQL, a backdoor-based SQL injection framework for text-to-SQL systems, we show how state-of-the-art text-to-SQL parsers can be easily misled to produce harmful SQL statements that can invalidate user queries or compromise sensitive information about the database. The study explores two specific injection attacks, namely boolean-based injection and union-based injection, which use different types of triggers to achieve distinct goals in compromising the parser. Experimental results demonstrate that both medium-sized models based on fine-tuning and LLM-based parsers using prompting techniques are vulnerable to this type of attack, with attack success rates as high as 99% and 89%, respectively. We hope that this study will raise more concerns about the potential security risks of building natural language interfaces to databases",
    "checked": true,
    "id": "09790d05514906e37715d1df794236ebb6d50b6b",
    "semantic_title": "trojansql: sql injection against natural language interface to database",
    "citation_count": 0,
    "authors": [
      "Jinchuan Zhang",
      "Yan Zhou",
      "Binyuan Hui",
      "Yaxin Liu",
      "Ziming Li",
      "Songlin Hu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.265": {
    "title": "Preserving Privacy Through Dememorization: An Unlearning Technique For Mitigating Memorization Risks In Language Models",
    "volume": "main",
    "abstract": "Large Language models (LLMs) are trained on vast amounts of data, including sensitive information that poses a risk to personal privacy if exposed. LLMs have shown the ability to memorize and reproduce portions of their training data when prompted by adversaries. Prior research has focused on addressing this memorization issue and preventing verbatim replication through techniques like knowledge unlearning and data pre-processing. However, these methods have limitations regarding the number of protected samples, limited privacy types, and potentially lower-quality generative models. To tackle this challenge more effectively, we propose \"DeMem,\" a novel unlearning approach that utilizes an efficient reinforcement learning feedback loop via proximal policy optimization. By fine-tuning the language model with a negative similarity score as a reward signal, we incentivize the LLMs to learn a paraphrasing policy to unlearn the pre-training data. Our experiments demonstrate that DeMem surpasses strong baselines and state-of-the-art methods in terms of its ability to generalize and strike a balance between maintaining privacy and LLM performance",
    "checked": true,
    "id": "efe7a13f062c82ede0a4fd419f64dce7f7156239",
    "semantic_title": "preserving privacy through dememorization: an unlearning technique for mitigating memorization risks in language models",
    "citation_count": 0,
    "authors": [
      "Aly Kassem",
      "Omar Mahmoud",
      "Sherif Saad"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.266": {
    "title": "MingOfficial: A Ming Official Career Dataset and a Historical Context-Aware Representation Learning Framework",
    "volume": "main",
    "abstract": "In Chinese studies, understanding the nuanced traits of historical figures, often not explicitly evident in biographical data, has been a key interest. However, identifying these traits can be challenging due to the need for domain expertise, specialist knowledge, and context-specific insights, making the process time-consuming and difficult to scale. Our focus on studying officials from China's Ming Dynasty is no exception. To tackle this challenge, we propose MingOfficial, a large-scale multi-modal dataset consisting of both structured (career records, annotated personnel types) and text (historical texts) data for 9,376 officials. We further couple the dataset with a a graph neural network (GNN) to combine both modalities in order to allow investigation of social structures and provide features to boost down-stream tasks. Experiments show that our proposed MingOfficial could enable exploratory analysis of official identities, and also significantly boost performance in tasks such as identifying nuance identities (e.g. civil officials holding military power) from 24.6% to 98.2% F1 score in hold-out test set. By making MingOfficial publicly available (see main text for the URL) as both a dataset and an interactive tool, we aim to stimulate further research into the role of social context and representation learning in identifying individual characteristics, and hope to provide inspiration for computational approaches in other fields beyond Chinese studies",
    "checked": true,
    "id": "2a6aa9649d5529c08a22463c8a9256373f008bfb",
    "semantic_title": "mingofficial: a ming official career dataset and a historical context-aware representation learning framework",
    "citation_count": 0,
    "authors": [
      "You-Jun Chen",
      "Hsin-Yi Hsieh",
      "Yu Lin",
      "Yingtao Tian",
      "Bert Chan",
      "Yu-Sin Liu",
      "Yi-Hsuan Lin",
      "Richard Tsai"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.267": {
    "title": "DPP-TTS: Diversifying prosodic features of speech via determinantal point processes",
    "volume": "main",
    "abstract": "With the rapid advancement in deep generative models, recent neural Text-To-Speech(TTS) models have succeeded in synthesizing human-like speech. There have been some efforts to generate speech with various prosody beyond monotonous prosody patterns. However, previous works have several limitations. First, typical TTS models depend on the scaled sampling temperature for boosting the diversity of prosody. Speech samples generated at high sampling temperatures often lack perceptual prosodic diversity, which can adversely affect the naturalness of the speech. Second, the diversity among samples is neglected since the sampling procedure often focuses on a single speech sample rather than multiple ones. In this paper, we propose DPP-TTS: a text-to-speech model based on Determinantal Point Processes (DPPs) with a prosody diversifying module. Our TTS model is capable of generating speech samples that simultaneously consider perceptual diversity in each sample and among multiple samples. We demonstrate that DPP-TTS generates speech samples with more diversified prosody than baselines in the side-by-side comparison test considering the naturalness of speech at the same time",
    "checked": true,
    "id": "8ed225549a6f54275a6c72a3ac900a787edb1144",
    "semantic_title": "dpp-tts: diversifying prosodic features of speech via determinantal point processes",
    "citation_count": 0,
    "authors": [
      "Seongho Joo",
      "Hyukhun Koh",
      "Kyomin Jung"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.268": {
    "title": "Meta-Learning Online Adaptation of Language Models",
    "volume": "main",
    "abstract": "Large language models encode impressively broad world knowledge in their parameters. However, the knowledge in static language models falls out of date, limiting the model's effective \"shelf life.\" While online fine-tuning can reduce this degradation, we find that naively fine-tuning on a stream of documents leads to a low level of information uptake. We hypothesize that online fine-tuning does not sufficiently attend to important information. That is, the gradient signal from important tokens representing factual information is drowned out by the gradient from inherently noisy tokens, suggesting that a dynamic, context-aware learning rate may be beneficial. We therefore propose learning which tokens to upweight. We meta-train a small, autoregressive model to reweight the language modeling loss for each token during online fine-tuning, with the objective of maximizing the out-of-date base question-answering model's ability to answer questions about a document after a single weighted gradient step. We call this approach Context-aware Meta-learned Loss Scaling (CaMeLS). Across three different distributions of documents, our experiments find that CaMeLS provides substantially improved information uptake on streams of thousands of documents compared with standard fine-tuning and baseline heuristics for reweighting token losses",
    "checked": true,
    "id": "c53e3020e4b8f9e1cd7b6ed35221480a2647ea26",
    "semantic_title": "meta-learning online adaptation of language models",
    "citation_count": 3,
    "authors": [
      "Nathan Hu",
      "Eric Mitchell",
      "Christopher Manning",
      "Chelsea Finn"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.269": {
    "title": "Self-Detoxifying Language Models via Toxification Reversal",
    "volume": "main",
    "abstract": "Language model detoxification aims to minimize the risk of generating offensive or harmful content in pretrained language models (PLMs) for safer deployment. Existing methods can be roughly categorized as finetuning-based and decoding-based. However, the former is often resource-intensive, while the latter relies on additional components and potentially compromises the generation fluency. In this paper, we propose a more lightweight approach that enables the PLM itself to achieve \"self-detoxification\". Our method is built upon the observation that prepending a negative steering prompt can effectively induce PLMs to generate toxic content. At the same time, we are inspired by the recent research in the interpretability field, which formulates the evolving contextualized representations within the PLM as an information stream facilitated by the attention layers. Drawing on this idea, we devise a method to identify the toxification direction from the normal generation process to the one prompted with the negative prefix, and then steer the generation to the reversed direction by manipulating the information movement within the attention layers. Experimental results show that our approach, without any fine-tuning or extra components, can achieve comparable performance with state-of-the-art methods",
    "checked": true,
    "id": "a1fd0cbdeae7fe6f44698139e240d9498fb74cbb",
    "semantic_title": "self-detoxifying language models via toxification reversal",
    "citation_count": 1,
    "authors": [
      "Chak Leong",
      "Yi Cheng",
      "Jiashuo Wang",
      "Jian Wang",
      "Wenjie Li"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.270": {
    "title": "Interactive Text Generation",
    "volume": "main",
    "abstract": "Users interact with text, image, code, or other editors on a daily basis. However, machine learning models are rarely trained in the settings that reflect the interactivity between users and their editor. This is understandable as training AI models with real users is not only slow and costly, but what these models learn may be specific to user interface design choices. Unfortunately, this means most of the research on text, code, and image generation has focused on non-interactive settings, whereby the model is expected to get everything right without accounting for any input from a user who may be willing to help. We introduce a new Interactive Text Generation task that allows training generation models interactively without the costs of involving real users, by using user simulators that provide edits that guide the model towards a given target text. We train our interactive models using Imitation Learning, and our experiments against competitive non-interactive generation models show that models trained interactively are superior to their non-interactive counterparts, even when all models are given the same budget of user inputs or edits",
    "checked": true,
    "id": "fc5df977f59773ff9b7742df1971955248982a47",
    "semantic_title": "interactive text generation",
    "citation_count": 0,
    "authors": [
      "Felix Faltings",
      "Michel Galley",
      "Kianté Brantley",
      "Baolin Peng",
      "Weixin Cai",
      "Yizhe Zhang",
      "Jianfeng Gao",
      "Bill Dolan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.271": {
    "title": "Knowledge Distillation ≈ Label Smoothing: Fact or Fallacy?",
    "volume": "main",
    "abstract": "Originally proposed as a method for knowledge transfer from one model to another, some recent studies have suggested that knowledge distillation (KD) is in fact a form of regularization. Perhaps the strongest argument of all for this new perspective comes from its apparent similarities with label smoothing (LS). Here we re-examine this stated equivalence between the two methods by comparing the predictive confidences of the models they train. Experiments on four text classification tasks involving models of different sizes show that: (a) In most settings, KD and LS drive model confidence in completely opposite directions, and (b) In KD, the student inherits not only its knowledge but also its confidence from the teacher, reinforcing the classical knowledge transfer view",
    "checked": true,
    "id": "99dbbb8266c3fb3e2c5e8b8153d652bf6f4332e3",
    "semantic_title": "knowledge distillation ≈ label smoothing: fact or fallacy?",
    "citation_count": 1,
    "authors": [
      "Md Sultan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.272": {
    "title": "Analyzing Cognitive Plausibility of Subword Tokenization",
    "volume": "main",
    "abstract": "Subword tokenization has become the de-facto standard for tokenization although comparative evaluations of their quality across languages are scarce. Existing evaluation studies focus on the effect of a tokenization algorithm on the performance in downstream tasks, or on engineering criteria such as the compression rate. We present a new evaluation paradigm that focuses on the cognitive plausibility of subword tokenization. We analyze the correlation of the tokenizer output with the reading time and accuracy of human responses on a lexical decision task. We compare three tokenization algorithms across several languages and vocabulary sizes. Our results indicate that the Unigram algorithm yields less cognitively plausible tokenization behavior and a worse coverage of derivational morphemes, in contrast with prior work",
    "checked": true,
    "id": "17d0cc24fd6a267e7fd59ae62054de3e5c552096",
    "semantic_title": "analyzing cognitive plausibility of subword tokenization",
    "citation_count": 0,
    "authors": [
      "Lisa Beinborn",
      "Yuval Pinter"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.273": {
    "title": "POE: Process of Elimination for Multiple Choice Reasoning",
    "volume": "main",
    "abstract": "Language models (LMs) are capable of conducting in-context learning for multiple choice reasoning tasks, but the options in these tasks are treated equally. As humans often first eliminate wrong options before picking the final correct answer, we argue a similar two-step strategy can make LMs better at these tasks. To this end, we present the Process of Elimination (POE), a two-step scoring method. In the first step, POE scores each option, and eliminates seemingly wrong options. In the second step, POE masks these wrong options, and makes the final prediction from the remaining options. Zero-shot experiments on 8 reasoning tasks illustrate the effectiveness of POE, and a following analysis finds our method to be especially performant on logical reasoning tasks. We further analyze the effect of masks, and show that POE applies to few-shot settings and large language models (LLMs) like ChatGPT",
    "checked": true,
    "id": "b0a0f36abc5a63ae886b11cb3eff8d40e6f746f0",
    "semantic_title": "poe: process of elimination for multiple choice reasoning",
    "citation_count": 0,
    "authors": [
      "Chenkai Ma",
      "Xinya Du"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.274": {
    "title": "NeuSTIP: A Neuro-Symbolic Model for Link and Time Prediction in Temporal Knowledge Graphs",
    "volume": "main",
    "abstract": "Neuro-symbolic (NS) models for knowledge graph completion (KGC) combine the benefits of symbolic models (interpretable inference) with those of distributed representations (parameter sharing, high accuracy). While several NS models exist for KGs with static facts, there is limited work on temporal KGC (TKGC) for KGs where a fact is associated with a time interval. In response, we propose a novel NS model for TKGC called NeuSTIP, which performs link prediction and time interval prediction in a TKG. NeuSTIP learns temporal rules with Allen predicates, which ensure temporal consistency between neighboring predicates in the rule body. We further design a unique scoring function that evaluates the confidence of the candidate answers while performing link and time interval predictions by utilizing the learned rules. Our empirical evaluation on two time interval based TKGC datasets shows that our model shows competitive performance on link prediction and establishes a new state of the art on time prediction",
    "checked": true,
    "id": "7d12abbffd38b4b698d214030e2d85bb5f1f5106",
    "semantic_title": "neustip: a neuro-symbolic model for link and time prediction in temporal knowledge graphs",
    "citation_count": 0,
    "authors": [
      "Ishaan Singh",
      "Navdeep Kaur",
      "Garima Gaur",
      "Mausam"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.275": {
    "title": "Standardizing Distress Analysis: Emotion-Driven Distress Identification and Cause Extraction (DICE) in Multimodal Online Posts",
    "volume": "main",
    "abstract": "Due to its growing impact on public opinion, hate speech on social media has garnered increased attention. While automated methods for identifying hate speech have been presented in the past, they have mostly been limited to analyzing textual content. The interpretability of such models has received very little attention, despite the social and legal consequences of erroneous predictions. In this work, we present a novel problem of Distress Identification and Cause Extraction (DICE) from multimodal online posts. We develop a multi-task deep framework for the simultaneous detection of distress content and identify connected causal phrases from the text using emotional information. The emotional information is incorporated into the training process using a zero-shot strategy, and a novel mechanism is devised to fuse the features from the multimodal inputs. Furthermore, we introduce the first-of-its-kind Distress and Cause annotated Multimodal (DCaM) dataset of 20,764 social media posts. We thoroughly evaluate our proposed method by comparing it to several existing benchmarks. Empirical assessment and comprehensive qualitative analysis demonstrate that our proposed method works well on distress detection and cause extraction tasks, improving F1 and ROS scores by 1.95% and 3%, respectively, relative to the best-performing baseline. The code and the dataset can be accessed from the following link: https://www.iitp.ac.in/~ai-nlp-ml/resources.html\\#DICE",
    "checked": true,
    "id": "14e29c60e685d3fed9daf002d90cde3eac13c043",
    "semantic_title": "standardizing distress analysis: emotion-driven distress identification and cause extraction (dice) in multimodal online posts",
    "citation_count": 0,
    "authors": [
      "Gopendra Singh",
      "Soumitra Ghosh",
      "Atul Verma",
      "Chetna Painkra",
      "Asif Ekbal"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.276": {
    "title": "Out-of-Distribution Generalization in Natural Language Processing: Past, Present, and Future",
    "volume": "main",
    "abstract": "Machine learning (ML) systems in natural language processing (NLP) face significant challenges in generalizing to out-of-distribution (OOD) data, where the test distribution differs from the training data distribution. This poses important questions about the robustness of NLP models and their high accuracy, which may be artificially inflated due to their underlying sensitivity to systematic biases. Despite these challenges, there is a lack of comprehensive surveys on the generalization challenge from an OOD perspective in natural language understanding. Therefore, this paper aims to fill this gap by presenting the first comprehensive review of recent progress, methods, and evaluations on this topic. We further discuss the challenges involved and potential future research directions. By providing convenient access to existing work, we hope this survey will encourage future research in this area",
    "checked": true,
    "id": "e8e3a5f23f336c64100960c45a28a730b72beec1",
    "semantic_title": "out-of-distribution generalization in natural language processing: past, present, and future",
    "citation_count": 0,
    "authors": [
      "Linyi Yang",
      "Yaoxian Song",
      "Xuan Ren",
      "Chenyang Lyu",
      "Yidong Wang",
      "Jingming Zhuo",
      "Lingqiao Liu",
      "Jindong Wang",
      "Jennifer Foster",
      "Yue Zhang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.277": {
    "title": "Noisy Exemplars Make Large Language Models More Robust: A Domain-Agnostic Behavioral Analysis",
    "volume": "main",
    "abstract": "Recent advances in prompt engineering enable large language models (LLMs) to solve multi-hop logical reasoning problems with impressive accuracy. However, there is little existing work investigating the robustness of LLMs with few-shot prompting techniques. Therefore, we introduce a systematic approach to test the robustness of LLMs in multi-hop reasoning tasks via domain-agnostic perturbations. We include perturbations at multiple levels of abstractions (e.g. lexical perturbations such as typos, and semantic perturbations such as the inclusion of intermediate reasoning steps in the questions) to conduct behavioral analysis on the LLMs. Throughout our experiments, we find that models are more sensitive to certain perturbations such as replacing words with their synonyms. We also demonstrate that increasing the proportion of perturbed exemplars in the prompts improves the robustness of few-shot prompting methods",
    "checked": true,
    "id": "675e079cc3c11f9234f8f70bab9f763911b97955",
    "semantic_title": "noisy exemplars make large language models more robust: a domain-agnostic behavioral analysis",
    "citation_count": 0,
    "authors": [
      "Hongyi Zheng",
      "Abulhair Saparov"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.278": {
    "title": "Can Large Language Models Capture Dissenting Human Voices?",
    "volume": "main",
    "abstract": "Large language models (LLMs) have shown impressive achievements in solving a broad range of tasks. Augmented by instruction fine-tuning, LLMs have also been shown to generalize in zero-shot settings as well. However, whether LLMs closely align with the human disagreement distribution has not been well-studied, especially within the scope of natural language inference (NLI). In this paper, we evaluate the performance and alignment of LLM distribution with humans using two different techniques to estimate the multinomial distribution: Monte Carlo Estimation (MCE) and Log Probability Estimation (LPE). As a result, we show LLMs exhibit limited ability in solving NLI tasks and simultaneously fail to capture human disagreement distribution. The inference and human alignment performances plunge even further on data samples with high human disagreement levels, raising concerns about their natural language understanding (NLU) ability and their representativeness to a larger human population",
    "checked": true,
    "id": "76750c59ec126cc4bfdfef30648598bd5b94220b",
    "semantic_title": "can large language models capture dissenting human voices?",
    "citation_count": 2,
    "authors": [
      "Noah Lee",
      "Na An",
      "James Thorne"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.279": {
    "title": "DecoMT: Decomposed Prompting for Machine Translation Between Related Languages using Large Language Models",
    "volume": "main",
    "abstract": "This study investigates machine translation between related languages i.e., languages within the same family that share linguistic characteristics such as word order and lexical similarity. Machine translation through few-shot prompting leverages a small set of translation pair examples to generate translations for test sentences. This procedure requires the model to learn how to generate translations while simultaneously ensuring that token ordering is maintained to produce a fluent and accurate translation. We propose that for related languages, the task of machine translation can be simplified by leveraging the monotonic alignment characteristic of such languages. We introduce DecoMT, a novel approach of few-shot prompting that decomposes the translation process into a sequence of word chunk translations. Through automatic and human evaluation conducted on multiple related language pairs across various language families, we demonstrate that our proposed approach of decomposed prompting surpasses multiple established few-shot baseline approaches. For example, DecoMT outperforms the strong few-shot prompting BLOOM model with an average improvement of 8 chrF++ scores across the examined languages",
    "checked": false,
    "id": "b6e5855b6a4e425ba251a93516f2bccffe5ba403",
    "semantic_title": "decomposed prompting for machine translation between related languages using large language models",
    "citation_count": 0,
    "authors": [
      "Ratish Puduppully",
      "Anoop Kunchukuttan",
      "Raj Dabre",
      "Ai Ti Aw",
      "Nancy Chen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.280": {
    "title": "Prototype-based HyperAdapter for Sample-Efficient Multi-task Tuning",
    "volume": "main",
    "abstract": "Parameter-efficient fine-tuning (PEFT) has shown its effectiveness in adapting the pre-trained language models to downstream tasks while only updating a small number of parameters. Despite the success, most existing methods independently adapt to each task without considering knowledge transfer between tasks and are limited to low-data regimes. To overcome this issue, we propose Prototype-based HyperAdapter (PHA), a novel framework built on the adapter-tuning and hypernetwork. It introduces an instance-dense retriever and a prototypical hypernetwork to generate the conditional modules in a sample-efficient manner. This leads to comparable performance improvements against existing PEFT methods on multi-task learning and few-shot transfer learning. More importantly, when the available data size gets smaller, our method outperforms other strong baselines by a large margin. Based on our extensive empirical experiments across various datasets, we demonstrate that PHA strikes a better trade-off between trainable parameters, accuracy on stream tasks, and sample efficiency. Our code is publicly available at https://github.com/Bumble666/PHA",
    "checked": true,
    "id": "faf1b006cd89c1bb02ddc152cc8c4f54c8d1f0c6",
    "semantic_title": "prototype-based hyperadapter for sample-efficient multi-task tuning",
    "citation_count": 0,
    "authors": [
      "Hao Zhao",
      "Jie Fu",
      "Zhaofeng He"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.281": {
    "title": "Towards Building More Robust NER datasets: An Empirical Study on NER Dataset Bias from a Dataset Difficulty View",
    "volume": "main",
    "abstract": "Recently, many studies have illustrated the robustness problem of Named Entity Recognition (NER) systems: the NER models often rely on superficial entity patterns for predictions, without considering evidence from the context. Consequently, even state-of-the-art NER models generalize poorly to out-of-domain scenarios when out-of-distribution (OOD) entity patterns are introduced. Previous research attributes the robustness problem to the existence of NER dataset bias, where simpler and regular entity patterns induce shortcut learning. In this work, we bring new insights into this problem by comprehensively investigating the NER dataset bias from a dataset difficulty view. We quantify the entity-context difficulty distribution in existing datasets and explain their relationship with model robustness. Based on our findings, we explore three potential ways to de-bias the NER datasets by altering entity-context distribution, and we validate the feasibility with intensive experiments. Finally, we show that the de-biased datasets can transfer to different models and even benefit existing model-based robustness-improving methods, indicating that building more robust datasets is fundamental for building more robust NER systems",
    "checked": true,
    "id": "78c2a290bdada095be43cdd62d46fedaed839de9",
    "semantic_title": "towards building more robust ner datasets: an empirical study on ner dataset bias from a dataset difficulty view",
    "citation_count": 0,
    "authors": [
      "Ruotian Ma",
      "Xiaolei Wang",
      "Xin Zhou",
      "Qi Zhang",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.282": {
    "title": "GradSim: Gradient-Based Language Grouping for Effective Multilingual Training",
    "volume": "main",
    "abstract": "Most languages of the world pose low-resource challenges to natural language processing models. With multilingual training, knowledge can be shared among languages. However, not all languages positively influence each other and it is an open research question how to select the most suitable set of languages for multilingual training and avoid negative interference among languages whose characteristics or data distributions are not compatible. In this paper, we propose GradSim, a language grouping method based on gradient similarity. Our experiments on three diverse multilingual benchmark datasets show that it leads to the largest performance gains compared to other similarity measures and it is better correlated with cross-lingual model performance. As a result, we set the new state of the art on AfriSenti, a benchmark dataset for sentiment analysis on low-resource African languages. In our extensive analysis, we further reveal that besides linguistic features, the topics of the datasets play an important role for language grouping and that lower layers of transformer models encode language-specific features while higher layers capture task-specific information",
    "checked": true,
    "id": "0e44b39b0763608e3a899e8e7cfd5999a7812ec2",
    "semantic_title": "gradsim: gradient-based language grouping for effective multilingual training",
    "citation_count": 0,
    "authors": [
      "Mingyang Wang",
      "Heike Adel",
      "Lukas Lange",
      "Jannik Strötgen",
      "Hinrich Schuetze"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.283": {
    "title": "Discovering Universal Geometry in Embeddings with ICA",
    "volume": "main",
    "abstract": "This study utilizes Independent Component Analysis (ICA) to unveil a consistent semantic structure within embeddings of words or images. Our approach extracts independent semantic components from the embeddings of a pre-trained model by leveraging anisotropic information that remains after the whitening process in Principal Component Analysis (PCA). We demonstrate that each embedding can be expressed as a composition of a few intrinsic interpretable axes and that these semantic axes remain consistent across different languages, algorithms, and modalities. The discovery of a universal semantic structure in the geometric patterns of embeddings enhances our understanding of the representations in embeddings",
    "checked": true,
    "id": "6150611f428a74195001919e7e52bc05d81067f3",
    "semantic_title": "discovering universal geometry in embeddings with ica",
    "citation_count": 0,
    "authors": [
      "Hiroaki Yamagiwa",
      "Momose Oyama",
      "Hidetoshi Shimodaira"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.284": {
    "title": "Toward a Critical Toponymy Framework for Named Entity Recognition: A Case Study of Airbnb in New York City",
    "volume": "main",
    "abstract": "Critical toponymy examines the dynamics of power, capital, and resistance through place names and the sites to which they refer. Studies here have traditionally focused on the semantic content of toponyms and the top-down institutional processes that produce them. However, they have generally ignored the ways in which toponyms are used by ordinary people in everyday discourse, as well as the other strategies of geospatial description that accompany and contextualize toponymic reference. Here, we develop computational methods to measure how cultural and economic capital shape the ways in which people refer to places, through a novel annotated dataset of 47,440 New York City Airbnb listings from the 2010s. Building on this dataset, we introduce a new named entity recognition (NER) model able to identify important discourse categories integral to the characterization of place. Our findings point toward new directions for critical toponymy and to a range of previously understudied linguistic signals relevant to research on neighborhood status, housing and tourism markets, and gentrification",
    "checked": true,
    "id": "8868047e61630e14ac281b8fc8f40ad77e222ef8",
    "semantic_title": "toward a critical toponymy framework for named entity recognition: a case study of airbnb in new york city",
    "citation_count": 0,
    "authors": [
      "Mikael Brunila",
      "Jack LaViolette",
      "Sky CH-Wang",
      "Priyanka Verma",
      "Clara Féré",
      "Grant McKenzie"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.285": {
    "title": "Well Begun is Half Done: Generator-agnostic Knowledge Pre-Selection for Knowledge-Grounded Dialogue",
    "volume": "main",
    "abstract": "Accurate knowledge selection is critical in knowledge-grounded dialogue systems. Towards a closer look at it, we offer a novel perspective to organize existing literature, i.e., knowledge selection coupled with, after, and before generation. We focus on the third under-explored category of study, which can not only select knowledge accurately in advance, but has the advantage to reduce the learning, adjustment, and interpretation burden of subsequent response generation models, especially LLMs. We propose \\tt{GATE}, a generator-agnostic knowledge selection method, to prepare knowledge for subsequent response generation models by selecting context-related knowledge among different knowledge structures and variable knowledge requirements. Experimental results demonstrate the superiority of \\tt{GATE}, and indicate that knowledge selection before generation is a lightweight yet effective way to facilitate LLMs (e.g., ChatGPT) to generate more informative responses",
    "checked": true,
    "id": "35ffe006979cde43fdfba519b73d9dc1642c4469",
    "semantic_title": "well begun is half done: generator-agnostic knowledge pre-selection for knowledge-grounded dialogue",
    "citation_count": 0,
    "authors": [
      "Lang Qin",
      "Yao Zhang",
      "Hongru Liang",
      "Jun Wang",
      "Zhenglu Yang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.286": {
    "title": "Merging Generated and Retrieved Knowledge for Open-Domain QA",
    "volume": "main",
    "abstract": "Open-domain question answering (QA) systems are often built with retrieval modules. However, retrieving passages from a given source is known to suffer from insufficient knowledge coverage. Alternatively, prompting large language models (LLMs) to generate contextual passages based on their parametric knowledge has been shown to improve QA performance. Yet, LLMs tend to \"hallucinate\" content that conflicts with the retrieved knowledge. Based on the intuition that answers supported by both sources are more likely to be correct, we propose COMBO, a Compatibility-Oriented knowledge Merging for Better Open-domain QA framework, to effectively leverage the two sources of information. Concretely, we match LLM-generated passages with retrieved counterparts into compatible pairs, based on discriminators trained with silver compatibility labels. Then a Fusion-in-Decoder-based reader model handles passage pairs to arrive at the final answer. Experiments show that COMBO outperforms competitive baselines on three out of four tested open-domain QA benchmarks. Further analysis reveals that our proposed framework demonstrates greater efficacy in scenarios with a higher degree of knowledge conflicts",
    "checked": true,
    "id": "a71236be6eaa57af7207288bb1153f067144e02a",
    "semantic_title": "merging generated and retrieved knowledge for open-domain qa",
    "citation_count": 0,
    "authors": [
      "Yunxiang Zhang",
      "Muhammad Khalifa",
      "Lajanugen Logeswaran",
      "Moontae Lee",
      "Honglak Lee",
      "Lu Wang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.287": {
    "title": "Best of Both Worlds: Towards Improving Temporal Knowledge Base Question Answering via Targeted Fact Extraction",
    "volume": "main",
    "abstract": "Temporal question answering (QA) is a special category of complex question answering task that requires reasoning over facts asserting time intervals of events. Previous works have predominately relied on Knowledge Base Question Answering (KBQA) for temporal QA. One of the major challenges faced by these systems is their inability to retrieve all relevant facts due to factors such as incomplete KB and entity/relation linking errors. A failure to fetch even a single fact will block KBQA from computing the answer. Such cases of KB incompleteness are even more profound in the temporal context. To address this issue, we explore an interesting direction where a targeted temporal fact extraction technique is used to assist KBQA whenever it fails to retrieve temporal facts from the KB. We model the extraction problem as an open-domain question answering task using off-the-shelf language models. This way, we target to extract from textual resources those facts that failed to get retrieved from the KB. Experimental results on two temporal QA benchmarks show promising ~30% & ~10% relative improvements in answer accuracies without any additional training cost",
    "checked": true,
    "id": "20e9504663b05624ca13864c2fab0e8bcce2d358",
    "semantic_title": "best of both worlds: towards improving temporal knowledge base question answering via targeted fact extraction",
    "citation_count": 0,
    "authors": [
      "Nithish Kannen",
      "Udit Sharma",
      "Sumit Neelam",
      "Dinesh Khandelwal",
      "Shajith Ikbal",
      "Hima Karanam",
      "L Subramaniam"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.288": {
    "title": "Text Fact Transfer",
    "volume": "main",
    "abstract": "Text style transfer is a prominent task that aims to control the style of text without inherently changing its factual content. To cover more text modification applications, such as adapting past news for current events and repurposing educational materials, we propose the task of text fact transfer, which seeks to transfer the factual content of a source text between topics without modifying its style. We find that existing language models struggle with text fact transfer, due to their inability to preserve the specificity and phrasing of the source text, and tendency to hallucinate errors. To address these issues, we design ModQGA, a framework that minimally modifies a source text with a novel combination of end-to-end question generation and specificity-aware question answering. Through experiments on four existing datasets adapted for text fact transfer, we show that ModQGA can accurately transfer factual content without sacrificing the style of the source text",
    "checked": true,
    "id": "74c58985723a439df4736a1c7f885a68aec73c62",
    "semantic_title": "text fact transfer",
    "citation_count": 0,
    "authors": [
      "Nishant Balepur",
      "Jie Huang",
      "Kevin Chang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.289": {
    "title": "A Cheaper and Better Diffusion Language Model with Soft-Masked Noise",
    "volume": "main",
    "abstract": "Diffusion models that are based on iterative denoising have been recently proposed and leveraged in various generation tasks like image generation. Whereas, as a way inherently built for continuous data, existing diffusion models still have some limitations in modeling discrete data, e.g., languages. For example, the generally used Gaussian noise can not handle the discrete corruption well, and the objectives in continuous spaces fail to be stable for textual data in the diffusion process especially when the dimension is high. To alleviate these issues, we introduce a novel diffusion model for language modeling, Masked-Diffuse LM, with lower training cost and better performances, inspired by linguistic features in languages. Specifically, we design a linguistic-informed forward process which adds corruptions to the text through strategically soft-masking to better noise the textual data. Also, we directly predict the categorical distribution with cross-entropy loss function in every diffusion step to connect the continuous space and discrete space in a more efficient and straightforward way. Through experiments on 5 controlled generation tasks, we demonstrate that our Masked-Diffuse LM can achieve better generation quality than the state-of-the-art diffusion models with better efficiency",
    "checked": true,
    "id": "e3950d18cc81edbeafd83129a646a62850cad62a",
    "semantic_title": "a cheaper and better diffusion language model with soft-masked noise",
    "citation_count": 7,
    "authors": [
      "Jiaao Chen",
      "Aston Zhang",
      "Mu Li",
      "Alex Smola",
      "Diyi Yang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.290": {
    "title": "Mirages. On Anthropomorphism in Dialogue Systems",
    "volume": "main",
    "abstract": "Automated dialogue or conversational systems are anthropomorphised by developers and personified by users. While a degree of anthropomorphism is inevitable, conscious and unconscious design choices can guide users to personify them to varying degrees. Encouraging users to relate to automated systems as if they were human can lead to transparency and trust issues, and high risk scenarios caused by over-reliance on their outputs. As a result, natural language processing researchers have investigated the factors that induce personification and develop resources to mitigate such effects. However, these efforts are fragmented, and many aspects of anthropomorphism have yet to be explored. In this paper, we discuss the linguistic factors that contribute to the anthropomorphism of dialogue systems and the harms that can arise thereof, including reinforcing gender stereotypes and conceptions of acceptable language. We recommend that future efforts towards developing dialogue systems take particular care in their design, development, release, and description; and attend to the many linguistic cues that can elicit personification by users",
    "checked": false,
    "id": "9379d519b8ddfa194ef6f575127451e5016e1803",
    "semantic_title": "mirages: on anthropomorphism in dialogue systems",
    "citation_count": 7,
    "authors": [
      "Gavin Abercrombie",
      "Amanda Curry",
      "Tanvi Dinkar",
      "Verena Rieser",
      "Zeerak Talat"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.291": {
    "title": "Cognitive Dissonance: Why Do Language Model Outputs Disagree with Internal Representations of Truthfulness?",
    "volume": "main",
    "abstract": "Neural language models (LMs) can be used to evaluate the truth of factual statements in two ways: they can be either queried for statement probabilities, or probed for internal representations of truthfulness. Past work has found that these two procedures sometimes disagree, and that probes tend to be more accurate than LM outputs. This has led some researchers to conclude that LMs \"lie' or otherwise encode non-cooperative communicative intents. Is this an accurate description of today's LMs, or can query–probe disagreement arise in other ways? We identify three different classes of disagreement, which we term confabulation, deception, and heterogeneity. In many cases, the superiority of probes is simply attributable to better calibration on uncertain answers rather than a greater fraction of correct, high-confidence answers. In some cases, queries and probes perform better on different subsets of inputs, and accuracy can further be improved by ensembling the two",
    "checked": true,
    "id": "0c1ef418e4104f487cc3cfa9b71229c39070c4e2",
    "semantic_title": "cognitive dissonance: why do language model outputs disagree with internal representations of truthfulness?",
    "citation_count": 0,
    "authors": [
      "Kevin Liu",
      "Stephen Casper",
      "Dylan Hadfield-Menell",
      "Jacob Andreas"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.292": {
    "title": "KEBAP: Korean Error Explainable Benchmark Dataset for ASR and Post-processing",
    "volume": "main",
    "abstract": "Automatic Speech Recognition (ASR) systems are instrumental across various applications, with their performance being critically tied to user satisfaction. Conventional evaluation metrics for ASR systems produce a singular aggregate score, which is insufficient for understanding specific system vulnerabilities. Therefore, we aim to address the limitations of the previous ASR evaluation methods by introducing the Korean Error Explainable Benchmark Dataset for ASR and Post-processing (KEBAP). KEBAP enables comprehensive analysis of ASR systems at both speech- and text levels, thereby facilitating a more balanced assessment encompassing speech recognition accuracy and user readability. KEBAP provides 37 newly defined speech-level resources incorporating diverse noise environments and speaker characteristics categories, also presenting 13 distinct text-level error types. This paper demonstrates detailed statistical analyses of colloquial noise categories and textual error types. Furthermore, we conduct extensive validation and analysis on commercially deployed ASR systems, providing valuable insights into their performance. As a more fine-grained and real-world-centric evaluation method, KEBAP contributes to identifying and mitigating potential weaknesses in ASR systems",
    "checked": true,
    "id": "a4107d22ee59d9ff98c283ffd3a9d14df0f842e9",
    "semantic_title": "kebap: korean error explainable benchmark dataset for asr and post-processing",
    "citation_count": 0,
    "authors": [
      "Seonmin Koo",
      "Chanjun Park",
      "Jinsung Kim",
      "Jaehyung Seo",
      "Sugyeong Eo",
      "Hyeonseok Moon",
      "Heuiseok Lim"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.293": {
    "title": "Adaptive Policy with Wait-k Model for Simultaneous Translation",
    "volume": "main",
    "abstract": "Simultaneous machine translation (SiMT) requires a robust read/write policy in conjunction with a high-quality translation model. Traditional methods rely on either a fixed wait-k policy coupled with a standalone wait-k translation model, or an adaptive policy jointly trained with the translation model. In this study, we propose a more flexible approach by decoupling the adaptive policy model from the translation model. Our motivation stems from the observation that a standalone multi-path wait-k model performs competitively with adaptive policies utilized in state-of-the-art SiMT approaches. Specifically, we introduce DaP, a divergence-based adaptive policy, that makes read/write decisions for any translation model based on the potential divergence in translation distributions resulting from future information. DaP extends a frozen wait-k model with lightweight parameters, and is both memory and computation efficient. Experimental results across various benchmarks demonstrate that our approach offers an improved trade-off between translation accuracy and latency, outperforming strong baselines",
    "checked": false,
    "id": "1a80d32e1800de71ed8c2f1aa4b63b4ce3f8efeb",
    "semantic_title": "adaptive policy with wait-$k$ model for simultaneous translation",
    "citation_count": 1,
    "authors": [
      "Libo Zhao",
      "Kai Fan",
      "Wei Luo",
      "Wu Jing",
      "Shushu Wang",
      "Ziqian Zeng",
      "Zhongqiang Huang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.294": {
    "title": "Cross-Document Event Coreference Resolution on Discourse Structure",
    "volume": "main",
    "abstract": "Cross-document event coreference resolution (CD-ECR) is a task of clustering event mentions across multiple documents that refer to the same real-world events. Previous studies usually model the CD-ECR task as a pairwise similarity comparison problem by using different event mention features, and consider the highly similar event mention pairs in the same cluster as coreferent. In general, most of them only consider the local context of event mentions and ignore their implicit global information, thus failing to capture the interactions of long-distance event mentions. To address the above issue, we regard discourse structure as global information to further improve CD-ECR. First, we use a discourse rhetorical structure constructor to construct tree structures to represent documents. Then, we obtain shortest dependency paths from the tree structures to represent interactions between event mention pairs. Finally, we feed the above information to a multi-layer perceptron to capture the similarities of event mention pairs for resolving coreferent events. Experimental results on the ECB+ dataset show that our proposed model outperforms several baselines and achieves the competitive performance with the start-of-the-art baselines",
    "checked": true,
    "id": "46d0161a67e8e1694479ee2722600738d8b614bd",
    "semantic_title": "cross-document event coreference resolution on discourse structure",
    "citation_count": 0,
    "authors": [
      "Xinyu Chen",
      "Sheng Xu",
      "Peifeng Li",
      "Qiaoming Zhu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.295": {
    "title": "Post-hoc Utterance Refining Method by Entity Mining for Faithful Knowledge Grounded Conversations",
    "volume": "main",
    "abstract": "Despite the striking advances in recent language generation performance, model-generated responses have suffered from the chronic problem of hallucinations that are either untrue or unfaithful to a given source. Especially in the task of knowledge grounded conversation, the models are required to generate informative responses, but hallucinated utterances lead to miscommunication. In particular, entity-level hallucination that causes critical misinformation and undesirable conversation is one of the major concerns. To address this issue, we propose a post-hoc refinement method called REM. It aims to enhance the quality and faithfulness of hallucinated utterances by refining them based on the source knowledge. If the generated utterance has a low source-faithfulness score with the given knowledge, REM mines the key entities in the knowledge and implicitly uses them for refining the utterances. We verify that our method reduces entity hallucination in the utterance. Also, we show the adaptability and efficacy of REM with extensive experiments and generative results. Our code is available at https://github.com/YOONNAJANG/REM",
    "checked": true,
    "id": "32c70dd2cdef0c301b30f03441d33018672f721c",
    "semantic_title": "post-hoc utterance refining method by entity mining for faithful knowledge grounded conversations",
    "citation_count": 0,
    "authors": [
      "Yoonna Jang",
      "Suhyune Son",
      "Jeongwoo Lee",
      "Junyoung Son",
      "Yuna Hur",
      "Jungwoo Lim",
      "Hyeonseok Moon",
      "Kisu Yang",
      "Heuiseok Lim"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.296": {
    "title": "Can We Edit Factual Knowledge by In-Context Learning?",
    "volume": "main",
    "abstract": "Previous studies have shown that large language models (LLMs) like GPTs store massive factual knowledge in their parameters. However, the stored knowledge could be false or outdated. Traditional knowledge editing methods refine LLMs via fine-tuning on texts containing specific knowledge. However, with the increasing scales of LLMs, these gradient-based approaches bring large computation costs. The trend of model-as-a-service also makes it impossible to modify knowledge in black-box LMs. Inspired by in-context learning (ICL), a new paradigm based on demonstration contexts without parameter updating, we explore whether ICL can edit factual knowledge. To answer this question, we give a comprehensive empirical study of ICL strategies. Experiments show that in-context knowledge editing (IKE), without any gradient and parameter updating, achieves a competitive success rate compared to gradient-based methods on GPT-J (6B) but with much fewer side effects, including less over-editing on similar but unrelated facts and less knowledge forgetting on previously stored knowledge. We also apply the method to larger LMs with tens or hundreds of parameters like OPT-175B, which shows the scalability of our method. The code is available at https://github.com/pkunlp-icler/IKE",
    "checked": true,
    "id": "ff2a0fb125e7f03428420230c6ecbeafd4cf07a8",
    "semantic_title": "can we edit factual knowledge by in-context learning?",
    "citation_count": 15,
    "authors": [
      "Ce Zheng",
      "Lei Li",
      "Qingxiu Dong",
      "Yuxuan Fan",
      "Zhiyong Wu",
      "Jingjing Xu",
      "Baobao Chang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.297": {
    "title": "EDIS: Entity-Driven Image Search over Multimodal Web Content",
    "volume": "main",
    "abstract": "Making image retrieval methods practical for real-world search applications requires significant progress in dataset scales, entity comprehension, and multimodal information fusion. In this work, we introduce Entity-Driven Image Search (EDIS), a challenging dataset for cross-modal image search in the news domain. EDIS consists of 1 million web images from actual search engine results and curated datasets, with each image paired with a textual description. Unlike datasets that assume a small set of single-modality candidates, EDIS reflects real-world web image search scenarios by including a million multimodal image-text pairs as candidates. EDIS encourages the development of retrieval models that simultaneously address cross-modal information fusion and matching. To achieve accurate ranking results, a model must: 1) understand named entities and events from text queries, 2) ground entities onto images or text descriptions, and 3) effectively fuse textual and visual representations. Our experimental results show that EDIS challenges state-of-the-art methods with dense entities and the large-scale candidate set. The ablation study also proves that fusing textual features with visual features is critical in improving retrieval results",
    "checked": true,
    "id": "9654f4054b543f2e15e3138342930e1ec33e037a",
    "semantic_title": "edis: entity-driven image search over multimodal web content",
    "citation_count": 1,
    "authors": [
      "Siqi Liu",
      "Weixi Feng",
      "Tsu-Jui Fu",
      "Wenhu Chen",
      "William Wang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.298": {
    "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints",
    "volume": "main",
    "abstract": "Multi-query attention (MQA), which only uses a single key-value head, drastically speeds up decoder inference. However, MQA can lead to quality degradation, and moreover it may not be desirable to train a separate model just for faster inference. We (1) propose a recipe for uptraining existing multi-head language model checkpoints into models with MQA using 5% of original pre-training compute, and (2) introduce grouped-query attention (GQA), a generalization of multi-query attention which uses an intermediate (more than one, less than number of query heads) number of key-value heads. We show that uptrained GQA achieves quality close to multi-head attention with comparable speed to MQA",
    "checked": true,
    "id": "5ae6fb6b5a3c7df515ff4a82ac9673bae6a8e200",
    "semantic_title": "gqa: training generalized multi-query transformer models from multi-head checkpoints",
    "citation_count": 27,
    "authors": [
      "Joshua Ainslie",
      "James Lee-Thorp",
      "Michiel de Jong",
      "Yury Zemlyanskiy",
      "Federico Lebron",
      "Sumit Sanghai"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.299": {
    "title": "Towards a Mechanistic Interpretation of Multi-Step Reasoning Capabilities of Language Models",
    "volume": "main",
    "abstract": "Recent work has shown that language models (LMs) have strong multi-step (i.e., procedural) reasoning capabilities. However, it is unclear whether LMs perform these tasks by cheating with answers memorized from pretraining corpus, or, via a multi-step reasoning mechanism. In this paper, we try to answer this question by exploring a mechanistic interpretation of LMs for multi-step reasoning tasks. Concretely, we hypothesize that the LM implicitly embeds a reasoning tree resembling the correct reasoning process within it. We test this hypothesis by introducing a new probing approach (called MechanisticProbe) that recovers the reasoning tree from the model's attention patterns. We use our probe to analyze two LMs: GPT-2 on a synthetic task (k-th smallest element), and LLaMA on two simple language-based reasoning tasks (ProofWriter & AI2 Reasoning Challenge). We show that MechanisticProbe is able to detect the information of the reasoning tree from the model's attentions for most examples, suggesting that the LM indeed is going through a process of multi-step reasoning within its architecture in many cases",
    "checked": true,
    "id": "70ca99ac3c21f353b3db948004510a09fdebc4f2",
    "semantic_title": "towards a mechanistic interpretation of multi-step reasoning capabilities of language models",
    "citation_count": 0,
    "authors": [
      "Yifan Hou",
      "Jiaoda Li",
      "Yu Fei",
      "Alessandro Stolfo",
      "Wangchunshu Zhou",
      "Guangtao Zeng",
      "Antoine Bosselut",
      "Mrinmaya Sachan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.300": {
    "title": "BiasX: \"Thinking Slow\" in Toxic Content Moderation with Explanations of Implied Social Biases",
    "volume": "main",
    "abstract": "Toxicity annotators and content moderators often default to mental shortcuts when making decisions. This can lead to subtle toxicity being missed, and seemingly toxic but harmless content being over-detected. We introduce BiasX, a framework that enhances content moderation setups with free-text explanations of statements' implied social biases, and explore its effectiveness through a large-scale crowdsourced user study. We show that indeed, participants substantially benefit from explanations for correctly identifying subtly (non-)toxic content. The quality of explanations is critical: imperfect machine-generated explanations (+2.4% on hard toxic examples) help less compared to expert-written human explanations (+7.2%). Our results showcase the promise of using free-text explanations to encourage more thoughtful toxicity moderation",
    "checked": true,
    "id": "85a5ffc509fa50c96b415e09ae87fb6e5f435b37",
    "semantic_title": "biasx: \"thinking slow\" in toxic content moderation with explanations of implied social biases",
    "citation_count": 0,
    "authors": [
      "Yiming Zhang",
      "Sravani Nanduri",
      "Liwei Jiang",
      "Tongshuang Wu",
      "Maarten Sap"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.301": {
    "title": "Text encoders bottleneck compositionality in contrastive vision-language models",
    "volume": "main",
    "abstract": "Performant vision-language (VL) models like CLIP represent captions using a single vector. How much information about language is lost in this bottleneck? We first curate CompPrompts, a set of increasingly compositional image captions that VL models should be able to capture (e.g., single object, to object+property, to multiple interacting objects). Then, we train text-only recovery probes that aim to reconstruct captions from single-vector text representations produced by several VL models. This approach does not require images, allowing us to test on a broader range of scenes compared to prior work. We find that: 1) CLIP's text encoder falls short on more compositional inputs, including object relationships, attribute-object association, counting, and negations; 2) some text encoders work significantly better than others; and 3) text-only recovery performance predicts multimodal matching performance on ControlledImCaps: a new evaluation benchmark we collect and release consisting of fine-grained compositional images and captions. Specifically, our results suggest text-only recoverability is a necessary (but not sufficient) condition for modeling compositional factors in contrastive VL models. We release our datasets and code",
    "checked": true,
    "id": "29fe4085c4e2cfa67a8bfe4fd060b7adfa866b5b",
    "semantic_title": "text encoders bottleneck compositionality in contrastive vision-language models",
    "citation_count": 2,
    "authors": [
      "Amita Kamath",
      "Jack Hessel",
      "Kai-Wei Chang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.302": {
    "title": "Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs Through a Global Prompt Hacking Competition",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) are increasingly being deployed in interactive contexts that involve direct user engagement, such as chatbots and writing assistants. These deployments are increasingly plagued by prompt injection and jailbreaking (collectively, prompt hacking), in which models are manipulated to ignore their original instructions and instead follow potentially malicious ones. Although widely acknowledged as a significant security threat, there is a dearth of a large-scale resource and quantitative study on prompt hacking. To address this lacuna, we launch a global prompt hacking competition, which allows for free-form human input attacks. We elicit 600K+ adversarial prompts against three state-of-the-art LLMs. We describe the dataset, which empirically verifies that current LLMs can indeed be manipulated via prompt hacking. We also present a comprehensive ontology of the types of adversarial prompts",
    "checked": true,
    "id": "d347fb65bd375771b013c517c861db926322e410",
    "semantic_title": "ignore this title and hackaprompt: exposing systemic vulnerabilities of llms through a global prompt hacking competition",
    "citation_count": 0,
    "authors": [
      "Sander Schulhoff",
      "Jeremy Pinto",
      "Anaum Khan",
      "Louis-François Bouchard",
      "Chenglei Si",
      "Svetlina Anati",
      "Valen Tagliabue",
      "Anson Kost",
      "Christopher Carnahan",
      "Jordan Boyd-Graber"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.303": {
    "title": "MMNMT: Modularizing Multilingual Neural Machine Translation with Flexibly Assembled MoE and Dense Blocks",
    "volume": "main",
    "abstract": "Mixture-of-Experts (MoE) based sparse architectures can significantly increase model capacity with sublinear computational overhead, which are hence widely used in massively multilingual neural machine translation (MNMT). However, they are prone to overfitting on low-resource language translation. In this paper, we propose a modularized MNMT framework that is able to flexibly assemble dense and MoE-based sparse modules to achieve the best of both worlds. The training strategy of the modularized MNMT framework consists of three stages: (1) Pre-training basic MNMT models with different training objectives or model structures, (2) Initializing modules of the framework with pre-trained couterparts (e.g., encoder, decoder and embedding layers) from the basic models and (3) Fine-tuning the modularized MNMT framework to fit modules from different models together. We pre-train three basic MNMT models from scratch: a dense model, an MoE-based sparse model and a new MoE model, termed as MoE-LGR that explores multiple Language-Group-specifc Routers to incorporate language group knowledge into MNMT. The strengths of these pre-trained models are either on low-resource language translation, high-resource language translation or zero-shot translation. Our modularized MNMT framework attempts to incorporate these advantages into a single model with reasonable initialization and fine-tuning. Experiments on widely-used benchmark datasets demonstrate that the proposed modularized MNMT framwork substantially outperforms both MoE and dense models on high- and low-resource language translation as well as zero-shot translation. Our framework facilitates the combination of different methods with their own strengths and recycling off-the-shelf models for multilingual neural machine translation. Codes are available at https://github.com/lishangjie1/MMNMT",
    "checked": true,
    "id": "1a220cad4f150036086f8e1ec437dfa11209c10b",
    "semantic_title": "mmnmt: modularizing multilingual neural machine translation with flexibly assembled moe and dense blocks",
    "citation_count": 0,
    "authors": [
      "Shangjie Li",
      "Xiangpeng Wei",
      "Shaolin Zhu",
      "Jun Xie",
      "Baosong Yang",
      "Deyi Xiong"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.304": {
    "title": "Localizing Active Objects from Egocentric Vision with Symbolic World Knowledge",
    "volume": "main",
    "abstract": "The ability to actively ground task instructions from an egocentric view is crucial for AI agents to accomplish tasks or assist humans virtually. One important step towards this goal is to localize and track key active objects that undergo major state change as a consequence of human actions/interactions to the environment without being told exactly what/where to ground (e.g., localizing and tracking the ‘sponge‘ in video from the instruction \"Dip the sponge into the bucket.\"). While existing works approach this problem from a pure vision perspective, we investigate to which extent the textual modality (i.e., task instructions) and their interaction with visual modality can be beneficial. Specifically, we propose to improve phrase grounding models' ability on localizing the active objects by: (1) learning the role of ‘objects undergoing change‘ and extracting them accurately from the instructions, (2) leveraging pre- and post-conditions of the objects during actions, and (3) recognizing the objects more robustly with descriptional knowledge. We leverage large language models (LLMs) to extract the aforementioned action-object knowledge, and design a per-object aggregation masking technique to effectively perform joint inference on object phrases and symbolic knowledge. We evaluate our framework on Ego4D and Epic-Kitchens datasets. Extensive experiments demonstrate the effectiveness of our proposed framework, which leads to>54% improvements in all standard metrics on the TREK-150-OPE-Det localization + tracking task, >7% improvements in all standard metrics on the TREK-150-OPE tracking task, and >3% improvements in average precision (AP) on the Ego4D SCOD task",
    "checked": true,
    "id": "1cbdccf8060cb5fb08ddef60242a7e96fc473cd3",
    "semantic_title": "localizing active objects from egocentric vision with symbolic world knowledge",
    "citation_count": 1,
    "authors": [
      "Te-Lin Wu",
      "Yu Zhou",
      "Nanyun Peng"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.305": {
    "title": "Introducing Rhetorical Parallelism Detection: A New Task with Datasets, Metrics, and Baselines",
    "volume": "main",
    "abstract": "Rhetoric, both spoken and written, involves not only content but also style. One common stylistic tool is parallelism: the juxtaposition of phrases which have the same sequence of linguistic (e.g., phonological, syntactic, semantic) features. Despite the ubiquity of parallelism, the field of natural language processing has seldom investigated it, missing a chance to better understand the nature of the structure, meaning, and intent that humans convey. To address this, we introduce the task of rhetorical parallelism detection. We construct a formal definition of it; we provide one new Latin dataset and one adapted Chinese dataset for it; we establish a family of metrics to evaluate performance on it; and, lastly, we create baseline systems and novel sequence labeling schemes to capture it. On our strictest metric, we attain F1 scores of 0.40 and 0.43 on our Latin and Chinese datasets, respectively",
    "checked": true,
    "id": "f7886c9ec4e56a48aef5a3866278215f4e74c9dc",
    "semantic_title": "introducing rhetorical parallelism detection: a new task with datasets, metrics, and baselines",
    "citation_count": 0,
    "authors": [
      "Stephen Bothwell",
      "Justin DeBenedetto",
      "Theresa Crnkovich",
      "Hildegund Muller",
      "David Chiang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.306": {
    "title": "Prompting is not a substitute for probability measurements in large language models",
    "volume": "main",
    "abstract": "Prompting is now a dominant method for evaluating the linguistic knowledge of large language models (LLMs). While other methods directly read out models' probability distributions over strings, prompting requires models to access this internal information by processing linguistic input, thereby implicitly testing a new type of emergent ability: metalinguistic judgment. In this study, we compare metalinguistic prompting and direct probability measurements as ways of measuring models' linguistic knowledge. Broadly, we find that LLMs' metalinguistic judgments are inferior to quantities directly derived from representations. Furthermore, consistency gets worse as the prompt query diverges from direct measurements of next-word probabilities. Our findings suggest that negative results relying on metalinguistic prompts cannot be taken as conclusive evidence that an LLM lacks a particular linguistic generalization. Our results also highlight the value that is lost with the move to closed APIs where access to probability distributions is limited",
    "checked": true,
    "id": "0e0d72be9950fde9b5e8996e2147d1318f216ebb",
    "semantic_title": "prompting is not a substitute for probability measurements in large language models",
    "citation_count": 5,
    "authors": [
      "Jennifer Hu",
      "Roger Levy"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.307": {
    "title": "Parameter-Efficient Language Model Tuning with Active Learning in Low-Resource Settings",
    "volume": "main",
    "abstract": "Pre-trained language models (PLMs) have ignited a surge in demand for effective fine-tuning techniques, particularly in low-resource domains and languages. Active learning (AL), a set of algorithms designed to decrease labeling costs by minimizing label complexity, has shown promise in confronting the labeling bottleneck. In parallel, adapter modules designed for parameter-efficient fine-tuning (PEFT) have demonstrated notable potential in low-resource settings. However, the interplay between AL and adapter-based PEFT remains unexplored. We present an empirical study of PEFT behavior with AL in low-resource settings for text classification tasks. Our findings affirm the superiority of PEFT over full-fine tuning (FFT) in low-resource settings and demonstrate that this advantage persists in AL setups. We further examine the properties of PEFT and FFT through the lens of forgetting dynamics and instance-level representations, where we find that PEFT yields more stable representations of early and middle layers compared to FFT. Our research underscores the synergistic potential of AL and PEFT in low-resource settings, paving the way for advancements in efficient and effective fine-tuning",
    "checked": true,
    "id": "7b50b4e98b7f4a5662dd87e4de929b1037bc37c4",
    "semantic_title": "parameter-efficient language model tuning with active learning in low-resource settings",
    "citation_count": 0,
    "authors": [
      "Josip Jukić",
      "Jan Snajder"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.308": {
    "title": "Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks",
    "volume": "main",
    "abstract": "Data contamination has become prevalent and challenging with the rise of models pretrained on large automatically-crawled corpora. For closed models, the training data becomes a trade secret, and even for open models, it is not trivial to detect contamination. Strategies such as leaderboards with hidden answers, or using test data which is guaranteed to be unseen, are expensive and become fragile with time. Assuming that all relevant actors value clean test data and will cooperate to mitigate data contamination, what can be done? We propose three strategies that can make a difference: (1) Test data made public should be encrypted with a public key and licensed to disallow derivative distribution; (2) demand training exclusion controls from closed API holders, and protect your test data by refusing to evaluate without them; (3) avoid data which appears with its solution on the internet, and release the web-page context of internet-derived data along with the data. These strategies are practical and can be effective in preventing data contamination",
    "checked": true,
    "id": "fc30093e9f55ae1c0a1d2c4c4e5341998adede66",
    "semantic_title": "stop uploading test data in plain text: practical strategies for mitigating data contamination by evaluation benchmarks",
    "citation_count": 22,
    "authors": [
      "Alon Jacovi",
      "Avi Caciularu",
      "Omer Goldman",
      "Yoav Goldberg"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.309": {
    "title": "CoLT5: Faster Long-Range Transformers with Conditional Computation",
    "volume": "main",
    "abstract": "Many natural language processing tasks benefit from long inputs, but processing long documents with Transformers is expensive – not only due to quadratic attention complexity but also from applying feedforward and projection layers to every token. However, not all tokens are equally important, especially for longer documents. We propose CoLT5, a long-input Transformer model that builds on this intuition by employing conditional computation, devoting more resources to important tokens in both feedforward and attention layers. We show that CoLT5 achieves stronger performance than LongT5 with much faster training and inference, achieving SOTA on the long-input SCROLLS benchmark. Moreover, CoLT5 can effectively and tractably make use of extremely long inputs, showing strong gains up to 64k input length",
    "checked": true,
    "id": "27d391d65ab42c30dc35595213ba6585633afa5d",
    "semantic_title": "colt5: faster long-range transformers with conditional computation",
    "citation_count": 27,
    "authors": [
      "Joshua Ainslie",
      "Tao Lei",
      "Michiel de Jong",
      "Santiago Ontanon",
      "Siddhartha Brahma",
      "Yury Zemlyanskiy",
      "David Uthus",
      "Mandy Guo",
      "James Lee-Thorp",
      "Yi Tay",
      "Yun-Hsuan Sung",
      "Sumit Sanghai"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.310": {
    "title": "DiSTRICT: Dialogue State Tracking with Retriever Driven In-Context Tuning",
    "volume": "main",
    "abstract": "Dialogue State Tracking (DST), a key component of task-oriented conversation systems, represents user intentions by determining the values of pre-defined slots in an ongoing dialogue. Existing approaches use hand-crafted templates and additional slot information to fine-tune and prompt large pre-trained language models and elicit slot values from the dialogue context. Significant manual effort and domain knowledge is required to design effective prompts, limiting the generalizability of these approaches to new domains and tasks. In this work, we propose DiSTRICT, a generalizable in-context tuning approach for DST that retrieves highly relevant training examples for a given dialogue to fine-tune the model without any hand-crafted templates. Experiments with the MultiWOZ benchmark datasets show that DiSTRICT outperforms existing approaches in various zero-shot and few-shot settings using a much smaller model, thereby providing an important advantage for real-world deployments that often have limited resource availability",
    "checked": true,
    "id": "8cdf08abce73b61bc700bcb3458cab27b26cdd03",
    "semantic_title": "district: dialogue state tracking with retriever driven in-context tuning",
    "citation_count": 2,
    "authors": [
      "Praveen Venkateswaran",
      "Evelyn Duesterwald",
      "Vatche Isahagian"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.311": {
    "title": "Cross-Cultural Analysis of Human Values, Morals, and Biases in Folk Tales",
    "volume": "main",
    "abstract": "Folk tales are strong cultural and social influences in children's lives, and they are known to teach morals and values. However, existing studies on folk tales are largely limited to European tales. In our study, we compile a large corpus of over 1,900 tales originating from 27 diverse cultures across six continents. Using a range of lexicons and correlation analyses, we examine how human values, morals, and gender biases are expressed in folk tales across cultures. We discover differences between cultures in prevalent values and morals, as well as cross-cultural trends in problematic gender biases. Furthermore, we find trends of reduced value expression when examining public-domain fiction stories, extrinsically validate our analyses against the multicultural Schwartz Survey of Cultural Values and the Global Gender Gap Report, and find traditional gender biases associated with values, morals, and agency. This large-scale cross-cultural study of folk tales paves the way towards future studies on how literature influences and reflects cultural norms",
    "checked": true,
    "id": "bbbc59161fcc847d2db3c4e8f285e9bc7429f8be",
    "semantic_title": "cross-cultural analysis of human values, morals, and biases in folk tales",
    "citation_count": 0,
    "authors": [
      "Winston Wu",
      "Lu Wang",
      "Rada Mihalcea"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.312": {
    "title": "Non-Programmers Can Label Programs Indirectly via Active Examples: A Case Study with Text-to-SQL",
    "volume": "main",
    "abstract": "Can non-programmers annotate natural language utterances with complex programs that represent their meaning? We introduce APEL, a framework in which non-programmers select among candidate programs generated by a seed semantic parser (e.g., Codex). Since they cannot understand the candidate programs, we ask them to select indirectly by examining the programs' input-ouput examples. For each utterance, APEL actively searches for a simple input on which the candidate programs tend to produce different outputs. It then asks the non-programmers only to choose the appropriate output, thus allowing us to infer which program is correct and could be used to fine-tune the parser. As a first case study, we recruited human non-programmers to use APEL to re-annotate SPIDER, a text-to-SQL dataset. Our approach achieved the same annotation accuracy as the original expert annotators (75%) and exposed many subtle errors in the original annotations",
    "checked": true,
    "id": "959adda6f1b874be32bea9bb0bd8f3b43856980b",
    "semantic_title": "non-programmers can label programs indirectly via active examples: a case study with text-to-sql",
    "citation_count": 2,
    "authors": [
      "Ruiqi Zhong",
      "Charlie Snell",
      "Dan Klein",
      "Jason Eisner"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.313": {
    "title": "LINC: A Neurosymbolic Approach for Logical Reasoning by Combining Language Models with First-Order Logic Provers",
    "volume": "main",
    "abstract": "Logical reasoning, i.e., deductively inferring the truth value of a conclusion from a set of premises, is an important task for artificial intelligence with wide potential impacts on science, mathematics, and society. While many prompting-based strategies have been proposed to enable Large Language Models (LLMs) to do such reasoning more effectively, they still appear unsatisfactory, often failing in subtle and unpredictable ways. In this work, we investigate the validity of instead reformulating such tasks as modular neurosymbolic programming, which we call LINC: Logical Inference via Neurosymbolic Computation. In LINC, the LLM acts as a semantic parser, translating premises and conclusions from natural language to expressions in first-order logic. These expressions are then offloaded to an external theorem prover, which symbolically performs deductive inference. Leveraging this approach, we observe significant performance gains on FOLIO and a balanced subset of ProofWriter for three different models in nearly all experimental conditions we evaluate. On ProofWriter, augmenting the comparatively small open-source StarCoder+ (15.5B parameters) with LINC even outperforms GPT-3.5 and GPT-4 with Chain-of-Thought (CoT) prompting by an absolute 38% and 10%, respectively. When used with GPT-4, LINC scores 26% higher than CoT on ProofWriter while performing comparatively on FOLIO. Further analysis reveals that although both methods on average succeed roughly equally often on this dataset, they exhibit distinct and complementary failure modes. We thus provide promising evidence for how logical reasoning over natural language can be tackled through jointly leveraging LLMs alongside symbolic provers. All corresponding code is publicly available",
    "checked": true,
    "id": "66d98dc2aad17c03532dbae21d05f098257cc2e2",
    "semantic_title": "linc: a neurosymbolic approach for logical reasoning by combining language models with first-order logic provers",
    "citation_count": 1,
    "authors": [
      "Theo Olausson",
      "Alex Gu",
      "Ben Lipkin",
      "Cedegao Zhang",
      "Armando Solar-Lezama",
      "Joshua Tenenbaum",
      "Roger Levy"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.314": {
    "title": "Non-autoregressive Streaming Transformer for Simultaneous Translation",
    "volume": "main",
    "abstract": "Simultaneous machine translation (SiMT) models are trained to strike a balance between latency and translation quality. However, training these models to achieve high quality while maintaining low latency often leads to a tendency for aggressive anticipation. We argue that such issue stems from the autoregressive architecture upon which most existing SiMT models are built. To address those issues, we propose non-autoregressive streaming Transformer (NAST) which comprises a unidirectional encoder and a non-autoregressive decoder with intra-chunk parallelism. We enable NAST to generate the blank token or repetitive tokens to adjust its READ/WRITE strategy flexibly, and train it to maximize the non-monotonic latent alignment with an alignment-based latency loss. Experiments on various SiMT benchmarks demonstrate that NAST outperforms previous strong autoregressive SiMT baselines",
    "checked": true,
    "id": "db89afe2ccd11581465dc4ca8b1a85188184a309",
    "semantic_title": "non-autoregressive streaming transformer for simultaneous translation",
    "citation_count": 3,
    "authors": [
      "Zhengrui Ma",
      "Shaolei Zhang",
      "Shoutao Guo",
      "Chenze Shao",
      "Min Zhang",
      "Yang Feng"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.315": {
    "title": "ViSoBERT: A Pre-Trained Language Model for Vietnamese Social Media Text Processing",
    "volume": "main",
    "abstract": "English and Chinese, known as resource-rich languages, have witnessed the strong development of transformer-based language models for natural language processing tasks. Although Vietnam has approximately 100M people speaking Vietnamese, several pre-trained models, e.g., PhoBERT, ViBERT, and vELECTRA, performed well on general Vietnamese NLP tasks, including POS tagging and named entity recognition. These pre-trained language models are still limited to Vietnamese social media tasks. In this paper, we present the first monolingual pre-trained language model for Vietnamese social media texts, ViSoBERT, which is pre-trained on a large-scale corpus of high-quality and diverse Vietnamese social media texts using XLM-R architecture. Moreover, we explored our pre-trained model on five important natural language downstream tasks on Vietnamese social media texts: emotion recognition, hate speech detection, sentiment analysis, spam reviews detection, and hate speech spans detection. Our experiments demonstrate that ViSoBERT, with far fewer parameters, surpasses the previous state-of-the-art models on multiple Vietnamese social media tasks. Our ViSoBERT model is available only for research purposes. Disclaimer: This paper contains actual comments on social networks that might be construed as abusive, offensive, or obscene",
    "checked": true,
    "id": "b44fbb613573f78d444361d7670c8ab130ef9174",
    "semantic_title": "visobert: a pre-trained language model for vietnamese social media text processing",
    "citation_count": 0,
    "authors": [
      "Nam Nguyen",
      "Thang Phan",
      "Duc-Vu Nguyen",
      "Kiet Nguyen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.316": {
    "title": "RAPL: A Relation-Aware Prototype Learning Approach for Few-Shot Document-Level Relation Extraction",
    "volume": "main",
    "abstract": "How to identify semantic relations among entities in a document when only a few labeled documents are available? Few-shot document-level relation extraction (FSDLRE) is crucial for addressing the pervasive data scarcity problem in real-world scenarios. Metric-based meta-learning is an effective framework widely adopted for FSDLRE, which constructs class prototypes for classification. However, existing works often struggle to obtain class prototypes with accurate relational semantics: 1) To build prototype for a target relation type, they aggregate the representations of all entity pairs holding that relation, while these entity pairs may also hold other relations, thus disturbing the prototype. 2) They use a set of generic NOTA (none-of-the-above) prototypes across all tasks, neglecting that the NOTA semantics differs in tasks with different target relation types. In this paper, we propose a relation-aware prototype learning method for FSDLRE to strengthen the relational semantics of prototype representations. By judiciously leveraging the relation descriptions and realistic NOTA instances as guidance, our method effectively refines the relation prototypes and generates task-specific NOTA prototypes. Extensive experiments demonstrate that our method outperforms state-of-the-art approaches by average 2.61% F1 across various settings of two FSDLRE benchmarks",
    "checked": true,
    "id": "d9e8168f50694d4b703099724095afceaf24ed1f",
    "semantic_title": "rapl: a relation-aware prototype learning approach for few-shot document-level relation extraction",
    "citation_count": 1,
    "authors": [
      "Shiao Meng",
      "Xuming Hu",
      "Aiwei Liu",
      "Shuang Li",
      "Fukun Ma",
      "Yawen Yang",
      "Lijie Wen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.317": {
    "title": "GeoLM: Empowering Language Models for Geospatially Grounded Language Understanding",
    "volume": "main",
    "abstract": "Humans subconsciously engage in geospatial reasoning when reading articles. We recognize place names and their spatial relations in text and mentally associate them with their physical locations on Earth. Although pretrained language models can mimic this cognitive process using linguistic context, they do not utilize valuable geospatial information in large, widely available geographical databases, e.g., OpenStreetMap. This paper introduces GeoLM, a geospatially grounded language model that enhances the understanding of geo-entities in natural language. GeoLM leverages geo-entity mentions as anchors to connect linguistic information in text corpora with geospatial information extracted from geographical databases. GeoLM connects the two types of context through contrastive learning and masked language modeling. It also incorporates a spatial coordinate embedding mechanism to encode distance and direction relations to capture geospatial context. In the experiment, we demonstrate that GeoLM exhibits promising capabilities in supporting toponym recognition, toponym linking, relation extraction, and geo-entity typing, which bridge the gap between natural language processing and geospatial sciences. The code is publicly available at https://github.com/knowledge-computing/geolm",
    "checked": true,
    "id": "468fc94845b52c6e96ba1f3c3884d0653d5421b4",
    "semantic_title": "geolm: empowering language models for geospatially grounded language understanding",
    "citation_count": 0,
    "authors": [
      "Zekun Li",
      "Wenxuan Zhou",
      "Yao-Yi Chiang",
      "Muhao Chen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.318": {
    "title": "Cross-Modal Conceptualization in Bottleneck Models",
    "volume": "main",
    "abstract": "Concept Bottleneck Models (CBMs) assume that training examples (e.g., x-ray images) are annotated with high-level concepts (e.g., types of abnormalities), and perform classification by first predicting the concepts, followed by predicting the label relying on these concepts. However, the primary challenge in employing CBMs lies in the requirement of defining concepts predictive of the label and annotating training examples with these concepts. In our approach, we adopt a more moderate assumption and instead use text descriptions (e.g., radiology reports), accompanying the images, to guide the induction of concepts. Our crossmodal approach treats concepts as discrete latent variables and promotes concepts that (1) are predictive of the label, and (2) can be predicted reliably from both the image and text. Through experiments conducted on datasets ranging from synthetic datasets (e.g., synthetic images with generated descriptions) to realistic medical imaging datasets, we demonstrate that crossmodal learning encourages the induction of interpretable concepts while also facilitating disentanglement",
    "checked": true,
    "id": "b02acabd6be1206d90af3f5619c516736891b647",
    "semantic_title": "cross-modal conceptualization in bottleneck models",
    "citation_count": 0,
    "authors": [
      "Danis Alukaev",
      "Semen Kiselev",
      "Ilya Pershin",
      "Bulat Ibragimov",
      "Vladimir Ivanov",
      "Alexey Kornaev",
      "Ivan Titov"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.319": {
    "title": "LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models",
    "volume": "main",
    "abstract": "The success of large language models (LLMs), like GPT-4 and ChatGPT, has led to the development of numerous cost-effective and accessible alternatives that are created by finetuning open-access LLMs with task-specific data (e.g., ChatDoctor) or instruction data (e.g., Alpaca). Among the various fine-tuning methods, adapter-based parameter-efficient fine-tuning (PEFT) is undoubtedly one of the most attractive topics, as it only requires fine-tuning a few external parameters instead of the entire LLMs while achieving comparable or even better performance. To enable further research on PEFT methods of LLMs, this paper presents LLM-Adapters, an easy-to-use framework that integrates various adapters into LLMs and can execute these adapter-based PEFT methods of LLMs for different tasks. The framework includes state-of-the-art open-access LLMs such as LLaMA, BLOOM, and GPT-J, as well as widely used adapters such as Series adapters, Parallel adapter, Prompt-based learning and Reparametrization-based methods. Moreover, we conduct extensive empirical studies on the impact of adapter types, placement locations, and hyper-parameters to the best design for each adapter-based methods. We evaluate the effectiveness of the adapters on fourteen datasets from two different reasoning tasks, Arithmetic Reasoning and Commonsense Reasoning. The results demonstrate that using adapter-based PEFT in smaller-scale LLMs (7B) with few extra trainable parameters yields comparable, and in some cases superior, performance to powerful LLMs (175B) in zero-shot inference on simple math reasoning datasets",
    "checked": true,
    "id": "bdb68c5e2369633b20e733774ac66eb4600c34d1",
    "semantic_title": "llm-adapters: an adapter family for parameter-efficient fine-tuning of large language models",
    "citation_count": 34,
    "authors": [
      "Zhiqiang Hu",
      "Lei Wang",
      "Yihuai Lan",
      "Wanyu Xu",
      "Ee-Peng Lim",
      "Lidong Bing",
      "Xing Xu",
      "Soujanya Poria",
      "Roy Lee"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.320": {
    "title": "DREAM: Deployment of Recombination and Ensembles in Argument Mining",
    "volume": "main",
    "abstract": "Current approaches to Argument Mining (AM) tend to take a holistic or black-box view of the overall pipeline. This paper, in contrast, aims to provide a solution to achieve increased performance based on current components instead of independent all-new solutions. To that end, it presents the Deployment of Recombination and Ensemble methods for Argument Miners (DREAM) framework that allows for the (automated) combination of AM components. Using ensemble methods, DREAM combines sets of AM systems to improve accuracy for the four tasks in the AM pipeline. Furthermore, it leverages recombination by using different argument miners elements throughout the pipeline. Experiments with five systems previously included in a benchmark show that the systems combined with DREAM can outperform the previous best single systems in terms of accuracy measured by an AM benchmark",
    "checked": true,
    "id": "115a24c2d9e056766403a9971127f4f140f5ca79",
    "semantic_title": "dream: deployment of recombination and ensembles in argument mining",
    "citation_count": 0,
    "authors": [
      "Florian Ruosch",
      "Cristina Sarasua",
      "Abraham Bernstein"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.321": {
    "title": "MILDSum: A Novel Benchmark Dataset for Multilingual Summarization of Indian Legal Case Judgments",
    "volume": "main",
    "abstract": "Automatic summarization of legal case judgments is a practically important problem that has attracted substantial research efforts in many countries. In the context of the Indian judiciary, there is an additional complexity – Indian legal case judgments are mostly written in complex English, but a significant portion of India's population lacks command of the English language. Hence, it is crucial to summarize the legal documents in Indian languages to ensure equitable access to justice. While prior research primarily focuses on summarizing legal case judgments in their source languages, this study presents a pioneering effort toward cross-lingual summarization of English legal documents into Hindi, the most frequently spoken Indian language. We construct the first high-quality legal corpus comprising of 3,122 case judgments from prominent Indian courts in English, along with their summaries in both English and Hindi, drafted by legal practitioners. We benchmark the performance of several diverse summarization approaches on our corpus and demonstrate the need for further research in cross-lingual summarization in the legal domain",
    "checked": true,
    "id": "b3829f2d2a670a08c435d445b6f80d734ddb8da4",
    "semantic_title": "mildsum: a novel benchmark dataset for multilingual summarization of indian legal case judgments",
    "citation_count": 0,
    "authors": [
      "Debtanu Datta",
      "Shubham Soni",
      "Rajdeep Mukherjee",
      "Saptarshi Ghosh"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.322": {
    "title": "Query Rewriting in Retrieval-Augmented Large Language Models",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) play powerful, black-box readers in the retrieve-then-read pipeline, making remarkable progress in knowledge-intensive tasks. This work introduces a new framework, Rewrite-Retrieve-Read instead of the previous retrieve-then-read for the retrieval-augmented LLMs from the perspective of the query rewriting. Unlike prior studies focusing on adapting either the retriever or the reader, our approach pays attention to the adaptation of the search query itself, for there is inevitably a gap between the input text and the needed knowledge in retrieval. We first prompt an LLM to generate the query, then use a web search engine to retrieve contexts. Furthermore, to better align the query to the frozen modules, we propose a trainable scheme for our pipeline. A small language model is adopted as a trainable rewriter to cater to the black-box LLM reader. The rewriter is trained using the feedback of the LLM reader by reinforcement learning. Evaluation is conducted on downstream tasks, open-domain QA and multiple-choice QA. Experiments results show consistent performance improvement, indicating that our framework is proven effective and scalable, and brings a new framework for retrieval-augmented LLM",
    "checked": true,
    "id": "3dd9b9c85431534ef74a8a677b991ccc405c85cb",
    "semantic_title": "query rewriting in retrieval-augmented large language models",
    "citation_count": 0,
    "authors": [
      "Xinbei Ma",
      "Yeyun Gong",
      "Pengcheng He",
      "Hai Zhao",
      "Nan Duan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.323": {
    "title": "PromptMix: A Class Boundary Augmentation Method for Large Language Model Distillation",
    "volume": "main",
    "abstract": "Data augmentation is a widely used technique to address the problem of text classification when there is a limited amount of training data. Recent work often tackles this problem using large language models (LLMs) like GPT3 that can generate new examples given already available ones. In this work, we propose a method to generate more helpful augmented data by utilizing the LLM's abilities to follow instructions and perform few-shot classifications. Our specific PromptMix method consists of two steps: 1) generate challenging text augmentations near class boundaries; however, generating borderline examples increases the risk of false positives in the dataset, so we 2) relabel the text augmentations using a prompting-based LLM classifier to enhance the correctness of labels in the generated data. We evaluate the proposed method in challenging 2-shot and zero-shot settings on four text classification datasets: Banking77, TREC6, Subjectivity (SUBJ), and Twitter Complaints. Our experiments show that generating and, crucially, relabeling borderline examples facilitates the transfer of knowledge of a massive LLM like GPT3.5-turbo into smaller and cheaper classifiers like DistilBERT-base and BERT-base. Furthermore, 2-shot PromptMix outperforms multiple 5-shot data augmentation methods on the four datasets. Our code is available at https://github.com/ServiceNow/PromptMix-EMNLP-2023",
    "checked": true,
    "id": "3d181992f7b6b65c889346ee7ea99fdc1570d9b6",
    "semantic_title": "promptmix: a class boundary augmentation method for large language model distillation",
    "citation_count": 5,
    "authors": [
      "Gaurav Sahu",
      "Olga Vechtomova",
      "Dzmitry Bahdanau",
      "Issam Laradji"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.324": {
    "title": "COHESENTIA: A Novel Benchmark of Incremental versus Holistic Assessment of Coherence in Generated Texts",
    "volume": "main",
    "abstract": "Coherence is a linguistic term that refers to the relations between small textual units (sentences, propositions), which make the text logically consistent and meaningful to the reader. With the advances of generative foundational models in NLP, there is a pressing need to automatically assess the human-perceived coherence of automatically generated texts. Up until now, little work has been done on explicitly assessing the coherence of generated texts and analyzing the factors contributing to (in)coherence. Previous work on the topic used other tasks, e.g., sentence reordering, as proxies of coherence, rather than approaching coherence detection heads on. In this paper, we introduce CoheSentia, a novel benchmark of human-perceived coherence of automatically generated texts. Our annotation protocol reflects two perspectives; one is global, assigning a single coherence score, and the other is incremental, scoring sentence by sentence. The incremental method produces an (in)coherence score for each text fragment and also pinpoints reasons for incoherence at that point. Our benchmark contains 500 automatically-generated and human-annotated paragraphs, each annotated in both methods, by multiple raters. Our analysis shows that the inter-annotator agreement in the incremental mode is higher than in the holistic alternative, and our experiments show that standard LMs fine-tuned for coherence detection show varied performance on the different factors contributing to (in)coherence. All in all, these models yield unsatisfactory performance, emphasizing the need for developing more reliable methods for coherence assessment",
    "checked": true,
    "id": "d67183bde34527d8a7643f4512379b3f21b9cc28",
    "semantic_title": "cohesentia: a novel benchmark of incremental versus holistic assessment of coherence in generated texts",
    "citation_count": 0,
    "authors": [
      "Aviya Maimon",
      "Reut Tsarfaty"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.325": {
    "title": "QUDeval: The Evaluation of Questions Under Discussion Discourse Parsing",
    "volume": "main",
    "abstract": "Questions Under Discussion (QUD) is a versatile linguistic framework in which discourse progresses as continuously asking questions and answering them. Automatic parsing of a discourse to produce a QUD structure thus entails a complex question generation task: given a document and an answer sentence, generate a question that satisfies linguistic constraints of QUD and can be grounded in an anchor sentence in prior context. These questions are known to be curiosity-driven and open-ended. This work introduces the first framework for the automatic evaluation of QUD parsing, instantiating the theoretical constraints of QUD in a concrete protocol. We present QUDeval, a dataset of fine-grained evaluation of 2,190 QUD questions generated from both fine-tuned systems and LLMs. Using QUDeval, we show that satisfying all constraints of QUD is still challenging for modern LLMs, and that existing evaluation metrics poorly approximate parser quality. Encouragingly, human-authored QUDs are scored highly by our human evaluators, suggesting that there is headroom for further progress on language modeling to improve both QUD parsing and QUD evaluation",
    "checked": true,
    "id": "fc7e0d394bb58c292ac8cca26c776fee53a4020c",
    "semantic_title": "qudeval: the evaluation of questions under discussion discourse parsing",
    "citation_count": 0,
    "authors": [
      "Yating Wu",
      "Ritika Mangla",
      "Greg Durrett",
      "Junyi Li"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.326": {
    "title": "PRCA: Fitting Black-Box Large Language Models for Retrieval Question Answering via Pluggable Reward-Driven Contextual Adapter",
    "volume": "main",
    "abstract": "The Retrieval Question Answering (ReQA) task employs the retrieval-augmented framework, composed of a retriever and generator. The generators formulate the answer based on the documents retrieved by the retriever. Incorporating Large Language Models (LLMs) as generators is beneficial due to their advanced QA capabilities, but they are typically too large to be fine-tuned with budget constraints while some of them are only accessible via APIs. To tackle this issue and further improve ReQA performance, we propose a trainable Pluggable Reward-Driven Contextual Adapter (PRCA), keeping the generator as a black box. Positioned between the retriever and generator in a Pluggable manner, PRCA refines the retrieved information by operating in a token-autoregressive strategy via maximizing rewards of the reinforcement learning phase. Our experiments validate PRCA's effectiveness in enhancing ReQA performance on three datasets by up to 20% improvement to fit black-box LLMs into existing frameworks, demonstrating its considerable potential in the LLMs era",
    "checked": true,
    "id": "d824b7154ad6df729fef49030ee170466911414b",
    "semantic_title": "prca: fitting black-box large language models for retrieval question answering via pluggable reward-driven contextual adapter",
    "citation_count": 0,
    "authors": [
      "Haoyan Yang",
      "Zhitao Li",
      "Yong Zhang",
      "Jianzong Wang",
      "Ning Cheng",
      "Ming Li",
      "Jing Xiao"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.327": {
    "title": "Exploring Chain of Thought Style Prompting for Text-to-SQL",
    "volume": "main",
    "abstract": "In-context learning with large language models (LLMs) has recently caught increasing attention due to its superior few-shot performance on various tasks. However, its performance on text-to-SQL parsing still has much room for improvement. In this paper, we hypothesize that a crucial aspect of LLMs to improve for text-to-SQL parsing is their multi-step reasoning ability. Thus, we systematically study how to enhance LLMs' reasoning ability through chain of thought (CoT) style prompting, including the original chain-of-thought prompting and least-to-most prompting. Our experiments demonstrate that iterative prompting as in least-to-most prompting may be unnecessary for text-to-SQL parsing, and using detailed reasoning steps tends to have more error propagation issues. Based on these findings, we propose a new CoT-style prompting method for text-to-SQL parsing. It brings 5.2 and 6.5 point absolute gains on the Spider development set and the Spider Realistic set, respectively, compared to the standard prompting method without reasoning steps; 2.4 and 1.5 point absolute gains, compared to the least-to-most prompting method",
    "checked": false,
    "id": "68ad0ed1e21fd9fd7b2bd58769d8bec88c996b01",
    "semantic_title": "exploring chain-of-thought style prompting for text-to-sql",
    "citation_count": 4,
    "authors": [
      "Chang-Yu Tai",
      "Ziru Chen",
      "Tianshu Zhang",
      "Xiang Deng",
      "Huan Sun"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.328": {
    "title": "Efficient Algorithms for Recognizing Weighted Tree-Adjoining Languages",
    "volume": "main",
    "abstract": "The class of tree-adjoining languages can be characterized by various two-level formalisms, consisting of a context-free grammar (CFG) or pushdown automaton (PDA) controlling another CFG or PDA. These four formalisms are equivalent to tree-adjoining grammars (TAG), linear indexed grammars (LIG), pushdown-adjoining automata (PAA), and embedded pushdown automata (EPDA). We define semiring-weighted versions of the above two-level formalisms, and we design new algorithms for computing their stringsums (the weight of all derivations of a string) and allsums (the weight of all derivations). From these, we also immediately obtain stringsum and allsum algorithms for TAG, LIG, PAA, and EPDA. For LIG, our algorithm is more time-efficient by a factor of 𝒪(n|𝒩|) (where n is the string length and |𝒩| is the size of the nonterminal set) and more space-efficient by a factor of 𝒪(|𝛤|) (where 𝛤 is the size of the stack alphabet) than the algorithm of Vijay-Shanker and Weir (1989). For EPDA, our algorithm is both more space-efficient and time-efficient than the algorithm of Alonso et al. (2001) by factors of 𝒪(|𝛤|2) and 𝒪(|𝛤|3), respectively. Finally, we give the first PAA stringsum and allsum algorithms",
    "checked": true,
    "id": "215bf37dbc12cb950ed6ee0e24b730ee1521f0f3",
    "semantic_title": "efficient algorithms for recognizing weighted tree-adjoining languages",
    "citation_count": 0,
    "authors": [
      "Alexandra Butoi",
      "Tim Vieira",
      "Ryan Cotterell",
      "David Chiang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.329": {
    "title": "Harnessing Black-Box Control to Boost Commonsense in LM's Generation",
    "volume": "main",
    "abstract": "Large language models (LLMs) such as GPT-3 have demonstrated a strong capability to generate coherent and contextually relevant text. However, amidst their successes, a crucial issue persists: their generated outputs still lack commonsense at times. Moreover, fine-tuning the entire LLM towards more commonsensical outputs is computationally expensive if not infeasible. In this paper, we present a computation-efficient framework that steers a frozen Pre-Trained Language Model (PTLM) towards more commonsensical generation (i.e., producing a plausible output that incorporates a list of concepts in a meaningful way). Specifically, we first construct a reference-free evaluator that assigns a sentence with a commonsensical score by grounding the sentence to a dynamic commonsense knowledge base from four different relational aspects. We then use the scorer as the oracle for commonsense knowledge, and extend the controllable generation method called NADO to train an auxiliary head that guides a fixed PTLM to better satisfy the oracle. We test our framework on a series of GPT-2-, Flan-T5-, and Alpaca-based language models (LMs) on two constrained concept-to-sentence benchmarks. Human evaluation results demonstrate that our method consistently leads to the most commonsensical outputs",
    "checked": false,
    "id": "5041d17836942757be4ce2c66152123742f4227a",
    "semantic_title": "boost: harnessing black-box control to boost commonsense in lms' generation",
    "citation_count": 0,
    "authors": [
      "Yufei Tian",
      "Felix Zhang",
      "Nanyun Peng"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.330": {
    "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback",
    "volume": "main",
    "abstract": "A trustworthy real-world prediction system should produce well-calibrated confidence scores; that is, its confidence in an answer should be indicative of the likelihood that the answer is correct, enabling deferral to an expert in cases of low-confidence predictions. Recent studies have shown that unsupervised pre-training produces large language models (LMs) whose conditional probabilities are remarkably well-calibrated. However, the most widely-used LMs are fine-tuned with reinforcement learning from human feedback (RLHF-LMs), and some studies have suggested that RLHF-LMs produce conditional probabilities that are very poorly calibrated. In light of this perceived weakness, we conduct a broad evaluation of methods for extracting confidence scores from RLHF-LMs. For RLHF-LMs such as ChatGPT, GPT-4, and Claude, we find that verbalized confidences emitted as output tokens are typically better-calibrated than the model's conditional probabilities on the TriviaQA, SciQ, and TruthfulQA benchmarks, often reducing the expected calibration error by a relative 50%",
    "checked": true,
    "id": "ab4ce5dda7ad4d9032995c9c049a89d65723c6aa",
    "semantic_title": "just ask for calibration: strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback",
    "citation_count": 29,
    "authors": [
      "Katherine Tian",
      "Eric Mitchell",
      "Allan Zhou",
      "Archit Sharma",
      "Rafael Rafailov",
      "Huaxiu Yao",
      "Chelsea Finn",
      "Christopher Manning"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.331": {
    "title": "Representative Demonstration Selection for In-Context Learning with Two-Stage Determinantal Point Process",
    "volume": "main",
    "abstract": "Although In-Context Learning has proven effective across a broad array of tasks, its efficiency is noticeably influenced by the selection of demonstrations. Existing methods tend to select different demonstrations for each test instance, which is time-consuming and poses limitations in practical scenarios. Therefore, this study aims to address the challenge of selecting a representative subset of in-context demonstrations that can effectively prompt different test instances in a specific task. We propose that this representative subset should be of high quality and diversity. Our empirical analyses confirm that demonstrations that meet these criteria can indeed bolster model performance. To satisfy these criteria, this paper further introduces a two-stage Determinantal Point Process (DPP) method designed to incorporate both quality and diversity in the process of demonstration selection, thereby obtaining representative in-context demonstrations. Through comprehensive experimentation, we have confirmed the efficacy of our proposed method, paving the way for more practical and effective In-Context Learning",
    "checked": true,
    "id": "d86b4712c1c64af64be5e693e796081c1a90e751",
    "semantic_title": "representative demonstration selection for in-context learning with two-stage determinantal point process",
    "citation_count": 0,
    "authors": [
      "Zhao Yang",
      "Yuanzhe Zhang",
      "Dianbo Sui",
      "Cao Liu",
      "Jun Zhao",
      "Kang Liu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.332": {
    "title": "The Effect of Scaling, Retrieval Augmentation and Form on the Factual Consistency of Language Models",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) make natural interfaces to factual knowledge, but their usefulness is limited by their tendency to deliver inconsistent answers to semantically equivalent questions. For example, a model might supply the answer \"Edinburgh\" to \"Anne Redpath passed away in X.\" and \"London\" to \"Anne Redpath's life ended in X.\" In this work, we identify potential causes of inconsistency and evaluate the effectiveness of two mitigation strategies: up-scaling and augmenting the LM with a passage retrieval database. Our results on the LLaMA and Atlas models show that both strategies reduce inconsistency but that retrieval augmentation is considerably more efficient. We further consider and disentangle the consistency contributions of different components of Atlas. For all LMs evaluated we find that syntactical form and task artifacts impact consistency. Taken together, our results provide a better understanding of the factors affecting the factual consistency of language models",
    "checked": true,
    "id": "d998ff93b2e58c9b219a9103f8c2ad714a41e4b9",
    "semantic_title": "the effect of scaling, retrieval augmentation and form on the factual consistency of language models",
    "citation_count": 0,
    "authors": [
      "Lovisa Hagström",
      "Denitsa Saynova",
      "Tobias Norlund",
      "Moa Johansson",
      "Richard Johansson"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.333": {
    "title": "ViPE: Visualise Pretty-much Everything",
    "volume": "main",
    "abstract": "Figurative and non-literal expressions are profoundly integrated in human communication. Visualising such expressions allow us to convey our creative thoughts, and evoke nuanced emotions. Recent text-to-image models like Stable Diffusion, on the other hand, struggle to depict non-literal expressions. Recent works primarily deal with this issue by compiling humanly annotated datasets on a small scale, which not only demands specialized expertise but also proves highly inefficient. To address this issue, we introduce ViPE: Visualise Pretty-much Everything. ViPE offers a series of lightweight and robust language models that have been trained on a large-scale set of lyrics with noisy visual descriptions that represent their implicit meaning. The synthetic visual descriptions are generated by GPT3.5 relying on neither human annotations nor images. ViPE effectively expresses any arbitrary piece of text into a visualisable description, enabling meaningful and high-quality image generation. We provide compelling evidence that ViPE is more robust than GPT3.5 in synthesising visual elaborations. ViPE also exhibits an understanding of figurative expressions comparable to human experts, providing a powerful and open-source backbone to many downstream applications such as music video and caption generation",
    "checked": true,
    "id": "d85d49d4429a63244286f7def1ec5c09490d6cc7",
    "semantic_title": "vipe: visualise pretty-much everything",
    "citation_count": 0,
    "authors": [
      "Hassan Shahmohammadi",
      "Adhiraj Ghosh",
      "Hendrik Lensch"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.334": {
    "title": "Semi-automatic Data Enhancement for Document-Level Relation Extraction with Distant Supervision from Large Language Models",
    "volume": "main",
    "abstract": "Document-level Relation Extraction (DocRE), which aims to extract relations from a long context, is a critical challenge in achieving fine-grained structural comprehension and generating interpretable document representations. Inspired by recent advances in in-context learning capabilities emergent from large language models (LLMs), such as ChatGPT, we aim to design an automated annotation method for DocRE with minimum human effort. Unfortunately, vanilla in-context learning is infeasible for DocRE due to the plenty of predefined fine-grained relation types and the uncontrolled generations of LLMs. To tackle this issue, we propose a method integrating an LLM and a natural language inference (NLI) module to generate relation triples, thereby augmenting document-level relation datasets. We demonstrate the effectiveness of our approach by introducing an enhanced dataset known as DocGNRE, which excels in re-annotating numerous long-tail relation types. We are confident that our method holds the potential for broader applications in domain-specific relation type definitions and offers tangible benefits in advancing generalized language semantic comprehension",
    "checked": true,
    "id": "159712301120bbc88f90848260d2bf846ba97203",
    "semantic_title": "semi-automatic data enhancement for document-level relation extraction with distant supervision from large language models",
    "citation_count": 0,
    "authors": [
      "Junpeng Li",
      "Zixia Jia",
      "Zilong Zheng"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.335": {
    "title": "Navigating the Grey Area: How Expressions of Uncertainty and Overconfidence Affect Language Models",
    "volume": "main",
    "abstract": "The increased deployment of LMs for real-world tasks involving knowledge and facts makes it important to understand model epistemology: what LMs think they know, and how their attitudes toward that knowledge are affected by language use in their inputs. Here, we study an aspect of model epistemology: how epistemic markers of certainty, uncertainty, or evidentiality like \"I'm sure it's\", \"I think it's\", or \"Wikipedia says it's\" affect models, and whether they contribute to model failures. We develop a typology of epistemic markers and inject 50 markers into prompts for question answering. We find that LMs are highly sensitive to epistemic markers in prompts, with accuracies varying more than 80%. Surprisingly, we find that expressions of high certainty result in a 7% decrease in accuracy as compared to low certainty expressions; similarly, factive verbs hurt performance, while evidentials benefit performance. Our analysis of a popular pretraining dataset shows that these markers of uncertainty are associated with answers on question-answering websites, while markers of certainty are associated with questions. These associations may suggest that the behavior of LMs is based on mimicking observed language use, rather than truly reflecting epistemic uncertainty",
    "checked": true,
    "id": "217e436fd23fe4184828e02a2b143835d6fd3b28",
    "semantic_title": "navigating the grey area: how expressions of uncertainty and overconfidence affect language models",
    "citation_count": 0,
    "authors": [
      "Kaitlyn Zhou",
      "Dan Jurafsky",
      "Tatsunori Hashimoto"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.336": {
    "title": "Elaborative Simplification as Implicit Questions Under Discussion",
    "volume": "main",
    "abstract": "Automated text simplification, a technique useful for making text more accessible to people such as children and emergent bilinguals, is often thought of as a monolingual translation task from complex sentences to simplified sentences using encoder-decoder models. This view fails to account for elaborative simplification, where new information is added into the simplified text. This paper proposes to view elaborative simplification through the lens of the Question Under Discussion (QUD) framework, providing a robust way to investigate what writers elaborate upon, how they elaborate, and how elaborations fit into the discourse context by viewing elaborations as explicit answers to implicit questions. We introduce ELABQUD, consisting of 1.3K elaborations accompanied with implicit QUDs, to study these phenomena. We show that explicitly modeling QUD (via question generation) not only provides essential understanding of elaborative simplification and how the elaborations connect with the rest of the discourse, but also substantially improves the quality of elaboration generation",
    "checked": true,
    "id": "c3940196dd7f08063d0ea0c1b905ab0dbf5285ab",
    "semantic_title": "elaborative simplification as implicit questions under discussion",
    "citation_count": 4,
    "authors": [
      "Yating Wu",
      "William Sheffield",
      "Kyle Mahowald",
      "Junyi Li"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.337": {
    "title": "EntSUMv2: Dataset, Models and Evaluation for More Abstractive Entity-Centric Summarization",
    "volume": "main",
    "abstract": "Entity-centric summarization is a form of controllable summarization that aims to generate a summary for a specific entity given a document. Concise summaries are valuable in various real-life applications, as they enable users to quickly grasp the main points of the document focusing on an entity of interest. This paper presents ENTSUMV2, a more abstractive version of the original entity-centric ENTSUM summarization dataset. In ENTSUMV2 the annotated summaries are intentionally made shorter to benefit more specific and useful entity-centric summaries for downstream users. We conduct extensive experiments on this dataset using multiple abstractive summarization approaches that employ supervised fine-tuning or large-scale instruction tuning. Additionally, we perform comprehensive human evaluation that incorporates metrics for measuring crucial facets. These metrics provide a more fine-grained interpretation of the current state-of-the-art systems and highlight areas for future improvement",
    "checked": true,
    "id": "37c4492a5932bbb6f5bdc2ce4a944ed8a51d3287",
    "semantic_title": "entsumv2: dataset, models and evaluation for more abstractive entity-centric summarization",
    "citation_count": 0,
    "authors": [
      "Dhruv Mehra",
      "Lingjue Xie",
      "Ella Hofmann-Coyle",
      "Mayank Kulkarni",
      "Daniel Preotiuc-Pietro"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.338": {
    "title": "SciRepEval: A Multi-Format Benchmark for Scientific Document Representations",
    "volume": "main",
    "abstract": "Learned representations of scientific documents can serve as valuable input features for downstream tasks without further fine-tuning. However, existing benchmarks for evaluating these representations fail to capture the diversity of relevant tasks. In response, we introduce SciRepEval, the first comprehensive benchmark for training and evaluating scientific document representations. It includes 24 challenging and realistic tasks, 8 of which are new, across four formats: classification, regression, ranking and search. We then use this benchmark to study and improve the generalization ability of scientific document representation models. We show how state-of-the-art models like SPECTER and SciNCL struggle to generalize across the task formats, and that simple multi-task training fails to improve them. However, a new approach that learns multiple embeddings per document, each tailored to a different format, can improve performance. We experiment with task-format-specific control codes and adapters and find they outperform the existing single-embedding state-of-the-art by over 2 points absolute. We release the resulting family of multi-format models, called SPECTER2, for the community to use and build on",
    "checked": true,
    "id": "0d68e244faa9a5944228b571d91bc7bed88083cd",
    "semantic_title": "scirepeval: a multi-format benchmark for scientific document representations",
    "citation_count": 16,
    "authors": [
      "Amanpreet Singh",
      "Mike D’Arcy",
      "Arman Cohan",
      "Doug Downey",
      "Sergey Feldman"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.339": {
    "title": "A Diachronic Perspective on User Trust in AI under Uncertainty",
    "volume": "main",
    "abstract": "In human-AI collaboration, users typically form a mental model of the AI system, which captures the user's beliefs about when the system performs well and when it does not. The construction of this mental model is guided by both the system's veracity as well as the system output presented to the user e.g., the system's confidence and an explanation for the prediction. However, modern NLP systems are seldom calibrated and are often confidently incorrect about their predictions, which violates users' mental model and erodes their trust. In this work, we design a study where users bet on the correctness of an NLP system, and use it to study the evolution of user trust as a response to these trust-eroding events and how the user trust is rebuilt as a function of time after these events. We find that even a few highly inaccurate confidence estimation instances are enough to damage users' trust in the system and performance, which does not easily recover over time. We further find that users are more forgiving to the NLP system if it is unconfidently correct rather than confidently incorrect, even though, from a game-theoretic perspective, their payoff is equivalent. Finally, we find that each user can entertain multiple mental models of the system based on the type of the question. These results highlight the importance of confidence calibration in developing user-centered NLP applications to avoid damaging user trust and compromising the collaboration performance",
    "checked": true,
    "id": "34ab87a0d5402fe299a9feeda19cd2f50c085bdf",
    "semantic_title": "a diachronic perspective on user trust in ai under uncertainty",
    "citation_count": 0,
    "authors": [
      "Shehzaad Dhuliawala",
      "Vilém Zouhar",
      "Mennatallah El-Assady",
      "Mrinmaya Sachan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.340": {
    "title": "CT-GAT: Cross-Task Generative Adversarial Attack based on Transferability",
    "volume": "main",
    "abstract": "Neural network models are vulnerable to adversarial examples, and adversarial transferability further increases the risk of adversarial attacks. Current methods based on transferability often rely on substitute models, which can be impractical and costly in real-world scenarios due to the unavailability of training data and the victim model's structural details. In this paper, we propose a novel approach that directly constructs adversarial examples by extracting transferable features across various tasks. Our key insight is that adversarial transferability can extend across different tasks. Specifically, we train a sequence-to-sequence generative model named CT-GAT (Cross-Task Generative Adversarial Attack) using adversarial sample data collected from multiple tasks to acquire universal adversarial features and generate adversarial examples for different tasks.We conduct experiments on ten distinct datasets, and the results demonstrate that our method achieves superior attack performance with small cost",
    "checked": true,
    "id": "8f48071cf937ba6c44c4b18ea0df05f3f3444cf5",
    "semantic_title": "ct-gat: cross-task generative adversarial attack based on transferability",
    "citation_count": 0,
    "authors": [
      "Minxuan Lv",
      "Chengwei Dai",
      "Kun Li",
      "Wei Zhou",
      "Songlin Hu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.341": {
    "title": "Improving Long Document Topic Segmentation Models With Enhanced Coherence Modeling",
    "volume": "main",
    "abstract": "Topic segmentation is critical for obtaining structured documents and improving down- stream tasks such as information retrieval. Due to its ability of automatically exploring clues of topic shift from abundant labeled data, recent supervised neural models have greatly promoted the development of long document topic segmentation, but leaving the deeper relationship between coherence and topic segmentation underexplored. Therefore, this paper enhances the ability of supervised models to capture coherence from both logical structure and semantic similarity perspectives to further improve the topic segmentation performance, proposing Topic-aware Sentence Structure Prediction (TSSP) and Contrastive Semantic Similarity Learning (CSSL). Specifically, the TSSP task is proposed to force the model to comprehend structural information by learning the original relations between adjacent sentences in a disarrayed document, which is constructed by jointly disrupting the original document at topic and sentence levels. Moreover, we utilize inter- and intra-topic information to construct contrastive samples and design the CSSL objective to ensure that the sentences representations in the same topic have higher similarity, while those in different topics are less similar. Extensive experiments show that the Longformer with our approach significantly outperforms old state-of-the-art (SOTA) methods. Our approach improve F1 of old SOTA by 3.42 (73.74 → 77.16) and reduces Pk by 1.11 points (15.0 → 13.89) on WIKI-727K and achieves an average relative reduction of 4.3% on Pk on WikiSection. The average relative Pk drop of 8.38% on two out-of-domain datasets also demonstrates the robustness of our approach",
    "checked": true,
    "id": "6cae6fb9ae36f0a0bfe6ab9634bd0dd409711654",
    "semantic_title": "improving long document topic segmentation models with enhanced coherence modeling",
    "citation_count": 0,
    "authors": [
      "Hai Yu",
      "Chong Deng",
      "Qinglin Zhang",
      "Jiaqing Liu",
      "Qian Chen",
      "Wen Wang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.342": {
    "title": "Dialogue Chain-of-Thought Distillation for Commonsense-aware Conversational Agents",
    "volume": "main",
    "abstract": "Human-like chatbots necessitate the use of commonsense reasoning in order to effectively comprehend and respond to implicit information present within conversations. Achieving such coherence and informativeness in responses, however, is a non-trivial task. Even for large language models (LLMs), the task of identifying and aggregating key evidence within a single hop presents a substantial challenge. This complexity arises because such evidence is scattered across multiple turns in a conversation, thus necessitating integration over multiple hops. Hence, our focus is to facilitate such multi-hop reasoning over a dialogue context, namely dialogue chain-of-thought (CoT) reasoning. To this end, we propose a knowledge distillation framework that leverages LLMs as unreliable teachers and selectively distills consistent and helpful rationales via alignment filters. We further present DOCTOR, a DialOgue Chain-of-ThOught Reasoner that provides reliable CoT rationales for response generation. We conduct extensive experiments to show that enhancing dialogue agents with high-quality rationales from DOCTOR significantly improves the quality of their responses",
    "checked": true,
    "id": "13244fdc42a091f87bc08eaaac2bcfd5883e8d0c",
    "semantic_title": "dialogue chain-of-thought distillation for commonsense-aware conversational agents",
    "citation_count": 1,
    "authors": [
      "Hyungjoo Chae",
      "Yongho Song",
      "Kai Ong",
      "Taeyoon Kwon",
      "Minjin Kim",
      "Youngjae Yu",
      "Dongha Lee",
      "Dongyeop Kang",
      "Jinyoung Yeo"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.343": {
    "title": "Information Value: Measuring Utterance Predictability as Distance from Plausible Alternatives",
    "volume": "main",
    "abstract": "We present information value, a measure which quantifies the predictability of an utterance relative to a set of plausible alternatives. We introduce a method to obtain interpretable estimates of information value using neural text generators, and exploit their psychometric predictive power to investigate the dimensions of predictability that drive human comprehension behaviour. Information value is a stronger predictor of utterance acceptability in written and spoken dialogue than aggregates of token-level surprisal and it is complementary to surprisal for predicting eye-tracked reading times",
    "checked": true,
    "id": "b3befd1ef00a45a028dea3a10264d5f8e4306d17",
    "semantic_title": "information value: measuring utterance predictability as distance from plausible alternatives",
    "citation_count": 1,
    "authors": [
      "Mario Giulianelli",
      "Sarenne Wallbridge",
      "Raquel Fernández"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.344": {
    "title": "Generating Commonsense Counterfactuals for Stable Relation Extraction",
    "volume": "main",
    "abstract": "Recent studies on counterfactual augmented data have achieved great success in the coarse-grained natural language processing tasks. However, existing methods encounter two major problems when dealing with the fine-grained relation extraction tasks. One is that they struggle to accurately identify causal terms under the invariant entity constraint. The other is that they ignore the commonsense constraint. To solve these problems, we propose a novel framework to generate commonsense counterfactuals for stable relation extraction. Specifically, to identify causal terms accurately, we introduce an intervention-based strategy and leverage a constituency parser for correction. To satisfy the commonsense constraint, we introduce the concept knowledge base WordNet and design a bottom-up relation expansion algorithm on it to uncover commonsense relations between entities. We conduct a series of comprehensive evaluations, including the low-resource, out-of-domain, and adversarial-attack settings. The results demonstrate that our framework significantly enhances the stability of base relation extraction models",
    "checked": true,
    "id": "55e67fb0bf20606af9bb931118c155383fc80c20",
    "semantic_title": "generating commonsense counterfactuals for stable relation extraction",
    "citation_count": 0,
    "authors": [
      "Xin Miao",
      "Yongqi Li",
      "Tieyun Qian"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.345": {
    "title": "C-STS: Conditional Semantic Textual Similarity",
    "volume": "main",
    "abstract": "Semantic textual similarity (STS) has been a cornerstone task in NLP that measures the degree of similarity between a pair of sentences, with applications in information retrieval, question answering, and embedding methods. However, it is an inherently ambiguous task, with the sentence similarity depending on the specific aspect of interest. We resolve this ambiguity by proposing a novel task called conditional STS (C-STS) which measures similarity conditioned on an aspect elucidated in natural language (hereon, condition). As an example, the similarity between the sentences \"The NBA player shoots a three-pointer.\" and \"A man throws a tennis ball into the air to serve.\" is higher for the condition \"The motion of the ball.\" (both upward) and lower for \"The size of the ball.\" (one large and one small). C-STS's advantages are two-fold: (1) it reduces the subjectivity and ambiguity of STS, and (2) enables fine-grained similarity evaluation using diverse conditions. C-STS contains almost 20,000 instances from diverse domains and we evaluate several state-of-the-art models to demonstrate that even the most performant fine-tuning and in-context learning models (GPT-4, Flan, SimCSE) find it challenging, with Spearman correlation scores of <50. We encourage the community to evaluate their models on C-STS to provide a more holistic view of semantic similarity and natural language understanding",
    "checked": false,
    "id": "b036a77b8c2667dfaf967c09d2662f05fbb5ebaa",
    "semantic_title": "csts: conditional semantic textual similarity",
    "citation_count": 0,
    "authors": [
      "Ameet Deshpande",
      "Carlos Jimenez",
      "Howard Chen",
      "Vishvak Murahari",
      "Victoria Graf",
      "Tanmay Rajpurohit",
      "Ashwin Kalyan",
      "Danqi Chen",
      "Karthik Narasimhan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.346": {
    "title": "Cross-lingual Transfer Can Worsen Bias in Sentiment Analysis",
    "volume": "main",
    "abstract": "Sentiment analysis (SA) systems are widely deployed in many of the world's languages, and there is well-documented evidence of demographic bias in these systems. In languages beyond English, scarcer training data is often supplemented with transfer learning using pre-trained models, including multilingual models trained on other languages. In some cases, even supervision data comes from other languages. Does cross-lingual transfer also import new biases? To answer this question, we use counterfactual evaluation to test whether gender or racial biases are imported when using cross-lingual transfer, compared to a monolingual transfer setting. Across five languages, we find that systems using cross-lingual transfer usually become more biased than their monolingual counterparts. We also find racial biases to be much more prevalent than gender biases. To spur further research on this topic, we release the sentiment models we used for this study, and the intermediate checkpoints throughout training, yielding 1,525 distinct models; we also release our evaluation code",
    "checked": true,
    "id": "0d106c5b14ce468298ee67310edc5e389c64bcdc",
    "semantic_title": "cross-lingual transfer can worsen bias in sentiment analysis",
    "citation_count": 0,
    "authors": [
      "Seraphina Goldfarb-Tarrant",
      "Björn Ross",
      "Adam Lopez"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.347": {
    "title": "Rumor Detection on Social Media with Crowd Intelligence and ChatGPT-Assisted Networks",
    "volume": "main",
    "abstract": "In the era of widespread dissemination through social media, the task of rumor detection plays a pivotal role in establishing a trustworthy and reliable information environment. Nonetheless, existing research on rumor detection confronts several challenges: the limited expressive power of text encoding sequences, difficulties in domain knowledge coverage and effective information extraction with knowledge graph-based methods, and insufficient mining of semantic structural information. To address these issues, we propose a Crowd Intelligence and ChatGPT-Assisted Network(CICAN) for rumor classification. Specifically, we present a crowd intelligence-based semantic feature learning module to capture textual content's sequential and hierarchical features. Then, we design a knowledge-based semantic structural mining module that leverages ChatGPT for knowledge enhancement. Finally, we construct an entity-sentence heterogeneous graph and design Entity-Aware Heterogeneous Attention to effectively integrate diverse structural information meta-paths. Experimental results demonstrate that CICAN achieves performance improvement in rumor detection tasks, validating the effectiveness and rationality of using large language models as auxiliary tools",
    "checked": true,
    "id": "d9610702dac020a6cedb1e90804207046383aca5",
    "semantic_title": "rumor detection on social media with crowd intelligence and chatgpt-assisted networks",
    "citation_count": 0,
    "authors": [
      "Chang Yang",
      "Peng Zhang",
      "Wenbo Qiao",
      "Hui Gao",
      "Jiaming Zhao"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.348": {
    "title": "Grounding Visual Illusions in Language: Do Vision-Language Models Perceive Illusions Like Humans?",
    "volume": "main",
    "abstract": "Vision-Language Models (VLMs) are trained on vast amounts of data captured by humans emulating our understanding of the world. However, known as visual illusions, human's perception of reality isn't always faithful to the physical world. This raises a key question: do VLMs have the similar kind of illusions as humans do, or do they faithfully learn to represent reality? To investigate this question, we build a dataset containing five types of visual illusions and formulate four tasks to examine visual illusions in state-of-the-art VLMs. Our findings have shown that although the overall alignment is low, larger models are closer to human perception and more susceptible to visual illusions. Our dataset and initial findings will promote a better understanding of visual illusions in humans and machines and provide a stepping stone for future computational models that can better align humans and machines in perceiving and communicating about the shared visual world. The code and data are available at [github.com/vl-illusion/dataset](https://github.com/vl-illusion/dataset)",
    "checked": true,
    "id": "85b292a8e5c0c297ed5262c80b521b83aa0555ac",
    "semantic_title": "grounding visual illusions in language: do vision-language models perceive illusions like humans?",
    "citation_count": 1,
    "authors": [
      "Yichi Zhang",
      "Jiayi Pan",
      "Yuchen Zhou",
      "Rui Pan",
      "Joyce Chai"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.349": {
    "title": "Analysing State-Backed Propaganda Websites: a New Dataset and Linguistic Study",
    "volume": "main",
    "abstract": "This paper analyses two hitherto unstudied sites sharing state-backed disinformation, Reliable Recent News (rrn.world) and WarOnFakes (waronfakes.com), which publish content in Arabic, Chinese, English, French, German, and Spanish. We describe our content acquisition methodology and perform cross-site unsupervised topic clustering on the resulting multilingual dataset. We also perform linguistic and temporal analysis of the web page translations and topics over time, and investigate articles with false publication dates. We make publicly available this new dataset of 14,053 articles, annotated with each language version, and additional metadata such as links and images. The main contribution of this paper for the NLP community is in the novel dataset which enables studies of disinformation networks, and the training of NLP tools for disinformation detection",
    "checked": true,
    "id": "325b9de5eeca6043220aebb543c5c548626e2a7e",
    "semantic_title": "analysing state-backed propaganda websites: a new dataset and linguistic study",
    "citation_count": 0,
    "authors": [
      "Freddy Heppell",
      "Kalina Bontcheva",
      "Carolina Scarton"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.350": {
    "title": "Controllable Contrastive Generation for Multilingual Biomedical Entity Linking",
    "volume": "main",
    "abstract": "Multilingual biomedical entity linking (MBEL) aims to map language-specific mentions in the biomedical text to standardized concepts in a multilingual knowledge base (KB) such as Unified Medical Language System (UMLS). In this paper, we propose Con2GEN, a prompt-based controllable contrastive generation framework for MBEL, which summarizes multidimensional information of the UMLS concept mentioned in biomedical text into a natural sentence following a predefined template. Instead of tackling the MBEL problem with a discriminative classifier, we formulate it as a sequence-to-sequence generation task, which better exploits the shared dependencies between source mentions and target entities. Moreover, Con2GEN matches against UMLS concepts in as many languages and types as possible, hence facilitating cross-information disambiguation. Extensive experiments show that our model achieves promising performance improvements compared with several state-of-the-art techniques on the XL-BEL and the Mantra GSC datasets spanning 12 typologically diverse languages",
    "checked": true,
    "id": "b40546a13483c141c33315d0ac295a695b33fabb",
    "semantic_title": "controllable contrastive generation for multilingual biomedical entity linking",
    "citation_count": 0,
    "authors": [
      "Tiantian Zhu",
      "Yang Qin",
      "Qingcai Chen",
      "Xin Mu",
      "Changlong Yu",
      "Yang Xiang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.351": {
    "title": "HyperRouter: Towards Efficient Training and Inference of Sparse Mixture of Experts",
    "volume": "main",
    "abstract": "By routing input tokens to only a few split experts, Sparse Mixture-of-Experts has enabled efficient training of large language models. Recent findings suggest that fixing the routers can achieve competitive performance by alleviating the collapsing problem, where all experts eventually learn similar representations. However, this strategy has two key limitations: (i) the policy derived from random routers might be sub-optimal, and (ii) it requires extensive resources during training and evaluation, leading to limited efficiency gains. This work introduces HyperRouter, which dynamically generates the router's parameters through a fixed hypernetwork and trainable embeddings to achieve a balance between training the routers and freezing them to learn an improved routing policy. Extensive experiments across a wide range of tasks demonstrate the superior performance and efficiency gains of HyperRouter compared to existing routing methods. Our implementation is publicly available at https://github.com/giangdip2410/HyperRouter",
    "checked": true,
    "id": "700b64470dda422e07d75f0a379a83039272396a",
    "semantic_title": "hyperrouter: towards efficient training and inference of sparse mixture of experts",
    "citation_count": 1,
    "authors": [
      "Truong Do",
      "Le Khiem",
      "Quang Pham",
      "TrungTin Nguyen",
      "Thanh-Nam Doan",
      "Binh Nguyen",
      "Chenghao Liu",
      "Savitha Ramasamy",
      "Xiaoli Li",
      "Steven Hoi"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.352": {
    "title": "MediaHG: Rethinking Eye-catchy Features in Social Media Headline Generation",
    "volume": "main",
    "abstract": "An attractive blog headline on social media platforms can immediately grab readers and trigger more clicks. However, a good headline shall not only contract the main content but also be eye-catchy with domain platform features, which are decided by the website's users and objectives. With effective headlines, bloggers can obtain more site traffic and profits, while readers can have easier access to topics of interest. In this paper, we propose a disentanglement-based headline generation model: MediaHG (Social Media Headline Generation), which can balance the content and contextual features. Specifically, we first devise a sample module for various document views and generate the corresponding headline candidates. Then, we incorporate contrastive learning and auxiliary multi-task to choose the best domain-suitable headline, according to the disentangled budgets. Besides, our separated processing gains more flexible adaptation for other headline generation tasks with special domain features. Our model is built from the content and headlines of 70k hot posts collected from REDBook, a Chinese social media platform for daily sharing. Experimental results with language metrics ROUGE and human evaluation show the improvement in the headline generation task for the platform",
    "checked": true,
    "id": "d34f006b56738ac49d11918f2aff14d3162ffce0",
    "semantic_title": "mediahg: rethinking eye-catchy features in social media headline generation",
    "citation_count": 0,
    "authors": [
      "Boning Zhang",
      "Yang Yang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.353": {
    "title": "Fine-tuned LLMs Know More, Hallucinate Less with Few-Shot Sequence-to-Sequence Semantic Parsing over Wikidata",
    "volume": "main",
    "abstract": "While large language models (LLMs) can answer many questions correctly, they can also hallucinate and give wrong answers. Wikidata, with its over 12 billion facts, can be used to ground LLMs to improve their factuality. This paper presents WikiWebQuestions, a high-quality question answering benchmark for Wikidata. Ported over from WebQuestions for Freebase, it consists of real-world data with SPARQL annotation. This paper presents a few-shot sequence-to-sequence semantic parser for Wikidata. We modify SPARQL to use the unique domain and property names instead of their IDs. We train the parser to use either the results from an entity linker or mentions in the query. We fine-tune LLaMA by adding the few-shot training data to that used to fine-tune Alpaca. Our experimental results demonstrate the effectiveness of this methodology, establishing a strong baseline of 76% and 65% answer accuracy in the dev and test sets of WikiWebQuestions, respectively. By pairing our semantic parser with GPT-3, we combine verifiable results with qualified GPT-3 guesses to provide useful answers to 96% of the questions in dev. We also show that our method outperforms the state-of-the-art for the QALD-7 Wikidata dataset by 3.6% in F1 score",
    "checked": true,
    "id": "ac4fc2bac4b8c4e2d0d18b1517c90549ae13d030",
    "semantic_title": "fine-tuned llms know more, hallucinate less with few-shot sequence-to-sequence semantic parsing over wikidata",
    "citation_count": 0,
    "authors": [
      "Silei Xu",
      "Shicheng Liu",
      "Theo Culhane",
      "Elizaveta Pertseva",
      "Meng-Hsi Wu",
      "Sina Semnani",
      "Monica Lam"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.354": {
    "title": "ZEROTOP: Zero-Shot Task-Oriented Semantic Parsing using Large Language Models",
    "volume": "main",
    "abstract": "We explore the use of large language models (LLMs) for zero-shot semantic parsing. Semantic parsing involves mapping natural language utterances to task-specific meaning representations. LLMs are generally trained on publicly available text and code and cannot be expected to directly generalize to domain-specific parsing tasks in a zero-shot setting. In this work, we propose ZEROTOP, a zero-shot task-oriented parsing method that decomposes semantic parsing problem into a set of abstractive and extractive question-answering (QA) problems. For each utterance, we prompt the LLM with questions corresponding to its top-level intent and a set of slots and use the LLM generations to construct the target meaning representation. We observe that current LLMs fail to detect unanswerable questions; and as a result, cannot handle questions corresponding to missing slots. We address this by fine-tuning a language model on public QA datasets using synthetic negative samples. Experimental results show that our QA-based decomposition paired with the fine-tuned LLM can zero-shot parse ≈ 16% of utterances in the MTOP dataset",
    "checked": true,
    "id": "b8d06dd769f89d08bdd9997d7bd363c89ede845b",
    "semantic_title": "zerotop: zero-shot task-oriented semantic parsing using large language models",
    "citation_count": 1,
    "authors": [
      "Dheeraj Mekala",
      "Jason Wolfe",
      "Subhro Roy"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.355": {
    "title": "Efficient Grammatical Error Correction Via Multi-Task Training and Optimized Training Schedule",
    "volume": "main",
    "abstract": "Progress in neural grammatical error correction (GEC) is hindered by the lack of annotated training data. Sufficient amounts of high-quality manually annotated data are not available, so recent research has relied on generating synthetic data, pretraining on it, and then fine-tuning on real datasets; performance gains have been achieved either by ensembling or by using huge pretrained models such as XXL-T5 as the backbone. In this work, we explore an orthogonal direction: how to use available data more efficiently. First, we propose auxiliary tasks that exploit the alignment between the original and corrected sentences, such as predicting a sequence of corrections. We formulate each task as a sequence-to-sequence problem and perform multi-task training. Second, we discover that the order of datasets used for training and even individual instances within a dataset may have important effects on the final performance, so we set out to find the best training schedule. Together, these two ideas lead to significant improvements, producing results that improve state of the art with much smaller models; in particular, we outperform the best models based on T5-XXL (11B parameters) with a BART-based model (400M parameters)",
    "checked": true,
    "id": "423012573f1c83d72fd9118b39362836257845a9",
    "semantic_title": "efficient grammatical error correction via multi-task training and optimized training schedule",
    "citation_count": 0,
    "authors": [
      "Andrey Bout",
      "Alexander Podolskiy",
      "Sergey Nikolenko",
      "Irina Piontkovskaya"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.356": {
    "title": "The BLA Benchmark: Investigating Basic Language Abilities of Pre-Trained Multimodal Models",
    "volume": "main",
    "abstract": "Despite the impressive performance achieved by pre-trained language-and-vision models in downstream tasks, it remains an open question whether this reflects a proper understanding of image-text interaction. In this work, we explore to what extent they handle basic linguistic constructions—active-passive voice, coordination, and relative clauses—that even preschool children can typically master. We present BLA, a novel, automatically constructed benchmark to evaluate multimodal models on these Basic Language Abilities. We show that different types of Transformer-based systems, such as CLIP, ViLBERT, and BLIP2, generally struggle with BLA in a zero-shot setting, in line with previous findings. Our experiments, in particular, show that most of the tested models only marginally benefit when fine-tuned or prompted with construction-specific samples. Yet, the generative BLIP2 shows promising trends, especially in an in-context learning setting. This opens the door to using BLA not only as an evaluation benchmark but also to improve models' basic language abilities",
    "checked": true,
    "id": "588ee5b7f2441754cdf3f9adbbe50d03d9c21b7a",
    "semantic_title": "the bla benchmark: investigating basic language abilities of pre-trained multimodal models",
    "citation_count": 0,
    "authors": [
      "Xinyi Chen",
      "Raquel Fernández",
      "Sandro Pezzelle"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.357": {
    "title": "RainProof: An Umbrella to Shield Text Generator from Out-Of-Distribution Data",
    "volume": "main",
    "abstract": "Implementing effective control mechanisms to ensure the proper functioning and security of deployed NLP models, from translation to chatbots, is essential. A key ingredient to ensure safe system behaviour is Out-Of-Distribution (OOD) detection, which aims to detect whether an input sample is statistically far from the training distribution. Although OOD detection is a widely covered topic in classification tasks, most methods rely on hidden features output by the encoder. In this work, we focus on leveraging soft-probabilities in a black-box framework, i.e. we can access the soft-predictions but not the internal states of the model. Our contributions include: (i) RAINPROOF a Relative informAItioN Projection OOD detection framework; and (ii) a more operational evaluation setting for OOD detection. Surprisingly, we find that OOD detection is not necessarily aligned with task-specific measures. The OOD detector may filter out samples well processed by the model and keep samples that are not, leading to weaker performance. Our results show that RAINPROOF provides OOD detection methods more aligned with task-specific performance metrics than traditional OOD detectors",
    "checked": false,
    "id": "022ed9823e99d09eba284ca3457fb59cba3ebfbb",
    "semantic_title": "rainproof: an umbrella to shield text generators from out-of-distribution data",
    "citation_count": 4,
    "authors": [
      "Maxime Darrin",
      "Pablo Piantanida",
      "Pierre Colombo"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.358": {
    "title": "KEPL: Knowledge Enhanced Prompt Learning for Chinese Hypernym-Hyponym Extraction",
    "volume": "main",
    "abstract": "Modeling hypernym-hyponym (\"is-a\") relations is very important for many natural language processing (NLP) tasks, such as classification, natural language inference and relation extraction. Existing work on is-a relation extraction is mostly in the English language environment. Due to the flexibility of language expression and the lack of high-quality Chinese annotation datasets, it is still a challenge to accurately identify such relations from Chinese unstructured texts. To tackle this problem, we propose a Knowledge Enhanced Prompt Learning (KEPL) method for Chinese hypernym-hyponym relation extraction. Our model uses the Hearst-like patterns as the prior knowledge. By exploiting a Dynamic Adaptor Architecture to select the matching pattern for the text into prompt, our model embeds patterns and text simultaneously. Additionally, we construct a Chinese hypernym-hyponym relation extraction dataset, which contains three typical scenarios, as baike, news and We-media. The experimental results on the dataset demonstrate the efficiency and effectiveness of our proposed model",
    "checked": true,
    "id": "caa1f27e344281df9c02d57edde5855fe37e657c",
    "semantic_title": "kepl: knowledge enhanced prompt learning for chinese hypernym-hyponym extraction",
    "citation_count": 0,
    "authors": [
      "Ningchen Ma",
      "Dong Wang",
      "Hongyun Bao",
      "Lei He",
      "Suncong Zheng"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.359": {
    "title": "Ditto: A Simple and Efficient Approach to Improve Sentence Embeddings",
    "volume": "main",
    "abstract": "Prior studies diagnose the anisotropy problem in sentence representations from pre-trained language models, e.g., BERT, without fine-tuning. Our analysis reveals that the sentence embeddings from BERT suffer from a bias towards uninformative words, limiting the performance in semantic textual similarity (STS) tasks. To address this bias, we propose a simple and efficient unsupervised approach, Diagonal Attention Pooling (Ditto), which weights words with model-based importance estimations and computes the weighted average of word representations from pre-trained models as sentence embeddings. Ditto can be easily applied to any pre-trained language model as a postprocessing operation. Compared to prior sentence embedding approaches, Ditto does not add parameters nor requires any learning. Empirical evaluations demonstrate that our proposed Ditto can alleviate the anisotropy problem and improve various pre-trained models on the STS benchmarks",
    "checked": true,
    "id": "f4791b1fb662b76d3fa9576a0db26e720fef3806",
    "semantic_title": "ditto: a simple and efficient approach to improve sentence embeddings",
    "citation_count": 0,
    "authors": [
      "Qian Chen",
      "Wen Wang",
      "Qinglin Zhang",
      "Siqi Zheng",
      "Chong Deng",
      "Hai Yu",
      "Jiaqing Liu",
      "Yukun Ma",
      "Chong Zhang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.360": {
    "title": "Preserving Knowledge Invariance: Rethinking Robustness Evaluation of Open Information Extraction",
    "volume": "main",
    "abstract": "The robustness to distribution changes ensures that NLP models can be successfully applied in the realistic world, especially for information extraction tasks. However, most prior evaluation benchmarks have been devoted to validating pairwise matching correctness, ignoring the crucial validation of robustness. In this paper, we present the first benchmark that simulates the evaluation of open information extraction models in the real world, where the syntactic and expressive distributions under the same knowledge meaning may drift variously. We design and annotate a large-scale testbed in which each example is a knowledge-invariant clique that consists of sentences with structured knowledge of the same meaning but with different syntactic and expressive forms. By further elaborating the robustness metric, a model is judged to be robust if its performance is consistently accurate on the overall cliques. We perform experiments on typical models published in the last decade as well as a representative large language model, and the results show that the existing successful models exhibit a frustrating degradation, with a maximum drop of 23.43 F1 score. Our resources and code will be publicly available",
    "checked": true,
    "id": "c24c67fc0b7547be5306801c01ee6f9e7bab7ebc",
    "semantic_title": "preserving knowledge invariance: rethinking robustness evaluation of open information extraction",
    "citation_count": 1,
    "authors": [
      "Ji Qi",
      "Chuchun Zhang",
      "Xiaozhi Wang",
      "Kaisheng Zeng",
      "Jifan Yu",
      "Jinxin Liu",
      "Lei Hou",
      "Juanzi Li",
      "Xu Bin"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.361": {
    "title": "Why Should This Article Be Deleted? Transparent Stance Detection in Multilingual Wikipedia Editor Discussions",
    "volume": "main",
    "abstract": "The moderation of content on online platforms is usually non-transparent. On Wikipedia, however, this discussion is carried out publicly and editors are encouraged to use the content moderation policies as explanations for making moderation decisions. Currently, only a few comments explicitly mention those policies – 20% of the English ones, but as few as 2% of the German and Turkish comments. To aid in this process of understanding how content is moderated, we construct a novel multilingual dataset of Wikipedia editor discussions along with their reasoning in three languages. The dataset contains the stances of the editors (keep, delete, merge, comment), along with the stated reason, and a content moderation policy, for each edit decision. We demonstrate that stance and corresponding reason (policy) can be predicted jointly with a high degree of accuracy, adding transparency to the decision-making process. We release both our joint prediction models and the multilingual content moderation dataset for further research on automated transparent content moderation",
    "checked": true,
    "id": "ea077c3fed68ba5657e7d8e7f175040e469c891f",
    "semantic_title": "why should this article be deleted? transparent stance detection in multilingual wikipedia editor discussions",
    "citation_count": 0,
    "authors": [
      "Lucie-Aimée Kaffee",
      "Arnav Arora",
      "Isabelle Augenstein"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.362": {
    "title": "Fast and Robust Early-Exiting Framework for Autoregressive Language Models with Synchronized Parallel Decoding",
    "volume": "main",
    "abstract": "To tackle the high inference latency exhibited by autoregressive language models, previous studies have proposed an early-exiting framework that allocates adaptive computation paths for each token based on the complexity of generating the subsequent token. However, we observed several shortcomings, including performance degradation caused by a state copying mechanism or numerous exit paths, and sensitivity to exit confidence thresholds. Consequently, we propose a Fast and Robust Early-Exiting (FREE) framework, which incorporates a shallow-deep module and a synchronized parallel decoding. Our framework enables faster inference by synchronizing the decoding process of the current token with previously stacked early-exited tokens. Furthermore, as parallel decoding allows us to observe predictions from both shallow and deep models, we present a novel adaptive threshold estimator that exploits a Beta mixture model to determine suitable confidence thresholds. We empirically demonstrated the superiority of our proposed framework on extensive generation tasks",
    "checked": true,
    "id": "564855d475ed9197dd7516594557ff886ff623e5",
    "semantic_title": "fast and robust early-exiting framework for autoregressive language models with synchronized parallel decoding",
    "citation_count": 1,
    "authors": [
      "Sangmin Bae",
      "Jongwoo Ko",
      "Hwanjun Song",
      "Se-Young Yun"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.363": {
    "title": "End-to-end Task-oriented Dialogue: A Survey of Tasks, Methods, and Future Directions",
    "volume": "main",
    "abstract": "End-to-end task-oriented dialogue (EToD) can directly generate responses in an end-to-end fashion without modular training, which attracts escalating popularity. The advancement of deep neural networks, especially the successful use of large pre-trained models, has further led to significant progress in EToD research in recent years. In this paper, we present a thorough review and provide a unified perspective to summarize existing approaches as well as recent trends to advance the development of EToD research. The contributions of this paper can be summarized: (1) First survey: to our knowledge, we take the first step to present a thorough survey of this research field; (2) New taxonomy: we first introduce a unified perspective for EToD, including (i) Modularly EToD and (ii) Fully EToD; (3) New Frontiers: we discuss some potential frontier areas as well as the corresponding challenges, hoping to spur breakthrough research in EToD field; (4) Abundant resources: we build a public website, where EToD researchers could directly access the recent progress. We hope this work can serve as a thorough reference for the EToD research community",
    "checked": true,
    "id": "05d1ce1d3f42cc466edc54f88162144a119433a3",
    "semantic_title": "end-to-end task-oriented dialogue: a survey of tasks, methods, and future directions",
    "citation_count": 1,
    "authors": [
      "Libo Qin",
      "Wenbo Pan",
      "Qiguang Chen",
      "Lizi Liao",
      "Zhou Yu",
      "Yue Zhang",
      "Wanxiang Che",
      "Min Li"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.364": {
    "title": "Answering Questions by Meta-Reasoning over Multiple Chains of Thought",
    "volume": "main",
    "abstract": "Modern systems for multi-hop question answering (QA) typically break questions into a sequence of reasoning steps, termed chain-of-thought (CoT), before arriving at a final answer. Often, multiple chains are sampled and aggregated through a voting mechanism over the final answers, but the intermediate steps themselves are discarded. While such approaches improve performance, they do not consider the relations between intermediate steps across chains and do not provide a unified explanation for the predicted answer. We introduce Multi-Chain Reasoning (MCR), an approach which prompts large language models to meta-reason over multiple chains of thought, rather than aggregate their answers. MCR examines different reasoning chains, mixes information between them and selects the most relevant facts in generating an explanation and predicting the answer. MCR outperforms strong baselines on 7 multi-hop QA datasets. Moreover, our analysis reveals that MCR explanations exhibit high quality, enabling humans to verify its answers",
    "checked": true,
    "id": "7e4f5589327b6b574cc950a03fd1d6236e9e6128",
    "semantic_title": "answering questions by meta-reasoning over multiple chains of thought",
    "citation_count": 33,
    "authors": [
      "Ori Yoran",
      "Tomer Wolfson",
      "Ben Bogin",
      "Uri Katz",
      "Daniel Deutch",
      "Jonathan Berant"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.365": {
    "title": "INSTRUCTSCORE: Towards Explainable Text Generation Evaluation with Automatic Feedback",
    "volume": "main",
    "abstract": "Automatically evaluating the quality of language generation is critical. Although recent learned metrics show high correlation with human judgement, these metrics do not provide explicit explanation of their verdict, nor associate the scores with defects in the generated text. To address this limitation, we present INSTRUCTSCORE, a fine-grained explainable evaluation metric for text generation. By harnessing both explicit human instruction and the implicit knowledge of GPT-4, we fine-tune a text evaluation metric based on LLaMA, producing both a score for generated text and a human readable diagnostic report. We evaluate INSTRUCTSCORE on a variety of generation tasks, including translation, captioning, data-to-text, and commonsense generation. Experiments show that our 7B model surpasses all other unsupervised metrics, including those based on 175B GPT-3 and GPT-4. Surprisingly, our INSTRUCTSCORE, even without direct supervision from human-rated data, achieves performance levels on par with state-of-the-art metrics like COMET22, which were fine-tuned on human ratings",
    "checked": true,
    "id": "460609e217fd59eaa34f5e11a820661f8ec8d7b6",
    "semantic_title": "instructscore: towards explainable text generation evaluation with automatic feedback",
    "citation_count": 17,
    "authors": [
      "Wenda Xu",
      "Danqing Wang",
      "Liangming Pan",
      "Zhenqiao Song",
      "Markus Freitag",
      "William Wang",
      "Lei Li"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.366": {
    "title": "Multi-level Contrastive Learning for Script-based Character Understanding",
    "volume": "main",
    "abstract": "In this work, we tackle the scenario of understanding characters in scripts, which aims to learn the characters' personalities and identities from their utterances. We begin by analyzing several challenges in this scenario, and then propose a multi-level contrastive learning framework to capture characters' global information in a fine-grained manner. To validate the proposed framework, we conduct extensive experiments on three character understanding sub-tasks by comparing with strong pre-trained language models, including SpanBERT, Longformer, BigBird and ChatGPT-3.5. Experimental results demonstrate that our method improves the performances by a considerable margin. Through further in-depth analysis, we show the effectiveness of our method in addressing the challenges and provide more hints on the scenario of character understanding. We will open-source our work in this URL",
    "checked": true,
    "id": "700a6b189877f6b3cd795a30b30acb27c18d3354",
    "semantic_title": "multi-level contrastive learning for script-based character understanding",
    "citation_count": 1,
    "authors": [
      "Dawei Li",
      "Hengyuan Zhang",
      "Yanran Li",
      "Shiping Yang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.367": {
    "title": "CHEF in the Language Kitchen: A Generative Data Augmentation Leveraging Korean Morpheme Ingredients",
    "volume": "main",
    "abstract": "Korean morphological variations present unique opportunities and challenges in natural language processing (NLP), necessitating an advanced understanding of morpheme-based sentence construction. The complexity of morphological variations allows for diverse sentence forms based on the syntactic-semantic integration of functional morphemes (i.e., affixes) to lexical morphemes (i.e., roots). With this in mind, we propose a method - CHEF, replicating the morphological transformations inherent in sentences based on lexical and functional morpheme combinations through generative data augmentation. CHEF operates using a morpheme blender and a label discriminator, thereby enhancing the diversity of Korean sentence forms by capturing the properties of agglutination while maintaining label consistency. We conduct experiments on Korean multiple classification datasets, improving model performance in full- and few-shot settings. Our proposed method boosts performance beyond the preceding data augmentation methods without incurring external data usage. We demonstrate that our approach achieves comparable results yielded by augmentation techniques that use large language models (LLMs)",
    "checked": true,
    "id": "b6b26eeb8947cbb7fbd1315125413c90b79d03ac",
    "semantic_title": "chef in the language kitchen: a generative data augmentation leveraging korean morpheme ingredients",
    "citation_count": 0,
    "authors": [
      "Jaehyung Seo",
      "Hyeonseok Moon",
      "Jaewook Lee",
      "Sugyeong Eo",
      "Chanjun Park",
      "Heuiseok Lim"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.368": {
    "title": "Automatic Debate Evaluation with Argumentation Semantics and Natural Language Argument Graph Networks",
    "volume": "main",
    "abstract": "The lack of annotated data on professional argumentation and complete argumentative debates has led to the oversimplification and the inability of approaching more complex natural language processing tasks. Such is the case of the automatic evaluation of complete professional argumentative debates. In this paper, we propose an original hybrid method to automatically predict the winning stance in this kind of debates. For that purpose, we combine concepts from argumentation theory such as argumentation frameworks and semantics, with Transformer-based architectures and neural graph networks. Furthermore, we obtain promising results that lay the basis on an unexplored new instance of the automatic analysis of natural language arguments",
    "checked": true,
    "id": "5cca0c11f0a12abb4872a466db1fb9c1d6ff4aa0",
    "semantic_title": "automatic debate evaluation with argumentation semantics and natural language argument graph networks",
    "citation_count": 2,
    "authors": [
      "Ramon Ruiz-Dolz",
      "Stella Heras",
      "Ana Garcia"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.369": {
    "title": "Transfer-Free Data-Efficient Multilingual Slot Labeling",
    "volume": "main",
    "abstract": "Slot labeling (SL) is a core component of task-oriented dialogue (TOD) systems, where slots and corresponding values are usually language-, task- and domain-specific. Therefore, extending the system to any new language-domain-task configuration requires (re)running an expensive and resource-intensive data annotation process. To mitigate the inherent data scarcity issue, current research on multilingual ToD assumes that sufficient English-language annotated data are always available for particular tasks and domains, and thus operates in a standard cross-lingual transfer setup. In this work, we depart from this often unrealistic assumption. We examine challenging scenarios where such transfer-enabling English annotated data cannot be guaranteed, and focus on bootstrapping multilingual data-efficient slot labelers in transfer-free scenarios directly in the target languages without any English-ready data. We propose a two-stage slot labeling approach (termed TWOSL) which transforms standard multilingual sentence encoders into effective slot labelers. In Stage 1, relying on SL-adapted contrastive learning with only a handful of SL-annotated examples, we turn sentence encoders into task-specific span encoders. In Stage 2, we recast SL from a token classification into a simpler, less data-intensive span classification task. Our results on two standard multilingual TOD datasets and across diverse languages confirm the effectiveness and robustness of TWOSL. It is especially effective for the most challenging transfer-free few-shot setups, paving the way for quick and data-efficient bootstrapping of multilingual slot labelers for TOD",
    "checked": true,
    "id": "a5fdcd648de7edf22a63d3febe9da3f50990439c",
    "semantic_title": "transfer-free data-efficient multilingual slot labeling",
    "citation_count": 0,
    "authors": [
      "Evgeniia Razumovskaia",
      "Ivan Vulić",
      "Anna Korhonen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.370": {
    "title": "Towards Interpretable Mental Health Analysis with Large Language Models",
    "volume": "main",
    "abstract": "The latest large language models (LLMs) such as ChatGPT, exhibit strong capabilities in automated mental health analysis. However, existing relevant studies bear several limitations, including inadequate evaluations, lack of prompting strategies, and ignorance of exploring LLMs for explainability. To bridge these gaps, we comprehensively evaluate the mental health analysis and emotional reasoning ability of LLMs on 11 datasets across 5 tasks. We explore the effects of different prompting strategies with unsupervised and distantly supervised emotional information. Based on these prompts, we explore LLMs for interpretable mental health analysis by instructing them to generate explanations for each of their decisions. We convey strict human evaluations to assess the quality of the generated explanations, leading to a novel dataset with 163 human-assessed explanations. We benchmark existing automatic evaluation metrics on this dataset to guide future related works. According to the results, ChatGPT shows strong in-context learning ability but still has a significant gap with advanced task-specific methods. Careful prompt engineering with emotional cues and expert-written few-shot examples can also effectively improve performance on mental health analysis. In addition, ChatGPT generates explanations that approach human performance, showing its great potential in explainable mental health analysis",
    "checked": true,
    "id": "5d879530c443dd06d3686f31d32cfe34c7ade9bc",
    "semantic_title": "towards interpretable mental health analysis with large language models",
    "citation_count": 2,
    "authors": [
      "Kailai Yang",
      "Shaoxiong Ji",
      "Tianlin Zhang",
      "Qianqian Xie",
      "Ziyan Kuang",
      "Sophia Ananiadou"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.371": {
    "title": "Learning to Rank Generation with Pairwise Partial Rewards",
    "volume": "main",
    "abstract": "This paper studies the use of reinforcement learning for conditional text generation, which overcomes the limitation of the prevalent supervised maximum likelihood estimation approach. However, it still suffers from challenges including the large action space and the delayed reward, as the reward can be computed only after an entire sequence is generated. To address these challenges, we propose a method that provides partial rewards for intermediate actions taken on partial sequences. This enables the model to promptly prioritize actions that lead to the generation of more desirable sequences. Our method's key contribution lies in its focus on distinguishing relatively more desirable actions rather than striving to precisely estimate pointwise values for arbitrary partial sequences. Instead, our model learns to discern the relative desirability between pairs of actions, or rank actions in a pairwise manner, only when necessary and feasible. This is materialized in an efficient way by leveraging the prefix tree constructed from the sampled sequences. Experimental results on paraphrase generation and constrained machine translation tasks showcase the effectiveness of our method",
    "checked": true,
    "id": "8e7f5c72b9745cfaf8d418b347e3771d1e76fb83",
    "semantic_title": "learning to rank generation with pairwise partial rewards",
    "citation_count": 0,
    "authors": [
      "Youngwon Lee",
      "Jinu Lee",
      "Seung-won Hwang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.372": {
    "title": "GreedyCAS: Unsupervised Scientific Abstract Segmentation with Normalized Mutual Information",
    "volume": "main",
    "abstract": "The abstracts of scientific papers typically contain both premises (e.g., background and observations) and conclusions. Although conclusion sentences are highlighted in structured abstracts, in non-structured abstracts the concluding information is not explicitly marked, which makes the automatic segmentation of conclusions from scientific abstracts a challenging task. In this work, we explore Normalized Mutual Information (NMI) as a means for abstract segmentation. We consider each abstract as a recurrent cycle of sentences and place two segmentation boundaries by greedily optimizing the NMI score between the two segments, assuming that conclusions are strongly semantically linked with preceding premises. On non-structured abstracts, our proposed unsupervised approach GreedyCAS achieves the best performance across all evaluation metrics; on structured abstracts, GreedyCAS outperforms all baseline methods measured by Pk. The strong correlation of NMI to our evaluation metrics reveals the effectiveness of NMI for abstract segmentation",
    "checked": true,
    "id": "d307150b079b9df25bd6ae53f6991a90226349a3",
    "semantic_title": "greedycas: unsupervised scientific abstract segmentation with normalized mutual information",
    "citation_count": 0,
    "authors": [
      "Yingqiang Gao",
      "Jessica Lam",
      "Nianlong Gu",
      "Richard Hahnloser"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.373": {
    "title": "Spoiler Detection as Semantic Text Matching",
    "volume": "main",
    "abstract": "Engaging with discussion of TV shows online often requires individuals to refrain from consuming show-related content for extended periods to avoid spoilers. While existing research on spoiler detection shows promising results in safeguarding viewers from general spoilers, it fails to address the issue of users abstaining from show-related content during their watch. This is primarily because the definition of a spoiler varies depending on the viewer's progress in the show, and conventional spoiler detection methods lack the granularity to capture this complexity. To tackle this challenge, we propose the task of spoiler matching, which involves assigning an episode number to a spoiler given a specific TV show. We frame this task as semantic text matching and introduce a dataset comprised of comments and episode summaries to evaluate model performance. Given the length of each example, our dataset can also serve as a benchmark for long-range language models",
    "checked": true,
    "id": "f12c3de10e2429d07b480c1fb1025a1238baea48",
    "semantic_title": "spoiler detection as semantic text matching",
    "citation_count": 0,
    "authors": [
      "Ryan Tran",
      "Canwen Xu",
      "Julian McAuley"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.374": {
    "title": "Multimodal Embodied Plan Prediction Augmented with Synthetic Embodied Dialogue",
    "volume": "main",
    "abstract": "Embodied task completion is a challenge where an agent in a simulated environment must predict environment actions to complete tasks based on natural language instructions and ego-centric visual observations. We propose a variant of this problem where the agent predicts actions at a higher level of abstraction called a plan, which helps make agent actions more interpretable and can be obtained from the appropriate prompting of large language models. We show that multimodal transformer models can outperform language-only models for this problem but fall significantly short of oracle plans. Since collecting human-human dialogues for embodied environments is expensive and time-consuming, we propose a method to synthetically generate such dialogues, which we then use as training data for plan prediction. We demonstrate that multimodal transformer models can attain strong zero-shot performance from our synthetic data, outperforming language-only models trained on human-human data",
    "checked": true,
    "id": "1c013a05bd429467f4a94f7dca18b83769d0b17b",
    "semantic_title": "multimodal embodied plan prediction augmented with synthetic embodied dialogue",
    "citation_count": 0,
    "authors": [
      "Aishwarya Padmakumar",
      "Mert Inan",
      "Spandana Gella",
      "Patrick Lange",
      "Dilek Hakkani-Tur"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.375": {
    "title": "GEM: Gestalt Enhanced Markup Language Model for Web Understanding via Render Tree",
    "volume": "main",
    "abstract": "Inexhaustible web content carries abundant perceptible information beyond text. Unfortunately, most prior efforts in pre-trained Language Models (LMs) ignore such cyber-richness, while few of them only employ plain HTMLs, and crucial information in the rendered web, such as visual, layout, and style, are excluded. Intuitively, those perceptible web information can provide essential intelligence to facilitate content understanding tasks. This study presents an innovative Gestalt Enhanced Markup (GEM) Language Model inspired by Gestalt psychological theory for hosting heterogeneous visual information from the render tree into the language model without requiring additional visual input. Comprehensive experiments on multiple downstream tasks, i.e., web question answering and web information extraction, validate GEM superiority",
    "checked": true,
    "id": "21c5041fcd520b4820ce3cde30ba5dc40f91bd87",
    "semantic_title": "gem: gestalt enhanced markup language model for web understanding via render tree",
    "citation_count": 0,
    "authors": [
      "Zirui Shao",
      "Feiyu Gao",
      "Zhongda Qi",
      "Hangdi Xing",
      "Jiajun Bu",
      "Zhi Yu",
      "Qi Zheng",
      "Xiaozhong Liu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.376": {
    "title": "Abstractive Open Information Extraction",
    "volume": "main",
    "abstract": "Open Information Extraction (OpenIE) is a traditional NLP task that extracts structured information from unstructured text to be used for other downstream applications. Traditionally, OpenIE focuses on extracting the surface forms of relations as they appear in the raw text, which we term extractive OpenIE. One of the main drawbacks of this approach is that implicit semantic relations (inferred relations) can not be extracted, compromising the performance of downstream applications. In this paper, we broaden the scope of OpenIE relations from merely the surface form of relations to include inferred relations, which we term abstractive OpenIE. This new task calls for the development of a new abstractive OpenIE training dataset and a baseline neural model that can extract those inferred relations. We also demonstrate the necessity for a new semantics-based metric for evaluating abstractive OpenIE extractions. Via a case study on Complex QA, we demonstrate the effectiveness of abstractive OpenIE",
    "checked": true,
    "id": "c0ae507085c8d921571fbd9bce276fe5e25c753f",
    "semantic_title": "abstractive open information extraction",
    "citation_count": 0,
    "authors": [
      "Kevin Pei",
      "Ishan Jindal",
      "Kevin Chang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.377": {
    "title": "CoSyn: Detecting Implicit Hate Speech in Online Conversations Using a Context Synergized Hyperbolic Network",
    "volume": "main",
    "abstract": "The tremendous growth of social media users interacting in online conversations has led to significant growth in hate speech affecting people from various demographics. Most of the prior works focus on detecting explicit hate speech, which is overt and leverages hateful phrases, with very little work focusing on detecting hate speech that is implicit or denotes hatred through indirect or coded language. In this paper, we present CoSyn, a context synergized neural network that explicitly incorporates user- and conversational-context for detecting implicit hate speech in online conversations. CoSyn introduces novel ways to encode these external contexts and employs a novel context interaction mechanism that clearly captures the interplay between them, making independent assessments of the amounts of information to be retrieved from these noisy contexts. Additionally, it carries out all these operations in the hyperbolic space to account for the scale-free dynamics of social media. We demonstrate the effectiveness of CoSyn on 6 hate speech datasets and show that CoSyn outperforms all our baselines in detecting implicit hate speech with absolute improvements in the range of 1.24% - 57.8%. We make our code available",
    "checked": true,
    "id": "02453313a18962f0cc5046cfe97085e2b8104350",
    "semantic_title": "cosyn: detecting implicit hate speech in online conversations using a context synergized hyperbolic network",
    "citation_count": 1,
    "authors": [
      "Sreyan Ghosh",
      "Manan Suri",
      "Purva Chiniya",
      "Utkarsh Tyagi",
      "Sonal Kumar",
      "Dinesh Manocha"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.378": {
    "title": "CLEME: Debiasing Multi-reference Evaluation for Grammatical Error Correction",
    "volume": "main",
    "abstract": "Evaluating the performance of Grammatical Error Correction (GEC) systems is a challenging task due to its subjectivity. Designing an evaluation metric that is as objective as possible is crucial to the development of GEC task. However, mainstream evaluation metrics, i.e., reference-based metrics, introduce bias into the multi-reference evaluation by extracting edits without considering the presence of multiple references. To overcome this issue, we propose Chunk-LE Multi-reference Evaluation (CLEME), designed to evaluate GEC systems in the multi-reference evaluation setting. CLEME builds chunk sequences with consistent boundaries for the source, the hypothesis and references, thus eliminating the bias caused by inconsistent edit boundaries. Furthermore, we observe the consistent boundary could also act as the boundary of grammatical errors, based on which the F0.5 score is then computed following the correction independence assumption. We conduct experiments on six English reference sets based on the CoNLL-2014 shared task. Extensive experiments and detailed analyses demonstrate the correctness of our discovery and the effectiveness of CLEME. Further analysis reveals that CLEME is robust to evaluate GEC systems across reference sets with varying numbers of references and annotation styles. All the source codes of CLEME are released at https://github.com/THUKElab/CLEME",
    "checked": true,
    "id": "a863656063087a452301c6b6cafae30fe4511b9a",
    "semantic_title": "cleme: debiasing multi-reference evaluation for grammatical error correction",
    "citation_count": 4,
    "authors": [
      "Jingheng Ye",
      "Yinghui Li",
      "Qingyu Zhou",
      "Yangning Li",
      "Shirong Ma",
      "Hai-Tao Zheng",
      "Ying Shen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.379": {
    "title": "Dynamic Top-k Estimation Consolidates Disagreement between Feature Attribution Methods",
    "volume": "main",
    "abstract": "Feature attribution scores are used for explaining the prediction of a text classifier to users by highlighting a k number of tokens. In this work, we propose a way to determine the number of optimal k tokens that should be displayed from sequential properties of the attribution scores. Our approach is dynamic across sentences, method-agnostic, and deals with sentence length bias. We compare agreement between multiple methods and humans on an NLI task, using fixed k and dynamic k. We find that perturbation-based methods and Vanilla Gradient exhibit highest agreement on most method–method and method–human agreement metrics with a static k. Their advantage over other methods disappears with dynamic ks which mainly improve Integrated Gradient and GradientXInput. To our knowledge, this is the first evidence that sequential properties of attribution scores are informative for consolidating attribution signals for human interpretation",
    "checked": true,
    "id": "7247ff39ec89f5094da27f751a788915ede83459",
    "semantic_title": "dynamic top-k estimation consolidates disagreement between feature attribution methods",
    "citation_count": 0,
    "authors": [
      "Jonathan Kamp",
      "Lisa Beinborn",
      "Antske Fokkens"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.380": {
    "title": "SentiStream: A Co-Training Framework for Adaptive Online Sentiment Analysis in Evolving Data Streams",
    "volume": "main",
    "abstract": "Online sentiment analysis has emerged as a crucial component in numerous data-driven applications, including social media monitoring, customer feedback analysis, and online reputation management. Despite their importance, current methodologies falter in effectively managing the continuously evolving nature of data streams, largely due to their reliance on substantial, pre-existing labelled datasets. This paper presents sentistream, a novel co-training framework specifically designed for efficient sentiment analysis within dynamic data streams. Comprising unsupervised, semi-supervised, and stream merge modules, sentistream guarantees constant adaptability to evolving data landscapes. This research delves into the continuous adaptation of language models for online sentiment analysis, focusing on real-world applications. Experimental evaluations using data streams derived from three benchmark sentiment analysis datasets confirm that our proposed methodology surpasses existing approaches in terms of both accuracy and computational efficiency",
    "checked": true,
    "id": "0a145523ee73c91b7e87ff973dd040b6cdb09526",
    "semantic_title": "sentistream: a co-training framework for adaptive online sentiment analysis in evolving data streams",
    "citation_count": 0,
    "authors": [
      "Yuhao Wu",
      "Karthick Sharma",
      "Chun Seah",
      "Shuhao Zhang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.381": {
    "title": "HyperNetwork-based Decoupling to Improve Model Generalization for Few-Shot Relation Extraction",
    "volume": "main",
    "abstract": "Few-shot relation extraction (FSRE) aims to train a model that can deal with new relations using only a few labeled examples. Most existing studies employ Prototypical Networks for FSRE, which usually overfits the relation classes in the training set and cannot generalize well to unseen relations. By investigating the class separation of an FSRE model, we find that model upper layers are prone to learn relation-specific knowledge. Therefore, in this paper, we propose a HyperNetwork-based Decoupling approach to improve the generalization of FSRE models. Specifically, our model consists of an encoder, a network generator (for producing relation classifiers) and the produced-then-finetuned classifiers for every N-way-K-shot episode. Meanwhile, we design a two-step training framework along with a class-agnostic aligner, in which the generated classifiers focus on acquiring relation-specific knowledge and the encoder is encouraged to learn more general relation knowledge. In this way, the roles of upper and lower layers in an FSRE model are explicitly decoupled, thus enhancing its generalizing capability during testing. Experiments on two public datasets demonstrate the effectiveness of our method",
    "checked": true,
    "id": "fd4134b2fbb169d147ede8d32a40d911742e1495",
    "semantic_title": "hypernetwork-based decoupling to improve model generalization for few-shot relation extraction",
    "citation_count": 0,
    "authors": [
      "Liang Zhang",
      "Chulun Zhou",
      "Fandong Meng",
      "Jinsong Su",
      "Yidong Chen",
      "Jie Zhou"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.382": {
    "title": "Solving Hard Analogy Questions with Relation Embedding Chains",
    "volume": "main",
    "abstract": "Modelling how concepts are related is a central topic in Lexical Semantics. A common strategy is to rely on knowledge graphs (KGs) such as ConceptNet, and to model the relation between two concepts as a set of paths. However, KGs are limited to a fixed set of relation types, and they are incomplete and often noisy. Another strategy is to distill relation embeddings from a fine-tuned language model. However, this is less suitable for words that are only indirectly related and it does not readily allow us to incorporate structured domain knowledge. In this paper, we aim to combine the best of both worlds. We model relations as paths but associate their edges with relation embeddings. The paths are obtained by first identifying suitable intermediate words and then selecting those words for which informative relation embeddings can be obtained. We empirically show that our proposed representations are useful for solving hard analogy questions",
    "checked": true,
    "id": "0005cf92f05e5c93ba2dc165bc08550797ebe8e7",
    "semantic_title": "solving hard analogy questions with relation embedding chains",
    "citation_count": 0,
    "authors": [
      "Nitesh Kumar",
      "Steven Schockaert"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.383": {
    "title": "Modeling Empathic Similarity in Personal Narratives",
    "volume": "main",
    "abstract": "The most meaningful connections between people are often fostered through expression of shared vulnerability and emotional experiences in personal narratives. We introduce a new task of identifying similarity in personal stories based on empathic resonance, i.e., the extent to which two people empathize with each others' experiences, as opposed to raw semantic or lexical similarity, as has predominantly been studied in NLP. Using insights from social psychology, we craft a framework that operationalizes empathic similarity in terms of three key features of stories: main events, emotional trajectories, and overall morals or takeaways. We create EmpathicStories, a dataset of 1,500 personal stories annotated with our empathic similarity features, and 2,000 pairs of stories annotated with empathic similarity scores. Using our dataset, we fine-tune a model to compute empathic similarity of story pairs, and show that this outperforms semantic similarity models on automated correlation and retrieval metrics. Through a user study with 150 participants, we also assess the effect our model has on retrieving stories that users empathize with, compared to naive semantic similarity-based retrieval, and find that participants empathized significantly more with stories retrieved by our model. Our work has strong implications for the use of empathy-aware models to foster human connection and empathy between people",
    "checked": true,
    "id": "14ddefae2be4b5bb50b9fcb4a085e45fbecb5c5c",
    "semantic_title": "modeling empathic similarity in personal narratives",
    "citation_count": 1,
    "authors": [
      "Jocelyn Shen",
      "Maarten Sap",
      "Pedro Colon-Hernandez",
      "Hae Park",
      "Cynthia Breazeal"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.384": {
    "title": "Tree Prompting: Efficient Task Adaptation without Fine-Tuning",
    "volume": "main",
    "abstract": "Prompting language models (LMs) is the main interface for applying them to new tasks. However, for smaller LMs, prompting provides low accuracy compared to gradient-based fine-tuning. Tree Prompting is an approach to prompting which builds a decision tree of prompts, linking multiple prompt-LM calls together to solve a task. At inference time, each call to the LM is determined by efficiently routing the outcome of the previous call using the tree. Experiments on classification datasets show that Tree Prompting improves accuracy over competing methods and is competitive with fine-tuning. We also show that variants of Tree Prompting allow inspection of a model's decision-making process",
    "checked": true,
    "id": "a706cd2053923114038142cbc62ddf4ec91a0293",
    "semantic_title": "tree prompting: efficient task adaptation without fine-tuning",
    "citation_count": 1,
    "authors": [
      "Chandan Singh",
      "John Morris",
      "Alexander Rush",
      "Jianfeng Gao",
      "Yuntian Deng"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.385": {
    "title": "Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data",
    "volume": "main",
    "abstract": "Chat models, such as ChatGPT, have shown impressive capabilities and have been rapidly adopted across numerous domains. However, these models are only accessible through a restricted API, creating barriers for new research and progress in the field. We propose a pipeline that can automatically generate a high-quality multi-turn chat corpus by leveraging ChatGPT to engage in a conversation with itself. Subsequently, we employ parameter-efficient tuning to enhance LLaMA, an open-source large language model. The resulting model, named Baize, demonstrates good performance in multi-turn dialogues with guardrails that minimize potential risks. Additionally, we propose a new technique called Self-Distill with Feedback, to further improve the performance of the Baize models with feedback from ChatGPT",
    "checked": true,
    "id": "42e741e0be43954ae684d14333e4074f4d0ae961",
    "semantic_title": "baize: an open-source chat model with parameter-efficient tuning on self-chat data",
    "citation_count": 122,
    "authors": [
      "Canwen Xu",
      "Daya Guo",
      "Nan Duan",
      "Julian McAuley"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.386": {
    "title": "Empathy Intent Drives Empathy Detection",
    "volume": "main",
    "abstract": "Empathy plays an important role in the human dialogue. Detecting the empathetic direction expressed by the user is necessary for empathetic dialogue systems because it is highly relevant to understanding the user's needs. Several studies have shown that empathy intent information improves the ability to response capacity of empathetic dialogue. However, the interaction between empathy detection and empathy intent recognition has not been explored. To this end, we invite 3 experts to manually annotate the healthy empathy detection datasets IEMPATHIZE and TwittEmp with 8 empathy intent labels, and perform joint training for the two tasks. Empirical study has shown that the introduction of empathy intent recognition task can improve the accuracy of empathy detection task, and we analyze possible reasons for this improvement. To make joint training of the two tasks more challenging, we propose a novel framework, Cascaded Label Signal Network, which uses the cascaded interactive attention module and the label signal enhancement module to capture feature exchange information between empathy and empathy intent representations. Experimental results show that our framework outperforms all baselines under both settings on the two datasets",
    "checked": true,
    "id": "531a1b47ee2468992ac015d9db0f4338c3388ba6",
    "semantic_title": "empathy intent drives empathy detection",
    "citation_count": 0,
    "authors": [
      "Liting Jiang",
      "Di Wu",
      "Bohui Mao",
      "Yanbing Li",
      "Wushour Slamu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.387": {
    "title": "Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling",
    "volume": "main",
    "abstract": "Recently slot filling has witnessed great development thanks to deep learning and the availability of large-scale annotated data. However, it poses a critical challenge to handle a novel domain whose samples are never seen during training. The recognition performance might be greatly degraded due to severe domain shifts. Most prior works deal with this problem in a two-pass pipeline manner based on metric learning. In practice, these dominant pipeline models may be limited in computational efficiency and generalization capacity because of non-parallel inference and context-free discrete label embeddings. To this end, we re-examine the typical metric-based methods, and propose a new adaptive end-to-end metric learning scheme for the challenging zero-shot slot filling. Considering simplicity, efficiency and generalizability, we present a cascade-style joint learning framework coupled with context-aware soft label representations and slot-level contrastive representation learning to mitigate the data and label shift problems effectively. Extensive experiments on public benchmarks demonstrate the superiority of the proposed approach over a series of competitive baselines",
    "checked": true,
    "id": "f11441c84ee3c9541308b6e4cbc5c2e027002712",
    "semantic_title": "adaptive end-to-end metric learning for zero-shot cross-domain slot filling",
    "citation_count": 0,
    "authors": [
      "Yuanjun Shi",
      "Linzhi Wu",
      "Minglai Shao"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.388": {
    "title": "BasahaCorpus: An Expanded Linguistic Resource for Readability Assessment in Central Philippine Languages",
    "volume": "main",
    "abstract": "Current research on automatic readability assessment (ARA) has focused on improving the performance of models in high-resource languages such as English. In this work, we introduce and release BasahaCorpus as part of an initiative aimed at expanding available corpora and baseline models for readability assessment in lower resource languages in the Philippines. We compiled a corpus of short fictional narratives written in Hiligaynon, Minasbate, Karay-a, and Rinconada—languages belonging to the Central Philippine family tree subgroup—to train ARA models using surface-level, syllable-pattern, and n-gram overlap features. We also propose a new hierarchical cross-lingual modeling approach that takes advantage of a language's placement in the family tree to increase the amount of available training data. Our study yields encouraging results that support previous work showcasing the efficacy of cross-lingual models in low-resource settings, as well as similarities in highly informative linguistic features for mutually intelligible languages",
    "checked": true,
    "id": "fd965454b40c455b7251c8e9a25089dddfaf15c6",
    "semantic_title": "basahacorpus: an expanded linguistic resource for readability assessment in central philippine languages",
    "citation_count": 0,
    "authors": [
      "Joseph Imperial",
      "Ekaterina Kochmar"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.389": {
    "title": "ReTAG: Reasoning Aware Table to Analytic Text Generation",
    "volume": "main",
    "abstract": "The task of table summarization involves generating text that both succinctly and accurately represents the table or a specific set of highlighted cells within a table. While significant progress has been made in table to text generation techniques, models still mostly generate descriptive summaries, which reiterates the information contained within the table in sentences. Through analysis of popular table to text benchmarks (ToTTo (Parikh et al., 2020 and InfoTabs (Gupta et al., 2020) we observe that in order to generate the ideal summary, multiple types of reasoning is needed coupled with access to knowledge beyond the scope of the table. To address this gap, we propose ReTAG, a table and reasoning aware model that uses vector-quantization to infuse different types of analytical reasoning into the output. ReTAG achieves 2.2%, 2.9% improvement on the PARENT metric in the relevant slice of ToTTo and InfoTabs for the table to text generation task over state of the art baselines. Through human evaluation, we observe that output from ReTAG is upto 12% more faithful and analytical compared to a strong table-aware model. To the best of our knowledge, ReTAG is the first model that can controllably use multiple reasoning methods within a structure-aware sequence to sequence model to surpass state of the art performance in multiple table to text tasks. We extend (and open source 35.6K analytical, 55.9k descriptive instances) the ToTTo, InfoTabs datasets with the reasoning categories used in each reference sentences",
    "checked": true,
    "id": "d3d8f37a809a0e3efe1cf6419033ed36424ffc8a",
    "semantic_title": "retag: reasoning aware table to analytic text generation",
    "citation_count": 0,
    "authors": [
      "Deepanway Ghosal",
      "Preksha Nema",
      "Aravindan Raghuveer"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.390": {
    "title": "Beyond Factuality: A Comprehensive Evaluation of Large Language Models as Knowledge Generators",
    "volume": "main",
    "abstract": "Large language models (LLMs) outperform information retrieval techniques for downstream knowledge-intensive tasks when being prompted to generate world knowledge. However, community concerns abound regarding the factuality and potential implications of using this uncensored knowledge. In light of this, we introduce CONNER, a COmpreheNsive kNowledge Evaluation fRamework, designed to systematically and automatically evaluate generated knowledge from six important perspectives – Factuality, Relevance, Coherence, Informativeness, Helpfulness and Validity. We conduct an extensive empirical analysis of the generated knowledge from three different types of LLMs on two widely studied knowledge-intensive tasks, i.e., open-domain question answering and knowledge-grounded dialogue. Surprisingly, our study reveals that the factuality of generated knowledge, even if lower, does not significantly hinder downstream tasks. Instead, the relevance and coherence of the outputs are more important than small factual mistakes. Further, we show how to use CONNER to improve knowledge-intensive tasks by designing two strategies: Prompt Engineering and Knowledge Selection. Our evaluation code and LLM-generated knowledge with human annotations will be released to facilitate future research",
    "checked": true,
    "id": "0f6fe87afd1a3571f77c790893b03717e5d0422a",
    "semantic_title": "beyond factuality: a comprehensive evaluation of large language models as knowledge generators",
    "citation_count": 5,
    "authors": [
      "Liang Chen",
      "Yang Deng",
      "Yatao Bian",
      "Zeyu Qin",
      "Bingzhe Wu",
      "Tat-Seng Chua",
      "Kam-Fai Wong"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.391": {
    "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
    "volume": "main",
    "abstract": "Large language models (LLMs) achieved remarkable performance across various tasks. However, they face challenges in managing long documents and extended conversations, due to significantly increased computational requirements, both in memory and inference time, and potential context truncation when the input exceeds the LLM's fixed context length. This paper proposes a method called Selective Context that enhances the inference efficiency of LLMs by identifying and pruning redundancy in the input context to make the input more compact. We test our approach using common data sources requiring long context processing: arXiv papers, news articles, and long conversations, on tasks of summarisation, question answering, and response generation. Experimental results show that Selective Context significantly reduces memory cost and decreases generation latency while maintaining comparable performance compared to that achieved when full context is used. Specifically, we achieve a 50% reduction in context cost, resulting in a 36% reduction in inference memory usage and a 32% reduction in inference time, while observing only a minor drop of .023 in BERTscore and .038 in faithfulness on four downstream applications, indicating that our method strikes a good balance between efficiency and performance",
    "checked": true,
    "id": "b12541867632737e826b7b01c7fbe1c4222d8655",
    "semantic_title": "compressing context to enhance inference efficiency of large language models",
    "citation_count": 0,
    "authors": [
      "Yucheng Li",
      "Bo Dong",
      "Frank Guerin",
      "Chenghua Lin"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.392": {
    "title": "MoT: Memory-of-Thought Enables ChatGPT to Self-Improve",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have shown impressive abilities on various tasks. However, fundamentally improving them depends on high-quality datasets or computationally expensive fine-tuning. On the contrary, humans can easily improve themselves by self-thinking and memory, without external resources. In this paper, we propose a framework, **MoT**, to let the LLM self-improve through **M**emory **o**f **T**houghts, without annotated datasets and parameter updates. Specifically, MoT is divided into two stages: 1. before the test stage, the LLM pre-thinks on the unlabeled dataset and saves the high-confidence thoughts as external memory; 2. During the test stage, given a test question, the LLM recalls relevant memory to help itself reason and answer it. Experimental results show that MoT can help ChatGPT significantly improve its abilities in arithmetic reasoning, commonsense reasoning, factual reasoning, and natural language inference. Further analyses show that each component contributes critically to the improvements and MoT can lead to consistent improvements across various CoT methods and LLMs",
    "checked": true,
    "id": "73207b9fd2dcfeead7fe086cfdb097e4929a7b44",
    "semantic_title": "mot: memory-of-thought enables chatgpt to self-improve",
    "citation_count": 5,
    "authors": [
      "Xiaonan Li",
      "Xipeng Qiu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.393": {
    "title": "4 and 7-bit Labeling for Projective and Non-Projective Dependency Trees",
    "volume": "main",
    "abstract": "We introduce an encoding for parsing as sequence labeling that can represent any projective dependency tree as a sequence of 4-bit labels, one per word. The bits in each word's label represent (1) whether it is a right or left dependent, (2) whether it is the outermost (left/right) dependent of its parent, (3) whether it has any left children and (4) whether it has any right children. We show that this provides an injective mapping from trees to labels that can be encoded and decoded in linear time. We then define a 7-bit extension that represents an extra plane of arcs, extending the coverage to almost full non-projectivity (over 99.9% empirical arc coverage). Results on a set of diverse treebanks show that our 7-bit encoding obtains substantial accuracy gains over the previously best-performing sequence labeling encodings",
    "checked": true,
    "id": "ce6adc73d86170c41aee544a2b6f5eba906dc78c",
    "semantic_title": "4 and 7-bit labeling for projective and non-projective dependency trees",
    "citation_count": 0,
    "authors": [
      "Carlos Gómez-Rodríguez",
      "Diego Roca",
      "David Vilares"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.394": {
    "title": "Can You Follow Me? Testing Situational Understanding for ChatGPT",
    "volume": "main",
    "abstract": "Understanding sentence meanings and updating information states appropriately across time—what we call \"situational understanding\" (SU)—is a critical ability for human-like AI agents. SU is essential in particular for chat models, such as ChatGPT, to enable consistent, coherent, and effective dialogue between humans and AI. Previous works have identified certain SU limitations in non-chatbot Large Language models (LLMs), but the extent and causes of these limitations are not well understood, and capabilities of current chat-based models in this domain have not been explored. In this work we tackle these questions, proposing a novel synthetic environment for SU testing which allows us to do controlled and systematic testing of SU in chat-oriented models, through assessment of models' ability to track and enumerate environment states. Our environment also allows for close analysis of dynamics of model performance, to better understand underlying causes for performance patterns. We apply our test to ChatGPT, the state-of-the-art chatbot, and find that despite the fundamental simplicity of the task, the model's performance reflects an inability to retain correct environment states across time. Our follow-up analyses suggest that performance degradation is largely because ChatGPT has non-persistent in-context memory (although it can access the full dialogue history) and it is susceptible to hallucinated updates—including updates that artificially inflate accuracies. Our findings suggest overall that ChatGPT is not currently equipped for robust tracking of situation states, and that trust in the impressive dialogue performance of ChatGPT comes with risks. We release the codebase for reproducing our test environment, as well as all prompts and API responses from ChatGPT, at https://github.com/yangalan123/SituationalTesting",
    "checked": true,
    "id": "ae2e5a54c11ef18c314cc76bdf886bb969153fe5",
    "semantic_title": "can you follow me? testing situational understanding for chatgpt",
    "citation_count": 0,
    "authors": [
      "Chenghao Yang",
      "Allyson Ettinger"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.395": {
    "title": "Towards Reliable Misinformation Mitigation: Generalization, Uncertainty, and GPT-4",
    "volume": "main",
    "abstract": "Misinformation poses a critical societal challenge, and current approaches have yet to produce an effective solution. We propose focusing on generalization, uncertainty, and how to leverage recent large language models, in order to create more practical tools to evaluate information veracity in contexts where perfect classification is impossible. We first demonstrate that GPT-4 can outperform prior methods in multiple settings and languages. Next, we explore generalization, revealing that GPT-4 and RoBERTa-large exhibit differences in failure modes. Third, we propose techniques to handle uncertainty that can detect impossible examples and strongly improve outcomes. We also discuss results on other language models, temperature, prompting, versioning, explainability, and web retrieval, each one providing practical insights and directions for future research. Finally, we publish the LIAR-New dataset with novel paired English and French misinformation data and Possibility labels that indicate if there is sufficient context for veracity evaluation. Overall, this research lays the groundwork for future tools that can drive real-world progress to combat misinformation",
    "checked": true,
    "id": "e3446ef313663e30d8251dee339bca52962e7bfd",
    "semantic_title": "towards reliable misinformation mitigation: generalization, uncertainty, and gpt-4",
    "citation_count": 5,
    "authors": [
      "Kellin Pelrine",
      "Anne Imouza",
      "Camille Thibault",
      "Meilina Reksoprodjo",
      "Caleb Gupta",
      "Joel Christoph",
      "Jean-François Godbout",
      "Reihaneh Rabbany"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.396": {
    "title": "Advancements in Arabic Grammatical Error Detection and Correction: An Empirical Investigation",
    "volume": "main",
    "abstract": "Grammatical error correction (GEC) is a well-explored problem in English with many existing models and datasets. However, research on GEC in morphologically rich languages has been limited due to challenges such as data scarcity and language complexity. In this paper, we present the first results on Arabic GEC using two newly developed Transformer-based pretrained sequence-to-sequence models. We also define the task of multi-class Arabic grammatical error detection (GED) and present the first results on multi-class Arabic GED. We show that using GED information as auxiliary input in GEC models improves GEC performance across three datasets spanning different genres. Moreover, we also investigate the use of contextual morphological preprocessing in aiding GEC systems. Our models achieve SOTA results on two Arabic GEC shared task datasets and establish a strong benchmark on a recently created dataset. We make our code, data, and pretrained models publicly available",
    "checked": true,
    "id": "83f804910211ed4cfdc158ba402b9c0d533ec94e",
    "semantic_title": "advancements in arabic grammatical error detection and correction: an empirical investigation",
    "citation_count": 1,
    "authors": [
      "Bashar Alhafni",
      "Go Inoue",
      "Christian Khairallah",
      "Nizar Habash"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.397": {
    "title": "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models",
    "volume": "main",
    "abstract": "Large language models (LLMs), such as ChatGPT, are prone to generate hallucinations, i.e., content that conflicts with the source or cannot be verified by the factual knowledge. To understand what types of content and to which extent LLMs are apt to hallucinate, we introduce the Hallucination Evaluation for Large Language Models (HaluEval) benchmark, a large collection of generated and human-annotated hallucinated samples for evaluating the performance of LLMs in recognizing hallucination. To generate these samples, we propose a ChatGPT-based two-step framework, i.e., sampling-then-filtering. Besides, we also hire some human labelers to annotate the hallucinations in ChatGPT responses. The empirical results suggest that ChatGPT is likely to generate hallucinated content in specific topics by fabricating unverifiable information (i.e., about 19.5% user queries). Moreover, existing LLMs face great challenges in recognizing the hallucinations in texts. While, our experiments also prove that the hallucination recognition can be improved by providing external knowledge or adding reasoning steps",
    "checked": true,
    "id": "e0384ba36555232c587d4a80d527895a095a9001",
    "semantic_title": "halueval: a large-scale hallucination evaluation benchmark for large language models",
    "citation_count": 25,
    "authors": [
      "Junyi Li",
      "Xiaoxue Cheng",
      "Xin Zhao",
      "Jian-Yun Nie",
      "Ji-Rong Wen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.398": {
    "title": "Enabling Large Language Models to Generate Text with Citations",
    "volume": "main",
    "abstract": "Large language models (LLMs) have emerged as a widely-used tool for information seeking, but their generated outputs are prone to hallucination. In this work, our aim is to allow LLMs to generate text with citations, improving their factual correctness and verifiability. Existing work mainly relies on commercial search engines and human evaluation, making it challenging to reproduce and compare different modeling approaches. We propose ALCE, the first benchmark for Automatic LLMs' Citation Evaluation. ALCE collects a diverse set of questions and retrieval corpora and requires building end-to-end systems to retrieve supporting evidence and generate answers with citations. We develop automatic metrics along three dimensions—fluency, correctness, and citation quality—and demonstrate their strong correlation with human judgements. Our experiments with state-of-the-art LLMs and novel prompting strategies show that current systems have considerable room for improvement—For example, on the ELI5 dataset, even the best models lack complete citation support 50% of the time. Our analyses further highlight promising future directions, including developing better retrievers, advancing long-context LLMs, and improving the ability to synthesize information from multiple sources",
    "checked": true,
    "id": "e7c97e953849f1a8e5d85ceb4cfcc0a5d54d2365",
    "semantic_title": "enabling large language models to generate text with citations",
    "citation_count": 44,
    "authors": [
      "Tianyu Gao",
      "Howard Yen",
      "Jiatong Yu",
      "Danqi Chen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.399": {
    "title": "Revisiting Machine Translation for Cross-lingual Classification",
    "volume": "main",
    "abstract": "Machine Translation (MT) has been widely used for cross-lingual classification, either by translating the test set into English and running inference with a monolingual model (translate-test), or translating the training set into the target languages and finetuning a multilingual model (translate-train). However, most research in the area focuses on the multilingual models rather than the MT component. We show that, by using a stronger MT system and mitigating the mismatch between training on original text and running inference on machine translated text, translate-test can do substantially better than previously assumed. The optimal approach, however, is highly task dependent, as we identify various sources of cross-lingual transfer gap that affect different tasks and approaches differently. Our work calls into question the dominance of multilingual models for cross-lingual classification, and prompts to pay more attention to MT-based baselines",
    "checked": true,
    "id": "d6b453da30f7c34009b85b2cd31b019d58d6d750",
    "semantic_title": "revisiting machine translation for cross-lingual classification",
    "citation_count": 13,
    "authors": [
      "Mikel Artetxe",
      "Vedanuj Goswami",
      "Shruti Bhosale",
      "Angela Fan",
      "Luke Zettlemoyer"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.400": {
    "title": "Pre-Trained Language Models Augmented with Synthetic Scanpaths for Natural Language Understanding",
    "volume": "main",
    "abstract": "Human gaze data offer cognitive information that reflects natural language comprehension. Indeed, augmenting language models with human scanpaths has proven beneficial for a range of NLP tasks, including language understanding. However, the applicability of this approach is hampered because the abundance of text corpora is contrasted by a scarcity of gaze data. Although models for the generation of human-like scanpaths during reading have been developed, the potential of synthetic gaze data across NLP tasks remains largely unexplored. We develop a model that integrates synthetic scanpath generation with a scanpath-augmented language model, eliminating the need for human gaze data. Since the model's error gradient can be propagated throughout all parts of the model, the scanpath generator can be fine-tuned to downstream tasks. We find that the proposed model not only outperforms the underlying language model, but achieves a performance that is comparable to a language model augmented with real human gaze data. Our code is publicly available",
    "checked": true,
    "id": "7e94ef5623b249d196efafb9a1cbf356bdc17694",
    "semantic_title": "pre-trained language models augmented with synthetic scanpaths for natural language understanding",
    "citation_count": 0,
    "authors": [
      "Shuwen Deng",
      "Paul Prasse",
      "David Reich",
      "Tobias Scheffer",
      "Lena Jäger"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.401": {
    "title": "Counting the Bugs in ChatGPT's Wugs: A Multilingual Investigation into the Morphological Capabilities of a Large Language Model",
    "volume": "main",
    "abstract": "Large language models (LLMs) have recently reached an impressive level of linguistic capability, prompting comparisons with human language skills. However, there have been relatively few systematic inquiries into the linguistic capabilities of the latest generation of LLMs, and those studies that do exist (i) ignore the remarkable ability of humans to generalize, (ii) focus only on English, and (iii) investigate syntax or semantics and overlook other capabilities that lie at the heart of human language, like morphology. Here, we close these gaps by conducting the first rigorous analysis of the morphological capabilities of ChatGPT in four typologically varied languages (specifically, English, German, Tamil, and Turkish). We apply a version of Berko's (1958) wug test to ChatGPT, using novel, uncontaminated datasets for the four examined languages. We find that ChatGPT massively underperforms purpose-built systems, particularly in English. Overall, our results—through the lens of morphology—cast a new light on the linguistic capabilities of ChatGPT, suggesting that claims of human-like language skills are premature and misleading",
    "checked": true,
    "id": "fabee69ea9a76d18fcb90b4a329a9f124ba35e11",
    "semantic_title": "counting the bugs in chatgpt's wugs: a multilingual investigation into the morphological capabilities of a large language model",
    "citation_count": 0,
    "authors": [
      "Leonie Weissweiler",
      "Valentin Hofmann",
      "Anjali Kantharuban",
      "Anna Cai",
      "Ritam Dutt",
      "Amey Hengle",
      "Anubha Kabra",
      "Atharva Kulkarni",
      "Abhishek Vijayakumar",
      "Haofei Yu",
      "Hinrich Schuetze",
      "Kemal Oflazer",
      "David Mortensen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.402": {
    "title": "Adapt in Contexts: Retrieval-Augmented Domain Adaptation via In-Context Learning",
    "volume": "main",
    "abstract": "Large language models (LLMs) have showcased their capability with few-shot inference known as in-context learning. However, in-domain demonstrations are not always readily available in real scenarios, leading to cross-domain in-context learning. Besides, LLMs are still facing challenges in long-tail knowledge in unseen and unfamiliar domains. The above limitations demonstrate the necessity of Unsupervised Domain Adaptation (UDA). In this paper, we study the UDA problem under an in-context learning setting to adapt language models from the source domain to the target domain without any target labels. The core idea is to retrieve a subset of cross-domain elements that are the most similar to the query, and elicit language model to adapt in an in-context manner by learning both target domain distribution and the discriminative task signal simultaneously with the augmented cross-domain in-context examples. We devise different prompting and training strategies, accounting for different LM architectures to learn the target distribution via language modeling. With extensive experiments on Sentiment Analysis (SA) and Named Entity Recognition (NER) tasks, we thoroughly study the effectiveness of ICL for domain transfer and demonstrate significant improvements over baseline models",
    "checked": true,
    "id": "4a6c1ee10c448840f598a281374496f6ebe11b5c",
    "semantic_title": "adapt in contexts: retrieval-augmented domain adaptation via in-context learning",
    "citation_count": 0,
    "authors": [
      "Quanyu Long",
      "Wenya Wang",
      "Sinno Pan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.403": {
    "title": "Understanding the Inner-workings of Language Models Through Representation Dissimilarity",
    "volume": "main",
    "abstract": "As language models are applied to an increasing number of real-world applications, understanding their inner workings has become an important issue in model trust, interpretability, and transparency. In this work we show that representation dissimilarity measures, which are functions that measure the extent to which two model's internal representations differ, can be a valuable tool for gaining insight into the mechanics of language models. Among our insights are: (i) an apparent asymmetry in the internal representations of model using SoLU and GeLU activation functions, (ii) evidence that dissimilarity measures can identify and locate generalization properties of models that are invisible via in-distribution test set performance, and (iii) new evaluations of how language model features vary as width and depth are increased. Our results suggest that dissimilarity measures are a promising set of tools for shedding light on the inner workings of language models",
    "checked": false,
    "id": "61889eb1cb94c66fc8782882b01c2cdd7e3a41ed",
    "semantic_title": "understanding the inner workings of language models through representation dissimilarity",
    "citation_count": 1,
    "authors": [
      "Davis Brown",
      "Charles Godfrey",
      "Nicholas Konz",
      "Jonathan Tu",
      "Henry Kvinge"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.404": {
    "title": "Efficient Classification of Long Documents via State-Space Models",
    "volume": "main",
    "abstract": "Transformer-based models have achieved state-of-the-art performance on numerous NLP applications. However, long documents which are prevalent in real-world scenarios cannot be efficiently processed by transformers with the vanilla self-attention module due to their quadratic computation complexity and limited length extrapolation ability. Instead of tackling the computation difficulty for self-attention with sparse or hierarchical structures, in this paper, we investigate the use of State-Space Models (SSMs) for long document classification tasks. We conducted extensive experiments on six long document classification datasets, including binary, multi-class, and multi-label classification, comparing SSMs (with and without pre-training) to self-attention-based models. We also introduce the SSM-pooler model and demonstrate that it achieves comparable performance while being on average 36% more efficient. Additionally our method exhibits higher robustness to the input noise even in the extreme scenario of 40%",
    "checked": true,
    "id": "76d1971deacfb477110ee9e17b6f7267f0b480f0",
    "semantic_title": "efficient classification of long documents via state-space models",
    "citation_count": 0,
    "authors": [
      "Peng Lu",
      "Suyuchen Wang",
      "Mehdi Rezagholizadeh",
      "Bang Liu",
      "Ivan Kobyzev"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.405": {
    "title": "Dual-Feedback Knowledge Retrieval for Task-Oriented Dialogue Systems",
    "volume": "main",
    "abstract": "Efficient knowledge retrieval plays a pivotal role in ensuring the success of end-to-end task-oriented dialogue systems by facilitating the selection of relevant information necessary to fulfill user requests. However, current approaches generally integrate knowledge retrieval and response generation, which poses scalability challenges when dealing with extensive knowledge bases. Taking inspiration from open-domain question answering, we propose a retriever-generator architecture that harnesses a retriever to retrieve pertinent knowledge and a generator to generate system responses. Due to the lack of retriever training labels, we propose relying on feedback from the generator as pseudo-labels to train the retriever. To achieve this, we introduce a dual-feedback mechanism that generates both positive and negative feedback based on the output of the generator. Our method demonstrates superior performance in task-oriented dialogue tasks, as evidenced by experimental results on three benchmark datasets",
    "checked": true,
    "id": "ee7d457aad39c5aa82c18c63a8b3f84be6276de7",
    "semantic_title": "dual-feedback knowledge retrieval for task-oriented dialogue systems",
    "citation_count": 0,
    "authors": [
      "Tianyuan Shi",
      "Liangzhi Li",
      "Zijian Lin",
      "Tao Yang",
      "Xiaojun Quan",
      "Qifan Wang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.406": {
    "title": "Construction Artifacts in Metaphor Identification Datasets",
    "volume": "main",
    "abstract": "Metaphor identification aims at understanding whether a given expression is used figuratively in context. However, in this paper we show how existing metaphor identification datasets can be gamed by fully ignoring the potential metaphorical expression or the context in which it occurs. We test this hypothesis in a variety of datasets and settings, and show that metaphor identification systems based on language models without complete information can be competitive with those using the full context. This is due to the construction procedures to build such datasets, which introduce unwanted biases for positive and negative classes. Finally, we test the same hypothesis on datasets that are carefully sampled from natural corpora and where this bias is not present, making these datasets more challenging and reliable",
    "checked": true,
    "id": "9a5c06c7fd5fd06789cb4cab86a97a241b5ea2b7",
    "semantic_title": "construction artifacts in metaphor identification datasets",
    "citation_count": 0,
    "authors": [
      "Joanne Boisson",
      "Luis Espinosa-Anke",
      "Jose Camacho-Collados"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.407": {
    "title": "MAF: Multi-Aspect Feedback for Improving Reasoning in Large Language Models",
    "volume": "main",
    "abstract": "Language Models (LMs) have shown impressive performance in various natural language tasks. However, when it comes to natural language reasoning, LMs still face challenges such as hallucination, generating incorrect intermediate reasoning steps, and making mathematical errors. Recent research has focused on enhancing LMs through *self-improvement* using feedback. Nevertheless, existing approaches relying on a single generic feedback source fail to address the diverse error types found in LM-generated reasoning chains. In this work, we propose **Multi-Aspect Feedback**, an iterative refinement framework that integrates multiple feedback modules, including frozen LMs and external tools, each focusing on a specific error category. Our experimental results demonstrate the efficacy of our approach to addressing several errors in the LM-generated reasoning chain and thus improving the overall performance of an LM in several reasoning tasks. We see an improvement of up to 20% in Mathematical Reasoning and up to 18% in Logical Entailment",
    "checked": true,
    "id": "20eecb9ead20ffe49a66588a9662336eefb20a54",
    "semantic_title": "maf: multi-aspect feedback for improving reasoning in large language models",
    "citation_count": 0,
    "authors": [
      "Deepak Nathani",
      "David Wang",
      "Liangming Pan",
      "William Wang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.408": {
    "title": "Granularity Matters: Pathological Graph-driven Cross-modal Alignment for Brain CT Report Generation",
    "volume": "main",
    "abstract": "The automatic Brain CT reports generation can improve the efficiency and accuracy of diagnosing cranial diseases. However, current methods are limited by 1) coarse-grained supervision: the training data in image-text format lacks detailed supervision for recognizing subtle abnormalities, and 2) coupled cross-modal alignment: visual-textual alignment may be inevitably coupled in a coarse-grained manner, resulting in tangled feature representation for report generation. In this paper, we propose a novel Pathological Graph-driven Cross-modal Alignment (PGCA) model for accurate and robust Brain CT report generation. Our approach effectively decouples the cross-modal alignment by constructing a Pathological Graph to learn fine-grained visual cues and align them with textual words. This graph comprises heterogeneous nodes representing essential pathological attributes (i.e., tissue and lesion) connected by intra- and inter-attribute edges with prior domain knowledge. Through carefully designed graph embedding and updating modules, our model refines the visual features of subtle tissues and lesions and aligns them with textual words using contrastive learning. Extensive experimental results confirm the viability of our method. We believe that our PGCA model holds the potential to greatly enhance the automatic generation of Brain CT reports and ultimately contribute to improved cranial disease diagnosis",
    "checked": true,
    "id": "ce3bc78e01e54210e003a1931b059c4675ee8e34",
    "semantic_title": "granularity matters: pathological graph-driven cross-modal alignment for brain ct report generation",
    "citation_count": 0,
    "authors": [
      "Yanzhao Shi",
      "Junzhong Ji",
      "Xiaodan Zhang",
      "Liangqiong Qu",
      "Ying Liu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.409": {
    "title": "Enhancing Structured Evidence Extraction for Fact Verification",
    "volume": "main",
    "abstract": "Open-domain fact verification is the task of verifying claims in natural language texts against extracted evidence. FEVEROUS is a benchmark that requires extracting and integrating both unstructured and structured evidence to verify a given claim. Previous models suffer from low recall of structured evidence extraction, i.e., table extraction and cell selection. In this paper, we propose a simple but effective method to enhance the extraction of structured evidence by leveraging the row and column semantics of tables. Our method comprises two components: (i) a coarse-grained table extraction module that selects tables based on rows and columns relevant to the claim and (ii) a fine-grained cell selection graph that combines both formats of evidence and enables multi-hop and numerical reasoning. We evaluate our method on FEVEROUS and achieve an evidence recall of 60.01% on the test set, which is 6.14% higher than the previous state-of-the-art performance. Our results demonstrate that our method can extract tables and select cells effectively, and provide better evidence sets for verdict prediction. Our code is released at https://github.com/WilliamZR/see-st",
    "checked": true,
    "id": "aec3b2fd88554e119436a0caa16c208c120b0def",
    "semantic_title": "enhancing structured evidence extraction for fact verification",
    "citation_count": 0,
    "authors": [
      "Zirui Wu",
      "Nan Hu",
      "Yansong Feng"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.410": {
    "title": "Rethinking Model Selection and Decoding for Keyphrase Generation with Pre-trained Sequence-to-Sequence Models",
    "volume": "main",
    "abstract": "Keyphrase Generation (KPG) is a longstanding task in NLP with widespread applications. The advent of sequence-to-sequence (seq2seq) pre-trained language models (PLMs) has ushered in a transformative era for KPG, yielding promising performance improvements. However, many design decisions remain unexplored and are often made arbitrarily. This paper undertakes a systematic analysis of the influence of model selection and decoding strategies on PLM-based KPG. We begin by elucidating why seq2seq PLMs are apt for KPG, anchored by an attention-driven hypothesis. We then establish that conventional wisdom for selecting seq2seq PLMs lacks depth: (1) merely increasing model size or performing task-specific adaptation is not parameter-efficient; (2) although combining in-domain pre-training with task adaptation benefits KPG, it does partially hinder generalization. Regarding decoding, we demonstrate that while greedy search achieves strong F1 scores, it lags in recall compared with sampling-based methods. Based on these insights, we propose DeSel, a likelihood-based decode-select algorithm for seq2seq PLMs. DeSel improves greedy search by an average of 4.7% semantic F1 across five datasets. Our collective findings pave the way for deeper future investigations into PLM-based KPG",
    "checked": true,
    "id": "23097f2d2deda9b92544fc2294d0c2f7d57cf12a",
    "semantic_title": "rethinking model selection and decoding for keyphrase generation with pre-trained sequence-to-sequence models",
    "citation_count": 1,
    "authors": [
      "Di Wu",
      "Wasi Ahmad",
      "Kai-Wei Chang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.411": {
    "title": "A Fair and In-Depth Evaluation of Existing End-to-End Entity Linking Systems",
    "volume": "main",
    "abstract": "Existing evaluations of entity linking systems often say little about how the system is going to perform for a particular application. There are two fundamental reasons for this. One is that many evaluations only use aggregate measures (like precision, recall, and F1 score), without a detailed error analysis or a closer look at the results. The other is that all of the widely used benchmarks have strong biases and artifacts, in particular: a strong focus on named entities, an unclear or missing specification of what else counts as an entity mention, poor handling of ambiguities, and an over- or underrepresentation of certain kinds of entities. We provide a more meaningful and fair in-depth evaluation of a variety of existing end-to-end entity linkers. We characterize their strengths and weaknesses and also report on reproducibility aspects. The detailed results of our evaluation can be inspected under https://elevant.cs.uni-freiburg.de/emnlp2023. Our evaluation is based on several widely used benchmarks, which exhibit the problems mentioned above to various degrees, as well as on two new benchmarks, which address the problems mentioned above. The new benchmarks can be found under https://github.com/ad-freiburg/fair-entity-linking-benchmarks",
    "checked": true,
    "id": "e0276e2b8569fd95734d1e5590f6241fe2de6879",
    "semantic_title": "a fair and in-depth evaluation of existing end-to-end entity linking systems",
    "citation_count": 0,
    "authors": [
      "Hannah Bast",
      "Matthias Hertel",
      "Natalie Prange"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.412": {
    "title": "A Multi-Task Dataset for Assessing Discourse Coherence in Chinese Essays: Structure, Theme, and Logic Analysis",
    "volume": "main",
    "abstract": "This paper introduces the Chinese Essay Discourse Coherence Corpus (CEDCC), a multi-task dataset for assessing discourse coherence. Existing research tends to focus on isolated dimensions of discourse coherence, a gap which the CEDCC addresses by integrating coherence grading, topical continuity, and discourse relations. This approach, alongside detailed annotations, captures the subtleties of real-world texts and stimulates progress in Chinese discourse coherence analysis. Our contributions include the development of the CEDCC, the establishment of baselines for further research, and the demonstration of the impact of coherence on discourse relation recognition and automated essay scoring. The dataset and related codes is available at https://github.com/cubenlp/CEDCC_corpus",
    "checked": true,
    "id": "ae241da2bf95f6a6d88359e1650c6d953410f0b0",
    "semantic_title": "a multi-task dataset for assessing discourse coherence in chinese essays: structure, theme, and logic analysis",
    "citation_count": 0,
    "authors": [
      "Hongyi Wu",
      "Xinshu Shen",
      "Man Lan",
      "Shaoguang Mao",
      "Xiaopeng Bai",
      "Yuanbin Wu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.413": {
    "title": "SKD-NER: Continual Named Entity Recognition via Span-based Knowledge Distillation with Reinforcement Learning",
    "volume": "main",
    "abstract": "Continual learning for named entity recognition (CL-NER) aims to enable models to continuously learn new entity types while retaining the ability to recognize previously learned ones. However, the current strategies fall short of effectively addressing the catastrophic forgetting of previously learned entity types. To tackle this issue, we propose the SKD-NER model, an efficient continual learning NER model based on the span-based approach, which innovatively incorporates reinforcement learning strategies to enhance the model's ability against catastrophic forgetting. Specifically, we leverage knowledge distillation (KD) to retain memory and employ reinforcement learning strategies during the KD process to optimize the soft labeling and distillation losses generated by the teacher model to effectively prevent catastrophic forgetting during continual learning. This approach effectively prevents or mitigates catastrophic forgetting during continuous learning, allowing the model to retain previously learned knowledge while acquiring new knowledge. Our experiments on two benchmark datasets demonstrate that our model significantly improves the performance of the CL-NER task, outperforming state-of-the-art methods",
    "checked": true,
    "id": "c8901c19a4ec89ffba51682b8b8bad424cce93a8",
    "semantic_title": "skd-ner: continual named entity recognition via span-based knowledge distillation with reinforcement learning",
    "citation_count": 0,
    "authors": [
      "Yi Chen",
      "Liang He"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.414": {
    "title": "Lifelong Sequence Generation with Dynamic Module Expansion and Adaptation",
    "volume": "main",
    "abstract": "Lifelong sequence generation (LSG), a problem in continual learning, aims to continually train a model on a sequence of generation tasks to learn constantly emerging new generation patterns while avoiding the forgetting of previous knowledge. Existing LSG methods mainly focus on maintaining old knowledge while paying little attention to knowledge transfer across tasks. In contrast, humans can better learn new tasks by leveraging previously acquired knowledge from similar tasks. Inspired by the learning paradigm of humans, we propose Dynamic Module Expansion and Adaptation (DMEA), which enables the model to dynamically determine the architecture for acquiring new knowledge based on task correlation and select the most similar previous tasks to facilitate adaptation to new tasks. In addition, as the learning process can easily be biased towards the current task which might cause more severe forgetting of previously learned knowledge, we propose dynamic gradient scaling to balance the learning of the current task and replayed tasks. With extensive experiments, we demonstrate that DMEA can consistently outperform existing methods in different LSG settings",
    "checked": true,
    "id": "7021719e050356ddd2f58f40d66e7bab3457ec8c",
    "semantic_title": "lifelong sequence generation with dynamic module expansion and adaptation",
    "citation_count": 1,
    "authors": [
      "Chengwei Qin",
      "Chen Chen",
      "Shafiq Joty"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.415": {
    "title": "When the Majority is Wrong: Modeling Annotator Disagreement for Subjective Tasks",
    "volume": "main",
    "abstract": "Though majority vote among annotators is typically used for ground truth labels in machine learning, annotator disagreement in tasks such as hate speech detection may reflect systematic differences in opinion across groups, not noise. Thus, a crucial problem in hate speech detection is determining if a statement is offensive to the demographic group that it targets, when that group may be a small fraction of the annotator pool. We construct a model that predicts individual annotator ratings on potentially offensive text and combines this information with the predicted target group of the text to predict the ratings of target group members. We show gains across a range of metrics, including raising performance over the baseline by 22% at predicting individual annotators' ratings and by 33% at predicting variance among annotators, which provides a metric for model uncertainty downstream. We find that annotators' ratings can be predicted using their demographic information as well as opinions on online content, and that non-invasive questions on annotators' online experiences minimize the need to collect demographic information when predicting annotators' opinions",
    "checked": true,
    "id": "d9e8a7e6d24e443dffeeec30e0ee8aea05c032a3",
    "semantic_title": "when the majority is wrong: modeling annotator disagreement for subjective tasks",
    "citation_count": 2,
    "authors": [
      "Eve Fleisig",
      "Rediet Abebe",
      "Dan Klein"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.416": {
    "title": "Lazy-k Decoding: Constrained Decoding for Information Extraction",
    "volume": "main",
    "abstract": "We explore the possibility of improving probabilistic models in structured prediction. Specifically, we combine the models with constrained decoding approaches in the context of token classification for information extraction. The decoding methods search for constraint-satisfying label-assignments while maximizing the total probability. To do this, we evaluate several existing approaches, as well as propose a novel decoding method called Lazy-k. Our findings demonstrate that constrained decoding approaches can significantly improve the models' performances, especially when using smaller models. The Lazy-k approach allows for more flexibility between decoding time and accuracy. The code for using Lazy-k decoding can be found at https://github.com/ArthurDevNL/lazyk",
    "checked": true,
    "id": "56d7add5193da25419152f687ecb46b88bf22619",
    "semantic_title": "lazy-k decoding: constrained decoding for information extraction",
    "citation_count": 0,
    "authors": [
      "Arthur Hemmer",
      "Mickael Coustaty",
      "Nicola Bartolo",
      "Jerome Brachat",
      "Jean-marc Ogier"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.417": {
    "title": "Personalized Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation",
    "volume": "main",
    "abstract": "With the rise of powerful closed-sourced LLMs (ChatGPT, GPT-4), there are increasing interests in distilling the capabilies of close-sourced LLMs to smaller open-sourced LLMs. Previous distillation methods usually prompt ChatGPT to generate a set of instructions and answers, for the student model to learn. However, such standard distillation approach neglects the merits and conditions of the student model. Inspired by modern teaching principles, we design a personalised distillation process, in which the student attempts to solve a task first, then the teacher provides an adaptive refinement for the student to improve. Instead of feeding the student with teacher's prior, personalised distillation enables personalised learning for the student model, as it only learns on examples it makes mistakes upon and learns to improve its own solution. On code generation, personalised distillation consistently outperforms standard distillation with only one third of the data. With only 2.5-3K personalised examples that incur a data-collection cost of 4-6$, we boost CodeGen-mono-16B by 7% to achieve 36.4% pass@1 and StarCoder by 12.2% to achieve 45.8% pass@1 on HumanEval",
    "checked": true,
    "id": "3424b252c1eea8e3b4b9926d21feb35f4280b0ab",
    "semantic_title": "personalized distillation: empowering open-sourced llms with adaptive learning for code generation",
    "citation_count": 0,
    "authors": [
      "Hailin Chen",
      "Amrita Saha",
      "Steven Hoi",
      "Shafiq Joty"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.418": {
    "title": "Do Language Models Have a Common Sense regarding Time? Revisiting Temporal Commonsense Reasoning in the Era of Large Language Models",
    "volume": "main",
    "abstract": "Temporal reasoning represents a vital component of human communication and understanding, yet remains an underexplored area within the context of Large Language Models (LLMs). Despite LLMs demonstrating significant proficiency in a range of tasks, a comprehensive, large-scale analysis of their temporal reasoning capabilities is missing. Our paper addresses this gap, presenting the first extensive benchmarking of LLMs on temporal reasoning tasks. We critically evaluate 8 different LLMs across 6 datasets using 3 distinct prompting strategies. Additionally, we broaden the scope of our evaluation by including in our analysis 2 Code Generation LMs. Beyond broad benchmarking of models and prompts, we also conduct a fine-grained investigation of performance across different categories of temporal tasks. We further analyze the LLMs on varying temporal aspects, offering insights into their proficiency in understanding and predicting the continuity, sequence, and progression of events over time. Our findings reveal a nuanced depiction of the capabilities and limitations of the models within temporal reasoning, offering a comprehensive reference for future research in this pivotal domain",
    "checked": true,
    "id": "46137166084af64a7d115c7a731a1ce0da4c066e",
    "semantic_title": "do language models have a common sense regarding time? revisiting temporal commonsense reasoning in the era of large language models",
    "citation_count": 0,
    "authors": [
      "Raghav Jain",
      "Daivik Sojitra",
      "Arkadeep Acharya",
      "Sriparna Saha",
      "Adam Jatowt",
      "Sandipan Dandapat"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.419": {
    "title": "Comparing Styles across Languages",
    "volume": "main",
    "abstract": "Understanding how styles differ across languages is advantageous for training both humans and computers to generate culturally appropriate text. We introduce an explanation framework to extract stylistic differences from multilingual LMs and compare styles across languages. Our framework (1) generates comprehensive style lexica in any language and (2) consolidates feature importances from LMs into comparable lexical categories. We apply this framework to compare politeness, creating the first holistic multilingual politeness dataset and exploring how politeness varies across four languages. Our approach enables an effective evaluation of how distinct linguistic categories contribute to stylistic variations and provides interpretable insights into how people communicate differently around the world",
    "checked": true,
    "id": "4fb1d7ad2cae60f01c44597cef58fcef734b785e",
    "semantic_title": "comparing styles across languages",
    "citation_count": 0,
    "authors": [
      "Shreya Havaldar",
      "Matthew Pressimone",
      "Eric Wong",
      "Lyle Ungar"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.420": {
    "title": "Event Causality Extraction via Implicit Cause-Effect Interactions",
    "volume": "main",
    "abstract": "Event Causality Extraction (ECE) aims to extract the cause-effect event pairs from the given text, which requires the model to possess a strong reasoning ability to capture event causalities. However, existing works have not adequately exploited the interactions between the cause and effect event that could provide crucial clues for causality reasoning. To this end, we propose an Implicit Cause-Effect interaction (ICE) framework, which formulates ECE as a template-based conditional generation problem. The proposed method captures the implicit intra- and inter-event interactions by incorporating the privileged information (ground truth event types and arguments) for reasoning, and a knowledge distillation mechanism is introduced to alleviate the unavailability of privileged information in the test stage. Furthermore, to facilitate knowledge transfer from teacher to student, we design an event-level alignment strategy named Cause-Effect Optimal Transport (CEOT) to strengthen the semantic interactions of cause-effect event types and arguments. Experimental results indicate that ICE achieves state-of-the-art performance on the ECE-CCKS dataset",
    "checked": true,
    "id": "767c2a788f84c4c560e911cb3d2e8c099f2d1916",
    "semantic_title": "event causality extraction via implicit cause-effect interactions",
    "citation_count": 0,
    "authors": [
      "Jintao Liu",
      "Zequn Zhang",
      "Kaiwen Wei",
      "Zhi Guo",
      "Xian Sun",
      "Li Jin",
      "Xiaoyu Li"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.421": {
    "title": "Evaluation of African American Language Bias in Natural Language Generation",
    "volume": "main",
    "abstract": "While biases disadvantaging African American Language (AAL) have been uncovered in models for tasks such as speech recognition and toxicity detection, there has been little investigation of these biases for language generation models like ChatGPT. We evaluate how well LLMs understand AAL in comparison to White Mainstream English (WME), the encouraged \"standard\" form of English taught in American classrooms. We measure large language model performance on two tasks: a counterpart generation task, where a model generates AAL given WME and vice versa, and a masked span prediction (MSP) task, where models predict a phrase hidden from their input. Using a novel dataset of AAL texts from a variety of regions and contexts, we present evidence of dialectal bias for six pre-trained LLMs through performance gaps on these tasks",
    "checked": true,
    "id": "3de99f885cfc0c2145cd584df7df4230cccaea04",
    "semantic_title": "evaluation of african american language bias in natural language generation",
    "citation_count": 0,
    "authors": [
      "Nicholas Deas",
      "Jessica Grieser",
      "Shana Kleiner",
      "Desmond Patton",
      "Elsbeth Turcan",
      "Kathleen McKeown"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.422": {
    "title": "A Systematic Study of Performance Disparities in Multilingual Task-Oriented Dialogue Systems",
    "volume": "main",
    "abstract": "Achieving robust language technologies that can perform well across the world's many languages is a central goal of multilingual NLP. In this work, we take stock of and empirically analyse task performance disparities that exist between multilingual task-oriented dialogue (ToD) systems. We first define new quantitative measures of absolute and relative equivalence in system performance, capturing disparities across languages and within individual languages. Through a series of controlled experiments, we demonstrate that performance disparities depend on a number of factors: the nature of the ToD task at hand, the underlying pretrained language model, the target language, and the amount of ToD annotated data. We empirically prove the existence of the adaptation and intrinsic biases in current ToD systems: e.g., ToD systems trained for Arabic or Turkish using annotated ToD data fully parallel to English ToD data still exhibit diminished ToD task performance. Beyond providing a series of insights into the performance disparities of ToD systems in different languages, our analyses offer practical tips on how to approach ToD data collection and system development for new languages",
    "checked": true,
    "id": "392a5d5629f23d6209479220c53c1e7760525af7",
    "semantic_title": "a systematic study of performance disparities in multilingual task-oriented dialogue systems",
    "citation_count": 0,
    "authors": [
      "Songbo Hu",
      "Han Zhou",
      "Moy Yuan",
      "Milan Gritta",
      "Guchun Zhang",
      "Ignacio Iacobacci",
      "Anna Korhonen",
      "Ivan Vulić"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.423": {
    "title": "Cognate Transformer for Automated Phonological Reconstruction and Cognate Reflex Prediction",
    "volume": "main",
    "abstract": "Phonological reconstruction is one of the central problems in historical linguistics where a proto-word of an ancestral language is determined from the observed cognate words of daughter languages. Computational approaches to historical linguistics attempt to automate the task by learning models on available linguistic data. Several ideas and techniques drawn from computational biology have been successfully applied in this area of computational historical linguistics. Following these lines, we adapt MSA Transformer, a protein language model, to the problem of automated phonological reconstruction. MSA Transformer trains on multiple sequence alignments as input and is, thus, apt for application on aligned cognate words. We, hence, name our model as Cognate Transformer. We also apply the model on another associated task, namely, cognate reflex prediction where a reflex word in a daughter language is predicted based on cognate words from other daughter languages. We show that our model outperforms the existing models on both the tasks, especially when it is pre-trained on masked word prediction task",
    "checked": true,
    "id": "5f417c98adcfc4f1a0732466548ea64282eb03e2",
    "semantic_title": "cognate transformer for automated phonological reconstruction and cognate reflex prediction",
    "citation_count": 0,
    "authors": [
      "V.S.D.S.Mahesh Akavarapu",
      "Arnab Bhattacharya"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.424": {
    "title": "Inference-Time Policy Adapters (IPA): Tailoring Extreme-Scale LMs without Fine-tuning",
    "volume": "main",
    "abstract": "While extreme-scale language models have demonstrated exceptional performance on a variety of language tasks, the degree of control over these language models through pure prompting can often be limited. Directly fine-tuning such language models can be effective for tailoring them, but it can be either extremely costly (e.g., GPT-3) or not even feasible for the broader community (e.g., GPT-4). We propose Inference-time Policy Adapters (IPA), which efficiently tailors a language model such as GPT-3 without fine-tuning it. IPA guides a large base model during decoding time through a lightweight policy adapter trained to optimize an arbitrary user objective with reinforcement learning. On five challenging text generation tasks, such as toxicity reduction and lexically constrained generation, IPA consistently brings significant improvements over off-the-shelf language models. It outperforms competitive baseline methods, sometimes even including expensive fine-tuning. In particular, tailoring GPT-2 with IPA can outperform GPT-3, while tailoring GPT-3 with IPA brings a major performance boost over GPT-3 (and sometimes even over GPT-4). Our promising results highlight the potential of IPA as a lightweight alternative to tailoring extreme-scale language models",
    "checked": true,
    "id": "ca991e0283a1c30a46eb585d9eb499fc0ec8ecc2",
    "semantic_title": "inference-time policy adapters (ipa): tailoring extreme-scale lms without fine-tuning",
    "citation_count": 1,
    "authors": [
      "Ximing Lu",
      "Faeze Brahman",
      "Peter West",
      "Jaehun Jung",
      "Khyathi Chandu",
      "Abhilasha Ravichander",
      "Prithviraj Ammanabrolu",
      "Liwei Jiang",
      "Sahana Ramnath",
      "Nouha Dziri",
      "Jillian Fisher",
      "Bill Lin",
      "Skyler Hallinan",
      "Lianhui Qin",
      "Xiang Ren",
      "Sean Welleck",
      "Yejin Choi"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.425": {
    "title": "Weakly Supervised Semantic Parsing with Execution-based Spurious Program Filtering",
    "volume": "main",
    "abstract": "The problem of spurious programs is a longstanding challenge when training a semantic parser from weak supervision. To eliminate such programs that have wrong semantics but correct denotation, existing methods focus on exploiting similarities between examples based on domain-specific knowledge. In this paper, we propose a domain-agnostic filtering mechanism based on program execution results. Specifically, for each program obtained through the search process, we first construct a representation that captures the program's semantics as execution results under various inputs. Then, we run a majority vote on these representations to identify and filter out programs with significantly different semantics from the other programs. In particular, our method is orthogonal to the program search process so that it can easily augment any of the existing weakly supervised semantic parsing frameworks. Empirical evaluations on the Natural Language Visual Reasoning and WikiTableQuestions demonstrate that applying our method to the existing semantic parsers induces significantly improved performances",
    "checked": true,
    "id": "d9663f1dccd8b27fffa3dba7b376e6a93d4c4cb6",
    "semantic_title": "weakly supervised semantic parsing with execution-based spurious program filtering",
    "citation_count": 0,
    "authors": [
      "Kang-il Lee",
      "Segwang Kim",
      "Kyomin Jung"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.426": {
    "title": "Taxonomy Expansion for Named Entity Recognition",
    "volume": "main",
    "abstract": "Training a Named Entity Recognition (NER) model often involves fixing a taxonomy of entity types. However, requirements evolve and we might need the NER model to recognize additional entity types. A simple approach is to re-annotate entire dataset with both existing and additional entity types and then train the model on the re-annotated dataset. However, this is an extremely laborious task. To remedy this, we propose a novel approach called Partial Label Model (PLM) that uses only partially annotated datasets. We experiment with 6 diverse datasets and show that PLM consistently performs better than most other approaches (0.5 - 2.5 F1), including in novel settings for taxonomy expansion not considered in prior work. The gap between PLM and all other approaches is especially large in settings where there is limited data available for the additional entity types (as much as 11 F1), thus suggesting a more cost effective approaches to taxonomy expansion",
    "checked": true,
    "id": "c08daa24894dc21154b4dbc4370b1e1b6fb59f20",
    "semantic_title": "taxonomy expansion for named entity recognition",
    "citation_count": 0,
    "authors": [
      "Karthikeyan K",
      "Yogarshi Vyas",
      "Jie Ma",
      "Giovanni Paolini",
      "Neha John",
      "Shuai Wang",
      "Yassine Benajiba",
      "Vittorio Castelli",
      "Dan Roth",
      "Miguel Ballesteros"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.427": {
    "title": "Rather a Nurse than a Physician - Contrastive Explanations under Investigation",
    "volume": "main",
    "abstract": "Contrastive explanations, where one decision is explained *in contrast to another*, are supposed to be closer to how humans explain a decision than non-contrastive explanations, where the decision is not necessarily referenced to an alternative. This claim has never been empirically validated. We analyze four English text-classification datasets (SST2, DynaSent, BIOS and DBpedia-Animals). We fine-tune and extract explanations from three different models (RoBERTa, GTP-2, and T5), each in three different sizes and apply three post-hoc explainability methods (LRP, GradientxInput, GradNorm). We furthermore collect and release human rationale annotations for a subset of 100 samples from the BIOS dataset for contrastive and non-contrastive settings. A cross-comparison between model-based rationales and human annotations, both in contrastive and non-contrastive settings, yields a high agreement between the two settings for models as well as for humans. Moreover, model-based explanations computed in both settings align equally well with human rationales. Thus, we empirically find that humans do not necessarily explain in a contrastive manner",
    "checked": true,
    "id": "b940088cc1fb62459ef1a848aa6658a67502b757",
    "semantic_title": "rather a nurse than a physician - contrastive explanations under investigation",
    "citation_count": 1,
    "authors": [
      "Oliver Eberle",
      "Ilias Chalkidis",
      "Laura Cabello",
      "Stephanie Brandl"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.428": {
    "title": "EtiCor: Corpus for Analyzing LLMs for Etiquettes",
    "volume": "main",
    "abstract": "Etiquettes are an essential ingredient of day-to-day interactions among people. Moreover, etiquettes are region-specific, and etiquettes in one region might contradict those in other regions. In this paper, we propose EtiCor, an Etiquettes Corpus, having texts about social norms from five different regions across the globe. The corpus provides a test bed for evaluating LLMs for knowledge and understanding of region-specific etiquettes. Additionally, we propose the task of Etiquette Sensitivity. We experiment with state-of-the-art LLMs (Delphi, Falcon40B, and GPT-3.5). Initial results indicate that LLMs, mostly fail to understand etiquettes from regions from non-Western world",
    "checked": true,
    "id": "8dd3fc5ecb04e6ae78aacb06fb71c5deddae9020",
    "semantic_title": "eticor: corpus for analyzing llms for etiquettes",
    "citation_count": 0,
    "authors": [
      "Ashutosh Dwivedi",
      "Pradhyumna Lavania",
      "Ashutosh Modi"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.429": {
    "title": "An Investigation of LLMs' Inefficacy in Understanding Converse Relations",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have achieved remarkable success in many formal language oriented tasks, such as structural data-to-text and semantic parsing. However current benchmarks mostly follow the data distribution of the pre-training data of LLMs. Therefore, a natural question rises that do LLMs really understand the structured semantics of formal languages. In this paper, we investigate this problem on a special case, converse binary relation. We introduce a new benchmark ConvRe focusing on converse relations, which contains 17 relations and 1240 triples extracted from popular knowledge graph completion datasets. Our ConvRE features two tasks, Re2Text and Text2Re, which are formulated as multi-choice question answering to evaluate LLMs' ability to determine the matching between relations and associated text. For the evaluation protocol, apart from different prompting methods, we further introduce variants to the test text and few-shot example text. We conduct experiments on three popular LLM families and have observed various scaling trends. The results suggest that LLMs often resort to shortcut learning and still face challenges on our proposed benchmark",
    "checked": true,
    "id": "7fff3df60b8d9a5e57627f4eb6b3022b60f3a39d",
    "semantic_title": "an investigation of llms' inefficacy in understanding converse relations",
    "citation_count": 0,
    "authors": [
      "Chengwen Qi",
      "Bowen Li",
      "Binyuan Hui",
      "Bailin Wang",
      "Jinyang Li",
      "Jinwang Wu",
      "Yuanjun Laili"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.430": {
    "title": "Towards Low-Resource Automatic Program Repair with Meta-Learning and Pretrained Language Models",
    "volume": "main",
    "abstract": "Automatic program repair (APR) has gained increasing attention as an essential technique in software development to reduce manual debugging efforts and boost developers' productivity. Recent advances in deep learning (DL) based models have demonstrated promising results by learning from large-scale bug-fix examples in a data-driven manner. However, in practical scenarios, software bugs have an imbalanced distribution, and the fixing knowledge learned by APR models often only capture the patterns of frequent error types, making it inapplicable to handle the rare error types. To address this limitation, we investigate a novel task of low-resource APR, and propose Meta-APR, a new meta-learning framework integrated with code pretrained language models to generate fixes for low-resource bugs with limited training samples. Our Meta-APR learns better error-specific knowledge from high-resource bugs through efficient first-order meta-learning optimization, which allows for a faster adaptation to the target low-resource bugs. Besides, while we adopt CodeT5, a pretrained code-aware encoder-decoder Transformer, as the backbone model for Meta-APR, it is a model-agnostic framework that can be integrated with any neural models. Extensive experimental results on three benchmarks in various programming languages verify the superiority of our method over existing DL-based APR approaches",
    "checked": true,
    "id": "321cb42c7b97755b19cf7fae43fb2cb6b349841d",
    "semantic_title": "towards low-resource automatic program repair with meta-learning and pretrained language models",
    "citation_count": 0,
    "authors": [
      "Weishi Wang",
      "Yue Wang",
      "Steven Hoi",
      "Shafiq Joty"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.431": {
    "title": "ZGUL: Zero-shot Generalization to Unseen Languages using Multi-source Ensembling of Language Adapters",
    "volume": "main",
    "abstract": "We tackle the problem of zero-shot cross-lingual transfer in NLP tasks via the use of language adapters (LAs). Most of the earlier works have explored training with adapter of a single source (often English), and testing either using the target LA or LA of another related language. Training target LA requires unlabeled data, which may not be readily available for low resource *unseen* languages: those that are neither seen by the underlying multilingual language model (e.g., mBERT), nor do we have any (labeled or unlabeled) data for them. We posit that for more effective cross-lingual transfer, instead of just one source LA, we need to leverage LAs of multiple (linguistically or geographically related) source languages, both at train and test-time - which we investigate via our novel neural architecture, ZGUL. Extensive experimentation across four language groups, covering 15 unseen target languages, demonstrates improvements of up to 3.2 average F1 points over standard fine-tuning and other strong baselines on POS tagging and NER tasks. We also extend ZGUL to settings where either (1) some unlabeled data or (2) few-shot training examples are available for the target language. We find that ZGUL continues to outperform baselines in these settings too",
    "checked": true,
    "id": "2aa6dda808680f188bd20a4d2326aa87a126f4de",
    "semantic_title": "zgul: zero-shot generalization to unseen languages using multi-source ensembling of language adapters",
    "citation_count": 0,
    "authors": [
      "Vipul Rathore",
      "Rajdeep Dhingra",
      "Parag Singla",
      "Mausam"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.432": {
    "title": "Log-FGAER: Logic-Guided Fine-Grained Address Entity Recognition from Multi-Turn Spoken Dialogue",
    "volume": "main",
    "abstract": "Fine-grained address entity recognition (FGAER) from multi-turn spoken dialogues is particularly challenging. The major reason lies in that a full address is often formed through a conversation process. Different parts of an address are distributed through multiple turns of a dialogue with spoken noises. It is nontrivial to extract by turn and combine them. This challenge has not been well emphasized by main-stream entity extraction algorithms. To address this issue, we propose in this paper a logic-guided fine-grained address recognition method (Log-FGAER), where we formulate the address hierarchy relationship as the logic rule and softly apply it in a probabilistic manner to improve the accuracy of FGAER. In addition, we provide an ontology-based data augmentation methodology that employs ChatGPT to augment a spoken dialogue dataset with labeled address entities. Experiments are conducted using datasets generated by the proposed data augmentation technique and derived from real-world scenarios. The results of the experiment demonstrate the efficacy of our proposal",
    "checked": true,
    "id": "434459123fb184b215528cbbc8afe88d0fa79b3a",
    "semantic_title": "log-fgaer: logic-guided fine-grained address entity recognition from multi-turn spoken dialogue",
    "citation_count": 0,
    "authors": [
      "Xue Han",
      "Yitong Wang",
      "Qian Hu",
      "Pengwei Hu",
      "Chao Deng",
      "Junlan Feng"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.433": {
    "title": "Unified Low-Resource Sequence Labeling by Sample-Aware Dynamic Sparse Finetuning",
    "volume": "main",
    "abstract": "Unified Sequence Labeling that articulates different sequence labeling problems such as Named Entity Recognition, Relation Extraction, Semantic Role Labeling, etc. in a generalized sequence-to-sequence format opens up the opportunity to make the maximum utilization of large language model knowledge toward structured prediction. Unfortunately, this requires formatting them into specialized augmented format unknown to the base pretrained language model (PLMs) necessitating finetuning to the target format. This significantly bounds its usefulness in data-limited settings where finetuning large models cannot properly generalize to the target format. To address this challenge and leverage PLM knowledge effectively, we propose FISH-DIP, a sample-aware dynamic sparse finetuning strategy that selectively focuses on a fraction of parameters, informed by feedback from highly regressing examples, during the fine-tuning process. By leveraging the dynamism of sparsity, our approach mitigates the impact of well-learned samples and prioritizes underperforming instances for improvement in generalization. Across five tasks of sequence labeling, we demonstrate that FISH-DIP can smoothly optimize the model in low resource settings offering upto 40% performance improvements over full fine-tuning depending on target evaluation settings. Also, compared to in-context learning and other parameter-efficient fine-tuning approaches, FISH-DIP performs comparably or better, notably in extreme low-resource settings. The source code of FISH-DIP will be available at [this URL](https://github.com/psunlpgroup/FISH-DIP)",
    "checked": true,
    "id": "9218965ff29c4747033c26098c598ee1e9bfa8e1",
    "semantic_title": "unified low-resource sequence labeling by sample-aware dynamic sparse finetuning",
    "citation_count": 0,
    "authors": [
      "Sarkar Snigdha Sarathi Das",
      "Haoran Zhang",
      "Peng Shi",
      "Wenpeng Yin",
      "Rui Zhang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.434": {
    "title": "On the Representational Capacity of Recurrent Neural Language Models",
    "volume": "main",
    "abstract": "This work investigates the computational expressivity of language models (LMs) based on recurrent neural networks (RNNs). Siegelmann and Sontag (1992) famously showed that RNNs with rational weights and hidden states and unbounded computation time are Turing complete. However, LMs define weightings over strings in addition to just (unweighted) language membership and the analysis of the computational power of RNN LMs (RLMs) should reflect this. We extend the Turing completeness result to the probabilistic case, showing how a rationally weighted RLM with unbounded computation time can simulate any deterministic probabilistic Turing machine (PTM) with rationally weighted transitions. Since, in practice, RLMs work in real-time, processing a symbol at every time step, we treat the above result as an upper bound on the expressivity of RLMs. We also provide a lower bound by showing that under the restriction to real-time computation, such models can simulate deterministic real-time rational PTMs",
    "checked": true,
    "id": "c169e5a40f4565693224b661370e85be940d49a4",
    "semantic_title": "on the representational capacity of recurrent neural language models",
    "citation_count": 0,
    "authors": [
      "Franz Nowak",
      "Anej Svete",
      "Li Du",
      "Ryan Cotterell"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.435": {
    "title": "A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis",
    "volume": "main",
    "abstract": "Mathematical reasoning in large language models (LMs) has garnered significant attention in recent work, but there is a limited understanding of how these models process and store information related to arithmetic tasks within their architecture. In order to improve our understanding of this aspect of language models, we present a mechanistic interpretation of Transformer-based LMs on arithmetic questions using a causal mediation analysis framework. By intervening on the activations of specific model components and measuring the resulting changes in predicted probabilities, we identify the subset of parameters responsible for specific predictions. This provides insights into how information related to arithmetic is processed by LMs. Our experimental results indicate that LMs process the input by transmitting the information relevant to the query from mid-sequence early layers to the final token using the attention mechanism. Then, this information is processed by a set of MLP modules, which generate result-related information that is incorporated into the residual stream. To assess the specificity of the observed activation dynamics, we compare the effects of different model components on arithmetic queries with other tasks, including number retrieval from prompts and factual knowledge questions",
    "checked": true,
    "id": "5dc15ac1c92ab7492f121471823fb13a95d273ba",
    "semantic_title": "a mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis",
    "citation_count": 1,
    "authors": [
      "Alessandro Stolfo",
      "Yonatan Belinkov",
      "Mrinmaya Sachan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.436": {
    "title": "Benchmarking and Improving Text-to-SQL Generation under Ambiguity",
    "volume": "main",
    "abstract": "Research in Text-to-SQL conversion has been largely benchmarked against datasets where each text query corresponds to one correct SQL. However, natural language queries over real-life databases frequently involve significant ambiguity about the intended SQL due to overlapping schema names and multiple confusing relationship paths. To bridge this gap, we develop a novel benchmark called AmbiQT with over 3000 examples where each text is interpretable as two plausible SQLs due to lexical and/or structural ambiguity. When faced with ambiguity, an ideal top-k decoder should generate all valid interpretations for possible disambiguation by the user. We evaluate several Text-to-SQL systems and decoding algorithms, including those employing state-of-the-art LLMs, and find them to be far from this ideal. The primary reason is that the prevalent beam search algorithm and its variants, treat SQL queries as a string and produce unhelpful token-level diversity in the top-k. We propose LogicalBeam, a new decoding algorithm that navigates the SQL logic space using a blend of plan-based template generation and constrained infilling. Counterfactually generated plans diversify templates while in-filling with a beam-search that branches solely on schema names provides value diversity. LogicalBeam is up to 2.5 times more effective than state-of-the-art models at generating all candidate SQLs in the top-k ranked outputs. It also enhances the top-5 Exact and Execution Match Accuracies on SPIDER and Kaggle DBQA",
    "checked": true,
    "id": "65dabd1ff4f1bc17ea26fe4d20a383407626df93",
    "semantic_title": "benchmarking and improving text-to-sql generation under ambiguity",
    "citation_count": 1,
    "authors": [
      "Adithya Bhaskar",
      "Tushar Tomar",
      "Ashutosh Sathe",
      "Sunita Sarawagi"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.437": {
    "title": "Non-autoregressive Text Editing with Copy-aware Latent Alignments",
    "volume": "main",
    "abstract": "Recent work has witnessed a paradigm shift from Seq2Seq to Seq2Edit in the field of text editing, with the aim of addressing the slow autoregressive inference problem posed by the former. Despite promising results, Seq2Edit approaches still face several challenges such as inflexibility in generation and difficulty in generalizing to other languages. In this work, we propose a novel non-autoregressive text editing method to circumvent the above issues, by modeling the edit process with latent CTC alignments. We make a crucial extension to CTC by introducing the copy operation into the edit space, thus enabling more efficient management of textual overlap in editing. We conduct extensive experiments on GEC and sentence fusion tasks, showing that our proposed method significantly outperforms existing Seq2Edit models and achieves similar or even better results than Seq2Seq with over 4× speedup. Moreover, it demonstrates good generalizability on German and Russian. In-depth analyses reveal the strengths of our method in terms of the robustness under various scenarios and generating fluent and flexible outputs",
    "checked": true,
    "id": "116277fd27c97d50bba2d8023d3c590c1ea8187b",
    "semantic_title": "non-autoregressive text editing with copy-aware latent alignments",
    "citation_count": 2,
    "authors": [
      "Yu Zhang",
      "Yue Zhang",
      "Leyang Cui",
      "Guohong Fu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.438": {
    "title": "Translating away Translationese without Parallel Data",
    "volume": "main",
    "abstract": "Translated texts exhibit systematic linguistic differences compared to original texts in the same language, and these differences are referred to as translationese. Translationese has effects on various cross-lingual natural language processing tasks, potentially leading to biased results. In this paper, we explore a novel approach to reduce translationese in translated texts: translation-based style transfer. As there are no parallel human-translated and original data in the same language, we use a self-supervised approach that can learn from comparable (rather than parallel) mono-lingual original and translated data. However, even this self-supervised approach requires some parallel data for validation. We show how we can eliminate the need for parallel validation data by combining the self-supervised loss with an unsupervised loss. This unsupervised loss leverages the original language model loss over the style-transferred output and a semantic similarity loss between the input and style-transferred output. We evaluate our approach in terms of original vs. translationese binary classification in addition to measuring content preservation and target-style fluency. The results show that our approach is able to reduce translationese classifier accuracy to a level of a random classifier after style transfer while adequately preserving the content and fluency in the target original style",
    "checked": true,
    "id": "ffad62c074efa712981b10539218a04e26f02458",
    "semantic_title": "translating away translationese without parallel data",
    "citation_count": 0,
    "authors": [
      "Rricha Jalota",
      "Koel Chowdhury",
      "Cristina España-Bonet",
      "Josef van Genabith"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.439": {
    "title": "Prompt-Based Monte-Carlo Tree Search for Goal-oriented Dialogue Policy Planning",
    "volume": "main",
    "abstract": "Planning for goal-oriented dialogue often requires simulating future dialogue interactions and estimating task progress. Many approaches thus consider training neural networks to perform look-ahead search algorithms such as A* search and Monte Carlo Tree Search (MCTS). However, this training often require abundant annotated data, which creates challenges when faced with noisy annotations or low-resource settings. We introduce GDP-Zero, an approach using Open-Loop MCTS to perform goal-oriented dialogue policy planning without any model training. GDP-Zero prompts a large language model to act as a policy prior, value function, user simulator, and system model during the tree search. We evaluate GDP-Zero on the goal-oriented task PersuasionForGood, and find that its responses are preferred over ChatGPT up to 59.32% of the time, and are rated more persuasive than ChatGPT during interactive evaluations",
    "checked": true,
    "id": "9573e2025440219a1d3393664b3c80bda51ac8f4",
    "semantic_title": "prompt-based monte-carlo tree search for goal-oriented dialogue policy planning",
    "citation_count": 3,
    "authors": [
      "Xiao Yu",
      "Maximillian Chen",
      "Zhou Yu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.440": {
    "title": "UniMath: A Foundational and Multimodal Mathematical Reasoner",
    "volume": "main",
    "abstract": "While significant progress has been made in natural language processing (NLP), existing methods exhibit limitations in effectively interpreting and processing diverse mathematical modalities. Therefore, we introduce UniMath, a versatile and unified system designed for multimodal mathematical reasoning tasks. Tackling complex problem-solving in arithmetic, geometry, and table-based math, UniMath utilizes a fine-tuned T5 model augmented with a variational autoencoder (VAE)-based image tokenizer. By jointly training and evaluating the model on three diverse datasets - SVAMP, GeoQA, and TableMWP, UniMath achieves state-of-the-art performance. The model's generalization ability is further demonstrated via fine-tuning on two additional datasets, MathQA and Geo-Proving. Through comprehensive evaluations, we showcase that joint training across diverse math tasks improves overall model performance and enhances its ability to generalize across different mathematical reasoning tasks. This pioneering approach provides a blueprint and inspires further efforts on unified mathematical reasoning with deep learning systems",
    "checked": true,
    "id": "bbb9d3c982a2a15b481e51bc0302a967818b6448",
    "semantic_title": "unimath: a foundational and multimodal mathematical reasoner",
    "citation_count": 0,
    "authors": [
      "Zhenwen Liang",
      "Tianyu Yang",
      "Jipeng Zhang",
      "Xiangliang Zhang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.441": {
    "title": "CaseEncoder: A Knowledge-enhanced Pre-trained Model for Legal Case Encoding",
    "volume": "main",
    "abstract": "Legal case retrieval is a critical process for modern legal information systems. While recent studies have utilized pre-trained language models (PLMs) based on the general domain self-supervised pre-training paradigm to build models for legal case retrieval, there are limitations in using general domain PLMs as backbones. Specifically, these models may not fully capture the underlying legal features in legal case documents. To address this issue, we propose CaseEncoder, a legal document encoder that leverages fine-grained legal knowledge in both the data sampling and pre-training phases. In the data sampling phase, we enhance the quality of the training data by utilizing fine-grained law article information to guide the selection of positive and negative examples. In the pre-training phase, we design legal-specific pre-training tasks that align with the judging criteria of relevant legal cases. Based on these tasks, we introduce an innovative loss function called Biased Circle Loss to enhance the model's ability to recognize case relevance in fine grains. Experimental results on multiple benchmarks demonstrate that CaseEncoder significantly outperforms both existing general pre-training models and legal-specific pre-training models in zero-shot legal case retrieval. The source code of CaseEncoder can be found at https://github.com/Anonymous-EMNLP2023/CaseEncoder",
    "checked": true,
    "id": "8d3db64debad96747b2ad1020a8ba8cba81620a5",
    "semantic_title": "caseencoder: a knowledge-enhanced pre-trained model for legal case encoding",
    "citation_count": 1,
    "authors": [
      "Yixiao Ma",
      "Yueyue Wu",
      "Weihang Su",
      "Qingyao Ai",
      "Yiqun Liu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.442": {
    "title": "HiddenTables and PyQTax: A Cooperative Game and Dataset For TableQA to Ensure Scale and Data Privacy Across a Myriad of Taxonomies",
    "volume": "main",
    "abstract": "A myriad of different Large Language Models (LLMs) face a common challenge in contextually analyzing table question-answering tasks. These challenges are engendered from (1) finite context windows for large tables, (2) multi-faceted discrepancies amongst tokenization patterns against cell boundaries, and (3) various limitations stemming from data confidentiality in the process of using external models such as gpt-35-turbo. We propose a cooperative game dubbed \"HiddenTables\" as a potential resolution to this challenge. In essence, \"HiddenTables\" is played between the code-generating LLM \"Solver\" and the \"Oracle\" which evaluates the ability of the LLM agents to solve TableQA tasks. This game is based on natural language schemas and importantly, ensures the security of the underlying data. We provide evidential experiments on a diverse set of tables that demonstrate an LLM's collective inability to generalize and perform on complex queries, handle compositional dependencies, and align natural language to programmatic commands when concrete table schemas are provided. Unlike encoder-based models, we have pushed the boundaries of \"HiddenTables\" to not be limited by the number of rows - therefore we exhibit improved efficiency in prompt and completion tokens. Our infrastructure has spawned a new dataset \"PyQTax\" that spans across 116,671 question-table-answer triplets and provides additional fine-grained breakdowns and labels for varying question taxonomies. Therefore, in tandem with our academic contributions regarding LLMs' deficiency in TableQA tasks, \"HiddenTables\" is a tactile manifestation of how LLMs can interact with massive datasets while ensuring data security and minimizing generation costs",
    "checked": true,
    "id": "65158f0949f5f869f90afe4653df911208b40a54",
    "semantic_title": "hiddentables and pyqtax: a cooperative game and dataset for tableqa to ensure scale and data privacy across a myriad of taxonomies",
    "citation_count": 0,
    "authors": [
      "William Watson",
      "Nicole Cho",
      "Tucker Balch",
      "Manuela Veloso"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.443": {
    "title": "Causal Document-Grounded Dialogue Pre-training",
    "volume": "main",
    "abstract": "The goal of document-grounded dialogue (DocGD) is to generate a response by anchoring the evidence in a supporting document in accordance with the dialogue context. This entails four causally interconnected variables. While task-specific pre-training has significantly enhanced performances on numerous downstream tasks, existing DocGD methods still rely on general pre-trained language models without a specifically tailored pre-training approach that explicitly captures the causal relationships. To address this, we present the first causally-complete dataset construction strategy for developing million-scale DocGD pre-training corpora. Additionally, we propose a causally-perturbed pre-training strategy to better capture causality by introducing perturbations on the variables and optimizing the overall causal effect. Experiments conducted on three benchmark datasets demonstrate that our causal pre-training yields substantial and consistent improvements in fully-supervised, low-resource, few-shot, and zero-shot settings",
    "checked": true,
    "id": "44a5a031c2487bca581a8cfc8013a026200aa6b0",
    "semantic_title": "causal document-grounded dialogue pre-training",
    "citation_count": 2,
    "authors": [
      "Yingxiu Zhao",
      "Bowen Yu",
      "Bowen Li",
      "Haiyang Yu",
      "Jinyang Li",
      "Chao Wang",
      "Fei Huang",
      "Yongbin Li",
      "Nevin Zhang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.444": {
    "title": "Accented Speech Recognition With Accent-specific Codebooks",
    "volume": "main",
    "abstract": "Speech accents pose a significant challenge to state-of-the-art automatic speech recognition (ASR) systems. Degradation in performance across underrepresented accents is a severe deterrent to the inclusive adoption of ASR. In this work, we propose a novel accent adaptation approach for end-to-end ASR systems using cross-attention with a trainable set of codebooks. These learnable codebooks capture accent-specific information and are integrated within the ASR encoder layers. The model is trained on accented English speech, while the test data also contained accents which were not seen during training. On the Mozilla Common Voice multi-accented dataset, we show that our proposed approach yields significant performance gains not only on the seen English accents (up to 37% relative improvement in word error rate) but also on the unseen accents (up to 5% relative improvement in WER). Further, we illustrate benefits for a zero-shot transfer setup on the L2Artic dataset. We also compare the performance with other approaches based on accent adversarial training",
    "checked": true,
    "id": "31c428b55cf8a7a001bf99164f935d4395fd462a",
    "semantic_title": "accented speech recognition with accent-specific codebooks",
    "citation_count": 0,
    "authors": [
      "Darshan Prabhu",
      "Preethi Jyothi",
      "Sriram Ganapathy",
      "Vinit Unni"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.445": {
    "title": "Linking Surface Facts to Large-Scale Knowledge Graphs",
    "volume": "main",
    "abstract": "Open Information Extraction (OIE) methods extract facts from natural language text in the form of (\"subject\"; \"relation\"; \"object\") triples. These facts are, however, merely surface forms, the ambiguity of which impedes their downstream usage; e.g., the surface phrase \"Michael Jordan\" may refer to either the former basketball player or the university professor. Knowledge Graphs (KGs), on the other hand, contain facts in a canonical (i.e., unambiguous) form, but their coverage is limited by a static schema (i.e., a fixed set of entities and predicates). To bridge this gap, we need the best of both worlds: (i) high coverage of free-text OIEs, and (ii) semantic precision (i.e., monosemy) of KGs. In order to achieve this goal, we propose a new benchmark with novel evaluation protocols that can, for example, measure fact linking performance on a granular triple slot level, while also measuring if a system has the ability to recognize that a surface form has no match in the existing KG. Our extensive evaluation of several baselines show that detection of out-of-KG entities and predicates is more difficult than accurate linking to existing ones, thus calling for more research efforts on this difficult task. We publicly release all resources (data, benchmark and code) on https://github.com/nec-research/fact-linking",
    "checked": true,
    "id": "59f3b696530ebe1599e7f653f175a95349babd89",
    "semantic_title": "linking surface facts to large-scale knowledge graphs",
    "citation_count": 0,
    "authors": [
      "Gorjan Radevski",
      "Kiril Gashteovski",
      "Chia-Chien Hung",
      "Carolin Lawrence",
      "Goran Glavaš"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.446": {
    "title": "Sentiment Analysis on Streaming User Reviews via Dual-Channel Dynamic Graph Neural Network",
    "volume": "main",
    "abstract": "Sentiment analysis on user reviews has achieved great success thanks to the rapid growth of deep learning techniques. The large number of online streaming reviews also provides the opportunity to model temporal dynamics for users and products on the timeline. However, existing methods model users and products in the real world based on a static assumption and neglect their time-varying characteristics. In this paper, we present DC-DGNN, a dual-channel framework based on a dynamic graph neural network (DGNN) that models temporal user and product dynamics for sentiment analysis. Specifically, a dual-channel text encoder is employed to extract current local and global contexts from review documents for users and products. Moreover, user review streams are integrated into the dynamic graph neural network by treating users and products as nodes and reviews as new edges. Node representations are dynamically updated along with the evolution of the dynamic graph and used for the final score prediction. Experimental results on five real-world datasets demonstrate the superiority of the proposed method",
    "checked": true,
    "id": "1f96c917aeb732fabfae1d4fb440d73fa8ef78fd",
    "semantic_title": "sentiment analysis on streaming user reviews via dual-channel dynamic graph neural network",
    "citation_count": 0,
    "authors": [
      "Xin Zhang",
      "Linhai Zhang",
      "Deyu Zhou"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.447": {
    "title": "DUMB: A Dutch Model Benchmark",
    "volume": "main",
    "abstract": "We introduce the Dutch Model Benchmark: DUMB. The benchmark includes a diverse set of datasets for low-, medium- and high-resource tasks. The total set of nine tasks includes four tasks that were previously not available in Dutch. Instead of relying on a mean score across tasks, we propose Relative Error Reduction (RER), which compares the DUMB performance of language models to a strong baseline which can be referred to in the future even when assessing different sets of language models. Through a comparison of 14 pre-trained language models (mono- and multi-lingual, of varying sizes), we assess the internal consistency of the benchmark tasks, as well as the factors that likely enable high performance. Our results indicate that current Dutch monolingual models under-perform and suggest training larger Dutch models with other architectures and pre-training objectives. At present, the highest performance is achieved by DeBERTaV3 (large), XLM-R (large) and mDeBERTaV3 (base). In addition to highlighting best strategies for training larger Dutch models, DUMB will foster further research on Dutch. A public leaderboard is available at https://dumbench.nl",
    "checked": true,
    "id": "79d1db51e91949b5c3af7d6ddbd6d12c1fd4c233",
    "semantic_title": "dumb: a dutch model benchmark",
    "citation_count": 0,
    "authors": [
      "Wietse de Vries",
      "Martijn Wieling",
      "Malvina Nissim"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.448": {
    "title": "OssCSE: Overcoming Surface Structure Bias in Contrastive Learning for Unsupervised Sentence Embedding",
    "volume": "main",
    "abstract": "Contrastive learning has been demonstrated effective in unsupervised sentence representation learning. Given one sentence, positive pairs are obtained by passing the sentence to the encoder twice using the different dropout masks, and negative pairs are obtained by taking another sentence in the same mini-batch. However, the method suffers from the surface structure bias, i.e., sentences with similar surface structures will be regarded as close in semantics while sentences with dissimilar surface structures will be viewed as distinct in semantics. This leads to the result that paraphrasing a sentence that is dissimilar in surface structure will receive a lower semantic similarity score than inserting a negative word into the sentence. In this paper, we first verify the bias by collecting a sentence transformation testset. Then we systematically probe the existing models by proposing novel splits based on benchmark datasets in accordance with semantic and surface structure similarity. We tackle the bias in two aspects: balancing the learning target by augmenting with data that counters the bias, and meanwhile preserving word semantics by leveraging recall loss to prevent catastrophic forgetting. We evaluate our model on standard semantic textual similarity (STS) tasks using different pre-trained backbones and achieve state-of-the-art averaged performance across the STS benchmarks. Particularly, our models that are fine-tuned with RoBERTabase and RoBERTalarge achieve significantly better performance on most benchmark datasets",
    "checked": true,
    "id": "311b0a3fb00fcaf8a92d38cbabf877dbcbd6fff4",
    "semantic_title": "osscse: overcoming surface structure bias in contrastive learning for unsupervised sentence embedding",
    "citation_count": 0,
    "authors": [
      "Zhan Shi",
      "Guoyin Wang",
      "Ke Bai",
      "Jiwei Li",
      "Xiang Li",
      "Qingjun Cui",
      "Belinda Zeng",
      "Trishul Chilimbi",
      "Xiaodan Zhu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.449": {
    "title": "End-to-End Single-Channel Speaker-Turn Aware Conversational Speech Translation",
    "volume": "main",
    "abstract": "Conventional speech-to-text translation (ST) systems are trained on single-speaker utterances, and they may not generalize to real-life scenarios where the audio contains conversations by multiple speakers. In this paper, we tackle single-channel multi-speaker conversational ST with an end-to-end and multi-task training model, named Speaker-Turn Aware Conversational Speech Translation, that combines automatic speech recognition, speech translation and speaker turn detection using special tokens in a serialized labeling format. We run experiments on the Fisher-CALLHOME corpus, which we adapted by merging the two single-speaker channels into one multi-speaker channel, thus representing the more realistic and challenging scenario with multi-speaker turns and cross-talk. Experimental results across single- and multi-speaker conditions and against conventional ST systems, show that our model outperforms the reference systems on the multi-speaker condition, while attaining comparable performance on the single-speaker condition. We release scripts for data processing and model training",
    "checked": true,
    "id": "f915d13f534eeb3196bb1d67ddb8a8bb16bf7ed6",
    "semantic_title": "end-to-end single-channel speaker-turn aware conversational speech translation",
    "citation_count": 0,
    "authors": [
      "Juan Pablo Zuluaga-Gomez",
      "Zhaocheng Huang",
      "Xing Niu",
      "Rohit Paturi",
      "Sundararajan Srinivasan",
      "Prashant Mathur",
      "Brian Thompson",
      "Marcello Federico"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.450": {
    "title": "A Fine-Grained Taxonomy of Replies to Hate Speech",
    "volume": "main",
    "abstract": "Countering rather than censoring hate speech has emerged as a promising strategy to address hatred. There are many types of counterspeech in user-generated content: addressing the hateful content or its author, generic requests, well-reasoned counter arguments, insults, etc. The effectiveness of counterspeech, which we define as subsequent incivility, depends on these types. In this paper, we present a theoretically grounded taxonomy of replies to hate speech and a new corpus. We work with real, user-generated hate speech and all the replies it elicits rather than replies generated by a third party. Our analyses provide insights into the content real users reply with as well as which replies are empirically most effective. We also experiment with models to characterize the replies to hate speech, thereby opening the door to estimating whether a reply to hate speech will result in further incivility",
    "checked": true,
    "id": "f6e765676b474aac5e26f8b7ea9f8a4cf65ac22c",
    "semantic_title": "a fine-grained taxonomy of replies to hate speech",
    "citation_count": 0,
    "authors": [
      "Xinchen Yu",
      "Ashley Zhao",
      "Eduardo Blanco",
      "Lingzi Hong"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.451": {
    "title": "JointMatch: A Unified Approach for Diverse and Collaborative Pseudo-Labeling to Semi-Supervised Text Classification",
    "volume": "main",
    "abstract": "Semi-supervised text classification (SSTC) has gained increasing attention due to its ability to leverage unlabeled data. However, existing approaches based on pseudo-labeling suffer from the issues of pseudo-label bias and error accumulation. In this paper, we propose JointMatch, a holistic approach for SSTC that addresses these challenges by unifying ideas from recent semi-supervised learning and the task of learning with noise. JointMatch adaptively adjusts classwise thresholds based on the learning status of different classes to mitigate model bias towards current easy classes. Additionally, JointMatch alleviates error accumulation by utilizing two differently initialized networks to teach each other in a cross-labeling manner. To maintain divergence between the two networks for mutual learning, we introduce a strategy that weighs more disagreement data while also allowing the utilization of high-quality agreement data for training. Experimental results on benchmark datasets demonstrate the superior performance of JointMatch, achieving a significant 5.13% improvement on average. Notably, JointMatch delivers impressive results even in the extremely-scarce-label setting, obtaining 86% accuracy on AG News with only 5 labels per class. We make our code available at https://github.com/HenryPengZou/JointMatch",
    "checked": true,
    "id": "89a1ca2bdc57f9911c240155a4f903a45a653efb",
    "semantic_title": "jointmatch: a unified approach for diverse and collaborative pseudo-labeling to semi-supervised text classification",
    "citation_count": 1,
    "authors": [
      "Henry Zou",
      "Cornelia Caragea"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.452": {
    "title": "Simple Temporal Adaptation to Changing Label Sets: Hashtag Prediction via Dense KNN",
    "volume": "main",
    "abstract": "User-generated social media data is constantly changing as new trends influence online discussion and personal information is deleted due to privacy concerns. However, traditional NLP models rely on fixed training datasets, which means they are unable to adapt to temporal change—both test distribution shift and deleted training data—without frequent, costly re-training. In this paper, we study temporal adaptation through the task of longitudinal hashtag prediction and propose a non-parametric dense retrieval technique, which does not require re-training, as a simple but effective solution. In experiments on a newly collected, publicly available, year-long Twitter dataset exhibiting temporal distribution shift, our method improves by 64% over the best static parametric baseline while avoiding costly gradient-based re-training. Our approach is also particularly well-suited to dynamically deleted user data in line with data privacy laws, with negligible computational cost/performance loss",
    "checked": true,
    "id": "c817d9901f788e40279c76068ea2622365cd9a1d",
    "semantic_title": "simple temporal adaptation to changing label sets: hashtag prediction via dense knn",
    "citation_count": 0,
    "authors": [
      "Niloofar Mireshghallah",
      "Nikolai Vogler",
      "Junxian He",
      "Omar Florez",
      "Ahmed El-Kishky",
      "Taylor Berg-Kirkpatrick"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.453": {
    "title": "Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4",
    "volume": "main",
    "abstract": "In this work, we carry out a data archaeology to infer books that are known to ChatGPT and GPT-4 using a name cloze membership inference query. We find that OpenAI models have memorized a wide collection of copyrighted materials, and that the degree of memorization is tied to the frequency with which passages of those books appear on the web. The ability of these models to memorize an unknown set of books complicates assessments of measurement validity for cultural analytics by contaminating test data; we show that models perform much better on memorized books than on non-memorized books for downstream tasks. We argue that this supports a case for open models whose training data is known",
    "checked": true,
    "id": "c47f0a5feb18b036004b5404ef78ac94a65fa489",
    "semantic_title": "speak, memory: an archaeology of books known to chatgpt/gpt-4",
    "citation_count": 28,
    "authors": [
      "Kent Chang",
      "Mackenzie Cramer",
      "Sandeep Soni",
      "David Bamman"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.454": {
    "title": "A Study on Accessing Linguistic Information in Pre-Trained Language Models by Using Prompts",
    "volume": "main",
    "abstract": "We study whether linguistic information in pre-trained multilingual language models can be accessed by human language: So far, there is no easy method to directly obtain linguistic information and gain insights into the linguistic principles encoded in such models. We use the technique of prompting and formulate linguistic tasks to test the LM's access to explicit grammatical principles and study how effective this method is at providing access to linguistic features. Our experiments on German, Icelandic and Spanish show that some linguistic properties can in fact be accessed through prompting, whereas others are harder to capture",
    "checked": true,
    "id": "6fa5aacf565f86541972521a7cc7dded86caeb7b",
    "semantic_title": "a study on accessing linguistic information in pre-trained language models by using prompts",
    "citation_count": 0,
    "authors": [
      "Marion Di Marco",
      "Katharina Hämmerl",
      "Alexander Fraser"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.455": {
    "title": "CiteBench: A Benchmark for Scientific Citation Text Generation",
    "volume": "main",
    "abstract": "Science progresses by building upon the prior body of knowledge documented in scientific publications. The acceleration of research makes it hard to stay up-to-date with the recent developments and to summarize the ever-growing body of prior work. To address this, the task of citation text generation aims to produce accurate textual summaries given a set of papers-to-cite and the citing paper context. Due to otherwise rare explicit anchoring of cited documents in the citing paper, citation text generation provides an excellent opportunity to study how humans aggregate and synthesize textual knowledge from sources. Yet, existing studies are based upon widely diverging task definitions, which makes it hard to study this task systematically. To address this challenge, we propose CiteBench: a benchmark for citation text generation that unifies multiple diverse datasets and enables standardized evaluation of citation text generation models across task designs and domains. Using the new benchmark, we investigate the performance of multiple strong baselines, test their transferability between the datasets, and deliver new insights into the task definition and evaluation to guide future research in citation text generation. We make the code for CiteBench publicly available at https://github.com/UKPLab/citebench",
    "checked": true,
    "id": "f51ecc29f205468c736cea9acb5612b1a0a48ba9",
    "semantic_title": "citebench: a benchmark for scientific citation text generation",
    "citation_count": 3,
    "authors": [
      "Martin Funkquist",
      "Ilia Kuznetsov",
      "Yufang Hou",
      "Iryna Gurevych"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.456": {
    "title": "From Heuristic to Analytic: Cognitively Motivated Strategies for Coherent Physical Commonsense Reasoning",
    "volume": "main",
    "abstract": "Pre-trained language models (PLMs) have shown impressive performance in various language tasks. However, they are prone to spurious correlations, and often generate illusory information. In real-world applications, PLMs should justify decisions with formalized, coherent reasoning chains, but this challenge remains under-explored. Cognitive psychology theorizes that humans are capable of utilizing fast and intuitive *heuristic* thinking to make decisions based on past experience, then rationalizing the decisions through slower and deliberative *analytic* reasoning. We incorporate these interlinked dual processes in fine-tuning and in-context learning with PLMs, applying them to two language understanding tasks that require coherent physical commonsense reasoning. We show that our proposed Heuristic-Analytic Reasoning (HAR) strategies drastically improve the coherence of rationalizations for model decisions, yielding state-of-the-art results on Tiered Reasoning for Intuitive Physics (TRIP). We also find that this improved coherence is a direct result of more faithful attention to relevant language context in each step of reasoning. Our findings suggest that human-like reasoning strategies can effectively improve the coherence and reliability of PLM reasoning",
    "checked": true,
    "id": "a32a461ba94367157bd15c1bfa03a37a9449e5da",
    "semantic_title": "from heuristic to analytic: cognitively motivated strategies for coherent physical commonsense reasoning",
    "citation_count": 0,
    "authors": [
      "Zheyuan Zhang",
      "Shane Storks",
      "Fengyuan Hu",
      "Sungryull Sohn",
      "Moontae Lee",
      "Honglak Lee",
      "Joyce Chai"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.457": {
    "title": "A Challenging Multimodal Video Summary: Simultaneously Extracting and Generating Keyframe-Caption Pairs from Video",
    "volume": "main",
    "abstract": "This paper proposes a practical multimodal video summarization task setting and a dataset to train and evaluate the task. The target task involves summarizing a given video into a predefined number of keyframe-caption pairs and displaying them in a listable format to grasp the video content quickly. This task aims to extract crucial scenes from the video in the form of images (keyframes) and generate corresponding captions explaining each keyframe's situation. This task is useful as a practical application and presents a highly challenging problem worthy of study. Specifically, achieving simultaneous optimization of the keyframe selection performance and caption quality necessitates careful consideration of the mutual dependence on both preceding and subsequent keyframes and captions. To facilitate subsequent research in this field, we also construct a dataset by expanding upon existing datasets and propose an evaluation framework. Furthermore, we develop two baseline systems and report their respective performance",
    "checked": true,
    "id": "bfa1ce05e9a1d6e8b15451fe838fff88563c49b4",
    "semantic_title": "a challenging multimodal video summary: simultaneously extracting and generating keyframe-caption pairs from video",
    "citation_count": 0,
    "authors": [
      "Keito Kudo",
      "Haruki Nagasawa",
      "Jun Suzuki",
      "Nobuyuki Shimizu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.458": {
    "title": "Copyright Violations and Large Language Models",
    "volume": "main",
    "abstract": "Language models may memorize more than just facts, including entire chunks of texts seen during training. Fair use exemptions to copyright laws typically allow for limited use of copyrighted material without permission from the copyright holder, but typically for extraction of information from copyrighted materials, rather than verbatim reproduction. This work explores the issue of copyright violations and large language models through the lens of verbatim memorization, focusing on possible redistribution of copyrighted text. We present experiments with a range of language models over a collection of popular books and coding problems, providing a conservative characterization of the extent to which language models can redistribute these materials. Overall, this research highlights the need for further examination and the potential impact on future developments in natural language processing to ensure adherence to copyright regulations. Code is at https://github.com/coastalcph/CopyrightLLMs",
    "checked": true,
    "id": "ea6c0620f4d56faa76a8f99d8963ad77fa6fbdb8",
    "semantic_title": "copyright violations and large language models",
    "citation_count": 1,
    "authors": [
      "Antonia Karamolegkou",
      "Jiaang Li",
      "Li Zhou",
      "Anders Søgaard"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.459": {
    "title": "Effects of sub-word segmentation on performance of transformer language models",
    "volume": "main",
    "abstract": "Language modeling is a fundamental task in natural language processing, which has been thoroughly explored with various architectures and hyperparameters. However, few studies focus on the effect of sub-word segmentation on the performance of language models (LMs). In this paper, we compare GPT and BERT models trained with the statistical segmentation algorithm BPE vs. two unsupervised algorithms for morphological segmentation — Morfessor and StateMorph. We train the models for several languages — including ones with very rich morphology — and compare their performance with different segmentation algorithms, vocabulary sizes, and model sizes. The results show that training with morphological segmentation allows the LMs to: (1) achieve lower perplexity, (2) converge more efficiently in terms of training time, and (3) achieve equivalent or better evaluation scores on downstream tasks. Lastly, we show that (4) LMs of smaller size using morphological segmentation can perform comparably to models of larger size trained with BPE — both in terms of (1) perplexity and (3) scores on downstream tasks. Points (2) and (4) impact on sustainability, since they reduce the model cost; and while 2 reduces cost only in the training phase, 4 does so also in the inference phase",
    "checked": true,
    "id": "0842aba60d8fc1fc61ad95df34872c17bcfcc123",
    "semantic_title": "effects of sub-word segmentation on performance of transformer language models",
    "citation_count": 0,
    "authors": [
      "Jue Hou",
      "Anisia Katinskaia",
      "Anh-Duc Vu",
      "Roman Yangarber"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.460": {
    "title": "Symbolic Planning and Code Generation for Grounded Dialogue",
    "volume": "main",
    "abstract": "Large language models (LLMs) excel at processing and generating text and code. However, LLMs have had limited applicability in grounded task-oriented dialogue as they are difficult to steer toward task objectives and fail to handle novel grounding. We present a modular and interpretable grounded dialogue system that addresses these shortcomings by composing LLMs with a symbolic planner and grounded code execution. Our system, consists of a reader and planner: the reader leverages an LLM to convert partner utterances into executable code, calling functions that perform grounding. The translated code's output is stored to track dialogue state, while a symbolic planner determines the next appropriate response. We evaluate our system's performance on the demanding OneCommon dialogue task, involving collaborative reference resolution on abstract images of scattered dots. Our system substantially outperforms the previous state-of-the-art, including improving task success in human evaluations from 56% to 69% in the most challenging setting",
    "checked": true,
    "id": "5b0458d5e839117884739d4b9dead0285beb4a76",
    "semantic_title": "symbolic planning and code generation for grounded dialogue",
    "citation_count": 0,
    "authors": [
      "Justin Chiu",
      "Wenting Zhao",
      "Derek Chen",
      "Saujas Vaduguru",
      "Alexander Rush",
      "Daniel Fried"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.461": {
    "title": "Universal Self-Adaptive Prompting",
    "volume": "main",
    "abstract": "A hallmark of modern large language models (LLMs) is their impressive general zero-shot and few-shot abilities, often elicited through in-context learning (ICL) via prompting. However, while highly coveted and being the most general, zero-shot performances in LLMs are still typically weaker due to the lack of guidance and the difficulty of applying existing automatic prompt design methods in general tasks when ground-truth labels are unavailable. In this study, we address this by presenting Universal Self-Adaptive Prompting (USP), an automatic prompt design approach specifically tailored for zero-shot learning (while compatible with few-shot). Requiring only a small amount of unlabeled data and an inference-only LLM, USP is highly versatile: to achieve universal prompting, USP categorizes a possible NLP task into one of the three possible task types and then uses a corresponding selector to select the most suitable queries and zero-shot model-generated responses as pseudo-demonstrations, thereby generalizing ICL to the zero-shot setup in a fully automated way. We evaluate USP with PaLM and PaLM 2 models and demonstrate performances that are considerably stronger than standard zero-shot baselines and often comparable to or even superior to few-shot baselines across more than 40 natural language understanding, natural language generation, and reasoning tasks",
    "checked": true,
    "id": "930e86d49477c9d3305cd1f9d01b93749f85bb8b",
    "semantic_title": "universal self-adaptive prompting",
    "citation_count": 3,
    "authors": [
      "Xingchen Wan",
      "Ruoxi Sun",
      "Hootan Nakhost",
      "Hanjun Dai",
      "Julian Eisenschlos",
      "Sercan Arik",
      "Tomas Pfister"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.462": {
    "title": "Somali Information Retrieval Corpus: Bridging the Gap between Query Translation and Dedicated Language Resources",
    "volume": "main",
    "abstract": "Despite the growing use of the Somali language in various online domains, research on Somali language information retrieval remains limited and primarily relies on query translation due to the lack of a dedicated corpus. To address this problem, we collaborated with language experts and natural language processing (NLP) researchers to create an annotated corpus for Somali information retrieval. This corpus comprises 2335 documents collected from various well-known online sites, such as hiiraan online, dhacdo net, and Somali poetry books. We explain how the corpus was constructed, and develop a Somali language information retrieval system using a pseudo-relevance feedback (PRF) query expansion technique on the corpus. Note that collecting such a data set for the low-resourced Somali language can help overcome NLP barriers, such as the lack of electronically available data sets. Which, if available, can enable the development of various NLP tools and applications such as question-answering and text classification. It also provides researchers with a valuable resource for investigating and developing new techniques and approaches for Somali",
    "checked": true,
    "id": "a0f65c82222f9080a24ae5730394f0a56a431f49",
    "semantic_title": "somali information retrieval corpus: bridging the gap between query translation and dedicated language resources",
    "citation_count": 0,
    "authors": [
      "Abdisalam Badel",
      "Ting Zhong",
      "Wenxin Tai",
      "Fan Zhou"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.463": {
    "title": "Beat LLMs at Their Own Game: Zero-Shot LLM-Generated Text Detection via Querying ChatGPT",
    "volume": "main",
    "abstract": "Large language models (LLMs), e.g., ChatGPT, have revolutionized the domain of natural language processing because of their excellent performance on various tasks. Despite their great potential, LLMs also incur serious concerns as they are likely to be misused. There are already reported cases of academic cheating by using LLMs. Thus, it is a pressing problem to identify LLM-generated texts. In this work, we design a zero-shot black-box method for detecting LLM-generated texts. The key idea is to revise the text to be detected using the ChatGPT model. Our method is based on the intuition that the ChatGPT model will make fewer revisions to LLM-generated texts than it does to human-written texts, because the texts generated by LLMs are more in accord with the generation logic and statistical patterns learned by LLMs like ChatGPT. Thus, if the text to be detected and its ChatGPT-revised version have a higher degree of similarity, the text is more likely to be LLM-generated. Extensive experiments on various datasets and tasks show that our method can effectively detect LLM-generated texts. Moreover, compared with other detection methods, our method has better generalization ability and is more stable across various datasets. The codes are publicly available at https://github.com/thunlp/LLM-generated-text-detection",
    "checked": true,
    "id": "43707e84a8e0e2919a3e724eb59aa6a5465c5cb7",
    "semantic_title": "beat llms at their own game: zero-shot llm-generated text detection via querying chatgpt",
    "citation_count": 0,
    "authors": [
      "Biru Zhu",
      "Lifan Yuan",
      "Ganqu Cui",
      "Yangyi Chen",
      "Chong Fu",
      "Bingxiang He",
      "Yangdong Deng",
      "Zhiyuan Liu",
      "Maosong Sun",
      "Ming Gu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.464": {
    "title": "Faithful Model Evaluation for Model-Based Metrics",
    "volume": "main",
    "abstract": "Statistical significance testing is used in natural language processing (NLP) to determine whether the results of a study or experiment are likely to be due to chance or if they reflect a genuine relationship. A key step in significance testing is the estimation of confidence interval which is a function of sample variance. Sample variance calculation is straightforward when evaluating against ground truth. However, in many cases, a metric model is often used for evaluation. For example, to compare toxicity of two large language models, a toxicity classifier is used for evaluation. Existing works usually do not consider the variance change due to metric model errors, which can lead to wrong conclusions. In this work, we establish the mathematical foundation of significance testing for model-based metrics. With experiments on public benchmark datasets and a production system, we show that considering metric model errors to calculate sample variances for model-based metrics changes the conclusions in certain experiments",
    "checked": true,
    "id": "9327dbbbe8218cb5780e66f21d09bb7571984e17",
    "semantic_title": "faithful model evaluation for model-based metrics",
    "citation_count": 0,
    "authors": [
      "Qian Hu",
      "Palash Goyal",
      "Rahul Gupta"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.465": {
    "title": "Content- and Topology-Aware Representation Learning for Scientific Multi-Literature",
    "volume": "main",
    "abstract": "Representation learning forms an essential building block in the development of natural language processing architectures. To date, mainstream approaches focus on learning textual information at the sentence- or document-level, unfortunately, overlooking the inter-document connections. This omission decreases the potency of downstream applications, particularly in multi-document settings. To address this issue, embeddings equipped with latent semantic and rich relatedness information are needed. In this paper, we propose SMRC2, which extends representation learning to the multi-document level. Our model jointly learns latent semantic information from content and rich relatedness information from topological networks. Unlike previous studies, our work takes multi-document as input and integrates both semantic and relatedness information using a shared space via language model and graph structure. Our extensive experiments confirm the superiority and effectiveness of our approach. To encourage further research in scientific multi-literature representation learning, we will release our code and a new dataset from the biomedical domain",
    "checked": true,
    "id": "06290d160b871a26a884a1b6facdf431528a14fd",
    "semantic_title": "content- and topology-aware representation learning for scientific multi-literature",
    "citation_count": 0,
    "authors": [
      "Kai Zhang",
      "Kaisong Song",
      "Yangyang Kang",
      "Xiaozhong Liu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.466": {
    "title": "Language Model Quality Correlates with Psychometric Predictive Power in Multiple Languages",
    "volume": "main",
    "abstract": "Surprisal theory (Hale, 2001; Levy, 2008) posits that a word's reading time is proportional to its surprisal (i.e., to its negative log probability given the proceeding context). Since we are unable to access a word's ground-truth probability, surprisal theory has been empirically tested using surprisal estimates from language models (LMs). Under the premise that surprisal theory holds, we would expect that higher quality language models provide more powerful predictors of human reading behavior—a conjecture we dub the quality–power (QP) hypothesis. Unfortunately, empirical support for the QP hypothesis is mixed. Some studies in English have found correlations between LM quality and predictive power, but other studies using Japanese data, as well as using larger English LMs, find no such correlations. In this work, we conduct a systematic crosslinguistic assessment of the QP hypothesis. We train LMs from scratch on small- and medium-sized datasets from 13 languages (across five language families) and assess their ability to predict eye tracking data. We find correlations between LM quality and power in eleven of these thirteen languages, suggesting that, within the range of model classes and sizes tested, better language models are indeed better predictors of human language processing behaviors",
    "checked": true,
    "id": "e78dec2acce8ae78ca640e4dea8841655d2314be",
    "semantic_title": "language model quality correlates with psychometric predictive power in multiple languages",
    "citation_count": 0,
    "authors": [
      "Ethan Wilcox",
      "Clara Meister",
      "Ryan Cotterell",
      "Tiago Pimentel"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.467": {
    "title": "Joint Entity and Relation Extraction with Span Pruning and Hypergraph Neural Networks",
    "volume": "main",
    "abstract": "Entity and Relation Extraction (ERE) is an important task in information extraction. Recent marker-based pipeline models achieve state-of-the-art performance, but still suffer from the error propagation issue. Also, most of current ERE models do not take into account higher-order interactions between multiple entities and relations, while higher-order modeling could be beneficial.In this work, we propose HyperGraph neural network for ERE (HGERE), which is built upon the PL-marker (a state-of-the-art marker-based pipleline model). To alleviate error propagation, we use a high-recall pruner mechanism to transfer the burden of entity identification and labeling from the NER module to the joint module of our model. For higher-order modeling, we build a hypergraph, where nodes are entities (provided by the span pruner) and relations thereof, and hyperedges encode interactions between two different relations or between a relation and its associated subject and object entities. We then run a hypergraph neural network for higher-order inference by applying message passing over the built hypergraph. Experiments on three widely used benchmarks (ACE2004, ACE2005 and SciERC) for ERE task show significant improvements over the previous state-of-the-art PL-marker",
    "checked": true,
    "id": "6d91bb9c5e2ad97f84bc19a79a1b66d2a8c64463",
    "semantic_title": "joint entity and relation extraction with span pruning and hypergraph neural networks",
    "citation_count": 0,
    "authors": [
      "Zhaohui Yan",
      "Songlin Yang",
      "Wei Liu",
      "Kewei Tu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.468": {
    "title": "Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models",
    "volume": "main",
    "abstract": "The performance of large language models (LLMs) on existing reasoning benchmarks has significantly improved over the past years. In response, we present JEEBench, a considerably more challenging benchmark dataset for evaluating the problem solving abilities of LLMs. We curate 515 challenging pre-engineering mathematics, physics and chemistry problems from the highly competitive IIT JEE-Advanced exam. Long-horizon reasoning on top of deep in-domain knowledge is essential for solving problems in this benchmark. Our evaluation on various open-source and proprietary models reveals that the highest performance, even after using techniques like self-consistency, self-refinement and chain-of-thought prompting, is less than 40%. The typical failure modes of GPT-4, the best model, are errors in algebraic manipulation, difficulty in grounding abstract concepts into mathematical equations accurately and failure in retrieving relevant domain-specific concepts. We also observe that by mere prompting, GPT-4 is unable to assess risk introduced by negative marking for incorrect answers. For this, we develop a post-hoc confidence-thresholding method over self-consistency, which enables effective response selection. We hope that our challenging benchmark will guide future re-search in problem-solving using LLMs",
    "checked": true,
    "id": "2cf1f6c723006f258599fd9f000bb616ae83387a",
    "semantic_title": "have llms advanced enough? a challenging problem solving benchmark for large language models",
    "citation_count": 9,
    "authors": [
      "Daman Arora",
      "Himanshu Singh",
      "Mausam"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.469": {
    "title": "StrAE: Autoencoding for Pre-Trained Embeddings using Explicit Structure",
    "volume": "main",
    "abstract": "This work presents StrAE: a Structured Autoencoder framework that through strict adherence to explicit structure, and use of a novel contrastive objective over tree-structured representations, enables effective learning of multi-level representations. Through comparison over different forms of structure, we verify that our results are directly attributable to the informativeness of the structure provided as input, and show that this is not the case for existing tree models. We then further extend StrAE to allow the model to define its own compositions using a simple localised-merge algorithm. This variant, called Self-StrAE, outperforms baselines that don't involve explicit hierarchical compositions, and is comparable to models given informative structure (e.g. constituency parses). Our experiments are conducted in a data-constrained (circa 10M tokens) setting to help tease apart the contribution of the inductive bias to effective learning. However, we find that this framework can be robust to scale, and when extended to a much larger dataset (circa 100M tokens), our 430 parameter model performs comparably to a 6-layer RoBERTa many orders of magnitude larger in size. Our findings support the utility of incorporating explicit composition as an inductive bias for effective representation learning",
    "checked": true,
    "id": "7da17d023d32c64e55c2aba6177374895fa39eee",
    "semantic_title": "strae: autoencoding for pre-trained embeddings using explicit structure",
    "citation_count": 0,
    "authors": [
      "Mattia Opper",
      "Victor Prokhorov",
      "Siddharth N"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.470": {
    "title": "WiCE: Real-World Entailment for Claims in Wikipedia",
    "volume": "main",
    "abstract": "Textual entailment models are increasingly applied in settings like fact-checking, presupposition verification in question answering, or summary evaluation. However, these represent a significant domain shift from existing entailment datasets, and models underperform as a result. We propose WiCE, a new fine-grained textual entailment dataset built on natural claim and evidence pairs extracted from Wikipedia. In addition to standard claim-level entailment, WiCE provides entailment judgments over sub-sentence units of the claim, and a minimal subset of evidence sentences that support each subclaim. To support this, we propose an automatic claim decomposition strategy using GPT-3.5 which we show is also effective at improving entailment models' performance on multiple datasets at test time. Finally, we show that real claims in our dataset involve challenging verification and retrieval problems that existing models fail to address",
    "checked": true,
    "id": "f7ec0e9237fbf960ffd20e2223e07287edb4f26a",
    "semantic_title": "wice: real-world entailment for claims in wikipedia",
    "citation_count": 19,
    "authors": [
      "Ryo Kamoi",
      "Tanya Goyal",
      "Juan Rodriguez",
      "Greg Durrett"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.471": {
    "title": "Natural Disaster Tweets Classification Using Multimodal Data",
    "volume": "main",
    "abstract": "Social media platforms are extensively used for expressing opinions or conveying information. The information available on such platforms can be used for various humanitarian and disaster-related tasks as distributing messages in different formats through social media is quick and easy. Often this useful information during disaster events goes to waste as efficient systems don't exist which can turn these unstructured data into meaningful format which can ultimately assist aid agencies. In disaster identification and assessment, information available is naturally multimodal, however, most existing work has been solely focused on single modalities e.g. images or texts separately. When information from different modalities are integrated , it produces significantly better results. In this paper, we have explored different models which can lead to the development of a system that deals with multimodal datasets and can perform sequential hierarchical classification. Specifically, we aim to find the damage and its severity along with classifying the data into humanitarian categories. The different stages in the hierarchical classification have had their respective models selected by researching with many different modality specific models and approaches of multimodal classification including multi task learning. The hierarchical model can give results at different abstraction levels according to the use cases. Through extensive quantitative and qualitative analysis, we show how our system is effective in classifying the multimodal tweets along with an excellent computational efficiency and assessment performance. With the help of our approach, we aim to support disaster management through identification of situations involving humanitarian tragedies and aid in assessing the severity and type of damage",
    "checked": true,
    "id": "afd35601f10c49eaae78844d1c0923145fdb20d3",
    "semantic_title": "natural disaster tweets classification using multimodal data",
    "citation_count": 0,
    "authors": [
      "Mohammad Basit",
      "Bashir Alam",
      "Zubaida Fatima",
      "Salman Shaikh"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.472": {
    "title": "On the Challenges of Using Black-Box APIs for Toxicity Evaluation in Research",
    "volume": "main",
    "abstract": "Perception of toxicity evolves over time and often differs between geographies and cultural backgrounds. Similarly, black-box commercially available APIs for detecting toxicity, such as the Perspective API, are not static, but frequently retrained to address any unattended weaknesses and biases. We evaluate the implications of these changes on the reproducibility of findings that compare the relative merits of models and methods that aim to curb toxicity. Our findings suggest that research that relied on inherited automatic toxicity scores to compare models and techniques may have resulted in inaccurate findings. Rescoring all models from HELM, a widely respected living benchmark, for toxicity with the recent version of the API led to a different ranking of widely used foundation models. We suggest caution in applying apples-to-apples comparisons between studies and call for a more structured approach to evaluating toxicity over time",
    "checked": true,
    "id": "dd40ccc5388c797a71c3e7c2a67903986f718f94",
    "semantic_title": "on the challenges of using black-box apis for toxicity evaluation in research",
    "citation_count": 15,
    "authors": [
      "Luiza Pozzobon",
      "Beyza Ermis",
      "Patrick Lewis",
      "Sara Hooker"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.473": {
    "title": "RoBoCoP: A Comprehensive ROmance BOrrowing COgnate Package and Benchmark for Multilingual Cognate Identification",
    "volume": "main",
    "abstract": "The identification of cognates is a fundamental process in historical linguistics, on which any further research is based. Even though there are several cognate databases for Romance languages, they are rather scattered, incomplete, noisy, contain unreliable information, or have uncertain availability. In this paper we introduce a comprehensive database of Romance cognates and borrowings based on the etymological information provided by the dictionaries. We extract pairs of cognates between any two Romance languages by parsing electronic dictionaries of Romanian, Italian, Spanish, Portuguese and French. Based on this resource, we propose a strong benchmark for the automatic detection of cognates, by applying machine learning and deep learning based methods on any two pairs of Romance languages. We find that automatic identification of cognates is possible with accuracy averaging around 94% for the more difficult task formulations",
    "checked": true,
    "id": "0a13b48dbd53f8ad15c24f59a3150d99148a4254",
    "semantic_title": "robocop: a comprehensive romance borrowing cognate package and benchmark for multilingual cognate identification",
    "citation_count": 0,
    "authors": [
      "Liviu Dinu",
      "Ana Uban",
      "Alina Cristea",
      "Anca Dinu",
      "Ioan-Bogdan Iordache",
      "Simona Georgescu",
      "Laurentiu Zoicas"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.474": {
    "title": "Instructive Dialogue Summarization with Query Aggregations",
    "volume": "main",
    "abstract": "Conventional dialogue summarization methods directly generate summaries and do not consider user's specific interests. This poses challenges in cases where the users are more focused on particular topics or aspects. With the advancement of instruction-finetuned language models, we introduce instruction-tuning to dialogues to expand the capability set of dialogue summarization models. To overcome the scarcity of instructive dialogue summarization data, we propose a three-step approach to synthesize high-quality query-based summarization triples. This process involves summary-anchored query generation, query filtering and query-based summary generation. By training a unified model called InstructDS (Instructive Dialogue Summarization) on three summarization datasets with multi-purpose instructive triples, we expand the capability of dialogue summarization models. We evaluate our method on four datasets, including dialogue summarization and dialogue reading comprehension. Experimental results show that our approach outperforms the state-of-the-art models and even models with larger sizes. Additionally, our model exhibits higher generalizability and faithfulness, as confirmed by human subjective evaluations",
    "checked": true,
    "id": "6d021c63331c4ff4e6673f9920f86eb37d1dd854",
    "semantic_title": "instructive dialogue summarization with query aggregations",
    "citation_count": 1,
    "authors": [
      "Bin Wang",
      "Zhengyuan Liu",
      "Nancy Chen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.475": {
    "title": "Semantic matching for text classification with complex class descriptions",
    "volume": "main",
    "abstract": "Text classifiers are an indispensable tool for machine learning practitioners, but adapting them to new classes is expensive. To reduce the cost of new classes, previous work exploits class descriptions and/or labels from existing classes. However, these approaches leave a gap in the model development cycle as they support either zero- or few-shot learning, but not both. Existing classifiers either do not work on zero-shot problems, or fail to improve much with few-shot labels. Further, prior work is aimed at concise class descriptions, which may be insufficient for complex classes. We overcome these shortcomings by casting text classification as a matching problem, where a model matches examples with relevant class descriptions. This formulation lets us leverage labels and complex class descriptions to perform zero- and few-shot learning on new classes. We compare this approach with numerous baselines on text classification tasks with complex class descriptions and find that it achieves strong zero-shot performance and scales well with few-shot samples, beating strong baselines by 22.48% (average precision) in the 10-shot setting. Furthermore, we extend the popular Model-Agnostic Meta-Learning algorithm to the zero-shot matching setting and show it improves zero-shot performance by 4.29%. Our results show that expressing text classification as a matching problem is a cost-effective way to address new classes. This strategy enables zero-shot learning for cold-start scenarios and few-shot learning so the model can improve until it is capable enough to deploy",
    "checked": true,
    "id": "a7070038f014b2f6465e6fad68c30a01614ce096",
    "semantic_title": "semantic matching for text classification with complex class descriptions",
    "citation_count": 0,
    "authors": [
      "Brian De Silva",
      "Kuan-Wen Huang",
      "Gwang Lee",
      "Karen Hovsepian",
      "Yan Xu",
      "Mingwei Shen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.476": {
    "title": "MADNet: Maximizing Addressee Deduction Expectation for Multi-Party Conversation Generation",
    "volume": "main",
    "abstract": "Modeling multi-party conversations (MPCs) with graph neural networks has been proven effective at capturing complicated and graphical information flows. However, existing methods rely heavily on the necessary addressee labels and can only be applied to an ideal setting where each utterance must be tagged with an \"@\" or other equivalent addressee label. To study the scarcity of addressee labels which is a common issue in MPCs, we propose MADNet that maximizes addressee deduction expectation in heterogeneous graph neural networks for MPC generation. Given an MPC with a few addressee labels missing, existing methods fail to build a consecutively connected conversation graph, but only a few separate conversation fragments instead. To ensure message passing between these conversation fragments, four additional types of latent edges are designed to complete a fully-connected graph. Besides, to optimize the edge-type-dependent message passing for those utterances without addressee labels, an Expectation-Maximization-based method that iteratively generates silver addressee labels (E step), and optimizes the quality of generated responses (M step), is designed. Experimental results on two Ubuntu IRC channel benchmarks show that MADNet outperforms various baseline models on the task of MPC generation, especially under the more common and challenging setting where part of addressee labels are missing",
    "checked": true,
    "id": "951019532a28f44d4df88af21cc741e64bfd08d2",
    "semantic_title": "madnet: maximizing addressee deduction expectation for multi-party conversation generation",
    "citation_count": 0,
    "authors": [
      "Jia-Chen Gu",
      "Chao-Hong Tan",
      "Caiyuan Chu",
      "Zhen-Hua Ling",
      "Chongyang Tao",
      "Quan Liu",
      "Cong Liu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.477": {
    "title": "GLEN: Generative Retrieval via Lexical Index Learning",
    "volume": "main",
    "abstract": "Generative retrieval shed light on a new paradigm of document retrieval, aiming to directly generate the identifier of a relevant document for a query. While it takes advantage of bypassing the construction of auxiliary index structures, existing studies face two significant challenges: (i) the discrepancy between the knowledge of pre-trained language models and identifiers and (ii) the gap between training and inference that poses difficulty in learning to rank. To overcome these challenges, we propose a novel generative retrieval method, namely Generative retrieval via LExical iNdex learning (GLEN). For training, GLEN effectively exploits a dynamic lexical identifier using a two-phase index learning strategy, enabling it to learn meaningful lexical identifiers and relevance signals between queries and documents. For inference, GLEN utilizes collision-free inference, using identifier weights to rank documents without additional overhead. Experimental results prove that GLEN achieves state-of-the-art or competitive performance against existing generative retrieval methods on various benchmark datasets, e.g., NQ320k, MS MARCO, and BEIR. The code is available at https://github.com/skleee/GLEN",
    "checked": true,
    "id": "fd3f976c47ea4c51fc0d3e159912d22fc751db99",
    "semantic_title": "glen: generative retrieval via lexical index learning",
    "citation_count": 0,
    "authors": [
      "Sunkyung Lee",
      "Minjin Choi",
      "Jongwuk Lee"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.478": {
    "title": "Turn-Level Active Learning for Dialogue State Tracking",
    "volume": "main",
    "abstract": "Dialogue state tracking (DST) plays an important role in task-oriented dialogue systems. However, collecting a large amount of turn-by-turn annotated dialogue data is costly and inefficient. In this paper, we propose a novel turn-level active learning framework for DST to actively select turns in dialogues to annotate. Given the limited labelling budget, experimental results demonstrate the effectiveness of selective annotation of dialogue turns. Additionally, our approach can effectively achieve comparable DST performance to traditional training approaches with significantly less annotated data, which provides a more efficient way to annotate new dialogue data",
    "checked": true,
    "id": "ec1967121c969690b60d5e6d3bd1480553e59dda",
    "semantic_title": "turn-level active learning for dialogue state tracking",
    "citation_count": 0,
    "authors": [
      "Zihan Zhang",
      "Meng Fang",
      "Fanghua Ye",
      "Ling Chen",
      "Mohammad-Reza Namazi-Rad"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.479": {
    "title": "ReSee: Responding through Seeing Fine-grained Visual Knowledge in Open-domain Dialogue",
    "volume": "main",
    "abstract": "Incorporating visual knowledge into text-only dialogue systems has become a potential direction to imitate the way humans think, imagine, and communicate. However, existing multimodal dialogue systems are either confined by the scale and quality of available datasets or the coarse concept of visual knowledge. To address these issues, we provide a new paradigm of constructing multimodal dialogues as well as two datasets extended from text-only dialogues under such paradigm (ReSee-WoW, ReSee-DD). We propose to explicitly split the visual knowledge into finer granularity (\"turn-level\" and \"entity-level\"). To further boost the accuracy and diversity of augmented visual information, we retrieve them from the Internet or a large image dataset. To demonstrate the superiority and universality of the provided visual knowledge, we propose a simple but effective framework ReSee to add visual representation into vanilla dialogue models by modality concatenations. We also conduct extensive experiments and ablations w.r.t. different model configurations and visual knowledge settings. Empirical, encouraging results not only demonstrate the effectiveness of introducing visual knowledge at both entity and turn level but also verify the proposed model ReSee outperforms several state-of-the-art methods on automatic and human evaluations. By leveraging text and vision knowledge, ReSee can produce informative responses with real-world visual concepts. Our code is available at https://github.com/ImKeTT/ReSee",
    "checked": true,
    "id": "c22a8e36b7ffa69da0d70c9db58c78252567400a",
    "semantic_title": "resee: responding through seeing fine-grained visual knowledge in open-domain dialogue",
    "citation_count": 2,
    "authors": [
      "Haoqin Tu",
      "Yitong Li",
      "Fei Mi",
      "Zhongliang Yang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.480": {
    "title": "Modeling Conceptual Attribute Likeness and Domain Inconsistency for Metaphor Detection",
    "volume": "main",
    "abstract": "Metaphor detection is an important and challenging task in natural language processing, which aims to distinguish between metaphorical and literal expressions in text. Previous studies mainly leverage the incongruity of source and target domains and contextual clues for detection, neglecting similar attributes shared between source and target concepts in metaphorical expressions. Based on conceptual metaphor theory, these similar attributes are essential to infer implicit meanings conveyed by the metaphor. Under the guidance of conceptual metaphor theory, in this paper, we model the likeness of attribute for the first time and propose a novel Attribute Likeness and Domain Inconsistency Learning framework (AIDIL) for word-pair metaphor detection. Specifically, we propose an attribute siamese network to mine similar attributes between source and target concepts. We then devise a domain contrastive learning strategy to learn the semantic inconsistency of concepts in source and target domains. Extensive experiments on four datasets verify that our method significantly outperforms the previous state-of-the-art methods, and demonstrate the generalization ability of our method",
    "checked": true,
    "id": "40533c6710496f41b042f0077df0ab65afdf2838",
    "semantic_title": "modeling conceptual attribute likeness and domain inconsistency for metaphor detection",
    "citation_count": 0,
    "authors": [
      "Yuan Tian",
      "Nan Xu",
      "Wenji Mao",
      "Daniel Zeng"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.481": {
    "title": "Referring Image Segmentation via Joint Mask Contextual Embedding Learning and Progressive Alignment Network",
    "volume": "main",
    "abstract": "Referring image segmentation is a task that aims to predict pixel-wise masks corresponding to objects in an image described by natural language expressions. Previous methods for referring image segmentation employ a cascade framework to break down complex problems into multiple stages. However, its defects also obvious: existing methods within the cascade framework may encounter challenges in both maintaining a strong focus on the most relevant information during specific stages of the referring image segmentation process and rectifying errors propagated from early stages, which can ultimately result in sub-optimal performance. To address these limitations, we propose the Joint Mask Contextual Embedding Learning Network (JMCELN). JMCELN is designed to enhance the Cascade Framework by incorporating a Learnable Contextual Embedding and a Progressive Alignment Network (PAN). The Learnable Contextual Embedding module dynamically stores and utilizes reasoning information based on the current mask prediction results, enabling the network to adaptively capture and refine pertinent information for improved mask prediction accuracy. Furthermore, the Progressive Alignment Network (PAN) is introduced as an integral part of JMCELN. PAN leverages the output from the previous layer as a filter for the current output, effectively reducing inconsistencies between predictions from different stages. By iteratively aligning the predictions, PAN guides the Learnable Contextual Embedding to incorporate more discriminative information for reasoning, leading to enhanced prediction quality and a reduction in error propagation. With these methods, we achieved state-of-the-art results on three commonly used benchmarks, especially in more intricate datasets. The code will be released",
    "checked": true,
    "id": "46ef5003934cf1ed32b963c92b4c88cfa6482157",
    "semantic_title": "referring image segmentation via joint mask contextual embedding learning and progressive alignment network",
    "citation_count": 0,
    "authors": [
      "Ziling Huang",
      "Shin’ichi Satoh"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.482": {
    "title": "Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study",
    "volume": "main",
    "abstract": "Large decoder-only language models (LMs) can be largely improved in terms of perplexity by retrieval (e.g., RETRO), but its impact on text generation quality and downstream task accuracy is unclear. Thus, it is still an open question: shall we pretrain large autoregressive LMs with retrieval? To answer it, we perform a comprehensive study on a scalable pre-trained retrieval-augmented LM (i.e., RETRO) compared with standard GPT and retrieval-augmented GPT incorporated at fine-tuning or inference stages. We first provide the recipe to reproduce RETRO up to 9.5B parameters while retrieving a text corpus with 330B tokens. Based on that, we have the following novel findings: i) RETRO outperforms GPT on text generation with much less degeneration (i.e., repetition), moderately higher factual accuracy, and slightly lower toxicity with a nontoxic retrieval database. ii) On the LM Evaluation Harness benchmark, RETRO largely outperforms GPT on knowledge-intensive tasks, but is on par with GPT on other tasks. Furthermore, we introduce a simple variant of the model, RETRO++, which largely improves open-domain QA results of original RETRO (e.g., EM score +8.6 on Natural Question) and significantly outperforms retrieval-augmented GPT across different model sizes. Our findings highlight the promising direction of pretraining autoregressive LMs with retrieval as future foundation models. We release our implementation at: https://github.com/NVIDIA/Megatron-LM/tree/main/tools/retro",
    "checked": true,
    "id": "b63e97330154acece935ffa6901e3f36518e5703",
    "semantic_title": "shall we pretrain autoregressive language models with retrieval? a comprehensive study",
    "citation_count": 11,
    "authors": [
      "Boxin Wang",
      "Wei Ping",
      "Peng Xu",
      "Lawrence McAfee",
      "Zihan Liu",
      "Mohammad Shoeybi",
      "Yi Dong",
      "Oleksii Kuchaiev",
      "Bo Li",
      "Chaowei Xiao",
      "Anima Anandkumar",
      "Bryan Catanzaro"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.483": {
    "title": "SCITAB: A Challenging Benchmark for Compositional Reasoning and Claim Verification on Scientific Tables",
    "volume": "main",
    "abstract": "Current scientific fact-checking benchmarks exhibit several shortcomings, such as biases arising from crowd-sourced claims and an over-reliance on text-based evidence. We present SCITAB, a challenging evaluation dataset consisting of 1.2K expert-verified scientific claims that 1) originate from authentic scientific publications and 2) require compositional reasoning for verification. The claims are paired with evidence-containing scientific tables annotated with labels. Through extensive evaluations, we demonstrate that SCITAB poses a significant challenge to state-of-the-art models, including table-based pretraining models and large language models. All models except GPT-4 achieved performance barely above random guessing. Popular prompting techniques, such as Chain-of-Thought, do not achieve much performance gains on SCITAB. Our analysis uncovers several unique challenges posed by SCITAB, including table grounding, claim ambiguity, and compositional reasoning. Our codes and data are publicly available at https://github.com/XinyuanLu00/SciTab",
    "checked": true,
    "id": "c20b18d6b919695a69e416debf8bf1ffeac03992",
    "semantic_title": "scitab: a challenging benchmark for compositional reasoning and claim verification on scientific tables",
    "citation_count": 3,
    "authors": [
      "Xinyuan Lu",
      "Liangming Pan",
      "Qian Liu",
      "Preslav Nakov",
      "Min-Yen Kan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.484": {
    "title": "Training Simultaneous Speech Translation with Robust and Random Wait-k-Tokens Strategy",
    "volume": "main",
    "abstract": "Simultaneous Speech Translation (SimulST) is a task focused on ensuring high-quality translation of speech in low-latency situations. Despite this, the modality gap (e.g., unknown word boundaries) between audio and text presents a challenge. This gap hinders the effective application of policies from simultaneous text translation (SimulMT) and compromises the performance of offline speech translation. To address this issue, we first leverage the Montreal Forced Aligner (MFA) and utilize audio transcription pairs in pre-training the acoustic encoder, and introduce a token-level cross-modal alignment that allows the wait-k policy from SimulMT to better adapt to SimulST. This token-level boundary alignment simplifies the decision-making process for predicting read/write actions, as if the decoder were directly processing text tokens. Subsequently, to optimize the SimulST task, we propose a robust and random wait-k-tokens strategy. This strategy allows a single model to meet various latency requirements and minimizes error accumulation of boundary alignment during inference. Our experiments on the MuST-C dataset show that our method achieves better trade-off between translation quality and latency",
    "checked": true,
    "id": "2ae4c485a97bd22f455f56a22b99cc725bb8f5ae",
    "semantic_title": "training simultaneous speech translation with robust and random wait-k-tokens strategy",
    "citation_count": 0,
    "authors": [
      "Linlin Zhang",
      "Kai Fan",
      "Jiajun Bu",
      "Zhongqiang Huang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.485": {
    "title": "SCENE: Self-Labeled Counterfactuals for Extrapolating to Negative Examples",
    "volume": "main",
    "abstract": "Detecting negatives (such as non-entailment relationships, unanswerable questions, and false claims) is an important and challenging aspect of many natural language understanding tasks. Though manually collecting challenging negative examples can help models detect them, it is both costly and domain-specific. In this work, we propose Self-labeled Counterfactuals for Extrapolating to Negative Examples (SCENE), an automatic method for synthesizing training data that greatly improves models' ability to detect challenging negative examples. In contrast with standard data augmentation, which synthesizes new examples for existing labels, SCENE can synthesize negative examples zero-shot from only positive ones. Given a positive example, SCENE perturbs it with a mask infilling model, then determines whether the resulting example is negative based on a self-training heuristic. With access to only answerable training examples, SCENE can close 69.6% of the performance gap on SQuAD 2.0, a dataset where half of the evaluation examples are unanswerable, compared to a model trained on SQuAD 2.0. Our method also extends to boolean question answering and recognizing textual entailment, and improves generalization from SQuAD to ACE-whQA, an out-of-domain extractive QA benchmark",
    "checked": true,
    "id": "b99a71d10b19eabea3b5cae514ec87748d44a462",
    "semantic_title": "scene: self-labeled counterfactuals for extrapolating to negative examples",
    "citation_count": 1,
    "authors": [
      "Deqing Fu",
      "Ameya Godbole",
      "Robin Jia"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.486": {
    "title": "Enhancing Code-Switching for Cross-lingual SLU: A Unified View of Semantic and Grammatical Coherence",
    "volume": "main",
    "abstract": "Despite the success of spoken language understanding (SLU) in high-resource languages, achieving similar performance in low-resource settings, such as zero-shot scenarios, remains challenging due to limited labeled training data. To improve zero-shot cross-lingual SLU, recent studies have explored code-switched sentences containing tokens from multiple languages. However, vanilla code-switched sentences often lack semantic and grammatical coherence. We ascribe this lack to two issues: (1) randomly replacing code-switched tokens with equal probability and (2) disregarding token-level dependency within each language. To tackle these issues, in this paper, we propose a novel method termed SoGo, for zero-shot cross-lingual SLU. First, we use a saliency-based substitution approach to extract keywords as substitution options. Then, we introduce a novel token-level alignment strategy that considers the similarity between the context and the code-switched tokens, ensuring grammatical coherence in code-switched sentences. Extensive experiments and analyses demonstrate the superior performance of SoGo across nine languages on MultiATIS++",
    "checked": true,
    "id": "9b4c049026b566966c092b9d9b17081dbf36695d",
    "semantic_title": "enhancing code-switching for cross-lingual slu: a unified view of semantic and grammatical coherence",
    "citation_count": 0,
    "authors": [
      "Zhihong Zhu",
      "Xuxin Cheng",
      "Zhiqi Huang",
      "Dongsheng Chen",
      "Yuexian Zou"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.487": {
    "title": "Task-Agnostic Low-Rank Adapters for Unseen English Dialects",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) are trained on corpora disproportionally weighted in favor of Standard American English. As a result, speakers of other dialects experience significantly more failures when interacting with these technologies. In practice, these speakers often accommodate their speech to be better understood. Our work shares the belief that language technologies should be designed to accommodate the diversity in English dialects and not the other way around. However, prior work on dialect struggle with generalizing to evolving and emerging dialects in a scalable manner. To fill this gap, our method, HyperLoRA, leverages expert linguistic knowledge to enable resource-efficient adaptation via hypernetworks. By disentangling dialect-specific and cross-dialectal information, HyperLoRA improves generalization to unseen dialects in a task-agnostic fashion. Not only is HyperLoRA more scalable in the number of parameters, but it also achieves the best or most competitive performance across 5 dialects in a zero-shot setting. In this way, our approach facilitates access to language technology for billions of English dialect speakers who are traditionally underrepresented",
    "checked": true,
    "id": "b6f4fc4ed72abaa8496c0db775d601d32fa9de4e",
    "semantic_title": "task-agnostic low-rank adapters for unseen english dialects",
    "citation_count": 0,
    "authors": [
      "Zedian Xiao",
      "William Held",
      "Yanchen Liu",
      "Diyi Yang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.488": {
    "title": "Federated Learning of Large Language Models with Parameter-Efficient Prompt Tuning and Adaptive Optimization",
    "volume": "main",
    "abstract": "Federated learning (FL) is a promising paradigm to enable collaborative model training with decentralized data. However, the training process of Large Language Models (LLMs) generally incurs the update of significant parameters, which limits the applicability of FL techniques to tackle the LLMs in real scenarios. Prompt tuning can significantly reduce the number of parameters to update, but it either incurs performance degradation or low training efficiency. The straightforward utilization of prompt tuning in the FL often raises non-trivial communication costs and dramatically degrades performance. In addition, the decentralized data is generally non-Independent and Identically Distributed (non-IID), which brings client drift problems and thus poor performance. This paper proposes a Parameter-efficient prompt Tuning approach with Adaptive Optimization, i.e., FedPepTAO, to enable efficient and effective FL of LLMs. First, an efficient partial prompt tuning approach is proposed to improve performance and efficiency simultaneously. Second, a novel adaptive optimization method is developed to address the client drift problems on both the device and server sides to enhance performance further. Extensive experiments based on 10 datasets demonstrate the superb performance (up to 60.8% in terms of accuracy) and efficiency (up to 97.59% in terms of training time) of FedPepTAO compared with 9 baseline approaches. Our code is available at https://github.com/llm-eff/FedPepTAO",
    "checked": true,
    "id": "67ffe6037cf058b8c5b39f59693c4c349cc1e456",
    "semantic_title": "federated learning of large language models with parameter-efficient prompt tuning and adaptive optimization",
    "citation_count": 3,
    "authors": [
      "Tianshi Che",
      "Ji Liu",
      "Yang Zhou",
      "Jiaxiang Ren",
      "Jiwen Zhou",
      "Victor Sheng",
      "Huaiyu Dai",
      "Dejing Dou"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.489": {
    "title": "TheoremQA: A Theorem-driven Question Answering Dataset",
    "volume": "main",
    "abstract": "The recent LLMs like GPT-4 and PaLM-2 have made tremendous progress in solving fundamental math problems like GSM8K by achieving over 90% accuracy. However, their capabilities to solve more challenging math problems which require domain-specific knowledge (i.e. theorem) have yet to be investigated. In this paper, we introduce TheoremQA, the first theorem-driven question-answering dataset designed to evaluate AI models' capabilities to apply theorems to solve challenging science problems. TheoremQA is curated by domain experts containing 800 high-quality questions covering 350 theorems from Math, Physics, EE&CS, and Finance. We evaluate a wide spectrum of 16 large language and code models with different prompting strategies like Chain-of-Thoughts and Program-of-Thoughts. We found that GPT-4's capabilities to solve these problems are unparalleled, achieving an accuracy of 51% with Program-of-Thoughts Prompting. All the existing open-sourced models are below 15%, barely surpassing the random-guess baseline. Given the diversity and broad coverage of TheoremQA, we believe it can be used as a better benchmark to evaluate LLMs' capabilities to solve challenging science problems",
    "checked": true,
    "id": "a52dd1e900200e0733eea927edc7d6c27aeba187",
    "semantic_title": "theoremqa: a theorem-driven question answering dataset",
    "citation_count": 20,
    "authors": [
      "Wenhu Chen",
      "Ming Yin",
      "Max Ku",
      "Pan Lu",
      "Yixin Wan",
      "Xueguang Ma",
      "Jianyu Xu",
      "Xinyi Wang",
      "Tony Xia"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.490": {
    "title": "Scalable-DSC: A Structural Template Prompt Approach to Scalable Dialogue State Correction",
    "volume": "main",
    "abstract": "Dialogue state error correction has recently been proposed to correct wrong slot values in predicted dialogue states, thereby mitigating the error propagation problem for dialogue state tracking (DST). These approaches, though effective, are heavily intertwined with specific DST models, limiting their applicability to other DST models. To solve this problem, we propose Scalable Dialogue State Correction (Scalable-DSC), which can correct wrong slot values in the dialogue state predicted by any DST model. Specifically, we propose a Structural Template Prompt (STP) that converts predicted dialogue state from any DST models into a standardized natural language sequence as a part of the historical context, associates them with dialogue history information, and generates a corrected dialogue state sequence based on predefined template options. We further enhance Scalable-DSC by introducing two training strategies. The first employs a predictive state simulator to simulate the predicted dialogue states as the training data to enhance the generalization ability of the model. The second involves using the dialogue state predicted by DST as the training data, aiming at mitigating the inconsistent error type distribution between the training and inference. Experiments confirm that our model achieves state-of-the-art results on MultiWOZ 2.0-2.4",
    "checked": true,
    "id": "27d64ee12445d368fee3e6f59fbadbaa0e9e7843",
    "semantic_title": "scalable-dsc: a structural template prompt approach to scalable dialogue state correction",
    "citation_count": 0,
    "authors": [
      "Haoxiang Su",
      "Hongyan Xie",
      "Hao Huang",
      "Shuangyong Song",
      "Ruiyu Fang",
      "Xiaomeng Huang",
      "Sijie Feng"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.491": {
    "title": "Don't Trust ChatGPT when your Question is not in English: A Study of Multilingual Abilities and Types of LLMs",
    "volume": "main",
    "abstract": "Large language models (LLMs) have demonstrated exceptional natural language understanding abilities, and have excelled in a variety of natural language processing (NLP) tasks. Despite the fact that most LLMs are trained predominantly on English, multiple studies have demonstrated their capabilities in a variety of languages. However, fundamental questions persist regarding how LLMs acquire their multilingual abilities and how performance varies across different languages. These inquiries are crucial for the study of LLMs since users and researchers often come from diverse language backgrounds, potentially influencing how they use LLMs and interpret their output. In this work, we propose a systematic way of qualitatively and quantitatively evaluating the multilingual capabilities of LLMs. We investigate the phenomenon of cross-language generalization in LLMs, wherein limited multilingual training data leads to advanced multilingual capabilities. To accomplish this, we employ a novel prompt back-translation method. The results demonstrate that LLMs, such as GPT, can effectively transfer learned knowledge across different languages, yielding relatively consistent results in translation-equivariant tasks, in which the correct output does not depend on the language of the input. However, LLMs struggle to provide accurate results in translation-variant tasks, which lack this property, requiring careful user judgment to evaluate the answers",
    "checked": true,
    "id": "7eb044170c11b7e2193b8df35f606edcfc7f2585",
    "semantic_title": "don't trust chatgpt when your question is not in english: a study of multilingual abilities and types of llms",
    "citation_count": 1,
    "authors": [
      "Xiang Zhang",
      "Senyu Li",
      "Bradley Hauer",
      "Ning Shi",
      "Grzegorz Kondrak"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.492": {
    "title": "M3Seg: A Maximum-Minimum Mutual Information Paradigm for Unsupervised Topic Segmentation in ASR Transcripts",
    "volume": "main",
    "abstract": "Topic segmentation aims to detect topic boundaries and split automatic speech recognition transcriptions (e.g., meeting transcripts) into segments that are bounded by thematic meanings. In this work, we propose M3Seg, a novel Maximum-Minimum Mutual information paradigm for linear topic segmentation without using any parallel data. Specifically, by employing sentence representations provided by pre-trained language models, M3Seg first learns a region-based segment encoder based on the maximization of mutual information between the global segment representation and the local contextual sentence representation. Secondly, an edge-based boundary detection module aims to segment the whole by topics based on minimizing the mutual information between different segments. Experiment results on two public datasets demonstrate the effectiveness of M3Seg, which outperform the state-of-the-art methods by a significant (18%–37% improvement) margin",
    "checked": false,
    "id": "3665290b09bb33f1b21edb27b4c3808275e0d81d",
    "semantic_title": "m³seg: a maximum-minimum mutual information paradigm for unsupervised topic segmentation in asr transcripts",
    "citation_count": 0,
    "authors": [
      "Ke Wang",
      "Xiutian Zhao",
      "Yanghui Li",
      "Wei Peng"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.493": {
    "title": "Empirical Study of Zero-Shot NER with ChatGPT",
    "volume": "main",
    "abstract": "Large language models (LLMs) exhibited powerful capability in various natural language processing tasks. This work focuses on exploring LLM performance on zero-shot information extraction, with a focus on the ChatGPT and named entity recognition (NER) task. Inspired by the remarkable reasoning capability of LLM on symbolic and arithmetic reasoning, we adapt the prevalent reasoning methods to NER and propose reasoning strategies tailored for NER. First, we explore a decomposed question-answering paradigm by breaking down the NER task into simpler subproblems by labels. Second, we propose syntactic augmentation to stimulate the model's intermediate thinking in two ways: syntactic prompting, which encourages the model to analyze the syntactic structure itself, and tool augmentation, which provides the model with the syntactic information generated by a parsing tool. Besides, we adapt self-consistency to NER by proposing a two-stage majority voting strategy, which first votes for the most consistent mentions, then the most consistent types. The proposed methods achieve remarkable improvements for zero-shot NER across seven benchmarks, including Chinese and English datasets, and on both domain-specific and general-domain scenarios. In addition, we present a comprehensive analysis of the error types with suggestions for optimization directions. We also verify the effectiveness of the proposed methods on the few-shot setting and other LLMs",
    "checked": true,
    "id": "3c55aa582a2d682eb53e9237589234854e0b92d8",
    "semantic_title": "empirical study of zero-shot ner with chatgpt",
    "citation_count": 2,
    "authors": [
      "Tingyu Xie",
      "Qi Li",
      "Jian Zhang",
      "Yan Zhang",
      "Zuozhu Liu",
      "Hongwei Wang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.494": {
    "title": "Automatic Prompt Optimization with \"Gradient Descent\" and Beam Search",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have shown impressive performance as general purpose agents, but their abilities remain highly dependent on prompts which are hand written with onerous trial-and-error effort. We propose a simple and nonparametric solution to this problem, Prompt Optimization with Textual Gradients (ProTeGi), which is inspired by numerical gradient descent to automatically improve prompts, assuming access to training data and an LLM API. The algorithm uses minibatches of data to form natural language \"gradients\" that criticize the current prompt, much like how numerical gradients point in the direction of error ascent. The natural language gradients are then \"propagated\" into the prompt by editing the prompt in the opposite semantic direction of the gradient. These gradient descent steps are guided by a beam search and bandit selection procedure which significantly improves algorithmic efficiency. Preliminary results across three benchmark NLP tasks and the novel problem of LLM jailbreak detection suggest that Automatic Prompt Optimization can outperform prior prompt editing techniques and improve an initial prompt's performance by up to 31%, by using data to rewrite vague task descriptions into more precise annotation instructions",
    "checked": true,
    "id": "c76dd4a70361c3afd2e19d046343e2dedd16ecc3",
    "semantic_title": "automatic prompt optimization with \"gradient descent\" and beam search",
    "citation_count": 41,
    "authors": [
      "Reid Pryzant",
      "Dan Iter",
      "Jerry Li",
      "Yin Lee",
      "Chenguang Zhu",
      "Michael Zeng"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.495": {
    "title": "Active Retrieval Augmented Generation",
    "volume": "main",
    "abstract": "Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method",
    "checked": true,
    "id": "88884b8806262a4095036041e3567d450dba39f7",
    "semantic_title": "active retrieval augmented generation",
    "citation_count": 30,
    "authors": [
      "Zhengbao Jiang",
      "Frank Xu",
      "Luyu Gao",
      "Zhiqing Sun",
      "Qian Liu",
      "Jane Dwivedi-Yu",
      "Yiming Yang",
      "Jamie Callan",
      "Graham Neubig"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.496": {
    "title": "GD-COMET: A Geo-Diverse Commonsense Inference Model",
    "volume": "main",
    "abstract": "With the increasing integration of AI into everyday life, it's becoming crucial to design AI systems to serve users from diverse backgrounds by making them culturally aware. In this paper, we present GD-COMET, a geo-diverse version of the COMET commonsense inference model. GD-COMET goes beyond Western commonsense knowledge and is capable of generating inferences pertaining to a broad range of cultures. We demonstrate the effectiveness of GD-COMET through a comprehensive human evaluation across 5 diverse cultures, as well as extrinsic evaluation on a geo-diverse task. The evaluation shows that GD-COMET captures and generates culturally nuanced commonsense knowledge, demonstrating its potential to benefit NLP applications across the board and contribute to making NLP more inclusive",
    "checked": true,
    "id": "8a5e3ae834e2fe724f978d574843470f7c28d18e",
    "semantic_title": "gd-comet: a geo-diverse commonsense inference model",
    "citation_count": 0,
    "authors": [
      "Mehar Bhatia",
      "Vered Shwartz"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.497": {
    "title": "Multi-level Adaptive Contrastive Learning for Knowledge Internalization in Dialogue Generation",
    "volume": "main",
    "abstract": "Knowledge-grounded dialogue generation aims to mitigate the issue of text degeneration by incorporating external knowledge to supplement the context. However, the model often fails to internalize this information into responses in a human-like manner. Instead, it simply inserts segments of the provided knowledge into generic responses. As a result, the generated responses tend to be tedious, incoherent, and in lack of interactivity which means the degeneration problem is still unsolved. In this work, we first find that such copying-style degeneration is primarily due to the weak likelihood objective, which allows the model to \"cheat\" the objective by merely duplicating knowledge segments in a superficial pattern matching based on overlap. To overcome this challenge, we then propose a Multi-level Adaptive Contrastive Learning (MACL) framework that dynamically samples negative examples and subsequently penalizes degeneration behaviors at both the token-level and sequence-level. Extensive experiments on the WoW dataset demonstrate the effectiveness of our approach across various pre-trained models and decoding strategies",
    "checked": true,
    "id": "3dad8cac04ceabdd65d3327e74f70091337bc635",
    "semantic_title": "multi-level adaptive contrastive learning for knowledge internalization in dialogue generation",
    "citation_count": 0,
    "authors": [
      "Chenxu Yang",
      "Zheng Lin",
      "Lanrui Wang",
      "Chong Tian",
      "Liang Pang",
      "Jiangnan Li",
      "Qirong Ho",
      "Yanan Cao",
      "Weiping Wang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.498": {
    "title": "Enhancing Biomedical Lay Summarisation with External Knowledge Graphs",
    "volume": "main",
    "abstract": "Previous approaches for automatic lay summarisation are exclusively reliant on the source article that, given it is written for a technical audience (e.g., researchers), is unlikely to explicitly define all technical concepts or state all of the background information that is relevant for a lay audience. We address this issue by augmenting eLife, an existing biomedical lay summarisation dataset, with article-specific knowledge graphs, each containing detailed information on relevant biomedical concepts. Using both automatic and human evaluations, we systematically investigate the effectiveness of three different approaches for incorporating knowledge graphs within lay summarisation models, with each method targeting a distinct area of the encoder-decoder model architecture. Our results confirm that integrating graph-based domain knowledge can significantly benefit lay summarisation by substantially increasing the readability of generated text and improving the explanation of technical concepts",
    "checked": true,
    "id": "af3dfaf07b7d10de7928df851afccd99a041c9e3",
    "semantic_title": "enhancing biomedical lay summarisation with external knowledge graphs",
    "citation_count": 2,
    "authors": [
      "Tomas Goldsack",
      "Zhihao Zhang",
      "Chen Tang",
      "Carolina Scarton",
      "Chenghua Lin"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.499": {
    "title": "A Diffusion Weighted Graph Framework for New Intent Discovery",
    "volume": "main",
    "abstract": "New Intent Discovery (NID) aims to recognize both new and known intents from unlabeled data with the aid of limited labeled data containing only known intents. Without considering structure relationships between samples, previous methods generate noisy supervisory signals which cannot strike a balance between quantity and quality, hindering the formation of new intent clusters and effective transfer of the pre-training knowledge. To mitigate this limitation, we propose a novel Diffusion Weighted Graph Framework (DWGF) to capture both semantic similarities and structure relationships inherent in data, enabling more sufficient and reliable supervisory signals. Specifically, for each sample, we diffuse neighborhood relationships along semantic paths guided by the nearest neighbors for multiple hops to characterize its local structure discriminately. Then, we sample its positive keys and weigh them based on semantic similarities and local structures for contrastive learning. During inference, we further propose Graph Smoothing Filter (GSF) to explicitly utilize the structure relationships to filter high-frequency noise embodied in semantically ambiguous samples on the cluster boundary. Extensive experiments show that our method outperforms state-of-the-art models on all evaluation metrics across multiple benchmark datasets. Code and data will be made public",
    "checked": true,
    "id": "3c351ac7e174d168c7ba9a3daaa1e97d875f6b52",
    "semantic_title": "a diffusion weighted graph framework for new intent discovery",
    "citation_count": 0,
    "authors": [
      "Wenkai Shi",
      "Wenbin An",
      "Feng Tian",
      "Qinghua Zheng",
      "QianYing Wang",
      "Ping Chen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.500": {
    "title": "A Self-enhancement Multitask Framework for Unsupervised Aspect Category Detection",
    "volume": "main",
    "abstract": "Our work addresses the problem of unsupervised Aspect Category Detection using a small set of seed words. Recent works have focused on learning embedding spaces for seed words and sentences to establish similarities between sentences and aspects. However, aspect representations are limited by the quality of initial seed words, and model performances are compromised by noise. To mitigate this limitation, we propose a simple framework that automatically enhances the quality of initial seed words and selects high-quality sentences for training instead of using the entire dataset. Our main concepts are to add a number of seed words to the initial set and to treat the task of noise resolution as a task of augmenting data for a low-resource task. In addition, we jointly train Aspect Category Detection with Aspect Term Extraction and Aspect Term Polarity to further enhance performance. This approach facilitates shared representation learning, allowing Aspect Category Detection to benefit from the additional guidance offered by other tasks. Extensive experiments demonstrate that our framework surpasses strong baselines on standard datasets",
    "checked": true,
    "id": "fadcd084d046311e4ed3a9929a84036364afbdef",
    "semantic_title": "a self-enhancement multitask framework for unsupervised aspect category detection",
    "citation_count": 0,
    "authors": [
      "Thi-Nhung Nguyen",
      "Hoang Ngo",
      "Kiem-Hieu Nguyen",
      "Tuan-Dung Cao"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.501": {
    "title": "DialCoT Meets PPO: Decomposing and Exploring Reasoning Paths in Smaller Language Models",
    "volume": "main",
    "abstract": "Chain-of-Thought (CoT) prompting has successfully enhanced the reasoning capabilities of Large Language Models (LLMs) with at least 100 billion parameters. However, it is ineffective, or even detrimental, to the performance on reasoning tasks in Smaller Language Models (SLMs) with less than 10 billion parameters. In this paper, we propose Dialogue-guided Chain-of-Thought (DialCoT) to improve the reasoning capabilities of SLMs, with the aim of generating intermediate reasoning steps in a dialogue format to guide the model to the final answer. Furthermore, we optimize the model to choose the optimal reasoning path through the Proximal Policy Optimization (PPO) algorithm, further enhancing its reasoning capabilities. Compared to previous methods, our advantages lie in: 1) We transform the process of solving complex reasoning problems into decomposing problems and solving a series of simpler sub-questions, significantly reducing task difficulty and making it more suitable for SLMs. 2) We optimize the model to choose the optimal reasoning path through the PPO algorithm. Comprehensive experiments on four arithmetic reasoning datasets show that our method can achieve significant performance gains over state-of-the-art competitors",
    "checked": true,
    "id": "a83724fd55cd2bcf5583ca181373c34571ac1f73",
    "semantic_title": "dialcot meets ppo: decomposing and exploring reasoning paths in smaller language models",
    "citation_count": 1,
    "authors": [
      "Chengcheng Han",
      "Xiaowei Du",
      "Che Zhang",
      "Yixin Lian",
      "Xiang Li",
      "Ming Gao",
      "Baoyuan Wang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.502": {
    "title": "Recurrent Neural Language Models as Probabilistic Finite-state Automata",
    "volume": "main",
    "abstract": "Studying language models (LMs) in terms of well-understood formalisms allows us to precisely characterize their abilities and limitations. Previous work has investigated the expressive power of recurrent neural network (RNN) LMs in terms of their capacity to recognize unweighted formal languages. However, LMs do not describe unweighted formal languages—rather, they define probability distributions over strings. In this work, we study what classes of such probability distributions RNN LMs can represent, which allows us to make more direct statements about their capabilities. We show that simple RNNs are equivalent to a subclass of probabilistic finite-state automata, and can thus model a strict subset of probability distributions expressible by finite-state models. Furthermore, we study the space complexity of representing finite-state LMs with RNNs. We show that, to represent an arbitrary deterministic finite-state LM with N states over an alphabet 𝛴, an RNN requires 𝛺\\left(N |𝛴|\\right) neurons. These results present a first step towards characterizing the classes of distributions RNN LMs can represent and thus help us understand their capabilities and limitations",
    "checked": true,
    "id": "eddba13b275e43a6eee83331613f928115dad0cc",
    "semantic_title": "recurrent neural language models as probabilistic finite-state automata",
    "citation_count": 0,
    "authors": [
      "Anej Svete",
      "Ryan Cotterell"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.503": {
    "title": "Revisiting Source Context in Nearest Neighbor Machine Translation",
    "volume": "main",
    "abstract": "Nearest neighbor machine translation (kNN-MT), which interpolates target token probabilities with estimates derived from additional examples, has achieved significant improvements and attracted extensive interest in recent years. However, existing research does not explicitly consider the source context when retrieving similar examples, potentially leading to suboptimal performance. To address this, we comprehensively revisit the role of source context and propose a simple and effective method for improving neural machine translation via source context enhancement, demonstrating its crucial role in both retrieving superior examples and determining more suitable interpolation coefficients. Furthermore, we reveal that the probability estimation can be further optimized by incorporating a source-aware distance calibration module. Comprehensive experiments show that our proposed approach can be seamlessly integrated with representative kNN-MT baselines, resulting in substantial improvements over these strong baselines across a number of settings and domains. Remarkably, these improvements can reach up to 1.6 BLEU points",
    "checked": true,
    "id": "7fbab1343b8c4faf45faed5a10ca0a59b32061c0",
    "semantic_title": "revisiting source context in nearest neighbor machine translation",
    "citation_count": 0,
    "authors": [
      "Xuanhong Li",
      "Peng Li",
      "Po Hu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.504": {
    "title": "Find-2-Find: Multitask Learning for Anaphora Resolution and Object Localization",
    "volume": "main",
    "abstract": "In multimodal understanding tasks, visual and linguistic ambiguities can arise. Visual ambiguity can occur when visual objects require a model to ground a referring expression in a video without strong supervision, while linguistic ambiguity can occur from changes in entities in action flows. As an example from the cooking domain, \"oil\" mixed with \"salt\" and \"pepper\" could later be referred to as a \"mixture\". Without a clear visual-linguistic alignment, we cannot know which among several objects shown is referred to by the language expression \"mixture\", and without resolved antecedents, we cannot pinpoint what the mixture is. We define this chicken-and-egg problem as Visual-linguistic Ambiguity. In this paper, we present Find2Find, a joint anaphora resolution and object localization dataset targeting the problem of visual-linguistic ambiguity, consisting of 500 anaphora-annotated recipes with corresponding videos. We present experimental results of a novel end-to-end joint multitask learning framework for Find2Find that fuses visual and textual information and shows improvements both for anaphora resolution and object localization with one joint model in multitask learning, as compared to a strong single-task baseline",
    "checked": true,
    "id": "bbf741c8417da2fe866acfa41448f1d377cfcc6f",
    "semantic_title": "find-2-find: multitask learning for anaphora resolution and object localization",
    "citation_count": 0,
    "authors": [
      "Cennet Oguz",
      "Pascal Denis",
      "Emmanuel Vincent",
      "Simon Ostermann",
      "Josef van Genabith"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.505": {
    "title": "Background Summarization of Event Timelines",
    "volume": "main",
    "abstract": "Generating concise summaries of news events is a challenging natural language processing task. While journalists often curate timelines to highlight key sub-events, newcomers to a news event face challenges in catching up on its historical context. In this paper, we address this need by introducing the task of background news summarization, which complements each timeline update with a background summary of relevant preceding events. We construct a dataset by merging existing timeline datasets and asking human annotators to write a background summary for each timestep of each news event. We establish strong baseline performance using state-of-the-art summarization systems and propose a query-focused variant to generate background summaries. To evaluate background summary quality, we present a question-answering-based evaluation metric, Background Utility Score (BUS), which measures the percentage of questions about a current event timestep that a background summary answers. Our experiments show the effectiveness of instruction fine-tuned systems such as Flan-T5, in addition to strong zero-shot performance using GPT-3.5",
    "checked": true,
    "id": "e3c648a8f58642d3fa52fea3328f7fa7299a3c56",
    "semantic_title": "background summarization of event timelines",
    "citation_count": 0,
    "authors": [
      "Adithya Pratapa",
      "Kevin Small",
      "Markus Dreyer"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.506": {
    "title": "Superlim: A Swedish Language Understanding Evaluation Benchmark",
    "volume": "main",
    "abstract": "We present Superlim, a multi-task NLP benchmark and analysis platform for evaluating Swedish language models, a counterpart to the English-language (Super)GLUE suite. We describe the dataset, the tasks, the leaderboard and report the baseline results yielded by a reference implementation. The tested models do not approach ceiling performance on any of the tasks, which suggests that Superlim is truly difficult, a desirable quality for a benchmark. We address methodological challenges, such as mitigating the Anglocentric bias when creating datasets for a less-resourced language; choosing the most appropriate measures; documenting the datasets and making the leaderboard convenient and transparent. We also highlight other potential usages of the dataset, such as, for instance, the evaluation of cross-lingual transfer learning",
    "checked": true,
    "id": "409d263d58765984eac4f4e214891fbbc23a3218",
    "semantic_title": "superlim: a swedish language understanding evaluation benchmark",
    "citation_count": 0,
    "authors": [
      "Aleksandrs Berdicevskis",
      "Gerlof Bouma",
      "Robin Kurtz",
      "Felix Morger",
      "Joey Öhman",
      "Yvonne Adesam",
      "Lars Borin",
      "Dana Dannélls",
      "Markus Forsberg",
      "Tim Isbister",
      "Anna Lindahl",
      "Martin Malmsten",
      "Faton Rekathati",
      "Magnus Sahlgren",
      "Elena Volodina",
      "Love Börjeson",
      "Simon Hengchen",
      "Nina Tahmasebi"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.507": {
    "title": "Reasoning with Language Model is Planning with World Model",
    "volume": "main",
    "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities, particularly with Chain-of-Thought-style prompts. However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks or performing complex math or logical reasoning. This is due to LLMs' absence of an internal world model for predicting world states (e.g., environment status, variable values) and simulating long-term action outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, Reasoning via Planning (RAP). RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monte Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, properly balancing exploration v.s. exploitation to achieve a high-reward reasoning path efficiently. We apply RAP to a variety of challenging reasoning problems, such as plan generation, math reasoning, and logical inference. Empirical results demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency, e.g., RAP on LLaMA-33B surpasses CoT on GPT-4 with 33% relative improvement in plan generation",
    "checked": true,
    "id": "5dbffedcabe3fa43060ebbe2b1789500edfd871f",
    "semantic_title": "reasoning with language model is planning with world model",
    "citation_count": 54,
    "authors": [
      "Shibo Hao",
      "Yi Gu",
      "Haodi Ma",
      "Joshua Hong",
      "Zhen Wang",
      "Daisy Wang",
      "Zhiting Hu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.508": {
    "title": "LLM-enhanced Self-training for Cross-domain Constituency Parsing",
    "volume": "main",
    "abstract": "Self-training has proven to be an effective approach for cross-domain tasks, and in this study, we explore its application to cross-domain constituency parsing. Traditional self-training methods rely on limited and potentially low-quality raw corpora. To overcome this limitation, we propose enhancing self-training with the large language model (LLM) to generate domain-specific raw corpora iteratively. For the constituency parsing, we introduce grammar rules that guide the LLM in generating raw corpora and establish criteria for selecting pseudo instances. Our experimental results demonstrate that self-training for constituency parsing, equipped with an LLM, outperforms traditional methods regardless of the LLM's performance. Moreover, the combination of grammar rules and confidence criteria for pseudo-data selection yields the highest performance in the cross-domain constituency parsing",
    "checked": true,
    "id": "af52fda0ede2f40d53cbe76a3bc1e3a82e3e723d",
    "semantic_title": "llm-enhanced self-training for cross-domain constituency parsing",
    "citation_count": 0,
    "authors": [
      "Jianling Li",
      "Meishan Zhang",
      "Peiming Guo",
      "Min Zhang",
      "Yue Zhang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.509": {
    "title": "Continual Named Entity Recognition without Catastrophic Forgetting",
    "volume": "main",
    "abstract": "Continual Named Entity Recognition (CNER) is a burgeoning area, which involves updating an existing model by incorporating new entity types sequentially. Nevertheless, continual learning approaches are often severely afflicted by catastrophic forgetting. This issue is intensified in CNER due to the consolidation of old entity types from previous steps into the non-entity type at each step, leading to what is known as the semantic shift problem of the non-entity type. In this paper, we introduce a pooled feature distillation loss that skillfully navigates the trade-off between retaining knowledge of old entity types and acquiring new ones, thereby more effectively mitigating the problem of catastrophic forgetting. Additionally, we develop a confidence-based pseudo-labeling for the non-entity type, i.e., predicting entity types using the old model to handle the semantic shift of the non-entity type. Following the pseudo-labeling process, we suggest an adaptive re-weighting type-balanced learning strategy to handle the issue of biased type distribution. We carried out comprehensive experiments on ten CNER settings using three different datasets. The results illustrate that our method significantly outperforms prior state-of-the-art approaches, registering an average improvement of 6.3% and 8.0% in Micro and Macro F1 scores, respectively",
    "checked": true,
    "id": "c1a2a16a5ba8b082677e9e4de7d6c113cd0f3d2c",
    "semantic_title": "continual named entity recognition without catastrophic forgetting",
    "citation_count": 0,
    "authors": [
      "Duzhen Zhang",
      "Wei Cong",
      "Jiahua Dong",
      "Yahan Yu",
      "Xiuyi Chen",
      "Yonggang Zhang",
      "Zhen Fang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.510": {
    "title": "DSI++: Updating Transformer Memory with New Documents",
    "volume": "main",
    "abstract": "Differentiable Search Indices (DSIs) encode a corpus of documents in the parameters of a model and use the same model to map queries directly to relevant document identifiers. Despite the solid performance of DSI models, successfully deploying them in scenarios where document corpora change with time is an open problem. In this work, we introduce DSI++, a continual learning challenge for DSI with the goal of continuously indexing new documents while being able to answer queries related to both previously and newly indexed documents. Across different model scales and document identifier representations, we show that continual indexing of new documents leads to considerable forgetting of previously indexed documents. We also hypothesize and verify that the model experiences forgetting events during training, leading to unstable learning. To mitigate these issues, we investigate two approaches. The first focuses on modifying the training dynamics. Flatter minima implicitly alleviates forgetting, so we explicitly optimize for flatter loss basins and show that the model stably memorizes more documents (+12%). Next, we introduce a parametric memory to generate pseudo-queries for documents and supplement them during incremental indexing to prevent forgetting for the retrieval task. Extensive experiments on a novel continual indexing benchmark based on Natural Questions demonstrate that our proposed solution mitigates the forgetting in DSI++ by a significant margin and improves the average Hits@10 by +21.1% over competitive baselines",
    "checked": true,
    "id": "eebe62c685bffd1c8880fb3ce70e3b6d42b4b9e9",
    "semantic_title": "dsi++: updating transformer memory with new documents",
    "citation_count": 10,
    "authors": [
      "Sanket Mehta",
      "Jai Gupta",
      "Yi Tay",
      "Mostafa Dehghani",
      "Vinh Tran",
      "Jinfeng Rao",
      "Marc Najork",
      "Emma Strubell",
      "Donald Metzler"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.511": {
    "title": "Editing Common Sense in Transformers",
    "volume": "main",
    "abstract": "Editing model parameters directly in Transformers makes updating open-source transformer-based models possible without re-training. However, these editing methods have only been evaluated on statements about encyclopedic knowledge with a single correct answer. Commonsense knowledge with multiple correct answers, e.g., an apple can be green or red but not transparent, has not been studied but is as essential for enhancing transformers' reliability and usefulness. In this paper, we investigate whether commonsense judgments are causally associated with localized, editable parameters in Transformers, and we provide an affirmative answer. We find that directly applying the MEMIT editing algorithm results in sub-par performance and improve it for the commonsense domain by varying edit tokens and improving the layer selection strategy, i.e., MEMITCSK. GPT-2 Large and XL models edited using MEMITCSK outperform best-fine-tuned baselines by 10.97% and 10.73% F1 scores on PEP3k and 20Q datasets. In addition, we propose a novel evaluation dataset, PROBE\\SET, that contains unaffected and affected neighborhoods, affected paraphrases, and affected reasoning challenges. MEMITCSK performs well across the metrics while fine-tuning baselines show significant trade-offs between unaffected and affected metrics. These results suggest a compelling future direction for incorporating feedback about common sense into Transformers through direct model editing",
    "checked": true,
    "id": "2b72888cc3ff048038f6011b8e3d89ba106540b6",
    "semantic_title": "editing common sense in transformers",
    "citation_count": 0,
    "authors": [
      "Anshita Gupta",
      "Debanjan Mondal",
      "Akshay Sheshadri",
      "Wenlong Zhao",
      "Xiang Li",
      "Sarah Wiegreffe",
      "Niket Tandon"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.512": {
    "title": "Air-Decoding: Attribute Distribution Reconstruction for Decoding-Time Controllable Text Generation",
    "volume": "main",
    "abstract": "Controllable text generation (CTG) aims to generate text with desired attributes, and decoding-time-based methods have shown promising performance on this task. However, in this paper, we identify the phenomenon of Attribute Collapse for the first time. It causes the fluency of generated text to rapidly decrease when the control strength exceeds a critical value, rendering the text completely unusable. This limitation hinders the effectiveness of decoding methods in achieving high levels of controllability. To address this problem, we propose a novel lightweight decoding framework named Air-Decoding. Its main idea is reconstructing the attribute distributions to balance the weights between attribute words and non-attribute words to generate more fluent text. Specifically, we train prefixes by prefix-tuning to obtain attribute distributions. Then we design a novel attribute distribution reconstruction method to balance the obtained distributions and use the reconstructed distributions to guide language models for generation, effectively avoiding the issue of Attribute Collapse. Experiments on multiple CTG tasks prove that our method achieves a new state-of-the-art control performance",
    "checked": true,
    "id": "062bbb6474309d3c42397d8ab808505a91ca6ef2",
    "semantic_title": "air-decoding: attribute distribution reconstruction for decoding-time controllable text generation",
    "citation_count": 0,
    "authors": [
      "Tianqi Zhong",
      "Quan Wang",
      "Jingxuan Han",
      "Yongdong Zhang",
      "Zhendong Mao"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.513": {
    "title": "Homophone Disambiguation Reveals Patterns of Context Mixing in Speech Transformers",
    "volume": "main",
    "abstract": "Transformers have become a key architecture in speech processing, but our understanding of how they build up representations of acoustic and linguistic structure is limited. In this study, we address this gap by investigating how measures of ‘context-mixing' developed for text models can be adapted and applied to models of spoken language. We identify a linguistic phenomenon that is ideal for such a case study: homophony in French (e.g. livre vs livres), where a speech recognition model has to attend to syntactic cues such as determiners and pronouns in order to disambiguate spoken words with identical pronunciations and transcribe them while respecting grammatical agreement. We perform a series of controlled experiments and probing analyses on Transformer-based speech models. Our findings reveal that representations in encoder-only models effectively incorporate these cues to identify the correct transcription, whereas encoders in encoder-decoder models mainly relegate the task of capturing contextual dependencies to decoder modules",
    "checked": true,
    "id": "5be6131acc7cebddca8c1e5ed86fa2ba2de8d44d",
    "semantic_title": "homophone disambiguation reveals patterns of context mixing in speech transformers",
    "citation_count": 0,
    "authors": [
      "Hosein Mohebbi",
      "Grzegorz Chrupała",
      "Willem Zuidema",
      "Afra Alishahi"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.514": {
    "title": "Retrieval-Generation Alignment for End-to-End Task-Oriented Dialogue System",
    "volume": "main",
    "abstract": "Developing an efficient retriever to retrieve knowledge from a large-scale knowledge base (KB) is critical for task-oriented dialogue systems to effectively handle localized and specialized tasks. However, widely used generative models such as T5 and ChatGPT often struggle to differentiate subtle differences among the retrieved KB records when generating responses, resulting in suboptimal quality of generated responses. In this paper, we propose the application of maximal marginal likelihood to train a perceptive retriever by utilizing signals from response generation for supervision. In addition, our approach goes beyond considering solely retrieved entities and incorporates various meta knowledge to guide the generator, thus improving the utilization of knowledge. We evaluate our approach on three task-oriented dialogue datasets using T5 and ChatGPT as the backbone models. The results demonstrate that when combined with meta knowledge, the response generator can effectively leverage high-quality knowledge records from the retriever and enhance the quality of generated responses. The code of this work is available at https://github.com/shenwzh3/MK-TOD",
    "checked": true,
    "id": "43a47e64654e21c651aa5f4ddd202913e2698e5a",
    "semantic_title": "retrieval-generation alignment for end-to-end task-oriented dialogue system",
    "citation_count": 0,
    "authors": [
      "Weizhou Shen",
      "Yingqi Gao",
      "Canbin Huang",
      "Fanqi Wan",
      "Xiaojun Quan",
      "Wei Bi"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.515": {
    "title": "IfQA: A Dataset for Open-domain Question Answering under Counterfactual Presuppositions",
    "volume": "main",
    "abstract": "Although counterfactual reasoning is a fundamental aspect of intelligence, the lack of large-scale counterfactual open-domain question-answering (QA) benchmarks makes it difficult to evaluate and improve models on this ability. To address this void, we introduce the first such dataset, named IfQA, where each question is based on a counterfactual presupposition via an \"if\" clause. Such questions require models to go beyond retrieving direct factual knowledge from the Web: they must identify the right information to retrieve and reason about an imagined situation that may even go against the facts built into their parameters. The IfQA dataset contains 3,800 questions that were annotated by crowdworkers on relevant Wikipedia passages. Empirical analysis reveals that the IfQA dataset is highly challenging for existing open-domain QA methods, including supervised retrieve-then-read pipeline methods (F1 score 44.5), as well as recent few-shot approaches such as chain-of-thought prompting with ChatGPT (F1 score 57.2). We hope the unique challenges posed by IfQA will push open-domain QA research on both retrieval and reasoning fronts, while also helping endow counterfactual reasoning abilities to today's language understanding models",
    "checked": true,
    "id": "138a78320301b6a3500f7752146d1a560de3aa32",
    "semantic_title": "ifqa: a dataset for open-domain question answering under counterfactual presuppositions",
    "citation_count": 6,
    "authors": [
      "Wenhao Yu",
      "Meng Jiang",
      "Peter Clark",
      "Ashish Sabharwal"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.516": {
    "title": "How Do Large Language Models Capture the Ever-changing World Knowledge? A Review of Recent Advances",
    "volume": "main",
    "abstract": "Although large language models (LLMs) are impressive in solving various tasks, they can quickly be outdated after deployment. Maintaining their up-to-date status is a pressing concern in the current era. This paper provides a comprehensive review of recent advances in aligning deployed LLMs with the ever-changing world knowledge. We categorize research works systemically and provide in-depth comparisons and discussions. We also discuss existing challenges and highlight future directions to facilitate research in this field",
    "checked": true,
    "id": "90ff14a19419a0b6bb0965f0ab5e359462556172",
    "semantic_title": "how do large language models capture the ever-changing world knowledge? a review of recent advances",
    "citation_count": 0,
    "authors": [
      "Zihan Zhang",
      "Meng Fang",
      "Ling Chen",
      "Mohammad-Reza Namazi-Rad",
      "Jun Wang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.517": {
    "title": "PreWoMe: Exploiting Presuppositions as Working Memory for Long Form Question Answering",
    "volume": "main",
    "abstract": "Information-seeking questions in long-form question answering (LFQA) often prove misleading due to ambiguity or false presupposition in the question. While many existing approaches handle misleading questions, they are tailored to limited questions, which are insufficient in a real-world setting with unpredictable input characteristics. In this work, we propose PreWoMe, a unified approach capable of handling any type of information-seeking question. The key idea of PreWoMe involves extracting presuppositions in the question and exploiting them as working memory to generate feedback and action about the question. Our experiment shows that PreWoMe is effective not only in tackling misleading questions but also in handling normal ones, thereby demonstrating the effectiveness of leveraging presuppositions, feedback, and action for real-world QA settings",
    "checked": true,
    "id": "89abd6efeec5f90b6feea13c739262d417b27cbf",
    "semantic_title": "prewome: exploiting presuppositions as working memory for long form question answering",
    "citation_count": 0,
    "authors": [
      "Wookje Han",
      "Jinsol Park",
      "Kyungjae Lee"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.518": {
    "title": "Memorisation Cartography: Mapping out the Memorisation-Generalisation Continuum in Neural Machine Translation",
    "volume": "main",
    "abstract": "When training a neural network, it will quickly memorise some source-target mappings from your dataset but never learn some others. Yet, memorisation is not easily expressed as a binary feature that is good or bad: individual datapoints lie on a memorisation-generalisation continuum. What determines a datapoint's position on that spectrum, and how does that spectrum influence neural models' performance? We address these two questions for neural machine translation (NMT) models. We use the counterfactual memorisation metric to (1) build a resource that places 5M NMT datapoints on a memorisation-generalisation map, (2) illustrate how the datapoints' surface-level characteristics and a models' per-datum training signals are predictive of memorisation in NMT, (3) and describe the influence that subsets of that map have on NMT systems' performance",
    "checked": true,
    "id": "43b5c36dd6ba3ad666ed4cef3eeb01bd096d78bc",
    "semantic_title": "memorisation cartography: mapping out the memorisation-generalisation continuum in neural machine translation",
    "citation_count": 0,
    "authors": [
      "Verna Dankers",
      "Ivan Titov",
      "Dieuwke Hupkes"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.519": {
    "title": "DecipherPref: Analyzing Influential Factors in Human Preference Judgments via GPT-4",
    "volume": "main",
    "abstract": "Human preference judgments are pivotal in guiding large language models (LLMs) to produce outputs that align with human values. Human evaluations are also used in summarization tasks to compare outputs from various systems, complementing existing automatic metrics. Despite their significance, however, there has been limited research probing these pairwise or k-wise comparisons. The collective impact and relative importance of factors such as output length, informativeness, fluency, and factual consistency are still not well understood. It is also unclear if there are other hidden factors influencing human judgments. In this paper, we conduct an in-depth examination of a collection of pairwise human judgments released by OpenAI. Utilizing the Bradley-Terry-Luce (BTL) model, we reveal the inherent preferences embedded in these human judgments. We find that the most favored factors vary across tasks and genres, whereas the least favored factors tend to be consistent, e.g., outputs are too brief, contain excessive off-focus content or hallucinated facts. Our findings have implications on the construction of balanced datasets in human preference evaluations, which is a crucial step in shaping the behaviors of future LLMs",
    "checked": true,
    "id": "8f49bd5a69d64d0547a9e8c59cb7f91fcb5ed3ab",
    "semantic_title": "decipherpref: analyzing influential factors in human preference judgments via gpt-4",
    "citation_count": 0,
    "authors": [
      "Yebowen Hu",
      "Kaiqiang Song",
      "Sangwoo Cho",
      "Xiaoyang Wang",
      "Hassan Foroosh",
      "Fei Liu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.520": {
    "title": "Gender Biases in Automatic Evaluation Metrics for Image Captioning",
    "volume": "main",
    "abstract": "Model-based evaluation metrics (e.g., CLIPScore and GPTScore) have demonstrated decent correlations with human judgments in various language generation tasks. However, their impact on fairness remains largely unexplored. It is widely recognized that pretrained models can inadvertently encode societal biases, thus employing these models for evaluation purposes may inadvertently perpetuate and amplify biases. For example, an evaluation metric may favor the caption \"a woman is calculating an account book\" over \"a man is calculating an account book,\" even if the image only shows male accountants. In this paper, we conduct a systematic study of gender biases in model-based automatic evaluation metrics for image captioning tasks. We start by curating a dataset comprising profession, activity, and object concepts associated with stereotypical gender associations. Then, we demonstrate the negative consequences of using these biased metrics, including the inability to differentiate between biased and unbiased generations, as well as the propagation of biases to generation models through reinforcement learning. Finally, we present a simple and effective way to mitigate the metric bias without hurting the correlations with human judgments. Our dataset and framework lay the foundation for understanding the potential harm of model-based evaluation metrics, and facilitate future works to develop more inclusive evaluation metrics",
    "checked": true,
    "id": "7751d96028e5a198a0b95257d183801d5cd73633",
    "semantic_title": "gender biases in automatic evaluation metrics for image captioning",
    "citation_count": 0,
    "authors": [
      "Haoyi Qiu",
      "Zi-Yi Dou",
      "Tianlu Wang",
      "Asli Celikyilmaz",
      "Nanyun Peng"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.521": {
    "title": "QA-NatVer: Question Answering for Natural Logic-based Fact Verification",
    "volume": "main",
    "abstract": "Fact verification systems assess a claim's veracity based on evidence. An important consideration in designing them is faithfulness, i.e. generating explanations that accurately reflect the reasoning of the model. Recent works have focused on natural logic, which operates directly on natural language by capturing the semantic relation of spans between an aligned claim with its evidence via set-theoretic operators. However, these approaches rely on substantial resources for training, which are only available for high-resource languages. To this end, we propose to use question answering to predict natural logic operators, taking advantage of the generalization capabilities of instruction-tuned language models. Thus, we obviate the need for annotated training data while still relying on a deterministic inference system. In a few-shot setting on FEVER, our approach outperforms the best baseline by 4.3 accuracy points, including a state-of-the-art pre-trained seq2seq natural logic system, as well as a state-of-the-art prompt-based classifier. Our system demonstrates its robustness and portability, achieving competitive performance on a counterfactual dataset and surpassing all approaches without further annotation on a Danish verification dataset. A human evaluation indicates that our approach produces more plausible proofs with fewer erroneous natural logic operators than previous natural logic-based systems",
    "checked": true,
    "id": "530352a8ea8d85e4e4dc96f9d3718ea5a1858bec",
    "semantic_title": "qa-natver: question answering for natural logic-based fact verification",
    "citation_count": 0,
    "authors": [
      "Rami Aly",
      "Marek Strong",
      "Andreas Vlachos"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.522": {
    "title": "Increasing Probability Mass on Answer Choices Does Not Always Improve Accuracy",
    "volume": "main",
    "abstract": "When pretrained language models (LMs) are applied to discriminative tasks such as multiple-choice questions, they place probability mass on vocabulary tokens that aren't among the given answer choices. Spreading probability mass across multiple surface forms with identical meaning (such as \"bath\" and \"bathtub\") is thought to cause an underestimation of a model's true performance, referred to as the \"surface form competition\" (SFC) hypothesis. This has motivated the introduction of various probability normalization methods. However, many core questions remain unanswered. How do we measure SFC? Are there direct ways of reducing it, and does doing so improve task performance? We propose a mathematical formalism for SFC which allows us to quantify and bound its impact for the first time. We identify a simple method for reducing it—namely, increasing probability mass on the given answer choices by a) including them in the prompt and b) using in-context learning with even just one example. We show this method eliminates the impact of SFC in the majority of instances. Our experiments on three diverse datasets and six LMs reveal several additional surprising findings. For example, both normalization and prompting methods for reducing SFC can be ineffective or even detrimental to task performance for some LMs. We conclude with practical insights for effectively prompting LMs for multiple-choice tasks",
    "checked": true,
    "id": "64d803276417158d5d459084dfae391a23143aa3",
    "semantic_title": "increasing probability mass on answer choices does not always improve accuracy",
    "citation_count": 0,
    "authors": [
      "Sarah Wiegreffe",
      "Matthew Finlayson",
      "Oyvind Tafjord",
      "Peter Clark",
      "Ashish Sabharwal"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.523": {
    "title": "Generating Data for Symbolic Language with Large Language Models",
    "volume": "main",
    "abstract": "While large language models (LLMs) bring not only performance but also complexity, recent work has started to turn LLMs into data generators rather than task inferencers, where another affordable task model is trained for efficient deployment and inference. However, such an approach has primarily been applied to natural language tasks, and has not yet been explored for symbolic language tasks with complex structured outputs (e.g., semantic parsing and code generation). In this paper, we propose SymGen which utilizes LLMs for generating various annotation-expensive symbolic language data. SymGen consists of an informative prompt to steer generation and an agreement-based verifier to improve data correctness. We conduct extensive experiments on six symbolic language tasks across various settings. Compared with the LLMs, we demonstrate the 1%-sized task model can achieve comparable or better performance, largely cutting inference and deployment costs. We also show that generated data with only a few human demonstrations can be as effective as over 10 times the amount of human-annotated data when training the task model, saving a considerable amount of annotation effort. SymGen takes a step toward data generation for annotation-expensive complex tasks, and we release the code at URL",
    "checked": true,
    "id": "6bf981314d81ca838d2cc55fc6f6265717792b67",
    "semantic_title": "generating data for symbolic language with large language models",
    "citation_count": 4,
    "authors": [
      "Jiacheng Ye",
      "Chengzu Li",
      "Lingpeng Kong",
      "Tao Yu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.524": {
    "title": "IDTraffickers: An Authorship Attribution Dataset to link and connect Potential Human-Trafficking Operations on Text Escort Advertisements",
    "volume": "main",
    "abstract": "Human trafficking (HT) is a pervasive global issue affecting vulnerable individuals, violating their fundamental human rights. Investigations reveal that a significant number of HT cases are associated with online advertisements (ads), particularly in escort markets. Consequently, identifying and connecting HT vendors has become increasingly challenging for Law Enforcement Agencies (LEAs). To address this issue, we introduce IDTraffickers, an extensive dataset consisting of 87,595 text ads and 5,244 vendor labels to enable the verification and identification of potential HT vendors on online escort markets. To establish a benchmark for authorship identification, we train a DeCLUTR-small model, achieving a macro-F1 score of 0.8656 in a closed-set classification environment. Next, we leverage the style representations extracted from the trained classifier to conduct authorship verification, resulting in a mean r-precision score of 0.8852 in an open-set ranking environment. Finally, to encourage further research and ensure responsible data sharing, we plan to release IDTraffickers for the authorship attribution task to researchers under specific conditions, considering the sensitive nature of the data. We believe that the availability of our dataset and benchmarks will empower future researchers to utilize our findings, thereby facilitating the effective linkage of escort ads and the development of more robust approaches for identifying HT indicators",
    "checked": true,
    "id": "221244d5c567301ddecd628e5a67e942f23e4ae4",
    "semantic_title": "idtraffickers: an authorship attribution dataset to link and connect potential human-trafficking operations on text escort advertisements",
    "citation_count": 0,
    "authors": [
      "Vageesh Saxena",
      "Benjamin Ashpole",
      "Gijs van Dijck",
      "Gerasimos Spanakis"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.525": {
    "title": "Evaluating Bias and Fairness in Gender-Neutral Pretrained Vision-and-Language Models",
    "volume": "main",
    "abstract": "Pretrained machine learning models are known to perpetuate and even amplify existing biases in data, which can result in unfair outcomes that ultimately impact user experience. Therefore, it is crucial to understand the mechanisms behind those prejudicial biases to ensure that model performance does not result in discriminatory behaviour toward certain groups or populations. In this work, we define gender bias as our case study. We quantify bias amplification in pretraining and after fine-tuning on three families of vision-and-language models. We investigate the connection, if any, between the two learning stages, and evaluate how bias amplification reflects on model performance. Overall, we find that bias amplification in pretraining and after fine-tuning are independent. We then examine the effect of continued pretraining on gender-neutral data, finding that this reduces group disparities, i.e., promotes fairness, on VQAv2 and retrieval tasks without significantly compromising task performance",
    "checked": true,
    "id": "56128cecc92acb881f339bd5d85e1bba5b4d960c",
    "semantic_title": "evaluating bias and fairness in gender-neutral pretrained vision-and-language models",
    "citation_count": 0,
    "authors": [
      "Laura Cabello",
      "Emanuele Bugliarello",
      "Stephanie Brandl",
      "Desmond Elliott"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.526": {
    "title": "Improving Dialogue Discourse Parsing via Reply-to Structures of Addressee Recognition",
    "volume": "main",
    "abstract": "Dialogue discourse parsing aims to reflect the relation-based structure of dialogue by establishing discourse links according to discourse relations. To alleviate data sparsity, previous studies have adopted multitasking approaches to jointly learn dialogue discourse parsing with related tasks (e.g., reading comprehension) that require additional human annotation, thus limiting their generality. In this paper, we propose a multitasking framework that integrates dialogue discourse parsing with its neighboring task addressee recognition. Addressee recognition reveals the reply-to structure that partially overlaps with the relation-based structure, which can be exploited to facilitate relation-based structure learning. To this end, we first proposed a reinforcement learning agent to identify training examples from addressee recognition that are most helpful for dialog discourse parsing. Then, a task-aware structure transformer is designed to capture the shared and private dialogue structure of different tasks, thereby further promoting dialogue discourse parsing. Experimental results on both the Molweni and STAC datasets show that our proposed method can outperform the SOTA baselines. The code will be available at https://github.com/yxfanSuda/RLTST",
    "checked": true,
    "id": "7baadac9141ee54bc734af0a80e5b931d5987fb3",
    "semantic_title": "improving dialogue discourse parsing via reply-to structures of addressee recognition",
    "citation_count": 0,
    "authors": [
      "Yaxin Fan",
      "Feng Jiang",
      "Peifeng Li",
      "Fang Kong",
      "Qiaoming Zhu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.527": {
    "title": "Improving Language Models' Meaning Understanding and Consistency by Learning Conceptual Roles from Dictionary",
    "volume": "main",
    "abstract": "The non-humanlike behaviour of contemporary pre-trained language models (PLMs) is a leading cause undermining their trustworthiness. A striking phenomenon of such faulty behaviours is the generation of inconsistent predictions, which produces logically contradictory results, such as generating different predictions for texts delivering the same meaning or violating logical properties. Previous studies exploited data augmentation or implemented specialised loss functions to alleviate the issue. However, their usage is limited, because they consume expensive training resources for large-sized PLMs and can only handle a certain consistency type. To this end, we propose a practical approach that alleviates the inconsistent behaviour issue by fundamentally improving PLMs' meaning awareness. Based on the conceptual role theory, our method allows PLMs to capture accurate meaning by learning precise interrelationships between concepts from word-definition pairs in a dictionary. Next, we propose an efficient parameter integration technique that updates only a few additional parameters to combine the learned interrelationship with PLMs' pre-trained knowledge. Our experimental results reveal that the approach can concurrently improve multiple types of consistency, enables efficient knowledge integration, and easily applies to other languages",
    "checked": false,
    "id": "a5246f262bc0919bbfbc4b0fa66e85be41d92712",
    "semantic_title": "improving language models meaning understanding and consistency by learning conceptual roles from dictionary",
    "citation_count": 0,
    "authors": [
      "Myeongjun Jang",
      "Thomas Lukasiewicz"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.528": {
    "title": "DALE: Generative Data Augmentation for Low-Resource Legal NLP",
    "volume": "main",
    "abstract": "We present DALE, a novel and effective generative Data Augmentation framework for low-resource LEgal NLP. DALE addresses the challenges existing frameworks pose in generating effective data augmentations of legal documents - legal language, with its specialized vocabulary and complex semantics, morphology, and syntax, does not benefit from data augmentations that merely rephrase the source sentence. To address this, DALE, built on an Encoder-Decoder Language Model, is pre-trained on a novel unsupervised text denoising objective based on selective masking - our masking strategy exploits the domain-specific language characteristics of templatized legal documents to mask collocated spans of text. Denoising these spans help DALE acquire broad legal knowledge and develop the ability to generate coherent and diverse augmentations with novel contexts. Finally, DALE performs conditional generation to generate synthetic augmentations for low-resource Legal NLP tasks. We demonstrate the effectiveness of DALE on 13 datasets spanning 6 tasks and 4 low-resource settings. DALE outperforms all our baselines, including LLMs, qualitatively and quantitatively, with absolute improvements of 1%-50%",
    "checked": true,
    "id": "a21ef5944c644b8e0f1737ab180a8e1887d83cb7",
    "semantic_title": "dale: generative data augmentation for low-resource legal nlp",
    "citation_count": 0,
    "authors": [
      "Sreyan Ghosh",
      "Chandra Kiran Reddy Evuru",
      "Sonal Kumar",
      "S Ramaneswaran",
      "S Sakshi",
      "Utkarsh Tyagi",
      "Dinesh Manocha"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.529": {
    "title": "FedID: Federated Interactive Distillation for Large-Scale Pretraining Language Models",
    "volume": "main",
    "abstract": "The growing concerns and regulations surrounding the protection of user data privacy have necessitated decentralized training paradigms. To this end, federated learning (FL) is widely studied in user-related natural language processing (NLP). However, it suffers from several critical limitations including extensive communication overhead, inability to handle heterogeneity, and vulnerability to white-box inference attacks. Federated distillation (FD) is proposed to alleviate these limitations, but its performance is faded by confirmation bias. To tackle this issue, we propose Federated Interactive Distillation (FedID), which utilizes a small amount of labeled data retained by the server to further rectify the local models during knowledge transfer. Additionally, based on the GLUE benchmark, we develop a benchmarking framework across multiple tasks with diverse data distributions to contribute to the research of FD in NLP community. Experiments show that our proposed FedID framework achieves the best results in homogeneous and heterogeneous federated scenarios. The code for this paper is available at: https://github.com/maxinge8698/FedID",
    "checked": true,
    "id": "ebb8e126e3a9c17d220240af07f2e34fab7587d0",
    "semantic_title": "fedid: federated interactive distillation for large-scale pretraining language models",
    "citation_count": 0,
    "authors": [
      "Xinge Ma",
      "Jiangming Liu",
      "Jin Wang",
      "Xuejie Zhang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.530": {
    "title": "trlX: A Framework for Large Scale Reinforcement Learning from Human Feedback",
    "volume": "main",
    "abstract": "Reinforcement learning from human feedback (RLHF) utilizes human feedback to better align large language models with human preferences via online optimization against a learned reward model. Current RLHF paradigms rely on Proximal Policy Optimization (PPO), which quickly becomes a challenge to implement and scale up to large architectures. To address this difficulty we present the AutoRLHF library as a feature complete open-source framework for RLHF fine-tuning of models up to and exceeding 70 billion parameters. To do so we implement support for multiple types of distributed training including distributed data parallel, model sharded, as well as tensor, sequential, and pipeline parallelism. Additionally, we implement compute and memory saving features, giving AutoRLHF the flexibility to support users with a wide range of compute resources. This includes offline RL methods like Implicit Language Q Learning (ILQL) as a compute efficient alternative to PPO. We find offline fine-tuning offers competitive performance relative to online algorithms while being easier to implement, train, and scale. To evaluate our framework we train RLHF models on two separate well-known tasks using publicly available human preference data. Models trained with AutoRLHF achieve preference win-rates over baselines at rates comparable to the original works",
    "checked": true,
    "id": "57daf938ab134ba05a1bef4d596c2074d367e81e",
    "semantic_title": "trlx: a framework for large scale reinforcement learning from human feedback",
    "citation_count": 1,
    "authors": [
      "Alexander Havrilla",
      "Maksym Zhuravinskyi",
      "Duy Phung",
      "Aman Tiwari",
      "Jonathan Tow",
      "Stella Biderman",
      "Quentin Anthony",
      "Louis Castricato"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.531": {
    "title": "This is not a Dataset: A Large Negation Benchmark to Challenge Large Language Models",
    "volume": "main",
    "abstract": "Although large language models (LLMs) have apparently acquired a certain level of grammatical knowledge and the ability to make generalizations, they fail to interpret negation, a crucial step in Natural Language Processing. We try to clarify the reasons for the sub-optimal performance of LLMs understanding negation. We introduce a large semi-automatically generated dataset of circa 400,000 descriptive sentences about commonsense knowledge that can be true or false in which negation is present in about 2/3 of the corpus in different forms. We have used our dataset with the largest available open LLMs in a zero-shot approach to grasp their generalization and inference capability and we have also fine-tuned some of the models to assess whether the understanding of negation can be trained. Our findings show that, while LLMs are proficient at classifying affirmative sentences, they struggle with negative sentences and lack a deep understanding of negation, often relying on superficial cues. Although fine-tuning the models on negative sentences improves their performance, the lack of generalization in handling negation is persistent, highlighting the ongoing challenges of LLMs regarding negation understanding and generalization. The dataset and code are publicly available",
    "checked": true,
    "id": "bd03ff387e7b70422dd8ce081a6f0e23efa20cce",
    "semantic_title": "this is not a dataset: a large negation benchmark to challenge large language models",
    "citation_count": 0,
    "authors": [
      "Iker García-Ferrero",
      "Begoña Altuna",
      "Javier Alvez",
      "Itziar Gonzalez-Dios",
      "German Rigau"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.532": {
    "title": "MT2: Towards a Multi-Task Machine Translation Model with Translation-Specific In-Context Learning",
    "volume": "main",
    "abstract": "Sentence-level translation, document-level translation, translation memory, and terminology constrained translation play an important role in machine translation. Most of the previous work uses separate models or methods to solve these tasks, which is not conducive to knowledge transfer of different tasks and increases the complexity of system construction. In this work, we explore the potential of pre-trained language model in machine translation tasks and propose a Multi-Task Machine Translation (MT2) model to integrate these translation tasks. We design a novel translation-specific In-Context Learning (ICL) paradigm for model training, in which all of the translation tasks can be modeled as context-learning tasks that integrate contextual information for performance improvement. Specifically, we propose a retrieval and alignment method to obtain a large scale context-enhancement training data, then we train the model in an in-context learning manner. Furthermore, we adopt two context-dependent training strategies to encourage the model to better understand and utilize contextual information for translation. Extensive experiments on translation memory, terminology constrained translation, document-level translation, and few-shot domain-adaptation tasks demonstrate the superior performance of our model, verifying the effectiveness of our proposed approach",
    "checked": true,
    "id": "918b08a07d0578b600a624c9a120bebde7f47d5c",
    "semantic_title": "mt2: towards a multi-task machine translation model with translation-specific in-context learning",
    "citation_count": 0,
    "authors": [
      "Chunyou Li",
      "Mingtong Liu",
      "Hongxiao Zhang",
      "Yufeng Chen",
      "Jinan Xu",
      "Ming Zhou"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.533": {
    "title": "CleanCoNLL: A Nearly Noise-Free Named Entity Recognition Dataset",
    "volume": "main",
    "abstract": "The CoNLL-03 corpus is arguably the most well-known and utilized benchmark dataset for named entity recognition (NER). However, prior works found significant numbers of annotation errors, incompleteness, and inconsistencies in the data. This poses challenges to objectively comparing NER approaches and analyzing their errors, as current state-of-the-art models achieve F1-scores that are comparable to or even exceed the estimated noise level in CoNLL-03. To address this issue, we present a comprehensive relabeling effort assisted by automatic consistency checking that corrects 7.0% of all labels in the English CoNLL-03. Our effort adds a layer of entity linking annotation both for better explainability of NER labels and as additional safeguard of annotation quality. Our experimental evaluation finds not only that state-of-the-art approaches reach significantly higher F1-scores (97.1%) on our data, but crucially that the share of correct predictions falsely counted as errors due to annotation noise drops from 47% to 6%. This indicates that our resource is well suited to analyze the remaining errors made by state-of-the-art models, and that the theoretical upper bound even on high resource, coarse-grained NER is not yet reached. To facilitate such analysis, we make CleanCoNLL publicly available to the research community",
    "checked": true,
    "id": "41f53a29be49b958e92faeba5520f7f44a60443e",
    "semantic_title": "cleanconll: a nearly noise-free named entity recognition dataset",
    "citation_count": 0,
    "authors": [
      "Susanna Rücker",
      "Alan Akbik"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.534": {
    "title": "Disentangling Transformer Language Models as Superposed Topic Models",
    "volume": "main",
    "abstract": "Topic Modelling is an established research area where the quality of a given topic is measured using coherence metrics. Often, we infer topics from Neural Topic Models (NTM) by interpreting their decoder weights, consisting of top-activated words projected from individual neurons. Transformer-based Language Models (TLM) similarly consist of decoder weights. However, due to its hypothesised superposition properties, the final logits originating from the residual path are considered uninterpretable. Therefore, we posit that we can interpret TLM as superposed NTM by proposing a novel weight-based, model-agnostic and corpus-agnostic approach to search and disentangle decoder-only TLM, potentially mapping individual neurons to multiple coherent topics. Our results show that it is empirically feasible to disentangle coherent topics from GPT-2 models using the Wikipedia corpus. We validate this approach for GPT-2 models using Zero-Shot Topic Modelling. Finally, we extend the proposed approach to disentangle and analyse LLaMA models",
    "checked": true,
    "id": "c769687bf581aa5dd9cafd1aa0e0e2ed6de15ff0",
    "semantic_title": "disentangling transformer language models as superposed topic models",
    "citation_count": 0,
    "authors": [
      "Jia Peng Lim",
      "Hady Lauw"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.535": {
    "title": "Conversational Semantic Parsing using Dynamic Context Graphs",
    "volume": "main",
    "abstract": "In this paper we consider the task of conversational semantic parsing over general purpose knowledge graphs (KGs) with millions of entities, and thousands of relation-types. We focus on models which are capable of interactively mapping user utterances into executable logical forms (e.g., Sparql) in the context of the conversational history. Our key idea is to represent information about an utterance and its context via a subgraph which is created dynamically, i.e., the number of nodes varies per utterance. Rather than treating the subgraph as a sequence, we exploit its underlying structure and encode it with a graph neural network which further allows us to represent a large number of (unseen) nodes. Experimental results show that dynamic context modeling is superior to static approaches, delivering performance improvements across the board (i.e., for simple and complex questions). Our results further confirm that modeling the structure of context is better at processing discourse information, (i.e., at handling ellipsis and resolving coreference) and longer interactions",
    "checked": true,
    "id": "637a1b1dce41fe18babbf90af4d11546f3c0cdc7",
    "semantic_title": "conversational semantic parsing using dynamic context graphs",
    "citation_count": 2,
    "authors": [
      "Parag Jain",
      "Mirella Lapata"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.536": {
    "title": "Not all quantifiers are equal: Probing Transformer-based language models' understanding of generalised quantifiers",
    "volume": "main",
    "abstract": "How do different generalised quantifiers affect the behaviour of transformer-based language models (TLMs)? The recent popularity of TLMs and the central role generalised quantifiers have traditionally played in linguistics and logic bring this question into particular focus. The current research investigating this subject has not utilised a task defined purely in a logical sense, and thus, has not captured the underlying logical significance of generalised quantifiers. Consequently, they have not answered the aforementioned question faithfully or adequately. Therefore, we investigate how different generalised quantifiers affect TLMs by employing a textual entailment problem defined in a purely logical sense, namely, model-checking with natural language. Our approach permits the automatic construction of datasets with respect to which we can assess the ability of TLMs to learn the meanings of generalised quantifiers. Our investigation reveals that TLMs generally can comprehend the logical semantics of the most common generalised quantifiers, but that distinct quantifiers influence TLMs in varying ways",
    "checked": true,
    "id": "17560ad9f94218d0953dcec8c1c3b7e5bf4fb7bc",
    "semantic_title": "not all quantifiers are equal: probing transformer-based language models' understanding of generalised quantifiers",
    "citation_count": 0,
    "authors": [
      "Tharindu Madusanka",
      "Iqra Zahid",
      "Hao Li",
      "Ian Pratt-Hartmann",
      "Riza Batista-Navarro"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.537": {
    "title": "Structure-aware Knowledge Graph-to-text Generation with Planning Selection and Similarity Distinction",
    "volume": "main",
    "abstract": "The knowledge graph-to-text (KG-to-text) generation task aims to synthesize coherent and engaging sentences that accurately convey the complex information derived from an input knowledge graph. One of the primary challenges in this task is bridging the gap between the diverse structures of the KG and the target text, while preserving the details of the input KG. To address this, we propose a novel approach that efficiently integrates graph structure-aware modules with pre-trained language models. Unlike conventional techniques, which only consider direct connections between first-order neighbors, our method delves deeper by incorporating Relative Distance Encoding as a bias within the graph structure-aware module. This enables our model to better capture the intricate topology information present in the KG. To further elevate the fidelity of the generated text, Planning Selection and Similarity Distinction are introduced. Our approach filters the most relevant linearized sequences by employing a planning scorer, while simultaneously distinguishing similar input KGs through contrastive learning techniques. Experiments on two datasets demonstrate the superiority of our model",
    "checked": true,
    "id": "179f52203bb1e42f2084164bc4d94018149e5f4d",
    "semantic_title": "structure-aware knowledge graph-to-text generation with planning selection and similarity distinction",
    "citation_count": 0,
    "authors": [
      "Feng Zhao",
      "Hongzhi Zou",
      "Cheng Yan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.538": {
    "title": "SOUL: Towards Sentiment and Opinion Understanding of Language",
    "volume": "main",
    "abstract": "Sentiment analysis is a well-established natural language processing task, with sentiment polarity classification being one of its most popular and representative tasks. However, despite the success of pre-trained language models in this area, they often fall short of capturing the broader complexities of sentiment analysis. To address this issue, we propose a new task called Sentiment and Opinion Understanding of Language (SOUL). SOUL aims to evaluate sentiment understanding through two subtasks: Review Comprehension (RC) and Justification Generation (JG). RC seeks to validate statements that focus on subjective information based on a review text, while JG requires models to provide explanations for their sentiment predictions. To enable comprehensive evaluation, we annotate a new dataset comprising 15,028 statements from 3,638 reviews. Experimental results indicate that SOUL is a challenging task for both small and large language models, with a performance gap of up to 27% when compared to human performance. Furthermore, evaluations conducted with both human experts and GPT-4 highlight the limitations of the small language model in generating reasoning-based justifications. These findings underscore the challenging nature of the SOUL task for existing models, emphasizing the need for further advancements in sentiment analysis to address its complexities. The new dataset and code are available at https://github.com/DAMO-NLP-SG/SOUL",
    "checked": true,
    "id": "b62bf6fcf87a2c47ebbdaeb8c0f42883cd9cb9c4",
    "semantic_title": "soul: towards sentiment and opinion understanding of language",
    "citation_count": 0,
    "authors": [
      "Yue Deng",
      "Wenxuan Zhang",
      "Sinno Pan",
      "Lidong Bing"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.539": {
    "title": "Regulation and NLP (RegNLP): Taming Large Language Models",
    "volume": "main",
    "abstract": "The scientific innovation in Natural Language Processing (NLP) and more broadly in artificial intelligence (AI) is at its fastest pace to date. As large language models (LLMs) unleash a new era of automation, important debates emerge regarding the benefits and risks of their development, deployment and use. Currently, these debates have been dominated by often polarized narratives mainly led by the AI Safety and AI Ethics movements. This polarization, often amplified by social media, is swaying political agendas on AI regulation and governance and posing issues of regulatory capture. Capture occurs when the regulator advances the interests of the industry it is supposed to regulate, or of special interest groups rather than pursuing the general public interest. Meanwhile in NLP research, attention has been increasingly paid to the discussion of regulating risks and harms. This often happens without systematic methodologies or sufficient rooting in the disciplines that inspire an extended scope of NLP research, jeopardizing the scientific integrity of these endeavors. Regulation studies are a rich source of knowledge on how to systematically deal with risk and uncertainty, as well as with scientific evidence, to evaluate and compare regulatory options. This resource has largely remained untapped so far. In this paper, we argue how NLP research on these topics can benefit from proximity to regulatory studies and adjacent fields. We do so by discussing basic tenets of regulation, and risk and uncertainty, and by highlighting the shortcomings of current NLP discussions dealing with risk assessment. Finally, we advocate for the development of a new multidisciplinary research space on regulation and NLP (RegNLP), focused on connecting scientific knowledge to regulatory processes based on systematic methodologies",
    "checked": true,
    "id": "add28feeb5c1348f987d990c07793f8781c11419",
    "semantic_title": "regulation and nlp (regnlp): taming large language models",
    "citation_count": 0,
    "authors": [
      "Catalina Goanta",
      "Nikolaos Aletras",
      "Ilias Chalkidis",
      "Sofia Ranchordás",
      "Gerasimos Spanakis"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.540": {
    "title": "MedEval: A Multi-Level, Multi-Task, and Multi-Domain Medical Benchmark for Language Model Evaluation",
    "volume": "main",
    "abstract": "Curated datasets for healthcare are often limited due to the need of human annotations from experts. In this paper, we present MedEval, a multi-level, multi-task, and multi-domain medical benchmark to facilitate the development of language models for healthcare. MedEval is comprehensive and consists of data from several healthcare systems and spans 35 human body regions from 8 examination modalities. With 22,779 collected sentences and 21,228 reports, we provide expert annotations at multiple levels, offering a granular potential usage of the data and supporting a wide range of tasks. Moreover, we systematically evaluated 10 generic and domain-specific language models under zero-shot and finetuning settings, from domain-adapted baselines in healthcare to general-purposed state-of-the-art large language models (e.g., ChatGPT). Our evaluations reveal varying effectiveness of the two categories of language models across different tasks, from which we notice the importance of instruction tuning for few-shot usage of large language models. Our investigation paves the way toward benchmarking language models for healthcare and provides valuable insights into the strengths and limitations of adopting large language models in medical domains, informing their practical applications and future advancements",
    "checked": true,
    "id": "39abce3268f309d5655247de0c442a28219df390",
    "semantic_title": "medeval: a multi-level, multi-task, and multi-domain medical benchmark for language model evaluation",
    "citation_count": 0,
    "authors": [
      "Zexue He",
      "Yu Wang",
      "An Yan",
      "Yao Liu",
      "Eric Chang",
      "Amilcare Gentili",
      "Julian McAuley",
      "Chun-Nan Hsu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.541": {
    "title": "Seeing through the mess: evolutionary dynamics of lexical polysemy",
    "volume": "main",
    "abstract": "Evidently, words can have multiple senses. For example, the word mess refers to a place to have food or to a confusing situation. How exactly multiple senses emerge is less clear. In this work, we propose and analyze a mathematical model of the evolution of lexical meaning to investigate mechanisms leading to polysemy. This model features factors that have been discussed to impact the semantic processing and transmission of words: word frequency, non-conformism, and semantic discriminability. We formally derive conditions under which a sense of a word tends to diversify itself into multiple senses that coexist stably. The model predicts that diversification is promoted by low frequency, a strong bias for non-conformist usage, and high semantic discriminability. We statistically validate these predictions with historical language data covering semantic developments of a set of English words. Multiple alternative measures are used to operationalize each variable involved, and we confirm the predicted tendencies for twelve combinations of measures",
    "checked": true,
    "id": "40fd20c52a2d9fceb22b8c0de4ebb21d9cade07b",
    "semantic_title": "seeing through the mess: evolutionary dynamics of lexical polysemy",
    "citation_count": 0,
    "authors": [
      "Andreas Baumann",
      "Andreas Stephan",
      "Benjamin Roth"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.542": {
    "title": "Are Embedded Potatoes Still Vegetables? On the Limitations of WordNet Embeddings for Lexical Semantics",
    "volume": "main",
    "abstract": "Knowledge Base Embedding (KBE) models have been widely used to encode structured information from knowledge bases, including WordNet. However, the existing literature has predominantly focused on link prediction as the evaluation task, often neglecting exploration of the models' semantic capabilities. In this paper, we investigate the potential disconnect between the performance of KBE models of WordNet on link prediction and their ability to encode semantic information, highlighting the limitations of current evaluation protocols. Our findings reveal that some top-performing KBE models on the WN18RR benchmark exhibit subpar results on two semantic tasks and two downstream tasks. These results demonstrate the inadequacy of link prediction benchmarks for evaluating the semantic capabilities of KBE models, suggesting the need for a more targeted assessment approach",
    "checked": true,
    "id": "94eb9c8e7e522c37b989e635931473c375587fa4",
    "semantic_title": "are embedded potatoes still vegetables? on the limitations of wordnet embeddings for lexical semantics",
    "citation_count": 0,
    "authors": [
      "Xuyou Cheng",
      "Michael Schlichtkrull",
      "Guy Emerson"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.543": {
    "title": "Evaluation Metrics in the Era of GPT-4: Reliably Evaluating Large Language Models on Sequence to Sequence Tasks",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) evaluation is a patchy and inconsistent landscape, and it is becoming clear that the quality of automatic evaluation metrics is not keeping up with the pace of development of generative models. We aim to improve the understanding of current models' performance by providing a preliminary and hybrid evaluation on a range of open and closed-source generative LLMs on three NLP benchmarks: text summarisation, text simplification and grammatical error correction (GEC), using both automatic and human evaluation. We also explore the potential of the recently released GPT-4 to act as an evaluator. We find that ChatGPT consistently outperforms many other popular models according to human reviewers on the majority of metrics, while scoring much more poorly when using classic automatic evaluation metrics. We also find that human reviewers rate the gold reference as much worse than the best models' outputs, indicating the poor quality of many popular benchmarks. Finally, we find that GPT-4 is capable of ranking models' outputs in a way which aligns reasonably closely to human judgement despite task-specific variations, with a lower alignment in the GEC task",
    "checked": true,
    "id": "2a33e4c93002ab97a99577ac89837be4d448725e",
    "semantic_title": "evaluation metrics in the era of gpt-4: reliably evaluating large language models on sequence to sequence tasks",
    "citation_count": 0,
    "authors": [
      "Andrea Sottana",
      "Bin Liang",
      "Kai Zou",
      "Zheng Yuan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.544": {
    "title": "Event-Location Tracking in Narratives: A Case Study on Holocaust Testimonies",
    "volume": "main",
    "abstract": "This work focuses on the spatial dimension of narrative understanding and presents the task of event-location tracking in narrative texts. The task intends to extract the sequence of locations where the narrative is set through its progression. We present several architectures for the task that seeks to model the global structure of the sequence, with varying levels of context awareness. We compare these methods to several baselines, including the use of strong methods applied over narrow contexts. We also develop methods for the generation of location embeddings and show that learning to predict a sequence of continuous embeddings, rather than a string of locations, is advantageous in terms of performance. We focus on the test case of Holocaust survivor testimonies. We argue for the moral and historical importance of studying this dataset in computational means and that it provides a unique case of a large set of narratives with a relatively restricted set of location trajectories. Our results show that models that are aware of the larger context of the narrative can generate more accurate location chains. We further corroborate the effectiveness of our methods by showing similar trends from experiments on an additional domain",
    "checked": true,
    "id": "3677f85b529909da6e2f9bd84f66f6082fef5c4e",
    "semantic_title": "event-location tracking in narratives: a case study on holocaust testimonies",
    "citation_count": 0,
    "authors": [
      "Eitan Wagner",
      "Renana Keydar",
      "Omri Abend"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.545": {
    "title": "Dialogizer: Context-aware Conversational-QA Dataset Generation from Textual Sources",
    "volume": "main",
    "abstract": "To address the data scarcity issue in Conversational question answering (ConvQA), a dialog inpainting method, which utilizes documents to generate ConvQA datasets, has been proposed. However, the original dialog inpainting model is trained solely on the dialog reconstruction task, resulting in the generation of questions with low contextual relevance due to insufficient learning of question-answer alignment. To overcome this limitation, we propose a novel framework called Dialogizer, which has the capability to automatically generate ConvQA datasets with high contextual relevance from textual sources. The framework incorporates two training tasks: question-answer matching (QAM) and topic-aware dialog generation (TDG). Moreover, re-ranking is conducted during the inference phase based on the contextual relevance of the generated questions. Using our framework, we produce four ConvQA datasets by utilizing documents from multiple domains as the primary source. Through automatic evaluation using diverse metrics, as well as human evaluation, we validate that our proposed framework exhibits the ability to generate datasets of higher quality compared to the baseline dialog inpainting model",
    "checked": true,
    "id": "5693cf94321d55e4b4b04f69e8fde0e74f0177d8",
    "semantic_title": "dialogizer: context-aware conversational-qa dataset generation from textual sources",
    "citation_count": 0,
    "authors": [
      "Yerin Hwang",
      "Yongil Kim",
      "Hyunkyung Bae",
      "Hwanhee Lee",
      "Jeesoo Bang",
      "Kyomin Jung"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.546": {
    "title": "Learning to Predict Task Transferability via Soft Prompt",
    "volume": "main",
    "abstract": "Fine-tuning pretrained language models on helpful intermediate tasks often greatly improves the performance of target tasks. However, how to efficiently find the source tasks that can successfully transfer still remains under-explored. In this work, we propose to learn an affinity scoring function to predict transferability between tasks. Specifically, we conduct prompt tuning and regard soft prompts as task embeddings that summarize task-specific information. Then we randomly sample task pairs to train an affinity scoring function. The goal is to predict the transfer gain (i.e., affinity) between a task pair, by conditioning on their task embeddings. Once the scoring function is trained, given a novel target task, we use it to predict the most transferable source tasks, without a brute-force search for all possible source-target pairs. Experimental results across 50 tasks show that our method efficiently identifies beneficial tasks for transfer learning",
    "checked": true,
    "id": "16a948cdc7c3e48f92bfac861cfb176359637e18",
    "semantic_title": "learning to predict task transferability via soft prompt",
    "citation_count": 0,
    "authors": [
      "Lingyun Feng"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.547": {
    "title": "Chain-of-Questions Training with Latent Answers for Robust Multistep Question Answering",
    "volume": "main",
    "abstract": "We propose Chain-of-Questions, a framework that trains a model to robustly answer multistep questions by generating and answering sub-questions. We obtain supervision for sub-questions from human-annotated question decomposition meaning representation (QDMR), but QDMR does not include annotated answers to sub-questions. To overcome this technical challenge, we treat sub-answers as latent variables and infer them with a novel dynamic mixture of Hard-EM and MAPO. Chain-of-Questions is effective and robust, greatly outperforming strong neuro-symbolic methods by 9.0 F1 on a DROP contrast set and GPT-3.5 by 24.3 F1 on a HotpotQA adversarial set",
    "checked": true,
    "id": "4ed2fbae7cc7836274e2ee98e336311530509513",
    "semantic_title": "chain-of-questions training with latent answers for robust multistep question answering",
    "citation_count": 2,
    "authors": [
      "Wang Zhu",
      "Jesse Thomason",
      "Robin Jia"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.548": {
    "title": "Mirror: A Universal Framework for Various Information Extraction Tasks",
    "volume": "main",
    "abstract": "Sharing knowledge between information extraction tasks has always been a challenge due to the diverse data formats and task variations. Meanwhile, this divergence leads to information waste and increases difficulties in building complex applications in real scenarios. Recent studies often formulate IE tasks as a triplet extraction problem. However, such a paradigm does not support multi-span and n-ary extraction, leading to weak versatility. To this end, we reorganize IE problems into unified multi-slot tuples and propose a universal framework for various IE tasks, namely Mirror. Specifically, we recast existing IE tasks as a multi-span cyclic graph extraction problem and devise a non-autoregressive graph decoding algorithm to extract all spans in a single step. It is worth noting that this graph structure is incredibly versatile, and it supports not only complex IE tasks, but also machine reading comprehension and classification tasks. We manually construct a corpus containing 57 datasets for model pretraining, and conduct experiments on 30 datasets across 8 downstream tasks. The experimental results demonstrate that our model has decent compatibility and outperforms or reaches competitive performance with SOTA systems under few-shot and zero-shot settings. The code, model weights, and pretraining corpus are available at https://github.com/Spico197/Mirror",
    "checked": true,
    "id": "99b0d1cb964c3942c6f6ce30d40d7d8eb4a8e66f",
    "semantic_title": "mirror: a universal framework for various information extraction tasks",
    "citation_count": 0,
    "authors": [
      "Tong Zhu",
      "Junfei Ren",
      "Zijian Yu",
      "Mengsong Wu",
      "Guoliang Zhang",
      "Xiaoye Qu",
      "Wenliang Chen",
      "Zhefeng Wang",
      "Baoxing Huai",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.549": {
    "title": "Mistakes Help Us Grow\": Facilitating and Evaluating Growth Mindset Supportive Language in Classrooms",
    "volume": "main",
    "abstract": "Teachers' growth mindset supportive language (GMSL)—rhetoric emphasizing that one's skills can be improved over time—has been shown to significantly reduce disparities in academic achievement and enhance students' learning outcomes. Although teachers espouse growth mindset principles, most find it difficult to adopt GMSL in their practice due the lack of effective coaching in this area. We explore whether large language models (LLMs) can provide automated, personalized coaching to support teachers' use of GMSL. We establish an effective coaching tool to reframe unsupportive utterances to GMSL by developing (i) a parallel dataset containing GMSL-trained teacher reframings of unsupportive statements with an accompanying annotation guide, (ii) a GMSL prompt framework to revise teachers' unsupportive language, and (iii) an evaluation framework grounded in psychological theory for evaluating GMSL with the help of students and teachers. We conduct a large-scale evaluation involving 174 teachers and 1,006 students, finding that both teachers and students perceive GMSL-trained teacher and model reframings as more effective in fostering a growth mindset and promoting challenge-seeking behavior, among other benefits. We also find that model-generated reframings outperform those from the GMSL-trained teachers. These results show promise for harnessing LLMs to provide automated GMSL feedback for teachers and, more broadly, LLMs' potentiality for supporting students' learning in the classroom. Our findings also demonstrate the benefit of large-scale human evaluations when applying LLMs in educational domains",
    "checked": true,
    "id": "d2b9625149de2527e412ecfa07c6e5e6f9b68a0b",
    "semantic_title": "mistakes help us grow\": facilitating and evaluating growth mindset supportive language in classrooms",
    "citation_count": 0,
    "authors": [
      "Kunal Handa",
      "Margarett Clapper",
      "Jessica Boyle",
      "Rose Wang",
      "Diyi Yang",
      "David Yeager",
      "Dorottya Demszky"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.550": {
    "title": "Unnatural Error Correction: GPT-4 Can Almost Perfectly Handle Unnatural Scrambled Text",
    "volume": "main",
    "abstract": "While Large Language Models (LLMs) have achieved remarkable performance in many tasks, much about their inner workings remains unclear. In this study, we present novel experimental insights into the resilience of LLMs, particularly GPT-4, when subjected to extensive character-level permutations. To investigate this, we first propose the Scrambled Bench, a suite designed to measure the capacity of LLMs to handle scrambled input, in terms of both recovering scrambled sentences and answering questions given scrambled context. The experimental results indicate that multiple advanced LLMs demonstrate the capability akin to typoglycemia, a phenomenon where humans can understand the meaning of words even when the letters within those words are scrambled, as long as the first and last letters remain in place. More surprisingly, we found that only GPT-4 nearly flawlessly processes inputs with unnatural errors, a task that poses significant challenges for other LLMs and often even for humans. Specifically, GPT-4 can almost perfectly reconstruct the original sentences from scrambled ones, decreasing the edit distance by 95%, even when all letters within each word are entirely scrambled. It is counter-intuitive that LLMs can exhibit such resilience despite severe disruption to input tokenization caused by scrambled text",
    "checked": true,
    "id": "7354a57261d27a281e56dc428b6ec146b9992afd",
    "semantic_title": "unnatural error correction: gpt-4 can almost perfectly handle unnatural scrambled text",
    "citation_count": 0,
    "authors": [
      "Qi Cao",
      "Takeshi Kojima",
      "Yutaka Matsuo",
      "Yusuke Iwasawa"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.551": {
    "title": "Detecting and Mitigating Hallucinations in Multilingual Summarisation",
    "volume": "main",
    "abstract": "Hallucinations pose a significant challenge to the reliability of neural models for abstractive summarisation. While automatically generated summaries may be fluent, they often lack faithfulness to the original document. This issue becomes even more pronounced in low-resource languages, where summarisation requires cross-lingual transfer. With the existing faithful metrics focusing on English, even measuring the extent of this phenomenon in cross-lingual settings is hard. To address this, we first develop a novel metric, mFACT, evaluating the faithfulness of non-English summaries, leveraging translation-based transfer from multiple English faithfulness metrics. Through extensive experiments in multiple languages, we demonstrate that mFACT is best suited to detect hallucinations compared to alternative metrics. With mFACT, we assess a broad range of multilingual large language models, and find that they all tend to hallucinate often in languages different from English. We then propose a simple but effective method to reduce hallucinations in cross-lingual transfer, which weighs the loss of each training example by its faithfulness score. This method drastically increases both performance and faithfulness according to both automatic and human evaluation when compared to strong baselines for cross-lingual transfer such as MAD-X. Our code and dataset are available at https://github.com/yfqiu-nlp/mfact-summ",
    "checked": true,
    "id": "0d5124d1fb7f21aa2efc0ae234feab97e8a23208",
    "semantic_title": "detecting and mitigating hallucinations in multilingual summarisation",
    "citation_count": 4,
    "authors": [
      "Yifu Qiu",
      "Yftah Ziser",
      "Anna Korhonen",
      "Edoardo Ponti",
      "Shay Cohen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.552": {
    "title": "Exploring Linguistic Probes for Morphological Inflection",
    "volume": "main",
    "abstract": "Modern work on the cross-linguistic computational modeling of morphological inflection has typically employed language-independent data splitting algorithms. In this paper, we supplement that approach with language-specific probes designed to test aspects of morphological generalization. Testing these probes on three morphologically distinct languages, English, Spanish, and Swahili, we find evidence that three leading morphological inflection systems employ distinct generalization strategies over conjugational classes and feature sets on both orthographic and phonologically transcribed inputs",
    "checked": true,
    "id": "4b3353b89a6187fff11389308f409d1b44d2ee7f",
    "semantic_title": "exploring linguistic probes for morphological inflection",
    "citation_count": 0,
    "authors": [
      "Jordan Kodner",
      "Salam Khalifa",
      "Sarah Ruth Brogden Payne"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.553": {
    "title": "AMR Parsing with Causal Hierarchical Attention and Pointers",
    "volume": "main",
    "abstract": "Translation-based AMR parsers have recently gained popularity due to their simplicity and effectiveness. They predict linearized graphs as free texts, avoiding explicit structure modeling. However, this simplicity neglects structural locality in AMR graphs and introduces unnecessary tokens to represent coreferences. In this paper, we introduce new target forms of AMR parsing and a novel model, CHAP, which is equipped with causal hierarchical attention and the pointer mechanism, enabling the integration of structures into the Transformer decoder. We empirically explore various alternative modeling options. Experiments show that our model outperforms baseline models on four out of five benchmarks in the setting of no additional data",
    "checked": true,
    "id": "a0ff78ef60f90b1fe9682d438a91eee1c9d82d05",
    "semantic_title": "amr parsing with causal hierarchical attention and pointers",
    "citation_count": 0,
    "authors": [
      "Chao Lou",
      "Kewei Tu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.554": {
    "title": "FLatS: Principled Out-of-Distribution Detection with Feature-Based Likelihood Ratio Score",
    "volume": "main",
    "abstract": "Detecting out-of-distribution (OOD) instances is crucial for NLP models in practical applications. Although numerous OOD detection methods exist, most of them are empirical. Backed by theoretical analysis, this paper advocates for the measurement of the \"OOD-ness\" of a test case x through the likelihood ratio between out-distribution \\mathcal Pout and in-distribution \\mathcal Pin. We argue that the state-of-the-art (SOTA) feature-based OOD detection methods, such as Maha and KNN, are suboptimal since they only estimate in-distribution density pin(x). To address this issue, we propose FLATS, a principled solution for OOD detection based on likelihood ratio. Moreover, we demonstrate that FLATS can serve as a general framework capable of enhancing other OOD detection methods by incorporating out-distribution density pout(x) estimation. Experiments show that FLATS establishes a new SOTA on popular benchmarks",
    "checked": true,
    "id": "6b5a1c06a91c07bf8bcac19368f44f5aa3fb07ab",
    "semantic_title": "flats: principled out-of-distribution detection with feature-based likelihood ratio score",
    "citation_count": 0,
    "authors": [
      "Haowei Lin",
      "Yuntian Gu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.555": {
    "title": "Self-Evolution Learning for Mixup: Enhance Data Augmentation on Few-Shot Text Classification Tasks",
    "volume": "main",
    "abstract": "Text classification tasks often encounter few-shot scenarios with limited labeled data, and addressing data scarcity is crucial. Data augmentation with mixup merges sample pairs to generate new pseudos, which can relieve the data deficiency issue in text classification. However, the quality of pseudo-samples generated by mixup exhibits significant variations. Most of the mixup methods fail to consider the varying degree of learning difficulty in different stages of training. And mixup generates new samples with one-hot labels, which encourages the model to produce a high prediction score for the correct class that is much larger than other classes, resulting in the model's over-confidence. In this paper, we propose a self-evolution learning (SE) based mixup approach for data augmentation in text classification, which can generate more adaptive and model-friendly pseudo samples for the model training. SE caters to the growth of the model learning ability and adapts to the ability when generating training samples. To alleviate the model over-confidence, we introduce an instance-specific label smoothing regularization approach, which linearly interpolates the model's output and one-hot labels of the original samples to generate new soft labels for label mixing up. Through experimental analysis, experiments show that our SE brings consistent and significant improvements upon different mixup methods. In-depth analyses demonstrate that SE enhances the model's generalization ability",
    "checked": true,
    "id": "2b786901bb5538a41670b0d0e7e1bbaf5cf6c0c0",
    "semantic_title": "self-evolution learning for mixup: enhance data augmentation on few-shot text classification tasks",
    "citation_count": 0,
    "authors": [
      "Haoqi Zheng",
      "Qihuang Zhong",
      "Liang Ding",
      "Zhiliang Tian",
      "Xin Niu",
      "Changjian Wang",
      "Dongsheng Li",
      "Dacheng Tao"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.556": {
    "title": "IC3: Image Captioning by Committee Consensus",
    "volume": "main",
    "abstract": "If you ask a human to describe an image, they might do so in a thousand different ways. Traditionally, image captioning models are trained to generate a single \"best' (most like a reference) image caption. Unfortunately, doing so encourages captions that are \"informationally impoverished,' and focus on only a subset of the possible details, while ignoring other potentially useful information in the scene. In this work, we introduce a simple, yet novel, method: \"Image Captioning by Committee Consensus' (IC3), designed to generate a single caption that captures high-level details from several annotator viewpoints. Humans rate captions produced by IC3 at least as helpful as baseline SOTA models more than two thirds of the time, and IC3 can improve the performance of SOTA automated recall systems by up to 84%, outperforming single human-generated reference captions, and indicating significant improvements over SOTA approaches for visual description. Code is available at [https://davidmchan.github.io/caption-by-committee/](https://davidmchan.github.io/caption-by-committee/)",
    "checked": true,
    "id": "0d53981f3bc7329016ca28c2793cd58b44d38ada",
    "semantic_title": "ic3: image captioning by committee consensus",
    "citation_count": 3,
    "authors": [
      "David Chan",
      "Austin Myers",
      "Sudheendra Vijayanarasimhan",
      "David Ross",
      "John Canny"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.557": {
    "title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models",
    "volume": "main",
    "abstract": "Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose \"SelfCheckGPT\", a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if an LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GPT-3 to generate passages about individuals from the WikiBio dataset, and manually annotate the factuality of the generated passages. We demonstrate that SelfCheckGPT can: i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. We compare our approach to several baselines and show that our approach has considerably higher AUC-PR scores in sentence-level hallucination detection and higher correlation scores in passage-level factuality assessment compared to grey-box methods",
    "checked": true,
    "id": "7c1707db9aafd209aa93db3251e7ebd593d55876",
    "semantic_title": "selfcheckgpt: zero-resource black-box hallucination detection for generative large language models",
    "citation_count": 107,
    "authors": [
      "Potsawee Manakul",
      "Adian Liusie",
      "Mark Gales"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.558": {
    "title": "Fair Without Leveling Down: A New Intersectional Fairness Definition",
    "volume": "main",
    "abstract": "In this work, we consider the problem of intersectional group fairness in the classification setting, where the objective is to learn discrimination-free models in the presence of several intersecting sensitive groups. First, we illustrate various shortcomings of existing fairness measures commonly used to capture intersectional fairness. Then, we propose a new definition called the 𝛼-Intersectional Fairness, which combines the absolute and the relative performance across sensitive groups and can be seen as a generalization of the notion of differential fairness. We highlight several desirable properties of the proposed definition and analyze its relation to other fairness measures. Finally, we benchmark multiple popular in-processing fair machine learning approaches using our new fairness definition and show that they do not achieve any improvement over a simple baseline. Our results reveal that the increase in fairness measured by previous definitions hides a \"leveling down\" effect, i.e., degrading the best performance over groups rather than improving the worst one",
    "checked": true,
    "id": "ad81697cd665a0f3f56e33a953943d948f054a1d",
    "semantic_title": "fair without leveling down: a new intersectional fairness definition",
    "citation_count": 0,
    "authors": [
      "Gaurav Maheshwari",
      "Aurélien Bellet",
      "Pascal Denis",
      "Mikaela Keller"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.559": {
    "title": "Revisiting Instruction Fine-tuned Model Evaluation to Guide Industrial Applications",
    "volume": "main",
    "abstract": "Instruction Fine-Tuning (IFT) is a powerful paradigm that strengthens the zero-shot capabilities of Large Language Models (LLMs), but in doing so induces new evaluation metric requirements. We show LLM-based metrics to be well adapted to these requirements, and leverage them to conduct an investigation of task-specialization strategies, quantifying the trade-offs that emerge in practical industrial settings. Our findings offer practitioners actionable insights for real-world IFT model deployment",
    "checked": true,
    "id": "5ee2d80a3b0e7b0a4c7d9418d7c3f2fee9b6011a",
    "semantic_title": "revisiting instruction fine-tuned model evaluation to guide industrial applications",
    "citation_count": 0,
    "authors": [
      "Manuel Faysse",
      "Gautier Viaud",
      "Céline Hudelot",
      "Pierre Colombo"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.560": {
    "title": "CLAD-ST: Contrastive Learning with Adversarial Data for Robust Speech Translation",
    "volume": "main",
    "abstract": "The cascaded approach continues to be the most popular choice for speech translation (ST). This approach consists of an automatic speech recognition (ASR) model and a machine translation (MT) model that are used in a pipeline to translate speech in one language to text in another language. MT models are often trained on the well-formed text and therefore lack robustness while translating noisy ASR outputs in the cascaded approach, degrading the overall translation quality significantly. We address this robustness problem in downstream MT models by forcing the MT encoder to bring the representations of a noisy input closer to its clean version in the semantic space. This is achieved by introducing a contrastive learning method that leverages adversarial examples in the form of ASR outputs paired with their corresponding human transcripts to optimize the network parameters. In addition, a curriculum learning strategy is then used to stabilize the training by alternating the standard MT log-likelihood loss and the contrastive losses. Our approach achieves significant gains of up to 3 BLEU scores in English-German and English-French speech translation without hurting the translation quality on clean text",
    "checked": true,
    "id": "64842d8dc2d6ac239f319b35fbaade98c58275df",
    "semantic_title": "clad-st: contrastive learning with adversarial data for robust speech translation",
    "citation_count": 0,
    "authors": [
      "Sathish Indurthi",
      "Shamil Chollampatt",
      "Ravi Agrawal",
      "Marco Turchi"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.561": {
    "title": "M2DF: Multi-grained Multi-curriculum Denoising Framework for Multimodal Aspect-based Sentiment Analysis",
    "volume": "main",
    "abstract": "Multimodal Aspect-based Sentiment Analysis (MABSA) is a fine-grained Sentiment Analysis task, which has attracted growing research interests recently. Existing work mainly utilizes image information to improve the performance of MABSA task. However, most of the studies overestimate the importance of images since there are many noise images unrelated to the text in the dataset, which will have a negative impact on model learning. Although some work attempts to filter low-quality noise images by setting thresholds, relying on thresholds will inevitably filter out a lot of useful image information. Therefore, in this work, we focus on whether the negative impact of noisy images can be reduced without modifying the data. To achieve this goal, we borrow the idea of Curriculum Learning and propose a Multi-grained Multi-curriculum Denoising Framework (M2DF), which can achieve denoising by adjusting the order of training data. Extensive experimental results show that our framework consistently outperforms state-of-the-art work on three sub-tasks of MABSA",
    "checked": true,
    "id": "a747b6cb2300119a6aa63a69baabb7f6343ad520",
    "semantic_title": "m2df: multi-grained multi-curriculum denoising framework for multimodal aspect-based sentiment analysis",
    "citation_count": 0,
    "authors": [
      "Fei Zhao",
      "Chunhui Li",
      "Zhen Wu",
      "Yawen Ouyang",
      "Jianbing Zhang",
      "Xinyu Dai"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.562": {
    "title": "Detection of Multiple Mental Disorders from Social Media with Two-Stream Psychiatric Experts",
    "volume": "main",
    "abstract": "Existing Mental Disease Detection (MDD) research largely studies the detection of a single disorder, overlooking the fact that mental diseases might occur in tandem. Many approaches are not backed by domain knowledge (e.g., psychiatric symptoms) and thus fail to produce interpretable results. To tackle these issues, we propose an MDD framework that is capable of learning the shared clues of all diseases, while also capturing the specificity of each single disease. The two-stream architecture which simultaneously processes text and symptom features can combine the strength of both modalities and offer knowledge-based explainability. Experiments on the detection of 7 diseases show that our model can boost detection performance by more than 10%, especially in relatively rare classes",
    "checked": true,
    "id": "39c571cbfd53a9fe5fecdebaa6c2835c67161ff0",
    "semantic_title": "detection of multiple mental disorders from social media with two-stream psychiatric experts",
    "citation_count": 0,
    "authors": [
      "Siyuan Chen",
      "Zhiling Zhang",
      "Mengyue Wu",
      "Kenny Zhu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.563": {
    "title": "Understanding the Role of Input Token Characters in Language Models: How Does Information Loss Affect Performance?",
    "volume": "main",
    "abstract": "Understanding how and what pre-trained language models (PLMs) learn about language is an open challenge in natural language processing. Previous work has focused on identifying whether they capture semantic and syntactic information, and how the data or the pre-training objective affects their performance. However, to the best of our knowledge, no previous work has specifically examined how information loss in input token characters affects the performance of PLMs. In this study, we address this gap by pre-training language models using small subsets of characters from individual tokens. Surprisingly, we find that pre-training even under extreme settings, i.e. using only one character of each token, the performance retention in standard NLU benchmarks and probing tasks compared to full-token models is high. For instance, a model pre-trained only on single first characters from tokens achieves performance retention of approximately 90% and 77% of the full-token model in SuperGLUE and GLUE tasks, respectively",
    "checked": true,
    "id": "7a1db1346d95b2ec6c9706e71d2ff92532bfe78d",
    "semantic_title": "understanding the role of input token characters in language models: how does information loss affect performance?",
    "citation_count": 0,
    "authors": [
      "Ahmed Alajrami",
      "Katerina Margatina",
      "Nikolaos Aletras"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.564": {
    "title": "Improved Unsupervised Chinese Word Segmentation Using Pre-trained Knowledge and Pseudo-labeling Transfer",
    "volume": "main",
    "abstract": "Unsupervised Chinese word segmentation (UCWS) has made progress by incorporating linguistic knowledge from pre-trained language models using parameter-free probing techniques. However, such approaches suffer from increased training time due to the need for multiple inferences using a pre-trained language model to perform word segmentation. This work introduces a novel way to enhance UCWS performance while maintaining training efficiency. Our proposed method integrates the segmentation signal from the unsupervised segmental language model to the pre-trained BERT classifier under a pseudo-labeling framework. Experimental results demonstrate that our approach achieves state-of-the-art performance on the eight UCWS tasks while considerably reducing the training time compared to previous approaches",
    "checked": true,
    "id": "a67e752b4257fddce59d60d5f52212b3aeab3f8f",
    "semantic_title": "improved unsupervised chinese word segmentation using pre-trained knowledge and pseudo-labeling transfer",
    "citation_count": 0,
    "authors": [
      "Hsiu-Wen Li",
      "Ying-Jia Lin",
      "Yi-Ting Li",
      "Chun Lin",
      "Hung-Yu Kao"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.565": {
    "title": "EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs",
    "volume": "main",
    "abstract": "Large language models (LLMs) have proven to be very superior to conventional methods in various tasks. However, their expensive computations and high memory requirements are prohibitive for deployment. Model quantization is an effective method for reducing this overhead. The problem is that in most previous works, the quantized model was calibrated using few samples from the training data, which might affect the generalization of the quantized LLMs to unknown cases and tasks. Hence in this work, we explore an important question: Can we design a data-independent quantization method for LLMs to guarantee its generalization performance? In this work, we propose EasyQuant, a training-free and data-independent weight-only quantization algorithm for LLMs. Our observation indicates that two factors: outliers in the weight and quantization ranges, are essential for reducing the quantization error. Therefore, in EasyQuant, we leave the outliers (less than 1%) unchanged and optimize the quantization range to reduce the reconstruction error. With these methods, we surprisingly find that EasyQuant achieves comparable performance to the original model. Since EasyQuant does not depend on any training data, the generalization performance of quantized LLMs is safely guaranteed. Moreover, EasyQuant can be implemented in parallel so that the quantized model could be attained in a few minutes even for LLMs over 100B. To our best knowledge, we are the first work that achieves almost lossless quantization performance for LLMs under a data-independent setting and our algorithm runs over 10 times faster than the data-dependent methods",
    "checked": true,
    "id": "d488d50008daac0fa6a51eecec811f63fdf43597",
    "semantic_title": "easyquant: an efficient data-free quantization algorithm for llms",
    "citation_count": 0,
    "authors": [
      "Hanlin Tang",
      "Yifu Sun",
      "Decheng Wu",
      "Kai Liu",
      "Jianchen Zhu",
      "Zhanhui Kang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.566": {
    "title": "Polar Ducks and Where to Find Them: Enhancing Entity Linking with Duck Typing and Polar Box Embeddings",
    "volume": "main",
    "abstract": "Entity linking methods based on dense retrieval are widely adopted in large-scale applications for their efficiency, but they can fall short of generative models, as they are sensitive to the structure of the embedding space. To address this issue, this paper introduces DUCK, an approach to infusing structural information in the space of entity representations, using prior knowledge of entity types. Inspired by duck typing in programming languages, we define the type of an entity based on its relations with other entities in a knowledge graph. Then, porting the concept of box embeddings to spherical polar coordinates, we represent relations as boxes on the hypersphere. We optimize the model to place entities inside the boxes corresponding to their relations, thereby clustering together entities of similar type. Our experiments show that our method sets new state-of-the-art results on standard entity-disambiguation benchmarks. It improves the performance of the model by up to 7.9 F1 points, outperforms other type-aware approaches, and matches the results of generative models with 18 times more parameters",
    "checked": true,
    "id": "fb69296a291d4be74f129a495953c519e5079f0b",
    "semantic_title": "polar ducks and where to find them: enhancing entity linking with duck typing and polar box embeddings",
    "citation_count": 0,
    "authors": [
      "Mattia Atzeni",
      "Mikhail Plekhanov",
      "Frederic Dreyer",
      "Nora Kassner",
      "Simone Merello",
      "Louis Martin",
      "Nicola Cancedda"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.567": {
    "title": "APrompt: Attention Prompt Tuning for Efficient Adaptation of Pre-trained Language Models",
    "volume": "main",
    "abstract": "With the continuous growth of large language models, the process of fine-tuning these models for new tasks has become increasingly parameter-intensive. Prompt tuning, a method that involves tuning a small set of soft prompts, has emerged as an effective and efficient approach for adapting large pre-trained language models. However, most existing prompt tuning approaches only introduce prompts at the input layer, limiting their performance and leaving large rooms for improvement. In this work, we propose a novel Attention Prompt tuning method, namely APrompt, for efficient adaptation of pre-trained language models. We first demonstrate that existing prompt tuning can be considered as a special case of attention prompt tuning. We then formally introduce APrompt, which incorporates query, key, and value prompts into the attention layer to guide the attention computation during fine-tuning. Experimental results on the SuperGLUE benchmark consistently demonstrate that our proposed approach outperforms state-of-the-art baselines and full fine-tuning method with pre-trained models at different scales. In addition, a comprehensive set of ablation studies validate the effectiveness of the prompt design, as well as the efficiency of our approach",
    "checked": true,
    "id": "a4a26d7c6bc022f604a246c6ce6cc1cfbe618441",
    "semantic_title": "aprompt: attention prompt tuning for efficient adaptation of pre-trained language models",
    "citation_count": 0,
    "authors": [
      "Qifan Wang",
      "Yuning Mao",
      "Jingang Wang",
      "Hanchao Yu",
      "Shaoliang Nie",
      "Sinong Wang",
      "Fuli Feng",
      "Lifu Huang",
      "Xiaojun Quan",
      "Zenglin Xu",
      "Dongfang Liu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.568": {
    "title": "What's \"up\" with vision-language models? Investigating their struggle with spatial reasoning",
    "volume": "main",
    "abstract": "Recent vision-language (VL) models are powerful, but can they reliably distinguish \"right\" from \"left\"? We curate three new corpora to quantify model comprehension of such basic spatial relations. These tests isolate spatial reasoning more precisely than existing datasets like VQAv2, e.g., our What'sUp benchmark contains sets of photographs varying only the spatial relations of objects, keeping their identity fixed (see Figure 1: models must comprehend not only the usual case of a dog under a table, but also, the same dog on top of the same table). We evaluate 18 VL models, finding that all perform poorly, e.g., BLIP finetuned on VQAv2, which nears human parity on VQAv2, achieves 56% accuracy on our benchmarks vs. humans at 99%. We conclude by studying causes of this surprising behavior, finding: 1) that popular vision-language pretraining corpora like LAION-2B contain little reliable data for learning spatial relationships; and 2) that basic modeling interventions like up-weighting preposition-containing instances or fine-tuning on our corpora are not sufficient to address the challenges our benchmarks pose. We are hopeful that these corpora will facilitate further research, and we release our data and code at https://github.com/amitakamath/whatsup_vlms",
    "checked": true,
    "id": "1fa674b4e1c82cf648868be2568637fdfb5fb1b3",
    "semantic_title": "what's \"up\" with vision-language models? investigating their struggle with spatial reasoning",
    "citation_count": 3,
    "authors": [
      "Amita Kamath",
      "Jack Hessel",
      "Kai-Wei Chang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.569": {
    "title": "IBADR: an Iterative Bias-Aware Dataset Refinement Framework for Debiasing NLU models",
    "volume": "main",
    "abstract": "As commonly-used methods for debiasing natural language understanding (NLU) models, dataset refinement approaches heavily rely on manual data analysis, and thus maybe unable to cover all the potential biased features. In this paper, we propose IBADR, an Iterative Bias-Aware Dataset Refinement framework, which debiases NLU models without predefining biased features. We maintain an iteratively expanded sample pool. Specifically, at each iteration, we first train a shallow model to quantify the bias degree of samples in the pool. Then, we pair each sample with a bias indicator representing its bias degree, and use these extended samples to train a sample generator. In this way, this generator can effectively learn the correspondence relationship between bias indicators and samples. Furthermore, we employ the generator to produce pseudo samples with fewer biased features by feeding specific bias indicators. Finally, we incorporate the generated pseudo samples into the pool. Experimental results and in-depth analyses on two NLU tasks show that IBADR not only significantly outperforms existing dataset refinement approaches, achieving SOTA, but also is compatible with model-centric methods",
    "checked": true,
    "id": "2a6ac0c014ada425a9e0470d6f3a2391782d59b3",
    "semantic_title": "ibadr: an iterative bias-aware dataset refinement framework for debiasing nlu models",
    "citation_count": 0,
    "authors": [
      "Xiaoyue Wang",
      "Xin Liu",
      "Lijie Wang",
      "Yaoxiang Wang",
      "Jinsong Su",
      "Hua Wu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.570": {
    "title": "Learning Preference Model for LLMs via Automatic Preference Data Generation",
    "volume": "main",
    "abstract": "Despite the advanced capacities of the state-of-the-art large language models (LLMs), they suffer from issues of hallucination, stereotype, etc. Preference models play an important role in LLM alignment, yet training preference models predominantly rely on human-annotated data. This reliance limits their versatility and scalability. In this paper, we propose learning the preference model for LLMs via automatic preference data generation (AutoPM). Our approach involves both In-Breadth Data Generation, which elicits pairwise preference data from LLMs following the helpful-honest-harmless (HHH) criteria, and In-Depth Data Generation, which enriches the dataset with responses spanning a wide quality range. With HHH-guided preference data, our approach simultaneously enables the LLMs to learn human preferences and align with human values. Quantitative assessments on five benchmark datasets demonstrate the reliability and potential of AutoPM, pointing out a more general and scalable way to improve LLM performance",
    "checked": true,
    "id": "df9b47b2d1fdf6b66dac31309fa1552907414ffe",
    "semantic_title": "learning preference model for llms via automatic preference data generation",
    "citation_count": 0,
    "authors": [
      "Shijia Huang",
      "Jianqiao Zhao",
      "Yanyang Li",
      "Liwei Wang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.571": {
    "title": "Multilingual k-Nearest-Neighbor Machine Translation",
    "volume": "main",
    "abstract": "k-nearest-neighbor machine translation has demonstrated remarkable improvements in machine translation quality by creating a datastore of cached examples. However, these improvements have been limited to high-resource language pairs, with large datastores, and remain a challenge for low-resource languages. In this paper, we address this issue by combining representations from multiple languages into a single datastore. Our results consistently demonstrate substantial improvements not only in low-resource translation quality (up to +3.6 BLEU), but also for high-resource translation quality (up to +0.5 BLEU). Our experiments show that it is possible to create multilingual datastores that are a quarter of the size, achieving a 5.3x speed improvement, by using linguistic similarities for datastore creation",
    "checked": true,
    "id": "abce5ef05320af4b03ca4b4ca8c7be560fd0de78",
    "semantic_title": "multilingual k-nearest-neighbor machine translation",
    "citation_count": 0,
    "authors": [
      "David Stap",
      "Christof Monz"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.572": {
    "title": "Understanding Computational Models of Semantic Change: New Insights from the Speech Community",
    "volume": "main",
    "abstract": "We investigate the descriptive relevance of widely used semantic change models in linguistic descriptions of present-day speech communities. We focus on the sociolinguistic issue of contact-induced semantic shifts in Quebec English, and analyze 40 target words using type-level and token-level word embeddings, empirical linguistic properties, and – crucially – acceptability ratings and qualitative remarks by 15 speakers from Montreal. Our results confirm the overall relevance of the computational approaches, but also highlight practical issues and the complementary nature of different semantic change estimates. To our knowledge, this is the first study to substantively engage with the speech community being described using semantic change models",
    "checked": true,
    "id": "fd0caca019180a3394ee2a8927be4bb6cb5c5614",
    "semantic_title": "understanding computational models of semantic change: new insights from the speech community",
    "citation_count": 0,
    "authors": [
      "Filip Miletić",
      "Anne Przewozny-Desriaux",
      "Ludovic Tanguy"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.573": {
    "title": "Causal Reasoning through Two Cognition Layers for Improving Generalization in Visual Question Answering",
    "volume": "main",
    "abstract": "Generalization in Visual Question Answering (VQA) requires models to answer questions about images with contexts beyond the training distribution. Existing attempts primarily refine unimodal aspects, overlooking enhancements in multimodal aspects. Besides, diverse interpretations of the input lead to various modes of answer generation, highlighting the role of causal reasoning between interpreting and answering steps in VQA. Through this lens, we propose Cognitive pathways VQA (CopVQA) improving the multimodal predictions by emphasizing causal reasoning factors. CopVQA first operates a pool of pathways that capture diverse causal reasoning flows through interpreting and answering stages. Mirroring human cognition, we decompose the responsibility of each stage into distinct experts and a cognition-enabled component (CC). The two CCs strategically execute one expert for each stage at a time. Finally, we prioritize answer predictions governed by pathways involving both CCs while disregarding answers produced by either CC, thereby emphasizing causal reasoning and supporting generalization. Our experiments on real-life and medical data consistently verify that CopVQA improves VQA performance and generalization across baselines and domains. Notably, CopVQA achieves a new state-of-the-art (SOTA) on the PathVQA dataset and comparable accuracy to the current SOTA on VQA-CPv2, VQAv2, and VQA- RAD, with one-fourth of the model size",
    "checked": true,
    "id": "c202fc756bbbe0b47b3021d806ee37b924a7db39",
    "semantic_title": "causal reasoning through two cognition layers for improving generalization in visual question answering",
    "citation_count": 0,
    "authors": [
      "Trang Nguyen",
      "Naoaki Okazaki"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.574": {
    "title": "StructGPT: A General Framework for Large Language Model to Reason over Structured Data",
    "volume": "main",
    "abstract": "In this paper, we aim to improve the reasoning ability of large language models (LLMs) over structured data in a unified way. Inspired by the studies on tool augmentation for LLMs, we develop an Iterative Reading-then-Reasoning (IRR) framework to solve question answering tasks based on structured data, called StructGPT. In this framework, we construct the specialized interfaces to collect relevant evidence from structured data (i.e., reading), and let LLMs concentrate on the reasoning task based on the collected information (i.e., reasoning). Specially, we propose an invoking-linearization-generation procedure to support LLMs in reasoning on the structured data with the help of the interfaces. By iterating this procedure with provided interfaces, our approach can gradually approach the target answers to a given query. Experiments conducted on three types of structured data show that StructGPT greatly improves the performance of LLMs, under the few-shot and zero-shot settings",
    "checked": true,
    "id": "e0f27336698c84709bd60b6b7f4ce588cbae66bf",
    "semantic_title": "structgpt: a general framework for large language model to reason over structured data",
    "citation_count": 34,
    "authors": [
      "Jinhao Jiang",
      "Kun Zhou",
      "Zican Dong",
      "Keming Ye",
      "Xin Zhao",
      "Ji-Rong Wen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.575": {
    "title": "Modeling Legal Reasoning: LM Annotation at the Edge of Human Agreement",
    "volume": "main",
    "abstract": "Generative language models (LMs) are increasingly used for document class-prediction tasks and promise enormous improvements in cost and efficiency. Existing research often examines simple classification tasks, but the capability of LMs to classify on complex or specialized tasks is less well understood. We consider a highly complex task that is challenging even for humans: the classification of legal reasoning according to jurisprudential philosophy. Using a novel dataset of historical United States Supreme Court opinions annotated by a team of domain experts, we systematically test the performance of a variety of LMs. We find that generative models perform poorly when given instructions (i.e. prompts) equal to the instructions presented to human annotators through our codebook. Our strongest results derive from fine-tuning models on the annotated dataset; the best performing model is an in-domain model, LEGAL-BERT. We apply predictions from this fine-tuned model to study historical trends in jurisprudence, an exercise that both aligns with prominent qualitative historical accounts and points to areas of possible refinement in those accounts. Our findings generally sound a note of caution in the use of generative LMs on complex tasks without fine-tuning and point to the continued relevance of human annotation-intensive classification methods",
    "checked": true,
    "id": "18009ddaa9da7facab9bc578d6aa9bbc1ab4fc40",
    "semantic_title": "modeling legal reasoning: lm annotation at the edge of human agreement",
    "citation_count": 0,
    "authors": [
      "Rosamond Thalken",
      "Edward Stiglitz",
      "David Mimno",
      "Matthew Wilkens"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.576": {
    "title": "Model-tuning Via Prompts Makes NLP Models Adversarially Robust",
    "volume": "main",
    "abstract": "In recent years, NLP practitioners have converged on the following practice: (i) import an off-the-shelf pretrained (masked) language model; (ii) append a multilayer perceptron atop the CLS token's hidden representation (with randomly initialized weights); and (iii) fine-tune the entire model on a downstream task (MLP-FT). This procedure has produced massive gains on standard NLP benchmarks, but these models remain brittle, even to mild adversarial perturbations. In this work, we demonstrate surprising gains in adversarial robustness enjoyed by Model-tuning Via Prompts (MVP), an alternative method of adapting to downstream tasks. Rather than appending an MLP head to make output prediction, MVP appends a prompt template to the input, and makes prediction via text infilling/completion. Across 5 NLP datasets, 4 adversarial attacks, and 3 different models, MVP improves performance against adversarial substitutions by an average of 8% over standard methods and even outperforms adversarial training-based state-of-art defenses by 3.5%. By combining MVP with adversarial training, we achieve further improvements in adversarial robustness while maintaining performance on unperturbed examples. Finally, we conduct ablations to investigate the mechanism underlying these gains. Notably, we find that the main causes of vulnerability of MLP-FT can be attributed to the misalignment between pre-training and fine-tuning tasks, and the randomly initialized MLP parameters",
    "checked": true,
    "id": "b6499bcc10d4a70c3ca8b84995270cfd0d29de4c",
    "semantic_title": "model-tuning via prompts makes nlp models adversarially robust",
    "citation_count": 5,
    "authors": [
      "Mrigank Raman",
      "Pratyush Maini",
      "J Kolter",
      "Zachary Lipton",
      "Danish Pruthi"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.577": {
    "title": "Learning Co-Speech Gesture for Multimodal Aphasia Type Detection",
    "volume": "main",
    "abstract": "Aphasia, a language disorder resulting from brain damage, requires accurate identification of specific aphasia types, such as Broca's and Wernicke's aphasia, for effective treatment. However, little attention has been paid to developing methods to detect different types of aphasia. Recognizing the importance of analyzing co-speech gestures for distinguish aphasia types, we propose a multimodal graph neural network for aphasia type detection using speech and corresponding gesture patterns. By learning the correlation between the speech and gesture modalities for each aphasia type, our model can generate textual representations sensitive to gesture information, leading to accurate aphasia type detection. Extensive experiments demonstrate the superiority of our approach over existing methods, achieving state-of-the-art results (F1 84.2%). We also show that gesture features outperform acoustic features, highlighting the significance of gesture expression in detecting aphasia types. We provide the codes for reproducibility purposes",
    "checked": true,
    "id": "74faceeaaf22f10b370e67a5d15414709b5e8e8f",
    "semantic_title": "learning co-speech gesture for multimodal aphasia type detection",
    "citation_count": 0,
    "authors": [
      "Daeun Lee",
      "Sejung Son",
      "Hyolim Jeon",
      "Seungbae Kim",
      "Jinyoung Han"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.578": {
    "title": "STINMatch: Semi-Supervised Semantic-Topological Iteration Network for Financial Risk Detection via News Label Diffusion",
    "volume": "main",
    "abstract": "Commercial news provide rich semantics and timely information for automated financial risk detection. However, unaffordable large-scale annotation as well as training data sparseness barrier the full exploitation of commercial news in risk detection. To address this problem, we propose a semi-supervised Semantic-Topological Iteration Network, STINMatch, along with a news-enterprise knowledge graph (NEKG) to endorse the risk detection enhancement. The proposed model incorporates a label correlation matrix and interactive consistency regularization techniques into the iterative joint learning framework of text and graph modules. The carefully designed framework takes full advantage of the labeled and unlabeled data as well as their interrelations, enabling deep label diffusion coordination between article-level semantics and label correlations following the topological structure. Extensive experiments demonstrate the superior effectiveness and generalization ability of STINMatch",
    "checked": true,
    "id": "b16496472a026fecede0bcfabe5c451a916d81d2",
    "semantic_title": "stinmatch: semi-supervised semantic-topological iteration network for financial risk detection via news label diffusion",
    "citation_count": 0,
    "authors": [
      "Xurui Li",
      "Yue Qin",
      "Rui Zhu",
      "Tianqianjin Lin",
      "Yongming Fan",
      "Yangyang Kang",
      "Kaisong Song",
      "Fubang Zhao",
      "Changlong Sun",
      "Haixu Tang",
      "Xiaozhong Liu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.579": {
    "title": "Centering the Margins: Outlier-Based Identification of Harmed Populations in Toxicity Detection",
    "volume": "main",
    "abstract": "The impact of AI models on marginalized communities has traditionally been measured by identifying performance differences between specified demographic subgroups. Though this approach aims to center vulnerable groups, it risks obscuring patterns of harm faced by intersectional subgroups or shared across multiple groups. To address this, we draw on theories of marginalization from disability studies and related disciplines, which state that people farther from the norm face greater adversity, to consider the \"margins\" in the domain of toxicity detection. We operationalize the \"margins\" of a dataset by employing outlier detection to identify text about people with demographic attributes distant from the \"norm\". We find that model performance is consistently worse for demographic outliers, with mean squared error (MSE) between outliers and non-outliers up to 70.4% worse across toxicity types. It is also worse for text outliers, with a MSE up to 68.4% higher for outliers than non-outliers. We also find text and demographic outliers to be particularly susceptible to errors in the classification of severe toxicity and identity attacks. Compared to analysis of disparities using traditional demographic breakdowns, we find that our outlier analysis frequently surfaces greater harms faced by a larger, more intersectional group, which suggests that outlier analysis is particularly beneficial for identifying harms against those groups",
    "checked": true,
    "id": "b16f0dec883168eaf844a873d3e4eb5874ec8d47",
    "semantic_title": "centering the margins: outlier-based identification of harmed populations in toxicity detection",
    "citation_count": 0,
    "authors": [
      "Vyoma Raman",
      "Eve Fleisig",
      "Dan Klein"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.580": {
    "title": "Describe Me an Auklet: Generating Grounded Perceptual Category Descriptions",
    "volume": "main",
    "abstract": "Human speakers can generate descriptions of perceptual concepts, abstracted from the instance-level. Moreover, such descriptions can be used by other speakers to learn provisional representations of those concepts. Learning and using abstract perceptual concepts is under-investigated in the language-and-vision field. The problem is also highly relevant to the field of representation learning in multi-modal NLP. In this paper, we introduce a framework for testing category-level perceptual grounding in multi-modal language models. In particular, we train separate neural networks to **generate** and **interpret** descriptions of visual categories. We measure the *communicative success* of the two models with the zero-shot classification performance of the interpretation model, which we argue is an indicator of perceptual grounding. Using this framework, we compare the performance of *prototype*- and *exemplar*-based representations. Finally, we show that communicative success exposes performance issues in the generation model, not captured by traditional intrinsic NLG evaluation metrics, and argue that these issues stem from a failure to properly ground language in vision at the category level",
    "checked": false,
    "id": "eecd56d997f04bfc08f59240c11f66bd6135b25f",
    "semantic_title": "describe me an aucklet: generating grounded perceptual category descriptions",
    "citation_count": 0,
    "authors": [
      "Bill Noble",
      "Nikolai Ilinykh"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.581": {
    "title": "Revisiting Automated Topic Model Evaluation with Large Language Models",
    "volume": "main",
    "abstract": "Topic models help us make sense of large text collections. Automatically evaluating their output and determining the optimal number of topics are both longstanding challenges, with no effective automated solutions to date. This paper proposes using large language models (LLMs) for these tasks. We find that LLMs appropriately assess the resulting topics, correlating more strongly with human judgments than existing automated metrics. However, the setup of the evaluation task is crucial — LLMs perform better on coherence ratings of word sets than on intrustion detection. We find that LLMs can also assist us in guiding us towards a reasonable number of topics. In actual applications, topic models are typically used to answer a research question related to a collection of texts. We can incorporate this research question in the prompt to the LLM, which helps estimating the optimal number of topics",
    "checked": false,
    "id": "a2943fe9c71f116cd08f5f43626033cb7039e966",
    "semantic_title": "re-visiting automated topic model evaluation with large language models",
    "citation_count": 0,
    "authors": [
      "Dominik Stammbach",
      "Vilém Zouhar",
      "Alexander Hoyle",
      "Mrinmaya Sachan",
      "Elliott Ash"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.582": {
    "title": "ORCHID: A Chinese Debate Corpus for Target-Independent Stance Detection and Argumentative Dialogue Summarization",
    "volume": "main",
    "abstract": "Dialogue agents have been receiving increasing attention for years, and this trend has been further boosted by the recent progress of large language models (LLMs). Stance detection and dialogue summarization are two core tasks of dialogue agents in application scenarios that involve argumentative dialogues. However, research on these tasks is limited by the insufficiency of public datasets, especially for non-English languages. To address this language resource gap in Chinese, we present ORCHID (Oral Chinese Debate), the first Chinese dataset for benchmarking target-independent stance detection and debate summarization. Our dataset consists of 1,218 real-world debates that were conducted in Chinese on 476 unique topics, containing 2,436 stance-specific summaries and 14,133 fully annotated utterances. Besides providing a versatile testbed for future research, we also conduct an empirical study on the dataset and propose an integrated task. The results show the challenging nature of the dataset and suggest a potential of incorporating stance detection in summarization for argumentative dialogue",
    "checked": true,
    "id": "d0f61d14394a4dd8c7a9357252dc4b8058910a04",
    "semantic_title": "orchid: a chinese debate corpus for target-independent stance detection and argumentative dialogue summarization",
    "citation_count": 0,
    "authors": [
      "Xiutian Zhao",
      "Ke Wang",
      "Wei Peng"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.583": {
    "title": "On the Benefits of Learning to Route in Mixture-of-Experts Models",
    "volume": "main",
    "abstract": "Mixture-of-Expert (MoE) Transformer models, such as the Switch Transformer, allow us to successfully scale up model sizes while keeping the amount of compute time fixed. Prior work has established the computational efficiency benefits of using these models. A core component of these models is a router that routes input tokens to different experts in a layer. We show theoretical and empirical evidence that the router's ability to route tokens intelligently confers a significant advantage to MoE models. We study synthetic settings where the input data is distributed in clusters and show theoretically and empirically that the router learns to route the inputs according to these clusters. Then we perform experiments on real data using the T5X library, where we observe that a trainable router confers a non-trivial benefit instead of a non-trainable router",
    "checked": true,
    "id": "ceb408c510c89a8e325c6db4de0844e93c79d653",
    "semantic_title": "on the benefits of learning to route in mixture-of-experts models",
    "citation_count": 0,
    "authors": [
      "Nishanth Dikkala",
      "Nikhil Ghosh",
      "Raghu Meka",
      "Rina Panigrahy",
      "Nikhil Vyas",
      "Xin Wang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.584": {
    "title": "SEAHORSE: A Multilingual, Multifaceted Dataset for Summarization Evaluation",
    "volume": "main",
    "abstract": "Reliable automatic evaluation of summarization systems is challenging due to the multifaceted and subjective nature of the task. This is especially the case for languages other than English, where human evaluations are scarce. In this work, we introduce SEAHORSE, a dataset for multilingual, multifaceted summarization evaluation. SEAHORSE consists of 96K summaries with human ratings along 6 dimensions of text quality: comprehensibility, repetition, grammar, attribution, main ideas, and conciseness, covering 6 languages, 9 systems, and 4 datasets. As a result of its size and scope, SEAHORSE can serve both as a benchmark to evaluate learnt metrics, as well as a large-scale resource for training such metrics. We show that metrics trained with SEAHORSE achieve strong performance on the out-of-domain meta-evaluation benchmarks TRUE (Honovich et al., 2022) and mFACE (Aharoni et al., 2022). We make the SEAHORSE dataset and metrics publicly available for future research on multilingual and multifaceted summarization evaluation",
    "checked": true,
    "id": "7ad0e4803dcf7232e33ea20842062aba99a5ddfb",
    "semantic_title": "seahorse: a multilingual, multifaceted dataset for summarization evaluation",
    "citation_count": 4,
    "authors": [
      "Elizabeth Clark",
      "Shruti Rijhwani",
      "Sebastian Gehrmann",
      "Joshua Maynez",
      "Roee Aharoni",
      "Vitaly Nikolaev",
      "Thibault Sellam",
      "Aditya Siddhant",
      "Dipanjan Das",
      "Ankur Parikh"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.585": {
    "title": "Query2doc: Query Expansion with Large Language Models",
    "volume": "main",
    "abstract": "This paper introduces a simple yet effective query expansion approach, denoted as query2doc, to improve both sparse and dense retrieval systems. The proposed method first generates pseudo-documents by few-shot prompting large language models (LLMs), and then expands the query with generated pseudo documents. LLMs are trained on web-scale text corpora and are adept at knowledge memorization. The pseudo-documents from LLMs often contain highly relevant information that can aid in query disambiguation and guide the retrievers. Experimental results demonstrate that query2doc boosts the performance of BM25 by 3% to 15% on ad-hoc IR datasets, such as MS-MARCO and TREC DL, without any model fine-tuning. Furthermore, our method also benefits state-of-the-art dense retrievers in terms of both in-domain and out-of-domain results",
    "checked": true,
    "id": "ccc772d88c231275f24c4fac9b28bbe0942e1107",
    "semantic_title": "query2doc: query expansion with large language models",
    "citation_count": 22,
    "authors": [
      "Liang Wang",
      "Nan Yang",
      "Furu Wei"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.586": {
    "title": "We Need to Talk About Reproducibility in NLP Model Comparison",
    "volume": "main",
    "abstract": "NLPers frequently face reproducibility crisis in a comparison of various models of a real-world NLP task. Many studies have empirically showed that the standard splits tend to produce low reproducible and unreliable conclusions, and they attempted to improve the splits by using more random repetitions. However, the improvement on the reproducibility in a comparison of NLP models is limited attributed to a lack of investigation on the relationship between the reproducibility and the estimator induced by a splitting strategy. In this paper, we formulate the reproducibility in a model comparison into a probabilistic function with regard to a conclusion. Furthermore, we theoretically illustrate that the reproducibility is qualitatively dominated by the signal-to-noise ratio (SNR) of a model performance estimator obtained on a corpus splitting strategy. Specifically, a higher value of the SNR of an estimator probably indicates a better reproducibility. On the basis of the theoretical motivations, we develop a novel mixture estimator of the performance of an NLP model with a regularized corpus splitting strategy based on a blocked 3× 2 cross-validation. We conduct numerical experiments on multiple NLP tasks to show that the proposed estimator achieves a high SNR, and it substantially increases the reproducibility. Therefore, we recommend the NLP practitioners to use the proposed method to compare NLP models instead of the methods based on the widely-used standard splits and the random splits with multiple repetitions",
    "checked": true,
    "id": "39172a2df40a5227a02d28818b69ac335332a909",
    "semantic_title": "we need to talk about reproducibility in nlp model comparison",
    "citation_count": 0,
    "authors": [
      "Yan Xue",
      "Xuefei Cao",
      "Xingli Yang",
      "Yu Wang",
      "Ruibo Wang",
      "Jihong Li"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.587": {
    "title": "Explore-Instruct: Enhancing Domain-Specific Instruction Coverage through Active Exploration",
    "volume": "main",
    "abstract": "Instruction-tuning can be substantially optimized through enhanced diversity, resulting in models capable of handling a broader spectrum of tasks. However, existing data employed for such tuning often exhibit an inadequate coverage of individual domains, limiting the scope for nuanced comprehension and interactions within these areas. To address this deficiency, we propose Explore-Instruct, a novel approach to enhance the data coverage to be used in domain-specific instruction-tuning through active exploration via Large Language Models (LLMs). Built upon representative domain use cases, Explore-Instruct explores a multitude of variations or possibilities by implementing a search algorithm to obtain diversified and domain-focused instruction-tuning data. Our data-centric analysis validates the effectiveness of this proposed approach in improving domain-specific instruction coverage. Moreover, our model's performance demonstrates considerable advancements over multiple baselines, including those utilizing domain-specific data enhancement. Our findings offer a promising opportunity to improve instruction coverage, especially in domain-specific contexts, thereby advancing the development of adaptable language models. Our code, model weights, and data are public at https://github.com/fanqiwan/Explore-Instruct",
    "checked": true,
    "id": "3b918b15178bcc84fd22af5094fe1efbcd388e72",
    "semantic_title": "explore-instruct: enhancing domain-specific instruction coverage through active exploration",
    "citation_count": 1,
    "authors": [
      "Fanqi Wan",
      "Xinting Huang",
      "Tao Yang",
      "Xiaojun Quan",
      "Wei Bi",
      "Shuming Shi"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.588": {
    "title": "Practical Computational Power of Linear Transformers and Their Recurrent and Self-Referential Extensions",
    "volume": "main",
    "abstract": "Recent studies of the computational power of recurrent neural networks (RNNs) reveal a hierarchy of RNN architectures, given real-time and finite-precision assumptions. Here we study auto-regressive Transformers with linearised attention, a.k.a. linear Transformers (LTs) or Fast Weight Programmers (FWPs). LTs are special in the sense that they are equivalent to RNN-like sequence processors with a fixed-size state, while they can also be expressed as the now-popular self-attention networks. We show that many well-known results for the standard Transformer directly transfer to LTs/FWPs. Our formal language recognition experiments demonstrate how recently proposed FWP extensions such as recurrent FWPs and self-referential weight matrices successfully overcome certain limitations of the LT, e.g., allowing for generalisation on the parity problem. Our code is public",
    "checked": true,
    "id": "9c71d178705989cd4371f8e760508f11b18a4bb4",
    "semantic_title": "practical computational power of linear transformers and their recurrent and self-referential extensions",
    "citation_count": 1,
    "authors": [
      "Kazuki Irie",
      "Róbert Csordás",
      "Jürgen Schmidhuber"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.589": {
    "title": "InterFair: Debiasing with Natural Language Feedback for Fair Interpretable Predictions",
    "volume": "main",
    "abstract": "Debiasing methods in NLP models traditionally focus on isolating information related to a sensitive attribute (e.g., gender or race). We instead argue that a favorable debiasing method should use sensitive information ‘fairly,' with explanations, rather than blindly eliminating it. This fair balance is often subjective and can be challenging to achieve algorithmically. We explore two interactive setups with a frozen predictive model and show that users able to provide feedback can achieve a better and fairer balance between task performance and bias mitigation. In one setup, users, by interacting with test examples, further decreased bias in the explanations (5-8%) while maintaining the same prediction accuracy. In the other setup, human feedback was able to disentangle associated bias and predictive information from the input leading to superior bias mitigation and improved task performance (4-5%) simultaneously",
    "checked": true,
    "id": "ae4876dfcb481a7c3d025b35fc67c18c744ce38e",
    "semantic_title": "interfair: debiasing with natural language feedback for fair interpretable predictions",
    "citation_count": 1,
    "authors": [
      "Bodhisattwa Majumder",
      "Zexue He",
      "Julian McAuley"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.590": {
    "title": "Just Adjust One Prompt: Enhancing In-Context Dialogue Scoring via Constructing the Optimal Subgraph of Demonstrations and Prompts",
    "volume": "main",
    "abstract": "The use of modern Large Language Models (LLMs) as chatbots still has some problems such as hallucinations and lack of empathy. Identifying these issues can help improve chatbot performance. The community has been continually iterating on reference-free dialogue evaluation methods based on large language models (LLMs) that can be readily applied. However, many of these LLM-based metrics require selecting specific datasets and developing specialized training tasks for different evaluation dimensions (e.g., coherence, informative). The developing step can be time-consuming and may need to be repeated for new evaluation dimensions. To enable efficient and flexible adaptation to diverse needs of dialogue evaluation, we propose a dimension-agnostic scoring method that leverages the in-context learning (ICL) capability of LLMs to learn from human scoring to the fullest extent. Our method has three key features. To begin with, rather than manual prompt crafting, we propose automatically generating prompts, allowing the LLM to observe human labels and summarize the most suitable prompt. Additionally, since the LLM has a token limit and ICL is sensitive to demonstration variations, we train a selector to finely customize demonstrations and prompts for each dialogue input. Finally, during inference, we propose to request the LLM multiple times with a subgraph of demonstrations and prompts that are diverse and suitable to maximize ICL from various human scoring. We validate the efficacy of our method on five datasets, even with a small amount of annotated data, our method outperforms all strong baselines. Code is available at https://github.com/iamlxb3/EMNLP2023-ADOROR",
    "checked": true,
    "id": "6b6beed9da4d45666f8203fd9ae2b83140784bc6",
    "semantic_title": "just adjust one prompt: enhancing in-context dialogue scoring via constructing the optimal subgraph of demonstrations and prompts",
    "citation_count": 0,
    "authors": [
      "Jiashu Pu",
      "Ling Cheng",
      "Lu Fan",
      "Tangjie Lv",
      "Rongsheng Zhang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.591": {
    "title": "Multilingual estimation of political-party positioning: From label aggregation to long-input Transformers",
    "volume": "main",
    "abstract": "Scaling analysis is a technique in computational political science that assigns a political actor (e.g. politician or party) a score on a predefined scale based on a (typically long) body of text (e.g. a parliamentary speech or an election manifesto). For example, political scientists have often used the left–right scale to systematically analyse political landscapes of different countries. NLP methods for automatic scaling analysis can find broad application provided they (i) are able to deal with long texts and (ii) work robustly across domains and languages. In this work, we implement and compare two approaches to automatic scaling analysis of political-party manifestos: label aggregation, a pipeline strategy relying on annotations of individual statements from the manifestos, and long-input-Transformer-based models, which compute scaling values directly from raw text. We carry out the analysis of the Comparative Manifestos Project dataset across 41 countries and 27 languages and find that the task can be efficiently solved by state-of-the-art models, with label aggregation producing the best results",
    "checked": true,
    "id": "61f07966e88e0a1330838f318d0e7e06ec648871",
    "semantic_title": "multilingual estimation of political-party positioning: from label aggregation to long-input transformers",
    "citation_count": 0,
    "authors": [
      "Dmitry Nikolaev",
      "Tanise Ceron",
      "Sebastian Padó"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.592": {
    "title": "ART: rule bAsed futuRe-inference deducTion",
    "volume": "main",
    "abstract": "Deductive reasoning is a crucial cognitive ability of humanity, allowing us to derive valid conclusions from premises and observations. However, existing works mainly focus on language-based premises and generally neglect deductive reasoning from visual observations. In this work, we introduce rule bAsed futuRe-inference deducTion (ART), which aims at deducing the correct future event based on the visual phenomenon (a video) and the rule-based premises, along with an explanation of the reasoning process. To advance this field, we construct a large-scale densely annotated dataset (Video-ART), where the premises, future event candidates, the reasoning process explanation, and auxiliary commonsense knowledge (e.g., actions and appearance) are annotated by annotators. Upon Video-ART, we develop a strong baseline named ARTNet. In essence, guided by commonsense knowledge, ARTNet learns to identify the target video character and perceives its visual clues related to the future event. Then, ARTNet rigorously applies the given premises to conduct reasoning from the identified information to future events, through a non-parametric rule reasoning network and a reasoning-path review module. Empirical studies validate the rationality of ARTNet in deductive reasoning upon visual observations and the effectiveness over existing works",
    "checked": true,
    "id": "f70365cd31c3e134ec7b0c09a1ca622e7b2276a9",
    "semantic_title": "art: rule based future-inference deduction",
    "citation_count": 0,
    "authors": [
      "Mengze Li",
      "Tianqi Zhao",
      "Bai Jionghao",
      "Baoyi He",
      "Jiaxu Miao",
      "Wei Ji",
      "Zheqi Lv",
      "Zhou Zhao",
      "Shengyu Zhang",
      "Wenqiao Zhang",
      "Fei Wu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.593": {
    "title": "EpiK-Eval: Evaluation for Language Models as Epistemic Models",
    "volume": "main",
    "abstract": "In the age of artificial intelligence, the role of large language models (LLMs) is becoming increasingly central. Despite their growing prevalence, their capacity to consolidate knowledge from different training documents—a crucial ability in numerous applications—remains unexplored. This paper presents the first study examining the capability of LLMs to effectively combine such information within their parameter space. We introduce EpiK-Eval, a novel question-answering benchmark tailored to evaluate LLMs' proficiency in formulating a coherent and consistent knowledge representation from segmented narratives. Evaluations across various LLMs reveal significant weaknesses in this domain. We contend that these shortcomings stem from the intrinsic nature of prevailing training objectives. Consequently, we advocate for refining the approach towards knowledge consolidation, as it harbors the potential to dramatically improve their overall effectiveness and performance. The findings from this study offer insights for developing more robust and reliable LLMs. Our code and benchmark are available at https://github.com/chandar-lab/EpiK-Eval",
    "checked": true,
    "id": "ec5b621ff9b33fa3920a34509464352d14d5ec68",
    "semantic_title": "epik-eval: evaluation for language models as epistemic models",
    "citation_count": 0,
    "authors": [
      "Gabriele Prato",
      "Jerry Huang",
      "Prasanna Parthasarathi",
      "Shagun Sodhani",
      "Sarath Chandar"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.594": {
    "title": "From Dissonance to Insights: Dissecting Disagreements in Rationale Construction for Case Outcome Classification",
    "volume": "main",
    "abstract": "In legal NLP, Case Outcome Classification (COC) must not only be accurate but also trustworthy and explainable. Existing work in explainable COC has been limited to annotations by a single expert. However, it is well-known that lawyers may disagree in their assessment of case facts. We hence collect a novel dataset RaVE: Rationale Variation in ECHR, which is obtained from two experts in the domain of international human rights law, for whom we observe weak agreement. We study their disagreements and build a two-level task-independent taxonomy, supplemented with COC-specific subcategories. To our knowledge, this is the first work in the legal NLP that focuses on human label variation. We quantitatively assess different taxonomy categories and find that disagreements mainly stem from underspecification of the legal context, which poses challenges given the typically limited granularity and noise in COC metadata. We further assess the explainablility of state-of-the-art COC models on RaVE and observe limited agreement between models and experts. Overall, our case study reveals hitherto underappreciated complexities in creating benchmark datasets in legal NLP that revolve around identifying aspects of a case's facts supposedly relevant for its outcome",
    "checked": true,
    "id": "48fb0fa3f82aa6d80694b5e4e15e6c89a2b169e1",
    "semantic_title": "from dissonance to insights: dissecting disagreements in rationale construction for case outcome classification",
    "citation_count": 1,
    "authors": [
      "Shanshan Xu",
      "Santosh T.y.s.s",
      "Oana Ichim",
      "Isabella Risini",
      "Barbara Plank",
      "Matthias Grabmair"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.595": {
    "title": "On Bilingual Lexicon Induction with Large Language Models",
    "volume": "main",
    "abstract": "Bilingual Lexicon Induction (BLI) is a core task in multilingual NLP that still, to a large extent, relies on calculating cross-lingual word representations. Inspired by the global paradigm shift in NLP towards Large Language Models (LLMs), we examine the potential of the latest generation of LLMs for the development of bilingual lexicons. We ask the following research question: Is it possible to prompt and fine-tune multilingual LLMs (mLLMs) for BLI, and how does this approach compare against and complement current BLI approaches? To this end, we systematically study 1) zero-shot prompting for unsupervised BLI and 2) few-shot in-context prompting with a set of seed translation pairs, both without any LLM fine-tuning, as well as 3) standard BLI-oriented fine-tuning of smaller LLMs. We experiment with 18 open-source text-to-text mLLMs of different sizes (from 0.3B to 13B parameters) on two standard BLI benchmarks covering a range of typologically diverse languages. Our work is the first to demonstrate strong BLI capabilities of text-to-text mLLMs. The results reveal that few-shot prompting with in-context examples from nearest neighbours achieves the best performance, establishing new state-of-the-art BLI scores for many language pairs. We also conduct a series of in-depth analyses and ablation studies, providing more insights on BLI with (m)LLMs, also along with their limitations",
    "checked": true,
    "id": "6036f424468a5be5dd9b427ae266b72cb8468b5f",
    "semantic_title": "on bilingual lexicon induction with large language models",
    "citation_count": 0,
    "authors": [
      "Yaoyiran Li",
      "Anna Korhonen",
      "Ivan Vulić"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.596": {
    "title": "Statistical Depth for Ranking and Characterizing Transformer-Based Text Embeddings",
    "volume": "main",
    "abstract": "The popularity of transformer-based text embeddings calls for better statistical tools for measuring distributions of such embeddings. One such tool would be a method for ranking texts within a corpus by centrality, i.e. assigning each text a number signifying how representative that text is of the corpus as a whole. However, an intrinsic center-outward ordering of high-dimensional text representations is not trivial. A statistical depth is a function for ranking k-dimensional objects by measuring centrality with respect to some observed k-dimensional distribution. We adopt a statistical depth to measure distributions of transformer-based text embeddings, transformer-based text embedding (TTE) depth, and introduce the practical use of this depth for both modeling and distributional inference in NLP pipelines. We first define TTE depth and an associated rank sum test for determining whether two corpora differ significantly in embedding space. We then use TTE depth for the task of in-context learning prompt selection, showing that this approach reliably improves performance over statistical baseline approaches across six text classification tasks. Finally, we use TTE depth and the associated rank sum test to characterize the distributions of synthesized and human-generated corpora, showing that five recent synthetic data augmentation processes cause a measurable distributional shift away from associated human-generated text",
    "checked": true,
    "id": "4c7bb94b7e3764d71a567ae38cc2673b74d8544b",
    "semantic_title": "statistical depth for ranking and characterizing transformer-based text embeddings",
    "citation_count": 0,
    "authors": [
      "Parker Seegmiller",
      "Sarah Preum"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.597": {
    "title": "CRaSh: Clustering, Removing, and Sharing Enhance Fine-tuning without Full Large Language Model",
    "volume": "main",
    "abstract": "Instruction tuning has recently been recognized as an effective way of aligning Large Language Models (LLMs) to enhance their generalization ability across various tasks. However, when tuning publicly accessible, centralized LLMs with private instruction data, privacy concerns are inevitable. While direct transfer of parameterized modules between models is a plausible approach to address this, its implications and effectiveness need further exploration. This paper focuses on Offsite-Tuning (OFT), a representative technique that transfers transformer blocks between centralized LLMs and downstream emulators. Given the limited understanding of the underlying mechanism of OFT, we perform an empirical analysis on LLMs from the perspectives of representation and functional similarity. Interestingly, our findings reveal a unique modular structure within the layers of LLMs that appears to emerge as the model size expands. Simultaneously, we note subtle but potentially significant changes in representation and intermediate predictions across the layers. Inspired by these observations, we propose CRaSh, involving Clustering, Removing, and Sharing, a training-free strategy to derive improved emulators from LLMs. CRaSh significantly boosts performance of OFT with billions of parameters. Furthermore, we investigate the optimal solutions yielded by fine-tuning with and without full model through the lens of loss landscape. Our findings demonstrate a linear connectivity among these optima falling over the same basin, thereby highlighting the effectiveness of CRaSh and OFT",
    "checked": true,
    "id": "61dfb47883a41745760fa714f8ef29bcb61e7e8a",
    "semantic_title": "crash: clustering, removing, and sharing enhance fine-tuning without full large language model",
    "citation_count": 0,
    "authors": [
      "Kaiyan Zhang",
      "Ning Ding",
      "Biqing Qi",
      "Xuekai Zhu",
      "Xinwei Long",
      "Bowen Zhou"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.598": {
    "title": "From Multilingual Complexity to Emotional Clarity: Leveraging Commonsense to Unveil Emotions in Code-Mixed Dialogues",
    "volume": "main",
    "abstract": "Understanding emotions during conversation is a fundamental aspect of human communication, driving NLP research for Emotion Recognition in Conversation (ERC). While considerable research has focused on discerning emotions of individual speakers in monolingual dialogues, understanding the emotional dynamics in code-mixed conversations has received relatively less attention. This motivates our undertaking of ERC for code-mixed conversations in this study. Recognizing that emotional intelligence encompasses a comprehension of worldly knowledge, we propose an innovative approach that integrates commonsense information with dialogue context to facilitate a deeper understanding of emotions. To achieve this, we devise an efficient pipeline that extracts relevant commonsense from existing knowledge graphs based on the code-mixed input. Subsequently, we develop an advanced fusion technique that seamlessly combines the acquired commonsense information with the dialogue representation obtained from a dedicated dialogue understanding module. Our comprehensive experimentation showcases the substantial performance improvement obtained through the systematic incorporation of commonsense in ERC. Both quantitative assessments and qualitative analyses further corroborate the validity of our hypothesis, reaffirming the pivotal role of commonsense integration in enhancing ERC",
    "checked": true,
    "id": "7756ffd332aa05df503c80a2425d06f00f0ae5ce",
    "semantic_title": "from multilingual complexity to emotional clarity: leveraging commonsense to unveil emotions in code-mixed dialogues",
    "citation_count": 0,
    "authors": [
      "Shivani Kumar",
      "Ramaneswaran S",
      "Md Akhtar",
      "Tanmoy Chakraborty"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.599": {
    "title": "Large Language Models are biased to overestimate profoundness",
    "volume": "main",
    "abstract": "Recent advancements in natural language processing by large language models (LLMs), such as GPT-4, have been suggested to approach Artificial General Intelligence. And yet, it is still under dispute whether LLMs possess similar reasoning abilities to humans. This study evaluates GPT-4 and various other LLMs in judging the profoundness of mundane, motivational, and pseudo-profound statements. We found a significant statement-to-statement correlation between the LLMs and humans, irrespective of the type of statements and the prompting technique used. However, LLMs systematically overestimate the profoundness of nonsensical statements, with the exception of Tk-instruct, which uniquely underestimates the profoundness of statements. Only few-shot learning prompts, as opposed to chain-of-thought prompting, draw LLMs ratings closer to humans. Furthermore, this work provides insights into the potential biases induced by Reinforcement Learning from Human Feedback (RLHF), inducing an increase in the bias to overestimate the profoundness of statements",
    "checked": true,
    "id": "d0ffb09a00b67365efb9e217c3fd45d804733810",
    "semantic_title": "large language models are biased to overestimate profoundness",
    "citation_count": 0,
    "authors": [
      "Eugenio Herrera-Berg",
      "Tomás Browne",
      "Pablo León-Villagrá",
      "Marc-Lluís Vives",
      "Cristian Calderon"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.600": {
    "title": "SummEdits: Measuring LLM Ability at Factual Reasoning Through The Lens of Summarization",
    "volume": "main",
    "abstract": "With the recent appearance of LLMs in practical settings, having methods that can effectively detect factual inconsistencies is crucial to reduce the propagation of misinformation and improve trust in model outputs. When testing on existing factual consistency benchmarks, we find that a few large language models (LLMs) perform competitively on classification benchmarks for factual inconsistency detection compared to traditional non-LLM methods. However, a closer analysis reveals issues with existing evaluation benchmarks, affecting evaluation precision. To address this, we propose a new protocol for inconsistency detection benchmark creation and implement it in a 10-domain benchmark called SummEdits. This new benchmark is 20 times more cost-effective per sample than previous benchmarks and highly reproducible, as we estimate inter-annotator agreement at about 0.9. Most LLMs struggle on SummEdits, with performance close to random chance. The best-performing model, GPT-4, is still 8% below estimated human performance, highlighting the gaps in LLMs' ability to reason about facts and detect inconsistencies when they occur",
    "checked": true,
    "id": "76d7bbb458573f53d95652de5e5906ae710183a7",
    "semantic_title": "summedits: measuring llm ability at factual reasoning through the lens of summarization",
    "citation_count": 0,
    "authors": [
      "Philippe Laban",
      "Wojciech Kryscinski",
      "Divyansh Agarwal",
      "Alexander Fabbri",
      "Caiming Xiong",
      "Shafiq Joty",
      "Chien-Sheng Wu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.601": {
    "title": "DIVE: Towards Descriptive and Diverse Visual Commonsense Generation",
    "volume": "main",
    "abstract": "Towards human-level visual understanding, visual commonsense generation has been introduced to generate commonsense inferences beyond images. However, current research on visual commonsense generation has overlooked an important human cognitive ability: generating descriptive and diverse inferences. In this work, we propose a novel visual commonsense generation framework, called DIVE, which aims to improve the descriptiveness and diversity of generated inferences. DIVE involves two methods, generic inference filtering and contrastive retrieval learning, which address the limitations of existing visual commonsense resources and training objectives. Experimental results verify that DIVE outperforms state-of-the-art models for visual commonsense generation in terms of both descriptiveness and diversity, while showing a superior quality in generating unique and novel inferences. Notably, DIVE achieves human-level descriptiveness and diversity on Visual Commonsense Graphs. Furthermore, human evaluations confirm that DIVE aligns closely with human judgments on descriptiveness and diversity",
    "checked": true,
    "id": "a549aa8b539222d73f7abf4d79d82f25b69a566f",
    "semantic_title": "dive: towards descriptive and diverse visual commonsense generation",
    "citation_count": 0,
    "authors": [
      "Jun-Hyung Park",
      "Hyuntae Park",
      "Youjin Kang",
      "Eojin Jeon",
      "SangKeun Lee"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.602": {
    "title": "Towards Conceptualization of \"Fair Explanation\": Disparate Impacts of anti-Asian Hate Speech Explanations on Content Moderators",
    "volume": "main",
    "abstract": "Recent research at the intersection of AI explainability and fairness has focused on how explanations can improve human-plus-AI task performance as assessed by fairness measures. We propose to characterize what constitutes an explanation that is itself \"fair\" – an explanation that does not adversely impact specific populations. We formulate a novel evaluation method of \"fair explanations\" using not just accuracy and label time, but also psychological impact of explanations on different user groups across many metrics (mental discomfort, stereotype activation, and perceived workload). We apply this method in the context of content moderation of potential hate speech, and its differential impact on Asian vs. non-Asian proxy moderators, across explanation approaches (saliency map and counterfactual explanation). We find that saliency maps generally perform better and show less evidence of disparate impact (group) and individual unfairness than counterfactual explanations. Content warning: This paper contains examples of hate speech and racially discriminatory language. The authors do not support such content. Please consider your risk of discomfort carefully before continuing reading!",
    "checked": true,
    "id": "24b575827b84c2c8ecbb89934a92a36b1105ac85",
    "semantic_title": "towards conceptualization of \"fair explanation\": disparate impacts of anti-asian hate speech explanations on content moderators",
    "citation_count": 0,
    "authors": [
      "Tin Nguyen",
      "Jiannan Xu",
      "Aayushi Roy",
      "Hal Daumé III",
      "Marine Carpuat"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.603": {
    "title": "Bridging Background Knowledge Gaps in Translation with Automatic Explicitation",
    "volume": "main",
    "abstract": "Translations help people understand content written in another language. However, even correct literal translations do not fulfill that goal when people lack the necessary background to understand them. Professional translators incorporate explicitations to explain the missing context by considering cultural differences between source and target audiences. Despite its potential to help users, NLP research on explicitation is limited because of the dearth of adequate evaluation methods. This work introduces techniques for automatically generating explicitations, motivated by WikiExpl: a dataset that we collect from Wikipedia and annotate with human translators. The resulting explicitations are useful as they help answer questions more accurately in a multilingual question answering framework",
    "checked": true,
    "id": "049a30eb32d4034333052d011707abaea6f1a44b",
    "semantic_title": "bridging background knowledge gaps in translation with automatic explicitation",
    "citation_count": 0,
    "authors": [
      "HyoJung Han",
      "Jordan Boyd-Graber",
      "Marine Carpuat"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.604": {
    "title": "A Quality-based Syntactic Template Retriever for Syntactically-Controlled Paraphrase Generation",
    "volume": "main",
    "abstract": "Existing syntactically-controlled paraphrase generation (SPG) models perform promisingly with human-annotated or well-chosen syntactic templates. However, the difficulty of obtaining such templates actually hinders the practical application of SPG models. For one thing, the prohibitive cost makes it unfeasible to manually design decent templates for every source sentence. For another, the templates automatically retrieved by current heuristic methods are usually unreliable for SPG models to generate qualified paraphrases. To escape this dilemma, we propose a novel Quality-based Syntactic Template Retriever (QSTR) to retrieve templates based on the quality of the to-be-generated paraphrases. Furthermore, for situations requiring multiple paraphrases for each source sentence, we design a Diverse Templates Search (DTS) algorithm, which can enhance the diversity between paraphrases without sacrificing quality. Experiments demonstrate that QSTR can significantly surpass existing retrieval methods in generating high-quality paraphrases and even perform comparably with human-annotated templates in terms of reference-free metrics. Additionally, human evaluation and the performance on downstream tasks using our generated paraphrases for data augmentation showcase the potential of our QSTR and DTS algorithm in practical scenarios",
    "checked": true,
    "id": "3372a1d43b4a9daa33f3ac15cf31841cf6940303",
    "semantic_title": "a quality-based syntactic template retriever for syntactically-controlled paraphrase generation",
    "citation_count": 0,
    "authors": [
      "Xue Zhang",
      "Songming Zhang",
      "Yunlong Liang",
      "Yufeng Chen",
      "Jian Liu",
      "Wenjuan Han",
      "Jinan Xu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.605": {
    "title": "Beyond Shared Vocabulary: Increasing Representational Word Similarities across Languages for Multilingual Machine Translation",
    "volume": "main",
    "abstract": "Using a shared vocabulary is common practice in Multilingual Neural Machine Translation (MNMT). In addition to its simple design, shared tokens play an important role in positive knowledge transfer, which manifests naturally when the shared tokens refer to similar meanings across languages. However, when words overlap is small, e.g., using different writing systems, transfer is inhibited. In this paper, we propose a re-parameterized method for building embeddings to alleviate this problem. More specifically, we define word-level information transfer pathways via word equivalence classes and rely on graph networks to fuse word embeddings across languages. Our experiments demonstrate the advantages of our approach: 1) the semantics of embeddings are better aligned across languages, 2) our method achieves evident BLEU improvements on high- and low-resource MNMT, and 3) only less than 1.0% additional trainable parameters are required with a limited increase in computational costs, while the inference time is identical to baselines",
    "checked": true,
    "id": "528f0e2478ddd554afe533533a0de192a6996cb2",
    "semantic_title": "beyond shared vocabulary: increasing representational word similarities across languages for multilingual machine translation",
    "citation_count": 3,
    "authors": [
      "Di Wu",
      "Christof Monz"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.606": {
    "title": "Quantifying the redundancy between prosody and text",
    "volume": "main",
    "abstract": "Prosody—the suprasegmental component of speech, including pitch, loudness, and tempo—carries critical aspects of meaning. However, the relationship between the information conveyed by prosody vs. by the words themselves remains poorly understood. We use large language models (LLMs) to estimate how much information is redundant between prosody and the words themselves. Using a large spoken corpus of English audiobooks, we extract prosodic features aligned to individual words and test how well they can be predicted from LLM embeddings, compared to non-contextual word embeddings. We find a high degree of redundancy between the information carried by the words and prosodic information across several prosodic features, including intensity, duration, pauses, and pitch contours. Furthermore, a word's prosodic information is redundant with both the word itself and the context preceding as well as following it. Still, we observe that prosodic features can not be fully predicted from text, suggesting that prosody carries information above and beyond the words. Along with this paper, we release a general-purpose data processing pipeline for quantifying the relationship between linguistic information and extra-linguistic features",
    "checked": true,
    "id": "645e7a5b9caa88a61c11bb87a10b07b90945640d",
    "semantic_title": "quantifying the redundancy between prosody and text",
    "citation_count": 0,
    "authors": [
      "Lukas Wolf",
      "Tiago Pimentel",
      "Evelina Fedorenko",
      "Ryan Cotterell",
      "Alex Warstadt",
      "Ethan Wilcox",
      "Tamar Regev"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.607": {
    "title": "CRoW: Benchmarking Commonsense Reasoning in Real-World Tasks",
    "volume": "main",
    "abstract": "Recent efforts in natural language processing (NLP) commonsense reasoning research have yielded a considerable number of new datasets and benchmarks. However, most of these datasets formulate commonsense reasoning challenges in artificial scenarios that are not reflective of the tasks which real-world NLP systems are designed to solve. In this work, we present CRoW, a manually-curated, multi-task benchmark that evaluates the ability of models to apply commonsense reasoning in the context of six real-world NLP tasks. CRoW is constructed using a multi-stage data collection pipeline that rewrites examples from existing datasets using commonsense-violating perturbations. We use CRoW to study how NLP systems perform across different dimensions of commonsense knowledge, such as physical, temporal, and social reasoning. We find a significant performance gap when NLP systems are evaluated on CRoW compared to humans, showcasing that commonsense reasoning is far from being solved in real-world task settings. We make our dataset and leaderboard available to the research community",
    "checked": true,
    "id": "23cc6b2ed88872fcd3767cf054100e8eddcdb0a1",
    "semantic_title": "crow: benchmarking commonsense reasoning in real-world tasks",
    "citation_count": 0,
    "authors": [
      "Mete Ismayilzada",
      "Debjit Paul",
      "Syrielle Montariol",
      "Mor Geva",
      "Antoine Bosselut"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.608": {
    "title": "A Video Is Worth 4096 Tokens: Verbalize Story Videos To Understand Them In Zero Shot",
    "volume": "main",
    "abstract": "Multimedia content, such as advertisements and story videos, exhibit a rich blend of creativity and multiple modalities. They incorporate elements like text, visuals, audio, and storytelling techniques, employing devices like emotions, symbolism, and slogans to convey meaning. There is a dearth of large annotated training datasets in the multimedia domain hindering the development of supervised learn-ing models with satisfactory performance for real-world applications. On the other hand, the rise of large language models (LLMs) has witnessed remarkable zero-shot performance in various natural language processing (NLP) tasks, such as emotion classification, question-answering, and topic classification. To leverage such advanced techniques to bridge this performance gap in multimedia understanding, we propose verbalizing long videos to generate their descriptions in natural language, followed by performing video-understanding tasks on the generated story as opposed to the original video. Through extensive experiments on fifteen video-understanding tasks, we demonstrate that our method, despite being zero-shot, achieves significantly better results than supervised baselines for video understanding. Furthermore, to alleviate a lack of story understanding benchmarks, we publicly release the first dataset on a crucial task in computational social science on persuasion strategy identification",
    "checked": true,
    "id": "fbad347962da015cd10af033fc697556d5e1be72",
    "semantic_title": "a video is worth 4096 tokens: verbalize story videos to understand them in zero shot",
    "citation_count": 8,
    "authors": [
      "Aanisha Bhattacharyya",
      "Yaman Singla",
      "Balaji Krishnamurthy",
      "Rajiv Shah",
      "Changyou Chen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.609": {
    "title": "Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning",
    "volume": "main",
    "abstract": "In-context learning (ICL) emerges as a promising capability of large language models (LLMs) by providing them with demonstration examples to perform diverse tasks. However, the underlying mechanism of how LLMs learn from the provided context remains under-explored. In this paper, we investigate the working mechanism of ICL through an information flow lens. Our findings reveal that label words in the demonstration examples function as anchors: (1) semantic information aggregates into label word representations during the shallow computation layers' processing; (2) the consolidated information in label words serves as a reference for LLMs' final predictions. Based on these insights, we introduce an anchor re-weighting method to improve ICL performance, a demonstration compression technique to expedite inference, and an analysis framework for diagnosing ICL errors in GPT2-XL. The promising applications of our findings again validate the uncovered ICL working mechanism and pave the way for future studies",
    "checked": true,
    "id": "7bd4ca8706a79983d31ab74e6c79bfdfd949602e",
    "semantic_title": "label words are anchors: an information flow perspective for understanding in-context learning",
    "citation_count": 4,
    "authors": [
      "Lean Wang",
      "Lei Li",
      "Damai Dai",
      "Deli Chen",
      "Hao Zhou",
      "Fandong Meng",
      "Jie Zhou",
      "Xu Sun"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.610": {
    "title": "Prompting Scientific Names for Zero-Shot Species Recognition",
    "volume": "main",
    "abstract": "Trained on web-scale image-text pairs, Vision-Language Models (VLMs) such as CLIP can recognize images of common objects in a zero-shot fashion. However, it is underexplored how to use CLIP for zero-shot recognition of highly specialized concepts, e.g., species of birds, plants, and animals, for which their scientific names are written in Latin or Greek. Indeed, CLIP performs poorly for zero-shot species recognition with prompts that use scientific names, e.g., \"a photo of Lepus Timidus\" (which is a scientific name in Latin). This is because these names are usually not included in CLIP's training set. To improve performance, we explore using large-language models (LLMs) to generate descriptions (e.g., of species color and shape) and additionally use them in prompts. However, this method improves only marginally. Instead, we are motivated to translate scientific names (e.g., Lepus Timidus) to common English names (e.g., mountain hare) and use such in the prompts. We find that common names are more likely to be included in CLIP's training set, and prompting them achieves 2~5 times higher accuracy on benchmarking datasets of fine-grained species recognition",
    "checked": true,
    "id": "6f1a6520095c2a7a3d78e8405895815f9fe4842e",
    "semantic_title": "prompting scientific names for zero-shot species recognition",
    "citation_count": 0,
    "authors": [
      "Shubham Parashar",
      "Zhiqiu Lin",
      "Yanan Li",
      "Shu Kong"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.611": {
    "title": "Active Learning for Natural Language Generation",
    "volume": "main",
    "abstract": "The field of Natural Language Generation (NLG) suffers from a severe shortage of labeled data due to the extremely expensive and time-consuming process involved in manual annotation. A natural approach for coping with this problem is active learning (AL), a well-known machine learning technique for improving annotation efficiency by selectively choosing the most informative examples to label. However, while AL has been well-researched in the context of text classification, its application to NLG remains largely unexplored. In this paper, we present a first systematic study of active learning for NLG, considering a diverse set of tasks and multiple leading selection strategies, and harnessing a strong instruction-tuned model. Our results indicate that the performance of existing AL strategies is inconsistent, surpassing the baseline of random example selection in some cases but not in others. We highlight some notable differences between the classification and generation scenarios, and analyze the selection behaviors of existing AL strategies. Our findings motivate exploring novel approaches for applying AL to generation tasks",
    "checked": true,
    "id": "954cf455d241d3948bcd9f6f7001b4c85020af76",
    "semantic_title": "active learning for natural language generation",
    "citation_count": 0,
    "authors": [
      "Yotam Perlitz",
      "Ariel Gera",
      "Michal Shmueli-Scheuer",
      "Dafna Sheinwald",
      "Noam Slonim",
      "Liat Ein-Dor"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.612": {
    "title": "Re3Dial: Retrieve, Reorganize and Rescale Conversations for Long-Turn Open-Domain Dialogue Pre-training",
    "volume": "main",
    "abstract": "Pre-training on large-scale open-domain dialogue data can substantially improve the performance of dialogue models. However, the pre-trained dialogue model's ability to utilize long-range context is limited due to the scarcity of long-turn dialogue sessions. Most dialogues in existing pre-training corpora contain fewer than three turns of dialogue. To alleviate this issue, we propose the Retrieve, Reorganize and Rescale framework (Re3Dial), which can automatically construct billion-scale long-turn dialogues by reorganizing existing short-turn ones. Given a short-turn session, Re3Dial first employs a session retriever to retrieve coherent consecutive sessions. To this end, we train the retriever to capture semantic and discourse relations within multi-turn dialogues through contrastive training. Next, Re3Dial samples a session from retrieved results following a diversity sampling strategy, which is designed to penalize repetitive or generic sessions. A longer session is then derived by concatenating the original session and the sampled session. By repeating the above process, Re3Dial can yield a coherent long-turn dialogue. Extensive experiments on multiple multi-turn dialogue benchmarks demonstrate that Re3Dial significantly improves the dialogue model's ability to utilize long-range context and thus generate more sensible and informative responses. Finally, we build a toolkit for efficiently rescaling conversations with Re3Dial, which enables us to construct a corpus containing 1B Chinese dialogue sessions with 11.3 turns on average (5X longer than the original corpus). We will release our retriever model, toolkit, and data for public use",
    "checked": false,
    "id": "5dda1816ae98a0d746d879d0b34fa181446e3b67",
    "semantic_title": "re³dial: retrieve, reorganize and rescale conversations for long-turn open-domain dialogue pre-training",
    "citation_count": 0,
    "authors": [
      "Jiaxin Wen",
      "Hao Zhou",
      "Jian Guan",
      "Jie Zhou",
      "Minlie Huang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.613": {
    "title": "MultiTurnCleanup: A Benchmark for Multi-Turn Spoken Conversational Transcript Cleanup",
    "volume": "main",
    "abstract": "Current disfluency detection models focus on individual utterances each from a single speaker. However, numerous discontinuity phenomena in spoken conversational transcripts occur across multiple turns, which can not be identified by disfluency detection models. This study addresses these phenomena by proposing an innovative Multi-Turn Cleanup task for spoken conversational transcripts and collecting a new dataset, MultiTurnCleanup. We design a data labeling schema to collect the high-quality dataset and provide extensive data analysis. Furthermore, we leverage two modeling approaches for experimental evaluation as benchmarks for future research",
    "checked": true,
    "id": "f631c058c2c6a6e3fe720929b96959fd93351956",
    "semantic_title": "multiturncleanup: a benchmark for multi-turn spoken conversational transcript cleanup",
    "citation_count": 0,
    "authors": [
      "Hua Shen",
      "Vicky Zayats",
      "Johann Rocholl",
      "Daniel Walker",
      "Dirk Padfield"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.614": {
    "title": "Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models",
    "volume": "main",
    "abstract": "Language models have graduated from being research prototypes to commercialized products offered as web APIs, and recent works have highlighted the multilingual capabilities of these products. The API vendors charge their users based on usage, more specifically on the number of \"tokens\" processed or generated by the underlying language models. What constitutes a token, however, is training data and model dependent with a large variance in the number of tokens required to convey the same information in different languages. In this work, we analyze the effect of this non-uniformity on the fairness of an API's pricing policy across languages. We conduct a systematic analysis of the cost and utility of OpenAI's language model API on multilingual benchmarks in 22 typologically diverse languages. We show evidence that speakers of a large number of the supported languages are overcharged while obtaining poorer results. These speakers tend to also come from regions where the APIs are less affordable, to begin with. Through these analyses, we aim to increase transparency around language model APIs' pricing policies and encourage the vendors to make them more equitable",
    "checked": true,
    "id": "17fbffb05fa14e21d1c506fd5f0f568b955fe983",
    "semantic_title": "do all languages cost the same? tokenization in the era of commercial language models",
    "citation_count": 6,
    "authors": [
      "Orevaoghene Ahia",
      "Sachin Kumar",
      "Hila Gonen",
      "Jungo Kasai",
      "David Mortensen",
      "Noah Smith",
      "Yulia Tsvetkov"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.615": {
    "title": "Characterizing Mechanisms for Factual Recall in Language Models",
    "volume": "main",
    "abstract": "Language Models (LMs) often must integrate facts they memorized in pretraining with new information that appears in a given context. These two sources can disagree, causing competition within the model, and it is unclear how an LM will resolve the conflict. On a dataset that queries for knowledge of world capitals, we investigate both distributional and mechanistic determinants of LM behavior in such situations. Specifically, we measure the proportion of the time an LM will use a counterfactual prefix (e.g., \"The capital of Poland is London\") to overwrite what it learned in pretraining (\"Warsaw\"). On Pythia and GPT2, the training frequency of both the query country (\"Poland\") and the in-context city (\"London\") highly affect the models' likelihood of using the counterfactual. We then use head attribution to identify individual attention heads that either promote the memorized answer or the in-context answer in the logits. By scaling up or down the value vector of these heads, we can control the likelihood of using the in-context answer on new data. This method can increase the rate of generating the in-context answer to 88% of the time simply by scaling a single head at runtime. Our work contributes to a body of evidence showing that we can often localize model behaviors to specific components and provides a proof of concept for how future methods might control model behavior dynamically at runtime",
    "checked": true,
    "id": "f44c1d2f1764bbf186c95af1b3b5a624b9801d18",
    "semantic_title": "characterizing mechanisms for factual recall in language models",
    "citation_count": 0,
    "authors": [
      "Qinan Yu",
      "Jack Merullo",
      "Ellie Pavlick"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.616": {
    "title": "MULTITuDE: Large-Scale Multilingual Machine-Generated Text Detection Benchmark",
    "volume": "main",
    "abstract": "There is a lack of research into capabilities of recent LLMs to generate convincing text in languages other than English and into performance of detectors of machine-generated text in multilingual settings. This is also reflected in the available benchmarks which lack authentic texts in languages other than English and predominantly cover older generators. To fill this gap, we introduce MULTITuDE, a novel benchmarking dataset for multilingual machine-generated text detection comprising of 74,081 authentic and machine-generated texts in 11 languages (ar, ca, cs, de, en, es, nl, pt, ru, uk, and zh) generated by 8 multilingual LLMs. Using this benchmark, we compare the performance of zero-shot (statistical and black-box) and fine-tuned detectors. Considering the multilinguality, we evaluate 1) how these detectors generalize to unseen languages (linguistically similar as well as dissimilar) and unseen LLMs and 2) whether the detectors improve their performance when trained on multiple languages",
    "checked": true,
    "id": "f050575e9567478ae6f1dcfe1d0aede50c76293e",
    "semantic_title": "multitude: large-scale multilingual machine-generated text detection benchmark",
    "citation_count": 4,
    "authors": [
      "Dominik Macko",
      "Robert Moro",
      "Adaku Uchendu",
      "Jason Lucas",
      "Michiharu Yamashita",
      "Matúš Pikuliak",
      "Ivan Srba",
      "Thai Le",
      "Dongwon Lee",
      "Jakub Simko",
      "Maria Bielikova"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.617": {
    "title": "Revisiting Block-based Quantisation: What is Important for Sub-8-bit LLM Inference?",
    "volume": "main",
    "abstract": "The inference of Large language models (LLMs) requires immense computation and memory resources. To curtail these costs, quantisation has emerged as a promising solution, but existing LLM quantisation mainly focuses on 8-bit. In this work, we explore the statistical and learning properties of the LLM layer and attribute the bottleneck of LLM quantisation to numerical scaling offsets. To address this, we adapt block quantisations for LLMs, a family of methods that share scaling factors across packed numbers. Block quantisations efficiently reduce the numerical scaling offsets solely from an arithmetic perspective, without additional treatments in the computational path. Our nearly-lossless quantised 6-bit LLMs achieve a 19× higher arithmetic density and 5× memory density than the float32 baseline, surpassing the prior art 8-bit quantisation by 2.5× in arithmetic density and 1.2× in memory density, without requiring any data calibration or re-training. We also share our insights into sub-8-bit LLM quantisation, including the mismatch between activation and weight distributions, optimal fine-tuning strategies, and a lower quantisation granularity inherent in the statistical properties of LLMs. The latter two tricks enable nearly-lossless 4-bit LLMs on downstream tasks. Our code is open-sourced",
    "checked": true,
    "id": "c51b8da83b1a9c52297c910b6d9341868e8f0962",
    "semantic_title": "revisiting block-based quantisation: what is important for sub-8-bit llm inference?",
    "citation_count": 0,
    "authors": [
      "Cheng Zhang",
      "Jianyi Cheng",
      "Ilia Shumailov",
      "George Constantinides",
      "Yiren Zhao"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.618": {
    "title": "Whispering LLaMA: A Cross-Modal Generative Error Correction Framework for Speech Recognition",
    "volume": "main",
    "abstract": "We introduce a new cross-modal fusion technique designed for generative error correction in automatic speech recognition (ASR). Our methodology leverages both acoustic information and external linguistic representations to generate accurate speech transcription contexts. This marks a step towards a fresh paradigm in generative error correction within the realm of n-best hypotheses. Unlike the existing ranking-based rescoring methods, our approach adeptly uses distinct initialization techniques and parameter-efficient algorithms to boost ASR performance derived from pre-trained speech and text models. Through evaluation across diverse ASR datasets, we assess our fusion technique, demonstrating a 37.66% improvement in word error rate (WER) relative performance compared to the n-best Oracle. To encourage future research, we have made our code and pre-trained models open source at [https://github.com/Srijith-rkr/Whispering-LLaMA](https://github.com/Srijith-rkr/Whispering-LLaMA)",
    "checked": true,
    "id": "75256fa7844ba49514b4fd4e29c327ca89998a80",
    "semantic_title": "whispering llama: a cross-modal generative error correction framework for speech recognition",
    "citation_count": 1,
    "authors": [
      "Srijith Radhakrishnan",
      "Chao-Han Yang",
      "Sumeer Khan",
      "Rohit Kumar",
      "Narsis Kiani",
      "David Gomez-Cabrero",
      "Jesper Tegnér"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.619": {
    "title": "Reducing Sequence Length by Predicting Edit Spans with Large Language Models",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance in various tasks and gained significant attention. LLMs are also used for local sequence transduction tasks, including grammatical error correction (GEC) and formality style transfer, where most tokens in a source text are kept unchanged. However, the models that generate all target tokens in such tasks have a tendency to simply copy the input text as is, without making needed changes, because the difference between input and output texts is minimal in the training data. This is also inefficient because the computational cost grows quadratically with the target sequence length with Transformer. This paper proposes predicting edit spans for the source text for local sequence transduction tasks. Representing an edit span with a position of the source text and corrected tokens, we can reduce the length of the target sequence and the computational cost for inference. We apply instruction tuning for LLMs on the supervision data of edit spans. Experiments show that the proposed method achieves comparable performance to the baseline in four tasks, paraphrasing, formality style transfer, GEC, and text simplification, despite reducing the length of the target text by as small as 21%. Furthermore, we report that the task-specific fine-tuning with the proposed method achieved state-of-the-art performance in the four tasks",
    "checked": true,
    "id": "417edcd9dc4a4ad6de36810bfb44c7871d8c8633",
    "semantic_title": "reducing sequence length by predicting edit spans with large language models",
    "citation_count": 2,
    "authors": [
      "Masahiro Kaneko",
      "Naoaki Okazaki"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.620": {
    "title": "Instruct and Extract: Instruction Tuning for On-Demand Information Extraction",
    "volume": "main",
    "abstract": "Large language models with instruction-following capabilities open the door to a wider group of users. However, when it comes to information extraction – a classic task in natural language processing – most task-specific systems cannot align well with long-tail ad hoc extraction use cases for non-expert users. To address this, we propose a novel paradigm, termed On-Demand Information Extraction, to fulfill the personalized demands of real-world users. Our task aims to follow the instructions to extract the desired content from the associated text and present it in a structured tabular format. The table headers can either be user-specified or inferred contextually by the model. To facilitate research in this emerging area, we present a benchmark named InstructIE, inclusive of both automatically generated training data, as well as the human-annotated test set. Building on InstructIE, we further develop an On-Demand Information Extractor, ODIE. Comprehensive evaluations on our benchmark reveal that ODIE substantially outperforms the existing open-source models of similar size",
    "checked": true,
    "id": "2967ab775f6cdabc6ab59010734f352dd3ebc8d6",
    "semantic_title": "instruct and extract: instruction tuning for on-demand information extraction",
    "citation_count": 0,
    "authors": [
      "Yizhu Jiao",
      "Ming Zhong",
      "Sha Li",
      "Ruining Zhao",
      "Siru Ouyang",
      "Heng Ji",
      "Jiawei Han"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.621": {
    "title": "Rethinking the Evaluation for Conversational Recommendation in the Era of Large Language Models",
    "volume": "main",
    "abstract": "The recent success of large language models (LLMs) has shown great potential to develop more powerful conversational recommender systems (CRSs), which rely on natural language conversations to satisfy user needs. In this paper, we embark on an investigation into the utilization of ChatGPT for CRSs, revealing the inadequacy of the existing evaluation protocol. It might overemphasize the matching with ground-truth items annotated by humans while neglecting the interactive nature of CRSs. To overcome the limitation, we further propose an **i**nteractive **Eva**luation approach based on **L**L**M**s, named **iEvaLM**, which harnesses LLM-based user simulators. Our evaluation approach can simulate various system-user interaction scenarios. Through the experiments on two public CRS datasets, we demonstrate notable improvements compared to the prevailing evaluation protocol. Furthermore, we emphasize the evaluation of explainability, and ChatGPT showcases persuasive explanation generation for its recommendations. Our study contributes to a deeper comprehension of the untapped potential of LLMs for CRSs and provides a more flexible and realistic evaluation approach for future research about LLM-based CRSs",
    "checked": true,
    "id": "c476c1587beda904133f97592a39965be418c8bf",
    "semantic_title": "rethinking the evaluation for conversational recommendation in the era of large language models",
    "citation_count": 16,
    "authors": [
      "Xiaolei Wang",
      "Xinyu Tang",
      "Xin Zhao",
      "Jingyuan Wang",
      "Ji-Rong Wen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.622": {
    "title": "ReCEval: Evaluating Reasoning Chains via Correctness and Informativeness",
    "volume": "main",
    "abstract": "Multi-step reasoning ability is fundamental to many natural language tasks, yet it is unclear what constitutes a good reasoning chain and how to evaluate them. Most existing methods focus solely on whether the reasoning chain leads to the correct conclusion, but this answer-oriented view may confound reasoning quality with other spurious shortcuts to predict the answer. To bridge this gap, we evaluate reasoning chains by viewing them as informal proofs that derive the final answer. Specifically, we propose ReCEval (Reasoning Chain Evaluation), a framework that evaluates reasoning chains via two key properties: (1) correctness, i.e., each step makes a valid inference based on information contained within the step, preceding steps, and input context, and (2) informativeness, i.e., each step provides new information that is helpful towards deriving the generated answer. We evaluate these properties by developing metrics using natural language inference models and 𝒱-Information. On multiple datasets, we show that ReCEval effectively identifies various error types and yields notable improvements compared to prior methods. We analyze the impact of step boundaries, and previous steps on evaluating correctness and demonstrate that our informativeness metric captures the expected flow of information in high-quality reasoning chains. Finally, we show that scoring reasoning chains based on ReCEval improves downstream task performance",
    "checked": true,
    "id": "69bef4ab1018cc956a77c3ccdcaa57b124ab9fcc",
    "semantic_title": "receval: evaluating reasoning chains via correctness and informativeness",
    "citation_count": 11,
    "authors": [
      "Archiki Prasad",
      "Swarnadeep Saha",
      "Xiang Zhou",
      "Mohit Bansal"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.623": {
    "title": "Expand, Highlight, Generate: RL-driven Document Generation for Passage Reranking",
    "volume": "main",
    "abstract": "Generating synthetic training data based on large language models (LLMs) for ranking models has gained attention recently. Prior studies use LLMs to build pseudo query-document pairs by generating synthetic queries from documents in a corpus. In this paper, we propose a new perspective of data augmentation: generating synthetic documents from queries. To achieve this, we propose DocGen, that consists of a three-step pipeline that utilizes the few-shot capabilities of LLMs. DocGen pipeline performs synthetic document generation by (i) expanding, (ii) highlighting the original query, and then (iii) generating a synthetic document that is likely to be relevant to the query. To further improve the relevance between generated synthetic documents and their corresponding queries, we propose DocGen-RL, which regards the estimated relevance of the document as a reward and leverages reinforcement learning (RL) to optimize DocGen pipeline. Extensive experiments demonstrate that DocGen pipeline and DocGen-RL significantly outperform existing state-of-theart data augmentation methods, such as InPars, indicating that our new perspective of generating documents leverages the capacity of LLMs in generating synthetic data more effectively. We release the code, generated data, and model checkpoints to foster research in this area",
    "checked": true,
    "id": "5b855c71a4bbb7c1a188ea2611f88ca596640901",
    "semantic_title": "expand, highlight, generate: rl-driven document generation for passage reranking",
    "citation_count": 0,
    "authors": [
      "Arian Askari",
      "Mohammad Aliannejadi",
      "Chuan Meng",
      "Evangelos Kanoulas",
      "Suzan Verberne"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.624": {
    "title": "Transformer-based Live Update Generation for Soccer Matches from Microblog Posts",
    "volume": "main",
    "abstract": "It has been known to be difficult to generate adequate sports updates from a sequence of vast amounts of diverse live tweets, although the live sports viewing experience with tweets is gaining the popularity. In this paper, we focus on soccer matches and work on building a system to generate live updates for soccer matches from tweets so that users can instantly grasp a match's progress and enjoy the excitement of the match from raw tweets. Our proposed system is based on a large pre-trained language model and incorporates a mechanism to control the number of updates and a mechanism to reduce the redundancy of duplicate and similar updates",
    "checked": true,
    "id": "da13350a895e3f85ec969a82b57087ec442b7f81",
    "semantic_title": "transformer-based live update generation for soccer matches from microblog posts",
    "citation_count": 0,
    "authors": [
      "Masashi Oshika",
      "Kosuke Yamada",
      "Ryohei Sasano",
      "Koichi Takeda"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.625": {
    "title": "Make Every Example Count: On the Stability and Utility of Self-Influence for Learning from Noisy NLP Datasets",
    "volume": "main",
    "abstract": "Increasingly larger datasets have become a standard ingredient to advancing the state-of-the-art in NLP. However, data quality might have already become the bottleneck to unlock further gains. Given the diversity and the sizes of modern datasets, standard data filtering is not straight-forward to apply, because of the multifacetedness of the harmful data and elusiveness of filtering rules that would generalize across multiple tasks. We study the fitness of task-agnostic self-influence scores of training examples for data cleaning, analyze their efficacy in capturing naturally occurring outliers, and investigate to what extent self-influence based data cleaning can improve downstream performance in machine translation, question answering and text classification, building up on recent approaches to self-influence calculation and automated curriculum learning",
    "checked": false,
    "id": "71b9999d0a6fa81238cda2fb216e22f6584a4598",
    "semantic_title": "make every example count: on stability and utility of self-influence for learning from noisy nlp datasets",
    "citation_count": 1,
    "authors": [
      "Irina Bejan",
      "Artem Sokolov",
      "Katja Filippova"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.626": {
    "title": "Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews",
    "volume": "main",
    "abstract": "Medical systematic reviews play a vital role in healthcare decision making and policy. However, their production is time-consuming, limiting the availability of high-quality and up-to-date evidence summaries. Recent advancements in LLMs offer the potential to automatically generate literature reviews on demand, addressing this issue. However, LLMs sometimes generate inaccurate (and potentially misleading) texts by hallucination or omission. In healthcare, this can make LLMs unusable at best and dangerous at worst. We conducted 16 interviews with international systematic review experts to characterize the perceived utility and risks of LLMs in the specific context of medical evidence reviews. Experts indicated that LLMs can assist in the writing process by drafting summaries, generating templates, distilling information, and crosschecking information. They also raised concerns regarding confidently composed but inaccurate LLM outputs and other potential downstream harms, including decreased accountability and proliferation of low-quality reviews. Informed by this qualitative analysis, we identify criteria for rigorous evaluation of biomedical LLMs aligned with domain expert views",
    "checked": true,
    "id": "784335a19e41dc0cedc5e030cba85b74ba142eff",
    "semantic_title": "appraising the potential uses and harms of llms for medical systematic reviews",
    "citation_count": 1,
    "authors": [
      "Hye Yun",
      "Iain Marshall",
      "Thomas Trikalinos",
      "Byron Wallace"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.627": {
    "title": "PromptST: Abstract Prompt Learning for End-to-End Speech Translation",
    "volume": "main",
    "abstract": "An end-to-end speech-to-text (S2T) translation model is usually initialized from a pre-trained speech recognition encoder and a pre-trained text-to-text (T2T) translation decoder. Although this straightforward setting has been shown empirically successful, there do not exist clear answers to the research questions: 1) how are speech and text modalities fused in S2T model and 2) how to better fuse the two modalities? In this paper, we take the first step toward understanding the fusion of speech and text features in S2T model. We first design and release a 10GB linguistic probing benchmark, namely Speech-Senteval, to investigate the acoustic and linguistic behaviors of S2T models. Preliminary analysis reveals that the uppermost encoder layers of the S2T model can not learn linguistic knowledge efficiently, which is crucial for accurate translation. Based on the finding, we further propose a simple plug-in prompt-learning strategy on the uppermost encoder layers to broaden the abstract representation power of the encoder of S2T models. We call such a prompt-enhanced S2T model PromptST. Experimental results on four widely-used S2T datasets show that PromptST can deliver significant improvements over a strong baseline by capturing richer linguistic knowledge. Benchmarks, code, and scripts are freely available at https://github.com/ytf-philp/PromptST",
    "checked": true,
    "id": "84a3c6e981636e5c6904ced7408b535e89242796",
    "semantic_title": "promptst: abstract prompt learning for end-to-end speech translation",
    "citation_count": 0,
    "authors": [
      "Tengfei Yu",
      "Liang Ding",
      "Xuebo Liu",
      "Kehai Chen",
      "Meishan Zhang",
      "Dacheng Tao",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.628": {
    "title": "Text Rendering Strategies for Pixel Language Models",
    "volume": "main",
    "abstract": "Pixel-based language models process text rendered as images, which allows them to handle any script, making them a promising approach to open vocabulary language modelling. However, recent approaches use text renderers that produce a large set of almost-equivalent input patches, which may prove sub-optimal for downstream tasks, due to redundancy in the input representations. In this paper, we investigate four approaches to rendering text in the PIXEL model (Rust et al., 2023), and find that simple character bigram rendering brings improved performance on sentence-level tasks without compromising performance on token-level or multilingual tasks. This new rendering strategy also makes it possible to train a more compact model with only 22M parameters that performs on par with the original 86M parameter model. Our analyses show that character bigram rendering leads to a consistently better model but with an anisotropic patch embedding space, driven by a patch frequency bias, highlighting the connections between image patch- and tokenization-based language models",
    "checked": true,
    "id": "d921732ca5920049b6a8194f559917689f02d096",
    "semantic_title": "text rendering strategies for pixel language models",
    "citation_count": 1,
    "authors": [
      "Jonas Lotz",
      "Elizabeth Salesky",
      "Phillip Rust",
      "Desmond Elliott"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.629": {
    "title": "APoLLo : Unified Adapter and Prompt Learning for Vision Language Models",
    "volume": "main",
    "abstract": "The choice of input text prompt plays a critical role in the performance of Vision-Language Pretrained (VLP) models such as CLIP. We present APoLLo, a unified multi-modal approach that combines Adapter and Prompt learning for Vision-Language models. Our method is designed to substantially improve the generalization capabilities of VLP models when they are fine-tuned in a few-shot setting. We introduce trainable cross-attention-based adapter layers in conjunction with vision and language encoders to strengthen the alignment between the two modalities. We enforce consistency between the respective encoder branches (receiving augmented inputs) to prevent overfitting in downstream tasks. Our method is evaluated on three representative tasks: generalization to novel classes, cross-dataset evaluation, and unseen domain shifts. In practice, APoLLo achieves a relative gain up to 6.03% over MaPLe (SOTA) on novel classes for 10 diverse image recognition datasets",
    "checked": true,
    "id": "dbe05ab53d0afa900633df965aaaf5bed8bacd86",
    "semantic_title": "apollo : unified adapter and prompt learning for vision language models",
    "citation_count": 0,
    "authors": [
      "Sanjoy Chowdhury",
      "Sayan Nag",
      "Dinesh Manocha"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.630": {
    "title": "SAMRank: Unsupervised Keyphrase Extraction using Self-Attention Map in BERT and GPT-2",
    "volume": "main",
    "abstract": "We propose a novel unsupervised keyphrase extraction approach, called SAMRank, which uses only a self-attention map in a pre-trained language model (PLM) to determine the importance of phrases. Most recent approaches for unsupervised keyphrase extraction mainly utilize contextualized embeddings to capture semantic relevance between words, sentences, and documents. However, due to the anisotropic nature of contextual embeddings, these approaches may not be optimal for semantic similarity measurements. SAMRank as proposed here computes the importance of phrases solely leveraging a self-attention map in a PLM, in this case BERT and GPT-2, eliminating the need to measure embedding similarities. To assess the level of importance, SAMRank combines both global and proportional attention scores through calculations using a self-attention map. We evaluate the SAMRank on three keyphrase extraction datasets: Inspec, SemEval2010, and SemEval2017. The experimental results show that SAMRank outperforms most embedding-based models on both long and short documents and demonstrating that it is possible to use only a self-attention map for keyphrase extraction without relying on embeddings. Source code is available at https://github.com/kangnlp/SAMRank",
    "checked": true,
    "id": "c45dda90e0d6db7a731979575fb0d3d2e49cb541",
    "semantic_title": "samrank: unsupervised keyphrase extraction using self-attention map in bert and gpt-2",
    "citation_count": 0,
    "authors": [
      "Byungha Kang",
      "Youhyun Shin"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.631": {
    "title": "Contrastive Learning for Inference in Dialogue",
    "volume": "main",
    "abstract": "Inference, especially those derived from inductive processes, is a crucial component in our conversation to complement the information implicitly or explicitly conveyed by a speaker. While recent large language models show remarkable advances in inference tasks, their performance in inductive reasoning, where not all information is present in the context, is far behind deductive reasoning. In this paper, we analyze the behavior of the models based on the task difficulty defined by the semantic information gap – which distinguishes inductive and deductive reasoning. Our analysis reveals that the information gap between dialogue contexts and desired inferences renders the inductive inference process more challenging. To mitigate this information gap, we investigate a contrastive learning approach by feeding negative samples. Our experiments suggest negative samples help models understand what is wrong and improve their inference generations",
    "checked": true,
    "id": "99f7bd216214b22c22974142fd558ce554f518b8",
    "semantic_title": "contrastive learning for inference in dialogue",
    "citation_count": 0,
    "authors": [
      "Etsuko Ishii",
      "Yan Xu",
      "Bryan Wilie",
      "Ziwei Ji",
      "Holy Lovenia",
      "Willy Chung",
      "Pascale Fung"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.632": {
    "title": "Editing Large Language Models: Problems, Methods, and Opportunities",
    "volume": "main",
    "abstract": "Despite the ability to train capable LLMs, the methodology for maintaining their relevancy and rectifying errors remains elusive. To this end, the past few years have witnessed a surge in techniques for editing LLMs, the objective of which is to alter the behavior of LLMs efficiently within a specific domain without negatively impacting performance across other inputs. This paper embarks on a deep exploration of the problems, methods, and opportunities related to model editing for LLMs. In particular, we provide an exhaustive overview of the task definition and challenges associated with model editing, along with an in-depth empirical analysis of the most progressive methods currently at our disposal. We also build a new benchmark dataset to facilitate a more robust evaluation and pinpoint enduring issues intrinsic to existing techniques. Our objective is to provide valuable insights into the effectiveness and feasibility of each editing technique, thereby assisting the community in making informed decisions on the selection of the most appropriate method for a specific task or context",
    "checked": true,
    "id": "f5c73d9e6641b018b633690102121f5605d34fb0",
    "semantic_title": "editing large language models: problems, methods, and opportunities",
    "citation_count": 29,
    "authors": [
      "Yunzhi Yao",
      "Peng Wang",
      "Bozhong Tian",
      "Siyuan Cheng",
      "Zhoubo Li",
      "Shumin Deng",
      "Huajun Chen",
      "Ningyu Zhang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.633": {
    "title": "MarkQA: A large scale KBQA dataset with numerical reasoning",
    "volume": "main",
    "abstract": "While question answering over knowledge bases (KBQA) has shown progress in addressing factoid questions, KBQA with numerical reasoning remains relatively unexplored. In this paper, we focus on the complex numerical reasoning in KBQA, and propose a new task, NR-KBQA, which necessitates the ability to perform both multi-hop reasoning and numerical reasoning. We also design a logic form in Python format called PyQL to represent the reasoning process of numerical reasoning questions. To facilitate the development of NR-KBQA, we present a large NR-KBQA dataset called MarkQA, which is automatically constructed by a small set of seeds. Each question in MarkQA is annotated with its corresponding SPARQL query, alongside the step-by-step reasoning path in the QDMR format and PyQL program. Experimental results of some state-of-the-art QA methods performed on the MarkQA dataset show that complex numerical reasoning in KBQA faces great challenges",
    "checked": true,
    "id": "8fc029a322212cc657236c9e733a30ac021b9297",
    "semantic_title": "markqa: a large scale kbqa dataset with numerical reasoning",
    "citation_count": 0,
    "authors": [
      "Xiang Huang",
      "Sitao Cheng",
      "Yuheng Bao",
      "Shanshan Huang",
      "Yuzhong Qu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.634": {
    "title": "Comparing Biases and the Impact of Multilingual Training across Multiple Languages",
    "volume": "main",
    "abstract": "Studies in bias and fairness in natural language processing have primarily examined social biases within a single language and/or across few attributes (e.g. gender, race). However, biases can manifest differently across various languages for individual attributes. As a result, it is critical to examine biases within each language and attribute. Of equal importance is to study how these biases compare across languages and how the biases are affected when training a model on multilingual data versus monolingual data. We present a bias analysis across Italian, Chinese, English, Hebrew, and Spanish on the downstream sentiment analysis task to observe whether specific demographics are viewed more positively. We study bias similarities and differences across these languages and investigate the impact of multilingual vs. monolingual training data. We adapt existing sentiment bias templates in English to Italian, Chinese, Hebrew, and Spanish for four attributes: race, religion, nationality, and gender. Our results reveal similarities in bias expression such as favoritism of groups that are dominant in each language's culture (e.g. majority religions and nationalities). Additionally, we find an increased variation in predictions across protected groups, indicating bias amplification, after multilingual finetuning in comparison to multilingual pretraining",
    "checked": true,
    "id": "b1e67b0cc5705d6aade931e6414ce23dc0ff44b3",
    "semantic_title": "comparing biases and the impact of multilingual training across multiple languages",
    "citation_count": 5,
    "authors": [
      "Sharon Levy",
      "Neha John",
      "Ling Liu",
      "Yogarshi Vyas",
      "Jie Ma",
      "Yoshinari Fujinuma",
      "Miguel Ballesteros",
      "Vittorio Castelli",
      "Dan Roth"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.635": {
    "title": "HutCRS: Hierarchical User-Interest Tracking for Conversational Recommender System",
    "volume": "main",
    "abstract": "Conversational Recommender System (CRS) aims to explicitly acquire user preferences towards items and attributes through natural language conversations. However, existing CRS methods ask users to provide explicit answers (yes/no) for each attribute they require, regardless of users' knowledge or interest, which may significantly reduce the user experience and semantic consistency. Furthermore, these methods assume that users like all attributes of the target item and dislike those unrelated to it, which can introduce bias in attribute-level feedback and impede the system's ability to accurately identify the target item. To address these issues, we propose a more realistic, user-friendly, and explainable CRS framework called Hierarchical User-Interest Tracking for Conversational Recommender System (HutCRS). HutCRS portrays the conversation as a hierarchical interest tree that consists of two stages. In stage I, the system identifies the aspects that the user prefers while the system asks about attributes related to these positive aspects or recommends items in stage II. In addition, we develop a Hierarchical-Interest Policy Learning (HIPL) module to integrate the decision-making process of which aspects to ask and when to ask about attributes or recommend items. Moreover, we classify the attribute-level feedback results to further enhance the system's ability to capture special information, such as attribute instances that are accepted by users but not presented in their historical interactive data. Extensive experiments on four benchmark datasets demonstrate the superiority of our method. The implementation of HutCRS is publicly available at https://github.com/xinle1129/HutCRS",
    "checked": true,
    "id": "183ea8819f4a2b30cd7bc0a0c518723d716e334b",
    "semantic_title": "hutcrs: hierarchical user-interest tracking for conversational recommender system",
    "citation_count": 0,
    "authors": [
      "Mingjie Qian",
      "Yongsen Zheng",
      "Jinghui Qin",
      "Liang Lin"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.636": {
    "title": "Large Language Models Meet Open-World Intent Discovery and Recognition: An Evaluation of ChatGPT",
    "volume": "main",
    "abstract": "The tasks of out-of-domain (OOD) intent discovery and generalized intent discovery (GID) aim to extend a closed intent classifier to open-world intent sets, which is crucial to task-oriented dialogue (TOD) systems. Previous methods address them by fine-tuning discriminative models. Recently, although some studies has been exploring the application of large language models (LLMs) represented by ChatGPT to various downstream tasks, it is still unclear for the ability of ChatGPT to discover and incrementally extent OOD intents. In this paper, we comprehensively evaluate ChatGPT on OOD intent discovery and GID, and then outline the strengths and weaknesses of ChatGPT. Overall, ChatGPT exhibits consistent advantages under zero-shot settings, but is still at a disadvantage compared to fine-tuned models. More deeply, through a series of analytical experiments, we summarize and discuss the challenges faced by LLMs including clustering, domain-specific understanding, and cross-domain in-context learning scenarios. Finally, we provide empirical guidance for future directions to address these challenges",
    "checked": true,
    "id": "6084f6c90b29ed7fce630c3b8a2e0e3a8d0289b1",
    "semantic_title": "large language models meet open-world intent discovery and recognition: an evaluation of chatgpt",
    "citation_count": 0,
    "authors": [
      "Xiaoshuai Song",
      "Keqing He",
      "Pei Wang",
      "Guanting Dong",
      "Yutao Mou",
      "Jingang Wang",
      "Yunsen Xian",
      "Xunliang Cai",
      "Weiran Xu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.637": {
    "title": "The Distributional Hypothesis Does Not Fully Explain the Benefits of Masked Language Model Pretraining",
    "volume": "main",
    "abstract": "We analyze the masked language modeling pretraining objective function from the perspective of the Distributional Hypothesis. We investigate whether the better sample efficiency and the better generalization capability of models pretrained with masked language modeling can be attributed to the semantic similarity encoded in the pretraining data's distributional property. Via a synthetic dataset, our analysis suggests that distributional property indeed leads to the better sample efficiency of pretrained masked language models, but does not fully explain the generalization capability. We also conduct an analysis over two real-world datasets and demonstrate that the distributional property does not explain the generalization ability of pretrained natural language models either. Our results illustrate our limited understanding of model pretraining and provide future research directions",
    "checked": true,
    "id": "c3ba8390e0bfe10d736ba23efce941de8d6ca75c",
    "semantic_title": "the distributional hypothesis does not fully explain the benefits of masked language model pretraining",
    "citation_count": 0,
    "authors": [
      "Ting-Rui Chiang",
      "Dani Yogatama"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.638": {
    "title": "Simple and Effective Input Reformulations for Translation",
    "volume": "main",
    "abstract": "Foundation language models learn from their finetuning input context in different ways. In this paper, we reformulate inputs during finetuning for challenging translation tasks, leveraging model strengths from pretraining in novel ways to improve downstream performance. These reformulations are simple data level modifications, require no additional collection of training data or modification of data at inference time. They can be applied either on single language pair translation tasks or massively multilingual translation tasks. Experiments with these techniques demonstrate significant performance improvements up to 3.5 chrF++ on the Flores200 translation benchmark. We hope our research accessibly improves finetuning data efficiency, enabling more effective training to scalably improve state-of-the-art performance. Our code is released here",
    "checked": true,
    "id": "7b32eeb169e674a851156002035cb51abf79f8d3",
    "semantic_title": "simple and effective input reformulations for translation",
    "citation_count": 0,
    "authors": [
      "Brian Yu",
      "Hansen Lillemark",
      "Kurt Keutzer"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.639": {
    "title": "Pointwise Mutual Information Based Metric and Decoding Strategy for Faithful Generation in Document Grounded Dialogs",
    "volume": "main",
    "abstract": "A major concern in using deep learning based generative models for document-grounded dialogs is the potential generation of responses that are not faithful to the underlying document. Existing automated metrics used for evaluating the faithfulness of response with respect to the grounding document measure the degree of similarity between the generated response and the document's content. However, these automated metrics are far from being well aligned with human judgments. Therefore, to improve the measurement of faithfulness, we propose a new metric that utilizes (Conditional) Point-wise Mutual Information (PMI) between the generated response and the source document, conditioned on the dialogue. PMI quantifies the extent to which the document influences the generated response – with a higher PMI indicating a more faithful response. We build upon this idea to create a new decoding technique that incorporates PMI into the response generation process to predict more faithful responses. Our experiments on the BEGIN benchmark demonstrate an improved correlation of our metric with human evaluation. We also show that our decoding technique is effective in generating more faithful responses when compared to standard decoding techniques on a set of publicly available document-grounded dialog datasets",
    "checked": true,
    "id": "fdb0f24fdf31b703921e4044ea1f55d82074b56f",
    "semantic_title": "pointwise mutual information based metric and decoding strategy for faithful generation in document grounded dialogs",
    "citation_count": 1,
    "authors": [
      "Yatin Nandwani",
      "Vineet Kumar",
      "Dinesh Raghu",
      "Sachindra Joshi",
      "Luis Lastras"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.640": {
    "title": "The ACL OCL Corpus: Advancing Open Science in Computational Linguistics",
    "volume": "main",
    "abstract": "We present ACL OCL, a scholarly corpus derived from the ACL Anthology to assist Open scientific research in the Computational Linguistics domain. Integrating and enhancing the previous versions of the ACL Anthology, the ACL OCL contributes metadata, PDF files, citation graphs and additional structured full texts with sections, figures, and links to a large knowledge resource (Semantic Scholar). The ACL OCL spans seven decades, containing 73K papers, alongside 210K figures. We spotlight how ACL OCL applies to observe trends in computational linguistics. By detecting paper topics with a supervised neural model, we note that interest in \"Syntax: Tagging, Chunking and Parsing\" is waning and \"Natural Language Generation\" is resurging. Our dataset is available from HuggingFace (https://huggingface.co/datasets/WINGNUS/ACL-OCL)",
    "checked": true,
    "id": "d89f6b942c4a8c0b09945462688290484493ed6b",
    "semantic_title": "the acl ocl corpus: advancing open science in computational linguistics",
    "citation_count": 1,
    "authors": [
      "Shaurya Rohatgi",
      "Yanxia Qin",
      "Benjamin Aw",
      "Niranjana Unnithan",
      "Min-Yen Kan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.641": {
    "title": "Using Artificial French Data to Understand the Emergence of Gender Bias in Transformer Language Models",
    "volume": "main",
    "abstract": "Numerous studies have demonstrated the ability of neural language models to learn various linguistic properties without direct supervision. This work takes an initial step towards exploring the less researched topic of how neural models discover linguistic properties of words, such as gender, as well as the rules governing their usage. We propose to use an artificial corpus generated by a PCFG based on French to precisely control the gender distribution in the training data and determine under which conditions a model correctly captures gender information or, on the contrary, appears gender-biased",
    "checked": true,
    "id": "04e1c2e2fae2a7829abd69aebd68ae03173a6c74",
    "semantic_title": "using artificial french data to understand the emergence of gender bias in transformer language models",
    "citation_count": 0,
    "authors": [
      "Lina Conti",
      "Guillaume Wisniewski"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.642": {
    "title": "Learning to Rank Context for Named Entity Recognition Using a Synthetic Dataset",
    "volume": "main",
    "abstract": "While recent pre-trained transformer-based models can perform named entity recognition (NER) with great accuracy, their limited range remains an issue when applied to long documents such as whole novels. To alleviate this issue, a solution is to retrieve relevant context at the document level. Unfortunately, the lack of supervision for such a task means one has to settle for unsupervised approaches. Instead, we propose to generate a synthetic context retrieval training dataset using Alpaca, an instruction-tuned large language model (LLM). Using this dataset, we train a neural context retriever based on a BERT model that is able to find relevant context for NER. We show that our method outperforms several retrieval baselines for the NER task on an English literary dataset composed of the first chapter of 40 books",
    "checked": true,
    "id": "b55dee915809f62d5cf96b2bea58bf5c591c9de7",
    "semantic_title": "learning to rank context for named entity recognition using a synthetic dataset",
    "citation_count": 0,
    "authors": [
      "Arthur Amalvy",
      "Vincent Labatut",
      "Richard Dufour"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.643": {
    "title": "Improving Diversity of Demographic Representation in Large Language Models via Collective-Critiques and Self-Voting",
    "volume": "main",
    "abstract": "A crucial challenge for generative large language models (LLMs) is diversity: when a user's prompt is under-specified, models may follow implicit assumptions while generating a response, which may result in homogenization of the responses, as well as certain demographic groups being under-represented or even erased from the generated responses. In this paper, we formalize the problem diversity of representation in LLM generations. We present evaluation datasets and propose metrics to measure diversity in generated responses along people and culture axes. We find that LLMs understand the notion of diversity, and that they can reason and critique their own responses for that goal. This finding motivated a new prompting technique called collective-critique and self-voting (CCSV) to self-improve people diversity of LLMs by tapping into its diversity reasoning capabilities, without relying on handcrafted examples or prompt tuning. Extensive empirical experiments with both human and automated evaluations show that our proposed approach is effective at improving people and culture diversity, and outperforms all baseline methods by a large margin",
    "checked": true,
    "id": "9c893f54d86a362b8e62e5883bb38c14240441f5",
    "semantic_title": "improving diversity of demographic representation in large language models via collective-critiques and self-voting",
    "citation_count": 2,
    "authors": [
      "Preethi Lahoti",
      "Nicholas Blumm",
      "Xiao Ma",
      "Raghavendra Kotikalapudi",
      "Sahitya Potluri",
      "Qijun Tan",
      "Hansa Srinivasan",
      "Ben Packer",
      "Ahmad Beirami",
      "Alex Beutel",
      "Jilin Chen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.644": {
    "title": "Hidding the Ghostwriters: An Adversarial Evaluation of AI-Generated Student Essay Detection",
    "volume": "main",
    "abstract": "Large language models (LLMs) have exhibited remarkable capabilities in text generation tasks. However, the utilization of these models carries inherent risks, including but not limited to plagiarism, the dissemination of fake news, and issues in educational exercises. Although several detectors have been proposed to address these concerns, their effectiveness against adversarial perturbations, specifically in the context of student essay writing, remains largely unexplored. This paper aims to bridge this gap by constructing AIG-ASAP, an AI-generated student essay dataset, employing a range of text perturbation methods that are expected to generate high-quality essays while evading detection. Through empirical experiments, we assess the performance of current AIGC detectors on the AIG-ASAP dataset. The results reveal that the existing detectors can be easily circumvented using straightforward automatic adversarial attacks. Specifically, we explore word substitution and sentence substitution perturbation methods that effectively evade detection while maintaining the quality of the generated essays. This highlights the urgent need for more accurate and robust methods to detect AI-generated student essays in the education domain. Code and data are released for public use",
    "checked": true,
    "id": "e89f3bd77fffc6b167b5cadea373a844eab8df38",
    "semantic_title": "hidding the ghostwriters: an adversarial evaluation of ai-generated student essay detection",
    "citation_count": 0,
    "authors": [
      "Xinlin Peng",
      "Ying Zhou",
      "Ben He",
      "Le Sun",
      "Yingfei Sun"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.645": {
    "title": "Contextual Interaction for Argument Post Quality Assessment",
    "volume": "main",
    "abstract": "Recently, there has been an increased emphasis on assessing the quality of natural language arguments. Existing approaches primarily focus on evaluating the quality of individual argument posts. However, they often fall short when it comes to effectively distinguishing arguments that possess a narrow quality margin. To address this limitation, this paper delves into two alternative methods for modeling the relative quality of different arguments. These approaches include: 1) Supervised contrastive learning that captures the intricate interactions between arguments. By incorporating this approach, we aim to enhance the assessment of argument quality by effectively distinguishing between arguments with subtle differences in quality. 2) Large language models (LLMs) with in-context examples that harness the power of LLMs and enrich them with in-context examples. Through extensive evaluation and analysis on the publicly available IBM-Rank-30k dataset, we demonstrate the superiority of our contrastive argument quality assessment approach over state-of-the-art baselines. On the other hand, while LLMs with in-context examples showcase a commendable ability to identify high-quality argument posts, they exhibit relatively limited efficacy in discerning between argument posts with a narrow quality gap",
    "checked": true,
    "id": "f071ed06a721a5445d128044318e69763916228a",
    "semantic_title": "contextual interaction for argument post quality assessment",
    "citation_count": 0,
    "authors": [
      "Yiran Wang",
      "Xuanang Chen",
      "Ben He",
      "Le Sun"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.646": {
    "title": "Pre-training Intent-Aware Encoders for Zero- and Few-Shot Intent Classification",
    "volume": "main",
    "abstract": "Intent classification (IC) plays an important role in task-oriented dialogue systems. However, IC models often generalize poorly when training without sufficient annotated examples for each user intent. We propose a novel pre-training method for text encoders that uses contrastive learning with intent psuedo-labels to produce embeddings that are well-suited for IC tasks, reducing the need for manual annotations. By applying this pre-training strategy, we also introduce Pre-trained Intent-aware Encoder (PIE), which is designed to align encodings of utterances with their intent names. Specifically, we first train a tagger to identify key phrases within utterances that are crucial for interpreting intents. We then use these extracted phrases to create examples for pre-training a text encoder in a contrastive manner. As a result, our PIE model achieves up to 5.4% and 4.0% higher accuracy than the previous state-of-the-art pre-trained text encoder for the N-way zero- and one-shot settings on four IC datasets",
    "checked": true,
    "id": "2a7813f1ab7394beaf5af65db95c1c3ee0f4b414",
    "semantic_title": "pre-training intent-aware encoders for zero- and few-shot intent classification",
    "citation_count": 0,
    "authors": [
      "Mujeen Sung",
      "James Gung",
      "Elman Mansimov",
      "Nikolaos Pappas",
      "Raphael Shu",
      "Salvatore Romeo",
      "Yi Zhang",
      "Vittorio Castelli"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.647": {
    "title": "Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations",
    "volume": "main",
    "abstract": "The collection and curation of high-quality training data is crucial for developing text classification models with superior performance, but it is often associated with significant costs and time investment. Researchers have recently explored using large language models (LLMs) to generate synthetic datasets as an alternative approach. However, the effectiveness of the LLM-generated synthetic data in supporting model training is inconsistent across different classification tasks. To better understand factors that moderate the effectiveness of the LLM-generated synthetic data, in this study, we look into how the performance of models trained on these synthetic data may vary with the subjectivity of classification. Our results indicate that subjectivity, at both the task level and instance level, is negatively associated with the performance of the model trained on synthetic data. We conclude by discussing the implications of our work on the potential and limitations of leveraging LLM for synthetic data generation",
    "checked": true,
    "id": "29f07e73b7aaa7e9e950c59710472c62316be74a",
    "semantic_title": "synthetic data generation with large language models for text classification: potential and limitations",
    "citation_count": 0,
    "authors": [
      "Zhuoyan Li",
      "Hangxiao Zhu",
      "Zhuoran Lu",
      "Ming Yin"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.648": {
    "title": "GazeVQA: A Video Question Answering Dataset for Multiview Eye-Gaze Task-Oriented Collaborations",
    "volume": "main",
    "abstract": "The usage of exocentric and egocentric videos in Video Question Answering (VQA) is a new endeavor in human-robot interaction and collaboration studies. Particularly for egocentric videos, one may leverage eye-gaze information to understand human intentions during the task. In this paper, we build a novel task-oriented VQA dataset, called GazeVQA, for collaborative tasks where gaze information is captured during the task process. GazeVQA is designed with a novel QA format that covers thirteen different reasoning types to capture multiple aspects of task information and user intent. For each participant, GazeVQA consists of more than 1,100 textual questions and more than 500 labeled images that were annotated with the assistance of the Segment Anything Model. In total, 2,967 video clips, 12,491 labeled images, and 25,040 questions from 22 participants were included in the dataset. Additionally, inspired by the assisting models and common ground theory for industrial task collaboration, we propose a new AI model called AssistGaze that is designed to answer the questions with three different answer types, namely textual, image, and video. AssistGaze can effectively ground the perceptual input into semantic information while reducing ambiguities. We conduct comprehensive experiments to demonstrate the challenges of GazeVQA and the effectiveness of AssistGaze",
    "checked": true,
    "id": "0c5859fbd6c0f3188cd1d264afe2296a54112b8d",
    "semantic_title": "gazevqa: a video question answering dataset for multiview eye-gaze task-oriented collaborations",
    "citation_count": 0,
    "authors": [
      "Muhammet Ilaslan",
      "Chenan Song",
      "Joya Chen",
      "Difei Gao",
      "Weixian Lei",
      "Qianli Xu",
      "Joo Lim",
      "Mike Shou"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.649": {
    "title": "People Make Better Edits: Measuring the Efficacy of LLM-Generated Counterfactually Augmented Data for Harmful Language Detection",
    "volume": "main",
    "abstract": "NLP models are used in a variety of critical social computing tasks, such as detecting sexist, racist, or otherwise hateful content. Therefore, it is imperative that these models are robust to spurious features. Past work has attempted to tackle such spurious features using training data augmentation, including Counterfactually Augmented Data (CADs). CADs introduce minimal changes to existing training data points and flip their labels; training on them may reduce model dependency on spurious features. However, manually generating CADs can be time-consuming and expensive. Hence in this work, we assess if this task can be automated using generative NLP models. We automatically generate CADs using Polyjuice, ChatGPT, and Flan-T5, and evaluate their usefulness in improving model robustness compared to manually-generated CADs. By testing both model performance on multiple out-of-domain test sets and individual data point efficacy, our results show that while manual CADs are still the most effective, CADs generated by ChatGPT come a close second. One key reason for the lower performance of automated methods is that the changes they introduce are often insufficient to flip the original label",
    "checked": true,
    "id": "578887182fcf49cfd4d99fa3b1de200e8ebb2c45",
    "semantic_title": "people make better edits: measuring the efficacy of llm-generated counterfactually augmented data for harmful language detection",
    "citation_count": 1,
    "authors": [
      "Indira Sen",
      "Dennis Assenmacher",
      "Mattia Samory",
      "Isabelle Augenstein",
      "Wil Aalst",
      "Claudia Wagner"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.650": {
    "title": "Unraveling Feature Extraction Mechanisms in Neural Networks",
    "volume": "main",
    "abstract": "The underlying mechanism of neural networks in capturing precise knowledge has been the subject of consistent research efforts. In this work, we propose a theoretical approach based on Neural Tangent Kernels (NTKs) to investigate such mechanisms. Specifically, considering the infinite network width, we hypothesize the learning dynamics of target models may intuitively unravel the features they acquire from training data, deepening our insights into their internal mechanisms. We apply our approach to several fundamental models and reveal how these models leverage statistical features during gradient descent and how they are integrated into final decisions. We also discovered that the choice of activation function can affect feature extraction. For instance, the use of the ReLU activation function could potentially introduce a bias in features, providing a plausible explanation for its replacement with alternative functions in recent pre-trained language models. Additionally, we find that while self-attention and CNN models may exhibit limitations in learning n-grams, multiplication-based models seem to excel in this area. We verify these theoretical findings through experiments and find that they can be applied to analyze language modeling tasks, which can be regarded as a special variant of classification. Our work may offer insights into the roles and capacities of fundamental modules within deep neural networks including large language models",
    "checked": true,
    "id": "1fbabc724cac5e500ca8328de39fa42a1dc58783",
    "semantic_title": "unraveling feature extraction mechanisms in neural networks",
    "citation_count": 0,
    "authors": [
      "Xiaobing Sun",
      "Jiaxi Li",
      "Wei Lu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.651": {
    "title": "CAPSTONE: Curriculum Sampling for Dense Retrieval with Document Expansion",
    "volume": "main",
    "abstract": "The dual-encoder has become the de facto architecture for dense retrieval. Typically, it computes the latent representations of the query and document independently, thus failing to fully capture the interactions between the query and document. To alleviate this, recent research has focused on obtaining query-informed document representations. During training, it expands the document with a real query, but during inference, it replaces the real query with a generated one. This inconsistency between training and inference causes the dense retrieval model to prioritize query information while disregarding the document when computing the document representation. Consequently, it performs even worse than the vanilla dense retrieval model because its performance heavily relies on the relevance between the generated queries and the real query. In this paper, we propose a curriculum sampling strategy that utilizes pseudo queries during training and progressively enhances the relevance between the generated query and the real query. By doing so, the retrieval model learns to extend its attention from the document alone to both the document and query, resulting in high-quality query-informed document representations. Experimental results on both in-domain and out-of-domain datasets demonstrate that our approach outperforms previous dense retrieval models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingwei He",
      "Yeyun Gong",
      "A-Long Jin",
      "Hang Zhang",
      "Anlei Dong",
      "Jian Jiao",
      "Siu Yiu",
      "Nan Duan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.652": {
    "title": "Balance Act: Mitigating Hubness in Cross-Modal Retrieval with Query and Gallery Banks",
    "volume": "main",
    "abstract": "In this work, we present a post-processing solution to address the hubness problem in cross-modal retrieval, a phenomenon where a small number of gallery data points are frequently retrieved, resulting in a decline in retrieval performance. We first theoretically demonstrate the necessity of incorporating both the gallery and query data for addressing hubness as hubs always exhibit high similarity with gallery and query data. Second, building on our theoretical results, we propose a novel framework, Dual Bank Normalization (DBNorm). While previous work has attempted to alleviate hubness by only utilizing the query samples, DBNorm leverages two banks constructed from the query and gallery samples to reduce the occurrence of hubs during inference. Next, to complement DBNorm, we introduce two novel methods, dual inverted softmax and dual dynamic inverted softmax, for normalizing similarity based on the two banks. Specifically, our proposed methods reduce the similarity between hubs and queries while improving the similarity between non-hubs and queries. Finally, we present extensive experimental results on diverse language-grounded benchmarks, including text-image, text-video, and text-audio, demonstrating the superior performance of our approaches compared to previous methods in addressing hubness and boosting retrieval performance",
    "checked": true,
    "id": "66a47abcc4a954fd35a610dc3d0760178a9ca10b",
    "semantic_title": "balance act: mitigating hubness in cross-modal retrieval with query and gallery banks",
    "citation_count": 1,
    "authors": [
      "Yimu Wang",
      "Xiangru Jian",
      "Bo Xue"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.653": {
    "title": "E-CORE: Emotion Correlation Enhanced Empathetic Dialogue Generation",
    "volume": "main",
    "abstract": "Achieving empathy is a crucial step toward humanized dialogue systems. Current approaches for empathetic dialogue generation mainly perceive an emotional label to generate an empathetic response conditioned on it, which simply treat emotions independently, but ignore the intrinsic emotion correlation in dialogues, resulting in inaccurate emotion perception and unsuitable response generation. In this paper, we propose a novel emotion correlation enhanced empathetic dialogue generation framework, which comprehensively realizes emotion correlation learning, utilization, and supervising. Specifically, a multi-resolution emotion graph is devised to capture context-based emotion interactions from different resolutions, further modeling emotion correlation. Then we propose an emotion correlation enhanced decoder, with a novel correlation-aware aggregation and soft/hard strategy, respectively improving the emotion perception and response generation. Experimental results on the benchmark dataset demonstrate the superiority of our model in both empathetic perception and expression",
    "checked": true,
    "id": "77f1b4d7186eaf1a54975f31f89cd1b26ea3c6e8",
    "semantic_title": "e-core: emotion correlation enhanced empathetic dialogue generation",
    "citation_count": 0,
    "authors": [
      "Fengyi Fu",
      "Lei Zhang",
      "Quan Wang",
      "Zhendong Mao"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.654": {
    "title": "What do Deck Chairs and Sun Hats Have in Common? Uncovering Shared Properties in Large Concept Vocabularies",
    "volume": "main",
    "abstract": "Concepts play a central role in many applications. This includes settings where concepts have to be modelled in the absence of sentence context. Previous work has therefore focused on distilling decontextualised concept embeddings from language models. But concepts can be modelled from different perspectives, whereas concept embeddings typically mostly capture taxonomic structure. To address this issue, we propose a strategy for identifying what different concepts, from a potentially large concept vocabulary, have in common with others. We then represent concepts in terms of the properties they share with the other concepts. To demonstrate the practical usefulness of this way of modelling concepts, we consider the task of ultra-fine entity typing, which is a challenging multi-label classification problem. We show that by augmenting the label set with shared properties, we can improve the performance of the state-of-the-art models for this task",
    "checked": true,
    "id": "46713ae2c00846f6d5d3da13027efecfe339d38e",
    "semantic_title": "what do deck chairs and sun hats have in common? uncovering shared properties in large concept vocabularies",
    "citation_count": 0,
    "authors": [
      "Amit Gajbhiye",
      "Zied Bouraoui",
      "Na Li",
      "Usashi Chatterjee",
      "Luis Espinosa-Anke",
      "Steven Schockaert"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.655": {
    "title": "ALDi: Quantifying the Arabic Level of Dialectness of Text",
    "volume": "main",
    "abstract": "Transcribed speech and user-generated text in Arabic typically contain a mixture of Modern Standard Arabic (MSA), the standardized language taught in schools, and Dialectal Arabic (DA), used in daily communications. To handle this variation, previous work in Arabic NLP has focused on Dialect Identification (DI) on the sentence or the token level. However, DI treats the task as binary, whereas we argue that Arabic speakers perceive a spectrum of dialectness, which we operationalize at the sentence level as the Arabic Level of Dialectness (ALDi), a continuous linguistic variable. We introduce the AOC-ALDi dataset (derived from the AOC dataset), containing 127,835 sentences (17% from news articles and 83% from user comments on those articles) which are manually labeled with their level of dialectness. We provide a detailed analysis of AOC-ALDi and show that a model trained on it can effectively identify levels of dialectness on a range of other corpora (including dialects and genres not included in AOC-ALDi), providing a more nuanced picture than traditional DI systems. Through case studies, we illustrate how ALDi can reveal Arabic speakers' stylistic choices in different situations, a useful property for sociolinguistic analyses",
    "checked": true,
    "id": "a69e4615779caa6c33e61d0d92becba45205dcad",
    "semantic_title": "aldi: quantifying the arabic level of dialectness of text",
    "citation_count": 1,
    "authors": [
      "Amr Keleg",
      "Sharon Goldwater",
      "Walid Magdy"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.656": {
    "title": "3DRP-Net: 3D Relative Position-aware Network for 3D Visual Grounding",
    "volume": "main",
    "abstract": "3D visual grounding aims to localize the target object in a 3D point cloud by a free-form language description. Typically, the sentences describing the target object tend to provide information about its relative relation between other objects and its position within the whole scene. In this work, we propose a relation-aware one-stage framework, named 3D Relative Position-aware Network (3DRP-Net), which can effectively capture the relative spatial relationships between objects and enhance object attributes. Specifically, 1) we propose a 3D Relative Position Multi-head Attention (3DRP-MA) module to analyze relative relations from different directions in the context of object pairs, which helps the model to focus on the specific object relations mentioned in the sentence. 2) We designed a soft-labeling strategy to alleviate the spatial ambiguity caused by redundant points, which further stabilizes and enhances the learning process through a constant and discriminative distribution. Extensive experiments conducted on three benchmarks (i.e., ScanRefer and Nr3D/Sr3D) demonstrate that our method outperforms all the state-of-the-art methods in general",
    "checked": true,
    "id": "e54c859515b68c5c7a56f66382b9e9605953df0b",
    "semantic_title": "3drp-net: 3d relative position-aware network for 3d visual grounding",
    "citation_count": 3,
    "authors": [
      "Zehan Wang",
      "Haifeng Huang",
      "Yang Zhao",
      "Linjun Li",
      "Xize Cheng",
      "Yichen Zhu",
      "Aoxiong Yin",
      "Zhou Zhao"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.657": {
    "title": "Goal-Driven Explainable Clustering via Language Descriptions",
    "volume": "main",
    "abstract": "Unsupervised clustering is widely used to explore large corpora, but existing formulations neither consider the users' goals nor explain clusters' meanings. We propose a new task formulation, \"Goal-Driven Clustering with Explanations\" (GoalEx), which represents both the goal and the explanations as free-form language descriptions. For example, to categorize the errors made by a summarization system, the input to GoalEx is a corpus of annotator-written comments for system-generated summaries and a goal description \"cluster the comments based on why the annotators think the summary is imperfect.\"; the outputs are text clusters each with an explanation (\"this cluster mentions that the summary misses important context information.\"), which relates to the goal and accurately explains which comments should (not) belong to a cluster. To tackle GoalEx, we prompt a language model with \"[corpus subset] + [goal] + Brainstorm a list of explanations each representing a cluster.\"; then we classify whether each sample belongs to a cluster based on its explanation; finally, we use integer linear programming to select a subset of candidate clusters to cover most samples while minimizing overlaps. Under both automatic and human evaluation on corpora with or without labels, our method produces more accurate and goal-related explanations than prior methods",
    "checked": true,
    "id": "9f83468092ccbe60deeff8e7d82e40a81f3ae8cf",
    "semantic_title": "goal-driven explainable clustering via language descriptions",
    "citation_count": 4,
    "authors": [
      "Zihan Wang",
      "Jingbo Shang",
      "Ruiqi Zhong"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.658": {
    "title": "Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models",
    "volume": "main",
    "abstract": "Multilingual large-scale Pretrained Language Models (PLMs) have been shown to store considerable amounts of factual knowledge, but large variations are observed across languages. With the ultimate goal of ensuring that users with different language backgrounds obtain consistent feedback from the same model, we study the cross-lingual consistency (CLC) of factual knowledge in various multilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy. Using this metric, we conduct an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level. Among other results, we find that increasing model size leads to higher factual probing accuracy in most languages, but does not improve cross-lingual consistency. Finally, we conduct a case study on CLC when new factual associations are inserted in the PLMs via model editing. Results on a small sample of facts inserted in English reveal a clear pattern whereby the new piece of knowledge transfers only to languages with which English has a high RankC score. All code and data are released at https://github.com/Betswish/Cross-Lingual-Consistency",
    "checked": true,
    "id": "468d1e2d75a23fecaf96fe65d8b01ff35ea5d0bd",
    "semantic_title": "cross-lingual consistency of factual knowledge in multilingual language models",
    "citation_count": 0,
    "authors": [
      "Jirui Qi",
      "Raquel Fernández",
      "Arianna Bisazza"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.659": {
    "title": "Learning from Mistakes via Cooperative Study Assistant for Large Language Models",
    "volume": "main",
    "abstract": "Large language models (LLMs) have demonstrated their potential to refine their generation based on their own feedback. However, the feedback from LLM itself is often inaccurate, thereby limiting its benefits. In this paper, we propose Study Assistant for Large LAnguage Model (SALAM), a novel framework with an auxiliary agent to assist the main LLM in learning from mistakes through interactive cooperation. In the gathering phase, the student assistant agent probes the main LLM, analyzes its errors, and collects the interaction in a mistake memory. During the examination phase, the study assistant provides guidelines by retrieving relevant cases to help the main LLM anticipate and avoid similar errors. We first investigate the effectiveness of a general study assistant and then customize it to provide LLM-specific guidance through imitation learning from successful guidance experiences. Our experiments on three LLMs using two challenging frameworks demonstrate that SALAM can significantly boost LLMs by an accuracy margin of up to 6.6 on BBH and 12.6 on BBQ",
    "checked": true,
    "id": "159b9c65260beea062cebdc023d9e802869768f3",
    "semantic_title": "learning from mistakes via cooperative study assistant for large language models",
    "citation_count": 0,
    "authors": [
      "Danqing Wang",
      "Lei Li"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.660": {
    "title": "Bridging the Digital Divide: Performance Variation across Socio-Economic Factors in Vision-Language Models",
    "volume": "main",
    "abstract": "Despite the impressive performance of current AI models reported across various tasks, performance reports often do not include evaluations of how these models perform on the specific groups that will be impacted by these technologies. Among the minority groups under-represented in AI, data from low-income households are often overlooked in data collection and model evaluation. We evaluate the performance of a state-of-the-art vision-language model (CLIP) on a geo-diverse dataset containing household images associated with different income values (DollarStreet) and show that performance inequality exists among households of different income levels. Our results indicate that performance for the poorer groups is consistently lower than the wealthier groups across various topics and countries. We highlight insights that can help mitigate these issues and propose actionable steps for economic-level inclusive AI development",
    "checked": true,
    "id": "4267474e3a7a9f202562de8fb9f365ec7e944c55",
    "semantic_title": "bridging the digital divide: performance variation across socio-economic factors in vision-language models",
    "citation_count": 0,
    "authors": [
      "Joan Nwatu",
      "Oana Ignat",
      "Rada Mihalcea"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.661": {
    "title": "Conceptor-Aided Debiasing of Large Language Models",
    "volume": "main",
    "abstract": "Pre-trained large language models (LLMs) reflect the inherent social biases of their training corpus. Many methods have been proposed to mitigate this issue, but they often fail to debias or they sacrifice model accuracy. We use *conceptors*–a soft projection method–to identify and remove the bias subspace in LLMs such as BERT and GPT. We propose two methods of applying conceptors (1) bias subspace projection by post-processing by the conceptor NOT operation; and (2) a new architecture, conceptor-intervened BERT (CI-BERT), which explicitly incorporates the conceptor projection into all layers during training. We find that conceptor post-processing achieves state-of-the-art (SoTA) debiasing results while maintaining LLMs' performance on the GLUE benchmark. Further, it is robust in various scenarios and can mitigate intersectional bias efficiently by its AND operation on the existing bias subspaces. Although CI-BERT's training takes all layers' bias into account and can beat its post-processing counterpart in bias mitigation, CI-BERT reduces the language model accuracy. We also show the importance of carefully constructing the bias subspace. The best results are obtained by removing outliers from the list of biased words, combining them (via the OR operation), and computing their embeddings using the sentences from a cleaner corpus",
    "checked": true,
    "id": "24ef0308b88952c13057dac4054b73dcf73e6288",
    "semantic_title": "conceptor-aided debiasing of large language models",
    "citation_count": 0,
    "authors": [
      "Li Yifei",
      "Lyle Ungar",
      "João Sedoc"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.662": {
    "title": "AMR Parsing is Far from Solved: GrAPES, the Granular AMR Parsing Evaluation Suite",
    "volume": "main",
    "abstract": "We present the Granular AMR Parsing Evaluation Suite (GrAPES), a challenge set for Abstract Meaning Representation (AMR) parsing with accompanying evaluation metrics. AMR parsers now obtain high scores on the standard AMR evaluation metric Smatch, close to or even above reported inter-annotator agreement. But that does not mean that AMR parsing is solved; in fact, human evaluation in previous work indicates that current parsers still quite frequently make errors on node labels or graph structure that substantially distort sentence meaning. Here, we provide an evaluation suite that tests AMR parsers on a range of phenomena of practical, technical, and linguistic interest. Our 36 categories range from seen and unseen labels, to structural generalization, to coreference. GrAPES reveals in depth the abilities and shortcomings of current AMR parsers",
    "checked": true,
    "id": "0a0a74ef80fa96b930a98da5c002df05e5f5ea5e",
    "semantic_title": "amr parsing is far from solved: grapes, the granular amr parsing evaluation suite",
    "citation_count": 0,
    "authors": [
      "Jonas Groschwitz",
      "Shay Cohen",
      "Lucia Donatelli",
      "Meaghan Fowlie"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.663": {
    "title": "Rethinking and Improving Multi-task Learning for End-to-end Speech Translation",
    "volume": "main",
    "abstract": "Significant improvements in end-to-end speech translation (ST) have been achieved through the application of multi-task learning. However, the extent to which auxiliary tasks are highly consistent with the ST task, and how much this approach truly helps, have not been thoroughly studied. In this paper, we investigate the consistency between different tasks, considering different times and modules. We find that the textual encoder primarily facilitates cross-modal conversion, but the presence of noise in speech impedes the consistency between text and speech representations. Furthermore, we propose an improved multi-task learning (IMTL) approach for the ST task, which bridges the modal gap by mitigating the difference in length and representation. We conduct experiments on the MuST-C dataset. The results demonstrate that our method attains state-of-the-art results. Moreover, when additional data is used, we achieve the new SOTA result on MuST-C English to Spanish task with 20.8% of the training time required by the current SOTA method",
    "checked": true,
    "id": "5422f8591b4bebcf623eb60cf43a112c3d3fcbdb",
    "semantic_title": "rethinking and improving multi-task learning for end-to-end speech translation",
    "citation_count": 0,
    "authors": [
      "Yuhao Zhang",
      "Chen Xu",
      "Bei Li",
      "Hao Chen",
      "Tong Xiao",
      "Chunliang Zhang",
      "Jingbo Zhu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.664": {
    "title": "AD-NLP: A Benchmark for Anomaly Detection in Natural Language Processing",
    "volume": "main",
    "abstract": "Deep learning models have reignited the interest in Anomaly Detection research in recent years. Methods for Anomaly Detection in text have shown strong empirical results on ad-hoc anomaly setups that are usually made by downsampling some classes of a labeled dataset. This can lead to reproducibility issues and models that are biased toward detecting particular anomalies while failing to recognize them in more sophisticated scenarios. In the present work, we provide a unified benchmark for detecting various types of anomalies, focusing on problems that can be naturally formulated as Anomaly Detection in text, ranging from syntax to stylistics. In this way, we are hoping to facilitate research in Text Anomaly Detection. We also evaluate and analyze two strong shallow baselines, as well as two of the current state-of-the-art neural approaches, providing insights into the knowledge the neural models are learning when performing the anomaly detection task. We provide the code for evaluation, downloading, and preprocessing the dataset at https://github.com/mateibejan1/ad-nlp/",
    "checked": true,
    "id": "428f0dafd33eb083eb77c6b002aaad9e8ad468f8",
    "semantic_title": "ad-nlp: a benchmark for anomaly detection in natural language processing",
    "citation_count": 0,
    "authors": [
      "Matei Bejan",
      "Andrei Manolache",
      "Marius Popescu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.665": {
    "title": "Enhancing the Ranking Context of Dense Retrieval through Reciprocal Nearest Neighbors",
    "volume": "main",
    "abstract": "Sparse annotation poses persistent challenges to training dense retrieval models; for example, it distorts the training signal when unlabeled relevant documents are used spuriously as negatives in contrastive learning. To alleviate this problem, we introduce evidence-based label smoothing, a novel, computationally efficient method that prevents penalizing the model for assigning high relevance to false negatives. To compute the target relevance distribution over candidate documents within the ranking context of a given query, we assign a non-zero relevance probability to those candidates most similar to the ground truth based on the degree of their similarity to the ground-truth document(s). To estimate relevance we leverage an improved similarity metric based on reciprocal nearest neighbors, which can also be used independently to rerank candidates in post-processing. Through extensive experiments on two large-scale ad hoc text retrieval datasets, we demonstrate that reciprocal nearest neighbors can improve the ranking effectiveness of dense retrieval models, both when used for label smoothing, as well as for reranking. This indicates that by considering relationships between documents and queries beyond simple geometric distance we can effectively enhance the ranking context",
    "checked": true,
    "id": "331d5e29442cd28618921f8db5264099a15ba686",
    "semantic_title": "enhancing the ranking context of dense retrieval through reciprocal nearest neighbors",
    "citation_count": 0,
    "authors": [
      "George Zerveas",
      "Navid Rekabsaz",
      "Carsten Eickhoff"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.666": {
    "title": "Cross-Lingual Cross-Target Stance Detection with Dual Knowledge Distillation Framework",
    "volume": "main",
    "abstract": "Stance detection aims to identify the user's attitude toward specific targets from text, which is an important research area in text mining and benefits a variety of application domains. Existing studies on stance detection were conducted mainly in English. Due to the low-resource problem in most non-English languages, cross-lingual stance detection was proposed to transfer knowledge from high-resource (source) language to low-resource (target) language. However, previous research has ignored the practical issue of no labeled training data available in target language. Moreover, target inconsistency in cross-lingual stance detection brings about the additional issue of unseen targets in target language, which in essence requires the transfer of both language and target-oriented knowledge from source to target language. To tackle these challenging issues, in this paper, we propose the new task of cross-lingual cross-target stance detection and develop the first computational work with dual knowledge distillation. Our proposed framework designs a cross-lingual teacher and a cross-target teacher using the source language data and a dual distillation process that transfers the two types of knowledge to target language. To bridge the target discrepancy between languages, cross-target teacher mines target category information and generalizes it to the unseen targets in target language via category-oriented learning. Experimental results on multilingual stance datasets demonstrate the effectiveness of our method compared to the competitive baselines",
    "checked": true,
    "id": "1d5696d0aca3513557fa84aac5aefa9162bf84fb",
    "semantic_title": "cross-lingual cross-target stance detection with dual knowledge distillation framework",
    "citation_count": 0,
    "authors": [
      "Ruike Zhang",
      "Hanxuan Yang",
      "Wenji Mao"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.667": {
    "title": "PRESTO: A Multilingual Dataset for Parsing Realistic Task-Oriented Dialogs",
    "volume": "main",
    "abstract": "Research interest in task-oriented dialogs has increased as systems such as Google Assistant, Alexa and Siri have become ubiquitous in everyday life. However, the impact of academic research in this area has been limited by the lack of datasets that realistically capture the wide array of user pain points. To enable research on some of the more challenging aspects of parsing realistic conversations, we introduce PRESTO, a public dataset of over 550K contextual multilingual conversations between humans and virtual assistants. PRESTO contains a diverse array of challenges that occur in real-world NLU tasks such as disfluencies, code-switching, and revisions. It is the only large scale human generated conversational parsing dataset that provides structured context such as a user's contacts and lists for each example. Our mT5 model based baselines demonstrate that the conversational phenomenon present in PRESTO are challenging to model, which is further pronounced in a low-resource setup",
    "checked": true,
    "id": "dc691448f9e1dc233692d886cc5d32fdd256c9c7",
    "semantic_title": "presto: a multilingual dataset for parsing realistic task-oriented dialogs",
    "citation_count": 6,
    "authors": [
      "Rahul Goel",
      "Waleed Ammar",
      "Aditya Gupta",
      "Siddharth Vashishtha",
      "Motoki Sano",
      "Faiz Surani",
      "Max Chang",
      "HyunJeong Choe",
      "David Greene",
      "Chuan He",
      "Rattima Nitisaroj",
      "Anna Trukhina",
      "Shachi Paul",
      "Pararth Shah",
      "Rushin Shah",
      "Zhou Yu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.668": {
    "title": "An Iteratively Parallel Generation Method with the Pre-Filling Strategy for Document-level Event Extraction",
    "volume": "main",
    "abstract": "In document-level event extraction (DEE) tasks, a document typically contains many event records with multiple event roles. Therefore, accurately extracting all event records is a big challenge since the number of event records is not given. Previous works present the entity-based directed acyclic graph (EDAG) generation methods to autoregressively generate event roles, which requires a given generation order. Meanwhile, parallel methods are proposed to generate all event roles simultaneously, but suffer from the inadequate training which manifests zero accuracies on some event roles. In this paper, we propose an Iteratively Parallel Generation method with the Pre-Filling strategy (IPGPF). Event roles in an event record are generated in parallel to avoid order selection, and the event records are iteratively generated to utilize historical results. Experiments on two public datasets show our IPGPF improves 11.7 F1 than previous parallel models and up to 5.1 F1 than auto-regressive models under the control variable settings. Moreover, our enhanced IPGPF outperforms other entity-enhanced models and achieves new state-of-the-art performance",
    "checked": true,
    "id": "6f722d0e769c59d28098328e6e28040b5cea35ac",
    "semantic_title": "an iteratively parallel generation method with the pre-filling strategy for document-level event extraction",
    "citation_count": 0,
    "authors": [
      "Guanhua Huang",
      "Runxin Xu",
      "Ying Zeng",
      "Jiaze Chen",
      "Zhouwang Yang",
      "Weinan E"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.669": {
    "title": "CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations",
    "volume": "main",
    "abstract": "Recent work has aimed to capture nuances of human behavior by using LLMs to simulate responses from particular demographics in settings like social science experiments and public opinion surveys. However, there are currently no established ways to discuss or evaluate the quality of such LLM simulations. Moreover, there is growing concern that these LLM simulations are flattened caricatures of the personas that they aim to simulate, failing to capture the multidimensionality of people and perpetuating stereotypes. To bridge these gaps, we present CoMPosT, a framework to characterize LLM simulations using four dimensions: Context, Model, Persona, and Topic. We use this framework to measure open-ended LLM simulations' susceptibility to caricature, defined via two criteria: individuation and exaggeration. We evaluate the level of caricature in scenarios from existing work on LLM simulations. We find that for GPT-4, simulations of certain demographics (political and marginalized groups) and topics (general, uncontroversial) are highly susceptible to caricature",
    "checked": true,
    "id": "7a4fe2f003241ad97bf1778e527cb0306fa90da2",
    "semantic_title": "compost: characterizing and evaluating caricature in llm simulations",
    "citation_count": 1,
    "authors": [
      "Myra Cheng",
      "Tiziano Piccardi",
      "Diyi Yang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.670": {
    "title": "Reduce Human Labor On Evaluating Conversational Information Retrieval System: A Human-Machine Collaboration Approach",
    "volume": "main",
    "abstract": "Evaluating conversational information retrieval (CIR) systems is a challenging task that requires a significant amount of human labor for annotation. It is imperative to invest significant effort into researching more labor-effective methods for evaluating CIR systems. To touch upon this challenge, we take the first step to involve active testing in CIR evaluation and propose a novel method, called HomCoE. It strategically selects a few data for human annotation, then calibrates the evaluation results to eliminate evaluation biases. As such, it makes an accurate evaluation of the CIR system at low human labor. We experimentally reveal that it consumes less than 1% of human labor and achieves a consistency rate of 95%-99% with human evaluation results. This emphasizes the superiority of our method over other baselines",
    "checked": true,
    "id": "56335ac6a8ee9adcf2cf100d23a2ee323c06604c",
    "semantic_title": "reduce human labor on evaluating conversational information retrieval system: a human-machine collaboration approach",
    "citation_count": 0,
    "authors": [
      "Chen Huang",
      "Peixin Qin",
      "Wenqiang Lei",
      "Jiancheng Lv"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.671": {
    "title": "BERTie Bott's Every Flavor Labels: A Tasty Introduction to Semantic Role Labeling for Galician",
    "volume": "main",
    "abstract": "In this paper, we leverage existing corpora, WordNet, and dependency parsing to build the first Galician dataset for training semantic role labeling systems in an effort to expand available NLP resources. Additionally, we introduce verb indexing, a new pre-processing method, which helps increase the performance when semantically parsing highly-complex sentences. We use transfer-learning to test both the resource and the verb indexing method. Our results show that the effects of verb indexing were amplified in scenarios where the model was both pre-trained and fine-tuned on datasets utilizing the method, but improvements are also noticeable when only used during fine-tuning. The best-performing Galician SRL model achieved an f1 score of 0.74, introducing a baseline for future Galician SRL systems. We also tested our method on Spanish where we achieved an f1 score of 0.83, outperforming the baseline set by the 2009 CoNLL Shared Task by 0.025 showing the merits of our verb indexing method for pre-processing",
    "checked": true,
    "id": "8821378d3fe3dff7a6834ec173bcb2c515103c85",
    "semantic_title": "bertie bott's every flavor labels: a tasty introduction to semantic role labeling for galician",
    "citation_count": 0,
    "authors": [
      "Micaella Bruton",
      "Meriem Beloucif"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.672": {
    "title": "Program Translation via Code Distillation",
    "volume": "main",
    "abstract": "Software version migration and program translation are an important and costly part of the lifecycle of large codebases. Traditional machine translation relies on parallel corpora for supervised translation, which is not feasible for program translation due to a dearth of aligned data. Recent unsupervised neural machine translation techniques have overcome data limitations by included techniques such as back translation and low level compiler intermediate representations (IR). These methods face significant challenges due to the noise in code snippet alignment and the diversity of IRs respectively. In this paper we propose a novel model called Code Distillation (CoDist) whereby we capture the semantic and structural equivalence of code in a language agnostic intermediate representation. Distilled code serves as a translation pivot for any programming language, leading by construction to parallel corpora which scale to all available source code by simply applying the distillation compiler. We demonstrate that our approach achieves state-of-the-art performance on CodeXGLUE and TransCoder GeeksForGeeks translation benchmarks, with an average absolute increase of 12.7% on the TransCoder GeeksforGeeks translation benchmark compare to TransCoder-ST",
    "checked": true,
    "id": "b1c999046d189f13863a4e59867bedf8c993bf4a",
    "semantic_title": "program translation via code distillation",
    "citation_count": 0,
    "authors": [
      "Yufan Huang",
      "Mengnan Qi",
      "Yongqiang Yao",
      "Maoquan Wang",
      "Bin Gu",
      "Colin Clement",
      "Neel Sundaresan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.673": {
    "title": "FaMeSumm: Investigating and Improving Faithfulness of Medical Summarization",
    "volume": "main",
    "abstract": "Summaries of medical text shall be faithful by being consistent and factual with source inputs, which is an important but understudied topic for safety and efficiency in healthcare. In this paper, we investigate and improve faithfulness in summarization on a broad range of medical summarization tasks. Our investigation reveals that current summarization models often produce unfaithful outputs for medical input text. We then introduce FaMeSumm, a framework to improve faithfulness by fine-tuning pre-trained language models based on medical knowledge. FaMeSumm performs contrastive learning on designed sets of faithful and unfaithful summaries, and it incorporates medical terms and their contexts to encourage faithful generation of medical terms. We conduct comprehensive experiments on three datasets in two languages: health question and radiology report summarization datasets in English, and a patient-doctor dialogue dataset in Chinese. Results demonstrate that FaMeSumm is flexible and effective by delivering consistent improvements over mainstream language models such as BART, T5, mT5, and PEGASUS, yielding state-of-the-art performances on metrics for faithfulness and general quality. Human evaluation by doctors also shows that FaMeSumm generates more faithful outputs. Our code is available at https://github.com/psunlpgroup/FaMeSumm",
    "checked": true,
    "id": "bc384f277c4d5a69cf8fda561082912dbd42d98b",
    "semantic_title": "famesumm: investigating and improving faithfulness of medical summarization",
    "citation_count": 0,
    "authors": [
      "Nan Zhang",
      "Yusen Zhang",
      "Wu Guo",
      "Prasenjit Mitra",
      "Rui Zhang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.674": {
    "title": "Grammar-Constrained Decoding for Structured NLP Tasks without Finetuning",
    "volume": "main",
    "abstract": "Despite their impressive performance, large language models (LMs) still struggle with reliably generating complex output structures when not finetuned to follow the required output format exactly. To address this issue, grammar-constrained decoding (GCD) can be used to control the generation of LMs, guaranteeing that the output follows a given structure. Most existing GCD methods are, however, limited to specific tasks, such as parsing or code generation. In this work, we demonstrate that formal grammars can describe the output space for a much wider range of tasks and argue that GCD can serve as a unified framework for structured NLP tasks in general. For increased flexibility, we introduce input-dependent grammars, which allow the grammar to depend on the input and thus enable the generation of different output structures for different inputs. We then empirically demonstrate the power and flexibility of GCD-enhanced LMs on (1) information extraction, (2) entity disambiguation, and (3) constituency parsing. Our results indicate that grammar-constrained LMs substantially outperform unconstrained LMs or even beat task-specific finetuned models. Grammar constraints thus hold great promise for harnessing off-the-shelf LMs for a wide range of structured NLP tasks, especially where training data is scarce or finetuning is expensive. Code and data: https://github.com/epfl-dlab/GCD",
    "checked": true,
    "id": "7e269bfabb451765a16ca0357de6b497cefb60bf",
    "semantic_title": "grammar-constrained decoding for structured nlp tasks without finetuning",
    "citation_count": 1,
    "authors": [
      "Saibo Geng",
      "Martin Josifoski",
      "Maxime Peyrard",
      "Robert West"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.675": {
    "title": "Systematic word meta-sense extension",
    "volume": "main",
    "abstract": "The meaning of polysemous words often varies in a highly productive yet predictable way. Generalizing the regularity between conventional senses to derive novel word meaning is crucial for automated processing of non-literal language uses such as figurative expressions. We introduce a novel task called systematic word meta-sense extension (SWORME) to test and improve language models' ability to extend word meaning to denote new semantic domains (also called meta-senses) that bear regular semantic relations with existing senses. We found that language models prefer incremental lexical semantic change toward conceptually similar meta-senses such as logical metonymy, and are much worse at predicting highly non-literal meaning extensions such as metaphors. We propose a novel analogy-based method of word meaning extension, and show that it effectively improves language model systematicity in making both gradual and radical types of meta-sense extension. We further demonstrate that learning systematic meta-sense extensions benefits language models on multiple benchmarks of figurative language understanding",
    "checked": true,
    "id": "fabb0c223fb151190bf5db2569cc481a89483702",
    "semantic_title": "systematic word meta-sense extension",
    "citation_count": 0,
    "authors": [
      "Lei Yu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.676": {
    "title": "Evaluating Evaluation Metrics: A Framework for Analyzing NLG Evaluation Metrics using Measurement Theory",
    "volume": "main",
    "abstract": "We address a fundamental challenge in Natural Language Generation (NLG) model evaluation—the design and evaluation of evaluation metrics. Recognizing the limitations of existing automatic metrics and noises from how current human evaluation was conducted, we propose MetricEval, a framework informed by measurement theory, the foundation of educational test design, for conceptualizing and evaluating the reliability and validity of NLG evaluation metrics. The framework formalizes the source of measurement error and offers statistical tools for evaluating evaluation metrics based on empirical data. With our framework, one can quantify the uncertainty of the metrics to better interpret the result. To exemplify the use of our framework in practice, we analyzed a set of evaluation metrics for summarization and identified issues related to conflated validity structure in human-eval and reliability in LLM-based metrics. Through MetricEval, we aim to promote the design, evaluation, and interpretation of valid and reliable metrics to advance robust and effective NLG models",
    "checked": true,
    "id": "a30d5f2f10cef8af4efd4f929dfe2ce90c8b3010",
    "semantic_title": "evaluating evaluation metrics: a framework for analyzing nlg evaluation metrics using measurement theory",
    "citation_count": 2,
    "authors": [
      "Ziang Xiao",
      "Susu Zhang",
      "Vivian Lai",
      "Q. Vera Liao"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.677": {
    "title": "Revisiting the Knowledge Injection Frameworks",
    "volume": "main",
    "abstract": "In recent years, large language models (LLMs), such as GPTs, have attained great impact worldwide. However, how to adapt these LLMs to better suit the vertical domain-specific tasks by utilizing external knowledge remains not completely solved. Indeed, there have emerged a few works on this line where most of them rely on an alignment heuristic that is built to inject the corresponding knowledge tuple into the associated text sample. However, despite the promise, we identify a pivotal problem in this work ubiquitously. Simply put, we find that injecting unaligned (i.e., random) knowledge tuple into the LLMs achieves comparable (and sometimes better) results than the aligned knowledge being injected. We therefore take a thorough investigation of this frustrating finding on a variety of related prior work and further provide a chain of potential interpretations for the phenomenon. Based on all that, we offer a simple remediated technique. Briefly, the core of this technique roots in an ideological emphasis on the pruning and purification of the external knowledge base to be injected into LLMs. At last, we show that by integrating this technique into most (if not all) knowledge injection frameworks and recent LLMs, it manages to overcome the aforementioned sanity problem and further pushes the boundary of the performance of the domain-adaptive LLMs",
    "checked": true,
    "id": "eeb6b31674ffa65045642a50710d13f7ae482b38",
    "semantic_title": "revisiting the knowledge injection frameworks",
    "citation_count": 0,
    "authors": [
      "Peng Fu",
      "Yiming Zhang",
      "Haobo Wang",
      "Weikang Qiu",
      "Junbo Zhao"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.678": {
    "title": "We Are What We Repeatedly Do: Inducing and Deploying Habitual Schemas in Persona-Based Responses",
    "volume": "main",
    "abstract": "Many practical applications of dialogue technology require the generation of responses according to a particular developer-specified persona. While a variety of personas can be elicited from recent large language models, the opaqueness and unpredictability of these models make it desirable to be able to specify personas in an explicit form. In previous work, personas have typically been represented as sets of one-off pieces of self-knowledge that are retrieved by the dialogue system for use in generation. However, in realistic human conversations, personas are often revealed through story-like narratives that involve rich habitual knowledge – knowledge about kinds of events that an agent often participates in (e.g., work activities, hobbies, sporting activities, favorite entertainments, etc.), including typical goals, sub-events, preconditions, and postconditions of those events. We capture such habitual knowledge using an explicit schema representation, and propose an approach to dialogue generation that retrieves relevant schemas to condition a large language model to generate persona-based responses. Furthermore, we demonstrate a method for bootstrapping the creation of such schemas by first generating generic passages from a set of simple facts, and then inducing schemas from the generated passages",
    "checked": true,
    "id": "78c1148d23302fa02c94ba71b399e6557064a254",
    "semantic_title": "we are what we repeatedly do: inducing and deploying habitual schemas in persona-based responses",
    "citation_count": 0,
    "authors": [
      "Benjamin Kane",
      "Lenhart Schubert"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.679": {
    "title": "Zero-shot Faithfulness Evaluation for Text Summarization with Foundation Language Model",
    "volume": "main",
    "abstract": "Despite tremendous improvements in natural language generation, summarization models still suffer from the unfaithfulness issue. Previous work evaluates faithfulness either using models trained on the other tasks or in-domain synthetic data, or prompting a large model such as ChatGPT. This paper proposes to do zero-shot faithfulness evaluation simply with a moderately-sized foundation language model. We introduce a new metric FFLM, which is a combination of probability changes based on the intuition that prefixing a piece of text that is consistent with the output will increase the probability of predicting the output. Experiments show that FFLM performs competitively with or even outperforms ChatGPT on both inconsistency detection and faithfulness rating with 24x fewer parameters. FFLM also achieves improvements over other strong baselines",
    "checked": true,
    "id": "1c0e4c0e0899147ceb07a997dd57c64bb3384a47",
    "semantic_title": "zero-shot faithfulness evaluation for text summarization with foundation language model",
    "citation_count": 0,
    "authors": [
      "Qi Jia",
      "Siyu Ren",
      "Yizhu Liu",
      "Kenny Zhu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.680": {
    "title": "TaskWeb: Selecting Better Source Tasks for Multi-task NLP",
    "volume": "main",
    "abstract": "Recent work in NLP has shown promising results in training models on large amounts of tasks to achieve better generalization. However, it is not well-understood how tasks are related, and how helpful training tasks can be chosen for a new task. In this work, we investigate whether knowing task relationships via pairwise task transfer improves choosing one or more source tasks that help to learn a new target task. We provide TaskWeb, a large-scale benchmark of pairwise task transfers for 22 NLP tasks using three different model types, sizes, and adaptation methods, spanning about 25,000 experiments. Then, we design a new method TaskShop based on our analysis of TaskWeb. TaskShop uses TaskWeb to estimate the benefit of using a source task for learning a new target task, and to choose a subset of helpful training tasks for multi-task training. Our method improves overall rankings and top-k precision of source tasks by 10% and 38%, respectively. We also use TaskShop to build much smaller multi-task training sets that improve zero-shot performances across 11 different target tasks by at least 4.3%",
    "checked": true,
    "id": "acbabae091bed349535537b05a9ee80467faf255",
    "semantic_title": "taskweb: selecting better source tasks for multi-task nlp",
    "citation_count": 4,
    "authors": [
      "Joongwon Kim",
      "Akari Asai",
      "Gabriel Ilharco",
      "Hannaneh Hajishirzi"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.681": {
    "title": "Improving Bias Mitigation through Bias Experts in Natural Language Understanding",
    "volume": "main",
    "abstract": "Biases in the dataset often enable the model to achieve high performance on in-distribution data, while poorly performing on out-of-distribution data. To mitigate the detrimental effect of the bias on the networks, previous works have proposed debiasing methods that down-weight the biased examples identified by an auxiliary model, which is trained with explicit bias labels. However, finding a type of bias in datasets is a costly process. Therefore, recent studies have attempted to make the auxiliary model biased without the guidance (or annotation) of bias labels, by constraining the model's training environment or the capability of the model itself. Despite the promising debiasing results of recent works, the multi-class learning objective, which has been naively used to train the auxiliary model, may harm the bias mitigation effect due to its regularization effect and competitive nature across classes. As an alternative, we propose a new debiasing framework that introduces binary classifiers between the auxiliary model and the main model, coined bias experts. Specifically, each bias expert is trained on a binary classification task derived from the multi-class classification task via the One-vs-Rest approach. Experimental results demonstrate that our proposed strategy improves the bias identification ability of the auxiliary model. Consequently, our debiased model consistently outperforms the state-of-the-art on various challenge datasets",
    "checked": true,
    "id": "10f50471214b14ad60e9d5ca4ffe7443c3264976",
    "semantic_title": "improving bias mitigation through bias experts in natural language understanding",
    "citation_count": 0,
    "authors": [
      "Eojin Jeon",
      "Mingyu Lee",
      "Juhyeong Park",
      "Yeachan Kim",
      "Wing-Lam Mok",
      "SangKeun Lee"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.682": {
    "title": "Semi-supervised multimodal coreference resolution in image narrations",
    "volume": "main",
    "abstract": "In this paper, we study multimodal coreference resolution, specifically where a longer descriptive text, i.e., a narration is paired with an image. This poses significant challenges due to fine-grained image-text alignment, inherent ambiguity present in narrative language, and unavailability of large annotated training sets. To tackle these challenges, we present a data efficient semi-supervised approach that utilizes image-narration pairs to resolve coreferences and narrative grounding in a multimodal context. Our approach incorporates losses for both labeled and unlabeled data within a cross-modal framework. Our evaluation shows that the proposed approach outperforms strong baselines both quantitatively and qualitatively, for the tasks of coreference resolution and narrative grounding",
    "checked": true,
    "id": "9629d0315a44bd2a43c4ab96421c5cbccfd21a1b",
    "semantic_title": "semi-supervised multimodal coreference resolution in image narrations",
    "citation_count": 0,
    "authors": [
      "Arushi Goel",
      "Basura Fernando",
      "Frank Keller",
      "Hakan Bilen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.683": {
    "title": "A Predictive Factor Analysis of Social Biases and Task-Performance in Pretrained Masked Language Models",
    "volume": "main",
    "abstract": "Various types of social biases have been reported with pretrained Masked Language Models (MLMs) in prior work. However, multiple underlying factors are associated with an MLM such as its model size, size of the training data, training objectives, the domain from which pretraining data is sampled, tokenization, and languages present in the pretrained corpora, to name a few. It remains unclear as to which of those factors influence social biases that are learned by MLMs. To study the relationship between model factors and the social biases learned by an MLM, as well as the downstream task performance of the model, we conduct a comprehensive study over 39 pretrained MLMs covering different model sizes, training objectives, tokenization methods, training data domains and languages. Our results shed light on important factors often neglected in prior literature, such as tokenization or model objectives",
    "checked": true,
    "id": "7dd4b7e6f5f1588eae707df8334589ffd503bc54",
    "semantic_title": "a predictive factor analysis of social biases and task-performance in pretrained masked language models",
    "citation_count": 0,
    "authors": [
      "Yi Zhou",
      "Jose Camacho-Collados",
      "Danushka Bollegala"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.684": {
    "title": "Argument-based Detection and Classification of Fallacies in Political Debates",
    "volume": "main",
    "abstract": "Fallacies are arguments that employ faulty reasoning. Given their persuasive and seemingly valid nature, fallacious arguments are often used in political debates. Employing these misleading arguments in politics can have detrimental consequences for society, since they can lead to inaccurate conclusions and invalid inferences from the public opinion and the policymakers. Automatically detecting and classifying fallacious arguments represents therefore a crucial challenge to limit the spread of misleading or manipulative claims and promote a more informed and healthier political discourse. Our contribution to address this challenging task is twofold. First, we extend the ElecDeb60To16 dataset of U.S. presidential debates annotated with fallacious arguments, by incorporating the most recent Trump-Biden presidential debate. We include updated token-level annotations, incorporating argumentative components (i.e., claims and premises), the relations between these components (i.e., support and attack), and six categories of fallacious arguments (i.e., Ad Hominem, Appeal to Authority, Appeal to Emotion, False Cause, Slippery Slope, and Slogans). Second, we perform the twofold task of fallacious argument detection and classification by defining neural network architectures based on Transformers models, combining text, argumentative features, and engineered features. Our results show the advantages of complementing transformer-generated text representations with non-text features",
    "checked": true,
    "id": "37f32d826c23d53305bc5bacd750748c80dfc05c",
    "semantic_title": "argument-based detection and classification of fallacies in political debates",
    "citation_count": 0,
    "authors": [
      "Pierpaolo Goffredo",
      "Mariana Espinoza",
      "Serena Villata",
      "Elena Cabrio"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.685": {
    "title": "Collaborative Generative AI: Integrating GPT-k for Efficient Editing in Text-to-Image Generation",
    "volume": "main",
    "abstract": "The field of text-to-image (T2I) generation has garnered significant attention both within the research community and among everyday users. Despite the advancements of T2I models, a common issue encountered by users is the need for repetitive editing of input prompts in order to receive a satisfactory image, which is time-consuming and labor-intensive. Given the demonstrated text generation power of large-scale language models, such as GPT-k, we investigate the potential of utilizing such models to improve the prompt editing process for T2I generation. We conduct a series of experiments to compare the common edits made by humans and GPT-k, evaluate the performance of GPT-k in prompting T2I, and examine factors that may influence this process. We found that GPT-k models focus more on inserting modifiers while humans tend to replace words and phrases, which includes changes to the subject matter. Experimental results show that GPT-k are more effective in adjusting modifiers rather than predicting spontaneous changes in the primary subject matters. Adopting the edit suggested by GPT-k models may reduce the percentage of remaining edits by 20-30%",
    "checked": true,
    "id": "eac4bb6b087e9d864e7a35ce0f505d60a555b58a",
    "semantic_title": "collaborative generative ai: integrating gpt-k for efficient editing in text-to-image generation",
    "citation_count": 1,
    "authors": [
      "Wanrong Zhu",
      "Xinyi Wang",
      "Yujie Lu",
      "Tsu-Jui Fu",
      "Xin Wang",
      "Miguel Eckstein",
      "William Wang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.686": {
    "title": "SpEL: Structured Prediction for Entity Linking",
    "volume": "main",
    "abstract": "Entity linking is a prominent thread of research focused on structured data creation by linking spans of text to an ontology or knowledge source. We revisit the use of structured prediction for entity linking which classifies each individual input token as an entity, and aggregates the token predictions. Our system, called SpEL (Structured prediction for Entity Linking) is a state-of-the-art entity linking system that uses some new ideas to apply structured prediction to the task of entity linking including: two refined fine-tuning steps; a context sensitive prediction aggregation strategy; reduction of the size of the model's output vocabulary, and; we address a common problem in entity-linking systems where there is a training vs. inference tokenization mismatch. Our experiments show that we can outperform the state-of-the-art on the commonly used AIDA benchmark dataset for entity linking to Wikipedia. Our method is also very compute efficient in terms of number of parameters and speed of inference",
    "checked": true,
    "id": "26c3551d5482f61b6b527e6d86de270fa0c256f8",
    "semantic_title": "spel: structured prediction for entity linking",
    "citation_count": 0,
    "authors": [
      "Hassan Shavarani",
      "Anoop Sarkar"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.687": {
    "title": "Architectural Sweet Spots for Modeling Human Label Variation by the Example of Argument Quality: It's Best to Relate Perspectives!",
    "volume": "main",
    "abstract": "Many annotation tasks in natural language processing are highly subjective in that there can be different valid and justified perspectives on what is a proper label for a given example. This also applies to the judgment of argument quality, where the assignment of a single ground truth is often questionable. At the same time, there are generally accepted concepts behind argumentation that form a common ground. To best represent the interplay of individual and shared perspectives, we consider a continuum of approaches ranging from models that fully aggregate perspectives into a majority label to \"share nothing\"-architectures in which each annotator is considered in isolation from all other annotators. In between these extremes, inspired by models used in the field of recommender systems, we investigate the extent to which architectures that predict labels for single annotators but include layers that model the relations between different annotators are beneficial. By means of two tasks of argument quality classification (argument concreteness and validity/novelty of conclusions), we show that recommender architectures increase the averaged annotator-individual F1-scores up to 43% over a majority-label model. Our findings indicate that approaches to subjectivity can benefit from relating individual perspectives",
    "checked": true,
    "id": "7b52346b17b6fa949b6ec5d149fa037014fcd55f",
    "semantic_title": "architectural sweet spots for modeling human label variation by the example of argument quality: it's best to relate perspectives!",
    "citation_count": 0,
    "authors": [
      "Philipp Heinisch",
      "Matthias Orlikowski",
      "Julia Romberg",
      "Philipp Cimiano"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.688": {
    "title": "Explicit Planning Helps Language Models in Logical Reasoning",
    "volume": "main",
    "abstract": "Language models have been shown to perform remarkably well on a wide range of natural language processing tasks. In this paper, we propose LEAP, a novel system that uses language models to perform multi-step logical reasoning and incorporates explicit planning into the inference procedure. Explicit planning enables the system to make more informed reasoning decisions at each step by looking ahead into their future effects. Moreover, we propose a training strategy that safeguards the planning process from being led astray by spurious features. Our full system significantly outperforms other competing methods on multiple standard datasets. When using small T5 models as its core selection and deduction components, our system performs competitively compared to GPT-3 despite having only about 1B parameters (i.e., 175 times smaller than GPT-3). When using GPT-3.5, it significantly outperforms chain-of-thought prompting on the challenging PrOntoQA dataset. We have conducted extensive empirical studies to demonstrate that explicit planning plays a crucial role in the system's performance",
    "checked": true,
    "id": "9cd398e75e89b9d8104837da44ad17e110a4e4f9",
    "semantic_title": "explicit planning helps language models in logical reasoning",
    "citation_count": 6,
    "authors": [
      "Hongyu Zhao",
      "Kangrui Wang",
      "Mo Yu",
      "Hongyuan Mei"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.689": {
    "title": "clembench: Using Game Play to Evaluate Chat-Optimized Language Models as Conversational Agents",
    "volume": "main",
    "abstract": "Recent work has proposed a methodology for the systematic evaluation of \"Situated Language Understanding Agents\" — agents that operate in rich linguistic and non-linguistic contexts — through testing them in carefully constructed interactive settings. Other recent work has argued that Large Language Models (LLMs), if suitably set up, can be understood as (simulators of) such agents. A connection suggests itself, which this paper explores: Can LLMs be evaluated meaningfully by exposing them to constrained game-like settings that are built to challenge specific capabilities? As a proof of concept, this paper investigates five interaction settings, showing that current chat-optimised LLMs are, to an extent, capable of following game-play instructions. Both this capability and the quality of the game play, measured by how well the objectives of the different games are met, follows the development cycle, with newer models generally performing better. The metrics even for the comparatively simple example games are far from being saturated, suggesting that the proposed instrument will remain to have diagnostic value",
    "checked": true,
    "id": "f52af5abe78ca2af836f70ce193f0161bc2e6264",
    "semantic_title": "clembench: using game play to evaluate chat-optimized language models as conversational agents",
    "citation_count": 4,
    "authors": [
      "Kranti Chalamalasetti",
      "Jana Götze",
      "Sherzod Hakimov",
      "Brielen Madureira",
      "Philipp Sadler",
      "David Schlangen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.690": {
    "title": "Explaining with Contrastive Phrasal Highlighting: A Case Study in Assisting Humans to Detect Translation Differences",
    "volume": "main",
    "abstract": "Explainable NLP techniques primarily explain by answering \"Which tokens in the input are responsible for this prediction?\". We argue that for NLP models that make predictions by comparing two input texts, it is more useful to explain by answering \"What differences between the two inputs explain this prediction?\". We introduce a technique to generate contrastive phrasal highlights that explain the predictions of a semantic divergence model via phrase alignment guided erasure. We show that the resulting highlights match human rationales of cross-lingual semantic differences better than popular post-hoc saliency techniques and that they successfully help people detect fine-grained meaning differences in human translations and critical machine translation errors",
    "checked": true,
    "id": "f2808fadc0dc75ff76d2fc8b4541379add7201df",
    "semantic_title": "explaining with contrastive phrasal highlighting: a case study in assisting humans to detect translation differences",
    "citation_count": 0,
    "authors": [
      "Eleftheria Briakou",
      "Navita Goyal",
      "Marine Carpuat"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.691": {
    "title": "Polyglot or Not? Measuring Multilingual Encyclopedic Knowledge in Foundation Models",
    "volume": "main",
    "abstract": "In this work, we assess the ability of foundation models to recall encyclopedic knowledge across a wide range of linguistic contexts. To support this, we: 1) produce a 20-language dataset that contains 303k factual associations paired with counterfactuals, 2) evaluate 5 models in a multilingual test, and 3) benchmark a diverse set of 24 models in an English-only test. Meta's LLaMA achieves the highest scores in both multilingual and English-only evaluations. Yet, an analysis of LLaMA's errors reveals significant limitations in its ability to recall facts in languages other than English, plus difficulties related to the location and gender of fact subjects. Overall, our findings suggest that today's foundation models are far from polyglots",
    "checked": true,
    "id": "507cb1d9fffeadd284eb3cd654aa3a7fd46dbf0f",
    "semantic_title": "polyglot or not? measuring multilingual encyclopedic knowledge in foundation models",
    "citation_count": 0,
    "authors": [
      "Tim Schott",
      "Daniel Furman",
      "Shreshta Bhat"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.692": {
    "title": "Anchoring Fine-tuning of Sentence Transformer with Semantic Label Information for Efficient Truly Few-shot Classification",
    "volume": "main",
    "abstract": "Few-shot classification is a powerful technique, but training requires substantial computing power and data. We propose an efficient method with small model sizes and less training data with only 2-8 training instances per class. Our proposed method, AncSetFit, targets low data scenarios by anchoring the task and label information through sentence embeddings in fine-tuning a Sentence Transformer model. It uses contrastive learning and a triplet loss to enforce training instances of a class to be closest to its own textual semantic label information in the embedding space - and thereby learning to embed different class instances more distinct. AncSetFit obtains strong performance in data-sparse scenarios compared to existing methods across SST-5, Emotion detection, and AG News data, even with just two examples per class",
    "checked": true,
    "id": "b2a9487ec86486f9aa15292f1ffb1845f45651bc",
    "semantic_title": "anchoring fine-tuning of sentence transformer with semantic label information for efficient truly few-shot classification",
    "citation_count": 0,
    "authors": [
      "Amalie Pauli",
      "Leon Derczynski",
      "Ira Assent"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.693": {
    "title": "UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers",
    "volume": "main",
    "abstract": "Many information retrieval tasks require large labeled datasets for fine-tuning. However, such datasets are often unavailable, and their utility for real-world applications can diminish quickly due to domain shifts. To address this challenge, we develop and motivate a method for using large language models (LLMs) to generate large numbers of synthetic queries cheaply. The method begins by generating a small number of synthetic queries using an expensive LLM. After that, a much less expensive one is used to create large numbers of synthetic queries, which are used to fine-tune a family of reranker models. These rerankers are then distilled into a single efficient retriever for use in the target domain. We show that this technique boosts zero-shot accuracy in long-tail domains and achieves substantially lower latency than standard reranking methods",
    "checked": true,
    "id": "44b0d2e884efa5344e50424dbe2edf616981f201",
    "semantic_title": "udapdr: unsupervised domain adaptation via llm prompting and distillation of rerankers",
    "citation_count": 8,
    "authors": [
      "Jon Saad-Falcon",
      "Omar Khattab",
      "Keshav Santhanam",
      "Radu Florian",
      "Martin Franz",
      "Salim Roukos",
      "Avirup Sil",
      "Md Sultan",
      "Christopher Potts"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.694": {
    "title": "TATA: Stance Detection via Topic-Agnostic and Topic-Aware Embeddings",
    "volume": "main",
    "abstract": "Stance detection is important for understanding different attitudes and beliefs on the Internet. However, given that a passage's stance toward a given topic is often highly dependent on that topic, building a stance detection model that generalizes to unseen topics is difficult. In this work, we propose using contrastive learning as well as an unlabeled dataset of news articles that cover a variety of different topics to train topic-agnostic/TAG and topic-aware/TAW embeddings for use in downstream stance detection. Combining these embeddings in our full TATA model, we achieve state-of-the-art performance across several public stance detection datasets (0.771 F1-score on the Zero-shot VAST dataset). We release our code and data at https://github.com/hanshanley/tata",
    "checked": true,
    "id": "69f4786769cad31d8ce2775161495153f261da3f",
    "semantic_title": "tata: stance detection via topic-agnostic and topic-aware embeddings",
    "citation_count": 0,
    "authors": [
      "Hans Hanley",
      "Zakir Durumeric"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.695": {
    "title": "Data Similarity is Not Enough to Explain Language Model Performance",
    "volume": "main",
    "abstract": "Large language models achieve high performance on many but not all downstream tasks. The interaction between pretraining data and task data is commonly assumed to determine this variance: a task with data that is more similar to a model's pretraining data is assumed to be easier for that model. We test whether distributional and example-specific similarity measures (embedding-, token- and model-based) correlate with language model performance through a large-scale comparison of the Pile and C4 pretraining datasets with downstream benchmarks. Similarity correlates with performance for multilingual datasets, but in other benchmarks, we surprisingly find that similarity metrics are not correlated with accuracy or even each other. This suggests that the relationship between pretraining data and downstream tasks is more complex than often assumed",
    "checked": true,
    "id": "d95807b3626bae578038978df0008fdde71c24de",
    "semantic_title": "data similarity is not enough to explain language model performance",
    "citation_count": 0,
    "authors": [
      "Gregory Yauney",
      "Emily Reif",
      "David Mimno"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.696": {
    "title": "Zero-shot Sharpness-Aware Quantization for Pre-trained Language Models",
    "volume": "main",
    "abstract": "Quantization is a promising approach for reducing memory overhead and accelerating inference, especially in large pre-trained language model (PLM) scenarios. While having no access to original training data due to security and privacy concerns has emerged the demand for zero-shot quantization. Most of the cutting-edge zero-shot quantization methods primarily 1) apply to computer vision tasks, and 2) neglect of overfitting problem in the generative adversarial learning process, leading to sub-optimal performance. Motivated by this, we propose a novel zero-shot sharpness-aware quantization (ZSAQ) framework for the zero-shot quantization of various PLMs. The key algorithm in solving ZSAQ is the SAM-SGA optimization, which aims to improve the quantization accuracy and model generalization via optimizing a minimax problem. We theoretically prove the convergence rate for the minimax optimization problem and this result can be applied to other nonconvex-PL minimax optimization frameworks. Extensive experiments on 11 tasks demonstrate that our method brings consistent and significant performance gains on both discriminative and generative PLMs, i.e., up to +6.98 average score. Furthermore, we empirically validate that our method can effectively improve the model generalization",
    "checked": true,
    "id": "22736a03ffe39b024b45f294e6508e0b2b90079d",
    "semantic_title": "zero-shot sharpness-aware quantization for pre-trained language models",
    "citation_count": 0,
    "authors": [
      "Miaoxi Zhu",
      "Qihuang Zhong",
      "Li Shen",
      "Liang Ding",
      "Juhua Liu",
      "Bo Du",
      "Dacheng Tao"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.697": {
    "title": "Deciphering Stereotypes in Pre-Trained Language Models",
    "volume": "main",
    "abstract": "Warning: This paper contains content that is stereotypical and may be upsetting. This paper addresses the issue of demographic stereotypes present in Transformer-based pre-trained language models (PLMs) and aims to deepen our understanding of how these biases are encoded in these models. To accomplish this, we introduce an easy-to-use framework for examining the stereotype-encoding behavior of PLMs through a combination of model probing and textual analyses. Our findings reveal that a small subset of attention heads within PLMs are primarily responsible for encoding stereotypes and that stereotypes toward specific minority groups can be identified using attention maps on these attention heads. Leveraging these insights, we propose an attention-head pruning method as a viable approach for debiasing PLMs, without compromising their language modeling capabilities or adversely affecting their performance on downstream tasks",
    "checked": true,
    "id": "ba16ec572b09189528f554e6ce7586a2d16fa08c",
    "semantic_title": "deciphering stereotypes in pre-trained language models",
    "citation_count": 0,
    "authors": [
      "Weicheng Ma",
      "Henry Scheible",
      "Brian Wang",
      "Goutham Veeramachaneni",
      "Pratim Chowdhary",
      "Alan Sun",
      "Andrew Koulogeorge",
      "Lili Wang",
      "Diyi Yang",
      "Soroush Vosoughi"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.698": {
    "title": "An \"Integrative Survey on Mental Health Conversational Agents to Bridge Computer Science and Medical Perspectives",
    "volume": "main",
    "abstract": "Mental health conversational agents (a.k.a. chatbots) are widely studied for their potential to offer accessible support to those experiencing mental health challenges. Previous surveys on the topic primarily consider papers published in either computer science or medicine, leading to a divide in understanding and hindering the sharing of beneficial knowledge between both domains. To bridge this gap, we conduct a comprehensive literature review using the PRISMA framework, reviewing 534 papers published in both computer science and medicine. Our systematic review reveals 136 key papers on building mental health-related conversational agents with diverse characteristics of modeling and experimental design techniques. We find that computer science papers focus on LLM techniques and evaluating response quality using automated metrics with little attention to the application while medical papers use rule-based conversational agents and outcome metrics to measure the health outcomes of participants. Based on our findings on transparency, ethics, and cultural heterogeneity in this review, we provide a few recommendations to help bridge the disciplinary divide and enable the cross-disciplinary development of mental health conversational agents",
    "checked": false,
    "id": "cf3e8af119bae0d72a9d62fbc2a66d44bf854e94",
    "semantic_title": "an integrative survey on mental health conversational agents to bridge computer science and medical perspectives",
    "citation_count": 0,
    "authors": [
      "Young Cho",
      "Sunny Rai",
      "Lyle Ungar",
      "João Sedoc",
      "Sharath Guntuku"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.699": {
    "title": "Do LLMs Understand Social Knowledge? Evaluating the Sociability of Large Language Models with SocKET Benchmark",
    "volume": "main",
    "abstract": "Large language models (LLMs) have been shown to perform well at a variety of syntactic, discourse, and reasoning tasks. While LLMs are increasingly deployed in many forms including conversational agents that interact with humans, we lack a grounded benchmark to measure how well LLMs understand social language. Here, we introduce a new theory-driven benchmark, SocKET, that contains 58 NLP tasks testing social knowledge which we group into five categories: humor & sarcasm, offensiveness, sentiment & emotion, and trustworthiness. In tests on the benchmark, we demonstrate that current models attain only moderate performance but reveal significant potential for task transfer among different types and categories of tasks, which were predicted from theory. Through zero-shot evaluations, we show that pretrained models already possess some innate but limited capabilities of social language understanding and training on one category of tasks can improve zero-shot testing on others. Our benchmark provides a systematic way to analyze model performance on an important dimension of language and points to clear room for improvement to build more socially-aware LLMs. The resources are released at https://github.com/minjechoi/SOCKET",
    "checked": true,
    "id": "c1592c211f8b7791a55afd7162249c723b87c237",
    "semantic_title": "do llms understand social knowledge? evaluating the sociability of large language models with socket benchmark",
    "citation_count": 11,
    "authors": [
      "Minje Choi",
      "Jiaxin Pei",
      "Sagar Kumar",
      "Chang Shu",
      "David Jurgens"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.700": {
    "title": "Interventional Rationalization",
    "volume": "main",
    "abstract": "Selective rationalizations improve the explainability of neural networks by selecting a subsequence of the input (i.e., rationales) to explain the prediction results. Although existing methods have achieved promising results, they still suffer from adopting the spurious correlations in data (aka., shortcuts) to compose rationales and make predictions. Inspired by the causal theory, in this paper, we develop an interventional rationalization (Inter-RAT) to discover the causal rationales. Specifically, we first analyse the causalities among the input, rationales and results with a structural causal model. Then, we discover spurious correlations between the input and rationales, and between rationales and results, respectively, by identifying the confounder in the causalities. Next, based on the backdoor adjustment, we propose a causal intervention method to remove the spurious correlations between input and rationales. Further, we discuss reasons why spurious correlations between the selected rationales and results exist by analysing the limitations of the sparsity constraint in the rationalization, and employ the causal intervention method to remove these correlations. Extensive experimental results on three real-world datasets clearly validate the effectiveness of our proposed method. The source code of Inter-RAT is available at https://github.com/yuelinan/Codes-of-Inter-RAT",
    "checked": true,
    "id": "cbe5f825e76872eb55d341cfc4581747432697ab",
    "semantic_title": "interventional rationalization",
    "citation_count": 0,
    "authors": [
      "Linan Yue",
      "Qi Liu",
      "Li Wang",
      "Yanqing An",
      "Yichao Du",
      "Zhenya Huang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.701": {
    "title": "Don't Take This Out of Context!: On the Need for Contextual Models and Evaluations for Stylistic Rewriting",
    "volume": "main",
    "abstract": "Most existing stylistic text rewriting methods and evaluation metrics operate on a sentence level, but ignoring the broader context of the text can lead to preferring generic, ambiguous, and incoherent rewrites. In this paper, we investigate integrating the preceding textual context into both the rewriting and evaluation stages of stylistic text rewriting, and introduce a new composite contextual evaluation metric CtxSimFit that combines similarity to the original sentence with contextual cohesiveness. We comparatively evaluate non-contextual and contextual rewrites in formality, toxicity, and sentiment transfer tasks. Our experiments show that humans significantly prefer contextual rewrites as more fitting and natural over non-contextual ones, yet existing sentence-level automatic metrics (e.g., ROUGE, SBERT) correlate poorly with human preferences (𝜌=0–0.3). In contrast, human preferences are much better reflected by both our novel CtxSimFit (𝜌=0.7–0.9) as well as proposed context-infused versions of common metrics (𝜌=0.4–0.7). Overall, our findings highlight the importance of integrating context into the generation and especially the evaluation stages of stylistic text rewriting",
    "checked": false,
    "id": "a89c30ceca55783a1b2ff843eb6a4793e4a54b66",
    "semantic_title": "don't take this out of context! on the need for contextual models and evaluations for stylistic rewriting",
    "citation_count": 1,
    "authors": [
      "Akhila Yerukola",
      "Xuhui Zhou",
      "Elizabeth Clark",
      "Maarten Sap"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.702": {
    "title": "Axiomatic Preference Modeling for Longform Question Answering",
    "volume": "main",
    "abstract": "The remarkable abilities of large language models (LLMs) like ChatGPT and GPT-4 partially stem from the post-training processes involving human preferences encoded within a reward model as part of a Reinforcement Learning from Human Feedback (RLHF) regimen. These reward models (RMs) often lack direct knowledge of why, or under what principles, the preferences annotations were made. In this study, we identify principles that guide RMs to better align with human preferences, and then develop an axiomatic framework to generate a rich variety of preference signals to uphold them. We use these axiomatic signals to train a model for the scoring answers to longform questions. Our approach yields a Preference Model with only about 220M parameters that agrees with gold human-annotated preference labels more often than GPT-4. The contributions of this work include: training a standalone preference model that can score human- and LLM-generated answers on the same scale; developing an axiomatic framework for generating training data pairs tailored to certain principles; and showing that a small amount of axiomatic signals can help small models outperform GPT-4 in preference scoring. We intend to release our axiomatic data and model",
    "checked": true,
    "id": "75b8ea8e9175232fe22f6d385006dd5e28e63ab0",
    "semantic_title": "axiomatic preference modeling for longform question answering",
    "citation_count": 0,
    "authors": [
      "Corby Rosset",
      "Guoqing Zheng",
      "Victor Dibia",
      "Ahmed Awadallah",
      "Paul Bennett"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.703": {
    "title": "Countering Misinformation via Emotional Response Generation",
    "volume": "main",
    "abstract": "The proliferation of misinformation on social media platforms (SMPs) poses a significant danger to public health, social cohesion and ultimately democracy. Previous research has shown how social correction can be an effective way to curb misinformation, by engaging directly in a constructive dialogue with users who spread – often in good faith – misleading messages. Although professional fact-checkers are crucial to debunking viral claims, they usually do not engage in conversations on social media. Thereby, significant effort has been made to automate the use of fact-checker material in social correction; however, no previous work has tried to integrate it with the style and pragmatics that are commonly employed in social media communication. To fill this gap, we present VerMouth, the first large-scale dataset comprising roughly 12 thousand claim-response pairs (linked to debunking articles), accounting for both SMP-style and basic emotions, two factors which have a significant role in misinformation credibility and spreading. To collect this dataset we used a technique based on an author-reviewer pipeline, which efficiently combines LLMs and human annotators to obtain high-quality data. We also provide comprehensive experiments showing how models trained on our proposed dataset have significant improvements in terms of output quality and generalization capabilities",
    "checked": true,
    "id": "1c3164a204dabde2dcecf990ead6b368fe2fc485",
    "semantic_title": "countering misinformation via emotional response generation",
    "citation_count": 0,
    "authors": [
      "Daniel Russo",
      "Shane Kaszefski-Yaschuk",
      "Jacopo Staiano",
      "Marco Guerini"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.704": {
    "title": "Seq2seq is All You Need for Coreference Resolution",
    "volume": "main",
    "abstract": "Existing works on coreference resolution suggest that task-specific models are necessary to achieve state-of-the-art performance. In this work, we present compelling evidence that such models are not necessary. We finetune a pretrained seq2seq transformer to map an input document to a tagged sequence encoding the coreference annotation. Despite the extreme simplicity, our model outperforms or closely matches the best coreference systems in the literature on an array of datasets. We consider an even simpler version of seq2seq that generates only the tagged spans and find it highly performant. Our analysis shows that the model size, the amount of supervision, and the choice of sequence representations are key factors in performance",
    "checked": true,
    "id": "81ca63040e3fbff89044f722c81a9a8e7e10f111",
    "semantic_title": "seq2seq is all you need for coreference resolution",
    "citation_count": 0,
    "authors": [
      "Wenzheng Zhang",
      "Sam Wiseman",
      "Karl Stratos"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.705": {
    "title": "Integrating Language Models into Direct Speech Translation: An Inference-Time Solution to Control Gender Inflection",
    "volume": "main",
    "abstract": "When translating words referring to the speaker, speech translation (ST) systems should not resort to default masculine generics nor rely on potentially misleading vocal traits. Rather, they should assign gender according to the speakers' preference. The existing solutions to do so, though effective, are hardly feasible in practice as they involve dedicated model re-training on gender-labeled ST data. To overcome these limitations, we propose the first inference-time solution to control speaker-related gender inflections in ST. Our approach partially replaces the (biased) internal language model (LM) implicitly learned by the ST decoder with gender-specific external LMs. Experiments on en→es/fr/it show that our solution outperforms the base models and the best training-time mitigation strategy by up to 31.0 and 1.6 points in gender accuracy, respectively, for feminine forms. The gains are even larger (up to 32.0 and 3.4) in the challenging condition where speakers' vocal traits conflict with their gender",
    "checked": true,
    "id": "76e5135741d5602e5186b570edbf1fae64466302",
    "semantic_title": "integrating language models into direct speech translation: an inference-time solution to control gender inflection",
    "citation_count": 0,
    "authors": [
      "Dennis Fucci",
      "Marco Gaido",
      "Sara Papi",
      "Mauro Cettolo",
      "Matteo Negri",
      "Luisa Bentivogli"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.706": {
    "title": "StoryAnalogy: Deriving Story-level Analogies from Large Language Models to Unlock Analogical Understanding",
    "volume": "main",
    "abstract": "Analogy-making between narratives is crucial for human reasoning. In this paper, we evaluate the ability to identify and generate analogies by constructing a first-of-its-kind large-scale story-level analogy corpus, StoryAnalogy, which contains 24K story pairs from diverse domains with human annotations on two similarities from the extended Structure-Mapping Theory. We design a set of tests on StoryAnalogy, presenting the first evaluation of story-level analogy identification and generation. Interestingly, we find that the analogy identification tasks are incredibly difficult not only for sentence embedding models but also for the recent large language models (LLMs) such as ChatGPT and LLaMa. ChatGPT, for example, only achieved around 30% accuracy in multiple-choice questions (compared to over 85% accuracy for humans). Furthermore, we observe that the data in StoryAnalogy can improve the quality of analogy generation in LLMs, where a fine-tuned FlanT5-xxl model achieves comparable performance to zero-shot ChatGPT",
    "checked": true,
    "id": "64410909714f421c153ac123f975f86cc15c1fec",
    "semantic_title": "storyanalogy: deriving story-level analogies from large language models to unlock analogical understanding",
    "citation_count": 0,
    "authors": [
      "Cheng Jiayang",
      "Lin Qiu",
      "Tsz Chan",
      "Tianqing Fang",
      "Weiqi Wang",
      "Chunkit Chan",
      "Dongyu Ru",
      "Qipeng Guo",
      "Hongming Zhang",
      "Yangqiu Song",
      "Yue Zhang",
      "Zheng Zhang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.707": {
    "title": "Beyond Detection: A Defend-and-Summarize Strategy for Robust and Interpretable Rumor Analysis on Social Media",
    "volume": "main",
    "abstract": "As the impact of social media gradually escalates, people are more likely to be exposed to indistinguishable fake news. Therefore, numerous studies have attempted to detect rumors on social media by analyzing the textual content and propagation paths. However, fewer works on rumor detection tasks consider the malicious attacks commonly observed at response level. Moreover, existing detection models have poor interpretability. To address these issues, we propose a novel framework named **D**efend-**A**nd-**S**ummarize (DAS) based on the concept that responses sharing similar opinions should exhibit similar features. Specifically, DAS filters out the attack responses and summarizes the responsive posts of each conversation thread in both extractive and abstractive ways to provide multi-perspective prediction explanations. Furthermore, we enhance our detection architecture with the transformer and Bi-directional Graph Convolutional Networks. Experiments on three public datasets, *i.e.*, RumorEval2019, Twitter15, and Twitter16, demonstrate that our DAS defends against malicious attacks and provides prediction explanations, and the proposed detection model achieves state-of-the-art",
    "checked": true,
    "id": "32bf8c4246b55761ea8d4aa5332868219d6c4a03",
    "semantic_title": "beyond detection: a defend-and-summarize strategy for robust and interpretable rumor analysis on social media",
    "citation_count": 0,
    "authors": [
      "Yi-Ting Chang",
      "Yun-Zhu Song",
      "Yi-Syuan Chen",
      "Hong-Han Shuai"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.708": {
    "title": "Crystal: Introspective Reasoners Reinforced with Self-Feedback",
    "volume": "main",
    "abstract": "Extensive work has shown that the performance and interpretability of commonsense reasoning can be improved via knowledge-augmented reasoning methods, where the knowledge that underpins the reasoning process is explicitly verbalized and utilized. However, existing implementations, including \"chain-of-thought\" and its variants, fall short in capturing the *introspective* nature of knowledge required in commonsense reasoning, and in accounting for the mutual adaptation between the generation and utilization of knowledge. We propose a novel method to develop an introspective commonsense reasoner, **Crystal**. To tackle commonsense problems, it first introspects for knowledge statements related to the given question, and subsequently makes an informed prediction that is grounded in the previously introspected knowledge. The knowledge introspection and knowledge-grounded reasoning modes of the model are tuned via reinforcement learning to mutually adapt, where the reward derives from the feedback given by the model itself. Experiments show that Crystal significantly outperforms both the standard supervised finetuning and chain-of-thought distilled methods, and enhances the transparency of the commonsense reasoning process. Our work ultimately validates the feasibility and potential of reinforcing a neural model with self-feedback",
    "checked": true,
    "id": "6db6e6e71cc54435265643e19fcbdc7f3ba4c772",
    "semantic_title": "crystal: introspective reasoners reinforced with self-feedback",
    "citation_count": 1,
    "authors": [
      "Jiacheng Liu",
      "Ramakanth Pasunuru",
      "Hannaneh Hajishirzi",
      "Yejin Choi",
      "Asli Celikyilmaz"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.709": {
    "title": "DiffS2UT: A Semantic Preserving Diffusion Model for Textless Direct Speech-to-Speech Translation",
    "volume": "main",
    "abstract": "While Diffusion Generative Models have achieved great success on image generation tasks, how to efficiently and effectively incorporate them into speech generation especially translation tasks remains a non-trivial problem. Specifically, due to the low information density of speech data, the transformed discrete speech unit sequence is much longer than the corresponding text transcription, posing significant challenges to existing auto-regressive models. Furthermore, it is not optimal to brutally apply discrete diffusion on the speech unit sequence while disregarding the continuous space structure, which will degrade the generation performance significantly. In this paper, we propose a novel diffusion model by applying the diffusion forward process in the continuous speech representation space, while employing the diffusion backward process in the discrete speech unit space. In this way, we preserve the semantic structure of the continuous speech representation space in the diffusion process and integrate the continuous and discrete diffusion models. We conduct extensive experiments on the textless direct speech-to-speech translation task, where the proposed method achieves comparable results to the computationally intensive auto-regressive baselines (500 steps on average) with significantly fewer decoding steps (50 steps)",
    "checked": true,
    "id": "6e13c65ba26e9c2f9d5abc02be9f0d2240937607",
    "semantic_title": "diffs2ut: a semantic preserving diffusion model for textless direct speech-to-speech translation",
    "citation_count": 0,
    "authors": [
      "Yongxin Zhu",
      "Zhujin Gao",
      "Xinyuan Zhou",
      "Ye Zhongyi",
      "Linli Xu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.710": {
    "title": "BioFEG: Generate Latent Features for Biomedical Entity Linking",
    "volume": "main",
    "abstract": "Biomedical entity linking is an essential task in biomedical text processing, which aims to map entity mentions in biomedical text, such as clinical notes, to standard terms in a given knowledge base. However, this task is challenging due to the rarity of many biomedical entities in real-world scenarios, which often leads to a lack of annotated data for them. Limited by understanding these unseen entities, traditional biomedical entity linking models suffer from multiple types of linking errors. In this paper, we propose a novel latent feature generation framework BioFEG to address these challenges. Specifically, our BioFEG leverages domain knowledge to train a generative adversarial network, which generates latent semantic features of corresponding mentions for unseen entities. Utilizing these features, we fine-tune our entity encoder to capture fine-grained coherence information of unseen entities and better understand them. This allows models to make linking decisions more accurately, particularly for ambiguous mentions involving rare entities. Extensive experiments on the two benchmark datasets demonstrate the superiority of our proposed framework",
    "checked": true,
    "id": "ccb293339ac1e7f63efcdb4e82f144c8c369d99e",
    "semantic_title": "biofeg: generate latent features for biomedical entity linking",
    "citation_count": 0,
    "authors": [
      "Xuhui Sui",
      "Ying Zhang",
      "Xiangrui Cai",
      "Kehui Song",
      "Baohang Zhou",
      "Xiaojie Yuan",
      "Wensheng Zhang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.711": {
    "title": "TRIGO: Benchmarking Formal Mathematical Proof Reduction for Generative Language Models",
    "volume": "main",
    "abstract": "Automated theorem proving (ATP) has become an appealing domain for exploring the reasoning ability of the recent successful generative language models. However, current ATP benchmarks are mainly focus on symbolic inference, but rarely involve the understanding of complex number combination reasoning. In this work, we propose TRIGO, an ATP benchmark that not only requires a model to reduce a trigonometric expression with step-by-step proof but also evaluates a generative LM's reasoning ability on formulas and capability to manipulate, group, and factor number terms. We gather trigonometric expressions and their reduced forms from web, annotate the simplification process manually, and translate it into the \"Lean\" formal language system. We then automatically generate additional examples from the annotated samples to expand the dataset. Furthermore, we also create three automatically generated training and testing datasets of varying difficulty and distributions. Our extensive experiments show our proposed TRIGO poses a new challenge for advanced generative LM's including GPT-4 which is pre-trained on a considerable amount of open-source formal theorem-proving language data, and provide a new tool to study the generative LM's ability on both formal and mathematical reasoning",
    "checked": true,
    "id": "a419e1d74cfc2b5ff400963476bda5c6ae66e172",
    "semantic_title": "trigo: benchmarking formal mathematical proof reduction for generative language models",
    "citation_count": 0,
    "authors": [
      "Jing Xiong",
      "Jianhao Shen",
      "Ye Yuan",
      "Haiming Wang",
      "Yichun Yin",
      "Zhengying Liu",
      "Lin Li",
      "Zhijiang Guo",
      "Qingxing Cao",
      "Yinya Huang",
      "Chuanyang Zheng",
      "Xiaodan Liang",
      "Ming Zhang",
      "Qun Liu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.712": {
    "title": "Physician Detection of Clinical Harm in Machine Translation: Quality Estimation Aids in Reliance and Backtranslation Identifies Critical Errors",
    "volume": "main",
    "abstract": "A major challenge in the practical use of Machine Translation (MT) is that users lack information on translation quality to make informed decisions about how to rely on outputs. Progress in quality estimation research provides techniques to automatically assess MT quality, but these techniques have primarily been evaluated in vitro by comparison against human judgments outside of a specific context of use. This paper evaluates quality estimation feedback in vivo with a human study in realistic high-stakes medical settings. Using Emergency Department discharge instructions, we study how interventions based on quality estimation versus backtranslation assist physicians in deciding whether to show MT outputs to a patient. We find that quality estimation improves appropriate reliance on MT, but backtranslation helps physicians detect more clinically harmful errors that QE alone often misses",
    "checked": true,
    "id": "6f105dd60f0b90dbc61b34aadaaaef67b5a530f0",
    "semantic_title": "physician detection of clinical harm in machine translation: quality estimation aids in reliance and backtranslation identifies critical errors",
    "citation_count": 0,
    "authors": [
      "Nikita Mehandru",
      "Sweta Agrawal",
      "Yimin Xiao",
      "Ge Gao",
      "Elaine Khoong",
      "Marine Carpuat",
      "Niloufar Salehi"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.713": {
    "title": "Vicarious Offense and Noise Audit of Offensive Speech Classifiers: Unifying Human and Machine Disagreement on What is Offensive",
    "volume": "main",
    "abstract": "Offensive speech detection is a key component of content moderation. However, what is offensive can be highly subjective. This paper investigates how machine and human moderators disagree on what is offensive when it comes to real-world social web political discourse. We show that (1) there is extensive disagreement among the moderators (humans and machines); and (2) human and large-language-model classifiers are unable to predict how other human raters will respond, based on their political leanings. For (1), we conduct a ***noise audit*** at an unprecedented scale that combines both machine and human responses. For (2), we introduce a first-of-its-kind dataset of ***vicarious offense***. Our noise audit reveals that moderation outcomes vary wildly across different machine moderators. Our experiments with human moderators suggest that political leanings combined with sensitive issues affect both first-person and vicarious offense. The dataset is available through https://github.com/Homan-Lab/voiced",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tharindu Weerasooriya",
      "Sujan Dutta",
      "Tharindu Ranasinghe",
      "Marcos Zampieri",
      "Christopher Homan",
      "Ashiqur KhudaBukhsh"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.714": {
    "title": "Generating Summaries with Controllable Readability Levels",
    "volume": "main",
    "abstract": "Readability refers to how easily a reader can understand a written text. Several factors affect the readability level, such as the complexity of the text, its subject matter, and the reader's background knowledge. Generating summaries based on different readability levels is critical for enabling knowledge consumption by diverse audiences. However, current text generation approaches lack refined control, resulting in texts that are not customized to readers' proficiency levels. In this work, we bridge this gap and study techniques to generate summaries at specified readability levels. Unlike previous methods that focus on a specific readability level (e.g., lay summarization), we generate summaries with fine-grained control over their readability. We develop three text generation techniques for controlling readability: (1) instruction-based readability control, (2) reinforcement learning to minimize the gap between requested and observed readability and (3) a decoding approach that uses lookahead to estimate the readability of upcoming decoding steps. We show that our generation methods significantly improve readability control on news summarization (CNN/DM dataset), as measured by various readability metrics and human judgement, establishing strong baselines for controllable readability in summarization",
    "checked": true,
    "id": "c146966ed1050872246fdf6c67cfe05cef225204",
    "semantic_title": "generating summaries with controllable readability levels",
    "citation_count": 2,
    "authors": [
      "Leonardo Ribeiro",
      "Mohit Bansal",
      "Markus Dreyer"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.715": {
    "title": "mAggretriever: A Simple yet Effective Approach to Zero-Shot Multilingual Dense Retrieval",
    "volume": "main",
    "abstract": "Multilingual information retrieval (MLIR) is a crucial yet challenging task due to the need for human annotations in multiple languages, making training data creation labor-intensive. In this paper, we introduce mAggretriever, which effectively leverages semantic and lexical features from pre-trained multilingual transformers (e.g., mBERT and XLM-R) for dense retrieval. To enhance training and inference efficiency, we employ approximate masked-language modeling prediction for computing lexical features, reducing 70–85% GPU memory requirement for mAggretriever fine-tuning. Empirical results demonstrate that mAggretriever, fine-tuned solely on English training data, surpasses existing state-of-the-art multilingual dense retrieval models that undergo further training on large-scale MLIR training data. Our code is available at url",
    "checked": true,
    "id": "fdda6f431c005b822c850e91ca5a67d218747d30",
    "semantic_title": "maggretriever: a simple yet effective approach to zero-shot multilingual dense retrieval",
    "citation_count": 0,
    "authors": [
      "Sheng-Chieh Lin",
      "Amin Ahmad",
      "Jimmy Lin"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.716": {
    "title": "CodeFusion: A Pre-trained Diffusion Model for Code Generation",
    "volume": "main",
    "abstract": "Imagine a developer who can only change their last line of code—how often would they have to start writing a function from scratch before it is correct? Auto-regressive models for code generation from natural language have a similar limitation: they do not easily allow reconsidering earlier tokens generated. We introduce CodeFusion, a pre-trained diffusion code generation model that addresses this limitation by iteratively denoising a complete program conditioned on the encoded natural language. We evaluate CodeFusion on the task of natural language to code generation for Bash, Python, and Microsoft Excel conditional formatting (CF) rules. Experiments show that CodeFusion (75M parameters) performs on par with state-of-the-art auto-regressive systems (350M-175B parameters) in top-1 accuracy and outperforms them in top-3 and top-5 accuracy due to its better balance in diversity versus quality",
    "checked": true,
    "id": "1bfe2a9a40a5f34c5c6b99c182d37a6e93f95aa9",
    "semantic_title": "codefusion: a pre-trained diffusion model for code generation",
    "citation_count": 2,
    "authors": [
      "Mukul Singh",
      "José Cambronero",
      "Sumit Gulwani",
      "Vu Le",
      "Carina Negreanu",
      "Gust Verbruggen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.717": {
    "title": "CESAR: Automatic Induction of Compositional Instructions for Multi-turn Dialogs",
    "volume": "main",
    "abstract": "Instruction-based multitasking has played a critical role in the success of large language models (LLMs) in multi-turn dialog applications. While publicly available LLMs have shown promising performance, when exposed to complex instructions with multiple constraints, they lag against state-of-the-art models like ChatGPT. In this work, we hypothesize that the availability of large-scale complex demonstrations is crucial in bridging this gap. Focusing on dialog applications, we propose a novel framework, CESAR, that unifies a large number of dialog tasks in the same format and allows programmatic induction of complex instructions without any manual effort. We apply CESAR on InstructDial, a benchmark for instruction-based dialog tasks. We further enhance InstructDial with new datasets and tasks and utilize CESAR to induce complex tasks with compositional instructions. This results in a new benchmark called InstructDial++, which includes 63 datasets with 86 basic tasks and 68 composite tasks. Through rigorous experiments, we demonstrate the scalability of CESAR in providing rich instructions. Models trained on InstructDial++ can follow compositional prompts, such as prompts that ask for multiple stylistic constraints",
    "checked": true,
    "id": "1658674b715e66ef3a8cf24369a1d1691580f4a9",
    "semantic_title": "cesar: automatic induction of compositional instructions for multi-turn dialogs",
    "citation_count": 0,
    "authors": [
      "Taha Aksu",
      "Devamanyu Hazarika",
      "Shikib Mehri",
      "Seokhwan Kim",
      "Dilek Hakkani-Tur",
      "Yang Liu",
      "Mahdi Namazifar"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.718": {
    "title": "VECHR: A Dataset for Explainable and Robust Classification of Vulnerability Type in the European Court of Human Rights",
    "volume": "main",
    "abstract": "Recognizing vulnerability is crucial for understanding and implementing targeted support to empower individuals in need. This is especially important at the European Court of Human Rights (ECtHR), where the court adapts Convention standards to meet actual individual needs and thus to ensure effective human rights protection. However, the concept of vulnerability remains elusive at the ECtHR and no prior NLP research has dealt with it. To enable future research in this area, we present VECHR, a novel expert-annotated multi-label dataset comprising of vulnerability type classification and explanation rationale. We benchmark the performance of state-of-the-art models on VECHR from both prediction and explainability perspective. Our results demonstrate the challenging nature of task with lower prediction performance and limited agreement between models and experts. Further, we analyze the robustness of these models in dealing with out-of-domain (OOD) data and observe overall limited performance. Our dataset poses unique challenges offering a significant room for improvement regarding performance, explainability and robustness",
    "checked": true,
    "id": "2eb08dd9efb745ae1ce94915940ab22078cb48f0",
    "semantic_title": "vechr: a dataset for explainable and robust classification of vulnerability type in the european court of human rights",
    "citation_count": 0,
    "authors": [
      "Shanshan Xu",
      "Leon Staufer",
      "Santosh T.y.s.s",
      "Oana Ichim",
      "Corina Heri",
      "Matthias Grabmair"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.719": {
    "title": "ACQUIRED: A Dataset for Answering Counterfactual Questions In Real-Life Videos",
    "volume": "main",
    "abstract": "Multimodal counterfactual reasoning is a vital yet challenging ability for AI systems. It involves predicting the outcomes of hypothetical circumstances based on vision and language inputs, which enables AI models to learn from failures and explore hypothetical scenarios. Despite its importance, there are only a few datasets targeting the counterfactual reasoning abilities of multimodal models. Among them, they only cover reasoning over synthetic environments or specific types of events (e.g. traffic collisions), making them hard to reliably benchmark the model generalization ability in diverse real-world scenarios and reasoning dimensions. To overcome these limitations, we develop a video question answering dataset, ACQUIRED: it consists of 3.9K annotated videos, encompassing a wide range of event types and incorporating both first and third-person viewpoints, which ensures a focus on real-world diversity. In addition, each video is annotated with questions that span three distinct dimensions of reasoning, including physical, social, and temporal, which can comprehensively evaluate the model counterfactual abilities along multiple aspects. We benchmark our dataset against several state-of-the-art language-only and multimodal models and experimental results demonstrate a significant performance gap (>13%) between models and humans. The findings suggest that multimodal counterfactual reasoning remains an open challenge and ACQUIRED is a comprehensive and reliable benchmark for inspiring future research in this direction",
    "checked": true,
    "id": "cbad7a061daa48d8135d186df78b95b0cb63cdc9",
    "semantic_title": "acquired: a dataset for answering counterfactual questions in real-life videos",
    "citation_count": 0,
    "authors": [
      "Te-Lin Wu",
      "Zi-Yi Dou",
      "Qingyuan Hu",
      "Yu Hou",
      "Nischal Chandra",
      "Marjorie Freedman",
      "Ralph Weischedel",
      "Nanyun Peng"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.720": {
    "title": "From Parse-Execute to Parse-Execute-Refine: Improving Semantic Parser for Complex Question Answering over Knowledge Base",
    "volume": "main",
    "abstract": "Parsing questions into executable logical forms has showed impressive results for knowledge-base question answering (KBQA). However, complex KBQA is a more challenging task that requires to perform complex multi-step reasoning. Recently, a new semantic parser called KoPL has been proposed to explicitly model the reasoning processes, which achieved the state-of-the-art on complex KBQA. In this paper, we further explore how to unlock the reasoning ability of semantic parsers by a simple proposed parse-execute-refine paradigm. We refine and improve the KoPL parser by demonstrating the executed intermediate reasoning steps to the KBQA model. We show that such simple strategy can significantly improve the ability of complex reasoning. Specifically, we propose three components: a parsing stage, an execution stage and a refinement stage, to enhance the ability of complex reasoning. The parser uses the KoPL to generate the transparent logical forms. Then, the execution stage aligns and executes the logical forms over knowledge base to obtain intermediate reasoning processes. Finally, the intermediate step-by-step reasoning processes are demonstrated to the KBQA model in the refinement stage. With the explicit reasoning processes, it is much easier to answer the complex questions. Experiments on benchmark dataset shows that the proposed PER-KBQA performs significantly better than the stage-of-the-art baselines on the complex KBQA",
    "checked": true,
    "id": "ad55fef5ecf6f75912cb4dbb12408c0a10ca490b",
    "semantic_title": "from parse-execute to parse-execute-refine: improving semantic parser for complex question answering over knowledge base",
    "citation_count": 0,
    "authors": [
      "Wangzhen Guo",
      "Linyin Luo",
      "Hanjiang Lai",
      "Jian Yin"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.721": {
    "title": "Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model",
    "volume": "main",
    "abstract": "While large language models have proven effective in a huge range of downstream applications, they often generate text that is problematic or lacks a desired attribute. In this paper, we introduce Reward-Augmented Decoding (RAD), a text generation procedure that uses a small unidirectional reward model to encourage a language model to generate text that has certain properties. Specifically, RAD uses the reward model to score generations as they are produced and rescales sampling probabilities to favor high-reward tokens. By using a unidirectional reward model, RAD can cache activations from prior generation steps to decrease computational overhead. Through experiments on generating non-toxic and sentiment-controlled text, we demonstrate that RAD performs best among methods that change only the generation procedure and matches the performance of state-of-the-art methods that involve re-training the language model. We further validate that RAD is effective on very large language models while incurring a minimal computational overhead",
    "checked": true,
    "id": "e5d0857feca845b474b89565d513ff599629851d",
    "semantic_title": "reward-augmented decoding: efficient controlled text generation with a unidirectional reward model",
    "citation_count": 2,
    "authors": [
      "Haikang Deng",
      "Colin Raffel"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.722": {
    "title": "CORE: A Few-Shot Company Relation Classification Dataset for Robust Domain Adaptation",
    "volume": "main",
    "abstract": "We introduce CORE, a dataset for few-shot relation classification (RC) focused on company relations and business entities. CORE includes 4,708 instances of 12 relation types with corresponding textual evidence extracted from company Wikipedia pages. Company names and business entities pose a challenge for few-shot RC models due to the rich and diverse information associated with them. For example, a company name may represent the legal entity, products, people, or business divisions depending on the context. Therefore, deriving the relation type between entities is highly dependent on textual context. To evaluate the performance of state-of-the-art RC models on the CORE dataset, we conduct experiments in the few-shot domain adaptation setting. Our results reveal substantial performance gaps, confirming that models trained on different domains struggle to adapt to CORE. Interestingly, we find that models trained on CORE showcase improved out-of-domain performance, which highlights the importance of high-quality data for robust domain generalization. Specifically, the information richness embedded in business entities allows models to focus on contextual nuances, reducing their reliance on superficial clues such as relation-specific verbs. In addition to the dataset, we provide relevant code snippets to facilitate reproducibility and encourage further research in the field. The CORE dataset and code are publicly available at https://anonymous.4open.science/r/CORE-D377",
    "checked": true,
    "id": "36decf192178de99ca91f0929387ad596f9b48e2",
    "semantic_title": "core: a few-shot company relation classification dataset for robust domain adaptation",
    "citation_count": 0,
    "authors": [
      "Philipp Borchert",
      "Jochen De Weerdt",
      "Kristof Coussement",
      "Arno De Caigny",
      "Marie-Francine Moens"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.723": {
    "title": "Models See Hallucinations: Evaluating the Factuality in Video Captioning",
    "volume": "main",
    "abstract": "Video captioning aims to describe events in a video with natural language. In recent years, many works have focused on improving captioning models' performance. However, like other text generation tasks, it risks introducing factual errors not supported by the input video. Factual errors can seriously affect the quality of the generated text, sometimes making it completely unusable. Although factual consistency has received much research attention in text-to-text tasks (e.g., summarization), it is less studied in vision-based text generation. In this work, we conduct the first human evaluation of the factuality in video captioning and annotate two factuality datasets. We find that 56% of the model-generated sentences have factual errors, indicating it is a severe problem in this field, but existing evaluation metrics show little correlation with human factuality annotation. We further propose a weakly-supervised, model-based factuality metric FactVC, which outperforms previous metrics on factuality evaluation of video captioning",
    "checked": true,
    "id": "01bc871c0ecb7f586e54b9f67d2e50e08cbedf3d",
    "semantic_title": "models see hallucinations: evaluating the factuality in video captioning",
    "citation_count": 2,
    "authors": [
      "Hui Liu",
      "Xiaojun Wan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.724": {
    "title": "Back Transcription as a Method for Evaluating Robustness of Natural Language Understanding Models to Speech Recognition Errors",
    "volume": "main",
    "abstract": "In a spoken dialogue system, an NLU model is preceded by a speech recognition system that can deteriorate the performance of natural language understanding. This paper proposes a method for investigating the impact of speech recognition errors on the performance of natural language understanding models. The proposed method combines the back transcription procedure with a fine-grained technique for categorizing the errors that affect the performance of NLU models. The method relies on the usage of synthesized speech for NLU evaluation. We show that the use of synthesized speech in place of audio recording does not change the outcomes of the presented technique in a significant way",
    "checked": true,
    "id": "c35638de30020a8fe7b4eeb74a7285753b23747c",
    "semantic_title": "back transcription as a method for evaluating robustness of natural language understanding models to speech recognition errors",
    "citation_count": 1,
    "authors": [
      "Marek Kubis",
      "Paweł Skórzewski",
      "Marcin Sowańnski",
      "Tomasz Zietkiewicz"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.725": {
    "title": "Cabbage Sweeter than Cake? Analysing the Potential of Large Language Models for Learning Conceptual Spaces",
    "volume": "main",
    "abstract": "The theory of Conceptual Spaces is an influential cognitive-linguistic framework for representing the meaning of concepts. Conceptual spaces are constructed from a set of quality dimensions, which essentially correspond to primitive perceptual features (e.g. hue or size). These quality dimensions are usually learned from human judgements, which means that applications of conceptual spaces tend to be limited to narrow domains (e.g. modelling colour or taste). Encouraged by recent findings about the ability of Large Language Models (LLMs) to learn perceptually grounded representations, we explore the potential of such models for learning conceptual spaces. Our experiments show that LLMs can indeed be used for learning meaningful representations to some extent. However, we also find that fine-tuned models of the BERT family are able to match or even outperform the largest GPT-3 model, despite being 2 to 3 orders of magnitude smaller",
    "checked": true,
    "id": "26807ae7f8a1c9e3d5017c64c63eeab1807adcee",
    "semantic_title": "cabbage sweeter than cake? analysing the potential of large language models for learning conceptual spaces",
    "citation_count": 0,
    "authors": [
      "Usashi Chatterjee",
      "Amit Gajbhiye",
      "Steven Schockaert"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.726": {
    "title": "Can Language Models Understand Physical Concepts?",
    "volume": "main",
    "abstract": "Language models (LMs) gradually become general-purpose interfaces in the interactive and embodied world, where the understanding of physical concepts is an essential prerequisite. However, it is unclear whether LMs can understand physical concepts in the human world. To investigate this, we design a benchmark VEC that covers the tasks of (i) Visual concepts, such as the shape and material of objects, and (ii) Embodied Concepts, learned from the interaction with the world such as the temperature of objects. Our zero (few)-shot prompting results show that the understanding of certain visual concepts emerges as scaling up LMs, but there are still basic concepts to which the scaling law does not apply. For example, OPT-175B performs close to humans with a zero-shot accuracy of 85% on the material concept, yet behaves like random guessing on the mass concept. Instead, vision-augmented LMs such as CLIP and BLIP achieve a human-level understanding of embodied concepts. Analysis indicates that the rich semantics in visual representation can serve as a valuable source of embodied knowledge. Inspired by this, we propose a distillation method to transfer embodied knowledge from VLMs to LMs, achieving performance gain comparable with that by scaling up parameters of LMs 134×. Our dataset is available at https://github.com/TobiasLee/VEC",
    "checked": true,
    "id": "1caa2a29d3ca38d0e5111f4f9ae140727bb7d567",
    "semantic_title": "can language models understand physical concepts?",
    "citation_count": 4,
    "authors": [
      "Lei Li",
      "Jingjing Xu",
      "Qingxiu Dong",
      "Ce Zheng",
      "Xu Sun",
      "Lingpeng Kong",
      "Qi Liu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.727": {
    "title": "SPT: Learning to Selectively Insert Prompts for Better Prompt Tuning",
    "volume": "main",
    "abstract": "Prompt tuning prepends a soft prompt to the input embeddings or hidden states and only optimizes the prompt to adapt pretrained models (PTMs) to downstream tasks. The previous work manually selects prompt layers which are far from optimal and failed to exploit the potential of prompt tuning. In this work, we propose a novel framework, Selective Prompt Tuning (SPT), that learns to select the proper prompt layers by inserting a prompt controlled by a learnable probabilistic gate at each intermediate layer. We further propose a novel bi-level optimization framework, SPT-DARTS, that can better optimize the learnable gates and improve the final prompt tuning performances of the learned prompt layer settings. We conduct extensive experiments with ten benchmark datasets under the full-data and few-shot scenarios. The results demonstrate that our SPT framework can perform better than the previous state-of-the-art PETuning baselines with comparable or fewer tunable parameters",
    "checked": true,
    "id": "22effc238e65136003690f9a0cdcfb73b72b6f0f",
    "semantic_title": "spt: learning to selectively insert prompts for better prompt tuning",
    "citation_count": 0,
    "authors": [
      "Wei Zhu",
      "Ming Tan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.728": {
    "title": "Once Upon a Time in Graph: Relative-Time Pretraining for Complex Temporal Reasoning",
    "volume": "main",
    "abstract": "Our physical world is constantly evolving over time, rendering challenges for pre-trained language models to understand and reason over the temporal contexts of texts. Existing work focuses on strengthening the direct association between a piece of text and its time-stamp. However, the knowledge-time association is usually insufficient for the downstream tasks that require reasoning over temporal dependencies between knowledge. In this work, we make use of the underlying nature of time, all temporally-scoped sentences are strung together through a one-dimensional time axis, and suggest creating a graph structure based on the relative placements of events along the time axis. Inspired by the graph view, we propose RemeMo ( ̲Relative Ti ̲me ̲Modeling), which explicitly connects all temporally-scoped facts by modeling the time relations between any two sentences. Experimental results show that RemeMo outperforms the baseline T5 on multiple temporal question answering datasets under various settings. Further analysis suggests that RemeMo is especially good at modeling long-range complex temporal dependencies",
    "checked": true,
    "id": "07b2f2f705288fa456a8e4a49ce421304fbc24aa",
    "semantic_title": "once upon a time in graph: relative-time pretraining for complex temporal reasoning",
    "citation_count": 0,
    "authors": [
      "Sen Yang",
      "Xin Li",
      "Lidong Bing",
      "Wai Lam"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.729": {
    "title": "Expository Text Generation: Imitate, Retrieve, Paraphrase",
    "volume": "main",
    "abstract": "Expository documents are vital resources for conveying complex information to readers. Despite their usefulness, writing expository text by hand is a challenging process that requires careful content planning, obtaining facts from multiple sources, and the ability to clearly synthesize these facts. To ease these burdens, we propose the task of expository text generation, which seeks to automatically generate an accurate and stylistically consistent expository text for a topic by intelligently searching a knowledge source. We solve our task by developing IRP, a framework that overcomes the limitations of retrieval-augmented models and iteratively performs content planning, fact retrieval, and rephrasing. Through experiments on three diverse, newly-collected datasets, we show that IRP produces factual and organized expository texts that accurately inform readers",
    "checked": true,
    "id": "37b115139dbeff3ffbcb9927d5383084443c27e2",
    "semantic_title": "expository text generation: imitate, retrieve, paraphrase",
    "citation_count": 1,
    "authors": [
      "Nishant Balepur",
      "Jie Huang",
      "Kevin Chang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.730": {
    "title": "Large-scale similarity search with Optimal Transport",
    "volume": "main",
    "abstract": "Wasserstein distance is a powerful tool for comparing probability distributions and is widely used for document classification and retrieval tasks in NLP. In particular, it is known as the word mover's distance (WMD) in the NLP community. WMD exhibits excellent performance for various NLP tasks; however, one of its limitations is its computational cost and thus is not useful for large-scale distribution comparisons. In this study, we propose a simple and effective nearest neighbor search based on the Wasserstein distance. Specifically, we employ the L1 embedding method based on the tree-based Wasserstein approximation and subsequently used the nearest neighbor search to efficiently find the k-nearest neighbors. Through benchmark experiments, we demonstrate that the proposed approximation has comparable performance to the vanilla Wasserstein distance and can be computed three orders of magnitude faster than the vanilla Wasserstein distance",
    "checked": true,
    "id": "d889eded46edafc8a6cb37aebd505ea8ca348b15",
    "semantic_title": "large-scale similarity search with optimal transport",
    "citation_count": 0,
    "authors": [
      "Cléa Laouar",
      "Yuki Takezawa",
      "Makoto Yamada"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.731": {
    "title": "Enhancing Textbooks with Visuals from the Web for Improved Learning",
    "volume": "main",
    "abstract": "Textbooks are one of the main mediums for delivering high-quality education to students. In particular, explanatory and illustrative visuals play a key role in retention, comprehension and general transfer of knowledge. However, many textbooks lack these interesting visuals to support student learning. In this paper, we investigate the effectiveness of vision-language models to automatically enhance textbooks with images from the web. We collect a dataset of e-textbooks in the math, science, social science and business domains. We then set up a text-image matching task that involves retrieving and appropriately assigning web images to textbooks, which we frame as a matching optimization problem. Through a crowd-sourced evaluation, we verify that (1) while the original textbook images are rated higher, automatically assigned ones are not far behind, and (2) the precise formulation of the optimization problem matters. We release the dataset of textbooks with an associated image bank to inspire further research in this intersectional area of computer vision and NLP for education",
    "checked": true,
    "id": "06f7eefcfe454a37a92459a9646a1feb6e7c0382",
    "semantic_title": "enhancing textbooks with visuals from the web for improved learning",
    "citation_count": 0,
    "authors": [
      "Janvijay Singh",
      "Vilém Zouhar",
      "Mrinmaya Sachan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.732": {
    "title": "Continual Event Extraction with Semantic Confusion Rectification",
    "volume": "main",
    "abstract": "We study continual event extraction, which aims to extract incessantly emerging event information while avoiding forgetting. We observe that the semantic confusion on event types stems from the annotations of the same text being updated over time. The imbalance between event types even aggravates this issue. This paper proposes a novel continual event extraction model with semantic confusion rectification. We mark pseudo labels for each sentence to alleviate semantic confusion. We transfer pivotal knowledge between current and previous models to enhance the understanding of event types. Moreover, we encourage the model to focus on the semantics of long-tailed event types by leveraging other associated types. Experimental results show that our model outperforms state-of-the-art baselines and is proficient in imbalanced datasets",
    "checked": true,
    "id": "edb4858c3d016d3cac8928b42a8dde97f2de71df",
    "semantic_title": "continual event extraction with semantic confusion rectification",
    "citation_count": 0,
    "authors": [
      "Zitao Wang",
      "Xinyi Wang",
      "Wei Hu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.733": {
    "title": "An Empirical Study of Translation Hypothesis Ensembling with Large Language Models",
    "volume": "main",
    "abstract": "Large language models (LLMs) are becoming a one-fits-many solution, but they sometimes hallucinate or produce unreliable output. In this paper, we investigate how hypothesis ensembling can improve the quality of the generated text for the specific problem of LLM-based machine translation. We experiment with several techniques for ensembling hypotheses produced by LLMs such as ChatGPT, LLaMA, and Alpaca. We provide a comprehensive study along multiple dimensions, including the method to generate hypotheses (multiple prompts, temperature-based sampling, and beam search) and the strategy to produce the final translation (instruction-based, quality-based reranking, and minimum Bayes risk (MBR) decoding). Our results show that MBR decoding is a very effective method, that translation quality can be improved using a small number of samples, and that instruction tuning has a strong impact on the relation between the diversity of the hypotheses and the sampling temperature",
    "checked": true,
    "id": "4b98dd7341f6907107951dfa8be168c15b36f520",
    "semantic_title": "an empirical study of translation hypothesis ensembling with large language models",
    "citation_count": 0,
    "authors": [
      "António Farinhas",
      "José de Souza",
      "Andre Martins"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.734": {
    "title": "FedTherapist: Mental Health Monitoring with User-Generated Linguistic Expressions on Smartphones via Federated Learning",
    "volume": "main",
    "abstract": "Psychiatrists diagnose mental disorders via the linguistic use of patients. Still, due to data privacy, existing passive mental health monitoring systems use alternative features such as activity, app usage, and location via mobile devices. We propose FedTherapist, a mobile mental health monitoring system that utilizes continuous speech and keyboard input in a privacy-preserving way via federated learning. We explore multiple model designs by comparing their performance and overhead for FedTherapist to overcome the complex nature of on-device language model training on smartphones. We further propose a Context-Aware Language Learning (CALL) methodology to effectively utilize smartphones' large and noisy text for mental health signal sensing. Our IRB-approved evaluation of the prediction of self-reported depression, stress, anxiety, and mood from 46 participants shows higher accuracy of FedTherapist compared with the performance with non-language features, achieving 0.15 AUROC improvement and 8.21% MAE reduction",
    "checked": true,
    "id": "d9bc5cbda85ca2872c01bdf951e88e14d982e10b",
    "semantic_title": "fedtherapist: mental health monitoring with user-generated linguistic expressions on smartphones via federated learning",
    "citation_count": 0,
    "authors": [
      "Jaemin Shin",
      "Hyungjun Yoon",
      "Seungjoo Lee",
      "Sungjoon Park",
      "Yunxin Liu",
      "Jinho Choi",
      "Sung-Ju Lee"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.735": {
    "title": "Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models",
    "volume": "main",
    "abstract": "Recent advances in Large Language Models (LLMs) have stimulated a surge of research aimed at extending their applications to the visual domain. While these models exhibit promise in generating abstract image captions and facilitating natural conversations, their performance on text-rich images still requires improvement. In this paper, we introduce Contrastive Reading Model (Cream), a novel neural architecture designed to enhance the language-image understanding capability of LLMs by capturing intricate details that are often overlooked in existing methods. Cream combines vision and auxiliary encoders, fortified by a contrastive feature alignment technique, to achieve a more effective comprehension of language information in visually situated contexts within the images. Our approach bridges the gap between vision and language understanding, paving the way for the development of more sophisticated Document Intelligence Assistants. Through rigorous evaluations across diverse visually-situated language understanding tasks that demand reasoning capabilities, we demonstrate the compelling performance of Cream, positioning it as a prominent model in the field of visual document understanding. We provide our codebase and newly-generated datasets at https://github.com/naver-ai/cream",
    "checked": false,
    "id": "08b562aa8066c2342f0d03824221dea18f0a18d2",
    "semantic_title": "cream: visually-situated natural language understanding with contrastive reading model and frozen large language models",
    "citation_count": 2,
    "authors": [
      "Geewook Kim",
      "Hodong Lee",
      "Daehee Kim",
      "Haeji Jung",
      "Sanghee Park",
      "Yoonsik Kim",
      "Sangdoo Yun",
      "Taeho Kil",
      "Bado Lee",
      "Seunghyun Park"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.736": {
    "title": "Continual Learning for Multilingual Neural Machine Translation via Dual Importance-based Model Division",
    "volume": "main",
    "abstract": "A persistent goal of multilingual neural machine translation (MNMT) is to continually adapt the model to support new language pairs or improve some current language pairs without accessing the previous training data. To achieve this, the existing methods primarily focus on preventing catastrophic forgetting by making compromises between the original and new language pairs, leading to sub-optimal performance on both translation tasks. To mitigate this problem, we propose a dual importance-based model division method to divide the model parameters into two parts and separately model the translation of the original and new tasks. Specifically, we first remove the parameters that are negligible to the original tasks but essential to the new tasks to obtain a pruned model, which is responsible for the original translation tasks. Then we expand the pruned model with external parameters and fine-tune the newly added parameters with new training data. The whole fine-tuned model will be used for the new translation tasks. Experimental results show that our method can efficiently adapt the original model to various new translation tasks while retaining the performance of the original tasks. Further analyses demonstrate that our method consistently outperforms several strong baselines under different incremental translation scenarios",
    "checked": true,
    "id": "9782cdf9838991c96018b01d5184c40dc1b06068",
    "semantic_title": "continual learning for multilingual neural machine translation via dual importance-based model division",
    "citation_count": 0,
    "authors": [
      "Junpeng Liu",
      "Kaiyu Huang",
      "Hao Yu",
      "Jiuyi Li",
      "Jinsong Su",
      "Degen Huang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.737": {
    "title": "SimCSE++: Improving Contrastive Learning for Sentence Embeddings from Two Perspectives",
    "volume": "main",
    "abstract": "This paper improves contrastive learning for sentence embeddings from two perspectives: handling dropout noise and addressing feature corruption. Specifically, for the first perspective, we identify that the dropout noise from negative pairs affects the model's performance. Therefore, we propose a simple yet effective method to deal with such type of noise. Secondly, we pinpoint the rank bottleneck of current solutions to feature corruption and propose a dimension-wise contrastive learning objective to address this issue. Both proposed methods are generic and can be applied to any contrastive learning based models for sentence embeddings. Experimental results on standard benchmarks demonstrate that combining both proposed methods leads to a gain of 1.8 points compared to the strong baseline SimCSE configured with BERT base. Furthermore, applying the proposed method to DiffCSE, another strong contrastive learning based baseline, results in a gain of 1.4 points",
    "checked": false,
    "id": "22b8b6fa5a982a06cea1dbf446804fc4f69e1f9b",
    "semantic_title": "imsimcse: improving contrastive learning for sentence embeddings from two perspectives",
    "citation_count": 2,
    "authors": [
      "Jiahao Xu",
      "Wei Shao",
      "Lihui Chen",
      "Lemao Liu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.738": {
    "title": "Unlearn What You Want to Forget: Efficient Unlearning for LLMs",
    "volume": "main",
    "abstract": "Large language models (LLMs) have achieved significant progress from pre-training on and memorizing a wide range of textual data, however, this process might suffer from privacy issues and violations of data protection regulations. As a result, the ability to easily remove data related to individual users from such models while not deteriorating their predictive quality after the removal becomes increasingly important. To address these issues, in this work, we propose an efficient unlearning framework that could efficiently update LLMs without having to retrain the whole model after data removals, by introducing lightweight unlearning layers learned with a selective teacher-student objective into the transformers. In addition, we introduce a fusion mechanism to effectively combine different unlearning layers that learns to forget different sets of data to handle a sequence of forgetting operations. Experiments on classification and generation tasks demonstrate the effectiveness of our proposed methods compared to the state-of-the-art baselines",
    "checked": true,
    "id": "0399533de2d1d21f456663d1bd5355c8b3c32a58",
    "semantic_title": "unlearn what you want to forget: efficient unlearning for llms",
    "citation_count": 1,
    "authors": [
      "Jiaao Chen",
      "Diyi Yang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.739": {
    "title": "Simplicity Level Estimate (SLE): A Learned Reference-Less Metric for Sentence Simplification",
    "volume": "main",
    "abstract": "Automatic evaluation for sentence simplification remains a challenging problem. Most popular evaluation metrics require multiple high-quality references – something not readily available for simplification – which makes it difficult to test performance on unseen domains. Furthermore, most existing metrics conflate simplicity with correlated attributes such as fluency or meaning preservation. We propose a new learned evaluation metric — SLE — which focuses on simplicity, outperforming almost all existing metrics in terms of correlation with human judgements",
    "checked": true,
    "id": "1115e688bd80dd20af3a3bf28191d0229c8b8976",
    "semantic_title": "simplicity level estimate (sle): a learned reference-less metric for sentence simplification",
    "citation_count": 1,
    "authors": [
      "Liam Cripwell",
      "Joël Legrand",
      "Claire Gardent"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.740": {
    "title": "Precedent-Enhanced Legal Judgment Prediction with LLM and Domain-Model Collaboration",
    "volume": "main",
    "abstract": "Legal Judgment Prediction (LJP) has become an increasingly crucial task in Legal AI, i.e., predicting the judgment of the case in terms of case fact description. Precedents are the previous legal cases with similar facts, which are the basis for the judgment of the subsequent case in national legal systems. Thus, it is worthwhile to explore the utilization of precedents in the LJP. Recent advances in deep learning have enabled a variety of techniques to be used to solve the LJP task. These can be broken down into two categories: large language models (LLMs) and domain-specific models. LLMs are capable of interpreting and generating complex natural language, while domain models are efficient in learning task-specific information. In this paper, we propose the precedent-enhanced LJP framework (PLJP) – a system that leverages the strength of both LLM and domain models in the context of precedents. Specifically, the domain models are designed to provide candidate labels and find the proper precedents efficiently, and the large models will make the final prediction with an in-context precedents comprehension. Experiments on the real-world dataset demonstrate the effectiveness of our PLJP. Moreover, our work shows a promising direction for LLM and domain-model collaboration that can be generalized to other vertical domains",
    "checked": true,
    "id": "a80546c9847710af1ba8d5f8dca9386e7a520d0a",
    "semantic_title": "precedent-enhanced legal judgment prediction with llm and domain-model collaboration",
    "citation_count": 1,
    "authors": [
      "Yiquan Wu",
      "Siying Zhou",
      "Yifei Liu",
      "Weiming Lu",
      "Xiaozhong Liu",
      "Yating Zhang",
      "Changlong Sun",
      "Fei Wu",
      "Kun Kuang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.741": {
    "title": "FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation",
    "volume": "main",
    "abstract": "Evaluating the factuality of long-form text generated by large language models (LMs) is non-trivial because (1) generations often contain a mixture of supported and unsupported pieces of information, making binary judgments of quality inadequate, and (2) human evaluation is time-consuming and costly. In this paper, we introduce FACTSCORE, a new evaluation that breaks a generation into a series of atomic facts and computes the percentage of atomic facts supported by a reliable knowledge source. We conduct an extensive human evaluation to obtain FACTSCOREs of people biographies generated by several state-of-the-art commercial LMs—InstructGPT, ChatGPT, and the retrieval-augmented PerplexityAI—and report new analysis demonstrating the need for such a fine-grained score (e.g., ChatGPT only achieves 58%). Since human evaluation is costly, we also introduce an automated model that estimates FACTSCORE using retrieval and a strong language model, with less than a 2% error rate. Finally, we use this automated metric to evaluate 6,500 generations from a new set of 13 recent LMs that would have cost $26K if evaluated by humans, with various findings: GPT-4 and ChatGPT are more factual than public models, and Vicuna and Alpaca are some of the best public models. FACTSCORE is available for public use via ‘pip install factscore‘",
    "checked": true,
    "id": "bd5deadc58ee45b5e004378ba1d54a96bc947b4a",
    "semantic_title": "factscore: fine-grained atomic evaluation of factual precision in long form text generation",
    "citation_count": 64,
    "authors": [
      "Sewon Min",
      "Kalpesh Krishna",
      "Xinxi Lyu",
      "Mike Lewis",
      "Wen-tau Yih",
      "Pang Koh",
      "Mohit Iyyer",
      "Luke Zettlemoyer",
      "Hannaneh Hajishirzi"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.742": {
    "title": "Calc-X and Calcformers: Empowering Arithmetical Chain-of-Thought through Interaction with Symbolic Systems",
    "volume": "main",
    "abstract": "Despite outstanding performance in many tasks, language models are notoriously inclined to make factual errors in tasks requiring arithmetic computation. We address this deficiency by creating Calc-X, a collection of datasets that demonstrates the appropriate use of a calculator in reasoning chains. Calc-X is suitable for teaching language models to offload computations to a symbolic system. We survey and unify several existing chain-of-thought datasets into a proposed format, resulting in a standard collection of over 300,000 samples requiring arithmetic reasoning. Finally, we use the new Calc-X collection to train open-source calculator-using models and show that these models approximately double the accuracy of generating correct results compared to vanilla language model baselines",
    "checked": true,
    "id": "c79a04da127d6ff28c19c5dc35feb01feaf3d2b9",
    "semantic_title": "calc-x and calcformers: empowering arithmetical chain-of-thought through interaction with symbolic systems",
    "citation_count": 0,
    "authors": [
      "Marek Kadlčík",
      "Michal Štefánik",
      "Ondrej Sotolar",
      "Vlastimil Martinek"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.743": {
    "title": "CoF-CoT: Enhancing Large Language Models with Coarse-to-Fine Chain-of-Thought Prompting for Multi-domain NLU Tasks",
    "volume": "main",
    "abstract": "While Chain-of-Thought prompting is popular in reasoning tasks, its application to Large Language Models (LLMs) in Natural Language Understanding (NLU) is under-explored. Motivated by multi-step reasoning of LLMs, we propose Coarse-to-Fine Chain-of-Thought (CoF-CoT) approach that breaks down NLU tasks into multiple reasoning steps where LLMs can learn to acquire and leverage essential concepts to solve tasks from different granularities. Moreover, we propose leveraging semantic-based Abstract Meaning Representation (AMR) structured knowledge as an intermediate step to capture the nuances and diverse structures of utterances, and to understand connections between their varying levels of granularity. Our proposed approach is demonstrated effective in assisting the LLMs adapt to the multi-grained NLU tasks under both zero-shot and few-shot multi-domain settings",
    "checked": true,
    "id": "4ade9cb9f75236679029b6fd60d71c3bf513eedb",
    "semantic_title": "cof-cot: enhancing large language models with coarse-to-fine chain-of-thought prompting for multi-domain nlu tasks",
    "citation_count": 0,
    "authors": [
      "Hoang Nguyen",
      "Ye Liu",
      "Chenwei Zhang",
      "Tao Zhang",
      "Philip Yu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.744": {
    "title": "When Language Models Fall in Love: Animacy Processing in Transformer Language Models",
    "volume": "main",
    "abstract": "Animacy—whether an entity is alive and sentient—is fundamental to cognitive processing, impacting areas such as memory, vision, and language. However, animacy is not always expressed directly in language: in English it often manifests indirectly, in the form of selectional constraints on verbs and adjectives. This poses a potential issue for transformer language models (LMs): they often train only on text, and thus lack access to extralinguistic information from which humans learn about animacy. We ask: how does this impact LMs' animacy processing—do they still behave as humans do? We answer this question using open-source LMs. Like previous studies, we find that LMs behave much like humans when presented with entities whose animacy is typical. However, we also show that even when presented with stories about atypically animate entities, such as a peanut in love, LMs adapt: they treat these entities as animate, though they do not adapt as well as humans. Even when the context indicating atypical animacy is very short, LMs pick up on subtle clues and change their behavior. We conclude that despite the limited signal through which LMs can learn about animacy, they are indeed sensitive to the relevant lexical semantic nuances available in English",
    "checked": true,
    "id": "ae05bc8b1e9ca6061f8c8ec8c1b891105b131f40",
    "semantic_title": "when language models fall in love: animacy processing in transformer language models",
    "citation_count": 0,
    "authors": [
      "Michael Hanna",
      "Yonatan Belinkov",
      "Sandro Pezzelle"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.745": {
    "title": "Improving Unsupervised Relation Extraction by Augmenting Diverse Sentence Pairs",
    "volume": "main",
    "abstract": "Unsupervised relation extraction (URE) aims to extract relations between named entities from raw text without requiring manual annotations or pre-existing knowledge bases. In recent studies of URE, researchers put a notable emphasis on contrastive learning strategies for acquiring relation representations. However, these studies often overlook two important aspects: the inclusion of diverse positive pairs for contrastive learning and the exploration of appropriate loss functions. In this paper, we propose AugURE with both within-sentence pairs augmentation and augmentation through cross-sentence pairs extraction to increase the diversity of positive pairs and strengthen the discriminative power of contrastive learning. We also identify the limitation of noise-contrastive estimation (NCE) loss for relation representation learning and propose to apply margin loss for sentence pairs. Experiments on NYT-FB and TACRED datasets demonstrate that the proposed relation representation learning and a simple K-Means clustering achieves state-of-the-art performance",
    "checked": true,
    "id": "ac1a0d55e731fbd51c564497649e148eb6d1c41f",
    "semantic_title": "improving unsupervised relation extraction by augmenting diverse sentence pairs",
    "citation_count": 0,
    "authors": [
      "Qing Wang",
      "Kang Zhou",
      "Qiao Qiao",
      "Yuepei Li",
      "Qi Li"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.746": {
    "title": "Paraphrase Types for Generation and Detection",
    "volume": "main",
    "abstract": "Current approaches in paraphrase generation and detection heavily rely on a single general similarity score, ignoring the intricate linguistic properties of language. This paper introduces two new tasks to address this shortcoming by considering paraphrase types - specific linguistic perturbations at particular text positions. We name these tasks Paraphrase Type Generation and Paraphrase Type Detection. Our results suggest that while current techniques perform well in a binary classification scenario, i.e., paraphrased or not, the inclusion of fine-grained paraphrase types poses a significant challenge. While most approaches are good at generating and detecting general semantic similar content, they fail to understand the intrinsic linguistic variables they manipulate. Models trained in generating and identifying paraphrase types also show improvements in tasks without them. In addition, scaling these models further improves their ability to understand paraphrase types. We believe paraphrase types can unlock a new paradigm for developing paraphrase models and solving tasks in the future",
    "checked": true,
    "id": "c54ed3d69422a0a15cdc89b3f050cec254e99f11",
    "semantic_title": "paraphrase types for generation and detection",
    "citation_count": 0,
    "authors": [
      "Jan Wahle",
      "Bela Gipp",
      "Terry Ruas"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.747": {
    "title": "Target-to-Source Augmentation for Aspect Sentiment Triplet Extraction",
    "volume": "main",
    "abstract": "Aspect Sentiment Triplet Extraction (ASTE) is an important task in sentiment analysis, aiming to extract aspect-level opinions and sentiments from user-generated reviews. The fine-grained nature of ASTE incurs a high annotation cost, while the scarcity of annotated data limits the performance of existing methods. This paper exploits data augmentation to address this issue. Traditional augmentation methods typically modify the input sentences of existing samples via heuristic rules or language models, which have shown success in text classification tasks. However, applying these methods to fine-grained tasks like ASTE poses challenges in generating diverse augmented samples while maintaining alignment between modified sentences and origin labels. Therefore, this paper proposes a target-to-source augmentation approach for ASTE. Our approach focuses on learning a generator that can directly generate new sentences based on labels and syntactic templates. With this generator, we can generate a substantial number of diverse augmented samples by mixing labels and syntactic templates from different samples. Besides, to ensure the quality of the generated sentence, we introduce fluency and alignment discriminators to provide feedback on the generated sentence and then use this feedback to optimize the generator via a reinforcement learning framework. Experiments demonstrate that our approach significantly enhances the performance of existing ASTE models",
    "checked": true,
    "id": "c2cf1a5e0f31b0f3977edcdd297676c7da3ca2b7",
    "semantic_title": "target-to-source augmentation for aspect sentiment triplet extraction",
    "citation_count": 0,
    "authors": [
      "Yice Zhang",
      "Yifan Yang",
      "Meng Li",
      "Bin Liang",
      "Shiwei Chen",
      "Ruifeng Xu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.748": {
    "title": "PAC-tuning: Fine-tuning Pre-trained Language Models with PAC-driven Perturbed Gradient Descent",
    "volume": "main",
    "abstract": "Fine-tuning pretrained language models (PLMs) for downstream tasks is a large-scale optimization problem, in which the choice of the training algorithm critically determines how well the trained model can generalize to unseen test data, especially in the context of few-shot learning. To achieve good generalization performance and avoid overfitting, techniques such as data augmentation and pruning are often applied. However, adding these regularizations necessitates heavy tuning of the hyperparameters of optimization algorithms, such as the popular Adam optimizer. In this paper, we propose a two-stage fine-tuning method, PAC-tuning, to address this optimization challenge. First, based on PAC-Bayes training, PAC-tuning directly minimizes the PAC-Bayes generalization bound to learn proper parameter distribution. Second, PAC-tuning modifies the gradient by injecting noise with the variance learned in the first stage into the model parameters during training, resulting in a variant of perturbed gradient descent (PGD). In the past, the few-shot scenario posed difficulties for PAC-Bayes training because the PAC-Bayes bound, when applied to large models with limited training data, might not be stringent. Our experimental results across 5 GLUE benchmark tasks demonstrate that PAC-tuning successfully handles the challenges of fine-tuning tasks and outperforms strong baseline methods by a visible margin, further confirming the potential to apply PAC training for any other settings where the Adam optimizer is currently used for training",
    "checked": false,
    "id": "ed4e9c26f15fbb72c512aaf74e74b3495872591d",
    "semantic_title": "pac-tuning: fine-tuning pretrained language models with pac-driven perturbed gradient descent",
    "citation_count": 0,
    "authors": [
      "Guangliang Liu",
      "Zhiyu Xue",
      "Xitong Zhang",
      "Kristen Johnson",
      "Rongrong Wang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.749": {
    "title": "Emergence of Abstract State Representations in Embodied Sequence Modeling",
    "volume": "main",
    "abstract": "Decision making via sequence modeling aims to mimic the success of language models, where actions taken by an embodied agent are modeled as tokens to predict. Despite their promising performance, it remains unclear if embodied sequence modeling leads to the emergence of internal representations that represent the environmental state information. A model that lacks abstract state representations would be liable to make decisions based on surface statistics which fail to generalize. We take the BabyAI environment, a grid world in which language-conditioned navigation tasks are performed, and build a sequence modeling Transformer, which takes a language instruction, a sequence of actions, and environmental observations as its inputs. In order to investigate the emergence of abstract state representations, we design a \"blindfolded\" navigation task, where only the initial environmental layout, the language instruction, and the action sequence to complete the task are available for training. Our probing results show that intermediate environmental layouts can be reasonably reconstructed from the internal activations of a trained model, and that language instructions play a role in the reconstruction accuracy. Our results suggest that many key features of state representations can emerge via embodied sequence modeling, supporting an optimistic outlook for applications of sequence modeling objectives to more complex embodied decision-making domains",
    "checked": true,
    "id": "62b0b511b62bf3c2a63d4f425128cc08e55ad8a1",
    "semantic_title": "emergence of abstract state representations in embodied sequence modeling",
    "citation_count": 0,
    "authors": [
      "Tian Yun",
      "Zilai Zeng",
      "Kunal Handa",
      "Ashish Thapliyal",
      "Bo Pang",
      "Ellie Pavlick",
      "Chen Sun"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.750": {
    "title": "Accelerating Toeplitz Neural Network with Constant-time Inference Complexity",
    "volume": "main",
    "abstract": "Toeplitz Neural Networks (TNNs) have exhibited outstanding performance in various sequence modeling tasks. They outperform commonly used Transformer-based models while benefiting from log-linear space-time complexities. On the other hand, State Space Models (SSMs) achieve lower performance than TNNs in language modeling but offer the advantage of constant inference complexity. In this paper, we aim to combine the strengths of TNNs and SSMs by converting TNNs to SSMs during inference, thereby enabling TNNs to achieve the same constant inference complexities as SSMs. To accomplish this, we formulate the conversion process as an optimization problem and provide a closed-form solution. We demonstrate how to transform the target equation into a Vandermonde linear system problem, which can be efficiently solved using the Discrete Fourier Transform (DFT). Notably, our method requires no training and maintains numerical stability. It can be also applied to any LongConv-based model. To assess its effectiveness, we conduct extensive experiments on language modeling tasks across various settings. Additionally, we compare our method to other gradient-descent solutions, highlighting the superior numerical stability of our approach. The source code is available at https://github.com/OpenNLPLab/ETSC-Exact-Toeplitz-to-SSM-Conversion",
    "checked": true,
    "id": "0664d52b1040e048fff7e7d1d13a310964207768",
    "semantic_title": "accelerating toeplitz neural network with constant-time inference complexity",
    "citation_count": 1,
    "authors": [
      "Zhen Qin",
      "Yiran Zhong"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.751": {
    "title": "Dissecting Recall of Factual Associations in Auto-Regressive Language Models",
    "volume": "main",
    "abstract": "Transformer-based language models (LMs) are known to capture factual knowledge in their parameters. While previous work looked into where factual associations are stored, only little is known about how they are retrieved internally during inference. We investigate this question through the lens of information flow. Given a subject-relation query, we study how the model aggregates information about the subject and relation to predict the correct attribute. With interventions on attention edges, we first identify two critical points where information propagates to the prediction: one from the relation positions followed by another from the subject positions. Next, by analyzing the information at these points, we unveil a three-step internal mechanism for attribute extraction. First, the representation at the last-subject position goes through an enrichment process, driven by the early MLP sublayers, to encode many subject-related attributes. Second, information from the relation propagates to the prediction. Third, the prediction representation \"queries\" the enriched subject to extract the attribute. Perhaps surprisingly, this extraction is typically done via attention heads, which often encode subject-attribute mappings in their parameters. Overall, our findings introduce a comprehensive view of how factual associations are stored and extracted internally in LMs, facilitating future research on knowledge localization and editing",
    "checked": true,
    "id": "133b97e40017a9bbbadd10bcd7f13088a97ca3cc",
    "semantic_title": "dissecting recall of factual associations in auto-regressive language models",
    "citation_count": 36,
    "authors": [
      "Mor Geva",
      "Jasmijn Bastings",
      "Katja Filippova",
      "Amir Globerson"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.752": {
    "title": "StereoMap: Quantifying the Awareness of Human-like Stereotypes in Large Language Models",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have been observed to encode and perpetuate harmful associations present in the training data. We propose a theoretically grounded framework called StereoMap to gain insights into their perceptions of how demographic groups have been viewed by society. The framework is grounded in the Stereotype Content Model (SCM); a well-established theory from psychology. According to SCM, stereotypes are not all alike. Instead, the dimensions of Warmth and Competence serve as the factors that delineate the nature of stereotypes. Based on the SCM theory, StereoMap maps LLMs' perceptions of social groups (defined by socio-demographic features) using the dimensions of Warmth and Competence. Furthermore, the framework enables the investigation of keywords and verbalizations of reasoning of LLMs' judgments to uncover underlying factors influencing their perceptions. Our results show that LLMs exhibit a diverse range of perceptions towards these groups, characterized by mixed evaluations along the dimensions of Warmth and Competence. Furthermore, analyzing the reasonings of LLMs, our findings indicate that LLMs demonstrate an awareness of social disparities, often stating statistical data and research findings to support their reasoning. This study contributes to the understanding of how LLMs perceive and represent social groups, shedding light on their potential biases and the perpetuation of harmful associations",
    "checked": true,
    "id": "549da43aacc3ef5986a126dd9154b7772594b76b",
    "semantic_title": "stereomap: quantifying the awareness of human-like stereotypes in large language models",
    "citation_count": 0,
    "authors": [
      "Sullam Jeoung",
      "Yubin Ge",
      "Jana Diesner"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.753": {
    "title": "Select, Prompt, Filter: Distilling Large Language Models for Summarizing Conversations",
    "volume": "main",
    "abstract": "Large language models (LLMs) like ChatGPT can be expensive to train, deploy, and use for specific natural language generation tasks such as text summarization and for certain domains. A promising alternative is to fine-tune relatively smaller language models (LMs) on a particular task using high-quality, in-domain datasets. However, it can be prohibitively expensive to get such high-quality training data. This issue has been mitigated by generating weakly supervised data via knowledge distillation (KD) of LLMs. We propose a three-step approach to distill ChatGPT and fine-tune smaller LMs for summarizing forum conversations. More specifically, we design a method to selectively sample a large unannotated corpus of forum conversation using a semantic similarity metric. Then, we use the same metric to retrieve suitable prompts for ChatGPT from a small annotated validation set in the same domain. The generated dataset is then filtered to remove low-quality instances. Our proposed select-prompt-filter KD approach leads to significant improvements of up to 6.6 ROUGE-2 score by leveraging sufficient in-domain pseudo-labeled data over a standard KD approach given the same size of training data",
    "checked": true,
    "id": "356c118db396d3059811b567473d17369ef10eb8",
    "semantic_title": "select, prompt, filter: distilling large language models for summarizing conversations",
    "citation_count": 0,
    "authors": [
      "Minh-Quang Pham",
      "Sathish Indurthi",
      "Shamil Chollampatt",
      "Marco Turchi"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.754": {
    "title": "Human Raters Cannot Distinguish English Translations from Original English Texts",
    "volume": "main",
    "abstract": "The term translationese describes the set of linguistic features unique to translated texts, which appear regardless of translation quality. Though automatic classifiers designed to distinguish translated texts achieve high accuracy and prior work has identified common hallmarks of translationese, human accuracy of identifying translated text is understudied. In this work, we perform a human evaluation of English original/translated texts in order to explore raters' ability to classify texts as being original or translated English and the features that lead a rater to judge text as being translated. Ultimately, we find that, regardless of the annotators' native language or the source language of the text, annotators are unable to distinguish translations from original English texts and also have low agreement. Our results provide critical insight into work in translation studies and context for assessments of translationese classifiers",
    "checked": true,
    "id": "b181ac719bdb6cfa25a8e6df85f5cd5f4e86560c",
    "semantic_title": "human raters cannot distinguish english translations from original english texts",
    "citation_count": 0,
    "authors": [
      "Shira Wein"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.755": {
    "title": "Impressions: Visual Semiotics and Aesthetic Impact Understanding",
    "volume": "main",
    "abstract": "Is aesthetic impact different from beauty? Is visual salience a reflection of its capacity for effective communication? We present Impressions, a novel dataset through which to investigate the semiotics of images, and how specific visual features and design choices can elicit specific emotions, thoughts and beliefs. We posit that the impactfulness of an image extends beyond formal definitions of aesthetics, to its success as a communicative act, where style contributes as much to meaning formation as the subject matter. We also acknowledge that existing Image Captioning datasets are not designed to empower state-of-the-art architectures to model potential human impressions or interpretations of images. To fill this need, we design an annotation task heavily inspired by image analysis techniques in the Visual Arts to collect 1,440 image-caption pairs and 4,320 unique annotations exploring impact, pragmatic image description, impressions and aesthetic design choices. We show that existing multimodal image captioning and conditional generation models struggle to simulate plausible human responses to images. However, this dataset significantly improves their ability to model impressions and aesthetic evaluations of images through fine-tuning and few-shot adaptation",
    "checked": true,
    "id": "a893bc9d3f1b4e8644ed3082ab2bcfdb52c9afd5",
    "semantic_title": "impressions: visual semiotics and aesthetic impact understanding",
    "citation_count": 0,
    "authors": [
      "Julia Kruk",
      "Caleb Ziems",
      "Diyi Yang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.756": {
    "title": "DNA: Denoised Neighborhood Aggregation for Fine-grained Category Discovery",
    "volume": "main",
    "abstract": "Discovering fine-grained categories from coarsely labeled data is a practical and challenging task, which can bridge the gap between the demand for fine-grained analysis and the high annotation cost. Previous works mainly focus on instance-level discrimination to learn low-level features, but ignore semantic similarities between data, which may prevent these models learning compact cluster representations. In this paper, we propose Denoised Neighborhood Aggregation (DNA), a self-supervised framework that encodes semantic structures of data into the embedding space. Specifically, we retrieve k-nearest neighbors of a query as its positive keys to capture semantic similarities between data and then aggregate information from the neighbors to learn compact cluster representations, which can make fine-grained categories more separatable. However, the retrieved neighbors can be noisy and contain many false-positive keys, which can degrade the quality of learned embeddings. To cope with this challenge, we propose three principles to filter out these false neighbors for better representation learning. Furthermore, we theoretically justify that the learning objective of our framework is equivalent to a clustering loss, which can capture semantic similarities between data to form compact fine-grained clusters. Extensive experiments on three benchmark datasets show that our method can retrieve more accurate neighbors (21.31% accuracy improvement) and outperform state-of-the-art models by a large margin (average 9.96% improvement on three metrics). Our code and data are available at https://github.com/Lackel/DNA",
    "checked": true,
    "id": "d7edb8d07a7acabee7cc785e06f510261bc9365f",
    "semantic_title": "dna: denoised neighborhood aggregation for fine-grained category discovery",
    "citation_count": 0,
    "authors": [
      "Wenbin An",
      "Feng Tian",
      "Wenkai Shi",
      "Yan Chen",
      "Qinghua Zheng",
      "QianYing Wang",
      "Ping Chen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.757": {
    "title": "Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models",
    "volume": "main",
    "abstract": "The prompt-based learning paradigm, which bridges the gap between pre-training and fine-tuning, achieves state-of-the-art performance on several NLP tasks, particularly in few-shot settings. Despite being widely applied, prompt-based learning is vulnerable to backdoor attacks. Textual backdoor attacks are designed to introduce targeted vulnerabilities into models by poisoning a subset of training samples through trigger injection and label modification. However, they suffer from flaws such as abnormal natural language expressions resulting from the trigger and incorrect labeling of poisoned samples. In this study, we propose ProAttack, a novel and efficient method for performing clean-label backdoor attacks based on the prompt, which uses the prompt itself as a trigger. Our method does not require external triggers and ensures correct labeling of poisoned samples, improving the stealthy nature of the backdoor attack. With extensive experiments on rich-resource and few-shot text classification tasks, we empirically validate ProAttack's competitive performance in textual backdoor attacks. Notably, in the rich-resource setting, ProAttack achieves state-of-the-art attack success rates in the clean-label backdoor attack benchmark without external triggers",
    "checked": true,
    "id": "3def0d3624211ffb012f15835f2071e28766f2d3",
    "semantic_title": "prompt as triggers for backdoor attack: examining the vulnerability in language models",
    "citation_count": 12,
    "authors": [
      "Shuai Zhao",
      "Jinming Wen",
      "Anh Luu",
      "Junbo Zhao",
      "Jie Fu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.758": {
    "title": "UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) are popular for their impressive abilities, but the need for model-specific fine-tuning or task-specific prompt engineering can hinder their generalization. We propose UPRISE (Universal Prompt Retrieval for Improving zero-Shot Evaluation), which tunes a lightweight and versatile retriever that automatically retrieves prompts for a given zero-shot task input. Specifically, we demonstrate universality in a cross-task and cross-model scenario: the retriever is tuned on diverse tasks, but tested on unseen task types; we use a small frozen LLM, GPT-Neo-2.7B, for tuning the retriever, but test the retriever on different LLMs of much larger scales, such as BLOOM-7.1B, OPT-66B and GPT3-175B. Additionally, we show that UPRISE mitigates the hallucination problem in our experiments with ChatGPT, suggesting its potential to improve even the strongest LLMs. Our model and code are available at https://github.com/microsoft/LMOps",
    "checked": true,
    "id": "f7212245d3787c66b8dc1e9fa4bc48349cef1155",
    "semantic_title": "uprise: universal prompt retrieval for improving zero-shot evaluation",
    "citation_count": 10,
    "authors": [
      "Daixuan Cheng",
      "Shaohan Huang",
      "Junyu Bi",
      "Yuefeng Zhan",
      "Jianfeng Liu",
      "Yujing Wang",
      "Hao Sun",
      "Furu Wei",
      "Weiwei Deng",
      "Qi Zhang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.759": {
    "title": "KRLS: Improving End-to-End Response Generation in Task Oriented Dialog with Reinforced Keywords Learning",
    "volume": "main",
    "abstract": "In task-oriented dialogs (TOD), reinforcement learning (RL) algorithms train a model to directly optimize response for task-related metrics. However, RL often needs to perform exploration, which can be time-consuming due to the slow auto-regressive sequence generation process. We investigate an approach to create a more efficient RL-based algorithm to improve TOD performance in an offline setting. First, we use a faster generation procedure that samples from independent next-word distributions after training the language model (LM) with supervised learning. We then introduce a fine-grained reward function to help the model focus on learning key information in a dialog, by measuring the importance and semantic closeness of each generated token. Experiments on the MultiWoZ dataset show our new training algorithm, Keywords Reinforcement Learning with Next-word Sampling (KRLS), achieves state-of-the-art performance on the end-to-end response generation task, with a 15% training time reduction compared to a standard RL algorithm using auto-regressive generation",
    "checked": true,
    "id": "1660de28d9f1121e7f0116b92ed9e21e17eb30aa",
    "semantic_title": "krls: improving end-to-end response generation in task oriented dialog with reinforced keywords learning",
    "citation_count": 2,
    "authors": [
      "Xiao Yu",
      "Qingyang Wu",
      "Kun Qian",
      "Zhou Yu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.760": {
    "title": "Large Language Models Only Pass Primary School Exams in Indonesia: A Comprehensive Test on IndoMMLU",
    "volume": "main",
    "abstract": "Although large language models (LLMs) are often pre-trained on large-scale multilingual texts, their reasoning abilities and real-world knowledge are mainly evaluated based on English datasets. Assessing LLM capabilities beyond English is increasingly vital but hindered due to the lack of suitable datasets. In this work, we introduce IndoMMLU, the first multi-task language understanding benchmark for Indonesian culture and languages, which consists of questions from primary school to university entrance exams in Indonesia. By employing professional teachers, we obtain 14,981 questions across 64 tasks and education levels, with 46% of the questions focusing on assessing proficiency in the Indonesian language and knowledge of nine local languages and cultures in Indonesia. Our empirical evaluations show that GPT-3.5 only manages to pass the Indonesian primary school level, with limited knowledge of local Indonesian languages and culture. Other smaller models such as BLOOMZ and Falcon perform at even lower levels",
    "checked": true,
    "id": "3f421c7defc20c21e97abad5fa92887a06ec5df8",
    "semantic_title": "large language models only pass primary school exams in indonesia: a comprehensive test on indommlu",
    "citation_count": 1,
    "authors": [
      "Fajri Koto",
      "Nurul Aisyah",
      "Haonan Li",
      "Timothy Baldwin"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.761": {
    "title": "Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs",
    "volume": "main",
    "abstract": "A popular approach for improving the correctness of output from large language models (LLMs) is Self-Consistency - poll the LLM multiple times and output the most frequent solution. Existing Self-Consistency techniques always generate a constant number of samples per question, where a better approach will be to non-uniformly distribute the available budget based on the amount of agreement in the samples generated so far. In response, we introduce Adaptive-Consistency, a cost-efficient, model-agnostic technique that dynamically adjusts the number of samples per question using a lightweight stopping criterion. Our experiments over 17 reasoning and code generation datasets and three LLMs demonstrate that Adaptive-Consistency reduces sample budget by up to 7.9 times with an average accuracy drop of less than 0.1%",
    "checked": true,
    "id": "b4a6c010724f0459c9791018e34a982cf96987cf",
    "semantic_title": "let's sample step by step: adaptive-consistency for efficient reasoning and coding with llms",
    "citation_count": 11,
    "authors": [
      "Pranjal Aggarwal",
      "Aman Madaan",
      "Yiming Yang",
      "Mausam"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.762": {
    "title": "Bridging Information-Theoretic and Geometric Compression in Language Models",
    "volume": "main",
    "abstract": "For a language model (LM) to faithfully model human language, it must compress vast, potentially infinite information into relatively few dimensions. We propose analyzing compression in (pre-trained) LMs from two points of view: geometric and information-theoretic. We demonstrate that the two views are highly correlated, such that the intrinsic geometric dimension of linguistic data predicts their coding length under the LM. We then show that, in turn, high compression of a linguistic dataset predicts rapid adaptation to that dataset, confirming that being able to compress linguistic information is an important part of successful LM performance. As a practical byproduct of our analysis, we evaluate a battery of intrinsic dimension estimators for the first time on linguistic data, showing that only some encapsulate the relationship between information-theoretic compression, geometric compression, and ease-of-adaptation",
    "checked": true,
    "id": "89c2fee67544af9c8813b81c14dba44a78db3caa",
    "semantic_title": "bridging information-theoretic and geometric compression in language models",
    "citation_count": 2,
    "authors": [
      "Emily Cheng",
      "Corentin Kervadec",
      "Marco Baroni"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.763": {
    "title": "Pre-training Language Models for Comparative Reasoning",
    "volume": "main",
    "abstract": "Comparative reasoning is a process of comparing objects, concepts, or entities to draw conclusions, which constitutes a fundamental cognitive ability. In this paper, we propose a novel framework to pre-train language models for enhancing their abilities of comparative reasoning over texts. While there have been approaches for NLP tasks that require comparative reasoning, they suffer from costly manual data labeling and limited generalizability to different tasks. Our approach introduces a novel method of collecting scalable data for text-based entity comparison, which leverages both structured and unstructured data. Moreover, we present a framework of pre-training language models via three novel objectives on comparative reasoning. Evaluation on downstream tasks including comparative question answering, question generation, and summarization shows that our pre-training framework significantly improves the comparative reasoning abilities of language models, especially under low-resource conditions. This work also releases the first integrated benchmark for comparative reasoning",
    "checked": true,
    "id": "5b43168efb6ca1cf6d1035c6ee2cccd20ba16d5a",
    "semantic_title": "pre-training language models for comparative reasoning",
    "citation_count": 2,
    "authors": [
      "Mengxia Yu",
      "Zhihan Zhang",
      "Wenhao Yu",
      "Meng Jiang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.764": {
    "title": "Improved Pseudo Data for Machine Translation Quality Estimation with Constrained Beam Search",
    "volume": "main",
    "abstract": "Machine translation (MT) quality estimation (QE) is a crucial task to estimate the quality of MT outputs when reference translations are unavailable. Many studies focus on generating pseudo data using large parallel corpus and achieve remarkable success in the supervised setting. However, pseudo data solutions are less satisfying in unsupervised scenarios because the pseudo labels are inaccurate or the pseudo translations differ from the real ones. To address these problems, we propose to generate pseudo data using the MT model with constrained beam search (CBSQE). CBSQE preserves the reference parts with high MT probabilities as correct translations, while the rest parts as the wrong ones for MT generation. Therefore, CBSQE can reduce the false negative labels caused by synonyms. Overall, beam search will prefer a more real hypothesis with a higher MT generation likelihood. Extensive experiments demonstrate that CBSQE outperforms strong baselines in both supervised and unsupervised settings. Analyses further show the superiority of CBSQE. The code is available at https://github.com/NJUNLP/njuqe",
    "checked": true,
    "id": "ee947cd0f25a99bc718a3339c2b3dfbfccf95e91",
    "semantic_title": "improved pseudo data for machine translation quality estimation with constrained beam search",
    "citation_count": 0,
    "authors": [
      "Xiang Geng",
      "Yu Zhang",
      "Zhejian Lai",
      "Shuaijie She",
      "Wei Zou",
      "Shimin Tao",
      "Hao Yang",
      "Jiajun Chen",
      "Shujian Huang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.765": {
    "title": "Text Embeddings Reveal (Almost) As Much As Text",
    "volume": "main",
    "abstract": "How much private information do text embeddings reveal about the original text? We investigate the problem of embedding inversion, reconstructing the full text represented in dense text embeddings. We frame the problem as controlled generation: generating text that, when reembedded, is close to a fixed point in latent space. We find that although a naive model conditioned on the embedding performs poorly, a multi-step method that iteratively corrects and re-embeds text is able to recover 92% of 32-token text inputs exactly. We train our model to decode text embeddings from two state-of-the-art embedding models, and also show that our model can recover important personal information (full names) from a dataset of clinical notes",
    "checked": true,
    "id": "d4c4f46b63e4812f0268d99b6528aa6a0c404377",
    "semantic_title": "text embeddings reveal (almost) as much as text",
    "citation_count": 6,
    "authors": [
      "John Morris",
      "Volodymyr Kuleshov",
      "Vitaly Shmatikov",
      "Alexander Rush"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.766": {
    "title": "AutoTrial: Prompting Language Models for Clinical Trial Design",
    "volume": "main",
    "abstract": "Clinical trials are critical for drug development. Constructing the appropriate eligibility criteria (i.e., the inclusion/exclusion criteria for patient recruitment) is essential for the trial's success. Proper design of clinical trial protocols should consider similar precedent trials and their eligibility criteria to ensure sufficient patient coverage. In this paper, we present a method named AutoTrial to aid the design of clinical eligibility criteria using language models. It allows (1) controllable generation under instructions via a hybrid of discrete and neural prompting, (2) scalable knowledge incorporation via in-context learning, and (3) explicit reasoning chains to provide rationales for understanding the outputs. Experiments on over 70K clinical trials verify that AutoTrial generates high-quality criteria texts that are fluent and coherent and with high accuracy in capturing the relevant clinical concepts to the target trial. It is noteworthy that our method, with a much smaller parameter size, gains around 60% winning rate against the GPT-3.5 baselines via human evaluations",
    "checked": true,
    "id": "33727cfa2710e9f502480b7eb9ac1925cb3bc06b",
    "semantic_title": "autotrial: prompting language models for clinical trial design",
    "citation_count": 4,
    "authors": [
      "Zifeng Wang",
      "Cao Xiao",
      "Jimeng Sun"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.767": {
    "title": "Faster Minimum Bayes Risk Decoding with Confidence-based Pruning",
    "volume": "main",
    "abstract": "Minimum Bayes risk (MBR) decoding outputs the hypothesis with the highest expected utility over the model distribution for some utility function. It has been shown to improve accuracy over beam search in conditional language generation problems and especially neural machine translation, in both human and automatic evaluations. However, the standard sampling-based algorithm for MBR is substantially more computationally expensive than beam search, requiring a large number of samples as well as a quadratic number of calls to the utility function, limiting its applicability. We describe an algorithm for MBR which gradually grows the number of samples used to estimate the utility while pruning hypotheses that are unlikely to have the highest utility according to confidence estimates obtained with bootstrap sampling. Our method requires fewer samples and drastically reduces the number of calls to the utility function compared to standard MBR while being statistically indistinguishable in terms of accuracy. We demonstrate the effectiveness of our approach in experiments on three language pairs, using chrF++ and COMET as utility/evaluation metrics",
    "checked": true,
    "id": "1bf8c4f93a8663b424f0d132a169344c93ec5a5c",
    "semantic_title": "faster minimum bayes risk decoding with confidence-based pruning",
    "citation_count": 0,
    "authors": [
      "Julius Cheng",
      "Andreas Vlachos"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.768": {
    "title": "Enhancing Generative Retrieval with Reinforcement Learning from Relevance Feedback",
    "volume": "main",
    "abstract": "The recent advent of end-to-end generative retrieval marks a significant shift in document retrieval methods, leveraging differentiable search indexes to directly produce relevant document identifiers (docids) in response to a specific query. Nevertheless, this approach faces two fundamental challenges: (i) a discrepancy between the token-level probabilistic optimization and the broader document-level relevance estimation; (ii) an overemphasis on top-1 results at the expense of overall ranking quality. To tackle these challenges, we propose a generative retrieval model with reinforcement learning from relevance feedback, which aims to align token-level docid generation with document-level relevance estimation. The training process incorporates three stages: supervised fine-tuning, relevance reward model training, and reinforced learning-to-rank from relevance feedback. To train a high-quality reward model, we define \"relevance\" under three progressive scenarios, which collectively offer a comprehensive evaluation of the document relevance. Experiments conducted on two benchmark datasets demonstrate the effectiveness of our proposed approach",
    "checked": true,
    "id": "522513e8835650962c930993392f4d37610d9c64",
    "semantic_title": "enhancing generative retrieval with reinforcement learning from relevance feedback",
    "citation_count": 0,
    "authors": [
      "Yujia Zhou",
      "Zhicheng Dou",
      "Ji-Rong Wen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.769": {
    "title": "Multi-Source Probing for Open-Domain Conversational Understanding",
    "volume": "main",
    "abstract": "Dialogue comprehension and generation are vital to the success of open-domain dialogue systems. Although pre-trained generative conversation models have made significant progress in generating fluent responses, people have difficulty judging whether they understand and efficiently model the contextual information of the conversation. In this study, we propose a Multi-Source Probing (MSP) method to probe the dialogue comprehension abilities of open-domain dialogue models. MSP aggregates features from multiple sources to accomplish diverse task goals and conducts downstream tasks in a generative manner that is consistent with dialogue model pre-training to leverage model capabilities. We conduct probing experiments on seven tasks that require various dialogue comprehension skills, based on the internal representations encoded by dialogue models. Experimental results show that open-domain dialogue models can encode semantic information in the intermediate hidden states, which facilitates dialogue comprehension tasks. Models of different scales and structures possess different conversational understanding capabilities. Our findings encourage a comprehensive evaluation and design of open-domain dialogue models",
    "checked": true,
    "id": "2725d16fbdc8b1811a5517be2d6787555be30cd7",
    "semantic_title": "multi-source probing for open-domain conversational understanding",
    "citation_count": 0,
    "authors": [
      "Yuanxi Li",
      "Hao Zhou",
      "Jie Zhou",
      "Minlie Huang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.770": {
    "title": "Hallucination Mitigation in Natural Language Generation from Large-Scale Open-Domain Knowledge Graphs",
    "volume": "main",
    "abstract": "In generating natural language descriptions for knowledge graph triples, prior works used either small-scale, human-annotated datasets or datasets with limited variety of graph shapes, e.g., those having mostly star graphs. Graph-to-text models trained and evaluated on such datasets are largely not assessed for more realistic large-scale, open-domain settings. We introduce a new dataset, GraphNarrative, to fill this gap. Fine-tuning transformer-based pre-trained language models has achieved state-of-the-art performance among graph-to-text models. However, this method suffers from information hallucination—the generated text may contain fabricated facts not present in input graphs. We propose a novel approach that, given a graph-sentence pair in GraphNarrative, trims the sentence to eliminate portions that are not present in the corresponding graph, by utilizing the sentence's dependency parse tree. Our experiment results verify this approach using models trained on GraphNarrative and existing datasets. The dataset, source code, and trained models are released at https://github.com/idirlab/graphnarrator",
    "checked": true,
    "id": "3e3754ae5cca49679186d300f97accfea613ec53",
    "semantic_title": "hallucination mitigation in natural language generation from large-scale open-domain knowledge graphs",
    "citation_count": 0,
    "authors": [
      "Xiao Shi",
      "Zhengyuan Zhu",
      "Zeyu Zhang",
      "Chengkai Li"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.771": {
    "title": "Multi-Source Multi-Type Knowledge Exploration and Exploitation for Dialogue Generation",
    "volume": "main",
    "abstract": "Open-domain multi-turn dialogue generation encounters the significant challenge of lacking various types of knowledge from diverse sources. Existing models typically focus on identifying specific types of dialogue knowledge and utilize corresponding datasets for training. However, this approach often leads to limited generalization capabilities and increased computational resource requirements. Recently, large language models (LLMs) have shown impressive performance on natural language processing tasks. To harness the knowledge storage of LLMs, we propose a framework named KnowEE that explores multi-source multi-type knowledge from LLMs by leveraging diverse datasets and then exploits the obtained knowledge for response generation. Our framework comprises two phases: First, we leverage five external datasets encompassing various types of knowledge to extract the most relevant samples to the dialogue context which are served as prompts to generate corresponding type of knowledge; Second, we inject the acquired knowledge into the ongoing dialogue context in fine-grained and coarse-grained manners, which is then fed into LLMs to generate the final dialogue response. Both automatic and manual evaluation results validate the effectiveness of our framework in exploring and exploiting multi-source multi-type knowledge to generate coherent, informative, and fluent responses",
    "checked": true,
    "id": "c6d6a9394f2751a124dc24bf0dfa821b71c6aedf",
    "semantic_title": "multi-source multi-type knowledge exploration and exploitation for dialogue generation",
    "citation_count": 0,
    "authors": [
      "Xuanfan Ni",
      "Hongliang Dai",
      "Zhaochun Ren",
      "Piji Li"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.772": {
    "title": "Focus Your Attention (with Adaptive IIR Filters)",
    "volume": "main",
    "abstract": "We present a new layer in which dynamic (i.e., input-dependent) Infinite Impulse Response (IIR) filters of order two are used to process the input sequence prior to applying conventional attention. The input is split into chunks, and the coefficients of these filters are determined based on previous chunks to maintain causality. Despite their relatively low order, the causal adaptive filters are shown to focus attention on the relevant sequence elements. The new layer is grounded in control theory, and is shown to generalize diagonal state-space layers. The layer performs on-par with state-of-the-art networks, with a fraction of their parameters and with time complexity that is sub-quadratic with input size. The obtained layer is favorable to layers such as Heyna, GPT2, and Mega, both with respect to the number of parameters and the obtained level of performance on multiple long-range sequence problems",
    "checked": true,
    "id": "2d01b6afbc86cba1cb895dbcd9396b13952bf0e5",
    "semantic_title": "focus your attention (with adaptive iir filters)",
    "citation_count": 4,
    "authors": [
      "Shahar Lutati",
      "Itamar Zimerman",
      "Lior Wolf"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.773": {
    "title": "Identifying Statements Crucial for Awareness of Interpretive Nonsense to Prevent Communication Breakdowns",
    "volume": "main",
    "abstract": "During remote conversations, communication breakdowns often occur when a listener misses certain statements. Our objective is to prevent such breakdowns by identifying Statements Crucial for Awareness of Interpretive Nonsense (SCAINs). If a listener misses a SCAIN, s/he may interpret subsequent statements differently from the speaker's intended meaning. To identify SCAINs, we adopt a unique approach where we create a dialogue by omitting two consecutive statements from the original dialogue and then generate text to make the following statement more specific. The novelty of the proposed method lies in simulating missing information by processing text with omissions. We validate the effectiveness of SCAINs through evaluation using a dialogue dataset. Furthermore, we demonstrate that SCAINs cannot be identified as merely important statements, highlighting the uniqueness of our proposed method",
    "checked": true,
    "id": "64603e5cf081e404fc78192bdc263a8760ca6cd1",
    "semantic_title": "identifying statements crucial for awareness of interpretive nonsense to prevent communication breakdowns",
    "citation_count": 0,
    "authors": [
      "Tomoyuki Maekawa",
      "Michita Imai"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.774": {
    "title": "Multilingual Large Language Models Are Not (Yet) Code-Switchers",
    "volume": "main",
    "abstract": "Multilingual Large Language Models (LLMs) have recently shown great capabilities in a wide range of tasks, exhibiting state-of-the-art performance through zero-shot or few-shot prompting methods. While there have been extensive studies on their abilities in monolingual tasks, the investigation of their potential in the context of code-switching (CSW), the practice of alternating languages within an utterance, remains relatively uncharted. In this paper, we provide a comprehensive empirical analysis of various multilingual LLMs, benchmarking their performance across four tasks: sentiment analysis, machine translation, summarization and word-level language identification. Our results indicate that despite multilingual LLMs exhibiting promising outcomes in certain tasks using zero or few-shot prompting, they still underperform in comparison to fine-tuned models of much smaller scales. We argue that current \"multilingualism' in LLMs does not inherently imply proficiency with code-switching texts, calling for future research to bridge this discrepancy",
    "checked": true,
    "id": "eda54452d8a8a412c2a985ef11572cb468906b1f",
    "semantic_title": "multilingual large language models are not (yet) code-switchers",
    "citation_count": 8,
    "authors": [
      "Ruochen Zhang",
      "Samuel Cahyawijaya",
      "Jan Christian Blaise Cruz",
      "Genta Winata",
      "Alham Aji"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.775": {
    "title": "Reinforced Target-driven Conversational Promotion",
    "volume": "main",
    "abstract": "The ability to proactively engage with users towards pitching products is highly desired for conversational assistants. However, existing conversational recommendation methods overemphasize on acquiring user preferences while ignore the strategic planning for nudging users towards accepting a designated item. Hence, these methods fail to promote specified items with engaging responses. In this work, we propose a Reinforced Target-driven Conversational Promotion (RTCP) framework for conversational promotion. RTCP integrates short-term and long-term planning via a balanced gating mechanism. Inside which, the dialogue actions are predicted via a knowledge-integrated multi-head attention and guided via reinforcement learning rewards. RTCP then employs action-guided prefix tuning to generate relevant responses. Experimental results demonstrate that our model outperforms state-of-the-art models on both automatic metrics and human evaluation. Moreover, RTCP has a strong capability in quickly adapting to unseen scenarios just by updating prefix parameters without re-training the whole model",
    "checked": true,
    "id": "01578c581cebcb50e9abdf7817db09f66df3b354",
    "semantic_title": "reinforced target-driven conversational promotion",
    "citation_count": 0,
    "authors": [
      "Huy Dao",
      "Lizi Liao",
      "Dung Le",
      "Yuxiang Nie"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.776": {
    "title": "Identification of Multimodal Stance Towards Frames of Communication",
    "volume": "main",
    "abstract": "Frames of communication are often evoked in multimedia documents. When an author decides to add an image to a text, one or both of the modalities may evoke a communication frame. Moreover, when evoking the frame, the author also conveys her/his stance towards the frame. Until now, determining if the author is in favor of, against or has no stance towards the frame was performed automatically only when processing texts. This is due to the absence of stance annotations on multimedia documents. In this paper we introduce MMVax-Stance, a dataset of 11,300 multimedia documents retrieved from social media, which have stance annotations towards 113 different frames of communication. This dataset allowed us to experiment with several models of multimedia stance detection, which revealed important interactions between texts and images in the inference of stance towards communication frames. When inferring the text/image relations, a set of 46,606 synthetic examples of multimodal documents with known stance was generated. This greatly impacted the quality of identifying multimedia stance, yielding an improvement of 20% in F1-score",
    "checked": true,
    "id": "7bc0a5588648229d27ce4c08549f328cc39716fa",
    "semantic_title": "identification of multimodal stance towards frames of communication",
    "citation_count": 0,
    "authors": [
      "Maxwell Weinzierl",
      "Sanda Harabagiu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.777": {
    "title": "Unsupervised Sounding Pixel Learning",
    "volume": "main",
    "abstract": "Sounding source localization is a challenging cross-modal task due to the difficulty of cross-modal alignment. Although supervised cross-modal methods achieve encouraging performance, heavy manual annotations are expensive and inefficient. Thus it is valuable and meaningful to develop unsupervised solutions. In this paper, we propose an **U**nsupervised **S**ounding **P**ixel **L**earning (USPL) approach which enables a pixel-level sounding source localization in unsupervised paradigm. We first design a mask augmentation based multi-instance contrastive learning to realize unsupervised cross-modal coarse localization, which aligns audio-visual features to obtain coarse sounding maps. Secondly, we present an *Unsupervised Sounding Map Refinement (SMR)* module which employs the visual semantic affinity learning to explore inter-pixel relations of adjacent coordinate features. It contributes to recovering the boundary of coarse sounding maps and obtaining fine sounding maps. Finally, a *Sounding Pixel Segmentation (SPS)* module is presented to realize audio-supervised semantic segmentation. Extensive experiments are performed on the AVSBench-S4 and VGGSound datasets, exhibiting encouraging results compared with previous SOTA methods",
    "checked": true,
    "id": "98ec767523d3678eae18628289c89e758599226e",
    "semantic_title": "unsupervised sounding pixel learning",
    "citation_count": 0,
    "authors": [
      "Yining Zhang",
      "Yanli Ji",
      "Yang Yang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.778": {
    "title": "LM vs LM: Detecting Factual Errors via Cross Examination",
    "volume": "main",
    "abstract": "A prominent weakness of modern language models (LMs) is their tendency to generate factually incorrect text, which hinders their usability. A natural question is whether such factual errors can be detected automatically. Inspired by truth-seeking mechanisms in law, we propose a factuality evaluation framework for LMs that is based on cross-examination. Our key idea is that an incorrect claim is likely to result in inconsistency with other claims that the model generates. To discover such inconsistencies, we facilitate a multi-turn interaction between the LM that generated the claim and another LM (acting as an examiner) which introduces questions to discover inconsistencies. We empirically evaluate our method on factual claims made by multiple recent LMs on four benchmarks, finding that it outperforms existing methods and baselines, often by a large gap. Our results demonstrate the potential of using interacting LMs for capturing factual errors",
    "checked": true,
    "id": "ed0ed87161a2beab9e1bed3e783d7487a5f1062a",
    "semantic_title": "lm vs lm: detecting factual errors via cross examination",
    "citation_count": 23,
    "authors": [
      "Roi Cohen",
      "May Hamri",
      "Mor Geva",
      "Amir Globerson"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.779": {
    "title": "Large Language Models: The Need for Nuance in Current Debates and a Pragmatic Perspective on Understanding",
    "volume": "main",
    "abstract": "Current Large Language Models (LLMs) are unparalleled in their ability to generate grammatically correct, fluent text. LLMs are appearing rapidly, and debates on LLM capacities have taken off, but reflection is lagging behind. Thus, in this position paper, we first zoom in on the debate and critically assess three points recurring in critiques of LLM capacities: i) that LLMs only parrot statistical patterns in the training data; ii) that LLMs master formal but not functional language competence; and iii) that language learning in LLMs cannot inform human language learning. Drawing on empirical and theoretical arguments, we show that these points need more nuance. Second, we outline a pragmatic perspective on the issue of ‘real' understanding and intentionality in LLMs. Understanding and intentionality pertain to unobservable mental states we attribute to other humans because they have pragmatic value: they allow us to abstract away from complex underlying mechanics and predict behaviour effectively. We reflect on the circumstances under which it would make sense for humans to similarly attribute mental states to LLMs, thereby outlining a pragmatic philosophical context for LLMs as an increasingly prominent technology in society",
    "checked": true,
    "id": "dde6c1910d0496c9e5d5483c2c18271a2a660e6c",
    "semantic_title": "large language models: the need for nuance in current debates and a pragmatic perspective on understanding",
    "citation_count": 1,
    "authors": [
      "Bram van Dijk",
      "Tom Kouwenhoven",
      "Marco Spruit",
      "Max Johannes van Duijn"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.780": {
    "title": "PIEClass: Weakly-Supervised Text Classification with Prompting and Noise-Robust Iterative Ensemble Training",
    "volume": "main",
    "abstract": "Weakly-supervised text classification trains a classifier using the label name of each target class as the only supervision, which largely reduces human annotation efforts. Most existing methods first use the label names as static keyword-based features to generate pseudo labels, which are then used for final classifier training. While reasonable, such a commonly adopted framework suffers from two limitations: (1) keywords can have different meanings in different contexts and some text may not have any keyword, so keyword matching can induce noisy and inadequate pseudo labels; (2) the errors made in the pseudo label generation stage will directly propagate to the classifier training stage without a chance of being corrected. In this paper, we propose a new method, PIEClass, consisting of two modules: (1) a pseudo label acquisition module that uses zero-shot prompting of pre-trained language models (PLM) to get pseudo labels based on contextualized text understanding beyond static keyword matching, and (2) a noise-robust iterative ensemble training module that iteratively trains classifiers and updates pseudo labels by utilizing two PLM fine-tuning methods that regularize each other. Extensive experiments show that PIEClass achieves overall better performance than existing strong baselines on seven benchmark datasets and even achieves similar performance to fully-supervised classifiers on sentiment classification tasks",
    "checked": true,
    "id": "a5960c6674f26118e1e81b95d5c2482dce159bfb",
    "semantic_title": "pieclass: weakly-supervised text classification with prompting and noise-robust iterative ensemble training",
    "citation_count": 1,
    "authors": [
      "Yunyi Zhang",
      "Minhao Jiang",
      "Yu Meng",
      "Yu Zhang",
      "Jiawei Han"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.781": {
    "title": "MeaeQ: Mount Model Extraction Attacks with Efficient Queries",
    "volume": "main",
    "abstract": "We study model extraction attacks in natural language processing (NLP) where attackers aim to steal victim models by repeatedly querying the open Application Programming Interfaces (APIs). Recent works focus on limited-query budget settings and adopt random sampling or active learning-based sampling strategies on publicly available, unannotated data sources. However, these methods often result in selected queries that lack task relevance and data diversity, leading to limited success in achieving satisfactory results with low query costs. In this paper, we propose MeaeQ (Model extraction attack with efficient Queries), a straightforward yet effective method to address these issues. Specifically, we initially utilize a zero-shot sequence inference classifier, combined with API service information, to filter task-relevant data from a public text corpus instead of a problem domain-specific dataset. Furthermore, we employ a clustering-based data reduction technique to obtain representative data as queries for the attack. Extensive experiments conducted on four benchmark datasets demonstrate that MeaeQ achieves higher functional similarity to the victim model than baselines while requiring fewer queries",
    "checked": true,
    "id": "aa4acbad4d6a3a8195d70c174564df752a5e1ba5",
    "semantic_title": "meaeq: mount model extraction attacks with efficient queries",
    "citation_count": 0,
    "authors": [
      "Chengwei Dai",
      "Minxuan Lv",
      "Kun Li",
      "Wei Zhou"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.782": {
    "title": "The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning",
    "volume": "main",
    "abstract": "Language models (LMs) with less than 100B parameters are known to perform poorly on chain-of-thought (CoT) reasoning in contrast to large LMs when solving unseen tasks. In this work, we aim to equip smaller LMs with the step-by-step reasoning capability by instruction tuning with CoT rationales. In order to achieve this goal, we first introduce a new instruction-tuning dataset called the CoT Collection, which augments the existing Flan Collection (including only 9 CoT tasks) with additional 1.84 million rationales across 1,060 tasks. We show that CoT fine-tuning Flan-T5 (3B & 11B) with CoT Collection enables smaller LMs to have better CoT capabilities on unseen tasks. On the BIG-Bench-Hard (BBH) benchmark, we report an average improvement of +4.34% (Flan-T5 3B) and +2.60% (Flan-T5 11B), in terms of zero-shot task accuracy. Furthermore, we show that instruction tuning with CoT Collection allows LMs to possess stronger few-shot learning capabilities on 4 domain-specific tasks, resulting in an improvement of +2.24% (Flan-T5 3B) and +2.37% (Flan-T5 11B), even outperforming ChatGPT utilizing demonstrations until the max length by a +13.98% margin. Our code, the CoT Collection data, and model checkpoints are publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seungone Kim",
      "Se Joo",
      "Doyoung Kim",
      "Joel Jang",
      "Seonghyeon Ye",
      "Jamin Shin",
      "Minjoon Seo"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.783": {
    "title": "Explaining Interactions Between Text Spans",
    "volume": "main",
    "abstract": "Reasoning over spans of tokens from different parts of the input is essential for natural language understanding (NLU) tasks such as fact-checking (FC), machine reading comprehension (MRC) or natural language inference (NLI). However, existing highlight-based explanations primarily focus on identifying individual important features or interactions only between adjacent tokens or tuples of tokens. Most notably, there is a lack of annotations capturing the human decision-making process with respect to the necessary interactions for informed decision-making in such tasks. To bridge this gap, we introduce SpanEx, a multi-annotator dataset of human span interaction explanations for two NLU tasks: NLI and FC. We then investigate the decision-making processes of multiple fine-tuned large language models in terms of the employed connections between spans in separate parts of the input and compare them to the human reasoning processes. Finally, we present a novel community detection based unsupervised method to extract such interaction explanations. We make the code and the dataset available on [Github](https://github.com/copenlu/spanex). The dataset is also available on [Huggingface datasets](https://huggingface.co/datasets/copenlu/spanex)",
    "checked": true,
    "id": "a9b82fbbf4ceb9aaf4c4219a28468aefd4d6ae4f",
    "semantic_title": "explaining interactions between text spans",
    "citation_count": 0,
    "authors": [
      "Sagnik Choudhury",
      "Pepa Atanasova",
      "Isabelle Augenstein"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.784": {
    "title": "Predictive Chemistry Augmented with Text Retrieval",
    "volume": "main",
    "abstract": "This paper focuses on using natural language descriptions to enhance predictive models in the chemistry field. Conventionally, chemoinformatics models are trained with extensive structured data manually extracted from the literature. In this paper, we introduce TextReact, a novel method that directly augments predictive chemistry with texts retrieved from the literature. TextReact retrieves text descriptions relevant for a given chemical reaction, and then aligns them with the molecular representation of the reaction. This alignment is enhanced via an auxiliary masked LM objective incorporated in the predictor training. We empirically validate the framework on two chemistry tasks: reaction condition recommendation and one-step retrosynthesis. By leveraging text retrieval, TextReact significantly outperforms state-of-the-art chemoinformatics models trained solely on molecular data",
    "checked": true,
    "id": "e5c99ba5744eed11a42cc12ed1e8ddeeefe55ad3",
    "semantic_title": "predictive chemistry augmented with text retrieval",
    "citation_count": 0,
    "authors": [
      "Yujie Qian",
      "Zhening Li",
      "Zhengkai Tu",
      "Connor Coley",
      "Regina Barzilay"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.785": {
    "title": "System Combination via Quality Estimation for Grammatical Error Correction",
    "volume": "main",
    "abstract": "Quality estimation models have been developed to assess the corrections made by grammatical error correction (GEC) models when the reference or gold-standard corrections are not available. An ideal quality estimator can be utilized to combine the outputs of multiple GEC systems by choosing the best subset of edits from the union of all edits proposed by the GEC base systems. However, we found that existing GEC quality estimation models are not good enough in differentiating good corrections from bad ones, resulting in a low F0.5 score when used for system combination. In this paper, we propose GRECO, a new state-of-the-art quality estimation model that gives a better estimate of the quality of a corrected sentence, as indicated by having a higher correlation to the F0.5 score of a corrected sentence. It results in a combined GEC system with a higher F0.5 score. We also propose three methods for utilizing GEC quality estimation models for system combination with varying generality: model-agnostic, model-agnostic with voting bias, and model-dependent method. The combined GEC system outperforms the state of the art on the CoNLL-2014 test set and the BEA-2019 test set, achieving the highest F0.5 scores published to date",
    "checked": true,
    "id": "d4edaf10b8aa291fc8ec0015fc98467d3f9cd92b",
    "semantic_title": "system combination via quality estimation for grammatical error correction",
    "citation_count": 0,
    "authors": [
      "Muhammad Reza Qorib",
      "Hwee Tou Ng"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.786": {
    "title": "Rethinking Negative Pairs in Code Search",
    "volume": "main",
    "abstract": "Recently, contrastive learning has become a key component in fine-tuning code search models for software development efficiency and effectiveness. It pulls together positive code snippets while pushing negative samples away given search queries. Among contrastive learning, InfoNCE is the most widely used loss function due to its better performance. However, the following problems in negative samples of InfoNCE may deteriorate its representation learning: 1) The existence of false negative samples in large code corpora due to duplications. 2). The failure to explicitly differentiate between the potential relevance of negative samples. As an example, a bubble sorting algorithm example is less \"negative\" than a file saving function for the quick sorting algorithm query. In this paper, we tackle the above problems by proposing a simple yet effective Soft-InfoNCE loss that inserts weight terms into InfoNCE. In our proposed loss function, we apply three methods to estimate the weights of negative pairs and show that the vanilla InfoNCE loss is a special case of Soft-InfoNCE. Theoretically, we analyze the effects of Soft-InfoNCE on controlling the distribution of learnt code representations and on deducing a more precise mutual information estimation. We furthermore discuss the superiority of proposed loss functions with other design alternatives. Extensive experiments demonstrate the effectiveness of Soft-InfoNCE and weights estimation methods under state-of-the-art code search models on a large-scale public dataset consisting of six programming languages",
    "checked": true,
    "id": "00df7b6605a44251f524ef3bd8ce6b096e4864f5",
    "semantic_title": "rethinking negative pairs in code search",
    "citation_count": 1,
    "authors": [
      "Haochen Li",
      "Xin Zhou",
      "Anh Luu",
      "Chunyan Miao"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.787": {
    "title": "Question Answering as Programming for Solving Time-Sensitive Questions",
    "volume": "main",
    "abstract": "Question answering plays a pivotal role in human daily life because it involves our acquisition of knowledge about the world. However, due to the dynamic and ever-changing nature of real-world facts, the answer can be completely different when the time constraint in the question changes. Recently, Large Language Models (LLMs) have shown remarkable intelligence in question answering, while our experiments reveal that the aforementioned problems still pose a significant challenge to existing LLMs. This can be attributed to the LLMs' inability to perform rigorous reasoning based on surface-level text semantics. To overcome this limitation, rather than requiring LLMs to directly answer the question, we propose a novel approach where we reframe the Question Answering task as Programming (QAaP). Concretely, by leveraging modern LLMs' superior capability in understanding both natural language and programming language, we endeavor to harness LLMs to represent diversely expressed text as well-structured code and select the best matching answer from multiple candidates through programming. We evaluate our QAaP framework on several time-sensitive question answering datasets and achieve decent improvement, up to 14.5% over strong baselines",
    "checked": true,
    "id": "ae80c69872d4af5814d9d7dfa771c794a93697d3",
    "semantic_title": "question answering as programming for solving time-sensitive questions",
    "citation_count": 3,
    "authors": [
      "Xinyu Zhu",
      "Cheng Yang",
      "Bei Chen",
      "Siheng Li",
      "Jian-Guang Lou",
      "Yujiu Yang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.788": {
    "title": "Joint Geometrical and Statistical Domain Adaptation for Cross-domain Code Vulnerability Detection",
    "volume": "main",
    "abstract": "In code vulnerability detection tasks, a detector trained on a label-rich source domain fails to provide accurate prediction on new or unseen target domains due to the lack of labeled training data on target domains. Previous studies mainly utilize domain adaptation to perform cross-domain vulnerability detection. But they ignore the negative effect of private semantic characteristics of the target domain for domain alignment, which easily causes the problem of negative transfer. In addition, these methods forcibly reduce the distribution discrepancy between domains and do not take into account the interference of irrelevant target instances for distributional domain alignment, which leads to the problem of excessive alignment. To address the above issues, we propose a novel cross-domain code vulnerability detection framework named MNCRI. Specifically, we introduce mutual nearest neighbor contrastive learning to align the source domain and target domain geometrically, which could align the common semantic characteristics of two domains and separate out the private semantic characteristics of each domain. Furthermore, we introduce an instance re-weighting scheme to alleviate the problem of excessive alignment. This scheme dynamically assign different weights to instances, reducing the contribution of irrelevant instances so as to achieve better domain alignment. Finally, extensive experiments demonstrate that MNCRI significantly outperforms state-of-the-art cross-domain code vulnerability detection methods by a large margin",
    "checked": true,
    "id": "a2fc658840a7c5d399b8a765f403476fe8c1809c",
    "semantic_title": "joint geometrical and statistical domain adaptation for cross-domain code vulnerability detection",
    "citation_count": 0,
    "authors": [
      "Qianjin Du",
      "Shiji Zhou",
      "Xiaohui Kuang",
      "Gang Zhao",
      "Jidong Zhai"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.789": {
    "title": "Revisiting Sparse Retrieval for Few-shot Entity Linking",
    "volume": "main",
    "abstract": "Entity linking aims to link ambiguous mentions to their corresponding entities in a knowledge base. One of the key challenges comes from insufficient labeled data for specific domains. Although dense retrievers have achieved excellent performance on several benchmarks, their performance decreases significantly when only a limited amount of in-domain labeled data is available. In such few-shot setting, we revisit the sparse retrieval method, and propose an ELECTRA-based keyword extractor to denoise the mention context and construct a better query expression. For training the extractor, we propose a distant supervision method to automatically generate training data based on overlapping tokens between mention contexts and entity descriptions. Experimental results on the ZESHEL dataset demonstrate that the proposed method outperforms state-of-the-art models by a significant margin across all test domains, showing the effectiveness of keyword-enhanced sparse retrieval",
    "checked": true,
    "id": "3cbef1bcef028696b777ce23746c9cc4363b0103",
    "semantic_title": "revisiting sparse retrieval for few-shot entity linking",
    "citation_count": 0,
    "authors": [
      "Yulin Chen",
      "Zhenran Xu",
      "Baotian Hu",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.790": {
    "title": "Controlling Pre-trained Language Models for Grade-Specific Text Simplification",
    "volume": "main",
    "abstract": "Text simplification systems rewrite text to make it more readable while preserving its content. However, what makes a text easy to read depends on the intended readers. Recent work has shown that pre-trained language models can simplify text using a wealth of techniques to control output simplicity, ranging from specifying only the desired reading grade level, to directly specifying low-level edit operations. Yet it remains unclear how to set these control parameters in practice. Existing approaches set them at the corpus level, disregarding the complexity of individual inputs and considering only one level of output complexity. In this work, we conduct an empirical study to understand how different control mechanisms impact the adequacy and simplicity of text simplification systems. Based on these insights, we introduce a simple method that predicts the edit operations required for simplifying a text for a specific grade level on an instance-per-instance basis. This approach improves the quality of the simplified outputs over corpus-level search-based heuristics",
    "checked": true,
    "id": "b9ac34a8a2def0eab16b7ab49162b68c362c9dca",
    "semantic_title": "controlling pre-trained language models for grade-specific text simplification",
    "citation_count": 1,
    "authors": [
      "Sweta Agrawal",
      "Marine Carpuat"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.791": {
    "title": "CLEVR-Implicit: A Diagnostic Dataset for Implicit Reasoning in Referring Expression Comprehension",
    "volume": "main",
    "abstract": "Recently, pre-trained vision-language (VL) models have achieved remarkable success in various cross-modal tasks, including referring expression comprehension (REC). These models are pre-trained on the large-scale image-text pairs to learn the alignment between words in textual descriptions and objects in the corresponding images and then fine-tuned on downstream tasks. However, the performance of VL models is hindered when dealing with implicit text, which describes objects through comparisons between two or more objects rather than explicitly mentioning them. This is because the models struggle to align the implicit text with the objects in the images. To address the challenge, we introduce CLEVR-Implicit, a dataset consisting of synthetic images and corresponding two types of implicit text for the REC task. Additionally, to enhance the performance of VL models on implicit text, we propose a method called Transforming Implicit text into Explicit text (TIE), which enables VL models to reason with the implicit text. TIE consists of two modules: (1) the prompt design module builds prompts for implicit text by adding masked tokens, and (2) the cloze procedure module fine-tunes the prompts by utilizing masked language modeling (MLM) to predict the explicit words with the implicit prompts. Experimental results on our dataset demonstrate a significant improvement of 37.94% in the performance of VL models on implicit text after employing our TIE method",
    "checked": true,
    "id": "0a441af33196d514d904752243072c3fcfa7ce64",
    "semantic_title": "clevr-implicit: a diagnostic dataset for implicit reasoning in referring expression comprehension",
    "citation_count": 0,
    "authors": [
      "Jingwei Zhang",
      "Xin Wu",
      "Yi Cai"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.792": {
    "title": "Are Your Explanations Reliable?\" Investigating the Stability of LIME in Explaining Text Classifiers by Marrying XAI and Adversarial Attack",
    "volume": "main",
    "abstract": "LIME has emerged as one of the most commonly referenced tools in explainable AI (XAI) frameworks that is integrated into critical machine learning applications (e.g., healthcare and finance). However, its stability remains little explored, especially in the context of text data, due to the unique text-space constraints. To address these challenges, in this paper, we first evaluate the inherent instability of LIME on text data to establish a baseline, and then propose a novel algorithm XAIFooler to perturb text inputs and manipulate explanations that casts investigation on the stability of LIME as a text perturbation optimization problem. XAIFooler conforms to the constraints to preserve text semantics and original prediction with small perturbations, and introduces Rank-biased Overlap (RBO) as a key part to guide the optimization of XAIFooler that satisfies all the requirements for explanation similarity measure. Extensive experiments on real-world text datasets demonstrate that XAIFooler significantly outperforms all baselines by large margins in its ability to manipulate LIME's explanations with high semantic preservability",
    "checked": true,
    "id": "0d791a32241198d2f9ee2ca33aa719b5484785b6",
    "semantic_title": "are your explanations reliable?\" investigating the stability of lime in explaining text classifiers by marrying xai and adversarial attack",
    "citation_count": 0,
    "authors": [
      "Christopher Burger",
      "Lingwei Chen",
      "Thai Le"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.793": {
    "title": "CQE: A Comprehensive Quantity Extractor",
    "volume": "main",
    "abstract": "Quantities are essential in documents to describe factual information. They are ubiquitous in application domains such as finance, business, medicine, and science in general. Compared to other information extraction approaches, interestingly only a few works exist that describe methods for a proper extraction and representation of quantities in text. In this paper, we present such a comprehensive quantity extraction framework from text data. It efficiently detects combinations of values and units, the behavior of a quantity (e.g., rising or falling), and the concept a quantity is associated with. Our framework makes use of dependency parsing and a dictionary of units, and it provides for a proper normalization and standardization of detected quantities. Using a novel dataset for evaluation, we show that our open source framework outperforms other systems and – to the best of our knowledge – is the first to detect concepts associated with identified quantities. The code and data underlying our framework are available at https://github.com/vivkaz/CQE",
    "checked": true,
    "id": "b46d42da3dde7d134ba497ea387ec52bee432101",
    "semantic_title": "cqe: a comprehensive quantity extractor",
    "citation_count": 0,
    "authors": [
      "Satya Almasian",
      "Vivian Kazakova",
      "Philipp Göldner",
      "Michael Gertz"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.794": {
    "title": "Context Compression for Auto-regressive Transformers with Sentinel Tokens",
    "volume": "main",
    "abstract": "The quadratic complexity of the attention module makes it gradually become the bulk of compute in Transformer-based LLMs during generation. Moreover, the excessive key-value cache that arises when dealing with long inputs also brings severe issues on memory footprint and inference latency. In this work, we propose a plug-and-play approach that is able to incrementally compress the intermediate activation of a specified span of tokens into compact ones, thereby reducing both memory and computational cost when processing subsequent context. Experiments on both in-domain language modeling and zero-shot open-ended document generation demonstrate the advantage of our approach over sparse attention baselines in terms of fluency, n-gram matching, and semantic similarity. At last, we comprehensively profile the benefit of context compression on improving the system throughout. Code is available at https://github.com/DRSY/KV_Compression",
    "checked": true,
    "id": "c43e75cbbf06a51a128ed41e5558db19f4ebbb45",
    "semantic_title": "context compression for auto-regressive transformers with sentinel tokens",
    "citation_count": 0,
    "authors": [
      "Siyu Ren",
      "Qi Jia",
      "Kenny Zhu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.795": {
    "title": "A Unified View of Evaluation Metrics for Structured Prediction",
    "volume": "main",
    "abstract": "We present a conceptual framework that unifies a variety of evaluation metrics for different structured prediction tasks (e.g. event and relation extraction, syntactic and semantic parsing). Our framework requires representing the outputs of these tasks as objects of certain data types, and derives metrics through matching of common substructures, possibly followed by normalization. We demonstrate how commonly used metrics for a number of tasks can be succinctly expressed by this framework, and show that new metrics can be naturally derived in a bottom-up way based on an output structure. We release a library that enables this derivation to create new metrics. Finally, we consider how specific characteristics of tasks motivate metric design decisions, and suggest possible modifications to existing metrics in line with those motivations",
    "checked": true,
    "id": "a0df8169889043dae6ac111136a61162a5185a77",
    "semantic_title": "a unified view of evaluation metrics for structured prediction",
    "citation_count": 1,
    "authors": [
      "Yunmo Chen",
      "William Gantt",
      "Tongfei Chen",
      "Aaron White",
      "Benjamin Van Durme"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.796": {
    "title": "A Deeper (Autoregressive) Approach to Non-Convergent Discourse Parsing",
    "volume": "main",
    "abstract": "Online social platforms provide a bustling arena for information-sharing and for multi-party discussions. Various frameworks for dialogic discourse parsing were developed and used for the processing of discussions and for predicting the productivity of a dialogue. However, most of these frameworks are not suitable for the analysis of contentious discussions that are commonplace in many online platforms. A novel multi-label scheme for contentious dialog parsing was recently introduced by Zakharov et al. (2021). While the schema is well developed, the computational approach they provide is both naive and inefficient, as a different model (architecture) using a different representation of the input, is trained for each of the 31 tags in the annotation scheme. Moreover, all their models assume full knowledge of label collocations and context, which is unlikely in any realistic setting. In this work, we present a unified model for Non-Convergent Discourse Parsing that does not require any additional input other than the previous dialog utterances. We fine-tuned a RoBERTa backbone, combining embeddings of the utterance, the context and the labels through GRN layers and an asymmetric loss function. Overall, our model achieves results comparable with SOTA, without using label collocation and without training a unique architecture/model for each label. Our proposed architecture makes the labeling feasible at large scale, promoting the development of tools that deepen our understanding of discourse dynamics",
    "checked": true,
    "id": "78531d8d3b5d99be400d4ce5db9244acda229471",
    "semantic_title": "a deeper (autoregressive) approach to non-convergent discourse parsing",
    "citation_count": 0,
    "authors": [
      "Oren Tsur",
      "Yoav Tulpan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.797": {
    "title": "We are Who We Cite: Bridges of Influence Between Natural Language Processing and Other Academic Fields",
    "volume": "main",
    "abstract": "Natural Language Processing (NLP) is poised to substantially influence the world. However, significant progress comes hand-in-hand with substantial risks. Addressing them requires broad engagement with various fields of study. Yet, little empirical work examines the state of such engagement (past or current). In this paper, we quantify the degree of influence between 23 fields of study and NLP (on each other). We analyzed ~77k NLP papers, ~3.1m citations from NLP papers to other papers, and ~1.8m citations from other papers to NLP papers. We show that, unlike most fields, the cross-field engagement of NLP, measured by our proposed Citation Field Diversity Index (CFDI), has declined from 0.58 in 1980 to 0.31 in 2022 (an all-time low). In addition, we find that NLP has grown more insular—citing increasingly more NLP papers and having fewer papers that act as bridges between fields. NLP citations are dominated by computer science; Less than 8% of NLP citations are to linguistics, and less than 3% are to math and psychology. These findings underscore NLP's urgent need to reflect on its engagement with various fields",
    "checked": true,
    "id": "712aa254275c8d888f73f3fe65f83ef0bce76271",
    "semantic_title": "we are who we cite: bridges of influence between natural language processing and other academic fields",
    "citation_count": 1,
    "authors": [
      "Jan Wahle",
      "Terry Ruas",
      "Mohamed Abdalla",
      "Bela Gipp",
      "Saif Mohammad"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.798": {
    "title": "Ties Matter: Meta-Evaluating Modern Metrics with Pairwise Accuracy and Tie Calibration",
    "volume": "main",
    "abstract": "Kendall's tau is frequently used to meta-evaluate how well machine translation (MT) evaluation metrics score individual translations. Its focus on pairwise score comparisons is intuitive but raises the question of how ties should be handled, a gray area that has motivated different variants in the literature. We demonstrate that, in settings like modern MT meta-evaluation, existing variants have weaknesses arising from their handling of ties, and in some situations can even be gamed. We propose instead to meta-evaluate metrics with a version of pairwise accuracy that gives metrics credit for correctly predicting ties, in combination with a tie calibration procedure that automatically introduces ties into metric scores, enabling fair comparison between metrics that do and do not predict ties. We argue and provide experimental evidence that these modifications lead to fairer ranking-based assessments of metric performance",
    "checked": true,
    "id": "f24ece8dc010132cad31c1ed6aa2cc4978777d73",
    "semantic_title": "ties matter: meta-evaluating modern metrics with pairwise accuracy and tie calibration",
    "citation_count": 1,
    "authors": [
      "Daniel Deutsch",
      "George Foster",
      "Markus Freitag"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.799": {
    "title": "SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization",
    "volume": "main",
    "abstract": "Data scarcity has been a long standing issue in the field of open-domain social dialogue. To quench this thirst, we present SODA: the first publicly available, million-scale high-quality social dialogue dataset. By contextualizing social commonsense knowledge from a knowledge graph, we are able to distill an exceptionally broad spectrum of social interactions from a large language model. Human evaluation shows that conversations in SODA are more consistent, specific, and (surprisingly) natural than those in prior human-authored datasets. Using SODA, we train COSMO: a generalizable conversation model that is significantly more natural and consistent on unseen datasets than best-performing conversation models (e.g., GODEL, BlenderBot-1, Koala, Vicuna). Experiments reveal COSMO is sometimes even preferred to the original human-written gold responses. Additionally, our results shed light on the distinction between knowledge-enriched conversations and natural social chitchats. We plan to make our data, model, and code public",
    "checked": true,
    "id": "f78fe02f681a0a9a6867b007bd39e3884de64a91",
    "semantic_title": "soda: million-scale dialogue distillation with social commonsense contextualization",
    "citation_count": 54,
    "authors": [
      "Hyunwoo Kim",
      "Jack Hessel",
      "Liwei Jiang",
      "Peter West",
      "Ximing Lu",
      "Youngjae Yu",
      "Pei Zhou",
      "Ronan Bras",
      "Malihe Alikhani",
      "Gunhee Kim",
      "Maarten Sap",
      "Yejin Choi"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.800": {
    "title": "Multi-view Contrastive Learning for Entity Typing over Knowledge Graphs",
    "volume": "main",
    "abstract": "Knowledge graph entity typing (KGET) aims at inferring plausible types of entities in knowledge graphs. Existing approaches to KGET focus on how to better encode the knowledge provided by the neighbors and types of an entity into its representation. However, they ignore the semantic knowledge provided by the way in which types can be clustered together. In this paper, we propose a novel method called Multi-view Contrastive Learning for knowledge graph Entity Typing MCLET, which effectively encodes the coarse-grained knowledge provided by clusters into entity and type embeddings. MCLET is composed of three modules: i) Multi-view Generation and Encoder module, which encodes structured information from entity-type, entity-cluster and cluster-type views; ii) Cross-view Contrastive Learning module, which encourages different views to collaboratively improve view-specific representations of entities and types; iii) Entity Typing Prediction module, which integrates multi-head attention and a Mixture-of-Experts strategy to infer missing entity types. Extensive experiments show the strong performance of MCLET compared to the state-of-the-art",
    "checked": true,
    "id": "457f8b9c005eba90063a7480159898dd8f8dd8ca",
    "semantic_title": "multi-view contrastive learning for entity typing over knowledge graphs",
    "citation_count": 0,
    "authors": [
      "Zhiwei Hu",
      "Victor Basulto",
      "Zhiliang Xiang",
      "Ru Li",
      "Jeff Pan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.801": {
    "title": "MailEx: Email Event and Argument Extraction",
    "volume": "main",
    "abstract": "In this work, we present the first dataset, MailEx, for performing event extraction from conversational email threads. To this end, we first proposed a new taxonomy covering 10 event types and 76 arguments in the email domain. Our final dataset includes 1.5K email threads and ~4K emails, which are annotated with a total of ~8K event instances. To understand the task challenges, we conducted a series of experiments comparing three types of approaches, i.e., fine-tuned sequence labeling, fine-tuned generative extraction, and few-shot in-context learning. Our results showed that the task of email event extraction is far from being addressed, due to challenges lying in, e.g., extracting non-continuous, shared trigger spans, extracting non-named entity arguments, and modeling the email conversational history. Our work thus suggests more future investigations in this domain-specific event extraction task",
    "checked": true,
    "id": "d4d9274a8968b97c95d94dd14c41c860a1ac9a1f",
    "semantic_title": "mailex: email event and argument extraction",
    "citation_count": 0,
    "authors": [
      "Saurabh Srivastava",
      "Gaurav Singh",
      "Shou Matsumoto",
      "Ali Raz",
      "Paulo Costa",
      "Joshua Poore",
      "Ziyu Yao"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.802": {
    "title": "Optimized Tokenization for Transcribed Error Correction",
    "volume": "main",
    "abstract": "The challenges facing speech recognition systems, such as variations in pronunciations, adverse audio conditions, and the scarcity of labeled data, emphasize the necessity for a post-processing step that corrects recurring errors. Previous research has shown the advantages of employing dedicated error correction models, yet training such models requires large amounts of labeled data which is not easily obtained. To overcome this limitation, synthetic transcribed-like data is often utilized, however, bridging the distribution gap between transcribed errors and synthetic noise is not trivial. In this paper, we demonstrate that the performance of correction models can be significantly increased by training solely using synthetic data. Specifically, we empirically show that: (1) synthetic data generated using the error distribution derived from a set of transcribed data outperforms the common approach of applying random perturbations; (2) applying language-specific adjustments to the vocabulary of a BPE tokenizer strike a balance between adapting to unseen distributions and retaining knowledge of transcribed errors. We showcase the benefits of these key observations, and evaluate our approach using multiple languages, speech recognition systems and prominent speech recognition datasets",
    "checked": true,
    "id": "cd70bbd80f4e07b18ee2cc339e13feb15b87e934",
    "semantic_title": "optimized tokenization for transcribed error correction",
    "citation_count": 0,
    "authors": [
      "Tomer Wullach",
      "Shlomo Chazan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.803": {
    "title": "Beware of Model Collapse! Fast and Stable Test-time Adaptation for Robust Question Answering",
    "volume": "main",
    "abstract": "Although pre-trained language models (PLM) have achieved great success in question answering (QA), their robustness is still insufficient to support their practical applications, especially in the face of distribution shifts. Recently, test-time adaptation (TTA) has shown great potential for solving this problem, which adapts the model to fit the test samples at test time. However, TTA sometimes causes model collapse, making almost all the model outputs incorrect, which has raised concerns about its stability and reliability. In this paper, we delve into why TTA causes model collapse and find that the imbalanced label distribution inherent in QA is the reason for it. To address this problem, we propose Anti-Collapse Fast test-time adaptation (Anti-CF), which utilizes the source model‘s output to regularize the update of the adapted model during test time. We further design an efficient side block to reduce its inference time. Extensive experiments on various distribution shift scenarios and pre-trained language models (e.g., XLM-RoBERTa, BLOOM) demonstrate that our method can achieve comparable or better results than previous TTA methods at a speed close to vanilla forward propagation, which is 1.8× to 4.4× speedup compared to previous TTA methods",
    "checked": true,
    "id": "58f51bf21c02babc968541abad7fc6566f92b2ea",
    "semantic_title": "beware of model collapse! fast and stable test-time adaptation for robust question answering",
    "citation_count": 0,
    "authors": [
      "Yi Su",
      "Yixin Ji",
      "Juntao Li",
      "Hai Ye",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.804": {
    "title": "Generative Adversarial Training with Perturbed Token Detection for Model Robustness",
    "volume": "main",
    "abstract": "Adversarial training is the dominant strategy towards model robustness. Current adversarial training methods typically apply perturbations to embedding representations, whereas actual text-based attacks introduce perturbations as discrete tokens. Thus there exists a gap between the continuous embedding representations and discrete text tokens that hampers the effectiveness of adversarial training. Moreover, the continuous representations of perturbations cannot be further utilized, resulting in the suboptimal performance. To bridge this gap for adversarial robustness, in this paper, we devise a novel generative adversarial training framework that integrates gradient-based learning, adversarial example generation and perturbed token detection. Our proposed framework consists of generative adversarial attack and adversarial training process. Specifically, in generative adversarial attack, the embeddings are shared between the classifier and the generative model, which enables the generative model to leverage the gradients from the classifier for generating perturbed tokens. Then, adversarial training process combines adversarial regularization with perturbed token detection to provide token-level supervision and improve the efficiency of sample utilization. Extensive experiments on five datasets from the AdvGLUE benchmark demonstrate that our framework significantly enhances the model robustness, surpassing the state-of-the-art results of ChatGPT by 10% in average accuracy",
    "checked": true,
    "id": "ba98be8d1720f42efbfd82ad9b61db16c4ce6feb",
    "semantic_title": "generative adversarial training with perturbed token detection for model robustness",
    "citation_count": 0,
    "authors": [
      "Jiahao Zhao",
      "Wenji Mao"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.805": {
    "title": "Multi-Task Knowledge Distillation with Embedding Constraints for Scholarly Keyphrase Boundary Classification",
    "volume": "main",
    "abstract": "The task of scholarly keyphrase boundary classification aims at identifying keyphrases from scientific papers and classifying them with their types from a set of predefined classes (e.g., task, process, or material). Despite the importance of keyphrases and their types in many downstream applications including indexing, searching, and question answering over scientific documents, scholarly keyphrase boundary classification is still an under-explored task. In this work, we propose a novel embedding constraint on multi-task knowledge distillation which enforces the teachers (single-task models) and the student (multi-task model) similarity in the embedding space. Specifically, we enforce that the student model is trained not only to imitate the teachers' output distribution over classes, but also to produce language representations that are similar to those produced by the teachers. Our results show that the proposed approach outperforms previous works and strong baselines on three datasets of scientific documents",
    "checked": true,
    "id": "8186fcf6f489144c711419f53e9fb48ada01ca11",
    "semantic_title": "multi-task knowledge distillation with embedding constraints for scholarly keyphrase boundary classification",
    "citation_count": 0,
    "authors": [
      "Seo Park",
      "Cornelia Caragea"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.806": {
    "title": "Set Learning for Generative Information Extraction",
    "volume": "main",
    "abstract": "Recent efforts have endeavored to employ the sequence-to-sequence (Seq2Seq) model in Information Extraction (IE) due to its potential to tackle multiple IE tasks in a unified manner. Under this formalization, multiple structured objects are concatenated as the target sequence in a predefined order. However, structured objects, by their nature, constitute an unordered set. Consequently, this formalization introduces a potential order bias, which can impair model learning. Targeting this issue, this paper proposes a set learning approach that considers multiple permutations of structured objects to optimize set probability approximately. Notably, our approach does not require any modifications to model structures, making it easily integrated into existing generative IE frameworks. Experiments show that our method consistently improves existing frameworks on vast tasks and datasets",
    "checked": true,
    "id": "ad9abd188a431f4135a3d323b8e36e7bab37910f",
    "semantic_title": "set learning for generative information extraction",
    "citation_count": 0,
    "authors": [
      "Jiangnan Li",
      "Yice Zhang",
      "Bin Liang",
      "Kam-Fai Wong",
      "Ruifeng Xu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.807": {
    "title": "Large Language Models and Multimodal Retrieval for Visual Word Sense Disambiguation",
    "volume": "main",
    "abstract": "Visual Word Sense Disambiguation (VWSD) is a novel challenging task with the goal of retrieving an image among a set of candidates, which better represents the meaning of an ambiguous word within a given context. In this paper, we make a substantial step towards unveiling this interesting task by applying a varying set of approaches. Since VWSD is primarily a text-image retrieval task, we explore the latest transformer-based methods for multimodal retrieval. Additionally, we utilize Large Language Models (LLMs) as knowledge bases to enhance the given phrases and resolve ambiguity related to the target word. We also study VWSD as a unimodal problem by converting to text-to-text and image-to-image retrieval, as well as question-answering (QA), to fully explore the capabilities of relevant models. To tap into the implicit knowledge of LLMs, we experiment with Chain-of-Thought (CoT) prompting to guide explainable answer generation. On top of all, we train a learn to rank (LTR) model in order to combine our different modules, achieving competitive ranking results. Extensive experiments on VWSD demonstrate valuable insights to effectively drive future directions",
    "checked": true,
    "id": "b2a0f97d9135f91f28196e1f8adea9ac79b230c2",
    "semantic_title": "large language models and multimodal retrieval for visual word sense disambiguation",
    "citation_count": 0,
    "authors": [
      "Anastasia Kritharoula",
      "Maria Lymperaiou",
      "Giorgos Stamou"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.808": {
    "title": "Be Selfish, But Wisely: Investigating the Impact of Agent Personality in Mixed-Motive Human-Agent Interactions",
    "volume": "main",
    "abstract": "A natural way to design a negotiation dialogue system is via self-play RL: train an agent that learns to maximize its performance by interacting with a simulated user that has been designed to imitate human-human dialogue data. Although this procedure has been adopted in prior work, we find that it results in a fundamentally flawed system that fails to learn the value of compromise in a negotiation, which can often lead to no agreements (i.e., the partner walking away without a deal), ultimately hurting the model's overall performance. We investigate this observation in the context of DealOrNoDeal task, a multi-issue negotiation over books, hats, and balls. Grounded in negotiation theory from Economics, we modify the training procedure in two novel ways to design agents with diverse personalities and analyze their performance with human partners. We find that although both techniques show promise, a selfish agent, which maximizes its own performance while also avoiding walkaways, performs superior to other variants by implicitly learning to generate value for both itself and the negotiation partner. We discuss the implications of our findings for what it means to be a successful negotiation dialogue system and how these systems should be designed in the future",
    "checked": true,
    "id": "b50e230cc783df3dbdee179c5adf7f20ab3df9b8",
    "semantic_title": "be selfish, but wisely: investigating the impact of agent personality in mixed-motive human-agent interactions",
    "citation_count": 0,
    "authors": [
      "Kushal Chawla",
      "Ian Wu",
      "Yu Rong",
      "Gale Lucas",
      "Jonathan Gratch"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.809": {
    "title": "Doolittle: Benchmarks and Corpora for Academic Writing Formalization",
    "volume": "main",
    "abstract": "Improving the quality of academic writing is a meaningful but challenging task. Conventional methods of language refinement focus on narrow, specific linguistic features within isolated sentences, such as grammatical errors and improper word use. We propose a more general task, Academic Writing Formalization (AWF), to improve the overall quality of formal academic writing at the paragraph level. We formulate this language refinement task as a formal text style transfer task which transfers informal-academic text to formal-academic and contribute a large-scale non-parallel dataset, Doolittle, for this purpose. Concurrently, we apply a method named metric-oriented reinforcement learning (MORL) to two large language models (LLM) where we incorporate different levels of automatic feedback into the training process. Our experiments reveal that existing text transfer models and grammatical error correction models address certain aspects of AWF but still have a significant performance gap compared to human performance. Meanwhile, language models fine-tuned with our MORL method exhibit considerably improved performance, rivaling the latest chatbot ChatGPT, but still have a non-negligible gap compared to the ground truth formal-academic texts in Doolittle",
    "checked": true,
    "id": "913952cc1a679c18e7ff6f21f8a4b9e0833b93e4",
    "semantic_title": "doolittle: benchmarks and corpora for academic writing formalization",
    "citation_count": 0,
    "authors": [
      "Shizhe Diao",
      "Yongyu Lei",
      "Liangming Pan",
      "Tianqing Fang",
      "Wangchunshu Zhou",
      "Sedrick Keh",
      "Min-Yen Kan",
      "Tong Zhang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.810": {
    "title": "Token Prediction as Implicit Classification to Identify LLM-Generated Text",
    "volume": "main",
    "abstract": "This paper introduces a novel approach for identifying the possible large language models (LLMs) involved in text generation. Instead of adding an additional classification layer to a base LM, we reframe the classification task as a next-token prediction task and directly fine-tune the base LM to perform it. We utilize the Text-to-Text Transfer Transformer (T5) model as the backbone for our experiments. We compared our approach to the more direct approach of utilizing hidden states for classification. Evaluation shows the exceptional performance of our method in the text classification task, highlighting its simplicity and efficiency. Furthermore, interpretability studies on the features extracted by our model reveal its ability to differentiate distinctive writing styles among various LLMs even in the absence of an explicit classifier. We also collected a dataset named OpenLLMText, containing approximately 340k text samples from human and LLMs, including GPT3.5, PaLM, LLaMA, and GPT2",
    "checked": true,
    "id": "7a25aa397ae2a7f82df87a936ce6ff7f03b7ac4c",
    "semantic_title": "token prediction as implicit classification to identify llm-generated text",
    "citation_count": 0,
    "authors": [
      "Yutian Chen",
      "Hao Kang",
      "Vivian Zhai",
      "Liangze Li",
      "Rita Singh",
      "Bhiksha Raj"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.811": {
    "title": "On Evaluation of Bangla Word Analogies",
    "volume": "main",
    "abstract": "This paper presents a benchmark dataset of Bangla word analogies for evaluating the quality of existing Bangla word embeddings. Despite being the 7th largest spoken language in the world, Bangla is still a low-resource language and popular NLP models often struggle to perform well on Bangla data sets. Therefore, developing a robust evaluation set is crucial for benchmarking and guiding future research on improving Bangla word embeddings, which is currently missing. To address this issue, we introduce a new evaluation set of 16,678 unique word analogies in Bangla as well as a translated and curated version of the original Mikolov dataset (10,594 samples) in Bangla. Our experiments with different state-of-the-art embedding models reveal that current Bangla word embeddings struggle to achieve high accuracy on both data sets, demonstrating a significant gap in multilingual NLP research",
    "checked": true,
    "id": "220332c1ae2991b6f1339032b2d31d2020d14f63",
    "semantic_title": "on evaluation of bangla word analogies",
    "citation_count": 0,
    "authors": [
      "Mousumi Akter",
      "Souvika Sarkar",
      "Shubhra Kanti Karmaker Santu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.812": {
    "title": "Reconstruct Before Summarize: An Efficient Two-Step Framework for Condensing and Summarizing Meeting Transcripts",
    "volume": "main",
    "abstract": "Meetings typically involve multiple participants and lengthy conversations, resulting in redundant and trivial content. To overcome these challenges, we propose a two-step framework, Reconstruct before Summarize (RbS), for effective and efficient meeting summarization. RbS first leverages a self-supervised paradigm to annotate essential contents by reconstructing the meeting transcripts. Secondly, we propose a relative positional bucketing (RPB) algorithm to equip (conventional) summarization models to generate the summary. Despite the additional reconstruction process, our proposed RPB significantly compresses the input, leading to faster processing and reduced memory consumption compared to traditional summarization methods. We validate the effectiveness and efficiency of our method through extensive evaluations and analyses. On two meeting summarization datasets, AMI and ICSI, our approach outperforms previous state-of-the-art approaches without relying on large-scale pre-training or expert-grade annotating tools",
    "checked": true,
    "id": "838a53870a700b60de3dee1d1398f1eaf64ebfd6",
    "semantic_title": "reconstruct before summarize: an efficient two-step framework for condensing and summarizing meeting transcripts",
    "citation_count": 0,
    "authors": [
      "Haochen Tan",
      "Han Wu",
      "Wei Shao",
      "Xinyun Zhang",
      "Mingjie Zhan",
      "Zhaohui Hou",
      "Ding Liang",
      "Linqi Song"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.813": {
    "title": "XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models",
    "volume": "main",
    "abstract": "Large multilingual language models typically rely on a single vocabulary shared across 100+ languages. As these models have increased in parameter count and depth, vocabulary size has remained largely unchanged. This vocabulary bottleneck limits the representational capabilities of multilingual models like XLM-R. In this paper, we introduce a new approach for scaling to very large multilingual vocabularies by de-emphasizing token sharing between languages with little lexical overlap and assigning vocabulary capacity to achieve sufficient coverage for each individual language. Tokenizations using our vocabulary are typically more semantically meaningful and shorter compared to XLM-R. Leveraging this improved vocabulary, we train XLM-V, a multilingual language model with a one million token vocabulary. XLM-V outperforms XLM-R on every task we tested on ranging from natural language inference (XNLI), question answering (MLQA, XQuAD, TyDiQA), to named entity recognition (WikiAnn). XLM-V is particularly effective on low-resource language tasks and outperforms XLM-R by 11.2% and 5.8% absolute on MasakhaNER and Americas NLI, respectively",
    "checked": true,
    "id": "62a7f3c14c15d8f5d1c643ceb991dddb36e3c47c",
    "semantic_title": "xlm-v: overcoming the vocabulary bottleneck in multilingual masked language models",
    "citation_count": 19,
    "authors": [
      "Davis Liang",
      "Hila Gonen",
      "Yuning Mao",
      "Rui Hou",
      "Naman Goyal",
      "Marjan Ghazvininejad",
      "Luke Zettlemoyer",
      "Madian Khabsa"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.814": {
    "title": "Character-LLM: A Trainable Agent for Role-Playing",
    "volume": "main",
    "abstract": "Large language models (LLMs) can be used to serve as agents to simulate human behaviors, given the powerful ability to understand human instructions and provide high-quality generated texts. Such ability stimulates us to wonder whether LLMs can simulate a person in a higher form than simple human behaviors. Therefore, we aim to train an agent with the profile, experience, and emotional states of a specific person instead of using limited prompts to instruct ChatGPT API. In this work, we introduce Character-LLM that teach LLMs to act as specific people such as Beethoven, Queen Cleopatra, Julius Caesar, etc. Our method focuses on editing profiles as experiences of a certain character and training models to be personal simulacra with these experiences. To assess the effectiveness of our approach, we build a test playground that interviews trained agents and evaluates whether the agents memorize their characters and experiences. Experimental results show interesting observations that help build future simulacra of humankind",
    "checked": true,
    "id": "6628f9ee35e36cdfdcac8a46cef4dba8d529a83b",
    "semantic_title": "character-llm: a trainable agent for role-playing",
    "citation_count": 4,
    "authors": [
      "Yunfan Shao",
      "Linyang Li",
      "Junqi Dai",
      "Xipeng Qiu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.815": {
    "title": "Natural Language Decompositions of Implicit Content Enable Better Text Representations",
    "volume": "main",
    "abstract": "When people interpret text, they rely on inferences that go beyond the observed language itself. Inspired by this observation, we introduce a method for the analysis of text that takes implicitly communicated content explicitly into account. We use a large language model to produce sets of propositions that are inferentially related to the text that has been observed, then validate the plausibility of the generated content via human judgments. Incorporating these explicit representations of implicit content proves useful in multiple problem settings that involve the human interpretation of utterances: assessing the similarity of arguments, making sense of a body of opinion data, and modeling legislative behavior. Our results suggest that modeling the meanings behind observed language, rather than the literal text alone, is a valuable direction for NLP and particularly its applications to social science",
    "checked": true,
    "id": "90b167b2193f4bcb4f3fc804514ec010b4a804fd",
    "semantic_title": "natural language decompositions of implicit content enable better text representations",
    "citation_count": 1,
    "authors": [
      "Alexander Hoyle",
      "Rupak Sarkar",
      "Pranav Goel",
      "Philip Resnik"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.816": {
    "title": "A Scalable Framework for Table of Contents Extraction from Complex ESG Annual Reports",
    "volume": "main",
    "abstract": "Table of contents (ToC) extraction centres on structuring documents in a hierarchical manner. In this paper, we propose a new dataset, ESGDoc, comprising 1,093 ESG annual reports from 563 companies spanning from 2001 to 2022. These reports pose significant challenges due to their diverse structures and extensive length. To address these challenges, we propose a new framework for Toc extraction, consisting of three steps: (1) Constructing an initial tree of text blocks based on reading order and font sizes; (2) Modelling each tree node (or text block) independently by considering its contextual information captured in node-centric subtree; (3) Modifying the original tree by taking appropriate action on each tree node (Keep, Delete, or Move). This construction-modelling-modification (CMM) process offers several benefits. It eliminates the need for pairwise modelling of section headings as in previous approaches, making document segmentation practically feasible. By incorporating structured information, each section heading can leverage both local and long-distance context relevant to itself. Experimental results show that our approach outperforms the previous state-of-the-art baseline with a fraction of running time. Our framework proves its scalability by effectively handling documents of any length",
    "checked": true,
    "id": "f5f6d52b5b739be9a5dd879bf921f0af33980e23",
    "semantic_title": "a scalable framework for table of contents extraction from complex esg annual reports",
    "citation_count": 0,
    "authors": [
      "Xinyu Wang",
      "Lin Gui",
      "Yulan He"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.817": {
    "title": "Semantic Space Grounded Weighted Decoding for Multi-Attribute Controllable Dialogue Generation",
    "volume": "main",
    "abstract": "Controlling chatbot utterance generation with multiple attributes such as personalities, emotions and dialogue acts is a practically useful but under-studied problem. We propose a novel framework called DASC that possesses strong controllability with a weighted decoding paradigm, while improving generation quality with the grounding in an attribute semantics space. Generation with multiple attributes is then intuitively implemented with an interpolation of multiple attribute embeddings, which results in substantial reduction in the model sizes. Experiments show that DASC can achieve high control accuracy in generation task with the simultaneous control of 3 aspects while also producing interesting and reasonably sensible responses, even in an out-of-distribution robustness test",
    "checked": true,
    "id": "b029cf8f6de2fdaa6a972166d0d377e236cfefb7",
    "semantic_title": "semantic space grounded weighted decoding for multi-attribute controllable dialogue generation",
    "citation_count": 0,
    "authors": [
      "Zhiling Zhang",
      "Mengyue Wu",
      "Kenny Zhu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.818": {
    "title": "How do languages influence each other? Studying cross-lingual data sharing during LM fine-tuning",
    "volume": "main",
    "abstract": "Multilingual language models (MLMs) are jointly trained on data from many different languages such that representation of individual languages can benefit from other languages' data. Impressive performance in zero-shot cross-lingual transfer shows that these models are able to exploit this property. Yet, it remains unclear to what extent, and under which conditions, languages rely on each other's data. To answer this question, we use TracIn (Pruthi et al., 2020), a training data attribution (TDA) method, to retrieve training samples from multilingual data that are most influential for test predictions in a given language. This allows us to analyse cross-lingual sharing mechanisms of MLMs from a new perspective. While previous work studied cross-lingual sharing at the model parameter level, we present the first approach to study it at the data level. We find that MLMs rely on data from multiple languages during fine-tuning and this reliance increases as fine-tuning progresses. We further find that training samples from other languages can both reinforce and complement the knowledge acquired from data of the test language itself",
    "checked": false,
    "id": "e6d1139f185acf6a08260190d4dba138f918e1df",
    "semantic_title": "how do languages influence each other? studying cross-lingual data sharing during llm fine-tuning",
    "citation_count": 3,
    "authors": [
      "Rochelle Choenni",
      "Dan Garrette",
      "Ekaterina Shutova"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.819": {
    "title": "COFFEE: Counterfactual Fairness for Personalized Text Generation in Explainable Recommendation",
    "volume": "main",
    "abstract": "As language models become increasingly integrated into our digital lives, Personalized Text Generation (PTG) has emerged as a pivotal component with a wide range of applications. However, the bias inherent in user written text, often used for PTG model training, can inadvertently associate different levels of linguistic quality with users' protected attributes. The model can inherit the bias and perpetuate inequality in generating text w.r.t. users' protected attributes, leading to unfair treatment when serving users. In this work, we investigate fairness of PTG in the context of personalized explanation generation for recommendations. We first discuss the biases in generated explanations and their fairness implications. To promote fairness, we introduce a general framework to achieve measure-specific counterfactual fairness in explanation generation. Extensive experiments and human evaluations demonstrate the effectiveness of our method",
    "checked": true,
    "id": "65546dd9aa9ae03dcc9ee75ccf1951f2726e336c",
    "semantic_title": "coffee: counterfactual fairness for personalized text generation in explainable recommendation",
    "citation_count": 3,
    "authors": [
      "Nan Wang",
      "Qifan Wang",
      "Yi-Chia Wang",
      "Maziar Sanjabi",
      "Jingzhou Liu",
      "Hamed Firooz",
      "Hongning Wang",
      "Shaoliang Nie"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.820": {
    "title": "NameGuess: Column Name Expansion for Tabular Data",
    "volume": "main",
    "abstract": "Recent advances in large language models have revolutionized many sectors, including the database industry. One common challenge when dealing with large volumes of tabular data is the pervasive use of abbreviated column names, which can negatively impact performance on various data search, access, and understanding tasks. To address this issue, we introduce a new task, called NameGuess, to expand column names (used in database schema) as a natural language generation problem. We create a training dataset of 384K abbreviated-expanded column pairs using a new data fabrication method and a human-annotated evaluation benchmark that includes 9.2K examples from real-world tables. To tackle the complexities associated with polysemy and ambiguity in NameGuess, we enhance auto-regressive language models by conditioning on table content and column header names – yielding a fine-tuned model (with 2.7B parameters) that matches human performance. Furthermore, we conduct a comprehensive analysis (on multiple LLMs) to validate the effectiveness of table content in NameGuess and identify promising future opportunities. Code has been made available at https://github.com/amazon-science/nameguess",
    "checked": true,
    "id": "38b75403553508c7623196f10d625c1d162510e5",
    "semantic_title": "nameguess: column name expansion for tabular data",
    "citation_count": 0,
    "authors": [
      "Jiani Zhang",
      "Zhengyuan Shen",
      "Balasubramaniam Srinivasan",
      "Shen Wang",
      "Huzefa Rangwala",
      "George Karypis"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.821": {
    "title": "BLESS: Benchmarking Large Language Models on Sentence Simplification",
    "volume": "main",
    "abstract": "We present BLESS, a comprehensive performance benchmark of the most recent state-of-the-art Large Language Models (LLMs) on the task of text simplification (TS). We examine how well off-the-shelf LLMs can solve this challenging task, assessing a total of 44 models, differing in size, architecture, pre-training methods, and accessibility, on three test sets from different domains (Wikipedia, news, and medical) under a few-shot setting. Our analysis considers a suite of automatic metrics, as well as a large-scale quantitative investigation into the types of common edit operations performed by the different models. Furthermore, we perform a manual qualitative analysis on a subset of model outputs to better gauge the quality of the generated simplifications. Our evaluation indicates that the best LLMs, despite not being trained on TS perform comparably with state-of-the-art TS baselines. Additionally, we find that certain LLMs demonstrate a greater range and diversity of edit operations. Our performance benchmark will be available as a resource for the development of future TS methods and evaluation metrics",
    "checked": true,
    "id": "b37690747ba314455b8a992e794b25223e105d1c",
    "semantic_title": "bless: benchmarking large language models on sentence simplification",
    "citation_count": 0,
    "authors": [
      "Tannon Kew",
      "Alison Chi",
      "Laura Vásquez-Rodríguez",
      "Sweta Agrawal",
      "Dennis Aumiller",
      "Fernando Alva-Manchego",
      "Matthew Shardlow"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.822": {
    "title": "To Build Our Future, We Must Know Our Past: Contextualizing Paradigm Shifts in Natural Language Processing",
    "volume": "main",
    "abstract": "NLP is in a period of disruptive change that is impacting our methodologies, funding sources, and public perception. In this work, we seek to understand how to shape our future by better understanding our past. We study factors that shape NLP as a field, including culture, incentives, and infrastructure by conducting long-form interviews with 26 NLP researchers of varying seniority, research area, institution, and social identity. Our interviewees identify cyclical patterns in the field, as well as new shifts without historical parallel, including changes in benchmark culture and software infrastructure. We complement this discussion with quantitative analysis of citation, authorship, and language use in the ACL Anthology over time. We conclude by discussing shared visions, concerns, and hopes for the future of NLP. We hope that this study of our field's past and present can prompt informed discussion of our community's implicit norms and more deliberate action to consciously shape the future",
    "checked": true,
    "id": "1433b8d43d446fcc7f3e1370b22f744a4dd7c8e4",
    "semantic_title": "to build our future, we must know our past: contextualizing paradigm shifts in natural language processing",
    "citation_count": 0,
    "authors": [
      "Sireesh Gururaja",
      "Amanda Bertsch",
      "Clara Na",
      "David Widder",
      "Emma Strubell"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.823": {
    "title": "PALS: Personalized Active Learning for Subjective Tasks in NLP",
    "volume": "main",
    "abstract": "For subjective NLP problems, such as classification of hate speech, aggression, or emotions, personalized solutions can be exploited. Then, the learned models infer about the perception of the content independently for each reader. To acquire training data, texts are commonly randomly assigned to users for annotation, which is expensive and highly inefficient. Therefore, for the first time, we suggest applying an active learning paradigm in a personalized context to better learn individual preferences. It aims to alleviate the labeling effort by selecting more relevant training samples. In this paper, we present novel Personalized Active Learning techniques for Subjective NLP tasks (PALS) to either reduce the cost of the annotation process or to boost the learning effect. Our five new measures allow us to determine the relevance of a text in the context of learning users personal preferences. We validated them on three datasets: Wiki discussion texts individually labeled with aggression and toxicity, and on Unhealthy Conversations dataset. Our PALS techniques outperform random selection even by more than 30%. They can also be used to reduce the number of necessary annotations while maintaining a given quality level. Personalized annotation assignments based on our controversy measure decrease the amount of data needed to just 25%-40% of the initial size",
    "checked": true,
    "id": "c7e2a2cbc6999a2b8dd562733939992f71212d2e",
    "semantic_title": "pals: personalized active learning for subjective tasks in nlp",
    "citation_count": 0,
    "authors": [
      "Kamil Kanclerz",
      "Konrad Karanowski",
      "Julita Bielaniewicz",
      "Marcin Gruza",
      "Piotr Miłkowski",
      "Jan Kocon",
      "Przemyslaw Kazienko"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.824": {
    "title": "ViStruct: Visual Structural Knowledge Extraction via Curriculum Guided Code-Vision Representation",
    "volume": "main",
    "abstract": "State-of-the-art vision-language models (VLMs) still have limited performance in structural knowledge extraction, such as relations between objects. In this work, we present ViStruct, a training framework to learn VLMs for effective visual structural knowledge extraction. Two novel designs are incorporated. First, we propose to leverage the inherent structure of programming language to depict visual structural information. This approach enables explicit and consistent representation of visual structural information of multiple granularities, such as concepts, relations, and events, in a well-organized structured format. Second, we introduce curriculum-based learning for VLMs to progressively comprehend visual structures, from fundamental visual concepts to intricate event structures. Our intuition is that lower-level knowledge may contribute to complex visual structure understanding. Furthermore, we compile and release a collection of datasets tailored for visual structural knowledge extraction. We adopt a weakly-supervised approach to directly generate visual event structures from captions for ViStruct training, capitalizing on abundant image-caption pairs from the web. In experiments, we evaluate ViStruct on visual structure prediction tasks, demonstrating its effectiveness in improving the understanding of visual structures. The code will be made public to facilitate future research",
    "checked": true,
    "id": "033607afc2e08a58383ad78deb98c844017109c1",
    "semantic_title": "vistruct: visual structural knowledge extraction via curriculum guided code-vision representation",
    "citation_count": 0,
    "authors": [
      "Yangyi Chen",
      "Xingyao Wang",
      "Manling Li",
      "Derek Hoiem",
      "Heng Ji"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.825": {
    "title": "LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models",
    "volume": "main",
    "abstract": "Large language models (LLMs) have been applied in various applications due to their astonishing capabilities. With advancements in technologies such as chain-of-thought (CoT) prompting and in-context learning (ICL), the prompts fed to LLMs are becoming increasingly lengthy, even exceeding tens of thousands of tokens. To accelerate model inference and reduce cost, this paper presents LLMLingua, a coarse-to-fine prompt compression method that involves a budget controller to maintain semantic integrity under high compression ratios, a token-level iterative compression algorithm to better model the interdependence between compressed contents, and an instruction tuning based method for distribution alignment between language models. We conduct experiments and analysis over four datasets from different scenarios, i.e., GSM8K, BBH, ShareGPT, and Arxiv-March23; showing that the proposed approach yields state-of-the-art performance and allows for up to 20x compression with little performance loss",
    "checked": true,
    "id": "2392b6d3a5cad9e5cf349169eaeee848266adf6a",
    "semantic_title": "llmlingua: compressing prompts for accelerated inference of large language models",
    "citation_count": 2,
    "authors": [
      "Huiqiang Jiang",
      "Qianhui Wu",
      "Chin-Yew Lin",
      "Yuqing Yang",
      "Lili Qiu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.826": {
    "title": "EXPLAIN, EDIT, GENERATE: Rationale-Sensitive Counterfactual Data Augmentation for Multi-hop Fact Verification",
    "volume": "main",
    "abstract": "Automatic multi-hop fact verification task has gained significant attention in recent years. Despite impressive results, these well-designed models perform poorly on out-of-domain data. One possible solution is to augment the training data with counterfactuals, which are generated by minimally altering the causal features of the original data. However, current counterfactual data augmentation techniques fail to handle multi-hop fact verification due to their incapability to preserve the complex logical relationships within multiple correlated texts. In this paper, we overcome this limitation by developing a rationale-sensitive method to generate linguistically diverse and label-flipping counterfactuals while preserving logical relationships. In specific, the diverse and fluent counterfactuals are generated via an Explain-Edit-Generate architecture. Moreover, the checking and filtering modules are proposed to regularize the counterfactual data with logical relations and flipped labels. Experimental results show that the proposed approach outperforms the SOTA baselines and can generate linguistically diverse counterfactual data without disrupting their logical relationships",
    "checked": true,
    "id": "649208082a025265d9897f981356e8012a29effb",
    "semantic_title": "explain, edit, generate: rationale-sensitive counterfactual data augmentation for multi-hop fact verification",
    "citation_count": 0,
    "authors": [
      "Yingjie Zhu",
      "Jiasheng Si",
      "Yibo Zhao",
      "Haiyang Zhu",
      "Deyu Zhou",
      "Yulan He"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.827": {
    "title": "An Exploration of Left-Corner Transformations",
    "volume": "main",
    "abstract": "The left-corner transformation (Rosenkrantz and Lewis, 1970) is used to remove left recursion from context-free grammars, which is an important step towards making the grammar parsable top-down with simple techniques. This paper generalizes prior left-corner transformations to support semiring-weighted production rules and to provide finer-grained control over which left corners may be moved. Our generalized left-corner transformation (GLCT) arose from unifying the left-corner transformation and speculation transformation (Eisner and Blatz, 2007), originally for logic programming. Our new transformation and speculation define equivalent weighted languages. Yet, their derivation trees are structurally different in an important way: GLCT replaces left recursion with right recursion, and speculation does not. We also provide several technical results regarding the formal relationships between the outputs of GLCT, speculation, and the original grammar. Lastly, we empirically investigate the efficiency of GLCT for left-recursion elimination from grammars of nine languages. Code: https://github.com/rycolab/left-corner",
    "checked": true,
    "id": "8ad2d7c2f490658fd895a1cd05eb56e03924f21f",
    "semantic_title": "an exploration of left-corner transformations",
    "citation_count": 0,
    "authors": [
      "Andreas Opedal",
      "Eleftheria Tsipidi",
      "Tiago Pimentel",
      "Ryan Cotterell",
      "Tim Vieira"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.828": {
    "title": "Characterizing and Verifying Scientific Claims: Qualitative Causal Structure is All You Need",
    "volume": "main",
    "abstract": "A scientific claim typically begins with the formulation of a research question or hypothesis, which is a tentative statement or proposition about a phenomenon or relationship between variables. Within the realm of scientific claim verification, considerable research efforts have been dedicated to attention architectures and leveraging the text comprehension capabilities of Pre-trained Language Models (PLMs), yielding promising performances. However, these models overlook the causal structure information inherent in scientific claims, thereby failing to establish a comprehensive chain of causal inference. This paper delves into the exploration to highlight the crucial role of qualitative causal structure in characterizing and verifying scientific claims based on evidence. We organize the qualitative causal structure into a heterogeneous graph and propose a novel attention-based graph neural network model to facilitate causal reasoning across relevant causally-potent factors. Our experiments demonstrate that by solely utilizing the qualitative causal structure, the proposed model achieves comparable performance to PLM-based models. Furthermore, by incorporating semantic features, our model outperforms state-of-the-art approaches comprehensively",
    "checked": true,
    "id": "4e3fcb6f18463a26a649bb1530de88bb6f7e9414",
    "semantic_title": "characterizing and verifying scientific claims: qualitative causal structure is all you need",
    "citation_count": 0,
    "authors": [
      "Jinxuan Wu",
      "Wenhan Chao",
      "Xian Zhou",
      "Zhunchen Luo"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.829": {
    "title": "FOCUS: Effective Embedding Initialization for Monolingual Specialization of Multilingual Models",
    "volume": "main",
    "abstract": "Using model weights pretrained on a high-resource language as a warm start can reduce the need for data and compute to obtain high-quality language models for other, especially low-resource, languages. However, if we want to use a new tokenizer specialized for the target language, we cannot transfer the source model's embedding matrix. In this paper, we propose FOCUS - **F**ast **O**verlapping Token **C**ombinations **U**sing **S**parsemax, a novel embedding initialization method that effectively initializes the embedding matrix for a new tokenizer based on information in the source model's embedding matrix. FOCUS represents newly added tokens as combinations of tokens in the overlap of the source and target vocabularies. The overlapping tokens are selected based on semantic similarity in an auxiliary static token embedding space. We focus our study on using the multilingual XLM-R as a source model and empirically show that FOCUS outperforms random initialization and previous work on language modeling and on a range of downstream tasks (NLI, QA, and NER). We publish our model checkpoints and code on GitHub",
    "checked": true,
    "id": "cb7c0f9aa1060e237ce2d29260334538e0e04a56",
    "semantic_title": "focus: effective embedding initialization for monolingual specialization of multilingual models",
    "citation_count": 0,
    "authors": [
      "Konstantin Dobler",
      "Gerard de Melo"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.830": {
    "title": "ByteSized32: A Corpus and Challenge Task for Generating Task-Specific World Models Expressed as Text Games",
    "volume": "main",
    "abstract": "In this work we investigate the capacity of language models to generate explicit, interpretable, and interactive world models of scientific and common-sense reasoning tasks. We operationalize this as a task of generating text games, expressed as hundreds of lines of Python code. To facilitate this task, we introduce ByteSized32, a corpus of 32 reasoning-focused text games totalling 20k lines of Python code. We empirically demonstrate that GPT-4 can use these games as templates for single-shot in-context learning, successfully producing runnable games on unseen topics in 28% of cases. When allowed to self-reflect on program errors, game runnability substantially increases to 58%. While evaluating simulation fidelity is labor intensive, we introduce a suite of automated metrics to assess game fidelity, technical validity, adherence to task specifications, and winnability, showing a high-degree of agreement with expert human ratings. We pose this as a challenge task to spur further development at the juncture of world modeling and code generation",
    "checked": true,
    "id": "070b91f80ac118b910c1d2ab5be9f65f685979fe",
    "semantic_title": "bytesized32: a corpus and challenge task for generating task-specific world models expressed as text games",
    "citation_count": 0,
    "authors": [
      "Ruoyao Wang",
      "Graham Todd",
      "Xingdi Yuan",
      "Ziang Xiao",
      "Marc-Alexandre Côté",
      "Peter Jansen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.831": {
    "title": "Skill-Based Few-Shot Selection for In-Context Learning",
    "volume": "main",
    "abstract": "*In-context learning* is the paradigm that adapts large language models to downstream tasks by providing a few examples. *Few-shot selection*—selecting appropriate examples for each test instance separately—is important for in-context learning. In this paper, we propose **Skill-KNN**, a skill-based few-shot selection method for in-context learning. The key advantages of Skill-KNN include: (1) it addresses the problem that existing methods based on pre-trained embeddings can be easily biased by surface natural language features that are not important for the target task; (2) it does not require training or fine-tuning of any models, making it suitable for frequently expanding or changing example banks. The key insight is to optimize the inputs fed into the embedding model, rather than tuning the model itself. Technically, Skill-KNN generates the skill-based descriptions for each test case and candidate example by utilizing a pre-processing few-shot prompting, thus eliminating unimportant surface features. Experimental results across five cross-domain semantic parsing datasets and six backbone models show that Skill-KNN significantly outperforms existing methods",
    "checked": true,
    "id": "04526876688e5a56106629229309fae272da1c79",
    "semantic_title": "skill-based few-shot selection for in-context learning",
    "citation_count": 5,
    "authors": [
      "Shengnan An",
      "Bo Zhou",
      "Zeqi Lin",
      "Qiang Fu",
      "Bei Chen",
      "Nanning Zheng",
      "Weizhu Chen",
      "Jian-Guang Lou"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.832": {
    "title": "MaNtLE: Model-agnostic Natural Language Explainer",
    "volume": "main",
    "abstract": "Understanding the internal reasoning behind the predictions of machine learning systems is increasingly vital, given their rising adoption and acceptance. While previous approaches, such as LIME generate algorithmic explanations by attributing importance to input features for individual examples, recent research indicates that practitioners prefer examining language explanations that explain sub-groups of examples (Lakkaraju et al., 2022). In this paper, we introduce MaNtLE, a model-agnostic natural language explainer that analyzes a set of classifier predictions and generates faithful natural language explanations of classifier rationale for structured classification tasks. MaNtLE uses multi-task training on thousands of synthetic classification tasks to generate faithful explanations. Our experiments indicate that, on average, MaNtLE-generated explanations are at least 11% more faithful compared to LIME and Anchors explanations across three tasks. Human evaluations demonstrate that users can better predict model behavior using explanations from MaNtLE compared to other techniques",
    "checked": true,
    "id": "5afba6ef7e017bff35200f764d3c7a06d5e639fc",
    "semantic_title": "mantle: model-agnostic natural language explainer",
    "citation_count": 0,
    "authors": [
      "Rakesh Menon",
      "Kerem Zaman",
      "Shashank Srivastava"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.833": {
    "title": "PTP: Boosting Stability and Performance of Prompt Tuning with Perturbation-Based Regularizer",
    "volume": "main",
    "abstract": "Recent studies show that prompt tuning can better leverage the power of large language models than fine-tuning on downstream natural language understanding tasks. However, the existing prompt tuning methods have training instability issues, as the variance of scores under different random seeds is quite large. To address this critical problem, we first investigate and find that the loss landscape of vanilla prompt tuning is precipitous when it is visualized, where a slight change of input data can cause a big fluctuation in the loss landscape. This is an essential factor that leads to the instability of prompt tuning. Based on this observation, we introduce perturbation-based regularizers, which can smooth the loss landscape, into prompt tuning. We propose a new algorithm, called Prompt Tuning with Perturbation-based regularizer (PTP), which can not only alleviate training instability dramatically but also boost the performance of prompt tuning. We design two kinds of perturbation-based regularizers, including random-noise-based and adversarial-based. In particular, our proposed perturbations are flexible on both text space and embedding space. Extensive experiments show the effectiveness of our proposed methods in stabilizing the training. Our new algorithms improve the state-of-the-art prompt tuning methods by 1.94% and 2.34% on SuperGLUE and FewGLUE benchmarks, respectively",
    "checked": true,
    "id": "28f7e4cf501e97f7940243452d817381635e55f8",
    "semantic_title": "ptp: boosting stability and performance of prompt tuning with perturbation-based regularizer",
    "citation_count": 4,
    "authors": [
      "Lichang Chen",
      "Jiuhai Chen",
      "Heng Huang",
      "Minhao Cheng"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.834": {
    "title": "Ling-CL: Understanding NLP Models through Linguistic Curricula",
    "volume": "main",
    "abstract": "We employ a characterization of linguistic complexity from psycholinguistic and language acquisition research to develop data-driven curricula to understand the underlying linguistic knowledge that models learn to address NLP tasks. The novelty of our approach is in the development of linguistic curricula derived from data, existing knowledge about linguistic complexity, and model behavior during training. Through the evaluation of several benchmark NLP datasets, our curriculum learning approaches identify sets of linguistic metrics (indices) that inform the challenges and reasoning required to address each task. Our work will inform future research in all NLP areas, allowing linguistic complexity to be considered early in the research and development process. In addition, our work prompts an examination of gold standards and fair evaluation in NLP",
    "checked": true,
    "id": "9aef17bcbb9fb6cad2cd6b0aa1c3ad053a31291e",
    "semantic_title": "ling-cl: understanding nlp models through linguistic curricula",
    "citation_count": 0,
    "authors": [
      "Mohamed Elgaar",
      "Hadi Amiri"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.835": {
    "title": "Towards Unsupervised Recognition of Token-level Semantic Differences in Related Documents",
    "volume": "main",
    "abstract": "Automatically highlighting words that cause semantic differences between two documents could be useful for a wide range of applications. We formulate recognizing semantic differences (RSD) as a token-level regression task and study three unsupervised approaches that rely on a masked language model. To assess the approaches, we begin with basic English sentences and gradually move to more complex, cross-lingual document pairs. Our results show that an approach based on word alignment and sentence-level contrastive learning has a robust correlation to gold labels. However, all unsupervised approaches still leave a large margin of improvement",
    "checked": true,
    "id": "0ff4c057dbb9329e80f4724bcfb1247b1a25efdd",
    "semantic_title": "towards unsupervised recognition of token-level semantic differences in related documents",
    "citation_count": 0,
    "authors": [
      "Jannis Vamvas",
      "Rico Sennrich"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.836": {
    "title": "Towards a Better Understanding of Variations in Zero-Shot Neural Machine Translation Performance",
    "volume": "main",
    "abstract": "Multilingual Neural Machine Translation (MNMT) facilitates knowledge sharing but often suffers from poor zero-shot (ZS) translation qualities. While prior work has explored the causes of overall low zero-shot translation qualities, our work introduces a fresh perspective: the presence of significant variations in zero-shot performance. This suggests that MNMT does not uniformly exhibit poor zero-shot capability; instead, certain translation directions yield reasonable results. Through systematic experimentation, spanning 1,560 language directions across 40 languages, we identify three key factors contributing to high variations in ZS NMT performance: 1) target-side translation quality, 2) vocabulary overlap, and 3) linguistic properties. Our findings highlight that the target side translation quality is the most influential factor, with vocabulary overlap consistently impacting zero-shot capabilities. Additionally, linguistic properties, such as language family and writing system, play a role, particularly with smaller models. Furthermore, we suggest that the off-target issue is a symptom of inadequate performance, emphasizing that zero-shot translation challenges extend beyond addressing the off-target problem. To support future research, we release the data and models as a benchmark for the study of ZS NMT",
    "checked": true,
    "id": "ac5c0bbc30de1857f26e41bc50eea2c1dbe71fe1",
    "semantic_title": "towards a better understanding of variations in zero-shot neural machine translation performance",
    "citation_count": 0,
    "authors": [
      "Shaomu Tan",
      "Christof Monz"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.837": {
    "title": "SEER : A Knapsack approach to Exemplar Selection for In-Context HybridQA",
    "volume": "main",
    "abstract": "Question answering over hybrid contexts is a complex task, which requires the combination of information extracted from unstructured texts and structured tables in various ways. Recently, In-Context Learning demonstrated significant performance advances for reasoning tasks. In this paradigm, a large language model performs predictions based on a small set of supporting exemplars. The performance of In-Context Learning depends heavily on the selection procedure of the supporting exemplars, particularly in the case of HybridQA, where considering the diversity of reasoning chains and the large size of the hybrid contexts becomes crucial. In this work, we present Selection of ExEmplars for hybrid Reasoning (SEER), a novel method for selecting a set of exemplars that is both representative and diverse. The key novelty of SEER is that it formulates exemplar selection as a Knapsack Integer Linear Program. The Knapsack framework provides the flexibility to incorporate diversity constraints that prioritize exemplars with desirable attributes, and capacity constraints that ensure that the prompt size respects the provided capacity budgets. The effectiveness of SEER is demonstrated on FinQA and TAT-QA, two real-world benchmarks for HybridQA, where it outperforms previous exemplar selection methods",
    "checked": true,
    "id": "6a3c67d35f3a9c10c8fde6c325a0535c03876068",
    "semantic_title": "seer : a knapsack approach to exemplar selection for in-context hybridqa",
    "citation_count": 0,
    "authors": [
      "Jonathan Tonglet",
      "Manon Reusens",
      "Philipp Borchert",
      "Bart Baesens"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.838": {
    "title": "Conversation Chronicles: Towards Diverse Temporal and Relational Dynamics in Multi-Session Conversations",
    "volume": "main",
    "abstract": "In the field of natural language processing, open-domain chatbots have emerged as an important research topic. However, a major limitation of existing open-domain chatbot research is its singular focus on short single-session dialogue, neglecting the potential need for understanding contextual information in multiple consecutive sessions that precede an ongoing dialogue. Among the elements that compose the context in multi-session conversation settings, the time intervals between sessions and the relationships between speakers would be particularly important. Despite their importance, current research efforts have not sufficiently addressed these dialogical components. In this paper, we introduce a new 1M multi-session dialogue dataset, called Conversation Chronicles, for implementing a long-term conversation setup in which time intervals and fine-grained speaker relationships are incorporated. Following recent works, we exploit a large language model to produce the data. The extensive human evaluation shows that dialogue episodes in Conversation Chronicles reflect those properties while maintaining coherent and consistent interactions across all the sessions. We also propose a dialogue model, called ReBot, which consists of chronological summarization and dialogue generation modules using only around 630M parameters. When trained on Conversation Chronicles, ReBot demonstrates long-term context understanding with a high human engagement score",
    "checked": true,
    "id": "d2af7f63861ad683b061b508316624615bff162d",
    "semantic_title": "conversation chronicles: towards diverse temporal and relational dynamics in multi-session conversations",
    "citation_count": 0,
    "authors": [
      "Jihyoung Jang",
      "Minseong Boo",
      "Hyounghun Kim"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.839": {
    "title": "DueT: Image-Text Contrastive Transfer Learning with Dual-adapter Tuning",
    "volume": "main",
    "abstract": "This paper presents DueT, a novel transfer learning method for vision and language models built by contrastive learning. In DueT, adapters are inserted into the image and text encoders, which have been initialized using models pre-trained on uni-modal corpora and then frozen. By training only these adapters, DueT enables efficient learning with a reduced number of trainable parameters. Moreover, unlike traditional adapters, those in DueT are equipped with a gating mechanism, enabling effective transfer and connection of knowledge acquired from pre-trained uni-modal encoders while preventing catastrophic forgetting. We report that DueT outperformed simple fine-tuning, the conventional method fixing only the image encoder and training only the text encoder, and the LoRA-based adapter method in accuracy and parameter efficiency for 0-shot image and text retrieval in both English and Japanese domains",
    "checked": true,
    "id": "363211a31b9ed7d79f5373bf88db5a93ab409868",
    "semantic_title": "duet: image-text contrastive transfer learning with dual-adapter tuning",
    "citation_count": 0,
    "authors": [
      "Taku Hasegawa",
      "Kyosuke Nishida",
      "Koki Maeda",
      "Kuniko Saito"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.840": {
    "title": "Towards a Unified Conversational Recommendation System: Multi-task Learning via Contextualized Knowledge Distillation",
    "volume": "main",
    "abstract": "In Conversational Recommendation System (CRS), an agent is asked to recommend a set of items to users within natural language conversations. To address the need for both conversational capability and personalized recommendations, prior works have utilized separate recommendation and dialogue modules. However, such approach inevitably results in a discrepancy between recommendation results and generated responses. To bridge the gap, we propose a multi-task learning for a unified CRS, where a single model jointly learns both tasks via Contextualized Knowledge Distillation (ConKD). We introduce two versions of ConKD: hard gate and soft gate. The former selectively gates between two task-specific teachers, while the latter integrates knowledge from both teachers. Our gates are computed on-the-fly in a context-specific manner, facilitating flexible integration of relevant knowledge. Extensive experiments demonstrate that our single model significantly improves recommendation performance while enhancing fluency, and achieves comparable results in terms of diversity",
    "checked": true,
    "id": "e7b3dcad596deda061ee5dded9483c0332b8e755",
    "semantic_title": "towards a unified conversational recommendation system: multi-task learning via contextualized knowledge distillation",
    "citation_count": 0,
    "authors": [
      "Yeongseo Jung",
      "Eunseo Jung",
      "Lei Chen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.841": {
    "title": "CLAIR: Evaluating Image Captions with Large Language Models",
    "volume": "main",
    "abstract": "The evaluation of machine-generated image captions poses an interesting yet persistent challenge. Effective evaluation measures must consider numerous dimensions of similarity, including semantic relevance, visual structure, object interactions, caption diversity, and specificity. Existing highly-engineered measures attempt to capture specific aspects, but fall short in providing a holistic score that aligns closely with human judgments. Here, we propose CLAIR, a novel method that leverages the zero-shot language modeling capabilities of large language models (LLMs) to evaluate candidate captions. In our evaluations, CLAIR demonstrates a stronger correlation with human judgments of caption quality compared to existing measures. Notably, on Flickr8K-Expert, CLAIR achieves relative correlation improvements over SPICE of 39.6% and over image-augmented methods such as RefCLIP-S of 18.3%. Moreover, CLAIR provides noisily interpretable results by allowing the language model to identify the underlying reasoning behind its assigned score",
    "checked": true,
    "id": "da4deaf81232d94e2f38a9d23c6b04ae1d79fbfc",
    "semantic_title": "clair: evaluating image captions with large language models",
    "citation_count": 1,
    "authors": [
      "David Chan",
      "Suzanne Petryk",
      "Joseph Gonzalez",
      "Trevor Darrell",
      "John Canny"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.842": {
    "title": "MoPe: Model Perturbation based Privacy Attacks on Language Models",
    "volume": "main",
    "abstract": "Recent work has shown that Large Language Models (LLMs) can unintentionally leak sensitive information present in their training data. In this paper, we present Model Perturbations (MoPe), a new method to identify with high confidence if a given text is in the training data of a pre-trained language model, given white-box access to the models parameters. MoPe adds noise to the model in parameter space and measures the drop in log-likelihood at a given point x, a statistic we show approximates the trace of the Hessian matrix with respect to model parameters. Across language models ranging from 70M to 12B parameters, we show that MoPe is more effective than existing loss-based attacks and recently proposed perturbation-based methods. We also examine the role of training point order and model size in attack success, and empirically demonstrate that MoPe accurately approximate the trace of the Hessian in practice. Our results show that the loss of a point alone is insufficient to determine extractability—there are training points we can recover using our method that have average loss. This casts some doubt on prior works that use the loss of a point as evidence of memorization or unlearning",
    "checked": false,
    "id": "6a7aa40ad830b043f2600d4b3431468ced008337",
    "semantic_title": "mope: model perturbation-based privacy attacks on language models",
    "citation_count": 0,
    "authors": [
      "Marvin Li",
      "Jason Wang",
      "Jeffrey Wang",
      "Seth Neel"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.843": {
    "title": "q2d: Turning Questions into Dialogs to Teach Models How to Search",
    "volume": "main",
    "abstract": "One of the exciting capabilities of recent language models for dialog is their ability to independently search for relevant information to ground a given dialog response. However, obtaining training data to teach models how to issue search queries is time and resource consuming. In this work, we propose q2d: an automatic data generation pipeline that generates information-seeking dialogs from questions. We prompt a large language model (PaLM) to create conversational versions of question answering datasets, and use it to improve query generation models that communicate with external search APIs to ground dialog responses. Unlike previous approaches which relied on human written dialogs with search queries, our method allows to automatically generate query-based grounded dialogs with better control and scale. Our experiments demonstrate that: (1) For query generation on the QReCC dataset, models trained on our synthetically-generated data achieve 90%-97% of the performance of models trained on the human-generated data; (2) We can successfully generate data for training dialog models in new domains without any existing dialog data as demonstrated on the multi-hop MuSiQue and Bamboogle QA datasets. (3) We perform a thorough analysis of the generated dialogs showing that humans find them of high quality and struggle to distinguish them from human-written dialogs",
    "checked": true,
    "id": "33729913908d187dc0db6e41073c35643324fe4f",
    "semantic_title": "q2d: turning questions into dialogs to teach models how to search",
    "citation_count": 1,
    "authors": [
      "Yonatan Bitton",
      "Shlomi Cohen-Ganor",
      "Ido Hakimi",
      "Yoad Lewenberg",
      "Roee Aharoni",
      "Enav Weinreb"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.844": {
    "title": "Aligning Large Language Models through Synthetic Feedback",
    "volume": "main",
    "abstract": "Aligning large language models (LLMs) to human values has become increasingly important as it enables sophisticated steering of LLMs. However, it requires significant human demonstrations and feedback or distillation from proprietary LLMs such as ChatGPT. In this work, we propose a novel alignment learning framework with synthetic feedback not dependent on extensive human annotations and proprietary LLMs. First, we perform reward modeling (RM) with synthetic feedback by contrasting responses from vanilla LLMs with various sizes and prompts. Then, we use the RM to simulate high-quality demonstrations to train a supervised policy and further optimize the model with reinforcement learning. Our resulting model, Aligned Language Model with Synthetic Training dataset (ALMoST), outperforms recent open-sourced models, which are trained on the outputs of InstructGPT or human-annotated demonstrations, in alignment benchmarks. In human evaluation, our model is preferred to Alpaca and Dolly-v2, 55.0% and 58.5% of the time, respectively. Further analyses demonstrate the efficacy and importance of synthetic feedback in our framework",
    "checked": true,
    "id": "5b8f0460d408a8688d9ee0cba127c779d3291d99",
    "semantic_title": "aligning large language models through synthetic feedback",
    "citation_count": 18,
    "authors": [
      "Sungdong Kim",
      "Sanghwan Bae",
      "Jamin Shin",
      "Soyoung Kang",
      "Donghyun Kwak",
      "Kang Yoo",
      "Minjoon Seo"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.845": {
    "title": "You Told Me That Joke Twice: A Systematic Investigation of Transferability and Robustness of Humor Detection Models",
    "volume": "main",
    "abstract": "In this study, we focus on automatic humor detection, a highly relevant task for conversational AI. To date, there are several English datasets for this task, but little research on how models trained on them generalize and behave in the wild. To fill this gap, we carefully analyze existing datasets, train RoBERTa-based and Naïve Bayes classifiers on each of them, and test on the rest. Training and testing on the same dataset yields good results, but the transferability of the models varies widely. Models trained on datasets with jokes from different sources show better transferability, while the amount of training data has a smaller impact. The behavior of the models on out-of-domain data is unstable, suggesting that some of the models overfit, while others learn non-specific humor characteristics. An adversarial attack shows that models trained on pun datasets are less robust. We also evaluate the sense of humor of the chatGPT and Flan-UL2 models in a zero-shot scenario. The LLMs demonstrate competitive results on humor datasets and a more stable behavior on out-of-domain data. We believe that the obtained results will facilitate the development of new datasets and evaluation methodologies in the field of computational humor. We've made all the data from the study and the trained models publicly available at https://github.com/Humor-Research/Humor-detection",
    "checked": true,
    "id": "b5e15ca4f2e32cf75c1855e3e77b918eeb4ed6e4",
    "semantic_title": "you told me that joke twice: a systematic investigation of transferability and robustness of humor detection models",
    "citation_count": 0,
    "authors": [
      "Alexander Baranov",
      "Vladimir Kniazhevsky",
      "Pavel Braslavski"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.846": {
    "title": "Reading Order Matters: Information Extraction from Visually-rich Documents by Token Path Prediction",
    "volume": "main",
    "abstract": "Recent advances in multimodal pre-trained models have significantly improved information extraction from visually-rich documents (VrDs), in which named entity recognition (NER) is treated as a sequence-labeling task of predicting the BIO entity tags for tokens, following the typical setting of NLP. However, BIO-tagging scheme relies on the correct order of model inputs, which is not guaranteed in real-world NER on scanned VrDs where text are recognized and arranged by OCR systems. Such reading order issue hinders the accurate marking of entities by BIO-tagging scheme, making it impossible for sequence-labeling methods to predict correct named entities. To address the reading order issue, we introduce Token Path Prediction (TPP), a simple prediction head to predict entity mentions as token sequences within documents. Alternative to token classification, TPP models the document layout as a complete directed graph of tokens, and predicts token paths within the graph as entities. For better evaluation of VrD-NER systems, we also propose two revised benchmark datasets of NER on scanned documents which can reflect real-world scenarios. Experiment results demonstrate the effectiveness of our method, and suggest its potential to be a universal solution to various information extraction tasks on documents",
    "checked": true,
    "id": "76ccab3374a0cf98d3d33e1be257e52086185592",
    "semantic_title": "reading order matters: information extraction from visually-rich documents by token path prediction",
    "citation_count": 0,
    "authors": [
      "Chong Zhang",
      "Ya Guo",
      "Yi Tu",
      "Huan Chen",
      "Jinyang Tang",
      "Huijia Zhu",
      "Qi Zhang",
      "Tao Gui"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.847": {
    "title": "Empower Nested Boolean Logic via Self-Supervised Curriculum Learning",
    "volume": "main",
    "abstract": "Beyond the great cognitive powers showcased by language models, it is crucial to scrutinize whether their reasoning capabilities stem from strong generalization or merely exposure to relevant data. As opposed to constructing increasingly complex logic, this paper probes into the boolean logic, the root capability of a logical reasoner. We find that any pre-trained language models even including large language models only behave like a random selector in the face of multi-nested boolean logic, a task that humans can handle with ease. To empower language models with this fundamental capability, this paper proposes a new self-supervised learning method Curriculum Logical Reasoning (Clr), where we augment the training data with nested boolean logic chain step-by-step, and program the training from simpler logical patterns gradually to harder ones. This new training paradigm allows language models to effectively generalize to much harder and longer-hop logic, which can hardly be learned through naive training. Furthermore, we show that boolean logic is a great foundation for improving the subsequent general logical tasks",
    "checked": true,
    "id": "2f109e1a9eaa891ab9515ece6788e77c48748663",
    "semantic_title": "empower nested boolean logic via self-supervised curriculum learning",
    "citation_count": 0,
    "authors": [
      "Hongqiu Wu",
      "Linfeng Liu",
      "Hai Zhao",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.848": {
    "title": "The Sentiment Problem: A Critical Survey towards Deconstructing Sentiment Analysis",
    "volume": "main",
    "abstract": "We conduct an inquiry into the sociotechnical aspects of sentiment analysis (SA) by critically examining 189 peer-reviewed papers on their applications, models, and datasets. Our investigation stems from the recognition that SA has become an integral component of diverse sociotechnical systems, exerting influence on both social and technical users. By delving into sociological and technological literature on sentiment, we unveil distinct conceptualizations of this term in domains such as finance, government, and medicine. Our study exposes a lack of explicit definitions and frameworks for characterizing sentiment, resulting in potential challenges and biases. To tackle this issue, we propose an ethics sheet encompassing critical inquiries to guide practitioners in ensuring equitable utilization of SA. Our findings underscore the significance of adopting an interdisciplinary approach to defining sentiment in SA and offer a pragmatic solution for its implementation",
    "checked": true,
    "id": "916dc0026896e65cccb3d2ca0ef48185ad10dc06",
    "semantic_title": "the sentiment problem: a critical survey towards deconstructing sentiment analysis",
    "citation_count": 0,
    "authors": [
      "Pranav Venkit",
      "Mukund Srinath",
      "Sanjana Gautam",
      "Saranya Venkatraman",
      "Vipul Gupta",
      "Rebecca Passonneau",
      "Shomir Wilson"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.849": {
    "title": "Poisoning Retrieval Corpora by Injecting Adversarial Passages",
    "volume": "main",
    "abstract": "Dense retrievers have achieved state-of-the-art performance in various information retrieval tasks, but to what extent can they be safely deployed in real-world applications? In this work, we propose a novel attack for dense retrieval systems in which a malicious user generates a small number of adversarial passages by perturbing discrete tokens to maximize similarity with a provided set of training queries. When these adversarial passages are inserted into a large retrieval corpus, we show that this attack is highly effective in fooling these systems to retrieve them for queries that were not seen by the attacker. More surprisingly, these adversarial passages can directly generalize to out-of-domain queries and corpora with a high success attack rate — for instance, we find that 50 generated passages optimized on Natural Questions can mislead >94% of questions posed in financial documents or online forums. We also benchmark and compare a range of state-of-the-art dense retrievers, both unsupervised and supervised. Although different systems exhibit varying levels of vulnerability, we show they can all be successfully attacked by injecting up to 500 passages, a small fraction compared to a retrieval corpus of millions of passages",
    "checked": true,
    "id": "e3a49a110801fd961db887f8af1239cc79b55a5b",
    "semantic_title": "poisoning retrieval corpora by injecting adversarial passages",
    "citation_count": 1,
    "authors": [
      "Zexuan Zhong",
      "Ziqing Huang",
      "Alexander Wettig",
      "Danqi Chen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.850": {
    "title": "DADA: Dialect Adaptation via Dynamic Aggregation of Linguistic Rules",
    "volume": "main",
    "abstract": "Existing large language models (LLMs) that mainly focus on Standard American English (SAE) often lead to significantly worse performance when being applied to other English dialects. While existing mitigations tackle discrepancies for individual target dialects, they assume access to high-accuracy dialect identification systems. The boundaries between dialects are inherently flexible, making it difficult to categorize language into discrete predefined categories. In this paper, we propose DADA (Dialect Adaptation via Dynamic Aggregation), a modular approach to imbue SAE-trained models with multi-dialectal robustness by composing adapters which handle specific linguistic features. The compositional architecture of DADA allows for both targeted adaptation to specific dialect variants and simultaneous adaptation to various dialects. We show that DADA is effective for both single task and instruction finetuned language models, offering an extensible and interpretable framework for adapting existing LLMs to different English dialects",
    "checked": true,
    "id": "5242aa4319203dc2773c31b1c6a1c159dc63fb7e",
    "semantic_title": "dada: dialect adaptation via dynamic aggregation of linguistic rules",
    "citation_count": 2,
    "authors": [
      "Yanchen Liu",
      "William Held",
      "Diyi Yang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.851": {
    "title": "Clustering Pseudo Language Family in Multilingual Translation Models with Fisher Information Matrix",
    "volume": "main",
    "abstract": "In multilingual translation research, the comprehension and utilization of language families are of paramount importance. Nevertheless, clustering languages based solely on their ancestral families can yield suboptimal results due to variations in the datasets employed during the model's training phase. To mitigate this challenge, we introduce an innovative method that leverages the fisher information matrix (FIM) to cluster language families, anchored on the multilingual translation model's characteristics. We hypothesize that language pairs with similar effects on model parameters exhibit a considerable degree of linguistic congruence and should thus be grouped cohesively. This concept has led us to define pseudo language families. We provide an in-depth discussion regarding the inception and application of these pseudo language families. Empirical evaluations reveal that employing these pseudo language families enhances performance over conventional language families in adapting a multilingual translation model to unfamiliar language pairs. The proposed methodology may also be extended to scenarios requiring language similarity measurements. The source code and associated scripts can be accessed at https://github.com/ecoli-hit/PseudoFamily",
    "checked": true,
    "id": "cb2b15abee4fe9f0444addc6b5c2c429c5ea4df6",
    "semantic_title": "clustering pseudo language family in multilingual translation models with fisher information matrix",
    "citation_count": 0,
    "authors": [
      "Xinyu Ma",
      "Xuebo Liu",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.852": {
    "title": "Unifying Discrete and Continuous Representations for Unsupervised Paraphrase Generation",
    "volume": "main",
    "abstract": "Unsupervised paraphrase generation is a challenging task that benefits a variety of downstream NLP applications. Current unsupervised methods for paraphrase generation typically employ round-trip translation or denoising, which require translation corpus and result in paraphrases overly similar to the original sentences in surface structure. Most of these methods lack explicit control over the similarity between the original and generated sentences, and the entities are also less correctly kept. To obviate the reliance on translation data and prompt greater variations in surface structure, we propose a self-supervised pseudo-data construction method that generates diverse pseudo-paraphrases in distinct surface structures for a given sentence. To control the similarity and generate accurate entities, we propose an unsupervised paraphrasing model that encodes the sentence meaning and the entities with discrete and continuous variables, respectively. The similarity can be controlled by sampling discrete variables and the entities are kept substantially accurate due to the specific modeling of entities using continuous variables. Experimental results on two benchmark datasets demonstrate the advantages of our pseudo-data construction method compared to round-trip translation, and the superiority of our paraphrasing model over the state-of-the-art unsupervised methods",
    "checked": true,
    "id": "6def09ca6cbb720267d96eb11aefe8c4cdd18259",
    "semantic_title": "unifying discrete and continuous representations for unsupervised paraphrase generation",
    "citation_count": 0,
    "authors": [
      "Mingfeng Xue",
      "Dayiheng Liu",
      "Wenqiang Lei",
      "Jie Fu",
      "Jian Lan",
      "Mei Li",
      "Baosong Yang",
      "Jun Xie",
      "Yidan Zhang",
      "Dezhong Peng",
      "Jiancheng Lv"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.853": {
    "title": "The Benefits of Label-Description Training for Zero-Shot Text Classification",
    "volume": "main",
    "abstract": "Pretrained language models have improved zero-shot text classification by allowing the transfer of semantic knowledge from the training data in order to classify among specific label sets in downstream tasks. We propose a simple way to further improve zero-shot accuracies with minimal effort. We curate small finetuning datasets intended to describe the labels for a task. Unlike typical finetuning data, which has texts annotated with labels, our data simply describes the labels in language, e.g., using a few related terms, dictionary/encyclopedia entries, and short templates. Across a range of topic and sentiment datasets, our method is more accurate than zero-shot by 17-19% absolute. It is also more robust to choices required for zero-shot classification, such as patterns for prompting the model to classify and mappings from labels to tokens in the model's vocabulary. Furthermore, since our data merely describes the labels but does not use input texts, finetuning on it yields a model that performs strongly on multiple text domains for a given label set, even improving over few-shot out-of-domain classification in multiple settings",
    "checked": true,
    "id": "f5927ed60855381d694d8af015fbdb1c4edb6532",
    "semantic_title": "the benefits of label-description training for zero-shot text classification",
    "citation_count": 2,
    "authors": [
      "Lingyu Gao",
      "Debanjan Ghosh",
      "Kevin Gimpel"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.854": {
    "title": "Multilingual Pixel Representations for Translation and Effective Cross-lingual Transfer",
    "volume": "main",
    "abstract": "We introduce and demonstrate how to effectively train multilingual machine translation models with pixel representations. We experiment with two different data settings with a variety of language and script coverage, demonstrating improved performance compared to subword embeddings. We explore various properties of pixel representations such as parameter sharing within and across scripts to better understand where they lead to positive transfer. We observe that these properties not only enable seamless cross-lingual transfer to unseen scripts, but make pixel representations more data-efficient than alternatives such as vocabulary expansion. We hope this work contributes to more extensible multilingual models for all languages and scripts",
    "checked": true,
    "id": "c60736e61f8961ec535ecfdc6f0398925d34d0b8",
    "semantic_title": "multilingual pixel representations for translation and effective cross-lingual transfer",
    "citation_count": 1,
    "authors": [
      "Elizabeth Salesky",
      "Neha Verma",
      "Philipp Koehn",
      "Matt Post"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.855": {
    "title": "Finding Authentic Counterhate Arguments: A Case Study with Public Figures",
    "volume": "main",
    "abstract": "We explore authentic counterhate arguments for online hateful content toward individuals. Previous efforts are limited to counterhate to fight against hateful content toward groups. Thus, we present a corpus of 54,816 hateful tweet-paragraph pairs, where the paragraphs are candidate counterhate arguments. The counterhate arguments are retrieved from 2,500 online articles from multiple sources. We propose a methodology that assures the authenticity of the counter argument and its specificity to the individual of interest. We show that finding arguments in online articles is an efficient alternative to counterhate generation approaches that may hallucinate unsupported arguments. We also present linguistic insights on the language used in counterhate arguments. Experimental results show promising results. It is more challenging, however, to identify counterhate arguments for hateful content toward individuals not included in the training set",
    "checked": true,
    "id": "1cfdcffe08d110d963dad7de01319af68653a309",
    "semantic_title": "finding authentic counterhate arguments: a case study with public figures",
    "citation_count": 0,
    "authors": [
      "Abdullah Albanyan",
      "Ahmed Hassan",
      "Eduardo Blanco"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.856": {
    "title": "Can We Edit Multimodal Large Language Models?",
    "volume": "main",
    "abstract": "In this paper, we focus on editing multimodal Large Language Models (LLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights",
    "checked": true,
    "id": "e8d513bc7554a83161f2fb26c8299b471581cdb6",
    "semantic_title": "can we edit multimodal large language models?",
    "citation_count": 2,
    "authors": [
      "Siyuan Cheng",
      "Bozhong Tian",
      "Qingbin Liu",
      "Xi Chen",
      "Yongheng Wang",
      "Huajun Chen",
      "Ningyu Zhang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.857": {
    "title": "Exploring Discourse Structure in Document-level Machine Translation",
    "volume": "main",
    "abstract": "Neural machine translation has achieved great success in the past few years with the help of transformer architectures and large-scale bilingual corpora. However, when the source text gradually grows into an entire document, the performance of current methods for document-level machine translation (DocMT) is less satisfactory. Although the context is beneficial to the translation in general, it is difficult for traditional methods to utilize such long-range information. Previous studies on DocMT have concentrated on extra contents such as multiple surrounding sentences and input instances divided by a fixed length. We suppose that they ignore the structure inside the source text, which leads to under-utilization of the context. In this paper, we present a more sound paragraph-to-paragraph translation mode and explore whether discourse structure can improve DocMT. We introduce several methods from different perspectives, among which our RST-Att model with a multi-granularity attention mechanism based on the RST parsing tree works best. The experiments show that our method indeed utilizes discourse information and performs better than previous work",
    "checked": true,
    "id": "632e9f5718c6f889af1e5fa677166b61f5cf5304",
    "semantic_title": "exploring discourse structure in document-level machine translation",
    "citation_count": 0,
    "authors": [
      "Xinyu Hu",
      "Xiaojun Wan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.858": {
    "title": "ClusterLLM: Large Language Models as a Guide for Text Clustering",
    "volume": "main",
    "abstract": "We introduce ClusterLLM, a novel text clustering framework that leverages feedback from an instruction-tuned large language model, such as ChatGPT. Compared with traditional unsupervised methods that builds upon \"small\" embedders, ClusterLLM exhibits two intriguing advantages: (1) it enjoys the emergent capability of LLM even if its embeddings are inaccessible; and (2) it understands the user's preference on clustering through textual instruction and/or a few annotated data. First, we prompt ChatGPT for insights on clustering perspective by constructing hard triplet questions <does A better correspond to B than C>, where A, B and C are similar data points that belong to different clusters according to small embedder. We empirically show that this strategy is both effective for fine-tuning small embedder and cost-efficient to query ChatGPT. Second, we prompt ChatGPT for helps on clustering granularity by carefully designed pairwise questions <do A and B belong to the same category>, and tune the granularity from cluster hierarchies that is the most consistent with the ChatGPT answers. Extensive experiments on 14 datasets show that ClusterLLM consistently improves clustering quality, at an average cost of ~$0.6 per dataset",
    "checked": true,
    "id": "a7beaf4ad0c59ad6c91a03af6eceaafd2d44cef9",
    "semantic_title": "clusterllm: large language models as a guide for text clustering",
    "citation_count": 5,
    "authors": [
      "Yuwei Zhang",
      "Zihan Wang",
      "Jingbo Shang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.859": {
    "title": "CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code",
    "volume": "main",
    "abstract": "Since the rise of neural natural-language-to-code models (NL→Code) that can generate long expressions and statements rather than a single next-token, one of the major problems has been reliably evaluating their generated output. In this paper, we propose CodeBERTScore: an evaluation metric for code generation, which builds on BERTScore (Zhang et al., 2020). Instead of encoding only the generated tokens as in BERTScore, CodeBERTScore also encodes the natural language input preceding the generated code, thus modeling the consistency between the generated code and its given natural language context as well. We perform an extensive evaluation of CodeBERTScore across four programming languages. We find that CodeBERTScore achieves a higher correlation with human preference and with functional correctness than all existing metrics. That is, generated code that receives a higher score by CodeBERTScore is more likely to be preferred by humans, as well as to function correctly when executed. We release five language-specific pretrained models to use with our publicly available code. Our language-specific models have been downloaded more than **1,000,000** times from the Huggingface Hub. Our code and data are available at https://github.com/neulab/code-bert-score",
    "checked": true,
    "id": "31366ff634fc905affd78dbd8ddc9a872c006a87",
    "semantic_title": "codebertscore: evaluating code generation with pretrained models of code",
    "citation_count": 23,
    "authors": [
      "Shuyan Zhou",
      "Uri Alon",
      "Sumit Agarwal",
      "Graham Neubig"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.860": {
    "title": "Learn and Consolidate: Continual Adaptation for Zero-Shot and Multilingual Neural Machine Translation",
    "volume": "main",
    "abstract": "Although existing multilingual neural machine translation (MNMT) models have demonstrated remarkable performance to handle multiple translation directions in a single model and achieved zero-shot translation between language pairs unseen in training, they still suffer from relatively poor translation qualities for some language pairs. A practical scenario is that how to continually update MNMT models for both supervised and zero-shot translations when limited new data arrives. To this end, we propose a two-stage approach that encourages original models to acquire language-agnostic multilingual representations from new data, and preserves the model architecture without introducing parameters. Experimental results and further analysis demonstrate that our method can efficiently improve performance of existing MNMT models in translation directions where they are initially weak, and mitigates the degeneration in the original well-performing translation directions, offering flexibility in the real-world scenario",
    "checked": true,
    "id": "874693cdc9b0a91497e4e81432cb3c8bb0828e1f",
    "semantic_title": "learn and consolidate: continual adaptation for zero-shot and multilingual neural machine translation",
    "citation_count": 0,
    "authors": [
      "Kaiyu Huang",
      "Peng Li",
      "Junpeng Liu",
      "Maosong Sun",
      "Yang Liu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.861": {
    "title": "e-THERAPIST: I suggest you to cultivate a mindset of positivity and nurture uplifting thoughts",
    "volume": "main",
    "abstract": "The shortage of therapists for mental health patients emphasizes the importance of globally accessible dialogue systems alleviating their issues. To have effective interpersonal psychotherapy, these systems must exhibit politeness and empathy when needed. However, these factors may vary as per the user's gender, age, persona, and sentiment. Hence, in order to establish trust and provide a personalized cordial experience, it is essential that generated responses should be tailored to individual profiles and attributes. Focusing on this objective, we propose e-THERAPIST, a novel polite interpersonal psychotherapy dialogue system to address issues like depression, anxiety, schizophrenia, etc. We begin by curating a unique conversational dataset for psychotherapy, called PsyCon. It is annotated at two levels: (i) dialogue-level - including user's profile information (gender, age, persona) and therapist's psychotherapeutic approach; and (ii) utterance-level - encompassing user's sentiment and therapist's politeness, and interpersonal behaviour. Then, we devise a novel reward model to adapt correct polite interpersonal behaviour and use it to train e-THERAPIST on PsyCon employing NLPO loss. Our extensive empirical analysis validates the effectiveness of each component of the proposed e-THERAPIST demonstrating its potential impact in psychotherapy settings",
    "checked": true,
    "id": "4546014f60802dee2545f544d1549b63d58c3902",
    "semantic_title": "e-therapist: i suggest you to cultivate a mindset of positivity and nurture uplifting thoughts",
    "citation_count": 0,
    "authors": [
      "Kshitij Mishra",
      "Priyanshu Priya",
      "Manisha Burja",
      "Asif Ekbal"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.862": {
    "title": "AfriSenti: A Twitter Sentiment Analysis Benchmark for African Languages",
    "volume": "main",
    "abstract": "Africa is home to over 2,000 languages from over six language families and has the highest linguistic diversity among all continents. This includes 75 languages with at least one million speakers each. Yet, there is little NLP research conducted on African languages. Crucial in enabling such research is the availability of high-quality annotated datasets. In this paper, we introduce AfriSenti, a sentiment analysis benchmark that contains a total of >110,000 tweets in 14 African languages (Amharic, Algerian Arabic, Hausa, Igbo, Kinyarwanda, Moroccan Arabic, Mozambican Portuguese, Nigerian Pidgin, Oromo, Swahili, Tigrinya, Twi, Xitsonga, and Yoruba) from four language families. The tweets were annotated by native speakers and used in the AfriSenti-SemEval shared task (with over 200 participants, see website: https://afrisenti-semeval.github.io). We describe the data collection methodology, annotation process, and the challenges we dealt with when curating each dataset. We further report baseline experiments conducted on the AfriSenti datasets and discuss their usefulness",
    "checked": true,
    "id": "13c6b2da558f8f77af78298ffa18eaf769262c6d",
    "semantic_title": "afrisenti: a twitter sentiment analysis benchmark for african languages",
    "citation_count": 42,
    "authors": [
      "Shamsuddeen Muhammad",
      "Idris Abdulmumin",
      "Abinew Ayele",
      "Nedjma Ousidhoum",
      "David Adelani",
      "Seid Yimam",
      "Ibrahim Ahmad",
      "Meriem Beloucif",
      "Saif Mohammad",
      "Sebastian Ruder",
      "Oumaima Hourrane",
      "Alipio Jorge",
      "Pavel Brazdil",
      "Felermino Ali",
      "Davis David",
      "Salomey Osei",
      "Bello Shehu-Bello",
      "Falalu Lawan",
      "Tajuddeen Gwadabe",
      "Samuel Rutunda",
      "Tadesse Belay",
      "Wendimu Messelle",
      "Hailu Balcha",
      "Sisay Chala",
      "Hagos Gebremichael",
      "Bernard Opoku",
      "Stephen Arthur"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.863": {
    "title": "Quantifying Character Similarity with Vision Transformers",
    "volume": "main",
    "abstract": "Record linkage is a bedrock of quantitative social science, as analyses often require linking data from multiple, noisy sources. Off-the-shelf string matching methods are widely used, as they are straightforward and cheap to implement and scale. Not all character substitutions are equally probable, and for some settings there are widely used handcrafted lists denoting which string substitutions are more likely, that improve the accuracy of string matching. However, such lists do not exist for many settings, skewing research with linked datasets towards a few high-resource contexts that are not representative of the diversity of human societies. This study develops an extensible way to measure character substitution costs for OCR'ed documents, by employing large-scale self-supervised training of vision transformers (ViT) with augmented digital fonts. For each language written with the CJK script, we contrastively learn a metric space where different augmentations of the same character are represented nearby. In this space, homoglyphic characters - those with similar appearance such as \"O\" and \"0\" - have similar vector representations. Using the cosine distance between characters' representations as the substitution cost in an edit distance matching algorithm significantly improves record linkage compared to other widely used string matching methods, as OCR errors tend to be homoglyphic in nature. Homoglyphs can plausibly capture character visual similarity across any script, including low-resource settings. We illustrate this by creating homoglyph sets for 3,000 year old ancient Chinese characters, which are highly pictorial. Fascinatingly, a ViT is able to capture relationships in how different abstract concepts were conceptualized by ancient societies, that have been noted in the archaeological literature",
    "checked": true,
    "id": "82fb8ca367e637ea3b84f10314ee05a0c12a435a",
    "semantic_title": "quantifying character similarity with vision transformers",
    "citation_count": 1,
    "authors": [
      "Xinmei Yang",
      "Abhishek Arora",
      "Shao-Yu Jheng",
      "Melissa Dell"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.864": {
    "title": "Syllogistic Reasoning for Legal Judgment Analysis",
    "volume": "main",
    "abstract": "Legal judgment assistants are developing fast due to impressive progress of large language models (LLMs). However, people can hardly trust the results generated by a model without reliable analysis of legal judgement. For legal practitioners, it is common practice to utilize syllogistic reasoning to select and evaluate the arguments of the parties as part of the legal decision-making process. But the development of syllogistic reasoning for legal judgment analysis is hindered by the lack of resources: (1) there is no large-scale syllogistic reasoning dataset for legal judgment analysis, and (2) there is no set of established benchmarks for legal judgment analysis. In this paper, we construct and manually correct a syllogistic reasoning dataset for legal judgment analysis. The dataset contains 11,239 criminal cases which cover 4 criminal elements, 80 charges and 124 articles. We also select a set of large language models as benchmarks, and conduct a in-depth analysis of the capacity of their legal judgment analysis",
    "checked": true,
    "id": "2c0e9075f40f2cdce56a288ae2b6a3212074309d",
    "semantic_title": "syllogistic reasoning for legal judgment analysis",
    "citation_count": 0,
    "authors": [
      "Wentao Deng",
      "Jiahuan Pei",
      "Keyi Kong",
      "Zhe Chen",
      "Furu Wei",
      "Yujun Li",
      "Zhaochun Ren",
      "Zhumin Chen",
      "Pengjie Ren"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.865": {
    "title": "Improving Transformer-based Program Repair Model through False Behavior Diagnosis",
    "volume": "main",
    "abstract": "Research on automated program repairs using transformer-based models has recently gained considerable attention. The comprehension of the erroneous behavior of a model enables the identification of its inherent capacity and provides insights for improvement. However, the current landscape of research on program repair models lacks an investigation of their false behavior. Thus, we propose a methodology for diagnosing and treating the false behaviors of transformer-based program repair models. Specifically, we propose 1) a behavior vector that quantifies the behavior of the model when it generates an output, 2) a behavior discriminator (BeDisc) that identifies false behaviors, and 3) two methods for false behavior treatment. Through a large-scale experiment on 55,562 instances employing four datasets and three models, the BeDisc exhibited a balanced accuracy of 86.6% for false behavior classification. The first treatment, namely, early abortion, successfully eliminated 60.4% of false behavior while preserving 97.4% repair accuracy. Furthermore, the second treatment, namely, masked bypassing, resulted in an average improvement of 40.5% in the top-1 repair accuracy. These experimental results demonstrated the importance of investigating false behaviors in program repair models",
    "checked": true,
    "id": "46f1fb8cbe7cfe82b9b342c4e7a8a3c395628ce5",
    "semantic_title": "improving transformer-based program repair model through false behavior diagnosis",
    "citation_count": 0,
    "authors": [
      "Youngkyoung Kim",
      "Misoo Kim",
      "Eunseok Lee"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.866": {
    "title": "SUT: Active Defects Probing for Transcompiler Models",
    "volume": "main",
    "abstract": "Automatic Program translation has enormous application value and hence has been attracting significant interest from AI researchers. However, we observe that current program translation models still make elementary syntax errors, particularly, when the target language does not have syntax elements in the source language. Metrics like BLUE, CodeBLUE and computation accuracy may not expose these issues. In this paper we introduce a new metrics for programming language translation and these metrics address these basic syntax errors. We develop a novel active defects probing suite called Syntactic Unit Tests (SUT) which includes a highly interpretable evaluation harness for accuracy and test scoring. Experiments have shown that even powerful models like ChatGPT still make mistakes on these basic unit tests. Specifically, compared to previous program translation task evaluation dataset, its pass rate on our unit tests has decreased by 26.15%. Further our evaluation harness reveal syntactic element errors in which these models exhibit deficiencies",
    "checked": true,
    "id": "8c75679d44147ca5e07939beda9dede872c53f70",
    "semantic_title": "sut: active defects probing for transcompiler models",
    "citation_count": 0,
    "authors": [
      "Mengnan Qi",
      "Yufan Huang",
      "Maoquan Wang",
      "Yongqiang Yao",
      "Zihan Liu",
      "Bin Gu",
      "Colin Clement",
      "Neel Sundaresan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.867": {
    "title": "KCTS: Knowledge-Constrained Tree Search Decoding with Token-Level Hallucination Detection",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable human-level natural language generation capabilities. However, their potential to generate misinformation, often called the *hallucination* problem, poses a significant risk to their deployment. A common approach to address this issue is to retrieve relevant knowledge and fine-tune the LLM with the knowledge in its input. Unfortunately, this method incurs high training costs and may cause catastrophic forgetting for multi-tasking models. To overcome these limitations, we propose a knowledge-constrained decoding method called KCTS (Knowledge-Constrained Tree Search), which guides a frozen LM to generate text aligned with the reference knowledge at each decoding step using a knowledge classifier score and MCTS (Monte-Carlo Tree Search). To adapt the sequence-level knowledge classifier to token-level guidance, we also propose a novel token-level hallucination detection method called RIPA (Reward Inflection Point Approximation). Our empirical results on knowledge-grounded dialogue and abstractive summarization demonstrate the strength of KCTS as a plug-and-play, model-agnostic decoding method that can effectively reduce hallucinations in natural language generation",
    "checked": true,
    "id": "e6f74f2746a9e8bc90701f2afcf3c47e5e98b2dd",
    "semantic_title": "kcts: knowledge-constrained tree search decoding with token-level hallucination detection",
    "citation_count": 2,
    "authors": [
      "Sehyun Choi",
      "Tianqing Fang",
      "Zhaowei Wang",
      "Yangqiu Song"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.868": {
    "title": "CRUSH4SQL: Collective Retrieval Using Schema Hallucination For Text2SQL",
    "volume": "main",
    "abstract": "Existing Text-to-SQL generators require the entire schema to be encoded with the user text. This is expensive or impractical for large databases with tens of thousands of columns. Standard dense retrieval techniques are inadequate for schema subsetting of a large structured database, where the correct semantics of retrieval demands that we rank sets of schema elements rather than individual documents. In response, we propose a two-stage process for effective coverage during retrieval. First, we use an LLM to hallucinate a minimal DB schema that it deems adequate to answer the query. We use the hallucinated schema to retrieve a subset of the actual schema, by composing the results from multiple dense retrievals. Remarkably, hallucination — generally considered a nuisance — turns out to be actually useful as a bridging mechanism. Since no existing benchmarks exist for schema subsetting on large databases, we introduce two benchmarks: (1) A semi-synthetic dataset of 4502 schema elements, by taking a union of schema on the well-known SPIDER dataset, and (2) A real-life benchmark called SocialDB sourced from an actual large data warehouse comprising of 17844 schema elements. We show that our method leads to significantly higher recall than SOTA retrieval-based augmentation methods",
    "checked": true,
    "id": "4fdcc7fc6682ff0b3a6a939a18b75b19f9dafa48",
    "semantic_title": "crush4sql: collective retrieval using schema hallucination for text2sql",
    "citation_count": 1,
    "authors": [
      "Mayank Kothyari",
      "Dhruva Dhingra",
      "Sunita Sarawagi",
      "Soumen Chakrabarti"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.869": {
    "title": "This Reads Like That: Deep Learning for Interpretable Natural Language Processing",
    "volume": "main",
    "abstract": "Prototype learning, a popular machine learning method designed for inherently interpretable decisions, leverages similarities to learned prototypes for classifying new data. While it is mainly applied in computer vision, in this work, we build upon prior research and further explore the extension of prototypical networks to natural language processing. We introduce a learned weighted similarity measure that enhances the similarity computation by focusing on informative dimensions of pre-trained sentence embeddings. Additionally, we propose a post-hoc explainability mechanism that extracts prediction-relevant words from both the prototype and input sentences. Finally, we empirically demonstrate that our proposed method not only improves predictive performance on the AG News and RT Polarity datasets over a previous prototype-based approach, but also improves the faithfulness of explanations compared to rationale-based recurrent convolutions",
    "checked": true,
    "id": "47abff66e18f2f4ff5ceeaf5d9a09299cf49f607",
    "semantic_title": "this reads like that: deep learning for interpretable natural language processing",
    "citation_count": 0,
    "authors": [
      "Claudio Fanconi",
      "Moritz Vandenhirtz",
      "Severin Husmann",
      "Julia Vogt"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.870": {
    "title": "Incorporating Structured Representations into Pretrained Vision & Language Models Using Scene Graphs",
    "volume": "main",
    "abstract": "Vision and language models (VLMs) have demonstrated remarkable zero-shot (ZS) performance in a variety of tasks. However, recent works have shown that even the best VLMs struggle to capture aspects of compositional scene understanding, such as object attributes, relations, and action states. In contrast, obtaining structured annotations, such as scene graphs (SGs), that could improve these models is time-consuming and costly, and thus cannot be used on a large scale. Here we ask whether small SG datasets can provide sufficient information for enhancing structured understanding of pretrained VLMs. We show that it is indeed possible to improve VLMs when learning from SGs by integrating components that incorporate structured information into both visual and textual representations. For the visual side, we incorporate a special \"SG Component\" in the image transformer trained to predict SG information, while for the textual side, we utilize SGs to generate fine-grained captions that highlight different compositional aspects of the scene. Our method improves the performance of several popular VLMs on multiple VL datasets with only a mild degradation in ZS capabilities",
    "checked": true,
    "id": "b0e8632c25a9e262944b8849eced6f29091e700f",
    "semantic_title": "incorporating structured representations into pretrained vision & language models using scene graphs",
    "citation_count": 8,
    "authors": [
      "Roei Herzig",
      "Alon Mendelson",
      "Leonid Karlinsky",
      "Assaf Arbelle",
      "Rogerio Feris",
      "Trevor Darrell",
      "Amir Globerson"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.871": {
    "title": "TLM: Token-Level Masking for Transformers",
    "volume": "main",
    "abstract": "Structured dropout approaches, such as attention dropout and DropHead, have been investigated to regularize the multi-head attention mechanism in Transformers. In this paper, we propose a new regularization scheme based on token-level rather than structure-level to reduce overfitting. Specifically, we devise a novel Token-Level Masking (TLM) training strategy for Transformers to regularize the connections of self-attention, which consists of two masking techniques that are effective and easy to implement. The underlying idea is to manipulate the connections between tokens in the multi-head attention via masking, where the networks are forced to exploit partial neighbors' information to produce a meaningful representation. The generality and effectiveness of TLM are thoroughly evaluated via extensive experiments on 4 diversified NLP tasks across 18 datasets, including natural language understanding benchmark GLUE, ChineseGLUE, Chinese Grammatical Error Correction, and data-to-text generation. The results indicate that TLM can consistently outperform attention dropout and DropHead, e.g., it increases by 0.5 points relative to DropHead with BERT-large on GLUE. Moreover, TLM can establish a new record on the data-to-text benchmark Rotowire (18.93 BLEU). Our code will be publicly available at https://github.com/Young1993/tlm",
    "checked": true,
    "id": "48c903ac22620ff4a8644b543cc161732251dab8",
    "semantic_title": "tlm: token-level masking for transformers",
    "citation_count": 0,
    "authors": [
      "Yangjun Wu",
      "Kebin Fang",
      "Dongxiang Zhang",
      "Han Wang",
      "Hao Zhang",
      "Gang Chen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.872": {
    "title": "Addressing NER Annotation Noises with Uncertainty-Guided Tree-Structured CRFs",
    "volume": "main",
    "abstract": "Real-world named entity recognition (NER) datasets are notorious for their noisy nature, attributed to annotation errors, inconsistencies, and subjective interpretations. Such noises present a substantial challenge for traditional supervised learning methods. In this paper, we present a new and unified approach to tackle annotation noises for NER. Our method considers NER as a constituency tree parsing problem, utilizing a tree-structured Conditional Random Fields (CRFs) with uncertainty evaluation for integration. Through extensive experiments conducted on four real-world datasets, we demonstrate the effectiveness of our model in addressing both partial and incorrect annotation errors. Remarkably, our model exhibits superb performance even in extreme scenarios with 90% annotation noise",
    "checked": true,
    "id": "366d5aefa1884143eb0707966542dbf6ae84dcb5",
    "semantic_title": "addressing ner annotation noises with uncertainty-guided tree-structured crfs",
    "citation_count": 0,
    "authors": [
      "Jian Liu",
      "Weichang Liu",
      "Yufeng Chen",
      "Jinan Xu",
      "Zhe Zhao"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.873": {
    "title": "Hi Guys or Hi Folks? Benchmarking Gender-Neutral Machine Translation with the GeNTE Corpus",
    "volume": "main",
    "abstract": "Gender inequality is embedded in our communication practices and perpetuated in translation technologies. This becomes particularly apparent when translating into grammatical gender languages, where machine translation (MT) often defaults to masculine and stereotypical representations by making undue binary gender assumptions. Our work addresses the rising demand for inclusive language by focusing head-on on gender-neutral translation from English to Italian. We start from the essentials: proposing a dedicated benchmark and exploring automated evaluation methods. First, we introduce GeNTE, a natural, bilingual test set for gender-neutral translation, whose creation was informed by a survey on the perception and use of neutral language. Based on GeNTE, we then overview existing reference-based evaluation approaches, highlight their limits, and propose a reference-free method more suitable to assess gender-neutral translation",
    "checked": true,
    "id": "e36128e4cc6ac3e7a99b487403537e869f0b518d",
    "semantic_title": "hi guys or hi folks? benchmarking gender-neutral machine translation with the gente corpus",
    "citation_count": 1,
    "authors": [
      "Andrea Piergentili",
      "Beatrice Savoldi",
      "Dennis Fucci",
      "Matteo Negri",
      "Luisa Bentivogli"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.874": {
    "title": "Multilingual Holistic Bias: Extending Descriptors and Patterns to Unveil Demographic Biases in Languages at Scale",
    "volume": "main",
    "abstract": "We introduce a multilingual extension of the HolisticBias dataset, the largest English template-based taxonomy of textual people references: Multilingual HolisticBias. This extension consists of 20,459 sentences in 50 languages distributed across 13 demographic axes. Source sentences are built from combinations of 118 demographic descriptors and three patterns, excluding nonsensical combinations. Multilingual translations include alternatives for gendered languages that cover gendered translations when there is ambiguity in English. Our dataset is intended to uncover demographic imbalances and be the tool to quantify mitigations towards them. Our initial findings show that translation quality for EN-to-XX translations is an average of almost 8 spBLEU better when evaluating with the masculine human reference compared to feminine. In the opposite direction, XX-to-EN, we compare the robustness of the model when the source input only differs in gender (masculine or feminine) and masculine translations are an average of almost 4 spBLEU better than feminine. When embedding sentences to a joint multilingual sentence representations space, we find that for most languages masculine translations are significantly closer to the English neutral sentences when embedded",
    "checked": true,
    "id": "015d73903bf2460ba0642080f97c9917133f7056",
    "semantic_title": "multilingual holistic bias: extending descriptors and patterns to unveil demographic biases in languages at scale",
    "citation_count": 4,
    "authors": [
      "Marta Costa-jussà",
      "Pierre Andrews",
      "Eric Smith",
      "Prangthip Hansanti",
      "Christophe Ropers",
      "Elahe Kalbassi",
      "Cynthia Gao",
      "Daniel Licht",
      "Carleigh Wood"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.875": {
    "title": "GlobalBench: A Benchmark for Global Progress in Natural Language Processing",
    "volume": "main",
    "abstract": "Despite the major advances in NLP, significant disparities in NLP system performance across languages still exist. Arguably, these are due to uneven resource allocation and sub-optimal incentives to work on less resourced languages. To track and further incentivize the global development of equitable language technology, we introduce GlobalBench. Prior multilingual benchmarks are static and have focused on a limited number of tasks and languages. In contrast, GlobalBench is an ever-expanding collection that aims to dynamically track progress on all NLP datasets in all languages. Rather than solely measuring accuracy, GlobalBench also tracks the estimated per-speaker utility and equity of technology across all languages, providing a multi-faceted view of how language technology is serving people of the world. Furthermore, GlobalBench is designed to identify the most under-served languages, and rewards research efforts directed towards those languages. At present, the most under-served languages are the ones with a relatively high population, but nonetheless overlooked by composite multilingual benchmarks (like Punjabi, Portuguese, and Wu Chinese). Currently, GlobalBench covers 966 datasets in 190 languages, and has 1,128 system submissions spanning 62 languages",
    "checked": true,
    "id": "17605c43ca3eb982c99642052ddc21a93d116594",
    "semantic_title": "globalbench: a benchmark for global progress in natural language processing",
    "citation_count": 1,
    "authors": [
      "Yueqi Song",
      "Simran Khanuja",
      "Pengfei Liu",
      "Fahim Faisal",
      "Alissa Ostapenko",
      "Genta Winata",
      "Alham Aji",
      "Samuel Cahyawijaya",
      "Yulia Tsvetkov",
      "Antonios Anastasopoulos",
      "Graham Neubig"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.876": {
    "title": "DetGPT: Detect What You Need via Reasoning",
    "volume": "main",
    "abstract": "In recent years, the field of computer vision has seen significant advancements thanks to the development of large language models (LLMs). These models have enabled more effective and sophisticated interactions between humans and machines, paving the way for novel techniques that blur the lines between human and machine intelligence. In this paper, we introduce a new paradigm for object detection that we call reasoning-based object detection. Unlike conventional object detection methods that rely on specific object names, our approach enables users to interact with the system using natural language instructions, allowing for a higher level of interactivity. Our proposed method, called DetGPT, leverages state-of-the-art multi-modal models and open-vocabulary object detectors to perform reasoning within the context of the user's instructions and the visual scene. This enables DetGPT to automatically locate the object of interest based on the user's expressed desires, even if the object is not explicitly mentioned. For instance, if a user expresses a desire for a cold beverage, DetGPT can analyze the image, identify a fridge, and use its knowledge of typical fridge contents to locate the beverage. This flexibility makes our system applicable across a wide range of fields, from robotics and automation to autonomous driving. Overall, our proposed paradigm and DetGPT demonstrate the potential for more sophisticated and intuitive interactions between humans and machines. We hope that our proposed paradigm and approach will provide inspiration to the community and open the door to more interactive and versatile object detection systems",
    "checked": true,
    "id": "2ad8183c72a90511383a32ccaeea313eb85f4085",
    "semantic_title": "detgpt: detect what you need via reasoning",
    "citation_count": 21,
    "authors": [
      "Renjie Pi",
      "Jiahui Gao",
      "Shizhe Diao",
      "Rui Pan",
      "Hanze Dong",
      "Jipeng Zhang",
      "Lewei Yao",
      "Jianhua Han",
      "Hang Xu",
      "Lingpeng Kong",
      "Tong Zhang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.877": {
    "title": "Language Models with Rationality",
    "volume": "main",
    "abstract": "While large language models (LLMs) are proficient at question-answering (QA), it is not always clear how (or even if) an answer follows from their latent \"beliefs\". This lack of interpretability is a growing impediment to widespread use of LLMs. To address this, our goals are to make model beliefs and their inferential relationships explicit, and to resolve inconsistencies that may exist, so that answers are supported by interpretable chains of reasoning drawn from a consistent network of beliefs. Our approach, which we call REFLEX, is to add a **rational, self-reflecting layer** on top of the LLM. First, given a question, we construct a **belief graph** using a backward-chaining process to materialize relevant model beliefs (including beliefs about answer candidates) and their inferential relationships. Second, we identify and minimize contradictions in that graph using a formal constraint reasoner. We find that REFLEX significantly improves consistency (by 8%-11% absolute) without harming overall answer accuracy, resulting in answers supported by faithful chains of reasoning drawn from a more consistent belief system. This suggests a new style of system architecture in which an LLM extended with a rational layer can provide an interpretable window into system beliefs, add a systematic reasoning capability, and repair latent inconsistencies present in the LLM",
    "checked": true,
    "id": "409c3c9c9c8729c46113845c304329a24489ebcb",
    "semantic_title": "language models with rationality",
    "citation_count": 1,
    "authors": [
      "Nora Kassner",
      "Oyvind Tafjord",
      "Ashish Sabharwal",
      "Kyle Richardson",
      "Hinrich Schuetze",
      "Peter Clark"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.878": {
    "title": "Self-Improvement of Non-autoregressive Model via Sequence-Level Distillation",
    "volume": "main",
    "abstract": "Although Non-autoregressive Transformer (NAT) models have achieved great success in terms of fast inference speed, this speedup comes with a performance drop due to the inherent multi-modality problem of the NAT model. Previous works commonly alleviate this problem by replacing the target side of the raw data with distilled data generated by Autoregressive Transformer (AT) models. However, the multi-modality problem in the distilled data is still significant and thus limits further improvement of the NAT models. In this paper, we propose a method called Sequence-Level Self-Distillation (SLSD), which aims to generate distilled data by the NAT model itself, eliminating the need for additional teacher networks. Furthermore, SLSD can adapt to different NAT models without precise adjustments since the self-distilled data is generated from the same types of NAT models. We conduct extensive experiments on WMT14 EN↔DE and WMT16 EN↔RO and choose four classic NAT models as the backbones to validate the generality and effectiveness of SLSD. The results show that our approach can consistently improve all models on both raw data and distilled data without sacrificing the inference speed",
    "checked": true,
    "id": "c8035ac257aa8a5980dbdb9ff5ec4ad04c70194d",
    "semantic_title": "self-improvement of non-autoregressive model via sequence-level distillation",
    "citation_count": 0,
    "authors": [
      "Yusheng Liao",
      "Shuyang Jiang",
      "Yiqi Li",
      "Yu Wang",
      "Yanfeng Wang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.879": {
    "title": "Mitigating Temporal Misalignment by Discarding Outdated Facts",
    "volume": "main",
    "abstract": "While large language models are able to retain vast amounts of world knowledge seen during pretraining, such knowledge is prone to going out of date and is nontrivial to update. Furthermore, these models are often used under temporal misalignment, tasked with answering questions about the present, despite having only been trained on data collected in the past. To mitigate the effects of temporal misalignment, we propose fact duration prediction: the task of predicting how long a given fact will remain true. In our experiments, we demonstrate that identifying which facts are prone to rapid change can help models avoid reciting outdated information and determine which predictions require seeking out up-to-date knowledge sources. We also show how modeling fact duration improves calibration for knowledge-intensive tasks, such as open-retrieval question answering, under temporal misalignment, by discarding volatile facts",
    "checked": true,
    "id": "de5e0a70af551df17e693223d29a1840900d6b74",
    "semantic_title": "mitigating temporal misalignment by discarding outdated facts",
    "citation_count": 2,
    "authors": [
      "Michael Zhang",
      "Eunsol Choi"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.880": {
    "title": "Open-world Semi-supervised Generalized Relation Discovery Aligned in a Real-world Setting",
    "volume": "main",
    "abstract": "Open-world Relation Extraction (OpenRE) has recently garnered significant attention. However, existing approaches tend to oversimplify the problem by assuming that all instances of unlabeled data belong to novel classes, thereby limiting the practicality of these methods. We argue that the OpenRE setting should be more aligned with the characteristics of real-world data. Specifically, we propose two key improvements: (a) unlabeled data should encompass known and novel classes, including negative instances; and (b) the set of novel classes should represent long-tail relation types. Furthermore, we observe that popular relations can often be implicitly inferred through specific patterns, while long-tail relations tend to be explicitly expressed. Motivated by these insights, we present a method called KNoRD (Known and Novel Relation Discovery), which effectively classifies explicitly and implicitly expressed relations from known and novel classes within unlabeled data. Experimental evaluations on several Open-world RE benchmarks demonstrate that KNoRD consistently outperforms other existing methods, achieving significant performance gains",
    "checked": true,
    "id": "cb88270d6c22206e7cd0a8a564389fa6c20b67fa",
    "semantic_title": "open-world semi-supervised generalized relation discovery aligned in a real-world setting",
    "citation_count": 0,
    "authors": [
      "William Hogan",
      "Jiacheng Li",
      "Jingbo Shang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.881": {
    "title": "IEKG: A Commonsense Knowledge Graph for Idiomatic Expressions",
    "volume": "main",
    "abstract": "Idiomatic expression (IE) processing and comprehension have challenged pre-trained language models (PTLMs) because their meanings are non-compositional. Unlike prior works that enable IE comprehension through fine-tuning PTLMs with sentences containing IEs, in this work, we construct IEKG, a commonsense knowledge graph for figurative interpretations of IEs. This extends the established ATOMIC2020 converting PTLMs into knowledge models (KMs) that encode and infer commonsense knowledge related to IE use. Experiments show that various PTLMs can be converted into KMs with IEKG. We verify the quality of IEKG and the ability of the trained KMs with automatic and human evaluation. Through applications in natural language understanding, we show that a PTLM injected with knowledge from IEKG exhibits improved IE comprehension ability and can generalize to IEs unseen during training",
    "checked": true,
    "id": "f10b618f3743b379ac08a5ed1596b82d6eeabfd3",
    "semantic_title": "iekg: a commonsense knowledge graph for idiomatic expressions",
    "citation_count": 0,
    "authors": [
      "Ziheng Zeng",
      "Kellen Cheng",
      "Srihari Nanniyur",
      "Jianing Zhou",
      "Suma Bhat"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.882": {
    "title": "Bias Neutralization in Non-Parallel Texts: A Cyclic Approach with Auxiliary Guidance",
    "volume": "main",
    "abstract": "Objectivity is a goal for Wikipedia and many news sites, as well as a guiding principle of many large language models. Indeed, several methods have recently been developed for automatic subjective bias neutralization. These methods, however, typically rely on parallel text for training (i.e. a biased sentence coupled with a non-biased sentence), demonstrate poor transfer to new domains, and can lose important bias-independent context. Toward expanding the reach of bias neutralization, we propose in this paper a new approach called FairBalance. Three of its unique features are: i) a cycle consistent adversarial network enables bias neutralization without the need for parallel text; ii) the model design preserves bias-independent content; and iii) through auxiliary guidance, the model highlights sequences of bias-inducing words, yielding strong results in terms of bias neutralization quality. Extensive experiments demonstrate how FairBalance significantly improves subjective bias neutralization compared to other methods",
    "checked": true,
    "id": "09655d0212d126c3e06e7184ff0b4b8e3bd72f3a",
    "semantic_title": "bias neutralization in non-parallel texts: a cyclic approach with auxiliary guidance",
    "citation_count": 0,
    "authors": [
      "Karthic Madanagopal",
      "James Caverlee"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.883": {
    "title": "Fighting Fire with Fire: The Dual Role of LLMs in Crafting and Detecting Elusive Disinformation",
    "volume": "main",
    "abstract": "Recent ubiquity and disruptive impacts of large language models (LLMs) have raised concerns about their potential to be misused (*.i.e, generating large-scale harmful and misleading content*). To combat this emerging risk of LLMs, we propose a novel \"***Fighting Fire with Fire***\" (F3) strategy that harnesses modern LLMs' generative and emergent reasoning capabilities to counter human-written and LLM-generated disinformation. First, we leverage GPT-3.5-turbo to synthesize authentic and deceptive LLM-generated content through paraphrase-based and perturbation-based prefix-style prompts, respectively. Second, we apply zero-shot in-context semantic reasoning techniques with cloze-style prompts to discern genuine from deceptive posts and news articles. In our extensive experiments, we observe GPT-3.5-turbo's zero-shot superiority for both in-distribution and out-of-distribution datasets, where GPT-3.5-turbo consistently achieved accuracy at 68-72%, unlike the decline observed in previous customized and fine-tuned disinformation detectors. Our codebase and dataset are available at https://github.com/mickeymst/F3",
    "checked": true,
    "id": "b91645729a769c09eddda2efe2512e2f6a750723",
    "semantic_title": "fighting fire with fire: the dual role of llms in crafting and detecting elusive disinformation",
    "citation_count": 1,
    "authors": [
      "Jason Lucas",
      "Adaku Uchendu",
      "Michiharu Yamashita",
      "Jooyoung Lee",
      "Shaurya Rohatgi",
      "Dongwon Lee"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.884": {
    "title": "SMoP: Towards Efficient and Effective Prompt Tuning with Sparse Mixture-of-Prompts",
    "volume": "main",
    "abstract": "Prompt tuning has emerged as a successful parameter-efficient alternative to the full fine-tuning of language models. However, prior works on prompt tuning often utilize long soft prompts of up to 100 tokens to improve performance, overlooking the inefficiency associated with extended inputs. In this paper, we propose a novel prompt tuning method SMoP (Sparse Mixture-of-Prompts) that utilizes short soft prompts for efficient training and inference while maintaining performance gains typically induced from longer soft prompts. To achieve this, SMoP employs a gating mechanism to train multiple short soft prompts specialized in handling different subsets of the data, providing an alternative to relying on a single long soft prompt to cover the entire data. Experimental results demonstrate that SMoP outperforms baseline methods while reducing training and inference costs. We release our code at https://github.com/jyjohnchoi/SMoP",
    "checked": true,
    "id": "50f2a5b103884c2edb11506445eabff613a79b6e",
    "semantic_title": "smop: towards efficient and effective prompt tuning with sparse mixture-of-prompts",
    "citation_count": 0,
    "authors": [
      "Joon-Young Choi",
      "Junho Kim",
      "Jun-Hyung Park",
      "Wing-Lam Mok",
      "SangKeun Lee"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.885": {
    "title": "BRAINTEASER: Lateral Thinking Puzzles for Large Language Models",
    "volume": "main",
    "abstract": "The success of language models has inspired the NLP community to attend to tasks that require implicit and complex reasoning, relying on human-like commonsense mechanisms. While such vertical thinking tasks have been relatively popular, lateral thinking puzzles have received little attention. To bridge this gap, we devise BrainTeaser: a multiple-choice Question Answering task designed to test the model's ability to exhibit lateral thinking and defy default commonsense associations. We design a three-step procedure for creating the first lateral thinking benchmark, consisting of data collection, distractor generation, and generation of adversarial examples, leading to 1,100 puzzles with high-quality annotations. To assess the consistency of lateral reasoning by models, we enrich BrainTeaser based on a semantic and contextual reconstruction of its questions. Our experiments with state-of-the-art instruction- and commonsense language models reveal a significant gap between human and model performance, which is further widened when consistency across adversarial formats is considered. We make all of our code and data available to stimulate work on developing and evaluating lateral thinking models",
    "checked": true,
    "id": "62ee2d23968b8ee97c958d75ae2b6ae13c84da1a",
    "semantic_title": "brainteaser: lateral thinking puzzles for large language models",
    "citation_count": 0,
    "authors": [
      "Yifan Jiang",
      "Filip Ilievski",
      "Kaixin Ma",
      "Zhivar Sourati"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.886": {
    "title": "When are Lemons Purple? The Concept Association Bias of Vision-Language Models",
    "volume": "main",
    "abstract": "Large-scale vision-language models such as CLIP have shown impressive performance on zero-shot image classification and image-to-text retrieval. However, such performance does not realize in tasks that require a finer-grained correspondence between vision and language, such as Visual Question Answering (VQA). We investigate why this is the case, and report an interesting phenomenon of vision-language models, which we call the Concept Association Bias (CAB), as a potential cause of the difficulty of applying these models to VQA and similar tasks. We find that models with CAB tend to treat input as a bag of concepts and attempt to fill in the other missing concept crossmodally, leading to an unexpected zero-shot prediction. We demonstrate CAB by showing that CLIP's zero-shot classification performance greatly suffers when there is a strong concept association between an object (e.g. eggplant) and an attribute (e.g. color purple). We also show that the strength of CAB predicts the performance on VQA. We observe that CAB is prevalent in vision-language models trained with contrastive losses, even when autoregressive losses are jointly employed. However, a model that solely relies on autoregressive loss seems to exhibit minimal or no signs of CAB",
    "checked": true,
    "id": "3b1b5f1c8a4b69bdcdb6a8446b0ae470aa3bce3a",
    "semantic_title": "when are lemons purple? the concept association bias of vision-language models",
    "citation_count": 0,
    "authors": [
      "Yingtian Tang",
      "Yutaro Yamada",
      "Yoyo Zhang",
      "Ilker Yildirim"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.887": {
    "title": "What Comes Next? Evaluating Uncertainty in Neural Text Generators Against Human Production Variability",
    "volume": "main",
    "abstract": "In Natural Language Generation (NLG) tasks, for any input, multiple communicative goals are plausible, and any goal can be put into words, or produced, in multiple ways. We characterise the extent to which human production varies lexically, syntactically, and semantically across four NLG tasks, connecting human production variability to aleatoric or data uncertainty. We then inspect the space of output strings shaped by a generation system's predicted probability distribution and decoding algorithm to probe its uncertainty. For each test input, we measure the generator's calibration to human production variability. Following this instance-level approach, we analyse NLG models and decoding strategies, demonstrating that probing a generator with multiple samples and, when possible, multiple references, provides the level of detail necessary to gain understanding of a model's representation of uncertainty",
    "checked": true,
    "id": "1ed6f5c4a2181efbc4aa4e3c056956ec6395c5cd",
    "semantic_title": "what comes next? evaluating uncertainty in neural text generators against human production variability",
    "citation_count": 7,
    "authors": [
      "Mario Giulianelli",
      "Joris Baan",
      "Wilker Aziz",
      "Raquel Fernández",
      "Barbara Plank"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.888": {
    "title": "Text Representation Distillation via Information Bottleneck Principle",
    "volume": "main",
    "abstract": "Pre-trained language models (PLMs) have recently shown great success in text representation field. However, the high computational cost and high-dimensional representation of PLMs pose significant challenges for practical applications. To make models more accessible, an effective method is to distill large models into smaller representation models. In order to relieve the issue of performance degradation after distillation, we propose a novel Knowledge Distillation method called IBKD. This approach is motivated by the Information Bottleneck principle and aims to maximize the mutual information between the final representation of the teacher and student model, while simultaneously reducing the mutual information between the student model's representation and the input data. This enables the student model to preserve important learned information while avoiding unnecessary information, thus reducing the risk of over-fitting. Empirical studies on two main downstream applications of text representation (Semantic Textual Similarity and Dense Retrieval tasks) demonstrate the effectiveness of our proposed approach",
    "checked": true,
    "id": "ab71258b6bbd39056da7fcf7fbde8e317ac8778a",
    "semantic_title": "text representation distillation via information bottleneck principle",
    "citation_count": 0,
    "authors": [
      "Yanzhao Zhang",
      "Dingkun Long",
      "Zehan Li",
      "Pengjun Xie"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.889": {
    "title": "Let GPT be a Math Tutor: Teaching Math Word Problem Solvers with Customized Exercise Generation",
    "volume": "main",
    "abstract": "In this paper, we present a novel approach for distilling math word problem solving capabilities from large language models (LLMs) into smaller, more efficient student models. Our approach is designed to consider the student model's weaknesses and foster a tailored learning experience by generating targeted exercises aligned with educational science principles, such as knowledge tracing and personalized learning. Concretely, we let GPT-3 be a math tutor and run two steps iteratively: 1) assessing the student model's current learning status on a GPT-generated exercise book, and 2) improving the student model by training it with tailored exercise samples generated by GPT-3. Experimental results reveal that our approach outperforms LLMs (e.g., GPT-3 and PaLM) in accuracy across three distinct benchmarks while employing significantly fewer parameters. Furthermore, we provide a comprehensive analysis of the various components within our methodology to substantiate their efficacy",
    "checked": true,
    "id": "073e6b91adc25c656d85002e3cb059e4530db20b",
    "semantic_title": "let gpt be a math tutor: teaching math word problem solvers with customized exercise generation",
    "citation_count": 5,
    "authors": [
      "Zhenwen Liang",
      "Wenhao Yu",
      "Tanmay Rajpurohit",
      "Peter Clark",
      "Xiangliang Zhang",
      "Ashwin Kalyan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.890": {
    "title": "FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions",
    "volume": "main",
    "abstract": "Theory of mind (ToM) evaluations currently focus on testing models using passive narratives that inherently lack interactivity. We introduce FANToM, a new benchmark designed to stress-test ToM within information-asymmetric conversational contexts via question answering. Our benchmark draws upon important theoretical requisites from psychology and necessary empirical considerations when evaluating large language models (LLMs). In particular, we formulate multiple types of questions that demand the same underlying reasoning to identify illusory or false sense of ToM capabilities in LLMs. We show that FANToM is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain-of-thought reasoning or fine-tuning",
    "checked": true,
    "id": "088a5fa00ed6c14351209da5f53e770b51fd2909",
    "semantic_title": "fantom: a benchmark for stress-testing machine theory of mind in interactions",
    "citation_count": 1,
    "authors": [
      "Hyunwoo Kim",
      "Melanie Sclar",
      "Xuhui Zhou",
      "Ronan Bras",
      "Gunhee Kim",
      "Yejin Choi",
      "Maarten Sap"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.891": {
    "title": "Exploring the Boundaries of GPT-4 in Radiology",
    "volume": "main",
    "abstract": "The recent success of general-domain large language models (LLMs) has significantly changed the natural language processing paradigm towards a unified foundation model across domains and applications. In this paper, we focus on assessing the performance of GPT-4, the most capable LLM so far, on the text-based applications for radiology reports, comparing against state-of-the-art (SOTA) radiology-specific models. Exploring various prompting strategies, we evaluated GPT-4 on a diverse range of common radiology tasks and we found GPT-4 either outperforms or is on par with current SOTA radiology models. With zero-shot prompting, GPT-4 already obtains substantial gains (≈ 10% absolute improvement) over radiology models in temporal sentence similarity classification (accuracy) and natural language inference (F1). For tasks that require learning dataset-specific style or schema (e.g. findings summarisation), GPT-4 improves with example-based prompting and matches supervised SOTA. Our extensive error analysis with a board-certified radiologist shows GPT-4 has a sufficient level of radiology knowledge with only occasional errors in complex context that require nuanced domain knowledge. For findings summarisation, GPT-4 outputs are found to be overall comparable with existing manually-written impressions",
    "checked": true,
    "id": "644a0aa82f58b7f387fb1b731f0932c84c2f200f",
    "semantic_title": "exploring the boundaries of gpt-4 in radiology",
    "citation_count": 0,
    "authors": [
      "Qianchu Liu",
      "Stephanie Hyland",
      "Shruthi Bannur",
      "Kenza Bouzid",
      "Daniel Castro",
      "Maria Wetscherek",
      "Robert Tinn",
      "Harshita Sharma",
      "Fernando Pérez-García",
      "Anton Schwaighofer",
      "Pranav Rajpurkar",
      "Sameer Khanna",
      "Hoifung Poon",
      "Naoto Usuyama",
      "Anja Thieme",
      "Aditya Nori",
      "Matthew Lungren",
      "Ozan Oktay",
      "Javier Alvarez-Valle"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.892": {
    "title": "A Frustratingly Easy Post-Training Quantization Scheme for LLMs",
    "volume": "main",
    "abstract": "Efficient inference has become crucial for hyper-scale AI models, including large language models, as their parameter count continues to increase for enhanced performance. This necessity holds true regardless of the computing environment, whether it be mobile devices or cloud servers. Quantization emerges as a solution to alleviate the computational burden during inference. By representing models with a reduced bit-width, quantization minimizes the frequency of DRAM access while fully exploiting the parallelism of operations through a dense matrix format. Consequently, quantized models achieve low end-to-end latency and optimize resource utilization by addressing both memory and computing bottlenecks. In this paper, we propose a straightforward post-training quantization scheme, called Z-Fold, that fully utilizes the feature of the Transformer structure widely employed in large language models",
    "checked": true,
    "id": "d0ae8f89f4b48c2fa45f0d3ccd1157682323a2fe",
    "semantic_title": "a frustratingly easy post-training quantization scheme for llms",
    "citation_count": 0,
    "authors": [
      "Yongkweon Jeon",
      "Chungman Lee",
      "Kyungphil Park",
      "Ho-young Kim"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.893": {
    "title": "A Comprehensive Evaluation of Biomedical Entity Linking Models",
    "volume": "main",
    "abstract": "Biomedical entity linking (BioEL) is the process of connecting entities referenced in documents to entries in biomedical databases such as the Unified Medical Language System (UMLS) or Medical Subject Headings (MeSH). The study objective was to comprehensively evaluate nine recent state-of-the-art biomedical entity linking models under a unified framework. We compare these models along axes of (1) accuracy, (2) speed, (3) ease of use, (4) generalization, and (5) adaptability to new ontologies and datasets. We additionally quantify the impact of various preprocessing choices such as abbreviation detection. Systematic evaluation reveals several notable gaps in current methods. In particular, current methods struggle to correctly link genes and proteins and often have difficulty effectively incorporating context into linking decisions. To expedite future development and baseline testing, we release our unified evaluation framework and all included models on GitHub at https://github.com/davidkartchner/biomedical-entity-linking",
    "checked": true,
    "id": "774b41178279afda086c41b9b9eee4b3f2426c58",
    "semantic_title": "a comprehensive evaluation of biomedical entity linking models",
    "citation_count": 0,
    "authors": [
      "David Kartchner",
      "Jennifer Deng",
      "Shubham Lohiya",
      "Tejasri Kopparthi",
      "Prasanth Bathala",
      "Daniel Domingo-Fernández",
      "Cassie Mitchell"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.894": {
    "title": "Exploring Jiu-Jitsu Argumentation for Writing Peer Review Rebuttals",
    "volume": "main",
    "abstract": "In many domains of argumentation, people's arguments are driven by so-called attitude roots, i.e., underlying beliefs and world views, and their corresponding attitude themes. Given the strength of these latent drivers of arguments, recent work in psychology suggests that instead of directly countering surface-level reasoning (e.g., falsifying the premises), one should follow an argumentation style inspired by the Jiu-Jitsu \"soft\" combat system: first, identify an arguer's attitude roots and themes, and then choose a prototypical rebuttal that is aligned with those drivers instead of trying to invalidate those. In this work, we are the first to explore Jiu-Jitsu argumentation for peer reviews by proposing the novel task of attitude and theme-guided rebuttal generation. To this end, we enrich an existing dataset for discourse structure in peer reviews with attitude roots, attitude themes, and canonical rebuttals. To facilitate this process, we recast established annotation concepts from the domain of peer reviews (e.g., aspects a review sentence is relating to) and train domain-specific models. We then propose strong rebuttal generation strategies, which we benchmark on our novel dataset for the task of end-to-end attitude and theme-guided rebuttal generation and two subtasks",
    "checked": true,
    "id": "4a174ed13b3095f6c7e3e37704369577929e194a",
    "semantic_title": "exploring jiu-jitsu argumentation for writing peer review rebuttals",
    "citation_count": 0,
    "authors": [
      "Sukannya Purkayastha",
      "Anne Lauscher",
      "Iryna Gurevych"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.895": {
    "title": "LIMIT: Language Identification, Misidentification, and Translation using Hierarchical Models in 350+ Languages",
    "volume": "main",
    "abstract": "Knowing the language of an input text/audio is a necessary first step for using almost every NLP tool such as taggers, parsers, or translation systems. Language identification is a well-studied problem, sometimes even considered solved; in reality, due to lack of data and computational challenges, current systems cannot accurately identify most of the world's 7000 languages. To tackle this bottleneck, we first compile a corpus, MCS-350, of 50K multilingual and parallel children's stories in 350+ languages. MCS-350 can serve as a benchmark for language identification of short texts and for 1400+ new translation directions in low-resource Indian and African languages. Second, we propose a novel misprediction-resolution hierarchical model, LIMIT, for language identification that reduces error by 55% (from 0.71 to 0.32) on our compiled children's stories dataset and by 40% (from 0.23 to 0.14) on the FLORES-200 benchmark. Our method can expand language identification coverage into low-resource languages by relying solely on systemic misprediction patterns, bypassing the need to retrain large models from scratch",
    "checked": true,
    "id": "94a29991a92c2ca654cd6fcc6e09593d91986279",
    "semantic_title": "limit: language identification, misidentification, and translation using hierarchical models in 350+ languages",
    "citation_count": 0,
    "authors": [
      "Milind Agarwal",
      "Md Mahfuz Ibn Alam",
      "Antonios Anastasopoulos"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.896": {
    "title": "FreeAL: Towards Human-Free Active Learning in the Era of Large Language Models",
    "volume": "main",
    "abstract": "Collecting high-quality labeled data for model training is notoriously time-consuming and labor-intensive for various NLP tasks. While copious solutions, such as active learning for small language models (SLMs) and prevalent in-context learning in the era of large language models (LLMs), have been proposed and alleviate the labeling burden to some extent, their performances are still subject to human intervention. It is still underexplored how to reduce the annotation cost in the LLMs era. To bridge this, we revolutionize traditional active learning and propose an innovative collaborative learning framework FreeAL to interactively distill and filter the task-specific knowledge from LLMs. During collaborative training, an LLM serves as an active annotator inculcating its coarse-grained knowledge, while a downstream SLM is incurred as a student to filter out high-quality in-context samples to feedback LLM for the subsequent label refinery. Extensive experiments on eight benchmark datasets demonstrate that FreeAL largely enhances the zero-shot performances for both SLM and LLM without any human supervision",
    "checked": true,
    "id": "386806bbd9d84e644ded24d4a9c7ea805c273891",
    "semantic_title": "freeal: towards human-free active learning in the era of large language models",
    "citation_count": 1,
    "authors": [
      "Ruixuan Xiao",
      "Yiwen Dong",
      "Junbo Zhao",
      "Runze Wu",
      "Minmin Lin",
      "Gang Chen",
      "Haobo Wang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.897": {
    "title": "API-Assisted Code Generation for Question Answering on Varied Table Structures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihan Cao",
      "Shuyi Chen",
      "Ryan Liu",
      "Zhiruo Wang",
      "Daniel Fried"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.898": {
    "title": "Data Factors for Better Compositional Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Zhou",
      "Yichen Jiang",
      "Mohit Bansal"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.899": {
    "title": "ChatEdit: Towards Multi-turn Interactive Facial Image Editing via Dialogue",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xing Cui",
      "Zekun Li",
      "Pei Li",
      "Yibo Hu",
      "Hailin Shi",
      "Chunshui Cao",
      "Zhaofeng He"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.900": {
    "title": "Bridging Continuous and Discrete Spaces: Interpretable Sentence Representation Learning via Compositional Operations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "James Huang",
      "Wenlin Yao",
      "Kaiqiang Song",
      "Hongming Zhang",
      "Muhao Chen",
      "Dong Yu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.901": {
    "title": "Outlier Dimensions Encode Task Specific Knowledge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "William Rudman",
      "Catherine Chen",
      "Carsten Eickhoff"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.902": {
    "title": "Hi-ArG: Exploring the Integration of Hierarchical Argumentation Graphs in Language Pretraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingcong Liang",
      "Rong Ye",
      "Meng Han",
      "Qi Zhang",
      "Ruofei Lai",
      "Xinyu Zhang",
      "Zhao Cao",
      "Xuanjing Huang",
      "Zhongyu Wei"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.903": {
    "title": "Biomedical Named Entity Recognition via Dictionary-based Synonym Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihao Fu",
      "Yixuan Su",
      "Zaiqiao Meng",
      "Nigel Collier"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.904": {
    "title": "GNAT: A General Narrative Alignment Tool",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tanzir Pial",
      "Steven Skiena"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.905": {
    "title": "Self-Ensemble of N-best Generation Hypotheses by Lexically Constrained Decoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryota Miyano",
      "Tomoyuki Kajiwara",
      "Yuki Arase"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.906": {
    "title": "UniChart: A Universal Vision-language Pretrained Model for Chart Comprehension and Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ahmed Masry",
      "Parsa Kavehzadeh",
      "Do Long",
      "Enamul Hoque",
      "Shafiq Joty"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.907": {
    "title": "Merging Experts into One: Improving Computational Efficiency of Mixture of Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shwai He",
      "Run-Ze Fan",
      "Liang Ding",
      "Li Shen",
      "Tianyi Zhou",
      "Dacheng Tao"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.908": {
    "title": "Distance-Based Propagation for Efficient Knowledge Graph Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Harry Shomer",
      "Yao Ma",
      "Juanhui Li",
      "Bo Wu",
      "Charu Aggarwal",
      "Jiliang Tang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.909": {
    "title": "What to Read in a Contract? Party-Specific Summarization of Legal Obligations, Entitlements, and Prohibitions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhilasha Sancheti",
      "Aparna Garimella",
      "Balaji Srinivasan",
      "Rachel Rudinger"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.910": {
    "title": "Enhancing Computation Efficiency in Large Language Models through Weight and Activation Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Janghwan Lee",
      "Minsoo Kim",
      "Seungcheol Baek",
      "Seok Hwang",
      "Wonyong Sung",
      "Jungwook Choi"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.911": {
    "title": "CP-BCS: Binary Code Summarization Guided by Control Flow Graph and Pseudo Code",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tong Ye",
      "Lingfei Wu",
      "Tengfei Ma",
      "Xuhong Zhang",
      "Yangkai Du",
      "Peiyu Liu",
      "Shouling Ji",
      "Wenhai Wang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.912": {
    "title": "Assessing Step-by-Step Reasoning against Lexical Negation: A Case Study on Syllogism",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengyu Ye",
      "Tatsuki Kuribayashi",
      "Jun Suzuki",
      "Goro Kobayashi",
      "Hiroaki Funayama"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.913": {
    "title": "Chain-of-Thought Tuning: Masked Language Models can also Think Step By Step in Natural Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Caoyun Fan",
      "Jidong Tian",
      "Yitian Li",
      "Wenqing Chen",
      "Hao He",
      "Yaohui Jin"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.914": {
    "title": "Large Language Models are Complex Table Parsers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowen Zhao",
      "Changkai Ji",
      "Yuejie Zhang",
      "Wen He",
      "Yingwen Wang",
      "Qing Wang",
      "Rui Feng",
      "Xiaobo Zhang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.915": {
    "title": "R2H: Building Multimodal Navigation Helpers that Respond to Help Requests",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Fan",
      "Jing Gu",
      "Kaizhi Zheng",
      "Xin Wang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.916": {
    "title": "Speech-enriched Memory for Inference-time Adaptation of ASR Models to Word Dictionaries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashish Mittal",
      "Sunita Sarawagi",
      "Preethi Jyothi",
      "George Saon",
      "Gakuto Kurata"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.917": {
    "title": "Generative Table Pre-training Empowers Models for Tabular Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianping Zhang",
      "Shaowen Wang",
      "Shuicheng Yan",
      "Li Jian",
      "Qian Liu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.918": {
    "title": "Learning to Describe for Predicting Zero-shot Drug-Drug Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangqi Zhu",
      "Yongqi Zhang",
      "Lei Chen",
      "Bing Qin",
      "Ruifeng Xu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.919": {
    "title": "A Simple Baseline for Knowledge-Based Visual Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexandros Xenos",
      "Themos Stafylakis",
      "Ioannis Patras",
      "Georgios Tzimiropoulos"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.920": {
    "title": "Unveiling the Essence of Poetry: Introducing a Comprehensive Dataset and Benchmark for Poem Summarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ridwan Mahbub",
      "Ifrad Khan",
      "Samiha Anuva",
      "Md Shahriar",
      "Md Tahmid Rahman Laskar",
      "Sabbir Ahmed"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.921": {
    "title": "Privacy Implications of Retrieval-Based Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangsibo Huang",
      "Samyak Gupta",
      "Zexuan Zhong",
      "Kai Li",
      "Danqi Chen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.922": {
    "title": "IMTLab: An Open-Source Platform for Building, Evaluating, and Diagnosing Interactive Machine Translation Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xu Huang",
      "Zhirui Zhang",
      "Ruize Gao",
      "Yichao Du",
      "Lemao Liu",
      "Guoping Huang",
      "Shuming Shi",
      "Jiajun Chen",
      "Shujian Huang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.923": {
    "title": "Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiwei Sun",
      "Lingyong Yan",
      "Xinyu Ma",
      "Shuaiqiang Wang",
      "Pengjie Ren",
      "Zhumin Chen",
      "Dawei Yin",
      "Zhaochun Ren"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.924": {
    "title": "DiNeR: A Large Realistic Dataset for Evaluating Compositional Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengang Hu",
      "Xiao Liu",
      "Yansong Feng"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.925": {
    "title": "Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Chen",
      "Hexiang Hu",
      "Yi Luan",
      "Haitian Sun",
      "Soravit Changpinyo",
      "Alan Ritter",
      "Ming-Wei Chang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.926": {
    "title": "EDeR: Towards Understanding Dependency Relations Between Events",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruiqi Li",
      "Patrik Haslum",
      "Leyang Cui"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.927": {
    "title": "It Ain't Over: A Multi-aspect Diverse Math Word Problem Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiwoo Kim",
      "Youngbin Kim",
      "Ilwoong Baek",
      "JinYeong Bak",
      "Jongwuk Lee"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.928": {
    "title": "Dr ChatGPT tell me what I want to hear: How different prompts impact health answer correctness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bevan Koopman",
      "Guido Zuccon"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.929": {
    "title": "kNN-LM Does Not Improve Open-ended Text Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shufan Wang",
      "Yixiao Song",
      "Andrew Drozdov",
      "Aparna Garimella",
      "Varun Manjunatha",
      "Mohit Iyyer"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.930": {
    "title": "Towards A Unified View of Sparse Feed-Forward Network in Pretraining Large Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyu Liu",
      "Tim Dettmers",
      "Xi Lin",
      "Veselin Stoyanov",
      "Xian Li"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.931": {
    "title": "Exploring the Impact of Model Scaling on Parameter-Efficient Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yusheng Su",
      "Chi-Min Chan",
      "Jiali Cheng",
      "Yujia Qin",
      "Yankai Lin",
      "Shengding Hu",
      "Zonghan Yang",
      "Ning Ding",
      "Xingzhi Sun",
      "Guotong Xie",
      "Zhiyuan Liu",
      "Maosong Sun"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.932": {
    "title": "STAIR: Learning Sparse Text and Image Representation in Grounded Tokens",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Chen",
      "Bowen Zhang",
      "Liangliang Cao",
      "Jiguang Shen",
      "Tom Gunter",
      "Albin Jose",
      "Alexander Toshev",
      "Yantao Zheng",
      "Jonathon Shlens",
      "Ruoming Pang",
      "Yinfei Yang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.933": {
    "title": "Crossing the Threshold: Idiomatic Machine Translation through Retrieval Augmentation and Loss Weighting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emmy Liu",
      "Aditi Chaudhary",
      "Graham Neubig"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.934": {
    "title": "CoRec: An Easy Approach for Coordination Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qing Wang",
      "Haojie Jia",
      "Wenfei Song",
      "Qi Li"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.935": {
    "title": "A linear time approximation of Wasserstein distance with word embedding selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sho Otao",
      "Makoto Yamada"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.936": {
    "title": "Exchange-of-Thought: Enhancing Large Language Model Capabilities through Cross-Model Communication",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhangyue Yin",
      "Qiushi Sun",
      "Cheng Chang",
      "Qipeng Guo",
      "Junqi Dai",
      "Xuanjing Huang",
      "Xipeng Qiu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.937": {
    "title": "Conversation Understanding using Relational Temporal Graph Neural Networks with Auxiliary Cross-Modality Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cam Van Thi Nguyen",
      "Tuan Mai",
      "Son The",
      "Dang Kieu",
      "Duc-Trong Le"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.938": {
    "title": "Connecting degree and polarity: An artificial language learning study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lisa Bylinina",
      "Alexey Tikhonov",
      "Ekaterina Garmash"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.939": {
    "title": "Prompting with Pseudo-Code Instructions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mayank Mishra",
      "Prince Kumar",
      "Riyaz Bhat",
      "Rudra Murthy",
      "Danish Contractor",
      "Srikanth Tamilselvam"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.940": {
    "title": "CRAB: Assessing the Strength of Causal Relationships Between Real-world Events",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Angelika Romanou",
      "Syrielle Montariol",
      "Debjit Paul",
      "Leo Laugier",
      "Karl Aberer",
      "Antoine Bosselut"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.941": {
    "title": "NORMSAGE: Multi-Lingual Multi-Cultural Norm Discovery from Conversations On-the-Fly",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Fung",
      "Tuhin Chakrabarty",
      "Hao Guo",
      "Owen Rambow",
      "Smaranda Muresan",
      "Heng Ji"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.942": {
    "title": "A State-Vector Framework for Dataset Effects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Esmat Sahak",
      "Zining Zhu",
      "Frank Rudzicz"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.943": {
    "title": "Challenges in Context-Aware Neural Machine Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linghao Jin",
      "Jacqueline He",
      "Jonathan May",
      "Xuezhe Ma"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.944": {
    "title": "Task-Adaptive Tokenization: Enhancing Long-Form Text Generation Efficacy in Mental Health and Beyond",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyang Liu",
      "Naihao Deng",
      "Sahand Sabour",
      "Yilin Jia",
      "Minlie Huang",
      "Rada Mihalcea"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.945": {
    "title": "FACTIFY3M: A benchmark for multimodal fact verification with explainability through 5W Question-Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Megha Chakraborty",
      "Khushbu Pahwa",
      "Anku Rani",
      "Shreyas Chatterjee",
      "Dwip Dalal",
      "Harshit Dave",
      "Ritvik G",
      "Preethi Gurumurthy",
      "Adarsh Mahor",
      "Samahriti Mukherjee",
      "Aditya Pakala",
      "Ishan Paul",
      "Janvita Reddy",
      "Arghya Sarkar",
      "Kinjal Sensharma",
      "Aman Chadha",
      "Amit Sheth",
      "Amitava Das"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.946": {
    "title": "Building Multi-domain Dialog State Trackers from Single-domain Dialogs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Zhu",
      "Zheng Zhang",
      "Xiaoyan Zhu",
      "Minlie Huang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.947": {
    "title": "Specialist or Generalist? Instruction Tuning for Specific NLP Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chufan Shi",
      "Yixuan Su",
      "Cheng Yang",
      "Yujiu Yang",
      "Deng Cai"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.948": {
    "title": "Making Large Language Models Better Data Creators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dong-Ho Lee",
      "Jay Pujara",
      "Mohit Sewak",
      "Ryen White",
      "Sujay Jauhar"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.949": {
    "title": "Hallucination Detection for Generative Large Language Models by Bayesian Sequential Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaohua Wang",
      "Yuliang Yan",
      "Longtao Huang",
      "Xiaoqing Zheng",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.950": {
    "title": "Guideline Learning for In-Context Information Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaoxu Pang",
      "Yixuan Cao",
      "Qiang Ding",
      "Ping Luo"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.951": {
    "title": "Open Information Extraction via Chunks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kuicai Dong",
      "Aixin Sun",
      "Jung-jae Kim",
      "Xiaoli Li"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.952": {
    "title": "Rethinking Word-Level Auto-Completion in Computer-Aided Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyu Chen",
      "Lemao Liu",
      "Guoping Huang",
      "Zhirui Zhang",
      "Mingming Yang",
      "Shuming Shi",
      "Rui Wang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.953": {
    "title": "Automatic Transcription of Handwritten Old Occitan Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Esteban Arias",
      "Vallari Pai",
      "Matthias Schöffel",
      "Christian Heumann",
      "Matthias Aenmacher"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.954": {
    "title": "CorefPrompt: Prompt-based Event Coreference Resolution by Measuring Event Type and Argument Compatibilities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng Xu",
      "Peifeng Li",
      "Qiaoming Zhu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.955": {
    "title": "Anaphor Assisted Document-Level Relation Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chonggang Lu",
      "Richong Zhang",
      "Kai Sun",
      "Jaein Kim",
      "Cunwang Zhang",
      "Yongyi Mao"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.956": {
    "title": "FinEntity: Entity-level Sentiment Classification for Financial Texts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixuan Tang",
      "Yi Yang",
      "Allen Huang",
      "Andy Tam",
      "Justin Tang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.957": {
    "title": "All Things Considered: Detecting Partisan Events from News Media with Cross-Article Comparison",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujian Liu",
      "Xinliang Zhang",
      "Kaijian Zou",
      "Ruihong Huang",
      "Nicholas Beauchamp",
      "Lu Wang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.958": {
    "title": "Rationale-Enhanced Language Models are Better Continual Relation Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weimin Xiong",
      "Yifan Song",
      "Peiyi Wang",
      "Sujian Li"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.959": {
    "title": "BanglaAbuseMeme: A Dataset for Bengali Abusive Meme Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mithun Das",
      "Animesh Mukherjee"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.960": {
    "title": "ScanDL: A Diffusion Model for Generating Synthetic Scanpaths on Texts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lena Bolliger",
      "David Reich",
      "Patrick Haller",
      "Deborah Jakobi",
      "Paul Prasse",
      "Lena Jäger"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.961": {
    "title": "From Values to Opinions: Predicting Human Behaviors and Stances Using Value-Injected Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongjun Kang",
      "Joonsuk Park",
      "Yohan Jo",
      "JinYeong Bak"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.962": {
    "title": "Analyzing Film Adaptation through Narrative Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tanzir Pial",
      "Shahreen Aunti",
      "Charuta Pethe",
      "Allen Kim",
      "Steven Skiena"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.963": {
    "title": "Inverse Scaling Can Become U-Shaped",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jason Wei",
      "Najoung Kim",
      "Yi Tay",
      "Quoc Le"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.964": {
    "title": "Nearest Neighbor Machine Translation is Meta-Optimizer on Output Projection Layer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruize Gao",
      "Zhirui Zhang",
      "Yichao Du",
      "Lemao Liu",
      "Rui Wang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.965": {
    "title": "Variance Matters: Detecting Semantic Differences without Corpus/Word Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryo Nagata",
      "Hiroya Takamura",
      "Naoki Otani",
      "Yoshifumi Kawasaki"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.966": {
    "title": "MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyuan Liu",
      "Sihang Li",
      "Yanchen Luo",
      "Hao Fei",
      "Yixin Cao",
      "Kenji Kawaguchi",
      "Xiang Wang",
      "Tat-Seng Chua"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.967": {
    "title": "A Training-Free Debiasing Framework with Counterfactual Reasoning for Conversational Emotion Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Geng Tu",
      "Ran Jing",
      "Bin Liang",
      "Min Yang",
      "Kam-Fai Wong",
      "Ruifeng Xu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.968": {
    "title": "Self-ICL: Zero-Shot In-Context Learning with Self-Generated Demonstrations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei-Lin Chen",
      "Cheng-Kuang Wu",
      "Yun-Nung Chen",
      "Hsin-Hsi Chen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.969": {
    "title": "Learning Knowledge-Enhanced Contextual Language Representations for Domain Natural Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taolin Zhang",
      "Ruyao Xu",
      "Chengyu Wang",
      "Zhongjie Duan",
      "Cen Chen",
      "Minghui Qiu",
      "Dawei Cheng",
      "Xiaofeng He",
      "Weining Qian"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.970": {
    "title": "ScdNER: Span-Based Consistency-Aware Document-Level Named Entity Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ying Wei",
      "Qi Li"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.971": {
    "title": "MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zexuan Zhong",
      "Zhengxuan Wu",
      "Christopher Manning",
      "Christopher Potts",
      "Danqi Chen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.972": {
    "title": "Stance Detection on Social Media with Background Knowledge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ang Li",
      "Bin Liang",
      "Jingqian Zhao",
      "Bowen Zhang",
      "Min Yang",
      "Ruifeng Xu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.973": {
    "title": "Vision-Enhanced Semantic Entity Recognition in Document Images via Visually-Asymmetric Consistency Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Wang",
      "Xiahua Chen",
      "Rui Wang",
      "Chenhui Chu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.974": {
    "title": "NormDial: A Comparable Bilingual Synthetic Dialog Dataset for Modeling Social Norm Adherence and Violation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oliver Li",
      "Mallika Subramanian",
      "Arkadiy Saakyan",
      "Sky CH-Wang",
      "Smaranda Muresan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.975": {
    "title": "ClimateBERT-NetZero: Detecting and Assessing Net Zero and Reduction Targets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tobias Schimanski",
      "Julia Bingler",
      "Mathias Kraus",
      "Camilla Hyslop",
      "Markus Leippold"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.976": {
    "title": "Leap-of-Thought: Accelerating Transformers via Dynamic Token Routing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yeachan Kim",
      "Junho Kim",
      "Jun-Hyung Park",
      "Mingyu Lee",
      "SangKeun Lee"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.977": {
    "title": "Reinforcement Replaces Supervision: Query focused Summarization using Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Swaroop Nath",
      "Pushpak Bhattacharyya",
      "Harshad Khadilkar"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.978": {
    "title": "Fair Text Classification with Wasserstein Independence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thibaud Leteno",
      "Antoine Gourru",
      "Charlotte Laclau",
      "Rémi Emonet",
      "Christophe Gravier"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.979": {
    "title": "TacoPrompt: A Collaborative Multi-Task Prompt Learning Method for Self-Supervised Taxonomy Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyuan Xu",
      "Ciyi Liu",
      "Yuhang Niu",
      "Yunong Chen",
      "Xiangrui Cai",
      "Yanlong Wen",
      "Xiaojie Yuan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.980": {
    "title": "An Attribution Method for Siamese Encoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucas Moeller",
      "Dmitry Nikolaev",
      "Sebastian Padó"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.981": {
    "title": "Global Voices, Local Biases: Socio-Cultural Prejudices across Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anjishnu Mukherjee",
      "Chahat Raj",
      "Ziwei Zhu",
      "Antonios Anastasopoulos"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.982": {
    "title": "Graph vs. Sequence: An Empirical Study on Knowledge Forms for Knowledge-Grounded Dialogue",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yizhe Yang",
      "Heyan Huang",
      "Yuhang Liu",
      "Yang Gao"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.983": {
    "title": "Are Compressed Language Models Less Subgroup Robust?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leonidas Gee",
      "Andrea Zugarini",
      "Novi Quadrianto"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.984": {
    "title": "Length Does Matter: Summary Length can Bias Summarization Metrics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaobo Guo",
      "Soroush Vosoughi"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.985": {
    "title": "NL2TL: Transforming Natural Languages to Temporal Logics using Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongchao Chen",
      "Rujul Gandhi",
      "Yang Zhang",
      "Chuchu Fan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.986": {
    "title": "Reformulating NLP tasks to Capture Longitudinal Manifestation of Language Disorders in People with Dementia",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dimitris Gkoumas",
      "Matthew Purver",
      "Maria Liakata"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.987": {
    "title": "Elevating Code-mixed Text Handling through Auditory Information of Words",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mamta Mamta",
      "Zishan Ahmad",
      "Asif Ekbal"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.988": {
    "title": "Predict and Use: Harnessing Predicted Gaze to Improve Multimodal Sarcasm Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Divyank Tiwari",
      "Diptesh Kanojia",
      "Anupama Ray",
      "Apoorva Nunna",
      "Pushpak Bhattacharyya"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.989": {
    "title": "Fine-grained Medical Vision-Language Representation Learning for Radiology Report Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyuan Wang",
      "Bo Peng",
      "Yichao Liu",
      "Qi Peng"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.990": {
    "title": "ViT-TTS: Visual Text-to-Speech with Scalable Diffusion Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huadai Liu",
      "Rongjie Huang",
      "Xuan Lin",
      "Wenqiang Xu",
      "Maozong Zheng",
      "Hong Chen",
      "Jinzheng He",
      "Zhou Zhao"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.991": {
    "title": "Consistency Analysis of ChatGPT",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Myeongjun Jang",
      "Thomas Lukasiewicz"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.992": {
    "title": "Do Differences in Values Influence Disagreements in Online Discussions?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michiel van der Meer",
      "Piek Vossen",
      "Catholijn Jonker",
      "Pradeep Murukannaiah"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.993": {
    "title": "Automated Fact-Checking in Dialogue: Are Specialized Models Needed?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eric Chamoun",
      "Marzieh Saeidi",
      "Andreas Vlachos"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.994": {
    "title": "A Digital Language Coherence Marker for Monitoring Dementia",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dimitris Gkoumas",
      "Adam Tsakalidis",
      "Maria Liakata"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.995": {
    "title": "Detecting Spoilers in Movie Reviews with External Movie Knowledge and User Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heng Wang",
      "Wenqian Zhang",
      "Yuyang Bai",
      "Zhaoxuan Tan",
      "Shangbin Feng",
      "Qinghua Zheng",
      "Minnan Luo"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.996": {
    "title": "Joyful: Joint Modality Fusion and Graph Contrastive Learning for Multimoda Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongyuan Li",
      "Yusong Wang",
      "Kotaro Funakoshi",
      "Manabu Okumura"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.997": {
    "title": "HyperRank: Hyperbolic Ranking Model for Unsupervised Keyphrase Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingyang Song",
      "Huafeng Liu",
      "Liping Jing"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.998": {
    "title": "Assessing the influence of attractor-verb distance on grammatical agreement in humans and language models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christos Zacharopoulos",
      "Théo Desbordes",
      "Mathias Sablé-Meyer"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.999": {
    "title": "Federated Meta-Learning for Emotion and Sentiment Aware Multi-modal Complaint Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Apoorva Singh",
      "Siddarth Chandrasekar",
      "Sriparna Saha",
      "Tanmay Sen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1000": {
    "title": "Semantic Similarity Models for Depression Severity Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anxo Pérez",
      "Neha Warikoo",
      "Kexin Wang",
      "Javier Parapar",
      "Iryna Gurevych"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1001": {
    "title": "Hop, Union, Generate: Explainable Multi-hop Reasoning without Rationale Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenting Zhao",
      "Justin Chiu",
      "Claire Cardie",
      "Alexander Rush"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1002": {
    "title": "To Split or Not to Split: Composing Compounds in Contextual Vector Spaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christopher Jenkins",
      "Filip Miletic",
      "Sabine Schulte im Walde"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1003": {
    "title": "ToolWriter: Question Specific Tool Synthesis for Tabular Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Carlos Gemmell",
      "Jeff Dalton"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1004": {
    "title": "Interactive Text-to-SQL Generation via Editable Step-by-Step Explanations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Tian",
      "Zheng Zhang",
      "Zheng Ning",
      "Toby Li",
      "Jonathan Kummerfeld",
      "Tianyi Zhang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1005": {
    "title": "CoCo: Coherence-Enhanced Machine-Generated Text Detection Under Low Resource With Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoming Liu",
      "Zhaohan Zhang",
      "Yichen Wang",
      "Hang Pu",
      "Yu Lan",
      "Chao Shen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1006": {
    "title": "AnyTOD: A Programmable Task-Oriented Dialog System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeffrey Zhao",
      "Yuan Cao",
      "Raghav Gupta",
      "Harrison Lee",
      "Abhinav Rastogi",
      "Mingqiu Wang",
      "Hagen Soltau",
      "Izhak Shafran",
      "Yonghui Wu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1007": {
    "title": "Can LMs Generalize to Future Data? An Empirical Analysis on Text Summarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chi Cheang",
      "Hou Chan",
      "Derek Wong",
      "Xuebo Liu",
      "Zhaocong Li",
      "Yanming Sun",
      "Shudong Liu",
      "Lidia Chao"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1008": {
    "title": "Zero-Shot Multi-Label Topic Inference with Sentence Encoders and LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Souvika Sarkar",
      "Dongji Feng",
      "Shubhra Kanti Karmaker Santu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1009": {
    "title": "TaskDiff: A Similarity Metric for Task-Oriented Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ankita Bhaumik",
      "Praveen Venkateswaran",
      "Yara Rizk",
      "Vatche Isahagian"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1010": {
    "title": "Not all Fake News is Written: A Dataset and Analysis of Misleading Video Headlines",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yoo Sung",
      "Jordan Boyd-Graber",
      "Naeemul Hassan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1011": {
    "title": "Learning From Free-Text Human Feedback – Collect New Datasets Or Extend Existing Ones?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dominic Petrak",
      "Nafise Moosavi",
      "Ye Tian",
      "Nikolai Rozanov",
      "Iryna Gurevych"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1012": {
    "title": "Euphemistic Abuse – A New Dataset and Classification Experiments for Implicitly Abusive Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Wiegand",
      "Jana Kampfmeier",
      "Elisabeth Eder",
      "Josef Ruppenhofer"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1013": {
    "title": "Exploring Distributional Shifts in Large Language Models for Code Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shushan Arakelyan",
      "Rocktim Das",
      "Yi Mao",
      "Xiang Ren"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1014": {
    "title": "ATHENA: Mathematical Reasoning with Thought Expansion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jb. Kim",
      "Hazel Kim",
      "Joonghyuk Hahn",
      "Yo-Sub Han"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1015": {
    "title": "A Benchmark for Reasoning with Spatial Prepositions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Iulia Comsa",
      "Srini Narayanan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1016": {
    "title": "TIMELINE: Exhaustive Annotation of Temporal Relations Supporting the Automatic Ordering of Events in News Articles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sarah Alsayyahi",
      "Riza Batista-Navarro"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1017": {
    "title": "Mitigating Over-Generation for Unsupervised Keyphrase Extraction with Heterogeneous Centrality Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingyang Song",
      "Pengyu Xu",
      "Yi Feng",
      "Huafeng Liu",
      "Liping Jing"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1018": {
    "title": "Towards Interpretable and Efficient Automatic Reference-Based Summarization Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixin Liu",
      "Alexander Fabbri",
      "Yilun Zhao",
      "Pengfei Liu",
      "Shafiq Joty",
      "Chien-Sheng Wu",
      "Caiming Xiong",
      "Dragomir Radev"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1019": {
    "title": "MAUD: An Expert-Annotated Legal NLP Dataset for Merger Agreement Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Steven Wang",
      "Antoine Scardigli",
      "Leonard Tang",
      "Wei Chen",
      "Dmitry Levkin",
      "Anya Chen",
      "Spencer Ball",
      "Thomas Woodside",
      "Oliver Zhang",
      "Dan Hendrycks"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1020": {
    "title": "PK-ICR: Persona-Knowledge Interactive Multi-Context Retrieval for Grounded Dialogue",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minsik Oh",
      "Joosung Lee",
      "Jiwei Li",
      "Guoyin Wang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1021": {
    "title": "More Than Spoken Words: Nonverbal Message Extraction and Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dian Yu",
      "Xiaoyang Wang",
      "Wanshun Chen",
      "Nan Du",
      "Longyue Wang",
      "Haitao Mi",
      "Dong Yu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1022": {
    "title": "Can language models learn analogical reasoning? Investigating training objectives and comparisons to human performance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Molly Petersen",
      "Lonneke van der Plas"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1023": {
    "title": "FAME: Flexible, Scalable Analogy Mappings Engine",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shahar Jacob",
      "Chen Shani",
      "Dafna Shahaf"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1024": {
    "title": "A Self-training Framework for Automated Medical Report Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyuan Wang",
      "Zheng Liu",
      "Bo Peng"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1025": {
    "title": "A Picture is Worth a Thousand Words: Language Models Plan from Pixels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anthony Liu",
      "Lajanugen Logeswaran",
      "Sungryull Sohn",
      "Honglak Lee"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1026": {
    "title": "Interpreting and Exploiting Functional Specialization in Multi-Head Attention under Multi-task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chong Li",
      "Shaonan Wang",
      "Yunhao Zhang",
      "Jiajun Zhang",
      "Chengqing Zong"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1027": {
    "title": "Multilingual Previously Fact-Checked Claim Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matúš Pikuliak",
      "Ivan Srba",
      "Robert Moro",
      "Timo Hromadka",
      "Timotej Smoleň",
      "Martin Melišek",
      "Ivan Vykopal",
      "Jakub Simko",
      "Juraj Podroužek",
      "Maria Bielikova"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1028": {
    "title": "ALCAP: Alignment-Augmented Music Captioner",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihao He",
      "Weituo Hao",
      "Wei-Tsung Lu",
      "Changyou Chen",
      "Kristina Lerman",
      "Xuchen Song"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1029": {
    "title": "Do Transformers Parse while Predicting the Masked Word?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Zhao",
      "Abhishek Panigrahi",
      "Rong Ge",
      "Sanjeev Arora"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1030": {
    "title": "Composable Text Controls in Latent Space with ODEs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangyi Liu",
      "Zeyu Feng",
      "Yuan Gao",
      "Zichao Yang",
      "Xiaodan Liang",
      "Junwei Bao",
      "Xiaodong He",
      "Shuguang Cui",
      "Zhen Li",
      "Zhiting Hu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1031": {
    "title": "P5: Plug-and-Play Persona Prompting for Personalized Response Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joosung Lee",
      "Minsik Oh",
      "Donghun Lee"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1032": {
    "title": "Reader: Model-based language-instructed reinforcement learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicola Dainese",
      "Pekka Marttinen",
      "Alexander Ilin"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1033": {
    "title": "Adapting Offline Speech Translation Models for Streaming with Future-Aware Distillation and Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Biao Fu",
      "Minpeng Liao",
      "Kai Fan",
      "Zhongqiang Huang",
      "Boxing Chen",
      "Yidong Chen",
      "Xiaodong Shi"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1034": {
    "title": "Relation-aware Ensemble Learning for Knowledge Graph Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ling Yue",
      "Yongqi Zhang",
      "Quanming Yao",
      "Yong Li",
      "Xian Wu",
      "Ziheng Zhang",
      "Zhenxi Lin",
      "Yefeng Zheng"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1035": {
    "title": "GenEx: A Commonsense-aware Unified Generative Framework for Explainable Cyberbullying Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Krishanu Maity",
      "Raghav Jain",
      "Prince Jha",
      "Sriparna Saha",
      "Pushpak Bhattacharyya"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1036": {
    "title": "Document-Level Machine Translation with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Longyue Wang",
      "Chenyang Lyu",
      "Tianbo Ji",
      "Zhirui Zhang",
      "Dian Yu",
      "Shuming Shi",
      "Zhaopeng Tu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1037": {
    "title": "Multilingual Simplification of Medical Texts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Joseph",
      "Kathryn Kazanas",
      "Keziah Reina",
      "Vishnesh Ramanathan",
      "Wei Xu",
      "Byron Wallace",
      "Junyi Li"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1038": {
    "title": "When Reviewers Lock Horns: Finding Disagreements in Scientific Peer Reviews",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sandeep Kumar",
      "Tirthankar Ghosal",
      "Asif Ekbal"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1039": {
    "title": "Argue with Me Tersely: Towards Sentence-Level Counter-Argument Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayu Lin",
      "Rong Ye",
      "Meng Han",
      "Qi Zhang",
      "Ruofei Lai",
      "Xinyu Zhang",
      "Zhao Cao",
      "Xuanjing Huang",
      "Zhongyu Wei"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1040": {
    "title": "JASMINE: Arabic GPT Models for Few-Shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "El Moatez Billah Nagoudi",
      "Muhammad Abdul-Mageed",
      "AbdelRahim Elmadany",
      "Alcides Inciarte",
      "Md Tawkat Islam Khondaker"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1041": {
    "title": "NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mael Jullien",
      "Marco Valentino",
      "Hannah Frost",
      "Paul O’Regan",
      "Dónal Landers",
      "Andre Freitas"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1042": {
    "title": "Addressing Linguistic Bias through a Contrastive Analysis of Academic Writing in the NLP Domain",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Robert Ridley",
      "Zhen Wu",
      "Jianbing Zhang",
      "Shujian Huang",
      "Xinyu Dai"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1043": {
    "title": "RobustGEC: Robust Grammatical Error Correction Against Subtle Context Perturbation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Zhang",
      "Leyang Cui",
      "Enbo Zhao",
      "Wei Bi",
      "Shuming Shi"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1044": {
    "title": "Detecting Propaganda Techniques in Code-Switched Social Media Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Salman",
      "Asif Hanif",
      "Shady Shehata",
      "Preslav Nakov"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1045": {
    "title": "Speech Recognition and Meaning Interpretation: Towards Disambiguation of Structurally Ambiguous Spoken Utterances in Indonesian",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruhiyah Widiaputri",
      "Ayu Purwarianti",
      "Dessi Lestari",
      "Kurniawati Azizah",
      "Dipta Tanaya",
      "Sakriani Sakti"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1046": {
    "title": "Target-Agnostic Gender-Aware Contrastive Learning for Mitigating Bias in Multilingual Machine Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minwoo Lee",
      "Hyukhun Koh",
      "Kang-il Lee",
      "Dongdong Zhang",
      "Minsung Kim",
      "Kyomin Jung"
    ]
  },
  "https://aclanthology.org/2023.emnlp-main.1047": {
    "title": "Code-Switching Metrics Using Intonation Units",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rebecca Pattichis",
      "Dora LaCasse",
      "Sonya Trawick",
      "Rena Cacoullos"
    ]
  },
  "https://aclanthology.org/2023.emnlp-tutorial.1": {
    "title": "NLP+Vis: NLP Meets Visualization",
    "volume": "tutorial",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shafiq Joty",
      "Enamul Hoque",
      "Jesse Vig"
    ]
  },
  "https://aclanthology.org/2023.emnlp-tutorial.2": {
    "title": "Security Challenges in Natural Language Processing Models",
    "volume": "tutorial",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiongkai Xu",
      "Xuanli He"
    ]
  },
  "https://aclanthology.org/2023.emnlp-tutorial.3": {
    "title": "Designing, Evaluating, and Learning from Humans Interacting with NLP Models",
    "volume": "tutorial",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tongshuang Wu",
      "Diyi Yang",
      "Sebastin Santy"
    ]
  },
  "https://aclanthology.org/2023.emnlp-tutorial.4": {
    "title": "LLM-driven Instruction Following: Progresses and Concerns",
    "volume": "tutorial",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenpeng Yin",
      "Qinyuan Ye",
      "Pengfei Liu",
      "Xiang Ren",
      "Hinrich Schütze"
    ]
  },
  "https://aclanthology.org/2023.emnlp-tutorial.5": {
    "title": "Mitigating Societal Harms in Large Language Models",
    "volume": "tutorial",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sachin Kumar",
      "Vidhisha Balachandran",
      "Lucille Njoo",
      "Antonios Anastasopoulos",
      "Yulia Tsvetkov"
    ]
  },
  "https://aclanthology.org/2023.emnlp-tutorial.6": {
    "title": "Creative Natural Language Generation",
    "volume": "tutorial",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tuhin Chakrabarty",
      "Vishakh Padmakumar",
      "He He",
      "Nanyun Peng"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.1": {
    "title": "Fabricator: An Open Source Toolkit for Generating Labeled Training Data with Teacher LLMs",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonas Golde",
      "Patrick Haller",
      "Felix Hamborg",
      "Julian Risch",
      "Alan Akbik"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.2": {
    "title": "End-to-End Evaluation for Low-Latency Simultaneous Speech Translation",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christian Huber",
      "Tu Anh Dinh",
      "Carlos Mullov",
      "Ngoc-Quan Pham",
      "Thai Binh Nguyen",
      "Fabian Retkowski",
      "Stefan Constantin",
      "Enes Ugan",
      "Danni Liu",
      "Zhaolin Li",
      "Sai Koneru",
      "Jan Niehues",
      "Alexander Waibel"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.3": {
    "title": "CHATREPORT: Democratizing Sustainability Disclosure Analysis through LLM-based Tools",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingwei Ni",
      "Julia Bingler",
      "Chiara Colesanti-Senni",
      "Mathias Kraus",
      "Glen Gostlow",
      "Tobias Schimanski",
      "Dominik Stammbach",
      "Saeid Ashraf Vaghefi",
      "Qian Wang",
      "Nicolas Webersinke",
      "Tobias Wekhof",
      "Tingyu Yu",
      "Markus Leippold"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.4": {
    "title": "RaLLe: A Framework for Developing and Evaluating Retrieval-Augmented Large Language Models",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yasuto Hoshi",
      "Daisuke Miyashita",
      "Youyang Ng",
      "Kento Tatsuno",
      "Yasuhiro Morioka",
      "Osamu Torii",
      "Jun Deguchi"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.5": {
    "title": "VIST5: An Adaptive, Retrieval-Augmented Language Model for Visualization-oriented Dialog",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Henrik Voigt",
      "Nuno Carvalhais",
      "Monique Meuschke",
      "Markus Reichstein",
      "Sina Zarrie",
      "Kai Lawonn"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.6": {
    "title": "H2O Open Ecosystem for State-of-the-art Large Language Models",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arno Candel",
      "Jon McKinney",
      "Philipp Singer",
      "Pascal Pfeiffer",
      "Maximilian Jeblick",
      "Chun Ming Lee",
      "Marcos Conde"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.7": {
    "title": "Koala: An Index for Quantifying Overlaps with Pre-training Corpora",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thuy-Trang Vu",
      "Xuanli He",
      "Gholamreza Haffari",
      "Ehsan Shareghi"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.8": {
    "title": "Sudowoodo: A Chinese Lyric Imitation System with Source Lyrics",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongzhu Chang",
      "Rongsheng Zhang",
      "Lin Jiang",
      "Qihang Chen",
      "Le Zhang",
      "Jiashu Pu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.9": {
    "title": "ConvLab-3: A Flexible Dialogue System Toolkit Based on a Unified Data Format",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Zhu",
      "Christian Geishauser",
      "Hsien-chin Lin",
      "Carel van Niekerk",
      "Baolin Peng",
      "Zheng Zhang",
      "Shutong Feng",
      "Michael Heck",
      "Nurul Lubis",
      "Dazhen Wan",
      "Xiaochen Zhu",
      "Jianfeng Gao",
      "Milica Gasic",
      "Minlie Huang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.10": {
    "title": "FLEEK: Factual Error Detection and Correction with Evidence Retrieved from External Knowledge",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Farima Fatahi Bayat",
      "Kun Qian",
      "Benjamin Han",
      "Yisi Sang",
      "Anton Belyy",
      "Samira Khorshidi",
      "Fei Wu",
      "Ihab Ilyas",
      "Yunyao Li"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.11": {
    "title": "YATO: Yet Another deep learning based Text analysis Open toolkit",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeqiang Wang",
      "Yile Wang",
      "Jiageng Wu",
      "Zhiyang Teng",
      "Jie Yang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.12": {
    "title": "Spacerini: Plug-and-play Search Engines with Pyserini and Hugging Face",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christopher Akiki",
      "Odunayo Ogundepo",
      "Aleksandra Piktus",
      "Xinyu Zhang",
      "Akintunde Oladipo",
      "Jimmy Lin",
      "Martin Potthast"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.13": {
    "title": "Adapters: A Unified Library for Parameter-Efficient and Modular Transfer Learning",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Clifton Poth",
      "Hannah Sterz",
      "Indraneil Paul",
      "Sukannya Purkayastha",
      "Leon Engländer",
      "Timo Imhof",
      "Ivan Vuli",
      "Sebastian Ruder",
      "Iryna Gurevych",
      "Jonas Pfeiffer"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.14": {
    "title": "INTELMO: Enhancing Models' Adoption of Interactive Interfaces",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chunxu Yang",
      "Chien-Sheng Wu",
      "Lidiya Murakhovs’ka",
      "Philippe Laban",
      "Xiang Chen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.15": {
    "title": "Humanoid Agents: Platform for Simulating Human-like Generative Agents",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhilin Wang",
      "Yu Ying Chiu",
      "Yu Cheung Chiu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.16": {
    "title": "TP-Detector: Detecting Turning Points in the Engineering Process of Large-scale Projects",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Wu",
      "WenHan Chao",
      "Xian Zhou",
      "Zhunchen Luo"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.17": {
    "title": "CLEVA: Chinese Language Models EVAluation Platform",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanyang Li",
      "Jianqiao Zhao",
      "Duo Zheng",
      "Zi-Yuan Hu",
      "Zhi Chen",
      "Xiaohui Su",
      "Yongfeng Huang",
      "Shijia Huang",
      "Dahua Lin",
      "Michael Lyu",
      "Liwei Wang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.18": {
    "title": "DOPA METER – A Tool Suite for Metrical Document Profiling and Aggregation",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christina Lohr",
      "Udo Hahn"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.19": {
    "title": "Muted: Multilingual Targeted Offensive Speech Identification and Visualization",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christoph Tillmann",
      "Aashka Trivedi",
      "Sara Rosenthal",
      "Santosh Borse",
      "Rong Zhang",
      "Avirup Sil",
      "Bishwaranjan Bhattacharjee"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.20": {
    "title": "Gentopia.AI: A Collaborative Platform for Tool-Augmented LLMs",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Binfeng Xu",
      "Xukun Liu",
      "Hua Shen",
      "Zeyu Han",
      "Yuhan Li",
      "Murong Yue",
      "Zhiyuan Peng",
      "Yuchen Liu",
      "Ziyu Yao",
      "Dongkuan Xu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.21": {
    "title": "MusicAgent: An AI Agent for Music Understanding and Generation with Large Language Models",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dingyao Yu",
      "Kaitao Song",
      "Peiling Lu",
      "Tianyu He",
      "Xu Tan",
      "Wei Ye",
      "Shikun Zhang",
      "Jiang Bian"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.22": {
    "title": "SentAlign: Accurate and Scalable Sentence Alignment",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Steinthor Steingrimsson",
      "Hrafn Loftsson",
      "Andy Way"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.23": {
    "title": "QACheck: A Demonstration System for Question-Guided Multi-Hop Fact-Checking",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liangming Pan",
      "Xinyuan Lu",
      "Min-Yen Kan",
      "Preslav Nakov"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.24": {
    "title": "RobustQA: A Framework for Adversarial Text Generation Analysis on Question Answering Systems",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yasaman Boreshban",
      "Seyed Morteza Mirbostani",
      "Seyedeh Fatemeh Ahmadi",
      "Gita Shojaee",
      "Fatemeh Kamani",
      "Gholamreza Ghassem-Sani",
      "Seyed Abolghasem Mirroshandel"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.25": {
    "title": "Kandinsky: An Improved Text-to-Image Synthesis with Image Prior and Latent Diffusion",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anton Razzhigaev",
      "Arseniy Shakhmatov",
      "Anastasia Maltseva",
      "Vladimir Arkhipkin",
      "Igor Pavlov",
      "Ilya Ryabov",
      "Angelina Kuts",
      "Alexander Panchenko",
      "Andrey Kuznetsov",
      "Denis Dimitrov"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.26": {
    "title": "NewsRecLib: A PyTorch-Lightning Library for Neural News Recommendation",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andreea Iana",
      "Goran Glavaš",
      "Heiko Paulheim"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.27": {
    "title": "MiniChain: A Small Library for Coding with Large Language Models",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Rush"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.28": {
    "title": "Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Viet Lai",
      "Chien Nguyen",
      "Nghia Ngo",
      "Thuat Nguyen",
      "Franck Dernoncourt",
      "Ryan Rossi",
      "Thien Nguyen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.29": {
    "title": "SAGEViz: SchemA GEneration and Visualization",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sugam Devare",
      "Mahnaz Koupaee",
      "Gautham Gunapati",
      "Sayontan Ghosh",
      "Sai Vallurupalli",
      "Yash Kumar Lal",
      "Francis Ferraro",
      "Nathanael Chambers",
      "Greg Durrett",
      "Raymond Mooney",
      "Katrin Erk",
      "Niranjan Balasubramanian"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.30": {
    "title": "Thresh: A Unified, Customizable and Deployable Platform for Fine-Grained Text Evaluation",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Heineman",
      "Yao Dou",
      "Wei Xu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.31": {
    "title": "InsightPilot: An LLM-Empowered Automated Data Exploration System",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pingchuan Ma",
      "Rui Ding",
      "Shuai Wang",
      "Shi Han",
      "Dongmei Zhang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.32": {
    "title": "SynJax: Structured Probability Distributions for JAX",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miloš Stanojević",
      "Laurent Sartran"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.33": {
    "title": "RESIN-EDITOR: A Schema-guided Hierarchical Event Graph Visualizer and Editor",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Khanh Duy Nguyen",
      "Zixuan Zhang",
      "Reece Suchocki",
      "Sha Li",
      "Martha Palmer",
      "Susan Windisch Brown",
      "Jiawei Han",
      "Heng Ji"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.34": {
    "title": "DRGCoder: Explainable Clinical Coding for the Early Prediction of Diagnostic-Related Groups",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Hajialigol",
      "Derek Kaknes",
      "Tanner Barbour",
      "Daphne Yao",
      "Chris North",
      "Jimeng Sun",
      "David Liem",
      "Xuan Wang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.35": {
    "title": "CAMRA: Copilot for AMR Annotation",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jon Cai",
      "Shafiuddin Rehan Ahmed",
      "Julia Bonn",
      "Kristin Wright-Bettner",
      "Martha Palmer",
      "James H. Martin"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.36": {
    "title": "Reaction Miner: An Integrated System for Chemical Reaction Extraction from Textual Data",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ming Zhong",
      "Siru Ouyang",
      "Yizhu Jiao",
      "Priyanka Kargupta",
      "Leo Luo",
      "Yanzhen Shen",
      "Bobby Zhou",
      "Xianrui Zhong",
      "Xuan Liu",
      "Hongxiang Li",
      "Jinfeng Xiao",
      "Minhao Jiang",
      "Vivian Hu",
      "Xuan Wang",
      "Heng Ji",
      "Martin Burke",
      "Huimin Zhao",
      "Jiawei Han"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.37": {
    "title": "CHAMP: Efficient Annotation and Consolidation of Cluster Hierarchies",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arie Cattan",
      "Tom Hope",
      "Doug Downey",
      "Roy Bar-Haim",
      "Lilach Eden",
      "Yoav Kantor",
      "Ido Dagan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.38": {
    "title": "Prompt2Model: Generating Deployable Models from Natural Language Instructions",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vijay Viswanathan",
      "Chenyang Zhao",
      "Amanda Bertsch",
      "Tongshuang Wu",
      "Graham Neubig"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.39": {
    "title": "NewsSense: Reference-free Verification via Cross-document Comparison",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeremiah Milbauer",
      "Ziqi Ding",
      "Zhijin Wu",
      "Tongshuang Wu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.40": {
    "title": "NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable Rails",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Traian Rebedea",
      "Razvan Dinu",
      "Makesh Narsimhan Sreedhar",
      "Christopher Parisien",
      "Jonathan Cohen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.41": {
    "title": "LM-Polygraph: Uncertainty Estimation for Language Models",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ekaterina Fadeeva",
      "Roman Vashurin",
      "Akim Tsvigun",
      "Artem Vazhentsev",
      "Sergey Petrakov",
      "Kirill Fedyanin",
      "Daniil Vasilev",
      "Elizaveta Goncharova",
      "Alexander Panchenko",
      "Maxim Panov",
      "Timothy Baldwin",
      "Artem Shelmanov"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.42": {
    "title": "Descriptive Knowledge Graph in Biomedical Domain",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kerui Zhu",
      "Jie Huang",
      "Kevin Chen-Chuan Chang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.43": {
    "title": "Prompterator: Iterate Efficiently towards More Effective Prompts",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel Sučik",
      "Daniel Skala",
      "Andrej Švec",
      "Peter Hraška",
      "Marek Šuppa"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.44": {
    "title": "ZhuJiu: A Multi-dimensional, Multi-faceted Chinese Benchmark for Large Language Models",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baoli Zhang",
      "Haining Xie",
      "Pengfan Du",
      "Junhao Chen",
      "Pengfei Cao",
      "Yubo Chen",
      "Shengping Liu",
      "Kang Liu",
      "Jun Zhao"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.45": {
    "title": "PaperMage: A Unified Toolkit for Processing, Representing, and Manipulating Visually-Rich Scientific Documents",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyle Lo",
      "Zejiang Shen",
      "Benjamin Newman",
      "Joseph Chang",
      "Russell Authur",
      "Erin Bransom",
      "Stefan Candra",
      "Yoganand Chandrasekhar",
      "Regan Huff",
      "Bailey Kuehl",
      "Amanpreet Singh",
      "Chris Wilhelm",
      "Angele Zamarron",
      "Marti A. Hearst",
      "Daniel Weld",
      "Doug Downey",
      "Luca Soldaini"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.46": {
    "title": "OmniEvent: A Comprehensive, Fair, and Easy-to-Use Toolkit for Event Understanding",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Peng",
      "Xiaozhi Wang",
      "Feng Yao",
      "Zimu Wang",
      "Chuzhao Zhu",
      "Kaisheng Zeng",
      "Lei Hou",
      "Juanzi Li"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.47": {
    "title": "CocoSciSum: A Scientific Summarization Toolkit with Compositional Controllability",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixi Ding",
      "Yanxia Qin",
      "Qian Liu",
      "Min-Yen Kan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.48": {
    "title": "CoLLiE: Collaborative Training of Large Language Models in an Efficient Way",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Lv",
      "Shuo Zhang",
      "Tianle Gu",
      "Shuhao Xing",
      "Jiawei Hong",
      "Keyu Chen",
      "Xiaoran Liu",
      "Yuqing Yang",
      "Honglin Guo",
      "Tengxiao Liu",
      "Yu Sun",
      "Qipeng Guo",
      "Hang Yan",
      "Xipeng Qiu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.49": {
    "title": "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hang Zhang",
      "Xin Li",
      "Lidong Bing"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.50": {
    "title": "SummHelper: Collaborative Human-Computer Summarization",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aviv Slobodkin",
      "Niv Nachum",
      "Shmuel Amar",
      "Ori Shapira",
      "Ido Dagan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.51": {
    "title": "ModelScope-Agent: Building Your Customizable Agent System with Open-source Large Language Models",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenliang Li",
      "He Chen",
      "Ming Yan",
      "Weizhou Shen",
      "Haiyang Xu",
      "Zhikai Wu",
      "Zhicheng Zhang",
      "Wenmeng Zhou",
      "Yingda Chen",
      "Chen Cheng",
      "Hongzhu Shi",
      "Ji Zhang",
      "Fei Huang",
      "Jingren Zhou"
    ]
  },
  "https://aclanthology.org/2023.emnlp-demo.52": {
    "title": "EfficientOCR: An Extensible, Open-Source Package for Efficiently Digitizing World Knowledge",
    "volume": "demo",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tom Bryan",
      "Jacob Carlson",
      "Abhishek Arora",
      "Melissa Dell"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.1": {
    "title": "BeautifulPrompt: Towards Automatic Prompt Engineering for Text-to-Image Synthesis",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tingfeng Cao",
      "Chengyu Wang",
      "Bingyan Liu",
      "Ziheng Wu",
      "Jinhui Zhu",
      "Jun Huang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.2": {
    "title": "Enhancing Language Model with Unit Test Techniques for Efficient Regular Expression Generation",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenhui Mao",
      "Xiexiong Lin",
      "Xin Jin",
      "Xin Zhang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.3": {
    "title": "A Comparative Analysis of Task-Agnostic Distillation Methods for Compressing Transformer Language Models",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Takuma Udagawa",
      "Aashka Trivedi",
      "Michele Merler",
      "Bishwaranjan Bhattacharjee"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.4": {
    "title": "Towards Effective Automatic Debt Collection with Persona Awareness",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tong Zhang",
      "Junhong Liu",
      "Chen Huang",
      "Jia Liu",
      "Hongru Liang",
      "Zujie Wen",
      "Wenqiang Lei"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.5": {
    "title": "Gatekeeper to save COGS and improve efficiency of Text Prediction",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nidhi Tiwari",
      "Sneha Kola",
      "Milos Milunovic",
      "Si-qing Chen",
      "Marjan Slavkovski"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.6": {
    "title": "Efficient Transformer Knowledge Distillation: A Performance Review",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nathan Brown",
      "Ashton Williamson",
      "Tahj Anderson",
      "Logan Lawrence"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.7": {
    "title": "CDD: A Large Scale Dataset for Legal Intelligence Research",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changzhen Ji",
      "Yating Zhang",
      "Adam Jatowt",
      "Haipang Wu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.8": {
    "title": "MUST&P-SRL: Multi-lingual and Unified Syllabification in Text and Phonetic Domains for Speech Representation Learning",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Noé Tits"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.9": {
    "title": "Personalized Dense Retrieval on Global Index for Voice-enabled Conversational Systems",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Masha Belyi",
      "Charlotte Dzialo",
      "Chaitanya Dwivedi",
      "Prajit Muppidi",
      "Kanna Shimizu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.10": {
    "title": "Text2Topic: Multi-Label Text Classification System for Efficient Topic Detection in User Generated Content with Zero-Shot Capabilities",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fengjun Wang",
      "Moran Beladev",
      "Ofri Kleinfeld",
      "Elina Frayerman",
      "Tal Shachar",
      "Eran Fainman",
      "Karen Lastmann Assaraf",
      "Sarai Mizrachi",
      "Benjamin Wang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.11": {
    "title": "Deep Metric Learning to Hierarchically Rank - An Application in Product Retrieval",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kee Kiat Koo",
      "Ashutosh Joshi",
      "Nishaanth Reddy",
      "Karim Bouyarmane",
      "Ismail Tutar",
      "Vaclav Petricek",
      "Changhe Yuan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.12": {
    "title": "A Pretrained Language Model for Cyber Threat Intelligence",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youngja Park",
      "Weiqiu You"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.13": {
    "title": "SAMP: A Model Inference Toolkit of Post-Training Quantization for Text Processing via Self-Adaptive Mixed-Precision",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rong Tian",
      "Zijing Zhao",
      "Weijie Liu",
      "Haoyan Liu",
      "Weiquan Mao",
      "Zhe Zhao",
      "Kan Zhou"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.14": {
    "title": "KD-Boost: Boosting Real-Time Semantic Matching in E-commerce with Knowledge Distillation",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanjay Agrawal",
      "Vivek Sembium",
      "Ankith M S"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.15": {
    "title": "Multi-teacher Distillation for Multilingual Spelling Correction",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingfen Zhang",
      "Xuan Guo",
      "Sravan Bodapati",
      "Christopher Potts"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.16": {
    "title": "Does Named Entity Recognition Truly Not Scale Up to Real-world Product Attribute Extraction?",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei-Te Chen",
      "Keiji Shinzato",
      "Naoki Yoshinaga",
      "Yandi Xia"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.17": {
    "title": "Investigating Table-to-Text Generation Capabilities of Large Language Models in Real-World Information Seeking Scenarios",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yilun Zhao",
      "Haowei Zhang",
      "Shengyun Si",
      "Linyong Nan",
      "Xiangru Tang",
      "Arman Cohan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.18": {
    "title": "TMID: A Comprehensive Real-world Dataset for Trademark Infringement Detection in E-Commerce",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tongxin Hu",
      "Zhuang Li",
      "Xin Jin",
      "Lizhen Qu",
      "Xin Zhang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.19": {
    "title": "Joint Dialogue Topic Segmentation and Categorization: A Case Study on Clinical Spoken Conversations",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengyuan Liu",
      "Siti Umairah Md Salleh",
      "Hong Choon Oh",
      "Pavitra Krishnaswamy",
      "Nancy Chen"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.20": {
    "title": "AdapterDistillation: Non-Destructive Task Composition with Knowledge Distillation",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junjie Wang",
      "Yicheng Chen",
      "Wangshu Zhang",
      "Sen Hu",
      "Teng Xu",
      "Jing Zheng"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.21": {
    "title": "PROMINET: Prototype-based Multi-View Network for Interpretable Email Response Prediction",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuqing Wang",
      "Prashanth Vijayaraghavan",
      "Ehsan Degan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.22": {
    "title": "Retrieval-Enhanced Dual Encoder Training for Product Matching",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Justin Chiu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.23": {
    "title": "WordArt Designer: User-Driven Artistic Typography Synthesis using Large Language Models",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun-Yan He",
      "Zhi-Qi Cheng",
      "Chenyang Li",
      "Jingdong Sun",
      "Wangmeng Xiang",
      "Xianhui Lin",
      "Xiaoyang Kang",
      "Zengke Jin",
      "Yusen Hu",
      "Bin Luo",
      "Yifeng Geng",
      "Xuansong Xie"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.24": {
    "title": "Lattice Path Edit Distance: A Romanization-aware Edit Distance for Extracting Misspelling-Correction Pairs from Japanese Search Query Logs",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nobuhiro Kaji"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.25": {
    "title": "Learning Multilingual Sentence Representations with Cross-lingual Consistency Regularization",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengzhi Gao",
      "Liwen Zhang",
      "Zhongjun He",
      "Hua Wu",
      "Haifeng Wang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.26": {
    "title": "Unveiling Identity Biases in Toxicity Detection : A Game-Focused Dataset and Reactivity Analysis Approach",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Josiane Van Dorpe",
      "Zachary Yang",
      "Nicolas Grenon-Godbout",
      "Grégoire Winterstein"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.27": {
    "title": "ORANGE: Text-video Retrieval via Watch-time-aware Heterogeneous Graph Contrastive Learning",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yucheng Lin",
      "Tim Chang",
      "Yaning Chang",
      "Jianqiang Ma",
      "Donghui Li",
      "Ting Peng",
      "Zang Li",
      "Zhiyi Zhou",
      "Feng Wang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.28": {
    "title": "Compute-Efficient Churn Reduction for Conversational Agents",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christopher Hidey",
      "Sarthak Sarthak"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.29": {
    "title": "Empower Large Language Model to Perform Better on Industrial Domain-Specific Question Answering",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangkai Yang",
      "Pu Zhao",
      "Zezhong Wang",
      "Lu Wang",
      "Bo Qiao",
      "Jue Zhang",
      "Mohit Garg",
      "Qingwei Lin",
      "Saravan Rajmohan",
      "Dongmei Zhang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.30": {
    "title": "Enhancing Extreme Multi-Label Text Classification: Addressing Challenges in Model, Data, and Evaluation",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dan Li",
      "Zi Long Zhu",
      "Janneke van de Loo",
      "Agnes Masip Gomez",
      "Vikrant Yadav",
      "Georgios Tsatsaronis",
      "Zubair Afzal"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.31": {
    "title": "Query-aware Multi-modal based Ranking Relevance in Video Search",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengcan Ye",
      "Ting Peng",
      "Tim Chang",
      "Zhiyi Zhou",
      "Feng Wang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.32": {
    "title": "Coordinated Replay Sample Selection for Continual Federated Learning",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jack Good",
      "Jimit Majmudar",
      "Christophe Dupuy",
      "Jixuan Wang",
      "Charith Peris",
      "Clement Chung",
      "Richard Zemel",
      "Rahul Gupta"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.33": {
    "title": "Building Real-World Meeting Summarization Systems using Large Language Models: A Practical Perspective",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Tahmid Rahman Laskar",
      "Xue-Yong Fu",
      "Cheng Chen",
      "Shashi Bhushan TN"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.34": {
    "title": "Creator Context for Tweet Recommendation",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Spurthi Amba Hombaiah",
      "Tao Chen",
      "Mingyang Zhang",
      "Michael Bendersky",
      "Marc Najork",
      "Matt Colen",
      "Sergey Levi",
      "Vladimir Ofitserov",
      "Tanvir Amin"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.35": {
    "title": "AdaBERT-CTC: Leveraging BERT-CTC for Text-Only Domain Adaptation in ASR",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tyler Vuong",
      "Karel Mundnich",
      "Dhanush Bekal",
      "Veera Elluru",
      "Srikanth Ronanki",
      "Sravan Bodapati"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.36": {
    "title": "Conversing with databases: Practical Natural Language Querying",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Denis Kochedykov",
      "Fenglin Yin",
      "Sreevidya Khatravath"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.37": {
    "title": "AART: AI-Assisted Red-Teaming with Diverse Data Generation for New LLM-powered Applications",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bhaktipriya Radharapu",
      "Kevin Robinson",
      "Lora Aroyo",
      "Preethi Lahoti"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.38": {
    "title": "Speakerly: A Voice-based Writing Assistant for Text Composition",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dhruv Kumar",
      "Vipul Raheja",
      "Alice Kaiser-Schatzlein",
      "Robyn Perry",
      "Apurva Joshi",
      "Justin Hugues-Nuger",
      "Samuel Lou",
      "Navid Chowdhury"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.39": {
    "title": "Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? A Study on Several Typical Tasks",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xianzhi Li",
      "Samuel Chan",
      "Xiaodan Zhu",
      "Yulong Pei",
      "Zhiqiang Ma",
      "Xiaomo Liu",
      "Sameena Shah"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.40": {
    "title": "CL-QR: Cross-Lingual Enhanced Query Reformulation for Multi-lingual Conversational AI Agents",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongkai Sun",
      "Zhengyang Zhao",
      "Sixing Lu",
      "Chengyuan Ma",
      "Xiaohu Liu",
      "Xing Fan",
      "Wei Shen",
      "Chenlei Guo"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.41": {
    "title": "Improving Contextual Query Rewrite for Conversational AI Agents through User-preference Feedback Learning",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongkai Sun",
      "Yingxue Zhou",
      "Jie Hao",
      "Xing Fan",
      "Yanbin Lu",
      "Chengyuan Ma",
      "Wei Shen",
      "Chenlei Guo"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.42": {
    "title": "Scaling Neural ITN for Numbers and Temporal Expressions in Tamil: Findings for an Agglutinative Low-resource Language",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bhavuk Singhal",
      "Sindhuja Gopalan",
      "Amrith Krishna",
      "Malolan Chetlur"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.43": {
    "title": "EELBERT: Tiny Models through Dynamic Embeddings",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gabrielle Cohn",
      "Rishika Agarwal",
      "Deepanshu Gupta",
      "Siddharth Patwardhan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.44": {
    "title": "Gold Standard Bangla OCR Dataset: An In-Depth Look at Data Preprocessing and Annotation Processes",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hasmot Ali",
      "AKM Shahariar Azad Rabby",
      "Md Majedul Islam",
      "A.k.m Mahamud",
      "Nazmul Hasan",
      "Fuad Rahman"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.45": {
    "title": "PILLOW: Enhancing Efficient Instruction Fine-tuning via Prompt Matching",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenting Qi",
      "Xiaoyu Tan",
      "Shaojie Shi",
      "Chao Qu",
      "Yinghui Xu",
      "Yuan Qi"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.46": {
    "title": "Welcome to the Real World: Efficient, Incremental and Scalable Key Point Analysis",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lilach Eden",
      "Yoav Kantor",
      "Matan Orbach",
      "Yoav Katz",
      "Noam Slonim",
      "Roy Bar-Haim"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.47": {
    "title": "Automatic Linking of Judgements to UK Supreme Court Hearings",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hadeel Saadany",
      "Constantin Orasan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.48": {
    "title": "Automatic Marketing Theme and Commodity Construction System for E-commerce",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiping Wang",
      "Peng Lin",
      "Hainan Zhang",
      "Hongshen Chen",
      "Tianhao Li",
      "Zhuoye Ding",
      "Sulong Xu",
      "Jinghe Hu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.49": {
    "title": "Towards Safer Operations: An Expert-involved Dataset of High-Pressure Gas Incidents for Preventing Future Failures",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shumpei Inoue",
      "Minh-Tien Nguyen",
      "Hiroki Mizokuchi",
      "Tuan-Anh Nguyen",
      "Huu-Hiep Nguyen",
      "Dung Le"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.50": {
    "title": "An Auxiliary Task Boosted Multi-task Learning Method for Service Account Retrieval with Limited Human Annotation",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanzhou Yao",
      "Zhao Zhang",
      "Kaijia Yang",
      "Huasheng Liang",
      "Qiang Yan",
      "Yongjun Xu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.51": {
    "title": "VKIE: The Application of Key Information Extraction on Video Text",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyu An",
      "Ye Liu",
      "Haoyuan Peng",
      "Di Yin"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.52": {
    "title": "Investigating the Role and Impact of Disfluency on Summarization",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Varun Nathan",
      "Ayush Kumar",
      "Jithendra Vepa"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.53": {
    "title": "InsightNet : Structured Insight Mining from Customer Feedback",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sandeep Sricharan Mukku",
      "Manan Soni",
      "Chetan Aggarwal",
      "Jitenkumar Rana",
      "Promod Yenigalla",
      "Rashmi Patange",
      "Shyam Mohan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.54": {
    "title": "E2E Spoken Entity Extraction for Virtual Agents",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karan Singla",
      "Yeon-Jun Kim",
      "Srinivas Bangalore"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.55": {
    "title": "Generative Models for Product Attribute Extraction",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ansel Blume",
      "Nasser Zalmout",
      "Heng Ji",
      "Xian Li"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.56": {
    "title": "CarExpert: Leveraging Large Language Models for In-Car Conversational Question Answering",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Rashad Al Hasan Rony",
      "Christian Suess",
      "Sinchana Ramakanth Bhat",
      "Viju Sudhi",
      "Julia Schneider",
      "Maximilian Vogel",
      "Roman Teucher",
      "Ken Friedl",
      "Soumya Sahoo"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.57": {
    "title": "BUSTER: a \"BUSiness Transaction Entity Recognition\" dataset",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrea Zugarini",
      "Andrew Zamai",
      "Marco Ernandes",
      "Leonardo Rigutini"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.58": {
    "title": "Multi-word Tokenization for Sequence Compression",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leonidas Gee",
      "Leonardo Rigutini",
      "Marco Ernandes",
      "Andrea Zugarini"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.59": {
    "title": "JarviX: A LLM No code Platform for Tabular Data Analysis and Optimization",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shang-Ching Liu",
      "ShengKun Wang",
      "Tsungyao Chang",
      "Wenqi Lin",
      "Chung-Wei Hsiung",
      "Yi-Chen Hsieh",
      "Yu-Ping Cheng",
      "Sian-Hong Luo",
      "Jianwei Zhang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.60": {
    "title": "Retrieve and Copy: Scaling ASR Personalization to Large Catalogs",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sai Muralidhar Jayanthi",
      "Devang Kulshreshtha",
      "Saket Dingliwal",
      "Srikanth Ronanki",
      "Sravan Bodapati"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.61": {
    "title": "STEER: Semantic Turn Extension-Expansion Recognition for Voice Assistants",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leon Zhang",
      "Jiarui Lu",
      "Joel Ruben Antony Moniz",
      "Aditya Kulkarni",
      "Dhivya Piraviperumal",
      "Tien Dung Tran",
      "Nick Tzou",
      "Hong Yu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.62": {
    "title": "Self-Criticism: Aligning Large Language Models with their Understanding of Helpfulness, Honesty, and Harmlessness",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyu Tan",
      "Shaojie Shi",
      "Xihe Qiu",
      "Chao Qu",
      "Zhenting Qi",
      "Yinghui Xu",
      "Yuan Qi"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.63": {
    "title": "InstructPTS: Instruction-Tuning LLMs for Product Title Summarization",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Besnik Fetahu",
      "Zhiyu Chen",
      "Oleg Rokhlenko",
      "Shervin Malmasi"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.64": {
    "title": "LLM4Vis: Explainable Visualization Recommendation using ChatGPT",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Wang",
      "Songheng Zhang",
      "Yun Wang",
      "Ee-Peng Lim",
      "Yong Wang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.65": {
    "title": "DUBLIN: Visual Document Understanding By Language-Image Network",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kriti Aggarwal",
      "Aditi Khandelwal",
      "Kumar Tanmay",
      "Owais Khan Mohammed",
      "Qiang Liu",
      "Monojit Choudhury",
      "Hardik Chauhan",
      "Subhojit Som",
      "Vishrav Chaudhary",
      "Saurabh Tiwary"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.66": {
    "title": "DocumentNet: Bridging the Data Gap in Document Pre-training",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lijun Yu",
      "Jin Miao",
      "Xiaoyu Sun",
      "Jiayi Chen",
      "Alexander Hauptmann",
      "Hanjun Dai",
      "Wei Wei"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.67": {
    "title": "Relevance-assisted Generation for Robust Zero-shot Retrieval",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jihyuk Kim",
      "Minsoo Kim",
      "Joonsuk Park",
      "Seung-won Hwang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.68": {
    "title": "Too much of product information : Don't worry, let's look for evidence!",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aryan Jain",
      "Jitenkumar Rana",
      "Chetan Aggarwal"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.69": {
    "title": "Harnessing LLMs for Temporal Data - A Study on Explainable Financial Time Series Forecasting",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinli Yu",
      "Zheng Chen",
      "Yanbin Lu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.70": {
    "title": "ViGPTQA - State-of-the-Art LLMs for Vietnamese Question Answering: System Overview, Core Models Training, and Evaluations",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minh Thuan Nguyen",
      "Khanh Tung Tran",
      "Nhu Van Nguyen",
      "Xuan-Son Vu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.71": {
    "title": "An Integrated Search System for Korea Weather Data",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinkyung Jo",
      "Dayeon Ki",
      "Soyoung Yoon",
      "Minjoon Seo"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.72": {
    "title": "Adaptive Hyper-parameter Learning for Deep Semantic Retrieval",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingming Li",
      "Chunyuan Yuan",
      "Huimu Wang",
      "Peng Wang",
      "Jingwei Zhuo",
      "Binbin Wang",
      "Lin Liu",
      "Sulong Xu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.73": {
    "title": "On Sample-Efficient Code Generation",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hojae Han",
      "Yu Jin Kim",
      "Byoungjip Kim",
      "Youngwon Lee",
      "Kyungjae Lee",
      "Kyungmin Lee",
      "Moontae Lee",
      "Kyunghoon Bae",
      "Seung-won Hwang"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.74": {
    "title": "Batch Prompting: Efficient Inference with Large Language Model APIs",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhoujun Cheng",
      "Jungo Kasai",
      "Tao Yu"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.75": {
    "title": "Graph Meets LLM: A Novel Approach to Collaborative Filtering for Robust Conversational Understanding",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheng Chen",
      "Ziyan Jiang",
      "Fan Yang",
      "Eunah Cho",
      "Xing Fan",
      "Xiaojiang Huang",
      "Yanbin Lu",
      "Aram Galstyan"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.76": {
    "title": "DELPHI: Data for Evaluating LLMs' Performance in Handling Controversial Issues",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Sun",
      "Artem Abzaliev",
      "Hadas Kotek",
      "Christopher Klein",
      "Zidi Xiu",
      "Jason Williams"
    ]
  },
  "https://aclanthology.org/2023.emnlp-industry.77": {
    "title": "Angel: Enterprise Search System for the Non-Profit Industry",
    "volume": "industry",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saiful Haq",
      "Ashutosh Sharma",
      "Pushpak Bhattacharyya"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1": {
    "title": "Multi Document Summarization Evaluation in the Presence of Damaging Content",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Avshalom Manevich",
      "David Carmel",
      "Nachshon Cohen",
      "Elad Kravi",
      "Ori Shapira"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.2": {
    "title": "Guiding AMR Parsing with Reverse Graph Linearization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bofei Gao",
      "Liang Chen",
      "Peiyi Wang",
      "Zhifang Sui",
      "Baobao Chang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.3": {
    "title": "Translate the Beauty in Songs: Jointly Learning to Align Melody and Translate Lyrics",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengxi Li",
      "Kai Fan",
      "Jiajun Bu",
      "Boxing Chen",
      "Zhongqiang Huang",
      "Zhi Yu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.4": {
    "title": "Aksharantar: Open Indic-language Transliteration datasets and models for the Next Billion Users",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yash Madhani",
      "Sushane Parthan",
      "Priyanka Bedekar",
      "Gokul Nc",
      "Ruchi Khapra",
      "Anoop Kunchukuttan",
      "Pratyush Kumar",
      "Mitesh Khapra"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.5": {
    "title": "Pretraining Without Attention",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junxiong Wang",
      "Jing Yan",
      "Albert Gu",
      "Alexander Rush"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.6": {
    "title": "Time-Aware Representation Learning for Time-Sensitive Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jungbin Son",
      "Alice Oh"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.7": {
    "title": "EffEval: A Comprehensive Evaluation of Efficiency for MT Evaluation Metrics",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniil Larionov",
      "Jens Grünwald",
      "Christoph Leiter",
      "Steffen Eger"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.8": {
    "title": "Unsupervised Opinion Summarization Using Approximate Geodesics",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Somnath Basu Roy Chowdhury",
      "Nicholas Monath",
      "Kumar Dubey",
      "Amr Ahmed",
      "Snigdha Chaturvedi"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.9": {
    "title": "Investigating the Frequency Distortion of Word Embeddings and Its Impact on Bias Metrics",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francisco Valentini",
      "Juan Sosa",
      "Diego Slezak",
      "Edgar Altszyler"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.10": {
    "title": "Improving Classifier Robustness through Active Generative Counterfactual Data Augmentation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ananth Balashankar",
      "Xuezhi Wang",
      "Yao Qin",
      "Ben Packer",
      "Nithum Thain",
      "Ed Chi",
      "Jilin Chen",
      "Alex Beutel"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.11": {
    "title": "Data Augmentation Techniques for Machine Translation of Code-Switched Texts: A Comparative Study",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Injy Hamed",
      "Nizar Habash",
      "Thang Vu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.12": {
    "title": "On the Relation between Sensitivity and Accuracy in In-Context Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanda Chen",
      "Chen Zhao",
      "Zhou Yu",
      "Kathleen McKeown",
      "He He"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.13": {
    "title": "Self-distilled Transitive Instance Weighting for Denoised Distantly Supervised Relation Extraction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangyu Lin",
      "Weijia Jia",
      "Zhiguo Gong"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.14": {
    "title": "MWE as WSD: Solving Multiword Expression Identification with Word Sense Disambiguation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joshua Tanner",
      "Jacob Hoffman"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.15": {
    "title": "Dual Contrastive Learning Framework for Incremental Text Classification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yigong Wang",
      "Zhuoyi Wang",
      "Yu Lin",
      "Jinghui Guo",
      "Sadaf Halim",
      "Latifur Khan"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.16": {
    "title": "Reference Free Domain Adaptation for Translation of Noisy Questions with Question Specific Rewards",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baban Gain",
      "Ramakrishna Appicharla",
      "Soumya Chennabasavaraj",
      "Nikesh Garera",
      "Asif Ekbal",
      "Muthusamy Chelliah"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.17": {
    "title": "Filtered Semi-Markov CRF",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Urchade Zaratiana",
      "Nadi Tomeh",
      "Niama El Khbir",
      "Pierre Holat",
      "Thierry Charnois"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.18": {
    "title": "Data Pruning for Efficient Model Pruning in Neural Machine Translation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abdul Azeemi",
      "Ihsan Qazi",
      "Agha Raza"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.19": {
    "title": "Long-Form Speech Translation through Segmentation with Finite-State Decoding Constraints on Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arya McCarthy",
      "Hao Zhang",
      "Shankar Kumar",
      "Felix Stahlberg",
      "Ke Wu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.20": {
    "title": "Re-Temp: Relation-Aware Temporal Representation Learning for Temporal Knowledge Graph Completion",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kunze Wang",
      "Caren Han",
      "Josiah Poon"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.21": {
    "title": "RethinkingTMSC: An Empirical Study for Target-Oriented Multimodal Sentiment Classification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junjie Ye",
      "Jie Zhou",
      "Junfeng Tian",
      "Rui Wang",
      "Qi Zhang",
      "Tao Gui",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.22": {
    "title": "Lexical Entrainment for Conversational Systems",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengxiang Shi",
      "Procheta Sen",
      "Aldo Lipani"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.23": {
    "title": "AutoReply: Detecting Nonsense in Dialogue with Discriminative Replies",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiyan Shi",
      "Emily Dinan",
      "Adi Renduchintala",
      "Daniel Fried",
      "Athul Jacob",
      "Zhou Yu",
      "Mike Lewis"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.24": {
    "title": "Follow-on Question Suggestion via Voice Hints for Voice Assistants",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Besnik Fetahu",
      "Pedro Faustini",
      "Anjie Fang",
      "Giuseppe Castellucci",
      "Oleg Rokhlenko",
      "Shervin Malmasi"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.25": {
    "title": "Bidirectional Masked Self-attention and N-gram Span Attention for Constituency Parsing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soohyeong Kim",
      "Whanhee Cho",
      "Minji Kim",
      "Yong Choi"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.26": {
    "title": "CR-COPEC: Causal Rationale of Corporate Performance Changes to learn from Financial Reports",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ye Chun",
      "Sunjae Kwon",
      "Kyunghwan Sohn",
      "Nakwon Sung",
      "Junyoup Lee",
      "Byoung Seo",
      "Kevin Compher",
      "Seung-won Hwang",
      "Jaesik Choi"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.27": {
    "title": "Plausibility Processing in Transformer Language Models: Focusing on the Role of Attention Heads in GPT",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soo Ryu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.28": {
    "title": "Automatic Unit Test Data Generation and Actor-Critic Reinforcement Learning for Code Synthesis",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Philip Gorinski",
      "Matthieu Zimmer",
      "Gerasimos Lampouras",
      "Derrick Goh Xin Deik",
      "Ignacio Iacobacci"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.29": {
    "title": "Unlocking the Heterogeneous Landscape of Big Data NLP with DUUI",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Leonhardt",
      "Giuseppe Abrami",
      "Daniel Baumartz",
      "Alexander Mehler"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.30": {
    "title": "Towards Agile Text Classifiers for Everyone",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maximilian Mozes",
      "Jessica Hoffmann",
      "Katrin Tomanek",
      "Muhamed Kouate",
      "Nithum Thain",
      "Ann Yuan",
      "Tolga Bolukbasi",
      "Lucas Dixon"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.31": {
    "title": "Beyond Good Intentions: Reporting the Research Landscape of NLP for Social Good",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fernando Adauto",
      "Zhijing Jin",
      "Bernhard Schölkopf",
      "Tom Hope",
      "Mrinmaya Sachan",
      "Rada Mihalcea"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.32": {
    "title": "PAXQA: Generating Cross-lingual Question Answering Examples at Training Scale",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bryan Li",
      "Chris Callison-Burch"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.33": {
    "title": "Sharing, Teaching and Aligning: Knowledgeable Transfer Learning for Cross-Lingual Machine Reading Comprehension",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tingfeng Cao",
      "Chengyu Wang",
      "Chuanqi Tan",
      "Jun Huang",
      "Jinhui Zhu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.34": {
    "title": "BERT Goes Off-Topic: Investigating the Domain Transfer Challenge using Genre Classification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dmitri Roussinov",
      "Serge Sharoff"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.35": {
    "title": "Toward Stronger Textual Attack Detectors",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pierre Colombo",
      "Marine Picot",
      "Nathan Noiry",
      "Guillaume Staerman",
      "Pablo Piantanida"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.36": {
    "title": "MEAL: Stable and Active Learning for Few-Shot Prompting",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abdullatif Köksal",
      "Timo Schick",
      "Hinrich Schuetze"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.37": {
    "title": "Structure and Label Constrained Data Augmentation for Cross-domain Few-shot NER",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyi Zhang",
      "Ying Zhang",
      "Yufeng Chen",
      "Jinan Xu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.38": {
    "title": "Weakly-supervised Deep Cognate Detection Framework for Low-Resourced Languages Using Morphological Knowledge of Closely-Related Languages",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Koustava Goswami",
      "Priya Rani",
      "Theodorus Fransen",
      "John McCrae"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.39": {
    "title": "SQLPrompt: In-Context Text-to-SQL with Minimal Labeled Data",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruoxi Sun",
      "Sercan Arik",
      "Rajarishi Sinha",
      "Hootan Nakhost",
      "Hanjun Dai",
      "Pengcheng Yin",
      "Tomas Pfister"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.40": {
    "title": "Toward Building General Foundation Models for Language, Vision, and Vision-Language Understanding Tasks",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinsong Zhang",
      "Yan Zeng",
      "Jipeng Zhang",
      "Hang Li"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.41": {
    "title": "Trigger Warnings: Bootstrapping a Violence Detector for Fan Fiction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Magdalena Wolska",
      "Matti Wiegmann",
      "Christopher Schröder",
      "Ole Borchardt",
      "Benno Stein",
      "Martin Potthast"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.42": {
    "title": "Pass-Tuning: Towards Structure-Aware Parameter-Efficient Tuning for Code Representation Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nuo Chen",
      "Qiushi Sun",
      "Jianing Wang",
      "Xiang Li",
      "Ming Gao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.43": {
    "title": "Counterfactual Augmentation for Multimodal Learning Under Presentation Bias",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Victoria Lin",
      "Louis-Philippe Morency",
      "Dimitrios Dimitriadis",
      "Srinagesh Sharma"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.44": {
    "title": "A Table-to-Text Framework with Heterogeneous Multidominance Attention and Self-Evaluated Multi-Pass Deliberation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xi Chen",
      "Xinjiang Lu",
      "Haoran Xin",
      "Wenjun Peng",
      "Haoyang Duan",
      "Feihu Jiang",
      "Jingbo Zhou",
      "Hui Xiong"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.45": {
    "title": "Crossing the Aisle: Unveiling Partisan and Counter-Partisan Events in News Reporting",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaijian Zou",
      "Xinliang Zhang",
      "Winston Wu",
      "Nicholas Beauchamp",
      "Lu Wang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.46": {
    "title": "Video-Text Retrieval by Supervised Sparse Multi-Grained Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yimu Wang",
      "Peng Shi"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.47": {
    "title": "Zero-Shot-BERT-Adapters: a Zero-Shot Pipeline for Unknown Intent Detection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniele Comi",
      "Dimitrios Christofidellis",
      "Pier Piazza",
      "Matteo Manica"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.48": {
    "title": "ReFSQL: A Retrieval-Augmentation Framework for Text-to-SQL Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kun Zhang",
      "Xiexiong Lin",
      "Yuanzhuo Wang",
      "Xin Zhang",
      "Fei Sun",
      "Cen Jianhe",
      "Hexiang Tan",
      "Xuhui Jiang",
      "Huawei Shen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.49": {
    "title": "Approximating Two-Layer Feedforward Networks for Efficient Transformers",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Róbert Csordás",
      "Kazuki Irie",
      "Jürgen Schmidhuber"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.50": {
    "title": "Adapter-TST: A Parameter Efficient Method for Multiple-Attribute Text Style Transfer",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiqiang Hu",
      "Nancy Chen",
      "Roy Lee"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.51": {
    "title": "Solving the Right Problem is Key for Translational NLP: A Case Study in UMLS Vocabulary Insertion",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bernal Gutierrez",
      "Yuqing Mao",
      "Vinh Nguyen",
      "Kin Fung",
      "Yu Su",
      "Olivier Bodenreider"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.52": {
    "title": "Improving Cross-lingual Transfer through Subtree-aware Word Reordering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ofir Arviv",
      "Dmitry Nikolaev",
      "Taelin Karidi",
      "Omri Abend"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.53": {
    "title": "Novel Slot Detection With an Incremental Setting",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Liang",
      "Hongliang Li",
      "Changhao Guan",
      "Qingbin Liu",
      "Jian Liu",
      "Jinan Xu",
      "Zhe Zhao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.54": {
    "title": "Self-supervised Post-processing Method to Enrich Pretrained Word Vectors",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hwiyeol Jo"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.55": {
    "title": "Automatic Model Selection with Large Language Models for Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "James Zhao",
      "Yuxi Xie",
      "Kenji Kawaguchi",
      "Junxian He",
      "Michael Xie"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.56": {
    "title": "ARKitSceneRefer: Text-based Localization of Small Objects in Diverse Real-World 3D Indoor Scenes",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shunya Kato",
      "Shuhei Kurita",
      "Chenhui Chu",
      "Sadao Kurohashi"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.57": {
    "title": "Improving Question Generation with Multi-level Content Planning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zehua Xia",
      "Qi Gou",
      "Bowen Yu",
      "Haiyang Yu",
      "Fei Huang",
      "Yongbin Li",
      "Nguyen Cam-Tu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.58": {
    "title": "Is ChatGPT a Financial Expert? Evaluating Language Models on Financial Natural Language Processing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Guo",
      "Zian Xu",
      "Yi Yang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.59": {
    "title": "DelucionQA: Detecting Hallucinations in Domain-specific Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mobashir Sadat",
      "Zhengyu Zhou",
      "Lukas Lange",
      "Jun Araki",
      "Arsalan Gundroo",
      "Bingqing Wang",
      "Rakesh Menon",
      "Md Parvez",
      "Zhe Feng"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.60": {
    "title": "InvGC: Robust Cross-Modal Retrieval by Inverse Graph Convolution",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangru Jian",
      "Yimu Wang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.61": {
    "title": "Dissecting In-Context Learning of Translations in GPT-3",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vikas Raunak",
      "Arul Menezes",
      "Hany Awadalla"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.62": {
    "title": "Social Commonsense-Guided Search Query Generation for Open-Domain Knowledge-Powered Conversations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Revanth Reddy",
      "Hao Bai",
      "Wentao Yao",
      "Sharath Chandra Etagi Suresh",
      "Heng Ji",
      "ChengXiang Zhai"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.63": {
    "title": "MixTEA: Semi-supervised Entity Alignment with Mixture Teaching",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feng Xie",
      "Xin Song",
      "Xiang Zeng",
      "Xuechen Zhao",
      "Lei Tian",
      "Bin Zhou",
      "Yusong Tan"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.64": {
    "title": "EZ-STANCE: A Large Dataset for Zero-Shot Stance Detection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenye Zhao",
      "Cornelia Caragea"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.65": {
    "title": "Boot and Switch: Alternating Distillation for Zero-Shot Dense Retrieval",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Jiang",
      "Qiongkai Xu",
      "Tom Drummond",
      "Trevor Cohn"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.66": {
    "title": "TESTA: Temporal-Spatial Token Aggregation for Long-form Video-Language Understanding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuhuai Ren",
      "Sishuo Chen",
      "Shicheng Li",
      "Xu Sun",
      "Lu Hou"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.67": {
    "title": "Fusing Temporal Graphs into Transformers for Time-Sensitive Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Su",
      "Phillip Howard",
      "Nagib Hakim",
      "Steven Bethard"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.68": {
    "title": "The Internal State of an LLM Knows When It's Lying",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amos Azaria",
      "Tom Mitchell"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.69": {
    "title": "Factual Relation Discrimination for Factuality-oriented Abstractive Summarization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiguang Gao",
      "Peifeng Li",
      "Feng Jiang",
      "Xiaomin Chu",
      "Qiaoming Zhu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.70": {
    "title": "Multi-Modal Knowledge Graph Transformer Framework for Multi-Modal Entity Alignment",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qian Li",
      "Cheng Ji",
      "Shu Guo",
      "Zhaoji Liang",
      "Lihong Wang",
      "Jianxin Li"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.71": {
    "title": "Is a Prestigious Job the same as a Prestigious Country? A Case Study on Multilingual Sentence Embeddings and European Countries",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jindřich Libovický"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.72": {
    "title": "Towards A Holistic Landscape of Situated Theory of Mind in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziqiao Ma",
      "Jacob Sansom",
      "Run Peng",
      "Joyce Chai"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.73": {
    "title": "Text Augmented Spatial Aware Zero-shot Referring Image Segmentation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yucheng Suo",
      "Linchao Zhu",
      "Yi Yang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.74": {
    "title": "IRFL: Image Recognition of Figurative Language",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ron Yosef",
      "Yonatan Bitton",
      "Dafna Shahaf"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.75": {
    "title": "Self-supervised Meta-Prompt Learning with Meta-Gradient Regularization for Few-shot Generalization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaihang Pan",
      "Juncheng Li",
      "Hongye Song",
      "Jun Lin",
      "Xiaozhong Liu",
      "Siliang Tang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.76": {
    "title": "An Adaptive Prompt Generation Framework for Task-oriented Dialogue System",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Gao",
      "Liuyu Xiang",
      "Huijia Wu",
      "Han Zhao",
      "Yiqi Tong",
      "Zhaofeng He"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.77": {
    "title": "Temporal Knowledge Graph Reasoning Based on N-tuple Modeling",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongni Hou",
      "Xiaolong Jin",
      "Zixuan Li",
      "Long Bai",
      "Saiping Guan",
      "Yutao Zeng",
      "Jiafeng Guo",
      "Xueqi Cheng"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.78": {
    "title": "Make Your Decision Convincing! A Unified Two-Stage Framework: Self-Attribution and Decision-Making",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanrui Du",
      "Sendong Zhao",
      "Haochun Wang",
      "Yuhan Chen",
      "Rui Bai",
      "Zewen Qiang",
      "Muzhen Cai",
      "Bing Qin"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.79": {
    "title": "Adaptive Structure Induction for Aspect-based Sentiment Analysis with Spectral Perspective",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Niu",
      "Yun Xiong",
      "Xiaosu Wang",
      "Wenjing Yu",
      "Yao Zhang",
      "Zhonglei Guo"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.80": {
    "title": "NovaCOMET: Open Commonsense Foundation Models with Symbolic Knowledge Distillation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peter West",
      "Ronan Bras",
      "Taylor Sorensen",
      "Bill Lin",
      "Liwei Jiang",
      "Ximing Lu",
      "Khyathi Chandu",
      "Jack Hessel",
      "Ashutosh Baheti",
      "Chandra Bhagavatula",
      "Yejin Choi"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.81": {
    "title": "In-Context Demonstration Selection with Cross Entropy Difference",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dan Iter",
      "Reid Pryzant",
      "Ruochen Xu",
      "Shuohang Wang",
      "Yang Liu",
      "Yichong Xu",
      "Chenguang Zhu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.82": {
    "title": "The Past, Present, and Future of Typological Databases in NLP",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emi Baylor",
      "Esther Ploeger",
      "Johannes Bjerva"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.83": {
    "title": "SoulChat: Improving LLMs' Empathy, Listening, and Comfort Abilities through Fine-tuning with Multi-turn Empathy Conversations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yirong Chen",
      "Xiaofen Xing",
      "Jingkai Lin",
      "Huimin Zheng",
      "Zhenyu Wang",
      "Qi Liu",
      "Xiangmin Xu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.84": {
    "title": "Can ChatGPT Assess Human Personalities? A General Evaluation Framework",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haocong Rao",
      "Cyril Leung",
      "Chunyan Miao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.85": {
    "title": "MoqaGPT : Zero-Shot Multi-modal Open-domain Question Answering with Large Language Model",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Le Zhang",
      "Yihong Wu",
      "Fengran Mo",
      "Jian-Yun Nie",
      "Aishwarya Agrawal"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.86": {
    "title": "Large Language Models Know Your Contextual Search Intent: A Prompting Framework for Conversational Search",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kelong Mao",
      "Zhicheng Dou",
      "Fengran Mo",
      "Jiewen Hou",
      "Haonan Chen",
      "Hongjin Qian"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.87": {
    "title": "DocAsRef: An Empirical Study on Repurposing Reference-based Summary Quality Metrics as Reference-free Metrics",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Forrest Bao",
      "Ruixuan Tu",
      "Ge Luo",
      "Yinfei Yang",
      "Hebi Li",
      "Minghui Qiu",
      "Youbiao He",
      "Cen Chen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.88": {
    "title": "Toxicity in chatgpt: Analyzing persona-assigned language models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ameet Deshpande",
      "Vishvak Murahari",
      "Tanmay Rajpurohit",
      "Ashwin Kalyan",
      "Karthik Narasimhan"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.89": {
    "title": "Execution-Based Evaluation for Open-Domain Code Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiruo Wang",
      "Shuyan Zhou",
      "Daniel Fried",
      "Graham Neubig"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.90": {
    "title": "Syntax-Aware Retrieval Augmented Code Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangyu Zhang",
      "Yu Zhou",
      "Guang Yang",
      "Taolue Chen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.91": {
    "title": "Selecting Key Views for Zero-Shot Entity Linking",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuhui Sui",
      "Ying Zhang",
      "Kehui Song",
      "Baohang Zhou",
      "Xiaojie Yuan",
      "Wensheng Zhang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.92": {
    "title": "Is Explanation the Cure? Misinformation Mitigation in the Short Term and Long Term",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi-Li Hsu",
      "Shih-Chieh Dai",
      "Aiping Xiong",
      "Lun-Wei Ku"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.93": {
    "title": "Improving the Robustness of Summarization Models by Detecting and Removing Input Noise",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kundan Krishna",
      "Yao Zhao",
      "Jie Ren",
      "Balaji Lakshminarayanan",
      "Jiaming Luo",
      "Mohammad Saleh",
      "Peter Liu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.94": {
    "title": "How Reliable Are AI-Generated-Text Detectors? An Assessment Framework Using Evasive Soft Prompts",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tharindu Kumarage",
      "Paras Sheth",
      "Raha Moraffah",
      "Joshua Garland",
      "Huan Liu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.95": {
    "title": "Knowledge is a Region in Weight Space for Fine-tuned Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Almog Gueta",
      "Elad Venezian",
      "Colin Raffel",
      "Noam Slonim",
      "Yoav Katz",
      "Leshem Choshen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.96": {
    "title": "Unveiling the Multi-Annotation Process: Examining the Influence of Annotation Quantity and Instance Difficulty on Model Performance",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pritam Kadasi",
      "Mayank Singh"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.97": {
    "title": "On the Risk of Misinformation Pollution with Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yikang Pan",
      "Liangming Pan",
      "Wenhu Chen",
      "Preslav Nakov",
      "Min-Yen Kan",
      "William Wang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.98": {
    "title": "Dolphin: A Challenging and Diverse Benchmark for Arabic NLG",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "El Moatez Billah Nagoudi",
      "AbdelRahim Elmadany",
      "Ahmed El-Shangiti",
      "Muhammad Abdul-Mageed"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.99": {
    "title": "Hierarchical Enhancement Framework for Aspect-based Argument Mining",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujie Fu",
      "Yang Li",
      "Suge Wang",
      "Xiaoli Li",
      "Deyu Li",
      "Jian Liao",
      "JianXing Zheng"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.100": {
    "title": "MenatQA: A New Dataset for Testing the Temporal Comprehension and Reasoning Abilities of Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Wei",
      "Yisong Su",
      "Huanhuan Ma",
      "Xiaoyan Yu",
      "Fangyu Lei",
      "Yuanzhe Zhang",
      "Jun Zhao",
      "Kang Liu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.101": {
    "title": "What Makes Chain-of-Thought Prompting Effective? A Counterfactual Study",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aman Madaan",
      "Katherine Hermann",
      "Amir Yazdanbakhsh"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.102": {
    "title": "Perceptual Structure in the absence of grounding: the impact of abstractedness and subjectivity in color language for LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pablo Loyola",
      "Edison Marrese-Taylor",
      "Andres Hoyos-Idrobo"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.103": {
    "title": "A Dataset for Investigating the Impact of Context for Offensive Language Detection in Tweets",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Musa İhtiyar",
      "Ömer Özdemir",
      "Mustafa Erengül",
      "Arzucan Özgür"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.104": {
    "title": "Remember what you did so you know what to do next",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manuel Ciosici",
      "Alex Hedges",
      "Yash Kankanampati",
      "Justin Martin",
      "Marjorie Freedman",
      "Ralph Weischedel"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.105": {
    "title": "An Empirical Study of Multimodal Model Merging",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi-Lin Sung",
      "Linjie Li",
      "Kevin Lin",
      "Zhe Gan",
      "Mohit Bansal",
      "Lijuan Wang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.106": {
    "title": "Learning to Abstract with Nonparametric Variational Information Bottleneck",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Melika Behjati",
      "Fabio Fehr",
      "James Henderson"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.107": {
    "title": "Global Structure Knowledge-Guided Relation Extraction Method for Visually-Rich Document",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangnan Chen",
      "Qian Xiao",
      "Juncheng Li",
      "Duo Dong",
      "Jun Lin",
      "Xiaozhong Liu",
      "Siliang Tang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.108": {
    "title": "Learning to Compose Representations of Different Encoder Layers towards Improving Compositional Generalization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Lin",
      "Shuangtao Li",
      "Yafang Zheng",
      "Biao Fu",
      "Shan Liu",
      "Yidong Chen",
      "Xiaodong Shi"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.109": {
    "title": "SelectNoise: Unsupervised Noise Injection to Enable Zero-Shot Machine Translation for Extremely Low-resource Languages",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maharaj Brahma",
      "Kaushal Maurya",
      "Maunendra Desarkar"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.110": {
    "title": "Breaking Boundaries in Retrieval Systems: Unsupervised Domain Adaptation with Denoise-Finetuning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Che Chen",
      "Ching Yang",
      "Chun-Yi Lin",
      "Hung-Yu Kao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.111": {
    "title": "Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheyuan Zhang",
      "Jifan Yu",
      "Juanzi Li",
      "Lei Hou"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.112": {
    "title": "Simpler neural networks prefer subregular languages",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Charles Torres",
      "Richard Futrell"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.113": {
    "title": "Simple Hardware-Efficient PCFGs with Independent Left and Right Productions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Liu",
      "Songlin Yang",
      "Yoon Kim",
      "Kewei Tu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.114": {
    "title": "R3 Prompting: Review, Rephrase and Resolve for Chain-of-Thought Reasoning in Large Language Models under Noisy Context",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingyuan Tian",
      "Hanlun Zhu",
      "Lei Wang",
      "Yang Li",
      "Yunshi Lan"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.115": {
    "title": "Quality Estimation-Assisted Automatic Post-Editing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sourabh Deoghare",
      "Diptesh Kanojia",
      "Fred Blain",
      "Tharindu Ranasinghe",
      "Pushpak Bhattacharyya"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.116": {
    "title": "Adapter Pruning using Tropical Characterization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rishabh Bhardwaj",
      "Tushar Vaidya",
      "Soujanya Poria"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.117": {
    "title": "Self-Supervised Rule Learning to Link Text Segments to Relational Elements of Structured Knowledge",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shajith Ikbal",
      "Udit Sharma",
      "Hima Karanam",
      "Sumit Neelam",
      "Ronny Luss",
      "Dheeraj Sreedhar",
      "Pavan Kapanipathi",
      "Naweed Khan",
      "Kyle Erwin",
      "Ndivhuwo Makondo",
      "Ibrahim Abdelaziz",
      "Achille Fokoue",
      "Alexander Gray",
      "Maxwell Crouse",
      "Subhajit Chaudhury",
      "Chitra Subramanian"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.118": {
    "title": "TaTA: A Multilingual Table-to-Text Dataset for African Languages",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Gehrmann",
      "Sebastian Ruder",
      "Vitaly Nikolaev",
      "Jan Botha",
      "Michael Chavinda",
      "Ankur Parikh",
      "Clara Rivera"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.119": {
    "title": "Explain-then-translate: an analysis on improving program translation with self-generated explanations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zilu Tang",
      "Mayank Agarwal",
      "Alexander Shypula",
      "Bailin Wang",
      "Derry Wijaya",
      "Jie Chen",
      "Yoon Kim"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.120": {
    "title": "Can Brain Signals Reveal Inner Alignment with Human Languages?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jielin Qiu",
      "William Han",
      "Jiacheng Zhu",
      "Mengdi Xu",
      "Douglas Weber",
      "Bo Li",
      "Ding Zhao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.121": {
    "title": "DemoSG: Demonstration-enhanced Schema-guided Generation for Low-resource Event Extraction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gang Zhao",
      "Xiaocheng Gong",
      "Xinjie Yang",
      "Guanting Dong",
      "Shudong Lu",
      "Si Li"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.122": {
    "title": "GLGR: Question-aware Global-to-Local Graph Reasoning for Multi-party Dialogue Reading Comprehension",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanling Li",
      "Bowei Zou",
      "Yifan Fan",
      "Xibo Li",
      "Ai Ti Aw",
      "Yu Hong"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.123": {
    "title": "Towards Mitigating LLM Hallucination via Self Reflection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziwei Ji",
      "Tiezheng Yu",
      "Yan Xu",
      "Nayeon Lee",
      "Etsuko Ishii",
      "Pascale Fung"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.124": {
    "title": "Making Body Movement in Sign Language Corpus Accessible for Linguists and Machines with Three-Dimensional Normalization of MediaPipe",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Victor Skobov",
      "Mayumi Bono"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.125": {
    "title": "XTREME-UP: A User-Centric Scarce-Data Benchmark for Under-Represented Languages",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Ruder",
      "Jonathan Clark",
      "Alexander Gutkin",
      "Mihir Kale",
      "Min Ma",
      "Massimo Nicosia",
      "Shruti Rijhwani",
      "Parker Riley",
      "Jean-Michel Sarr",
      "Xinyi Wang",
      "John Wieting",
      "Nitish Gupta",
      "Anna Katanova",
      "Christo Kirov",
      "Dana Dickinson",
      "Brian Roark",
      "Bidisha Samanta",
      "Connie Tao",
      "David Adelani",
      "Vera Axelrod",
      "Isaac Caswell",
      "Colin Cherry",
      "Dan Garrette",
      "Reeve Ingle",
      "Melvin Johnson",
      "Dmitry Panteleev",
      "Partha Talukdar"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.126": {
    "title": "DiffuVST: Narrating Fictional Scenes with Global-History-Guided Denoising Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengguang Wu",
      "Mei Yuan",
      "Qi Su"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.127": {
    "title": "DiFair: A Benchmark for Disentangled Assessment of Gender Knowledge and Bias",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mahdi Zakizadeh",
      "Kaveh Miandoab",
      "Mohammad Pilehvar"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.128": {
    "title": "Transformer-Based Language Model Surprisal Predicts Human Reading Times Best with About Two Billion Training Tokens",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Byung-Doh Oh",
      "William Schuler"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.129": {
    "title": "ExplainCPE: A Free-text Explanation Benchmark of Chinese Pharmacist Examination",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongfang Li",
      "Jindi Yu",
      "Baotian Hu",
      "Zhenran Xu",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.130": {
    "title": "CLASS: A Design Framework for Building Intelligent Tutoring Systems Based on Learning Science principles",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shashank Sonkar",
      "Naiming Liu",
      "Debshila Mallick",
      "Richard Baraniuk"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.131": {
    "title": "Normal-Abnormal Decoupling Memory for Medical Report Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guosheng Zhao",
      "Yan Yan",
      "Zijian Zhao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.132": {
    "title": "mmT5: Modular Multilingual Pre-Training Solves Source Language Hallucinations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonas Pfeiffer",
      "Francesco Piccinno",
      "Massimo Nicosia",
      "Xinyi Wang",
      "Machel Reid",
      "Sebastian Ruder"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.133": {
    "title": "ImageNetVC: Zero- and Few-Shot Visual Commonsense Evaluation on 1000 ImageNet Categories",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heming Xia",
      "Qingxiu Dong",
      "Lei Li",
      "Jingjing Xu",
      "Tianyu Liu",
      "Ziwei Qin",
      "Zhifang Sui"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.134": {
    "title": "MultiCoNER v2: a Large Multilingual dataset for Fine-grained and Noisy Named Entity Recognition",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Besnik Fetahu",
      "Zhiyu Chen",
      "Sudipta Kar",
      "Oleg Rokhlenko",
      "Shervin Malmasi"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.135": {
    "title": "A Query-Parallel Machine Reading Comprehension Framework for Low-resource NER",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhao Zhang",
      "Yongliang Wang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.136": {
    "title": "BiSPN: Generating Entity Set and Relation Set Coherently in One Pass",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxin He",
      "Buzhou Tang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.137": {
    "title": "MEEP: Is this Engaging? Prompting Large Language Models for Dialogue Evaluation in Multilingual Settings",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amila Ferron",
      "Amber Shore",
      "Ekata Mitra",
      "Ameeta Agrawal"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.138": {
    "title": "Exploring the Impact of Corpus Diversity on Financial Pretrained Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaeyoung Choe",
      "Keonwoong Noh",
      "Nayeon Kim",
      "Seyun Ahn",
      "Woohwan Jung"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.139": {
    "title": "LLMDet: A Third Party Large Language Models Generated Text Detection Tool",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kangxi Wu",
      "Liang Pang",
      "Huawei Shen",
      "Xueqi Cheng",
      "Tat-Seng Chua"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.140": {
    "title": "RECAP: Towards Precise Radiology Report Generation via Dynamic Disease Progression Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenjun Hou",
      "Yi Cheng",
      "Kaishuai Xu",
      "Wenjie Li",
      "Jiang Liu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.141": {
    "title": "Causal Intervention for Abstractive Related Work Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiachang Liu",
      "Qi Zhang",
      "Chongyang Shi",
      "Usman Naseem",
      "Shoujin Wang",
      "Liang Hu",
      "Ivor Tsang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.142": {
    "title": "G-SPEED: General SParse Efficient Editing MoDel",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoke Zhang",
      "Yue Wang",
      "Juntao Li",
      "Xiabing Zhou",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.143": {
    "title": "Attack Prompt Generation for Red Teaming and Defending Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boyi Deng",
      "Wenjie Wang",
      "Fuli Feng",
      "Yang Deng",
      "Qifan Wang",
      "Xiangnan He"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.144": {
    "title": "Smart \"Chef\": Verifying the Effect of Role-based Paraphrasing for Aspect Term Extraction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxiang Chen",
      "Yu Hong",
      "Qingting Xu",
      "Jianmin Yao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.145": {
    "title": "Multi-Defendant Legal Judgment Prediction via Hierarchical Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yougang Lyu",
      "Jitai Hao",
      "Zihan Wang",
      "Kai Zhao",
      "Shen Gao",
      "Pengjie Ren",
      "Zhumin Chen",
      "Fang Wang",
      "Zhaochun Ren"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.146": {
    "title": "Interpreting Indirect Answers to Yes-No Questions in Multiple Languages",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijie Wang",
      "Md Hossain",
      "Shivam Mathur",
      "Terry Melo",
      "Kadir Ozler",
      "Keun Park",
      "Jacob Quintero",
      "MohammadHossein Rezaei",
      "Shreya Shakya",
      "Md Uddin",
      "Eduardo Blanco"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.147": {
    "title": "Generalizing Few-Shot Named Entity Recognizers to Unseen Domains with Type-Related Features",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihan Wang",
      "Ziqi Zhao",
      "Zhumin Chen",
      "Pengjie Ren",
      "Maarten de Rijke",
      "Zhaochun Ren"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.148": {
    "title": "Intervention-Based Alignment of Code Search with Execution Feedback",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hojae Han",
      "Minsoo Kim",
      "Seung-won Hwang",
      "Nan Duan",
      "Shuai Lu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.149": {
    "title": "Enhancing Neural Machine Translation with Semantic Units",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Langlin Huang",
      "Shuhao Gu",
      "Zhang Zhuocheng",
      "Yang Feng"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.150": {
    "title": "DRAFT: Dense Retrieval Augmented Few-shot Topic classifier Framework",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keonwoo Kim",
      "Younggun Lee"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.151": {
    "title": "A Framework for Exploring Player Perceptions of LLM-Generated Dialogue in Commercial Video Games",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nader Akoury",
      "Qian Yang",
      "Mohit Iyyer"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.152": {
    "title": "Generative Calibration for In-context Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongtao Jiang",
      "Yuanzhe Zhang",
      "Cao Liu",
      "Jun Zhao",
      "Kang Liu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.153": {
    "title": "Chain of Thought with Explicit Evidence Reasoning for Few-shot Relation Extraction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xilai Ma",
      "Jing Li",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.154": {
    "title": "AdaTranS: Adapting with Boundary-based Shrinking for End-to-End Speech Translation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingshan Zeng",
      "Liangyou Li",
      "Qun Liu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.155": {
    "title": "No offence, Bert - I insult only humans! Multilingual sentence-level attack on toxicity detection networks",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sergey Berezin",
      "Reza Farahbakhsh",
      "Noel Crespi"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.156": {
    "title": "Manipulating the Perceived Personality Traits of Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Graham Caron",
      "Shashank Srivastava"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.157": {
    "title": "WikiChat: Stopping the Hallucination of Large Language Model Chatbots by Few-Shot Grounding on Wikipedia",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sina Semnani",
      "Violet Yao",
      "Heidi Zhang",
      "Monica Lam"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.158": {
    "title": "Automated Few-Shot Classification with Instruction-Finetuned Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rami Aly",
      "Xingjian Shi",
      "Kaixiang Lin",
      "Aston Zhang",
      "Andrew Wilson"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.159": {
    "title": "Meta-Learning of Prompt Generation for Lightweight Prompt Engineering on Language-Model-as-a-Service",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyeonmin Ha",
      "Jihye Lee",
      "Wookje Han",
      "Byung-Gon Chun"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.160": {
    "title": "Beneath Surface Similarity: Large Language Models Make Reasonable Scientific Analogies after Structure Abduction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyu Yuan",
      "Jiangjie Chen",
      "Xuyang Ge",
      "Yanghua Xiao",
      "Deqing Yang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.161": {
    "title": "HiCL: Hierarchical Contrastive Learning of Unsupervised Sentence Embeddings",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuofeng Wu",
      "Chaowei Xiao",
      "VG Vinod Vydiswaran"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.162": {
    "title": "Density-Aware Prototypical Network for Few-Shot Relation Classification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianfeng Wu",
      "Mengting Hu",
      "Yike Wu",
      "Bingzhe Wu",
      "Yalan Xie",
      "Mingming Liu",
      "Renhong Cheng"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.163": {
    "title": "Improved Training of Deep Text Clustering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zonghao Yang",
      "Wenpeng Hu",
      "Yushan Tan",
      "Zhunchen Luo"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.164": {
    "title": "RegaVAE: A Retrieval-Augmented Gaussian Mixture Variational Auto-Encoder for Language Modeling",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingcheng Deng",
      "Liang Pang",
      "Huawei Shen",
      "Xueqi Cheng"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.165": {
    "title": "RefGPT: Dialogue Generation of GPT, by GPT, and for GPT",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongjie Yang",
      "Ruifeng Yuan",
      "Yuantao Fan",
      "Yifei Yang",
      "Zili Wang",
      "Shusen Wang",
      "Hai Zhao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.166": {
    "title": "INA: An Integrative Approach for Enhancing Negotiation Strategies with Reward-Based Dialogue Agent",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zishan Ahmad",
      "Suman Saurabh",
      "Vaishakh Menon",
      "Asif Ekbal",
      "Roshni Ramnani",
      "Anutosh Maitra"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.167": {
    "title": "Large Language Models are Better Reasoners with Self-Verification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixuan Weng",
      "Minjun Zhu",
      "Fei Xia",
      "Bin Li",
      "Shizhu He",
      "Shengping Liu",
      "Bin Sun",
      "Kang Liu",
      "Jun Zhao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.168": {
    "title": "Multi-Granularity Information Interaction Framework for Incomplete Utterance Rewriting",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haowei Du",
      "Dinghao Zhang",
      "Chen Li",
      "Yang Li",
      "Dongyan Zhao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.169": {
    "title": "Accuracy is not enough: Evaluating Personalization in Summarizers",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rahul Vansh",
      "Darsh Rank",
      "Sourish Dasgupta",
      "Tanmoy Chakraborty"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.170": {
    "title": "For Generated Text, Is NLI-Neutral Text the Best Text?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michail Mersinias",
      "Kyle Mahowald"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.171": {
    "title": "Combining Counting Processes and Classification Improves a Stopping Rule for Technology Assisted Review",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Reem Hezam",
      "Mark Stevenson"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.172": {
    "title": "Complexity-Guided Curriculum Learning for Text Graphs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nidhi Vakil",
      "Hadi Amiri"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.173": {
    "title": "CoVariance-based Causal Debiasing for Entity and Relation Extraction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lin Ren",
      "Yongbin Liu",
      "Yixin Cao",
      "Chunping Ouyang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.174": {
    "title": "Multi-label and Multi-target Sampling of Machine Annotation for Computational Stance Detection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengyuan Liu",
      "Hai Chieu",
      "Nancy Chen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.175": {
    "title": "In What Languages are Generative Language Models the Most Formal? Analyzing Formality Distribution across Languages",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Asım Ersoy",
      "Gerson Vizcarra",
      "Tahsin Mayeesha",
      "Benjamin Muller"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.176": {
    "title": "MaXM: Towards Multilingual Visual Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soravit Changpinyo",
      "Linting Xue",
      "Michal Yarom",
      "Ashish Thapliyal",
      "Idan Szpektor",
      "Julien Amelot",
      "Xi Chen",
      "Radu Soricut"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.177": {
    "title": "Efficient Latent Variable Modeling for Knowledge-Grounded Dialogue Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gunsoo Han",
      "Daejin Jo",
      "Daniel Nam",
      "Eunseop Yoon",
      "Taehwan Kwon",
      "Seungeun Rho",
      "Kyoung-Woon On",
      "Chang Yoo",
      "Sungwoong Kim"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.178": {
    "title": "Ask To The Point: Open-Domain Entity-Centric Question Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxiang Liu",
      "Jie Huang",
      "Kevin Chang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.179": {
    "title": "Self-prompted Chain-of-Thought on Large Language Models for Open-domain Multi-hop Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinyuan Wang",
      "Junlong Li",
      "Hai Zhao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.180": {
    "title": "CASE: Commonsense-Augmented Score with an Expanded Answer Space",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenkai Chen",
      "Sahithya Ravi",
      "Vered Shwartz"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.181": {
    "title": "GRENADE: Graph-Centric Language Model for Self-Supervised Representation Learning on Text-Attributed Graphs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yichuan Li",
      "Kaize Ding",
      "Kyumin Lee"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.182": {
    "title": "Sources of Hallucination by Large Language Models on Inference Tasks",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nick McKenna",
      "Tianyi Li",
      "Liang Cheng",
      "Mohammad Hosseini",
      "Mark Johnson",
      "Mark Steedman"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.183": {
    "title": "Efficient Long-Range Transformers: You Need to Attend More, but Not Necessarily at Every Layer",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingru Zhang",
      "Dhananjay Ram",
      "Cole Hawkins",
      "Sheng Zha",
      "Tuo Zhao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.184": {
    "title": "Prompting ChatGPT in MNER: Enhanced Multimodal Named Entity Recognition with Auxiliary Refined Knowledge",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinyuan Li",
      "Han Li",
      "Zhuo Pan",
      "Di Sun",
      "Jiahao Wang",
      "Wenkun Zhang",
      "Gang Pan"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.185": {
    "title": "Understanding HTML with Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Izzeddin Gur",
      "Ofir Nachum",
      "Yingjie Miao",
      "Mustafa Safdari",
      "Austin Huang",
      "Aakanksha Chowdhery",
      "Sharan Narang",
      "Noah Fiedel",
      "Aleksandra Faust"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.186": {
    "title": "The PEACE-Reviews dataset: Modeling Cognitive Appraisals in Emotion Text Analysis",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gerard Yeo",
      "Kokil Jaidka"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.187": {
    "title": "UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiabo Ye",
      "Anwen Hu",
      "Haiyang Xu",
      "Qinghao Ye",
      "Ming Yan",
      "Guohai Xu",
      "Chenliang Li",
      "Junfeng Tian",
      "Qi Qian",
      "Ji Zhang",
      "Qin Jin",
      "Liang He",
      "Xin Lin",
      "Fei Huang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.188": {
    "title": "Loose lips sink ships: Mitigating Length Bias in Reinforcement Learning from Human Feedback",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Shen",
      "Rui Zheng",
      "Wenyu Zhan",
      "Jun Zhao",
      "Shihan Dou",
      "Tao Gui",
      "Qi Zhang",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.189": {
    "title": "Filling the Image Information Gap for VQA: Prompting Large Language Models to Proactively Ask Questions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyue Wang",
      "Chi Chen",
      "Peng Li",
      "Yang Liu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.190": {
    "title": "Take a Closer Look at Multilinguality! Improve Multilingual Pre-Training Using Monolingual Corpora Only",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinliang Lu",
      "Yu Lu",
      "Jiajun Zhang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.191": {
    "title": "LogiCoT: Logical Chain-of-Thought Instruction Tuning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanmeng Liu",
      "Zhiyang Teng",
      "Leyang Cui",
      "Chaoli Zhang",
      "Qiji Zhou",
      "Yue Zhang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.192": {
    "title": "Hiding in Plain Sight: Tweets with Hate Speech Masked by Homoglyphs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Portia Cooper",
      "Mihai Surdeanu",
      "Eduardo Blanco"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.193": {
    "title": "Reducing Spurious Correlations in Aspect-based Sentiment Analysis with Explanation from Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qianlong Wang",
      "Keyang Ding",
      "Bin Liang",
      "Min Yang",
      "Ruifeng Xu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.194": {
    "title": "High-quality argumentative information in low resources approaches improve counter-narrative generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Damián Furman",
      "Pablo Torres",
      "José Rodríguez",
      "Diego Letzen",
      "Maria Martinez",
      "Laura Alemany"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.195": {
    "title": "A Reference-free Segmentation Quality Index (SegReFree)",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Evan Lucas",
      "Dylan Kangas",
      "Timothy Havens"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.196": {
    "title": "In-context Learning for Few-shot Multimodal Named Entity Recognition",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenran Cai",
      "Qianlong Wang",
      "Bin Liang",
      "Bing Qin",
      "Min Yang",
      "Kam-Fai Wong",
      "Ruifeng Xu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.197": {
    "title": "On Uncertainty Calibration and Selective Generation in Probabilistic Neural Summarization: A Benchmark Study",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Polina Zablotskaia",
      "Du Phan",
      "Joshua Maynez",
      "Shashi Narayan",
      "Jie Ren",
      "Jeremiah Liu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.198": {
    "title": "Handshape-Aware Sign Language Recognition: Extended Datasets and Exploration of Handshape-Inclusive Methods",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan Zhang",
      "Kevin Duh"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.199": {
    "title": "SimCKP: Simple Contrastive Learning of Keyphrase Representations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minseok Choi",
      "Chaeheon Gwak",
      "Seho Kim",
      "Si Kim",
      "Jaegul Choo"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.200": {
    "title": "LEXTREME: A Multi-Lingual and Multi-Task Benchmark for the Legal Domain",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joel Niklaus",
      "Veton Matoshi",
      "Pooja Rani",
      "Andrea Galassi",
      "Matthias Stürmer",
      "Ilias Chalkidis"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.201": {
    "title": "Three Questions Concerning the Use of Large Language Models to Facilitate Mathematics Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "An-Zi Yen",
      "Wei-Ling Hsu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.202": {
    "title": "Simultaneous Machine Translation with Tailored Reference",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shoutao Guo",
      "Shaolei Zhang",
      "Yang Feng"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.203": {
    "title": "Dynamic Voting for Efficient Reasoning in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingfeng Xue",
      "Dayiheng Liu",
      "Wenqiang Lei",
      "Xingzhang Ren",
      "Baosong Yang",
      "Jun Xie",
      "Yidan Zhang",
      "Dezhong Peng",
      "Jiancheng Lv"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.204": {
    "title": "On Surgical Fine-tuning for Language Encoders",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhilasha Lodha",
      "Gayatri Belapurkar",
      "Saloni Chalkapurkar",
      "Yuanming Tao",
      "Reshmi Ghosh",
      "Samyadeep Basu",
      "Dmitrii Petrov",
      "Soundararajan Srinivasan"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.205": {
    "title": "AutoPlan: Automatic Planning of Interactive Decision-Making Tasks With Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siqi Ouyang",
      "Lei Li"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.206": {
    "title": "Measuring Faithful and Plausible Visual Grounding in VQA",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Reich",
      "Felix Putze",
      "Tanja Schultz"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.207": {
    "title": "Improving Zero-shot Reader by Reducing Distractions from Irrelevant Documents in Open-Domain Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sukmin Cho",
      "Jeongyeon Seo",
      "Soyeong Jeong",
      "Jong Park"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.208": {
    "title": "Can you Summarize my learnings? Towards Perspective-based Educational Dialogue Summarization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raghav Jain",
      "Tulika Saha",
      "Jhagrut Lalwani",
      "Sriparna Saha"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.209": {
    "title": "Adaptive Textual Label Noise Learning based on Pre-trained Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaohuan Cheng",
      "Wenyu Chen",
      "Fu Mingsheng",
      "Xuanting Xie",
      "Hong Qu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.210": {
    "title": "Towards Informative Open-ended Text Generation with Dynamic Knowledge Triples",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixuan Ren",
      "Yang Zhao",
      "Chengqing Zong"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.211": {
    "title": "Novel Relation Detection: Discovering Unknown Relation Types via Multi-Strategy Self-Supervised Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingbin Liu",
      "Yin Kung",
      "Yanchao Hao",
      "Dianbo Sui",
      "Siyuan Cheng",
      "Xi Chen",
      "Ningyu Zhang",
      "Jiaoyan Chen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.212": {
    "title": "Ask Language Model to Clean Your Noisy Translation Data",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quinten Bolding",
      "Baohao Liao",
      "Brandon Denis",
      "Jun Luo",
      "Christof Monz"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.213": {
    "title": "Multi-User MultiWOZ: Task-Oriented Dialogues among Multiple Users",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yohan Jo",
      "Xinyan Zhao",
      "Arijit Biswas",
      "Nikoletta Basiou",
      "Vincent Auvray",
      "Nikolaos Malandrakis",
      "Angeliki Metallinou",
      "Alexandros Potamianos"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.214": {
    "title": "Extractive Summarization via ChatGPT for Faithful Summary Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haopeng Zhang",
      "Xiao Liu",
      "Jiawei Zhang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.215": {
    "title": "MAPO: Boosting Large Language Model Performance with Model-Adaptive Prompt Optimization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuyan Chen",
      "Zhihao Wen",
      "Ge Fan",
      "Zhengyu Chen",
      "Wei Wu",
      "Dayiheng Liu",
      "Zhixu Li",
      "Bang Liu",
      "Yanghua Xiao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.216": {
    "title": "PsyCoT: Psychological Questionnaire as Powerful Chain-of-Thought for Personality Detection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Yang",
      "Tianyuan Shi",
      "Fanqi Wan",
      "Xiaojun Quan",
      "Qifan Wang",
      "Bingzhe Wu",
      "Jiaxiang Wu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.217": {
    "title": "Harnessing the power of LLMs: Evaluating human-AI text co-creation through the lens of news headline generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijian Ding",
      "Alison Smith-Renner",
      "Wenjuan Zhang",
      "Joel Tetreault",
      "Alejandro Jaimes"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.218": {
    "title": "NERetrieve: Dataset for Next Generation Named Entity Recognition and Retrieval",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Uri Katz",
      "Matan Vetzler",
      "Amir Cohen",
      "Yoav Goldberg"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.219": {
    "title": "SWEET - Weakly Supervised Person Name Extraction for Fighting Human Trafficking",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Javin Liu",
      "Hao Yu",
      "Vidya Sujaya",
      "Pratheeksha Nair",
      "Kellin Pelrine",
      "Reihaneh Rabbany"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.220": {
    "title": "Watermarking LLMs with Weight Quantization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linyang Li",
      "Botian Jiang",
      "Pengyu Wang",
      "Ke Ren",
      "Hang Yan",
      "Xipeng Qiu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.221": {
    "title": "Disentangling Extraction and Reasoning in Multi-hop Spatial Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roshanak Mirzaee",
      "Parisa Kordjamshidi"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.222": {
    "title": "PsyAttention: Psychological Attention Model for Personality Detection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baohua Zhang",
      "Yongyi Huang",
      "Wenyao Cui",
      "Zhang Huaping",
      "Jianyun Shang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.223": {
    "title": "RoAST: Robustifying Language Models via Adversarial Perturbation with Selective Training",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaehyung Kim",
      "Yuning Mao",
      "Rui Hou",
      "Hanchao Yu",
      "Davis Liang",
      "Pascale Fung",
      "Qifan Wang",
      "Fuli Feng",
      "Lifu Huang",
      "Madian Khabsa"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.224": {
    "title": "The Law and NLP: Bridging Disciplinary Disconnects",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Robert Mahari",
      "Dominik Stammbach",
      "Elliott Ash",
      "Alex Pentland"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.225": {
    "title": "Symbolization, Prompt, and Classification: A Framework for Implicit Speaker Identification in Novels",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Chen",
      "Tianwei He",
      "Hongbin Zhou",
      "Jia-Chen Gu",
      "Heng Lu",
      "Zhen-Hua Ling"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.226": {
    "title": "Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gabriel Sarch",
      "Yue Wu",
      "Michael Tarr",
      "Katerina Fragkiadaki"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.227": {
    "title": "ACT-SQL: In-Context Learning for Text-to-SQL with Automatically-Generated Chain-of-Thought",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanchong Zhang",
      "Ruisheng Cao",
      "Lu Chen",
      "Hongshen Xu",
      "Kai Yu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.228": {
    "title": "Manifold-Preserving Transformers are Effective for Short-Long Range Encoding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ayan Sengupta",
      "Md Akhtar",
      "Tanmoy Chakraborty"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.229": {
    "title": "ASPIRO: Any-shot Structured Parsing-error-Induced ReprOmpting for Consistent Data-to-Text Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Martin Vejvar",
      "Yasutaka Fujimoto"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.230": {
    "title": "Detecting Syntactic Change with Pre-trained Transformer Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liwen Hou",
      "David Smith"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.231": {
    "title": "A Word Sense Distribution-based approach for Semantic Change Prediction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaohang Tang",
      "Yi Zhou",
      "Taichi Aida",
      "Procheta Sen",
      "Danushka Bollegala"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.232": {
    "title": "Gold: A Global and Local-aware Denoising Framework for Commonsense Knowledge Graph Noise Detection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheye Deng",
      "Weiqi Wang",
      "Zhaowei Wang",
      "Xin Liu",
      "Yangqiu Song"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.233": {
    "title": "Improving Conversational Recommendation Systems via Bias Analysis and Language-Model-Enhanced Data Augmentation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xi Wang",
      "Hossein Rahmani",
      "Jiqun Liu",
      "Emine Yilmaz"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.234": {
    "title": "Exploring Graph Pre-training for Aspect-based Sentiment Analysis",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyi Bao",
      "Zhongqing Wang",
      "Guodong Zhou"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.235": {
    "title": "DemaFormer: Damped Exponential Moving Average Transformer with Energy-Based Modeling for Temporal Language Grounding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thong Nguyen",
      "Xiaobao Wu",
      "Xinshuai Dong",
      "Cong-Duy Nguyen",
      "See-Kiong Ng",
      "Anh Luu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.236": {
    "title": "Test-time Augmentation for Factual Probing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Go Kamoda",
      "Benjamin Heinzerling",
      "Keisuke Sakaguchi",
      "Kentaro Inui"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.237": {
    "title": "Methodological Insights in Detecting Subtle Semantic Shifts with Contextualized and Static Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanne Hoeken",
      "Özge Alacam",
      "Antske Fokkens",
      "Pia Sommerauer"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.238": {
    "title": "Disfluent Cues for Enhanced Speech Understanding in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Morteza Rohanian",
      "Farhad Nooralahzadeh",
      "Omid Rohanian",
      "David Clifton",
      "Michael Krauthammer"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.239": {
    "title": "Watermarking PLMs on Classification Tasks by Combining Contrastive Learning with Weight Perturbation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenxi Gu",
      "Xiaoqing Zheng",
      "Jianhan Xu",
      "Muling Wu",
      "Cenyuan Zhang",
      "Chengsong Huang",
      "Hua Cai",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.240": {
    "title": "BanLemma: A Word Formation Dependent Rule and Dictionary Based Bangla Lemmatizer",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sadia Afrin",
      "Md. Shahad Mahmud Chowdhury",
      "Md. Islam",
      "Faisal Khan",
      "Labib Chowdhury",
      "Md. Mahtab",
      "Nazifa Chowdhury",
      "Massud Forkan",
      "Neelima Kundu",
      "Hakim Arif",
      "Mohammad Mamun Or Rashid",
      "Mohammad Amin",
      "Nabeel Mohammed"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.241": {
    "title": "Exploring the Sensitivity of LLMs' Decision-Making Capabilities: Insights from Prompt Variations and Hyperparameters",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manikanta Loya",
      "Divya Sinha",
      "Richard Futrell"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.242": {
    "title": "Search Augmented Instruction Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyin Luo",
      "Tianhua Zhang",
      "Yung-Sung Chuang",
      "Yuan Gong",
      "Yoon Kim",
      "Xixin Wu",
      "Helen Meng",
      "James Glass"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.243": {
    "title": "Kelly is a Warm Person, Joseph is a Role Model\": Gender Biases in LLM-Generated Reference Letters",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixin Wan",
      "George Pu",
      "Jiao Sun",
      "Aparna Garimella",
      "Kai-Wei Chang",
      "Nanyun Peng"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.244": {
    "title": "TextMixer: Mixing Multiple Inputs for Privacy-Preserving Inference",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Zhou",
      "Yi Lu",
      "Ruotian Ma",
      "Tao Gui",
      "Qi Zhang",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.245": {
    "title": "FinePrompt: Unveiling the Role of Finetuned Inductive Bias on Compositional Reasoning in GPT-4",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeonghwan Kim",
      "Giwon Hong",
      "Sung-Hyon Myaeng",
      "Joyce Whang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.246": {
    "title": "Teacher Perception of Automatically Extracted Grammar Concepts for L2 Language Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditi Chaudhary",
      "Arun Sampath",
      "Ashwin Sheshadri",
      "Antonios Anastasopoulos",
      "Graham Neubig"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.247": {
    "title": "Allies: Prompting Large Language Model with Beam Search",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Sun",
      "Xiao Liu",
      "Yeyun Gong",
      "Yan Zhang",
      "Daxin Jiang",
      "Linjun Yang",
      "Nan Duan"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.248": {
    "title": "Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liangming Pan",
      "Alon Albalak",
      "Xinyi Wang",
      "William Wang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.249": {
    "title": "SiMFy: A Simple Yet Effective Approach for Temporal Knowledge Graph Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengtao Liu",
      "Lei Tan",
      "Mengfan Li",
      "Yao Wan",
      "Hai Jin",
      "Xuanhua Shi"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.250": {
    "title": "Understanding Translationese in Cross-Lingual Summarization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaan Wang",
      "Fandong Meng",
      "Yunlong Liang",
      "Tingyi Zhang",
      "Jiarong Xu",
      "Zhixu Li",
      "Jie Zhou"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.251": {
    "title": "The Truth, The Whole Truth, and Nothing but the Truth: A New Benchmark Dataset for Hebrew Text Credibility Assessment",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ben Hagag",
      "Reut Tsarfaty"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.252": {
    "title": "IndiSocialFT: Multilingual Word Representation for Indian languages in code-mixed environment",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saurabh Kumar",
      "Ranbir Sanasam",
      "Sukumar Nandi"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.253": {
    "title": "Adaptive Hinge Balance Loss for Document-Level Relation Extraction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jize Wang",
      "Xinyi Le",
      "Xiaodi Peng",
      "Cailian Chen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.254": {
    "title": "Answer-state Recurrent Relational Network (AsRRN) for Constructed Response Assessment and Feedback Grouping",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaohui Li",
      "Susan Lloyd",
      "Matthew Beckman",
      "Rebecca Passonneau"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.255": {
    "title": "Low-Resource Comparative Opinion Quintuple Extraction by Data Augmentation with Prompting",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingting Xu",
      "Yu Hong",
      "Fubang Zhao",
      "Kaisong Song",
      "Yangyang Kang",
      "Jiaxiang Chen",
      "Guodong Zhou"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.256": {
    "title": "A New Benchmark and Reverse Validation Method for Passage-level Hallucination Detection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiping Yang",
      "Renliang Sun",
      "Xiaojun Wan"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.257": {
    "title": "Speculative Decoding: Exploiting Speculative Execution for Accelerating Seq2seq Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heming Xia",
      "Tao Ge",
      "Peiyi Wang",
      "Si-Qing Chen",
      "Furu Wei",
      "Zhifang Sui"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.258": {
    "title": "APP: Adaptive Prototypical Pseudo-Labeling for Few-shot OOD Detection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pei Wang",
      "Keqing He",
      "Yutao Mou",
      "Xiaoshuai Song",
      "Yanan Wu",
      "Jingang Wang",
      "Yunsen Xian",
      "Xunliang Cai",
      "Weiran Xu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.259": {
    "title": "2INER: Instructive and In-Context Learning on Few-Shot Named Entity Recognition",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiasheng Zhang",
      "Xikai Liu",
      "Xinyi Lai",
      "Yan Gao",
      "Shusen Wang",
      "Yao Hu",
      "Yiqing Lin"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.260": {
    "title": "Generative Emotion Cause Triplet Extraction in Conversations with Commonsense Knowledge",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fanfan Wang",
      "Jianfei Yu",
      "Rui Xia"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.261": {
    "title": "Proto-lm: A Prototypical Network-Based Framework for Built-in Interpretability in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sean Xie",
      "Soroush Vosoughi",
      "Saeed Hassanpour"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.262": {
    "title": "GROVE: A Retrieval-augmented Complex Story Generation Framework with A Forest of Evidence",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihua Wen",
      "Zhiliang Tian",
      "Wei Wu",
      "Yuxin Yang",
      "Yanqi Shi",
      "Zhen Huang",
      "Dongsheng Li"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.263": {
    "title": "KAPALM: Knowledge grAPh enhAnced Language Models for Fake News Detection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Ma",
      "Chen Chen",
      "Chunyan Hou",
      "Xiaojie Yuan"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.264": {
    "title": "Comparing the Evaluation and Production of Loophole Behavior in Humans and Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sonia Murthy",
      "Kiera Parece",
      "Sophie Bridgers",
      "Peng Qian",
      "Tomer Ullman"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.265": {
    "title": "InstructExcel: A Benchmark for Natural Language Instruction in Excel",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Justin Payan",
      "Swaroop Mishra",
      "Mukul Singh",
      "Carina Negreanu",
      "Christian Poelitz",
      "Chitta Baral",
      "Subhro Roy",
      "Rasika Chakravarthy",
      "Benjamin Van Durme",
      "Elnaz Nouri"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.266": {
    "title": "Hallucination Detection for Grounded Instruction Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingjun Zhao",
      "Khanh Nguyen",
      "Hal Daumé III"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.267": {
    "title": "Definitions Matter: Guiding GPT for Multi-label Classification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youri Peskine",
      "Damir Korenčić",
      "Ivan Grubisic",
      "Paolo Papotti",
      "Raphael Troncy",
      "Paolo Rosso"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.268": {
    "title": "ECHo: A Visio-Linguistic Dataset for Event Causality Inference via Human-Centric Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxi Xie",
      "Guanzhen Li",
      "Min-Yen Kan"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.269": {
    "title": "An Empirical Study of Instruction-tuning Large Language Models in Chinese",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingyi Si",
      "Tong Wang",
      "Zheng Lin",
      "Xu Zhang",
      "Yanan Cao",
      "Weiping Wang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.270": {
    "title": "Debiasing Multimodal Models via Causal Information Minimization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vaidehi Patil",
      "Adyasha Maharana",
      "Mohit Bansal"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.271": {
    "title": "Evaluating Emotion Arcs Across Languages: Bridging the Global Divide in Sentiment Analysis",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniela Teodorescu",
      "Saif Mohammad"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.272": {
    "title": "Multi-step Jailbreaking Privacy Attacks on ChatGPT",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoran Li",
      "Dadi Guo",
      "Wei Fan",
      "Mingshi Xu",
      "Jie Huang",
      "Fanpu Meng",
      "Yangqiu Song"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.273": {
    "title": "Chain-of-Thought Embeddings for Stance Detection on Social Media",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joseph Gatto",
      "Omar Sharif",
      "Sarah Preum"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.274": {
    "title": "Using LLM for Improving Key Event Discovery: Temporal-Guided News Stream Clustering with Event Summaries",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nishanth Nakshatri",
      "Siyi Liu",
      "Sihao Chen",
      "Dan Roth",
      "Dan Goldwasser",
      "Daniel Hopkins"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.275": {
    "title": "Descriptive Prompt Paraphrasing for Target-Oriented Multimodal Sentiment Classification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dan Liu",
      "Lin Li",
      "Xiaohui Tao",
      "Jian Cui",
      "Qing Xie"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.276": {
    "title": "Joint Semantic and Strategy Matching for Persuasive Dialogue",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuhao Jin",
      "Yutao Zhu",
      "Lingzhen Kong",
      "Shijie Li",
      "Xiao Zhang",
      "Ruihua Song",
      "Xu Chen",
      "Huan Chen",
      "Yuchong Sun",
      "Yu Chen",
      "Jun Xu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.277": {
    "title": "Non-Autoregressive Sentence Ordering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Bin",
      "Wenhao Shi",
      "Bin Ji",
      "Jipeng Zhang",
      "Yujuan Ding",
      "Yang Yang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.278": {
    "title": "Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenhui Shen",
      "Liying Cheng",
      "Xuan-Phi Nguyen",
      "Yang You",
      "Lidong Bing"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.279": {
    "title": "Women Wearing Lipstick: Measuring the Bias Between an Object and Its Related Gender",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ahmed Sabir",
      "Lluís Padró"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.280": {
    "title": "FREDSum: A Dialogue Summarization Corpus for French Political Debates",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Virgile Rennard",
      "Guokan Shang",
      "Damien Grari",
      "Julie Hunter",
      "Michalis Vazirgiannis"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.281": {
    "title": "Towards Zero-shot Relation Extraction in Web Mining: A Multimodal Approach with Relative XML Path",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zilong Wang",
      "Jingbo Shang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.282": {
    "title": "Narrative Style and the Spread of Health Misinformation on Twitter",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Achyutarama Ganti",
      "Eslam Ali Hassan Hussein",
      "Steven Wilson",
      "Zexin Ma",
      "Xinyan Zhao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.283": {
    "title": "HadSkip: Homotopic and Adaptive Layer Skipping of Pre-trained Language Models for Efficient Inference",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Wang",
      "Yaqing Wang",
      "Tianci Liu",
      "Tuo Zhao",
      "Jing Gao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.284": {
    "title": "Empowering Psychotherapy with Large Language Models: Cognitive Distortion Detection through Diagnosis of Thought Prompting",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyu Chen",
      "Yujie Lu",
      "William Wang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.285": {
    "title": "Measuring the Knowledge Acquisition-Utilization Gap in Pretrained Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amirhossein Kazemnejad",
      "Mehdi Rezagholizadeh",
      "Prasanna Parthasarathi",
      "Sarath Chandar"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.286": {
    "title": "Non-compositional Expression Generation Based on Curriculum Learning and Continual Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianing Zhou",
      "Ziheng Zeng",
      "Hongyu Gong",
      "Suma Bhat"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.287": {
    "title": "Information Extraction from Legal Wills: How Well Does GPT-4 Do?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alice Kwak",
      "Cheonkam Jeong",
      "Gaetano Forte",
      "Derek Bambauer",
      "Clayton Morrison",
      "Mihai Surdeanu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.288": {
    "title": "Transparency at the Source: Evaluating and Interpreting Language Models With Access to the True Distribution",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaap Jumelet",
      "Willem Zuidema"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.289": {
    "title": "Continual Generalized Intent Discovery: Marching Towards Dynamic and Open-world Intent Recognition",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoshuai Song",
      "Yutao Mou",
      "Keqing He",
      "Yueyan Qiu",
      "Jinxu Zhao",
      "Pei Wang",
      "Weiran Xu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.290": {
    "title": "Frugal Prompting for Dialog Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bishal Santra",
      "Sakya Basak",
      "Abhinandan De",
      "Manish Gupta",
      "Pawan Goyal"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.291": {
    "title": "The Interpreter Understands Your Meaning: End-to-end Spoken Language Understanding Aided by Speech Translation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mutian He",
      "Philip Garner"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.292": {
    "title": "MacLaSa: Multi-Aspect Controllable Text Generation via Efficient Sampling from Compact Latent Space",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanxing Ding",
      "Liang Pang",
      "Zihao Wei",
      "Huawei Shen",
      "Xueqi Cheng",
      "Tat-Seng Chua"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.293": {
    "title": "HPE: Answering Complex Questions over Text by Hybrid Question Parsing and Execution",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ye Liu",
      "Semih Yavuz",
      "Rui Meng",
      "Dragomir Radev",
      "Caiming Xiong",
      "Shafiq Joty",
      "Yingbo Zhou"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.294": {
    "title": "Length-Adaptive Distillation: Customizing Small Language Model for Dynamic Token Pruning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chang Liu",
      "Chongyang Tao",
      "Jianxin Liang",
      "Jiazhan Feng",
      "Tao Shen",
      "Quzhe Huang",
      "Dongyan Zhao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.295": {
    "title": "Toxicity, Morality, and Speech Act Guided Stance Detection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Apoorva Upadhyaya",
      "Marco Fisichella",
      "Wolfgang Nejdl"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.296": {
    "title": "Reasoning about Ambiguous Definite Descriptions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefan Schouten",
      "Peter Bloem",
      "Ilia Markov",
      "Piek Vossen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.297": {
    "title": "A Framework for Bidirectional Decoding: Case Study in Morphological Inflection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marc Canby",
      "Julia Hockenmaier"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.298": {
    "title": "Text-guided 3D Human Generation from 2D Collections",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tsu-Jui Fu",
      "Wenhan Xiong",
      "Yixin Nie",
      "Jingyu Liu",
      "Barlas Oguz",
      "William Wang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.299": {
    "title": "Statistically Profiling Biases in Natural Language Reasoning Datasets and Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shanshan Huang",
      "Kenny Zhu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.300": {
    "title": "Verb Conjugation in Transformers Is Determined by Linear Encodings of Subject Number",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sophie Hao",
      "Tal Linzen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.301": {
    "title": "MUX-PLMs: Data Multiplexing for High-throughput Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vishvak Murahari",
      "Ameet Deshpande",
      "Carlos Jimenez",
      "Izhak Shafran",
      "Mingqiu Wang",
      "Yuan Cao",
      "Karthik Narasimhan"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.302": {
    "title": "That was the last straw, we need more: Are Translation Systems Sensitive to Disambiguating Context?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaechan Lee",
      "Alisa Liu",
      "Orevaoghene Ahia",
      "Hila Gonen",
      "Noah Smith"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.303": {
    "title": "MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Damien Sileo",
      "Antoine Lernould"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.304": {
    "title": "LATENTLOGIC: Learning Logic Rules in Latent Space over Knowledge Graphs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junnan Liu",
      "Qianren Mao",
      "Chenghua Lin",
      "Yangqiu Song",
      "Jianxin Li"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.305": {
    "title": "RobustEmbed: Robust Sentence Embeddings Using Self-Supervised Contrastive Pre-Training",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Javad Asl",
      "Eduardo Blanco",
      "Daniel Takabi"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.306": {
    "title": "More than Votes? Voting and Language based Partisanship in the US Supreme Court",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Biaoyan Fang",
      "Trevor Cohn",
      "Timothy Baldwin",
      "Lea Frermann"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.307": {
    "title": "Automatic Evaluation of Attribution by Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Yue",
      "Boshi Wang",
      "Ziru Chen",
      "Kai Zhang",
      "Yu Su",
      "Huan Sun"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.308": {
    "title": "Modeling Highlighting of Metaphors in Multitask Contrastive Learning Paradigms",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meghdut Sengupta",
      "Milad Alshomary",
      "Ingrid Scharlau",
      "Henning Wachsmuth"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.309": {
    "title": "LDM2: A Large Decision Model Imitating Human Cognition with Dynamic Memory Enhancement",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingjin Wang",
      "Linjing Li",
      "Daniel Zeng"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.310": {
    "title": "ZARA: Improving Few-Shot Self-Rationalization for Small Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei-Lin Chen",
      "An-Zi Yen",
      "Cheng-Kuang Wu",
      "Hen-Hsen Huang",
      "Hsin-Hsi Chen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.311": {
    "title": "ToxicChat: Unveiling Hidden Challenges of Toxicity Detection in Real-World User-AI Conversation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zi Lin",
      "Zihan Wang",
      "Yongqi Tong",
      "Yangkun Wang",
      "Yuxin Guo",
      "Yujia Wang",
      "Jingbo Shang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.312": {
    "title": "Mind the Gap: Automated Corpus Creation for Enthymeme Detection and Reconstruction in Learner Arguments",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maja Stahl",
      "Nick Düsterhus",
      "Mei-Hua Chen",
      "Henning Wachsmuth"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.313": {
    "title": "Dior-CVAE: Pre-trained Language Models and Diffusion Priors for Variational Dialog Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyu Yang",
      "Thy Tran",
      "Iryna Gurevych"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.314": {
    "title": "Retrieving Multimodal Information for Augmented Generation: A Survey",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruochen Zhao",
      "Hailin Chen",
      "Weishi Wang",
      "Fangkai Jiao",
      "Do Long",
      "Chengwei Qin",
      "Bosheng Ding",
      "Xiaobao Guo",
      "Minzhi Li",
      "Xingxuan Li",
      "Shafiq Joty"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.315": {
    "title": "Improving Contrastive Learning of Sentence Embeddings with Focal InfoNCE",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengyue Hou",
      "Xingyu Li"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.316": {
    "title": "The Vault: A Comprehensive Multilingual Dataset for Advancing Code Understanding and Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dung Nguyen",
      "Le Nam",
      "Anh Dau",
      "Anh Nguyen",
      "Khanh Nghiem",
      "Jin Guo",
      "Nghi Bui"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.317": {
    "title": "SDOH-NLI: a Dataset for Inferring Social Determinants of Health from Clinical Notes",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adam Lelkes",
      "Eric Loreaux",
      "Tal Schuster",
      "Ming-Jun Chen",
      "Alvin Rajkomar"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.318": {
    "title": "On the Zero-Shot Generalization of Machine-Generated Text Detectors",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Pu",
      "Jingyu Zhang",
      "Xiaochuang Han",
      "Yulia Tsvetkov",
      "Tianxing He"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.319": {
    "title": "Complex Event Schema Induction with Knowledge-Enriched Diffusion Model",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yupu Hao",
      "Pengfei Cao",
      "Yubo Chen",
      "Kang Liu",
      "Jiexin Xu",
      "Huaijun Li",
      "Xiaojian Jiang",
      "Jun Zhao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.320": {
    "title": "Exploiting Emotion-Semantic Correlations for Empathetic Response Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhou Yang",
      "Zhaochun Ren",
      "Wang Yufeng",
      "Xiaofei Zhu",
      "Zhihao Chen",
      "Tiecheng Cai",
      "Wu Yunbing",
      "Yisong Su",
      "Sibo Ju",
      "Xiangwen Liao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.321": {
    "title": "Long-Range Language Modeling with Selective Cache",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinting Huang",
      "Nora Hollenstein"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.322": {
    "title": "Medical Text Simplification: Optimizing for Readability with Unlikelihood Training and Reranked Beam Search Decoding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lorenzo Jaime Yu Flores",
      "Heyuan Huang",
      "Kejian Shi",
      "Sophie Chheang",
      "Arman Cohan"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.323": {
    "title": "FaLA: Fast Linear Adaptation for Replacing Backbone Models on Edge Devices",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuo Huang",
      "Lizhen Qu",
      "Xingliang Yuan",
      "Chunyang Chen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.324": {
    "title": "Intuitive Multilingual Audio-Visual Speech Recognition with a Single-Trained Model",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joanna Hong",
      "Se Park",
      "Yong Ro"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.325": {
    "title": "Controllable Chest X-Ray Report Generation from Longitudinal Representations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesco Serra",
      "Chaoyang Wang",
      "Fani Deligianni",
      "Jeff Dalton",
      "Alison O’Neil"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.326": {
    "title": "Is ChatGPT a Good Multi-Party Conversation Solver?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao-Hong Tan",
      "Jia-Chen Gu",
      "Zhen-Hua Ling"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.327": {
    "title": "Improving End-to-End Speech Processing by Efficient Text Data Utilization with Latent Synthesis",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianqiao Lu",
      "Wenyong Huang",
      "Nianzu Zheng",
      "Xingshan Zeng",
      "Yu Yeung",
      "Xiao Chen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.328": {
    "title": "Bipartite Graph Pre-training for Unsupervised Extractive Summarization with Graph Convolutional Auto-Encoders",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qianren Mao",
      "Shaobo Zhao",
      "Jiarui Li",
      "Xiaolei Gu",
      "Shizhu He",
      "Bo Li",
      "Jianxin Li"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.329": {
    "title": "Bayesian Multi-Task Transfer Learning for Soft Prompt Tuning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haeju Lee",
      "Minchan Jeong",
      "Se-Young Yun",
      "Kee-Eung Kim"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.330": {
    "title": "CCIM: Cross-modal Cross-lingual Interactive Image Translation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cong Ma",
      "Yaping Zhang",
      "Mei Tu",
      "Yang Zhao",
      "Yu Zhou",
      "Chengqing Zong"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.331": {
    "title": "TRAMS: Training-free Memory Selection for Long-range Language Modeling",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haofei Yu",
      "Cunxiang Wang",
      "Yue Zhang",
      "Wei Bi"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.332": {
    "title": "A Critical Analysis of Document Out-of-Distribution Detection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiuxiang Gu",
      "Yifei Ming",
      "Yi Zhou",
      "Jason Kuen",
      "Vlad Morariu",
      "Handong Zhao",
      "Ruiyi Zhang",
      "Nikolaos Barmpalios",
      "Anqi Liu",
      "Yixuan Li",
      "Tong Sun",
      "Ani Nenkova"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.333": {
    "title": "Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ke Wang",
      "Jun Xie",
      "Yuqi Zhang",
      "Yu Zhao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.334": {
    "title": "Active Learning Principles for In-Context Learning with Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Katerina Margatina",
      "Timo Schick",
      "Nikolaos Aletras",
      "Jane Dwivedi-Yu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.335": {
    "title": "InteMATs: Integrating Granularity-Specific Multilingual Adapters for Cross-Lingual Transfer",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meizhen Liu",
      "Xu Guo",
      "He Jiakai",
      "Jianye Chen",
      "Fengyu Zhou",
      "Siu Hui"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.336": {
    "title": "PlugMed: Improving Specificity in Patient-Centered Medical Dialogue Generation using In-Context Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengfeng Dou",
      "Zhi Jin",
      "Wenpin Jiao",
      "Haiyan Zhao",
      "Yongqiang Zhao",
      "Zhengwei Tao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.337": {
    "title": "CodeTransOcean: A Comprehensive Multilingual Benchmark for Code Translation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weixiang Yan",
      "Yuchen Tian",
      "Yunzhe Li",
      "Qian Chen",
      "Wen Wang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.338": {
    "title": "impact of sample selection on in-context learning for entity extraction from scientific writing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Necva Bölücü",
      "Maciej Rybinski",
      "Stephen Wan"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.339": {
    "title": "Goodtriever: Adaptive Toxicity Mitigation with Retrieval-augmented Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luiza Pozzobon",
      "Beyza Ermis",
      "Patrick Lewis",
      "Sara Hooker"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.340": {
    "title": "Robustness Tests for Automatic Machine Translation Metrics with Adversarial Attacks",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yichen Huang",
      "Timothy Baldwin"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.341": {
    "title": "Time-Considerable Dialogue Models via Reranking by Time Dependency",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuiko Tsunomori",
      "Masakazu Ishihata",
      "Hiroaki Sugiyama"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.342": {
    "title": "Non-Compositionality in Sentiment: New Data and Analyses",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Verna Dankers",
      "Christopher Lucas"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.343": {
    "title": "MPrompt: Exploring Multi-level Prompt Tuning for Machine Reading Comprehension",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guoxin Chen",
      "Yiming Qian",
      "Bowen Wang",
      "Liangzhi Li"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.344": {
    "title": "DocTrack: A Visually-Rich Document Dataset Really Aligned with Human Eye Movement for Machine Reading",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Wang",
      "Qingxuan Wang",
      "Yue Li",
      "Changqing Wang",
      "Chenhui Chu",
      "Rui Wang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.345": {
    "title": "Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiefeng Chen",
      "Jinsung Yoon",
      "Sayna Ebrahimi",
      "Sercan Arik",
      "Tomas Pfister",
      "Somesh Jha"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.346": {
    "title": "Bi-Drop: Enhancing Fine-tuning Generalization via Synchronous sub-net Estimation and Optimization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shoujie Tong",
      "Heming Xia",
      "Damai Dai",
      "Runxin Xu",
      "Tianyu Liu",
      "Binghuai Lin",
      "Yunbo Cao",
      "Zhifang Sui"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.347": {
    "title": "ClozEx: A Task toward Generation of English Cloze Explanation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zizheng Zhang",
      "Masato Mita",
      "Mamoru Komachi"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.348": {
    "title": "Is Probing All You Need? Indicator Tasks as an Alternative to Probing Embedding Spaces",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tal Levy",
      "Omer Goldman",
      "Reut Tsarfaty"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.349": {
    "title": "The Cost of Compression: Investigating the Impact of Compression on Parametric Knowledge in Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Satya Sai Srinath Namburi",
      "Makesh Sreedhar",
      "Srinath Srinivasan",
      "Frederic Sala"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.350": {
    "title": "CoEdIT: Text Editing by Task-Specific Instruction Tuning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vipul Raheja",
      "Dhruv Kumar",
      "Ryan Koo",
      "Dongyeop Kang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.351": {
    "title": "Exploring Large Language Models for Multi-Modal Out-of-Distribution Detection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Dai",
      "Hao Lang",
      "Kaisheng Zeng",
      "Fei Huang",
      "Yongbin Li"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.352": {
    "title": "Better Together: Enhancing Generative Knowledge Graph Completion with Language Models and Neighborhood Information",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alla Chepurova",
      "Aydar Bulatov",
      "Yuri Kuratov",
      "Mikhail Burtsev"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.353": {
    "title": "DeltaScore: Fine-Grained Story Evaluation with Perturbations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuohan Xie",
      "Miao Li",
      "Trevor Cohn",
      "Jey Lau"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.354": {
    "title": "MuG: A Multimodal Classification Benchmark on Game Data with Tabular, Textual, and Visual Fields",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaying Lu",
      "Yongchen Qian",
      "Shifan Zhao",
      "Yuanzhe Xi",
      "Carl Yang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.355": {
    "title": "Don't waste a single annotation: improving single-label classifiers through soft labels",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ben Wu",
      "Yue Li",
      "Yida Mu",
      "Carolina Scarton",
      "Kalina Bontcheva",
      "Xingyi Song"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.356": {
    "title": "Black-Box Tuning of Vision-Language Models with Effective Gradient Approximation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixian Guo",
      "Yuxiang Wei",
      "Ming Liu",
      "Zhilong Ji",
      "Jinfeng Bai",
      "Yiwen Guo",
      "Wangmeng Zuo"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.357": {
    "title": "How to Determine the Most Powerful Pre-trained Language Model without Brute Force Fine-tuning? An Empirical Survey",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Bai",
      "Xiaofeng Zhang",
      "Chen Li",
      "Hanhua Hong",
      "Xi Xu",
      "Chenghua Lin",
      "Wenge Rong"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.358": {
    "title": "Licon: A Diverse, Controllable and Challenging Linguistic Concept Learning Benchmark",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shenglong Yu",
      "Ying Zhang",
      "Wenya Guo",
      "Zhengkun Zhang",
      "Ru Zhou",
      "Xiaojie Yuan"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.359": {
    "title": "InterroLang: Exploring NLP Models and Datasets through Dialogue-based Explanations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nils Feldhus",
      "Qianli Wang",
      "Tatiana Anikina",
      "Sahil Chopra",
      "Cennet Oguz",
      "Sebastian Möller"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.360": {
    "title": "INVITE: a Testbed of Automatically Generated Invalid Questions to Evaluate Large Language Models for Hallucinations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anil Ramakrishna",
      "Rahul Gupta",
      "Jens Lehmann",
      "Morteza Ziyadi"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.361": {
    "title": "Multimodal Automated Fact-Checking: A Survey",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mubashara Akhtar",
      "Michael Schlichtkrull",
      "Zhijiang Guo",
      "Oana Cocarascu",
      "Elena Simperl",
      "Andreas Vlachos"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.362": {
    "title": "PROTEGE: Prompt-based Diverse Question Generation from Web Articles",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vinayak Puranik",
      "Anirban Majumder",
      "Vineet Chaoji"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.363": {
    "title": "GPT-4 as an Effective Zero-Shot Evaluator for Scientific Figure Captions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ting-Yao Hsu",
      "Chieh-Yang Huang",
      "Ryan Rossi",
      "Sungchul Kim",
      "C. Giles",
      "Ting-Hao Huang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.364": {
    "title": "Mulan: A Multi-Level Alignment Model for Video Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Fu",
      "Cong Cao",
      "Yuling Yang",
      "Yuhai Lu",
      "Fangfang Yuan",
      "Dakui Wang",
      "Yanbing Liu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.365": {
    "title": "HARE: Explainable Hate Speech Detection with Step-by-Step Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongjin Yang",
      "Joonkee Kim",
      "Yujin Kim",
      "Namgyu Ho",
      "James Thorne",
      "Se-Young Yun"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.366": {
    "title": "ReLM: Leveraging Language Models for Enhanced Chemical Reaction Prediction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaorui Shi",
      "An Zhang",
      "Enzhi Zhang",
      "Zhiyuan Liu",
      "Xiang Wang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.367": {
    "title": "Decomposing Complex Queries for Tip-of-the-tongue Retrieval",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kevin Lin",
      "Kyle Lo",
      "Joseph Gonzalez",
      "Dan Klein"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.368": {
    "title": "Values, Ethics, Morals? On the Use of Moral Concepts in NLP Research",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karina Vida",
      "Judith Simon",
      "Anne Lauscher"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.369": {
    "title": "Self-Supervised Behavior Cloned Transformers are Path Crawlers for Text Games",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruoyao Wang",
      "Peter Jansen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.370": {
    "title": "Adapting Pretrained Text-to-Text Models for Long Text Sequences",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhan Xiong",
      "Anchit Gupta",
      "Shubham Toshniwal",
      "Yashar Mehdad",
      "Scott Yih"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.371": {
    "title": "xDial-Eval: A Multilingual Open-Domain Dialogue Evaluation Benchmark",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Zhang",
      "Luis D’Haro",
      "Chengguang Tang",
      "Ke Shi",
      "Guohua Tang",
      "Haizhou Li"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.372": {
    "title": "MathDial: A Dialogue Tutoring Dataset with Rich Pedagogical Properties Grounded in Math Reasoning Problems",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jakub Macina",
      "Nico Daheim",
      "Sankalan Chowdhury",
      "Tanmay Sinha",
      "Manu Kapur",
      "Iryna Gurevych",
      "Mrinmaya Sachan"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.373": {
    "title": "Towards Making the Most of ChatGPT for Machine Translation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keqin Peng",
      "Liang Ding",
      "Qihuang Zhong",
      "Li Shen",
      "Xuebo Liu",
      "Min Zhang",
      "Yuanxin Ouyang",
      "Dacheng Tao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.374": {
    "title": "Enhancing Reasoning Capabilities by Instruction Learning and Chain-of-Thoughts for Implicit Discourse Relation Recognition",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxiang Lu",
      "Yu Hong",
      "Zhipang Wang",
      "Guodong Zhou"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.375": {
    "title": "Large-Scale and Multi-Perspective Opinion Summarization with Diverse Review Subsets",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Jiang",
      "Rui Wang",
      "Zhihua Wei",
      "Yu Li",
      "Xinpeng Wang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.376": {
    "title": "Topic-Informed Dialogue Summarization using Topic Distribution and Prompt-based Modeling",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaeah You",
      "Youngjoong Ko"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.377": {
    "title": "Disentangling Structure and Style: Political Bias Detection in News by Inducing Document Hierarchy",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiwoo Hong",
      "Yejin Cho",
      "Jiyoung Han",
      "Jaemin Jung",
      "James Thorne"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.378": {
    "title": "Measuring and Narrowing the Compositionality Gap in Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ofir Press",
      "Muru Zhang",
      "Sewon Min",
      "Ludwig Schmidt",
      "Noah Smith",
      "Mike Lewis"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.379": {
    "title": "Unsupervised Candidate Answer Extraction through Differentiable Masker-Reconstructor Model",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuoer Wang",
      "Yicheng Wang",
      "Ziwei Zhu",
      "James Caverlee"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.380": {
    "title": "HoneyBee: Progressive Instruction Finetuning of Large Language Models for Materials Science",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Song",
      "Santiago Miret",
      "Huan Zhang",
      "Bang Liu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.381": {
    "title": "Prompt-Based Editing for Text Style Transfer",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guoqing Luo",
      "Yu Han",
      "Lili Mou",
      "Mauajama Firdaus"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.382": {
    "title": "Representativeness as a Forgotten Lesson for Multilingual and Code-switched Data Collection and Preparation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "A. Doğruöz",
      "Sunayana Sitaram",
      "Zheng Yong"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.383": {
    "title": "NERvous About My Health: Constructing a Bengali Medical Named Entity Recognition Dataset",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alvi Khan",
      "Fida Kamal",
      "Nuzhat Nower",
      "Tasnim Ahmed",
      "Sabbir Ahmed",
      "Tareque Chowdhury"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.384": {
    "title": "Sparse Black-Box Multimodal Attack for Vision-Language Adversary Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Yu",
      "Zhou Qin",
      "Zhenhua Chen",
      "Meihui Lian",
      "Haojun Fu",
      "Weigao Wen",
      "Hui Xue",
      "Kun He"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.385": {
    "title": "Towards a Unified Framework for Reference Retrieval and Related Work Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengliang Shi",
      "Shen Gao",
      "Zhen Zhang",
      "Xiuying Chen",
      "Zhumin Chen",
      "Pengjie Ren",
      "Zhaochun Ren"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.386": {
    "title": "Visual Storytelling with Question-Answer Plans",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Danyang Liu",
      "Mirella Lapata",
      "Frank Keller"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.387": {
    "title": "Investigating Online Community Engagement through Stancetaking",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jai Aggarwal",
      "Brian Diep",
      "Julia Watson",
      "Suzanne Stevenson"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.388": {
    "title": "ASSERT: Automated Safety Scenario Red Teaming for Evaluating the Robustness of Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alex Mei",
      "Sharon Levy",
      "William Wang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.389": {
    "title": "Learning to Correct Noisy Labels for Fine-Grained Entity Typing via Co-Prediction Prompt Tuning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minghao Tang",
      "Yongquan He",
      "Yongxiu Xu",
      "Hongbo Xu",
      "Wenyuan Zhang",
      "Yang Lin"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.390": {
    "title": "Co2PT: Mitigating Bias in Pre-trained Language Models through Counterfactual Contrastive Prompt Tuning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangjue Dong",
      "Ziwei Zhu",
      "Zhuoer Wang",
      "Maria Teleki",
      "James Caverlee"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.391": {
    "title": "A Hierarchical Encoding-Decoding Scheme for Abstractive Multi-document Summarization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenhui Shen",
      "Liying Cheng",
      "Xuan-Phi Nguyen",
      "Yang You",
      "Lidong Bing"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.392": {
    "title": "Universal Domain Adaptation for Robust Handling of Distributional Shifts in NLP",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyuhng Kim",
      "Hyunsoo Cho",
      "Sang-Woo Lee",
      "Junyeob Kim",
      "Choonghyun Park",
      "Sang-goo Lee",
      "Kang Yoo",
      "Taeuk Kim"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.393": {
    "title": "Aligning Language Models to User Opinions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "EunJeong Hwang",
      "Bodhisattwa Majumder",
      "Niket Tandon"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.394": {
    "title": "CCSRD: Content-Centric Speech Representation Disentanglement Learning for End-to-End Speech Translation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaohu Zhao",
      "Haoran Sun",
      "Yikun Lei",
      "Shaolin Zhu",
      "Deyi Xiong"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.395": {
    "title": "Miracle: Towards Personalized Dialogue Generation with Latent-Space Multiple Personal Attribute Control",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenyi Lu",
      "Wei Wei",
      "Xiaoye Qu",
      "Xian-Ling Mao",
      "Dangyang Chen",
      "Jixiong Chen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.396": {
    "title": "Towards Multilingual Interlinear Morphological Glossing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shu Okabe",
      "François Yvon"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.397": {
    "title": "Transformer Working Memory Enables Regular Language Reasoning And Natural Language Length Extrapolation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ta-Chung Chi",
      "Ting-Han Fan",
      "Alexander Rudnicky",
      "Peter Ramadge"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.398": {
    "title": "Enhancing Conversational Search: Large Language Model-Aided Informative Query Rewriting",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fanghua Ye",
      "Meng Fang",
      "Shenghui Li",
      "Emine Yilmaz"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.399": {
    "title": "Distilling ChatGPT for Explainable Automated Student Answer Assessment",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiazheng Li",
      "Lin Gui",
      "Yuxiang Zhou",
      "David West",
      "Cesare Aloisi",
      "Yulan He"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.400": {
    "title": "Grammatical Error Correction via Mixed-Grained Weighted Training",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahao Li",
      "Quan Wang",
      "Chiwei Zhu",
      "Zhendong Mao",
      "Yongdong Zhang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.401": {
    "title": "A Unified Framework for Synaesthesia Analysis",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kun Sheng",
      "Zhongqing Wang",
      "Qingqing Zhao",
      "Xiaotong Jiang",
      "Guodong Zhou"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.402": {
    "title": "Domain Private Transformers for Multi-Domain Dialog Systems",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anmol Kabra",
      "Ethan Elenberg"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.403": {
    "title": "Visual Elements Mining as Prompts for Instruction Learning for Target-Oriented Multimodal Sentiment Classification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bin Yang",
      "Jinlong Li"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.404": {
    "title": "NASH: A Simple Unified Framework of Structured Pruning for Accelerating Encoder-Decoder Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jongwoo Ko",
      "Seungjoon Park",
      "Yujin Kim",
      "Sumyeong Ahn",
      "Du-Seong Chang",
      "Euijai Ahn",
      "Se-Young Yun"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.405": {
    "title": "GBT: Generative Boosting Training Approach for Paraphrase Identification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Peng",
      "Zhiling Jin",
      "Yu Hong"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.406": {
    "title": "DeCrisisMB: Debiased Semi-Supervised Learning for Crisis Tweet Classification via Memory Bank",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Henry Zou",
      "Yue Zhou",
      "Weizhi Zhang",
      "Cornelia Caragea"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.407": {
    "title": "Probing LLMs for hate speech detection: strengths and vulnerabilities",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sarthak Roy",
      "Ashish Harshvardhan",
      "Animesh Mukherjee",
      "Punyajoy Saha"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.408": {
    "title": "From Simple to Complex: A Progressive Framework for Document-level Informative Argument Extraction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quzhe Huang",
      "Yanxi Zhang",
      "Dongyan Zhao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.409": {
    "title": "MultiCMET: A Novel Chinese Benchmark for Understanding Multimodal Metaphor",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongyu Zhang",
      "Jingwei Yu",
      "Senyuan Jin",
      "Liang Yang",
      "Hongfei Lin"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.410": {
    "title": "GlotLID: Language Identification for Low-Resource Languages",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amir Kargaran",
      "Ayyoob Imani",
      "François Yvon",
      "Hinrich Schuetze"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.411": {
    "title": "Finding Support Examples for In-Context Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaonan Li",
      "Xipeng Qiu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.412": {
    "title": "Uncovering the Root of Hate Speech: A Dataset for Identifying Hate Instigating Speech",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyoungjun Park",
      "Ho Shim",
      "Kyuhan Lee"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.413": {
    "title": "Responsible AI Considerations in Text Summarization Research: A Review of Current Practices",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Liu",
      "Meng Cao",
      "Su Blodgett",
      "Jackie Cheung",
      "Alexandra Olteanu",
      "Adam Trischler"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.414": {
    "title": "Improving Speech Translation by Fusing Speech and Text",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenbiao Yin",
      "Zhicheng Liu",
      "Chengqi Zhao",
      "Tao Wang",
      "Jian Tong",
      "Rong Ye"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.415": {
    "title": "Narrative Order Aware Story Generation via Bidirectional Pretraining Model with Optimal Transport Reward",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhicong Lu",
      "Li Jin",
      "Guangluan Xu",
      "Linmei Hu",
      "Nayu Liu",
      "Xiaoyu Li",
      "Xian Sun",
      "Zequn Zhang",
      "Kaiwen Wei"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.416": {
    "title": "Explainable Claim Verification via Knowledge-Grounded Reasoning with Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoran Wang",
      "Kai Shu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.417": {
    "title": "Strong and Efficient Baselines for Open Domain Conversational Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrei Coman",
      "Gianni Barlacchi",
      "Adrià de Gispert"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.418": {
    "title": "Efficient Continue Training of Temporal Language Model with Structural Information",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaochen Su",
      "Juntao Li",
      "Zikang Zhang",
      "Zihan Zhou",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.419": {
    "title": "Retrieval-Augmented Parsing for Complex Graphs by Exploiting Structure and Uncertainty",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zi Lin",
      "Quan Yuan",
      "Panupong Pasupat",
      "Jeremiah Liu",
      "Jingbo Shang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.420": {
    "title": "When it Rains, it Pours: Modeling Media Storms and the News Ecosystem",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Litterer",
      "David Jurgens",
      "Dallas Card"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.421": {
    "title": "Intra-Event and Inter-Event Dependency-Aware Graph Network for Event Argument Extraction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Li",
      "Yanan Cao",
      "Yubing Ren",
      "Fang Fang",
      "Lanxue Zhang",
      "Yingjie Li",
      "Shi Wang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.422": {
    "title": "From Relevance to Utility: Evidence Retrieval with Feedback for Fact Verification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hengran Zhang",
      "Ruqing Zhang",
      "Jiafeng Guo",
      "Maarten de Rijke",
      "Yixing Fan",
      "Xueqi Cheng"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.423": {
    "title": "How to Train Your Dragon: Diverse Augmentation Towards Generalizable Dense Retrieval",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng-Chieh Lin",
      "Akari Asai",
      "Minghan Li",
      "Barlas Oguz",
      "Jimmy Lin",
      "Yashar Mehdad",
      "Wen-tau Yih",
      "Xilun Chen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.424": {
    "title": "Discovering Highly Influential Shortcut Reasoning: An Automated Template-Free Approach",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daichi Haraguchi",
      "Kiyoaki Shirai",
      "Naoya Inoue",
      "Natthawut Kertkeidkachorn"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.425": {
    "title": "Schema-adaptable Knowledge Graph Construction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongbin Ye",
      "Honghao Gui",
      "Xin Xu",
      "Xi Chen",
      "Huajun Chen",
      "Ningyu Zhang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.426": {
    "title": "Evaluating the Knowledge Base Completion Potential of GPT",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Blerta Veseli",
      "Simon Razniewski",
      "Jan-Christoph Kalo",
      "Gerhard Weikum"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.427": {
    "title": "Conic10K: A Challenging Math Problem Understanding and Reasoning Dataset",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyi Wu",
      "Wenyang Hui",
      "Yezeng Chen",
      "Weiqi Wu",
      "Kewei Tu",
      "Yi Zhou"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.428": {
    "title": "DepWiGNN: A Depth-wise Graph Neural Network for Multi-hop Spatial Reasoning in Text",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuaiyi Li",
      "Yang Deng",
      "Wai Lam"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.429": {
    "title": "TK-KNN: A Balanced Distance-Based Pseudo Labeling Approach for Semi-Supervised Intent Classification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicholas Botzer",
      "David Vazquez",
      "Tim Weninger",
      "Issam Laradji"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.430": {
    "title": "Late Fusion of Transformers for Sentiment Analysis of Code-Switched Data",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gagan Sharma",
      "R Chinmay",
      "Raksha Sharma"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.431": {
    "title": "Inductive Relation Inference of Knowledge Graph Enhanced by Ontology Information",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wentao Zhou",
      "Jun Zhao",
      "Tao Gui",
      "Qi Zhang",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.432": {
    "title": "Dynamic Stance: Modeling Discussions by Labeling the Interactions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Blanca Figueras",
      "Irene Baucells",
      "Tommaso Caselli"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.433": {
    "title": "Harnessing the Power of Large Language Models for Empathetic Response Generation: Empirical Investigations and Improvements",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yushan Qian",
      "Weinan Zhang",
      "Ting Liu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.434": {
    "title": "GPT Deciphering Fedspeak: Quantifying Dissent Among Hawks and Doves",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Denis Peskoff",
      "Adam Visokay",
      "Sander Schulhoff",
      "Benjamin Wachspress",
      "Alan Blinder",
      "Brandon Stewart"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.435": {
    "title": "DialogQAE: N-to-N Question Answer Pair Extraction from Customer Service Chatlog",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Zheng",
      "Tianyu Liu",
      "Haoran Meng",
      "Xu Wang",
      "Yufan Jiang",
      "Mengliang Rao",
      "Binghuai Lin",
      "Yunbo Cao",
      "Zhifang Sui"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.436": {
    "title": "Inverse Reinforcement Learning for Text Summarization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Fu",
      "Deyi Xiong",
      "Yue Dong"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.437": {
    "title": "MM-Reasoner: A Multi-Modal Knowledge-Aware Framework for Knowledge-Based Visual Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mahmoud Khademi",
      "Ziyi Yang",
      "Felipe Frujeri",
      "Chenguang Zhu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.438": {
    "title": "Toward Joint Language Modeling for Speech Units and Text",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ju-Chieh Chou",
      "Chung-Ming Chien",
      "Wei-Ning Hsu",
      "Karen Livescu",
      "Arun Babu",
      "Alexis Conneau",
      "Alexei Baevski",
      "Michael Auli"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.439": {
    "title": "From Chaos to Clarity: Claim Normalization to Empower Fact-Checking",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Megha Sundriyal",
      "Tanmoy Chakraborty",
      "Preslav Nakov"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.440": {
    "title": "Mitigating Biases in Hate Speech Detection from A Causal Perspective",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhehao Zhang",
      "Jiaao Chen",
      "Diyi Yang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.441": {
    "title": "Unmasking the Hidden Meaning: Bridging Implicit and Explicit Hate Speech Embedding Representations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicolas Ocampo",
      "Elena Cabrio",
      "Serena Villata"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.442": {
    "title": "PerturbScore: Connecting Discrete and Continuous Perturbations in NLP",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linyang Li",
      "Ke Ren",
      "Yunfan Shao",
      "Pengyu Wang",
      "Xipeng Qiu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.443": {
    "title": "InstructoR: Instructing Unsupervised Conversational Dense Retrieval with Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuoran Jin",
      "Pengfei Cao",
      "Yubo Chen",
      "Kang Liu",
      "Jun Zhao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.444": {
    "title": "The Iron(ic) Melting Pot: Reviewing Human Evaluation in Humour, Irony and Sarcasm Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tyler Loakman",
      "Aaron Maladry",
      "Chenghua Lin"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.445": {
    "title": "INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "H S V N S Kowndinya Renduchintala",
      "Krishnateja Killamsetty",
      "Sumit Bhatia",
      "Milan Aggarwal",
      "Ganesh Ramakrishnan",
      "Rishabh Iyer",
      "Balaji Krishnamurthy"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.446": {
    "title": "Towards General Error Diagnosis via Behavioral Testing in Machine Translation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junjie Wu",
      "Lemao Liu",
      "Dit-Yan Yeung"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.447": {
    "title": "Retrieval-Augmented Few-shot Text Classification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guoxin Yu",
      "Lemao Liu",
      "Haiyun Jiang",
      "Shuming Shi",
      "Xiang Ao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.448": {
    "title": "Temporal Extrapolation and Knowledge Transfer for Lifelong Temporal Knowledge Graph Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongwu Chen",
      "Chengjin Xu",
      "Fenglong Su",
      "Zhen Huang",
      "Yong Dou"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.449": {
    "title": "Comparing Prompt-Based and Standard Fine-Tuning for Urdu Text Classification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Faizad Ullah",
      "Ubaid Azam",
      "Ali Faheem",
      "Faisal Kamiran",
      "Asim Karim"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.450": {
    "title": "Explore the Way: Exploring Reasoning Path by Bridging Entities for Effective Cross-Document Relation Extraction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyoung Son",
      "Jinsung Kim",
      "Jungwoo Lim",
      "Yoonna Jang",
      "Heuiseok Lim"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.451": {
    "title": "The student becomes the master: Outperforming GPT3 on Scientific Factual Error Correction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dhananjay Ashok",
      "Atharva Kulkarni",
      "Hai Pham",
      "Barnabas Poczos"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.452": {
    "title": "Leveraging Structured Information for Explainable Multi-hop Question Answering and Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruosen Li",
      "Xinya Du"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.453": {
    "title": "Hierarchical Catalogue Generation for Literature Review: A Benchmark",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kun Zhu",
      "Xiaocheng Feng",
      "Xiachong Feng",
      "Yingsheng Wu",
      "Bing Qin"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.454": {
    "title": "MCC-KD: Multi-CoT Consistent Knowledge Distillation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongzhan Chen",
      "Siyue Wu",
      "Xiaojun Quan",
      "Rui Wang",
      "Ming Yan",
      "Ji Zhang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.455": {
    "title": "An Empirical Study of Frame Selection for Text-to-Video Retrieval",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengxia Wu",
      "Min Cao",
      "Yang Bai",
      "Ziyin Zeng",
      "Chen Chen",
      "Liqiang Nie",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.456": {
    "title": "Conditional Natural Language Inference",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youngwoo Kim",
      "Razieh Rahimi",
      "James Allan"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.457": {
    "title": "Contrastive Distant Supervision for Debiased and Denoised Machine Reading Comprehension",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ning Bian",
      "Hongyu Lin",
      "Xianpei Han",
      "Ben He",
      "Le Sun"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.458": {
    "title": "KEPLET: Knowledge-Enhanced Pretrained Language Model with Topic Entity Awareness",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yichuan Li",
      "Jialong Han",
      "Kyumin Lee",
      "Chengyuan Ma",
      "Benjamin Yao",
      "Xiaohu Liu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.459": {
    "title": "Revisiting Large Language Models as Zero-shot Relation Extractors",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guozheng Li",
      "Peng Wang",
      "Wenjun Ke"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.460": {
    "title": "Multi-Stage Pre-training Enhanced by ChatGPT for Multi-Scenario Multi-Domain Dialogue Summarization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weixiao Zhou",
      "Gengyao Li",
      "Xianfu Cheng",
      "Xinnian Liang",
      "Junnan Zhu",
      "Feifei Zhai",
      "Zhoujun Li"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.461": {
    "title": "Towards large language model-based personal agents in the enterprise: Current trends and open problems",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vinod Muthusamy",
      "Yara Rizk",
      "Kiran Kate",
      "Praveen Venkateswaran",
      "Vatche Isahagian",
      "Ashu Gulati",
      "Parijat Dube"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.462": {
    "title": "CREATOR: Tool Creation for Disentangling Abstract and Concrete Reasoning of Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Qian",
      "Chi Han",
      "Yi Fung",
      "Yujia Qin",
      "Zhiyuan Liu",
      "Heng Ji"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.463": {
    "title": "Query-based Image Captioning from Multi-context 360cdegree Images",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Koki Maeda",
      "Shuhei Kurita",
      "Taiki Miyanishi",
      "Naoaki Okazaki"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.464": {
    "title": "Auto Search Indexer for End-to-End Document Retrieval",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianchi Yang",
      "Minghui Song",
      "Zihan Zhang",
      "Haizhen Huang",
      "Weiwei Deng",
      "Feng Sun",
      "Qi Zhang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.465": {
    "title": "‘Person' == Light-skinned, Western Man, and Sexualization of Women of Color: Stereotypes in Stable Diffusion",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sourojit Ghosh",
      "Aylin Caliskan"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.466": {
    "title": "Task-Attentive Transformer Architecture for Continual Learning of Vision-and-Language Tasks Using Knowledge Distillation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuliang Cai",
      "Jesse Thomason",
      "Mohammad Rostami"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.467": {
    "title": "Evaluating Verifiability in Generative Search Engines",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nelson Liu",
      "Tianyi Zhang",
      "Percy Liang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.468": {
    "title": "Enhancing Abstractiveness of Summarization Models through Calibrated Distillation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hwanjun Song",
      "Igor Shalyminov",
      "Hang Su",
      "Siffi Singh",
      "Kaisheng Yao",
      "Saab Mansour"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.469": {
    "title": "Visually Grounded Continual Language Learning with Selective Specialization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyra Ahrens",
      "Lennart Bengtson",
      "Jae Lee",
      "Stefan Wermter"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.470": {
    "title": "RoMQA: A Benchmark for Robust, Multi-evidence, Multi-answer Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Victor Zhong",
      "Weijia Shi",
      "Wen-tau Yih",
      "Luke Zettlemoyer"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.471": {
    "title": "Leveraging Multiple Teachers for Test-Time Adaptation of Language-Guided Classifiers",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kangda Wei",
      "Sayan Ghosh",
      "Rakesh Menon",
      "Shashank Srivastava"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.472": {
    "title": "Summarizing Multiple Documents with Conversational Structure for Meta-Review Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miao Li",
      "Eduard Hovy",
      "Jey Lau"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.473": {
    "title": "VIPHY: Probing \"Visible\" Physical Commonsense Knowledge",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shikhar Singh",
      "Ehsan Qasemi",
      "Muhao Chen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.474": {
    "title": "Two Directions for Clinical Data Generation with Large Language Models: Data-to-Label and Label-to-Data",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rumeng Li",
      "Xun Wang",
      "Hong Yu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.475": {
    "title": "Stylized Dialogue Generation with Feature-Guided Knowledge Augmentation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinpeng Li",
      "Zekai Zhang",
      "Xiuying Chen",
      "Dongyan Zhao",
      "Rui Yan"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.476": {
    "title": "Probing LLMs for Joint Encoding of Linguistic Categories",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giulio Starace",
      "Konstantinos Papakostas",
      "Rochelle Choenni",
      "Apostolos Panagiotopoulos",
      "Matteo Rosati",
      "Alina Leidinger",
      "Ekaterina Shutova"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.477": {
    "title": "On Robustness of Finetuned Transformer-based NLP Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pavan Kalyan Reddy Neerudu",
      "Subba Oota",
      "Mounika Marreddy",
      "Venkateswara Kagita",
      "Manish Gupta"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.478": {
    "title": "Measuring and Mitigating Constraint Violations of In-Context Learning for Utterance-to-API Semantic Parsing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shufan Wang",
      "Sébastien Jean",
      "Sailik Sengupta",
      "James Gung",
      "Nikolaos Pappas",
      "Yi Zhang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.479": {
    "title": "Entity Disambiguation on a Tight Labeling Budget",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Audi Primadhanty",
      "Ariadna Quattoni"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.480": {
    "title": "Topic-DPR: Topic-based Prompts for Dense Passage Retrieval",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingfa Xiao",
      "Shuangyin Li",
      "Lei Chen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.481": {
    "title": "Quantifying the Dialect Gap and its Correlates Across Languages",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anjali Kantharuban",
      "Ivan Vulić",
      "Anna Korhonen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.482": {
    "title": "RECAL: Sample-Relation Guided Confidence Calibration over Tabular Data",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wang HaoTian",
      "Zhen Zhang",
      "Mengting Hu",
      "Qichao Wang",
      "Liang Chen",
      "Yatao Bian",
      "Bingzhe Wu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.483": {
    "title": "Parameter-Efficient Cross-lingual Transfer of Vision and Language Models via Translation-based Alignment",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Zhang",
      "Jialu Wang",
      "Xin Wang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.484": {
    "title": "Lexical Repetitions Lead to Rote Learning: Unveiling the Impact of Lexical Overlap in Train and Test Reference Summaries",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prafulla Choubey",
      "Alexander Fabbri",
      "Caiming Xiong",
      "Chien-Sheng Wu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.485": {
    "title": "Pseudointelligence: A Unifying Lens on Language Model Evaluation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shikhar Murty",
      "Orr Paradise",
      "Pratyusha Sharma"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.486": {
    "title": "GDA: Grammar-based Data Augmentation for Text Classification using Slot Information",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joonghyuk Hahn",
      "Hyunjoon Cheon",
      "Elizabeth Orwig",
      "Su-Hyeon Kim",
      "Sang-Ki Ko",
      "Yo-Sub Han"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.487": {
    "title": "Implicit Sense-labeled Connective Recognition as Text Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yui Oka",
      "Tsutomu Hirao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.488": {
    "title": "VISTA: Visual-Textual Knowledge Graph Representation Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaejun Lee",
      "Chanyoung Chung",
      "Hochang Lee",
      "Sungho Jo",
      "Joyce Whang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.489": {
    "title": "Dynamic Stashing Quantization for Efficient Transformer Training",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guo Yang",
      "Daniel Lo",
      "Robert Mullins",
      "Yiren Zhao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.490": {
    "title": "A Comprehensive Evaluation of Large Language Models on Legal Judgment Prediction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruihao Shui",
      "Yixin Cao",
      "Xiang Wang",
      "Tat-Seng Chua"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.491": {
    "title": "A Lightweight Method to Generate Unanswerable Questions in English",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vagrant Gautam",
      "Miaoran Zhang",
      "Dietrich Klakow"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.492": {
    "title": "Automatic Evaluate Dialogue Appropriateness by Using Dialogue Act",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bao Chen",
      "Yuanjie Wang",
      "Zeming Liu",
      "Yuhang Guo"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.493": {
    "title": "TabPrompt: Graph-based Pre-training and Prompting for Few-shot Table Understanding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rihui Jin",
      "Jianan Wang",
      "Wei Tan",
      "Yongrui Chen",
      "Guilin Qi",
      "Wang Hao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.494": {
    "title": "Towards Formality-Aware Neural Machine Translation by Leveraging Context Information",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dohee Kim",
      "Yujin Baek",
      "Soyoung Yang",
      "Jaegul Choo"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.495": {
    "title": "Improving Seq2Seq Grammatical Error Correction via Decoding Interventions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Houquan Zhou",
      "Yumeng Liu",
      "Zhenghua Li",
      "Min Zhang",
      "Bo Zhang",
      "Chen Li",
      "Ji Zhang",
      "Fei Huang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.496": {
    "title": "Exploring the Potential of Large Language Models in Generating Code-Tracing Questions for Introductory Programming Courses",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aysa Fan",
      "Haoran Zhang",
      "Luc Paquette",
      "Rui Zhang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.497": {
    "title": "Learning Easily Updated General Purpose Text Representations with Adaptable Task-Specific Prefix",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kuan-Hao Huang",
      "Liang Tan",
      "Rui Hou",
      "Sinong Wang",
      "Amjad Almahairi",
      "Ruty Rinott"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.498": {
    "title": "Good Meta-tasks Make A Better Cross-lingual Meta-transfer Learning for Low-resource Languages",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linjuan Wu",
      "Zongyi Guo",
      "Baoliang Cui",
      "Haihong Tang",
      "Weiming Lu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.499": {
    "title": "Reasoning Makes Good Annotators : An Automatic Task-specific Rules Distilling Framework for Low-resource Relation Extraction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yilin Lu",
      "Juncheng Li",
      "Xiaoqiang Wang",
      "Haochen Shi",
      "Tao Chen",
      "Siliang Tang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.500": {
    "title": "Co-training and Co-distillation for Quality Improvement and Compression of Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hayeon Lee",
      "Rui Hou",
      "Jongpil Kim",
      "Davis Liang",
      "Hongbo Zhang",
      "Sung Hwang",
      "Alexander Min"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.501": {
    "title": "ReadPrompt: A Readable Prompting Method for Reliable Knowledge Probing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zezhong Wang",
      "Luyao Ye",
      "Hongru Wang",
      "Wai-Chung Kwan",
      "David Ho",
      "Kam-Fai Wong"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.502": {
    "title": "Coherent Entity Disambiguation via Modeling Topic and Categorical Dependency",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zilin Xiao",
      "Linjun Shou",
      "Xingyao Zhang",
      "Jie Wu",
      "Ming Gong",
      "Daxin Jiang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.503": {
    "title": "How Predictable Are Large Language Model Capabilities? A Case Study on BIG-bench",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qinyuan Ye",
      "Harvey Fu",
      "Xiang Ren",
      "Robin Jia"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.504": {
    "title": "POSQA: Probe the World Models of LLMs with Size Comparisons",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chang Shu",
      "Jiuzhou Han",
      "Fangyu Liu",
      "Ehsan Shareghi",
      "Nigel Collier"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.505": {
    "title": "Hierarchical Fusion for Online Multimodal Dialog Act Classification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Messal Monem Miah",
      "Adarsh Pyarelal",
      "Ruihong Huang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.506": {
    "title": "STEER: Unified Style Transfer with Expert Reinforcement",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Skyler Hallinan",
      "Faeze Brahman",
      "Ximing Lu",
      "Jaehun Jung",
      "Sean Welleck",
      "Yejin Choi"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.507": {
    "title": "Enhancing Argument Structure Extraction with Efficient Leverage of Contextual Information",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yun Luo",
      "Zhen Yang",
      "Fandong Meng",
      "Yingjie Li",
      "Jie Zhou",
      "Yue Zhang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.508": {
    "title": "Examining Inter-Consistency of Large Language Models Collaboration: An In-depth Analysis via Debate",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Xiong",
      "Xiao Ding",
      "Yixin Cao",
      "Ting Liu",
      "Bing Qin"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.509": {
    "title": "Culturally Aware Natural Language Inference",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Huang",
      "Diyi Yang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.510": {
    "title": "End-to-End Autoregressive Retrieval via Bootstrapping for Smart Reply Systems",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Towle",
      "Ke Zhou"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.511": {
    "title": "Evaluating Dependencies in Fact Editing for Language Models: Specificity and Implication Awareness",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zichao Li",
      "Ines Arous",
      "Siva Reddy",
      "Jackie Cheung"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.512": {
    "title": "Effects of Human Adversarial and Affable Samples on BERT Generalization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aparna Elangovan",
      "Estrid He",
      "Yuan Li",
      "Karin Verspoor"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.513": {
    "title": "Logic Unveils Truth, While Disguise Obscures It: Transition Logic Augmented Response Selection for Multi-Turn Dialogue",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tingchen Fu",
      "Xueliang Zhao",
      "Lemao Liu",
      "Rui Yan"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.514": {
    "title": "Are Language Models Worse than Humans at Following Prompts? It's Complicated",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Albert Webson",
      "Alyssa Loo",
      "Qinan Yu",
      "Ellie Pavlick"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.515": {
    "title": "A Sequence-to-Structure Approach to Document-level Targeted Sentiment Analysis",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nan Song",
      "Hongjie Cai",
      "Rui Xia",
      "Jianfei Yu",
      "Zhen Wu",
      "Xinyu Dai"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.516": {
    "title": "Generating Extractive Answers: Gated Recurrent Memory Reader for Conversational Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuanyu Zhang",
      "Qing Yang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.517": {
    "title": "Text2Tree: Aligning Text Representation to the Label Tree Hierarchy for Imbalanced Medical Classification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahuan Yan",
      "Haojun Gao",
      "Zhang Kai",
      "Weize Liu",
      "Danny Chen",
      "Jian Wu",
      "Jintai Chen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.518": {
    "title": "Impact of Co-occurrence on Factual Knowledge of Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheongwoong Kang",
      "Jaesik Choi"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.519": {
    "title": "CTQScorer: Combining Multiple Features for In-context Example Selection for Machine Translation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aswanth M",
      "Ratish Puduppully",
      "Raj Dabre",
      "Anoop Kunchukuttan"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.520": {
    "title": "Swap and Predict – Predicting the Semantic Changes in Words across Corpora by Context Swapping",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taichi Aida",
      "Danushka Bollegala"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.521": {
    "title": "Beyond Layout Embedding: Layout Attention with Gaussian Biases for Structured Document Understanding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xi Zhu",
      "Xue Han",
      "Shuyuan Peng",
      "Shuo Lei",
      "Chao Deng",
      "Junlan Feng"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.522": {
    "title": "ESPVR: Entity Spans Position Visual Regions for Multimodal Named Entity Recognition",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiujiao Li",
      "Guanglu Sun",
      "Xinyu Liu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.523": {
    "title": "Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingfeng Shen",
      "Weiting Tan",
      "Boyuan Zheng",
      "Daniel Khashabi"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.524": {
    "title": "Detecting Erroneously Recognized Handwritten Byzantine Text",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "John Pavlopoulos",
      "Vasiliki Kougia",
      "Paraskevi Platanou",
      "Holger Essler"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.525": {
    "title": "Improving Factual Consistency for Knowledge-Grounded Dialogue Systems via Knowledge Enhancement and Alignment",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boyang Xue",
      "Weichao Wang",
      "Hongru Wang",
      "Fei Mi",
      "Rui Wang",
      "Yasheng Wang",
      "Lifeng Shang",
      "Xin Jiang",
      "Qun Liu",
      "Kam-Fai Wong"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.526": {
    "title": "TRIP: Accelerating Document-level Multilingual Pre-training via Triangular Document-level Pre-training on Parallel Data Triplets",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyuan Lu",
      "Haoyang Huang",
      "Shuming Ma",
      "Dongdong Zhang",
      "Wai Lam",
      "Zhaochuan Gao",
      "Anthony Aue",
      "Arul Menezes",
      "Furu Wei"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.527": {
    "title": "Frequency Balanced Datasets Lead to Better Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rodolfo Zevallos",
      "Mireia Farrús",
      "Núria Bel"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.528": {
    "title": "Uncertainty-aware Parameter-Efficient Self-training for Semi-supervised Language Understanding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianing Wang",
      "Qiushi Sun",
      "Nuo Chen",
      "Chengyu Wang",
      "Jun Huang",
      "Ming Gao",
      "Xiang Li"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.529": {
    "title": "TR-Rules: Rule-based Model for Link Forecasting on Temporal Knowledge Graph Considering Temporal Redundancy",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ningyuan Li",
      "Haihong E",
      "Shi Li",
      "Mingzhi Sun",
      "Tianyu Yao",
      "Meina Song",
      "Yong Wang",
      "Haoran Luo"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.530": {
    "title": "On the Transferability of Visually Grounded PCFGs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanpeng Zhao",
      "Ivan Titov"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.531": {
    "title": "Analysis of Style-Shifting on Social Media: Using Neural Language Model Conditioned by Social Meanings",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seiya Kawano",
      "Shota Kanezaki",
      "Angel Fernando Garcia Contreras",
      "Akishige Yuguchi",
      "Marie Katsurai",
      "Koichiro Yoshino"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.532": {
    "title": "Linguistic Compression in Single-Sentence Human-Written Summaries",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangcong Yin",
      "Marten van Schijndel"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.533": {
    "title": "MCLF: A Multi-grained Contrastive Learning Framework for ASR-robust Spoken Language Understanding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiqi Huang",
      "Dongsheng Chen",
      "Zhihong Zhu",
      "Xuxin Cheng"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.534": {
    "title": "Beyond Candidates : Adaptive Dialogue Agent Utilizing Persona and Knowledge",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jungwoo Lim",
      "Myunghoon Kang",
      "Jinsung Kim",
      "Jeongwook Kim",
      "Yuna Hur",
      "Heuiseok Lim"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.535": {
    "title": "SmartSpanNER: Making SpanNER Robust in Low Resource Scenarios",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Min Zhang",
      "Xiaosong Qiao",
      "Yanqing Zhao",
      "Shimin Tao",
      "Hao Yang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.536": {
    "title": "ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Uri Shaham",
      "Maor Ivgi",
      "Avia Efrat",
      "Jonathan Berant",
      "Omer Levy"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.537": {
    "title": "Data Selection Curriculum for Abstractive Text Summarization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shichao Sun",
      "Ruifeng Yuan",
      "Jianfei He",
      "Ziqiang Cao",
      "Wenjie Li",
      "Xiaohua Jia"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.538": {
    "title": "Romanization-based Large-scale Adaptation of Multilingual Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sukannya Purkayastha",
      "Sebastian Ruder",
      "Jonas Pfeiffer",
      "Iryna Gurevych",
      "Ivan Vulić"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.539": {
    "title": "Measuring bias in Instruction-Following models with P-AT",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dario Onorati",
      "Elena Ruzzetti",
      "Davide Venditti",
      "Leonardo Ranaldi",
      "Fabio Zanzotto"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.540": {
    "title": "Open-ended Commonsense Reasoning with Unrestricted Answer Candidates",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Ling",
      "Xuchao Zhang",
      "Xujiang Zhao",
      "Yanchi Liu",
      "Wei Cheng",
      "Mika Oishi",
      "Takao Osaki",
      "Katsushi Matsuda",
      "Haifeng Chen",
      "Liang Zhao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.541": {
    "title": "Speaking Style Conversion in the Waveform Domain Using Discrete Self-Supervised Units",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gallil Maimon",
      "Yossi Adi"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.542": {
    "title": "Knowledge-Selective Pretraining for Attribute Value Extraction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hui Liu",
      "Qingyu Yin",
      "Zhengyang Wang",
      "Chenwei Zhang",
      "Haoming Jiang",
      "Yifan Gao",
      "Zheng Li",
      "Xian Li",
      "Chao Zhang",
      "Bing Yin",
      "William Wang",
      "Xiaodan Zhu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.543": {
    "title": "New Datasets and Controllable Iterative Data Augmentation Method for Code-switching ASR Error Correction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaohong Wan",
      "Xiaojun Wan",
      "Wei Peng",
      "Rongjun Li"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.544": {
    "title": "Efficient k-NN Search with Cross-Encoders using Adaptive Multi-Round CUR Decomposition",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nishant Yadav",
      "Nicholas Monath",
      "Manzil Zaheer",
      "Andrew McCallum"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.545": {
    "title": "Isotropic Representation Can Improve Zero-Shot Cross-Lingual Transfer on Multilingual Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixin Ji",
      "Jikai Wang",
      "Juntao Li",
      "Hai Ye",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.546": {
    "title": "Blackbird language matrices (BLM), a new task for rule-like generalization in neural networks: Can Large Language Models pass the test?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paola Merlo"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.547": {
    "title": "DistillCSE: Distilled Contrastive Learning for Sentence Embeddings",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahao Xu",
      "Wei Shao",
      "Lihui Chen",
      "Lemao Liu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.548": {
    "title": "GSAP-NER: A Novel Task, Corpus, and Baseline for Scholarly Entity Extraction Focused on Machine Learning Models and Datasets",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wolfgang Otto",
      "Matthäus Zloch",
      "Lu Gan",
      "Saurav Karmakar",
      "Stefan Dietze"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.549": {
    "title": "Open Domain Multi-document Summarization: A Comprehensive Study of Model Brittleness under Retrieval",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "John Giorgi",
      "Luca Soldaini",
      "Bo Wang",
      "Gary Bader",
      "Kyle Lo",
      "Lucy Wang",
      "Arman Cohan"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.550": {
    "title": "Few-shot Unified Question Answering: Tuning Models or Prompts?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Srijan Bansal",
      "Semih Yavuz",
      "Bo Pang",
      "Meghana Bhat",
      "Yingbo Zhou"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.551": {
    "title": "Finding Common Ground: Annotating and Predicting Common Ground in Spoken Conversations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Magdalena Markowska",
      "Mohammad Taghizadeh",
      "Adil Soubki",
      "Seyed Mirroshandel",
      "Owen Rambow"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.552": {
    "title": "Getting MoRE out of Mixture of Language Model Reasoning Experts",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenglei Si",
      "Weijia Shi",
      "Chen Zhao",
      "Luke Zettlemoyer",
      "Jordan Boyd-Graber"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.553": {
    "title": "You Are An Expert Linguistic Annotator\": Limits of LLMs as Analyzers of Abstract Meaning Representation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Allyson Ettinger",
      "Jena Hwang",
      "Valentina Pyatkin",
      "Chandra Bhagavatula",
      "Yejin Choi"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.554": {
    "title": "Zero-Shot Data Maps. Efficient Dataset Cartography Without Model Training",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Angelo Basile",
      "Marc Franco-Salvador",
      "Paolo Rosso"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.555": {
    "title": "Isotropy-Enhanced Conditional Masked Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pei Guo",
      "Yisheng Xiao",
      "Juntao Li",
      "Yixin Ji",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.556": {
    "title": "Scaling Law for Document Neural Machine Translation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhang Zhuocheng",
      "Shuhao Gu",
      "Min Zhang",
      "Yang Feng"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.557": {
    "title": "Automatic Pronunciation Assessment - A Review",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yassine Kheir",
      "Ahmed Ali",
      "Shammur Chowdhury"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.558": {
    "title": "Segmented Recurrent Transformer: An Efficient Sequence-to-Sequence Model",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinghan Long",
      "Sayeed Chowdhury",
      "Kaushik Roy"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.559": {
    "title": "PUNR: Pre-training with User Behavior Modeling for News Recommendation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangyuan Ma",
      "Hongtao Liu",
      "Xing W",
      "Wanhui Qian",
      "Zhepeng Lv",
      "Qing Yang",
      "Songlin Hu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.560": {
    "title": "Monte Carlo Thought Search: Large Language Model Querying for Complex Scientific Reasoning in Catalyst Design",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Henry Sprueill",
      "Carl Edwards",
      "Mariefel Olarte",
      "Udishnu Sanyal",
      "Heng Ji",
      "Sutanay Choudhury"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.561": {
    "title": "Measure Children's Mindreading Ability with Machine Reading",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuliang Yan",
      "Xiaohua Wang",
      "Xiang Zhou",
      "Xiaoqing Zheng",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.562": {
    "title": "Crosslingual Transfer Learning for Low-Resource Languages Based on Multilingual Colexification Graphs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihong Liu",
      "Haotian Ye",
      "Leonie Weissweiler",
      "Renhao Pei",
      "Hinrich Schuetze"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.563": {
    "title": "Injecting structural hints: Using language models to study inductive biases in language learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Isabel Papadimitriou",
      "Dan Jurafsky"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.564": {
    "title": "Machine Reading Comprehension using Case-based Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dung Thai",
      "Dhruv Agarwal",
      "Mudit Chaudhary",
      "Wenlong Zhao",
      "Rajarshi Das",
      "Jay-Yoon Lee",
      "Hannaneh Hajishirzi",
      "Manzil Zaheer",
      "Andrew McCallum"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.565": {
    "title": "Unleashing the Power of Language Models in Text-Attributed Graph",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Kuang",
      "Jiarong Xu",
      "Haozhe Zhang",
      "Zuyu Zhao",
      "Qi Zhang",
      "Xuanjing Huang",
      "Zhongyu Wei"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.566": {
    "title": "Locally Differentially Private Document Generation Using Zero Shot Prompting",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saiteja Utpala",
      "Sara Hooker",
      "Pin-Yu Chen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.567": {
    "title": "Contrastive Deterministic Autoencoders For Language Modeling",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amur Ghose",
      "Pascal Poupart"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.568": {
    "title": "CHiLL: Zero-shot Custom Interpretable Feature Extraction from Clinical Notes with Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Denis McInerney",
      "Geoffrey Young",
      "Jan-Willem van de Meent",
      "Byron Wallace"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.569": {
    "title": "Guiding LLM to Fool Itself: Automatically Manipulating Machine Reading Comprehension Shortcut Triggers",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mosh Levy",
      "Shauli Ravfogel",
      "Yoav Goldberg"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.570": {
    "title": "Large Language Models Meet Harry Potter: A Dataset for Aligning Dialogue Agents with Characters",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nuo Chen",
      "Yan Wang",
      "Haiyun Jiang",
      "Deng Cai",
      "Yuhan Li",
      "Ziyang Chen",
      "Longyue Wang",
      "Jia Li"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.571": {
    "title": "Quick Back-Translation for Unsupervised Machine Translation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Brimacombe",
      "Jiawei Zhou"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.572": {
    "title": "SIR-ABSC: Incorporating Syntax into RoBERTa-based Sentiment Analysis Models with a Special Aggregator Token",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ikhyun Cho",
      "Yoonhwa Jung",
      "Julia Hockenmaier"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.573": {
    "title": "Citance-Contextualized Summarization of Scientific Papers",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shahbaz Syed",
      "Ahmad Hakimi",
      "Khalid Al-Khatib",
      "Martin Potthast"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.574": {
    "title": "SegAugment: Maximizing the Utility of Speech Translation Data with Segmentation-based Augmentations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ioannis Tsiamas",
      "José Fonollosa",
      "Marta Costa-jussà"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.575": {
    "title": "Intersectional Stereotypes in Large Language Models: Dataset and Analysis",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weicheng Ma",
      "Brian Chiang",
      "Tong Wu",
      "Lili Wang",
      "Soroush Vosoughi"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.576": {
    "title": "Dataset Bias Mitigation in Multiple-Choice Visual Question Answering and Beyond",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhecan Wang",
      "Long Chen",
      "Haoxuan You",
      "Keyang Xu",
      "Yicheng He",
      "Wenhao Li",
      "Noel Codella",
      "Kai-Wei Chang",
      "Shih-Fu Chang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.577": {
    "title": "The Intended Uses of Automated Fact-Checking Artefacts: Why, How and Who",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Schlichtkrull",
      "Nedjma Ousidhoum",
      "Andreas Vlachos"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.578": {
    "title": "Retrieval-based Knowledge Transfer: An Effective Approach for Extreme Large Language Model Compression",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiduan Liu",
      "Jiahao Liu",
      "Qifan Wang",
      "Jingang Wang",
      "Xunliang Cai",
      "Dongyan Zhao",
      "Ran Wang",
      "Rui Yan"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.579": {
    "title": "COUNT: COntrastive UNlikelihood Text Style Transfer for Text Detoxification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Mahdi Abdollah Pour",
      "Parsa Farinneya",
      "Manasa Bharadwaj",
      "Nikhil Verma",
      "Ali Pesaranghader",
      "Scott Sanner"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.580": {
    "title": "KICGPT: Large Language Model with Knowledge in Context for Knowledge Graph Completion",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanbin Wei",
      "Qiushi Huang",
      "Yu Zhang",
      "James Kwok"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.581": {
    "title": "Show, Write, and Retrieve: Entity-aware Article Generation and Retrieval",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongping Zhang",
      "Yiwen Gu",
      "Bryan Plummer"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.582": {
    "title": "A Language Model with Limited Memory Capacity Captures Interference in Human Sentence Processing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "William Timkey",
      "Tal Linzen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.583": {
    "title": "Annotations Are Not All You Need: A Cross-modal Knowledge Transfer Network for Unsupervised Temporal Sentence Grounding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Fang",
      "Daizong Liu",
      "Wanlong Fang",
      "Pan Zhou",
      "Yu Cheng",
      "Keke Tang",
      "Kai Zou"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.584": {
    "title": "Parameter Efficient Multi-task Fine-tuning by Learning to Transfer Token-wise Prompts",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muling Wu",
      "Wenhao Liu",
      "Jianhan Xu",
      "Changze Lv",
      "Zixuan Ling",
      "Tianlong Li",
      "Longtao Huang",
      "Xiaoqing Zheng",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.585": {
    "title": "A Rewriting Approach for Gender Inclusivity in Portuguese",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leonor Veloso",
      "Luisa Coheur",
      "Rui Ribeiro"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.586": {
    "title": "EARA: Improving Biomedical Semantic Textual Similarity with Entity-Aligned Attention and Retrieval Augmentation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ying Xiong",
      "Xin Yang",
      "Linjing Liu",
      "Ka-Chun Wong",
      "Qingcai Chen",
      "Yang Xiang",
      "Buzhou Tang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.587": {
    "title": "Neuro-Symbolic Sentiment Analysis with Dynamic Word Sense Disambiguation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xulang Zhang",
      "Rui Mao",
      "Kai He",
      "Erik Cambria"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.588": {
    "title": "Role of Context in Unsupervised Sentence Representation Learning: the Case of Dialog Act Modeling",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rastislav Hronsky",
      "Emmanuel Keuleers"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.589": {
    "title": "CLMSM: A Multi-Task Learning Framework for Pre-training on Procedural Text",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhilash Nandy",
      "Manav Kapadnis",
      "Pawan Goyal",
      "Niloy Ganguly"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.590": {
    "title": "Open-source Large Language Models are Strong Zero-shot Query Likelihood Models for Document Ranking",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengyao Zhuang",
      "Bing Liu",
      "Bevan Koopman",
      "Guido Zuccon"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.591": {
    "title": "On General Language Understanding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Schlangen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.592": {
    "title": "USB: A Unified Summarization Benchmark Across Tasks and Domains",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kundan Krishna",
      "Prakhar Gupta",
      "Sanjana Ramprasad",
      "Byron Wallace",
      "Jeffrey Bigham",
      "Zachary Lipton"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.593": {
    "title": "tagE: Enabling an Embodied Agent to Understand Human Instructions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chayan Sarkar",
      "Avik Mitra",
      "Pradip Pramanick",
      "Tapas Nayak"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.594": {
    "title": "Instances and Labels: Hierarchy-aware Joint Supervised Contrastive Learning for Hierarchical Multi-Label Text Classification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simon Chi Lok Yu",
      "Jie He",
      "Victor Basulto",
      "Jeff Pan"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.595": {
    "title": "Uncovering Limitations in Text-to-Image Generation: A Contrastive Approach with Structured Semantic Alignment",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qianyu Feng",
      "Yulei Sui",
      "Hongyu Zhang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.596": {
    "title": "An Intent-based and Annotation-free Method for Duplicate Question Detection in CQA Forums",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yubo Shu",
      "Hansu Gu",
      "Peng Zhang",
      "Tun Lu",
      "Ning Gu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.597": {
    "title": "Accelerating Multiple Intent Detection and Slot Filling via Targeted Knowledge Distillation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuxin Cheng",
      "Zhihong Zhu",
      "Wanshi Xu",
      "Yaowei Li",
      "Hongxiang Li",
      "Yuexian Zou"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.598": {
    "title": "Type-Aware Decomposed Framework for Few-Shot Named Entity Recognition",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongqi Li",
      "Yu Yu",
      "Tieyun Qian"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.599": {
    "title": "A Closer Look into Using Large Language Models for Automatic Evaluation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng-Han Chiang",
      "Hung-yi Lee"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.600": {
    "title": "Connecting the Dots: What Graph-Based Text Representations Work Best for Text Classification using Graph Neural Networks?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Margarita Bugueño",
      "Gerard de Melo"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.601": {
    "title": "Natural Language Annotations for Reasoning about Program Semantics",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marco Zocca"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.602": {
    "title": "Pre-trained Speech Processing Models Contain Human-Like Biases that Propagate to Speech Emotion Recognition",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Isaac Slaughter",
      "Craig Greenberg",
      "Reva Schwartz",
      "Aylin Caliskan"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.603": {
    "title": "Text Classification via Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaofei Sun",
      "Xiaoya Li",
      "Jiwei Li",
      "Fei Wu",
      "Shangwei Guo",
      "Tianwei Zhang",
      "Guoyin Wang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.604": {
    "title": "On Task-personalized Multimodal Few-shot Learning for Visually-rich Document Entity Retrieval",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayi Chen",
      "Hanjun Dai",
      "Bo Dai",
      "Aidong Zhang",
      "Wei Wei"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.605": {
    "title": "Semi-Structured Object Sequence Encoders",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rudra Murthy",
      "Riyaz Bhat",
      "Chulaka Gunasekara",
      "Siva Patel",
      "Hui Wan",
      "Tejas Dhamecha",
      "Danish Contractor",
      "Marina Danilevsky"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.606": {
    "title": "DeTiME: Diffusion-Enhanced Topic Modeling using Encoder-decoder based LLM",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weijie Xu",
      "Wenxiang Hu",
      "Fanyou Wu",
      "Srinivasan Sengamedu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.607": {
    "title": "Energy and Carbon Considerations of Fine-Tuning BERT",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaorong Wang",
      "Clara Na",
      "Emma Strubell",
      "Sorelle Friedler",
      "Sasha Luccioni"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.608": {
    "title": "Democratizing LLMs: An Exploration of Cost-Performance Trade-offs in Self-Refined Open-Source Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sumuk Shashidhar",
      "Abhinav Chinta",
      "Vaibhav Sahai",
      "Zhenhailong Wang",
      "Heng Ji"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.609": {
    "title": "Chinese Metaphorical Relation Extraction: Dataset and Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guihua Chen",
      "Tiantian Wu",
      "MiaoMiao Cheng",
      "Xu Han",
      "Jiefu Gong",
      "Shijin Wang",
      "Wei Song"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.610": {
    "title": "Example-based Hypernetworks for Multi-source Adaptation to Unseen Domains",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tomer Volk",
      "Eyal Ben-David",
      "Ohad Amosy",
      "Gal Chechik",
      "Roi Reichart"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.611": {
    "title": "Beneath the Surface: Unveiling Harmful Memes with Multimodal Reasoning Distilled from Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongzhan Lin",
      "Ziyang Luo",
      "Jing Ma",
      "Long Chen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.612": {
    "title": "Domain Adaptation for Conversational Query Production with the RAG Model Feedback",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ante Wang",
      "Linfeng Song",
      "Ge Xu",
      "Jinsong Su"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.613": {
    "title": "LEGO: A Multi-agent Collaborative Framework with Role-playing and Iterative Feedback for Causality Explanation Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhitao He",
      "Pengfei Cao",
      "Yubo Chen",
      "Kang Liu",
      "Ruopeng Li",
      "Mengshu Sun",
      "Jun Zhao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.614": {
    "title": "Ranking LLM-Generated Loop Invariants for Program Verification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saikat Chakraborty",
      "Shuvendu Lahiri",
      "Sarah Fakhoury",
      "Akash Lal",
      "Madanlal Musuvathi",
      "Aseem Rastogi",
      "Aditya Senthilnathan",
      "Rahul Sharma",
      "Nikhil Swamy"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.615": {
    "title": "WordNet Is All You Need: A Surprisingly Effective Unsupervised Method for Graded Lexical Entailment",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joseph Renner",
      "Pascal Denis",
      "Rémi Gilleron"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.616": {
    "title": "Knowledge Corpus Error in Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yejoon Lee",
      "Philhoon Oh",
      "James Thorne"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.617": {
    "title": "Epsilon Sampling Rocks: Investigating Sampling Strategies for Minimum Bayes Risk Decoding for Machine Translation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Markus Freitag",
      "Behrooz Ghorbani",
      "Patrick Fernandes"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.618": {
    "title": "The language of prompting: What linguistic properties make a prompt successful?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alina Leidinger",
      "Robert van Rooij",
      "Ekaterina Shutova"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.619": {
    "title": "When and Why Does Bias Mitigation Work?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhilasha Ravichander",
      "Joe Stacey",
      "Marek Rei"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.620": {
    "title": "Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihong Shao",
      "Yeyun Gong",
      "Yelong Shen",
      "Minlie Huang",
      "Nan Duan",
      "Weizhu Chen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.621": {
    "title": "Dynamic Low-rank Estimation for Transformer-based Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ting Hua",
      "Xiao Li",
      "Shangqian Gao",
      "Yen-Chang Hsu",
      "Yilin Shen",
      "Hongxia Jin"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.622": {
    "title": "Non-parallel Accent Transfer based on Fine-grained Controllable Accent Modelling",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linqin Wang",
      "Zhengtao Yu",
      "Yuanzhang Yang",
      "Shengxiang Gao",
      "Cunli Mao",
      "Yuxin Huang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.623": {
    "title": "Compositional Generalization for Data-to-Text Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinnuo Xu",
      "Ivan Titov",
      "Mirella Lapata"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.624": {
    "title": "In-Context Learning Creates Task Vectors",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roee Hendel",
      "Mor Geva",
      "Amir Globerson"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.625": {
    "title": "TalkUp: Paving the Way for Understanding Empowering Language",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucille Njoo",
      "Chan Park",
      "Octavia Stappart",
      "Marvin Thielk",
      "Yi Chu",
      "Yulia Tsvetkov"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.626": {
    "title": "Unifying Text, Tables, and Images for Multimodal Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haohao Luo",
      "Ying Shen",
      "Yang Deng"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.627": {
    "title": "Unsupervised Lexical Simplification with Context Augmentation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Takashi Wada",
      "Timothy Baldwin",
      "Jey Lau"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.628": {
    "title": "mLongT5: A Multilingual and Efficient Text-To-Text Transformer for Longer Sequences",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Uthus",
      "Santiago Ontanon",
      "Joshua Ainslie",
      "Mandy Guo"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.629": {
    "title": "Multilingual Lottery Tickets to Pretrain Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaeseong Lee",
      "Seung-won Hwang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.630": {
    "title": "Target-Aware Spatio-Temporal Reasoning via Answering Questions in Dynamic Audio-Visual Scenarios",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanyuan Jiang",
      "Jianqin Yin"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.631": {
    "title": "KG-GPT: A General Framework for Reasoning on Knowledge Graphs Using Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiho Kim",
      "Yeonsu Kwon",
      "Yohan Jo",
      "Edward Choi"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.632": {
    "title": "Breaking the Language Barrier: Improving Cross-Lingual Reasoning with Structured Self-Attention",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Negar Foroutan",
      "Mohammadreza Banaei",
      "Karl Aberer",
      "Antoine Bosselut"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.633": {
    "title": "CITB: A Benchmark for Continual Instruction Tuning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihan Zhang",
      "Meng Fang",
      "Ling Chen",
      "Mohammad-Reza Namazi-Rad"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.634": {
    "title": "Mixture-of-Linguistic-Experts Adapters for Improving and Interpreting Pre-trained Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raymond Li",
      "Gabriel Murray",
      "Giuseppe Carenini"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.635": {
    "title": "Towards Better Representations for Multi-Label Text Classification with Multi-granularity Information",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangfang Li",
      "Puzhen Su",
      "Junwen Duan",
      "Weidong Xiao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.636": {
    "title": "PCMID: Multi-Intent Detection through Supervised Prototypical Contrastive Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yurun Song",
      "Junchen Zhao",
      "Spencer Koehler",
      "Amir Abdullah",
      "Ian Harris"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.637": {
    "title": "Is GPT-4 a Good Data Analyst?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liying Cheng",
      "Xingxuan Li",
      "Lidong Bing"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.638": {
    "title": "DiffusionRet: Diffusion-Enhanced Generative Retriever using Constrained Decoding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shanbao Qiao",
      "Xuebing Liu",
      "Seung-Hoon Na"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.639": {
    "title": "Estimating Large Language Model Capabilities without Labeled Test Data",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Harvey Fu",
      "Qinyuan Ye",
      "Albert Xu",
      "Xiang Ren",
      "Robin Jia"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.640": {
    "title": "A Novel Contrastive Learning Method for Clickbait Detection on RoCliCo: A Romanian Clickbait Corpus of News Articles",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daria Broscoteanu",
      "Radu Ionescu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.641": {
    "title": "Large Language Models as Source Planner for Personalized Knowledge-grounded Dialogues",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongru Wang",
      "Minda Hu",
      "Yang Deng",
      "Rui Wang",
      "Fei Mi",
      "Weichao Wang",
      "Yasheng Wang",
      "Wai-Chung Kwan",
      "Irwin King",
      "Kam-Fai Wong"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.642": {
    "title": "Toxicity in Multilingual Machine Translation at Scale",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marta Costa-jussà",
      "Eric Smith",
      "Christophe Ropers",
      "Daniel Licht",
      "Jean Maillard",
      "Javier Ferrando",
      "Carlos Escolano"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.643": {
    "title": "Conversational Recommender System and Large Language Model Are Made for Each Other in E-commerce Pre-sales Dialogue",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanxing Liu",
      "Weinan Zhang",
      "Yifan Chen",
      "Yuchi Zhang",
      "Haopeng Bai",
      "Fan Feng",
      "Hengbin Cui",
      "Yongbin Li",
      "Wanxiang Che"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.644": {
    "title": "VIP5: Towards Multimodal Foundation Models for Recommendation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shijie Geng",
      "Juntao Tan",
      "Shuchang Liu",
      "Zuohui Fu",
      "Yongfeng Zhang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.645": {
    "title": "A Spectral Viewpoint on Continual Relation Extraction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huy Nguyen",
      "Chien Nguyen",
      "Linh Ngo",
      "Anh Luu",
      "Thien Nguyen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.646": {
    "title": "Learning to Follow Object-Centric Image Editing Instructions Faithfully",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tuhin Chakrabarty",
      "Kanishk Singh",
      "Arkadiy Saakyan",
      "Smaranda Muresan"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.647": {
    "title": "Zero-shot Topical Text Classification with LLMs - an Experimental Study",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shai Gretz",
      "Alon Halfon",
      "Ilya Shnayderman",
      "Orith Toledo-Ronen",
      "Artem Spector",
      "Lena Dankin",
      "Yannis Katsis",
      "Ofir Arviv",
      "Yoav Katz",
      "Noam Slonim",
      "Liat Ein-Dor"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.648": {
    "title": "Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixin Wan",
      "Jieyu Zhao",
      "Aman Chadha",
      "Nanyun Peng",
      "Kai-Wei Chang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.649": {
    "title": "A Black-Box Attack on Code Models via Representation Nearest Neighbor Search",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Zhang",
      "Wei Ma",
      "Qiang Hu",
      "Shangqing Liu",
      "Xiaofei Xie",
      "Yves Le Traon",
      "Yang Liu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.650": {
    "title": "How Well Do Text Embedding Models Understand Syntax?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Zhang",
      "Zhaopeng Feng",
      "Zhiyang Teng",
      "Zuozhu Liu",
      "Haizhou Li"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.651": {
    "title": "CASSI: Contextual and Semantic Structure-based Interpolation Augmentation for Low-Resource NER",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tanmay Surana",
      "Thi-Nga Ho",
      "Kyaw Tun",
      "Eng Siong Chng"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.652": {
    "title": "NEWTON: Are Large Language Models Capable of Physical Reasoning?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Wang",
      "Jiafei Duan",
      "Dieter Fox",
      "Siddhartha Srinivasa"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.653": {
    "title": "Beyond Denouncing Hate: Strategies for Countering Implied Biases and Stereotypes in Language",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jimin Mun",
      "Emily Allaway",
      "Akhila Yerukola",
      "Laura Vianna",
      "Sarah-Jane Leslie",
      "Maarten Sap"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.654": {
    "title": "On the Calibration of Large Language Models and Alignment",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chiwei Zhu",
      "Benfeng Xu",
      "Quan Wang",
      "Yongdong Zhang",
      "Zhendong Mao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.655": {
    "title": "TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for Inference Cost Reduction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyi Liu",
      "Liangzhi Li",
      "Tong Xiang",
      "Bowen Wang",
      "Yiming Qian"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.656": {
    "title": "Identifying Conspiracy Theories News based on Event Relation Graph",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanyuan Lei",
      "Ruihong Huang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.657": {
    "title": "Salespeople vs SalesBot: Exploring the Role of Educational Value in Conversational Recommender Systems",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lidiya Murakhovs’ka",
      "Philippe Laban",
      "Tian Xie",
      "Caiming Xiong",
      "Chien-Sheng Wu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.658": {
    "title": "Dynamic Open-book Prompt for Conversational Recommender System",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan Ma",
      "Tieyun Qian",
      "Ke Sun"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.659": {
    "title": "Auto-Instruct: Automatic Instruction Generation and Ranking for Black-Box Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihan Zhang",
      "Shuohang Wang",
      "Wenhao Yu",
      "Yichong Xu",
      "Dan Iter",
      "Qingkai Zeng",
      "Yang Liu",
      "Chenguang Zhu",
      "Meng Jiang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.660": {
    "title": "DiffuSeq-v2: Bridging Discrete and Continuous Text Spaces for Accelerated Seq2Seq Diffusion Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shansan Gong",
      "Mukai Li",
      "Jiangtao Feng",
      "Zhiyong Wu",
      "Lingpeng Kong"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.661": {
    "title": "M2C: Towards Automatic Multimodal Manga Complement",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongcheng Guo",
      "Boyang Wang",
      "Jiaqi Bai",
      "Jiaheng Liu",
      "Jian Yang",
      "Zhoujun Li"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.662": {
    "title": "Learn Your Tokens: Word-Pooled Tokenization for Language Modeling",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Avijit Thawani",
      "Saurabh Ghanekar",
      "Xiaoyuan Zhu",
      "Jay Pujara"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.663": {
    "title": "Towards Detecting Contextual Real-Time Toxicity for In-Game Chat",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zachary Yang",
      "Nicolas Grenon-Godbout",
      "Reihaneh Rabbany"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.664": {
    "title": "JWSign: A Highly Multilingual Corpus of Bible Translations for more Diversity in Sign Language Processing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shester Gueuwou",
      "Sophie Siake",
      "Colin Leong",
      "Mathias Müller"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.665": {
    "title": "Do Stochastic Parrots have Feelings Too? Improving Neural Detection of Synthetic Text via Emotion Recognition",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alan Cowap",
      "Yvette Graham",
      "Jennifer Foster"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.666": {
    "title": "Variator: Accelerating Pre-trained Models with Plug-and-Play Compression Modules",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaojun Xiao",
      "Yuqi Luo",
      "Wenbin Zhang",
      "Pengle Zhang",
      "Xu Han",
      "Yankai Lin",
      "Zhengyan Zhang",
      "Ruobing Xie",
      "Zhiyuan Liu",
      "Maosong Sun",
      "Jie Zhou"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.667": {
    "title": "PivotFEC: Enhancing Few-shot Factual Error Correction with a Pivot Task Approach using Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingwei He",
      "A-Long Jin",
      "Jun Ma",
      "Yuan Yuan",
      "Siu Yiu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.668": {
    "title": "Semantic Similarity Covariance Matrix Shrinkage",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guillaume Becquin",
      "Saher Esmeir"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.669": {
    "title": "LLM-in-the-loop: Leveraging Large Language Model for Thematic Analysis",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shih-Chieh Dai",
      "Aiping Xiong",
      "Lun-Wei Ku"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.670": {
    "title": "LLM aided semi-supervision for efficient Extractive Dialog Summarization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nishant Mishra",
      "Gaurav Sahu",
      "Iacer Calixto",
      "Ameen Abu-Hanna",
      "Issam Laradji"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.671": {
    "title": "Investigating Multilingual Coreference Resolution by Universal Annotations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haixia Chai",
      "Michael Strube"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.672": {
    "title": "FactSpotter: Evaluating the Factual Faithfulness of Graph-to-Text Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kun Zhang",
      "Oana Balalau",
      "Ioana Manolescu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.673": {
    "title": "LayoutDIT: Layout-Aware End-to-End Document Image Translation with Multi-Step Conductive Decoder",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyang Zhang",
      "Yaping Zhang",
      "Yupu Liang",
      "Lu Xiang",
      "Yang Zhao",
      "Yu Zhou",
      "Chengqing Zong"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.674": {
    "title": "Balaur: Language Model Pretraining with Lexical Semantic Relations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrei Mircea",
      "Jackie Cheung"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.675": {
    "title": "Exploring In-Context Learning for Knowledge Grounded Dialog Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qinyu Chen",
      "Wenhao Wu",
      "Sujian Li"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.676": {
    "title": "Towards Enhancing Relational Rules for Knowledge Graph Link Prediction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuhan Wu",
      "Huaiyu Wan",
      "Wei Chen",
      "Yuting Wu",
      "Junfeng Shen",
      "Youfang Lin"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.677": {
    "title": "Are NLP Models Good at Tracing Thoughts: An Overview of Narrative Understanding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lixing Zhu",
      "Runcong Zhao",
      "Lin Gui",
      "Yulan He"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.678": {
    "title": "Who is Speaking? Speaker-Aware Multiparty Dialogue Act Classification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ayesha Qamar",
      "Adarsh Pyarelal",
      "Ruihong Huang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.679": {
    "title": "Demystifying Prompts in Language Models via Perplexity Estimation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hila Gonen",
      "Srini Iyer",
      "Terra Blevins",
      "Noah Smith",
      "Luke Zettlemoyer"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.680": {
    "title": "C2D2 Dataset: A Resource for the Cognitive Distortion Analysis and Its Impact on Mental Health",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bichen Wang",
      "Pengfei Deng",
      "Yanyan Zhao",
      "Bing Qin"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.681": {
    "title": "MixEdit: Revisiting Data Augmentation and Beyond for Grammatical Error Correction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingheng Ye",
      "Yinghui Li",
      "Yangning Li",
      "Hai-Tao Zheng"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.682": {
    "title": "CCEval: A Representative Evaluation Benchmark for the Chinese-centric Multilingual Machine Translation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lianzhang Lou",
      "Xi Yin",
      "Yutao Xie",
      "Yang Xiang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.683": {
    "title": "ROME: Evaluating Pre-trained Vision-Language Models on Reasoning beyond Visual Common Sense",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kankan Zhou",
      "Eason Lai",
      "Wei Bin Au Yeong",
      "Kyriakos Mouratidis",
      "Jing Jiang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.684": {
    "title": "Automatic Analysis of Substantiation in Scientific Peer Reviews",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanzhu Guo",
      "Guokan Shang",
      "Virgile Rennard",
      "Michalis Vazirgiannis",
      "Chloé Clavel"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.685": {
    "title": "Hierarchical Prompting Assists Large Language Model on Web Navigation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Robert Lo",
      "Abishek Sridhar",
      "Frank Xu",
      "Hao Zhu",
      "Shuyan Zhou"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.686": {
    "title": "Can Large Language Models Fix Data Annotation Errors? An Empirical Study Using Debatepedia for Query-Focused Text Summarization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Tahmid Rahman Laskar",
      "Mizanur Rahman",
      "Israt Jahan",
      "Enamul Hoque",
      "Jimmy Huang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.687": {
    "title": "TSTR: Target Similarity Tuning Meets the Real World",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anirudh Khatry",
      "Sumit Gulwani",
      "Priyanshu Gupta",
      "Vu Le",
      "Mukul Singh",
      "Ananya Singha",
      "Gust Verbruggen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.688": {
    "title": "RealBehavior: A Framework for Faithfully Characterizing Foundation Models' Human-like Behavior Mechanisms",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Enyu Zhou",
      "Rui Zheng",
      "Zhiheng Xi",
      "Songyang Gao",
      "Xiaoran Fan",
      "Zichu Fei",
      "Jingting Ye",
      "Tao Gui",
      "Qi Zhang",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.689": {
    "title": "Unraveling Downstream Gender Bias from Large Language Models: A Study on AI Educational Writing Assistance",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thiemo Wambsganss",
      "Xiaotian Su",
      "Vinitra Swamy",
      "Seyed Neshaei",
      "Roman Rietsche",
      "Tanja Käser"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.690": {
    "title": "VERVE: Template-based ReflectiVE Rewriting for MotiVational IntErviewing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Do Min",
      "Veronica Perez-Rosas",
      "Ken Resnicow",
      "Rada Mihalcea"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.691": {
    "title": "Self-Knowledge Guided Retrieval Augmentation for Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yile Wang",
      "Peng Li",
      "Maosong Sun",
      "Yang Liu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.692": {
    "title": "Pretraining Language Models with Text-Attributed Heterogeneous Graphs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Zou",
      "Le Yu",
      "Yifei Huang",
      "Leilei Sun",
      "Bowen Du"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.693": {
    "title": "CReTIHC: Designing Causal Reasoning Tasks about Temporal Interventions and Hallucinated Confoundings",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changwoo Chun",
      "SongEun Lee",
      "Jaehyung Seo",
      "Heuiseok Lim"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.694": {
    "title": "On the Dimensionality of Sentence Embeddings",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongwei Wang",
      "Hongming Zhang",
      "Dong Yu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.695": {
    "title": "Pit One Against Many: Leveraging Attention-head Embeddings for Parameter-efficient Multi-head Attention",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huiyin Xue",
      "Nikolaos Aletras"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.696": {
    "title": "Entity-Based Evaluation of Political Bias in Automatic Summarization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karen Zhou",
      "Chenhao Tan"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.697": {
    "title": "StyleBART: Decorate Pretrained Model with Style Adapters for Unsupervised Stylistic Headline Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanqing Wang",
      "Yajing Luo",
      "Boya Xiong",
      "Guanhua Chen",
      "Yun Chen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.698": {
    "title": "RSVP: Customer Intent Detection via Agent Response Contrastive and Generative Pre-Training",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu-Chien Tang",
      "Wei-Yao Wang",
      "An-Zi Yen",
      "Wen-Chih Peng"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.699": {
    "title": "Improving Low-resource Question Answering by Augmenting Question Information",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andong Chen",
      "Yuan Sun",
      "Xiaobing Zhao",
      "Rosella Galindo Esparza",
      "Kehai Chen",
      "Yang Xiang",
      "Tiejun Zhao",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.700": {
    "title": "InstructSafety: A Unified Framework for Building Multidimensional and Explainable Safety Detector through Instruction Tuning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhexin Zhang",
      "Jiale Cheng",
      "Hao Sun",
      "Jiawen Deng",
      "Minlie Huang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.701": {
    "title": "A Tale of Two Movements': Identifying and Comparing Perspectives in #BlackLivesMatter and #BlueLivesMatter Movements-related Tweets using Weakly Supervised Graph-based Structured Prediction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shamik Roy",
      "Dan Goldwasser"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.702": {
    "title": "ClusterPrompt: Cluster Semantic Enhanced Prompt Learning for New Intent Discovery",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinggui Liang",
      "Lizi Liao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.703": {
    "title": "Investigating the Effect of Pre-finetuning BERT Models on NLI Involving Presuppositions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jad Kabbara",
      "Jackie Cheung"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.704": {
    "title": "MRRL: Modifying the Reference via Reinforcement Learning for Non-Autoregressive Joint Multiple Intent Detection and Slot Filling",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuxin Cheng",
      "Zhihong Zhu",
      "Bowen Cao",
      "Qichen Ye",
      "Yuexian Zou"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.705": {
    "title": "DemoNSF: A Multi-task Demonstration-based Generative Framework for Noisy Slot Filling Task",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanting Dong",
      "Tingfeng Hui",
      "Zhuoma GongQue",
      "Jinxu Zhao",
      "Daichi Guo",
      "Gang Zhao",
      "Keqing He",
      "Weiran Xu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.706": {
    "title": "SHARCS: Efficient Transformers Through Routing with Dynamic Width Sub-networks",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammadreza Salehi",
      "Sachin Mehta",
      "Aditya Kusupati",
      "Ali Farhadi",
      "Hannaneh Hajishirzi"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.707": {
    "title": "Always the Best Fit: Adaptive Domain Gap Filling from Causal Perspective for Few-Shot Relation Extraction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ge Bai",
      "Chenji Lu",
      "Jiaxiang Geng",
      "Shilong Li",
      "Yidong Shi",
      "Xiyan Liu",
      "Ying Liu",
      "Zhang Zhang",
      "Ruifang Liu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.708": {
    "title": "MEGClass: Extremely Weakly Supervised Text Classification via Mutually-Enhancing Text Granularities",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Priyanka Kargupta",
      "Tanay Komarlu",
      "Susik Yoon",
      "Xuan Wang",
      "Jiawei Han"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.709": {
    "title": "Causal Inference from Text: Unveiling Interactions between Variables",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxiang Zhou",
      "Yulan He"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.710": {
    "title": "Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yubo Ma",
      "Yixin Cao",
      "Yong Hong",
      "Aixin Sun"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.711": {
    "title": "Prompting and Evaluating Large Language Models for Proactive Dialogues: Clarification, Target-guided, and Non-collaboration",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Deng",
      "Lizi Liao",
      "Liang Chen",
      "Hongru Wang",
      "Wenqiang Lei",
      "Tat-Seng Chua"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.712": {
    "title": "Ecologically Valid Explanations for Label Variation in NLI",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nan-Jiang Jiang",
      "Chenhao Tan",
      "Marie-Catherine de Marneffe"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.713": {
    "title": "A Benchmark for Semi-Inductive Link Prediction in Knowledge Graphs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adrian Kochsiek",
      "Rainer Gemulla"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.714": {
    "title": "SummIt: Iterative Text Summarization via ChatGPT",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haopeng Zhang",
      "Xiao Liu",
      "Jiawei Zhang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.715": {
    "title": "Orthogonal Subspace Learning for Language Model Continual Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Wang",
      "Tianze Chen",
      "Qiming Ge",
      "Han Xia",
      "Rong Bao",
      "Rui Zheng",
      "Qi Zhang",
      "Tao Gui",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.716": {
    "title": "Attention-Enhancing Backdoor Attacks Against BERT-based Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weimin Lyu",
      "Songzhu Zheng",
      "Lu Pang",
      "Haibin Ling",
      "Chao Chen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.717": {
    "title": "Hi-ToM: A Benchmark for Evaluating Higher-Order Theory of Mind Reasoning in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufan Wu",
      "Yinghui He",
      "Yilin Jia",
      "Rada Mihalcea",
      "Yulong Chen",
      "Naihao Deng"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.718": {
    "title": "Image and Text: Fighting the same Battle? Super Resolution Learning for Imbalanced Text Classification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Romain Meunier",
      "Benamara Farah",
      "Véronique Moriceau",
      "Patricia Stolf"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.719": {
    "title": "SELFOOD: Self-Supervised Out-Of-Distribution Detection via Learning to Rank",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dheeraj Mekala",
      "Adithya Samavedhi",
      "Chengyu Dong",
      "Jingbo Shang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.720": {
    "title": "Mind the Gap Between Conversations for Improved Long-Term Dialogue Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiang Zhang",
      "Jason Naradowsky",
      "Yusuke Miyao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.721": {
    "title": "A Structure-Aware Generative Adversarial Network for Bilingual Lexicon Induction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bocheng Han",
      "Qian Tao",
      "Lusi Li",
      "Zhihao Xiong"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.722": {
    "title": "NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oscar Sainz",
      "Jon Campos",
      "Iker García-Ferrero",
      "Julen Etxaniz",
      "Oier Lopez de Lacalle",
      "Eneko Agirre"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.723": {
    "title": "Improving Pacing in Long-Form Story Planning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yichen Wang",
      "Kevin Yang",
      "Xiaoming Liu",
      "Dan Klein"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.724": {
    "title": "Argument mining as a multi-hop generative machine reading comprehension task",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boyang Liu",
      "Viktor Schlegel",
      "Riza Batista-Navarro",
      "Sophia Ananiadou"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.725": {
    "title": "HuatuoGPT, Towards Taming Language Model to Be a Doctor",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongbo Zhang",
      "Junying Chen",
      "Feng Jiang",
      "Fei Yu",
      "Zhihong Chen",
      "Guiming Chen",
      "Jianquan Li",
      "Xiangbo Wu",
      "Zhang Zhiyi",
      "Qingying Xiao",
      "Xiang Wan",
      "Benyou Wang",
      "Haizhou Li"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.726": {
    "title": "Debias NLU Datasets via Training-free Perturbations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Guo",
      "Yuanhang Tang",
      "Yawen Ouyang",
      "Zhen Wu",
      "Xinyu Dai"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.727": {
    "title": "Aspect-to-Scope Oriented Multi-view Contrastive Learning for Aspect-based Sentiment Analysis",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heyan Chai",
      "Ziyi Yao",
      "Siyu Tang",
      "Ye Wang",
      "Liqiang Nie",
      "Binxing Fang",
      "Qing Liao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.728": {
    "title": "Robustness of Named-Entity Replacements for In-Context Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saeed Goodarzi",
      "Nikhil Kagita",
      "Dennis Minn",
      "Shufan Wang",
      "Roberto Dessi",
      "Shubham Toshniwal",
      "Adina Williams",
      "Jack Lanchantin",
      "Koustuv Sinha"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.729": {
    "title": "Contrastive Learning-based Sentence Encoders Implicitly Weight Informative Words",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hiroto Kurita",
      "Goro Kobayashi",
      "Sho Yokoi",
      "Kentaro Inui"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.730": {
    "title": "Legally Enforceable Hate Speech Detection for Public Forums",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chu Luo",
      "Rohan Bhambhoria",
      "Samuel Dahan",
      "Xiaodan Zhu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.731": {
    "title": "ConPrompt: Pre-training a Language Model with Machine-Generated Data for Implicit Hate Speech Detection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youngwook Kim",
      "Shinwoo Park",
      "Youngsoo Namgoong",
      "Yo-Sub Han"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.732": {
    "title": "Incorporating Syntactic Knowledge into Pre-trained Language Model using Optimization for Overcoming Catastrophic Forgetting",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ran Iwamoto",
      "Issei Yoshida",
      "Hiroshi Kanayama",
      "Takuya Ohko",
      "Masayasu Muraoka"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.733": {
    "title": "Toward Human Readable Prompt Tuning: Kubrick's The Shining is a good movie, and a good prompt too?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weijia Shi",
      "Xiaochuang Han",
      "Hila Gonen",
      "Ari Holtzman",
      "Yulia Tsvetkov",
      "Luke Zettlemoyer"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.734": {
    "title": "Chain-of-Thought Reasoning in Tabular Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingyu Zheng",
      "Hao Yang",
      "Wenbin Jiang",
      "Zheng Lin",
      "Yajuan Lyu",
      "Qiaoqiao She",
      "Weiping Wang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.735": {
    "title": "Diffusion Language Model with Query-Document Relevance for Query-Focused Summarization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaoyao Huang",
      "Luozheng Qin",
      "Ziqiang Cao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.736": {
    "title": "Grounded and well-rounded: a methodological approach to the study of cross-modal and cross-lingual grounding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Timothee Mickus",
      "Elaine Zosa",
      "Denis Paperno"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.737": {
    "title": "EMO-KNOW: A Large Scale Dataset on Emotion-Cause",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mia Nguyen",
      "Yasith Samaradivakara",
      "Prasanth Sasikumar",
      "Chitralekha Gupta",
      "Suranga Nanayakkara"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.738": {
    "title": "Boosting Inference Efficiency: Unleashing the Power of Parameter-Shared Pre-trained Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weize Chen",
      "Xiaoyue Xu",
      "Xu Han",
      "Yankai Lin",
      "Ruobing Xie",
      "Zhiyuan Liu",
      "Maosong Sun",
      "Jie Zhou"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.739": {
    "title": "Natural Response Generation for Chinese Reading Comprehension",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nuo Chen",
      "Hongguang Li",
      "Yinan Bao",
      "Baoyuan Wang",
      "Jia Li"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.740": {
    "title": "Treepiece: Faster Semantic Parsing via Tree Tokenization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sid Wang",
      "Akshat Shrivastava",
      "Aleksandr Livshits"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.741": {
    "title": "Semantic Parsing by Large Language Models for Intricate Updating Strategies of Zero-Shot Dialogue State Tracking",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxiang Wu",
      "Guanting Dong",
      "Weiran Xu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.742": {
    "title": "Mitigating Framing Bias with Polarity Minimization Loss",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yejin Bang",
      "Nayeon Lee",
      "Pascale Fung"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.743": {
    "title": "Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinglong Gao",
      "Xiao Ding",
      "Bing Qin",
      "Ting Liu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.744": {
    "title": "Steering Large Language Models for Machine Translation with Finetuning and In-Context Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Duarte Alves",
      "Nuno Guerreiro",
      "João Alves",
      "José Pombal",
      "Ricardo Rei",
      "José de Souza",
      "Pierre Colombo",
      "Andre Martins"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.745": {
    "title": "How Many Demonstrations Do You Need for In-context Learning?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiuhai Chen",
      "Lichang Chen",
      "Chen Zhu",
      "Tianyi Zhou"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.746": {
    "title": "Improving word mover's distance by leveraging self-attention matrix",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hiroaki Yamagiwa",
      "Sho Yokoi",
      "Hidetoshi Shimodaira"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.747": {
    "title": "Improving Span Representation by Efficient Span-Level Attention",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengyu Ji",
      "Songlin Yang",
      "Kewei Tu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.748": {
    "title": "Long-Horizon Dialogue Understanding for Role Identification in the Game of Avalon with Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simon Stepputtis",
      "Joseph Campbell",
      "Yaqi Xie",
      "Zhengyang Qi",
      "Wenxin Zhang",
      "Ruiyi Wang",
      "Sanketh Rangreji",
      "Charles Lewis",
      "Katia Sycara"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.749": {
    "title": "Improving Sequential Model Editing with Fact Retrieval",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoqi Han",
      "Ru Li",
      "Hongye Tan",
      "Wang Yuanlong",
      "Qinghua Chai",
      "Jeff Pan"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.750": {
    "title": "Battle of the Large Language Models: Dolly vs LLaMA vs Vicuna vs Guanaco vs Bard vs ChatGPT - A Text-to-SQL Parsing Comparison",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuo Sun",
      "Yuchen Zhang",
      "Jiahuan Yan",
      "Yuze Gao",
      "Donovan Ong",
      "Bin Chen",
      "Jian Su"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.751": {
    "title": "KBioXLM: A Knowledge-anchored Biomedical Multilingual Pretrained Language Model",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Geng",
      "Xu Yan",
      "Ziqiang Cao",
      "Juntao Li",
      "Wenjie Li",
      "Sujian Li",
      "Xinjie Zhou",
      "Yang Yang",
      "Jun Zhang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.752": {
    "title": "Words, Subwords, and Morphemes: What Really Matters in the Surprisal-Reading Time Relationship?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sathvik Nair",
      "Philip Resnik"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.753": {
    "title": "A Zero-Shot Language Agent for Computer Control with Structured Reflection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Li",
      "Gang Li",
      "Zhiwei Deng",
      "Bryan Wang",
      "Yang Li"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.754": {
    "title": "SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to RLHF",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Dong",
      "Zhilin Wang",
      "Makesh Sreedhar",
      "Xianchao Wu",
      "Oleksii Kuchaiev"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.755": {
    "title": "IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoxuan You",
      "Rui Sun",
      "Zhecan Wang",
      "Long Chen",
      "Gengyu Wang",
      "Hammad Ayyubi",
      "Kai-Wei Chang",
      "Shih-Fu Chang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.756": {
    "title": "GRI: Graph-based Relative Isomorphism of Word Embedding Spaces",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Ali",
      "Yan Hu",
      "Jianbin Qin",
      "Di Wang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.757": {
    "title": "PersonaLM: Language Model Personalization via Domain-distributed Span Aggregated K-Nearest N-gram Retrieval Augmentation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Puneet Mathur",
      "Zhe Liu",
      "Ke Li",
      "Yingyi Ma",
      "Gil Keren",
      "Zeeshan Ahmed",
      "Dinesh Manocha",
      "Xuedong Zhang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.758": {
    "title": "Scaling Vision-Language Models with Sparse Mixture of Experts",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng Shen",
      "Zhewei Yao",
      "Chunyuan Li",
      "Trevor Darrell",
      "Kurt Keutzer",
      "Yuxiong He"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.759": {
    "title": "Aspect-Category Enhanced Learning with a Neural Coherence Model for Implicit Sentiment Analysis",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jin Cui",
      "Fumiyo Fukumoto",
      "Xinfeng Wang",
      "Yoshimi Suzuki",
      "Jiyi Li",
      "Wanzeng Kong"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.760": {
    "title": "End-to-end Adversarial Sample Generation for Data Augmentation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyuan Liu",
      "Yuqing Sun"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.761": {
    "title": "Query2Triple: Unified Query Encoding for Answering Diverse Complex Queries over Knowledge Graphs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yao Xu",
      "Shizhu He",
      "Cunguang Wang",
      "Li Cai",
      "Kang Liu",
      "Jun Zhao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.762": {
    "title": "Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiheng Xi",
      "Senjie Jin",
      "Yuhao Zhou",
      "Rui Zheng",
      "Songyang Gao",
      "Jia Liu",
      "Tao Gui",
      "Qi Zhang",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.763": {
    "title": "Breaking through Deterministic Barriers: Randomized Pruning Mask Generation and Selection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianwei Li",
      "Weizhi Gao",
      "Qi Lei",
      "Dongkuan Xu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.764": {
    "title": "Eyes Show the Way: Modelling Gaze Behaviour for Hallucination Detection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kishan Maharaj",
      "Ashita Saxena",
      "Raja Kumar",
      "Abhijit Mishra",
      "Pushpak Bhattacharyya"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.765": {
    "title": "Noisy Pair Corrector for Dense Retrieval",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hang Zhang",
      "Yeyun Gong",
      "Xingwei He",
      "Dayiheng Liu",
      "Daya Guo",
      "Jiancheng Lv",
      "Jian Guo"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.766": {
    "title": "Enhancing Accessible Communication: from European Portuguese to Portuguese Sign Language",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Catarina Sousa",
      "Luisa Coheur",
      "Mara Moita"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.767": {
    "title": "Diversifying language models for lesser-studied languages and language-usage contexts: A case of second language Korean",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hakyung Sung",
      "Gyu-Ho Shin"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.768": {
    "title": "Improving generalization in large langue model by learning prefix subspaces",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Louis Falissard",
      "Vincent Guigue",
      "Laure Soulier"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.769": {
    "title": "Domain Adaptation for Sentiment Analysis Using Robust Internal Representations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Rostami",
      "Digbalay Bose",
      "Shrikanth Narayanan",
      "Aram Galstyan"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.770": {
    "title": "KeFVP: Knowledge-enhanced Financial Volatility Prediction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Niu",
      "Yun Xiong",
      "Xiaosu Wang",
      "Wenjing Yu",
      "Yao Zhang",
      "Weizu Yang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.771": {
    "title": "A Frustratingly Easy Plug-and-Play Detection-and-Reasoning Module for Chinese Spelling Check",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haojing Huang",
      "Jingheng Ye",
      "Qingyu Zhou",
      "Yinghui Li",
      "Yangning Li",
      "Feng Zhou",
      "Hai-Tao Zheng"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.772": {
    "title": "Asking Clarification Questions to Handle Ambiguity in Open-Domain QA",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongryeol Lee",
      "Segwang Kim",
      "Minwoo Lee",
      "Hwanhee Lee",
      "Joonsuk Park",
      "Sang-Woo Lee",
      "Kyomin Jung"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.773": {
    "title": "Addressing the Length Bias Challenge in Document-Level Neural Machine Translation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhang Zhuocheng",
      "Shuhao Gu",
      "Min Zhang",
      "Yang Feng"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.774": {
    "title": "EconBERTa: Towards Robust Extraction of Named Entities in Economics",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karim Lasri",
      "Pedro Vitor Quinta de Castro",
      "Mona Schirmer",
      "Luis Eduardo San Martin",
      "Linxi Wang",
      "Tomáš Dulka",
      "Haaya Naushan",
      "John Pougué-Biyong",
      "Arianna Legovini",
      "Samuel Fraiberger"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.775": {
    "title": "Consonant is all you need: a compact representation of English text for efficient NLP",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maged Al-shaibani",
      "Irfan Ahmad"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.776": {
    "title": "Detrimental Contexts in Open-Domain Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Philhoon Oh",
      "James Thorne"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.777": {
    "title": "PMIndiaSum: Multilingual and Cross-lingual Headline Summarization for Languages in India",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashok Urlana",
      "Pinzhen Chen",
      "Zheng Zhao",
      "Shay Cohen",
      "Manish Shrivastava",
      "Barry Haddow"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.778": {
    "title": "Beyond Labels: Empowering Human Annotators with Natural Language Explanations through a Novel Active-Learning Architecture",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bingsheng Yao",
      "Ishan Jindal",
      "Lucian Popa",
      "Yannis Katsis",
      "Sayan Ghosh",
      "Lihong He",
      "Yuxuan Lu",
      "Shashank Srivastava",
      "Yunyao Li",
      "James Hendler",
      "Dakuo Wang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.779": {
    "title": "Decoding Stumpers: Large Language Models vs. Human Problem-Solvers",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alon Goldstein",
      "Miriam Havin",
      "Roi Reichart",
      "Ariel Goldstein"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.780": {
    "title": "Efficient Cross-Task Prompt Tuning for Few-Shot Conversational Emotion Recognition",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yige Xu",
      "Zhiwei Zeng",
      "Zhiqi Shen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.781": {
    "title": "SYMPTOMIFY: Transforming Symptom Annotations with Language Model Knowledge Harvesting",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bosung Kim",
      "Ndapa Nakashole"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.782": {
    "title": "TokenDrop + BucketSampler: Towards Efficient Padding-free Fine-tuning of Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amrit Nagarajan",
      "Anand Raghunathan"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.783": {
    "title": "Unified Representation for Non-compositional and Compositional Expressions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziheng Zeng",
      "Suma Bhat"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.784": {
    "title": "Context Quality Matters in Training Fusion-in-Decoder for Extractive Open-Domain Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kosuke Akimoto",
      "Kunihiro Takeoka",
      "Masafumi Oyamada"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.785": {
    "title": "Error Detection for Text-to-SQL Semantic Parsing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shijie Chen",
      "Ziru Chen",
      "Huan Sun",
      "Yu Su"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.786": {
    "title": "Ultra-Fine Entity Typing with Prior Knowledge about Labels: A Simple Clustering Based Strategy",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Na Li",
      "Zied Bouraoui",
      "Steven Schockaert"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.787": {
    "title": "Multilingual Coarse Political Stance Classification of Media. The Editorial Line of a ChatGPT and Bard Newspaper",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cristina España-Bonet"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.788": {
    "title": "Do \"English\" Named Entity Recognizers Work Well on Global Englishes?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Shan",
      "John Bauer",
      "Riley Carlson",
      "Christopher Manning"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.789": {
    "title": "Affective and Dynamic Beam Search for Story Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tenghao Huang",
      "Ehsan Qasemi",
      "Bangzheng Li",
      "He Wang",
      "Faeze Brahman",
      "Muhao Chen",
      "Snigdha Chaturvedi"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.790": {
    "title": "Multiview Clickbait Detection via Jointly Modeling Subjective and Objective Preference",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chongyang Shi",
      "Yijun Yin",
      "Qi Zhang",
      "Liang Xiao",
      "Usman Naseem",
      "Shoujin Wang",
      "Liang Hu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.791": {
    "title": "Let's Synthesize Step by Step: Iterative Dataset Synthesis with Large Language Models by Extrapolating Errors from Small Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruida Wang",
      "Wangchunshu Zhou",
      "Mrinmaya Sachan"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.792": {
    "title": "Identifying Early Maladaptive Schemas from Mental Health Question Texts",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sujatha Gollapalli",
      "Beng Ang",
      "See-Kiong Ng"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.793": {
    "title": "Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuolin Yang",
      "Wei Ping",
      "Zihan Liu",
      "Vijay Korthikanti",
      "Weili Nie",
      "De-An Huang",
      "Linxi Fan",
      "Zhiding Yu",
      "Shiyi Lan",
      "Bo Li",
      "Mohammad Shoeybi",
      "Ming-Yu Liu",
      "Yuke Zhu",
      "Bryan Catanzaro",
      "Chaowei Xiao",
      "Anima Anandkumar"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.794": {
    "title": "Syntax Matters: Towards Spoken Language Understanding via Syntax-Aware Attention",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifeng Xie",
      "Zhihong Zhu",
      "Xuxin Cheng",
      "Zhiqi Huang",
      "Dongsheng Chen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.795": {
    "title": "Can ChatGPT Defend its Belief in Truth? Evaluating LLM Reasoning via Debate",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boshi Wang",
      "Xiang Yue",
      "Huan Sun"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.796": {
    "title": "Using In-Context Learning to Improve Dialogue Safety",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicholas Meade",
      "Spandana Gella",
      "Devamanyu Hazarika",
      "Prakhar Gupta",
      "Di Jin",
      "Siva Reddy",
      "Yang Liu",
      "Dilek Hakkani-Tur"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.797": {
    "title": "HEAR: Hearing Enhanced Audio Response for Video-grounded Dialogue",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sunjae Yoon",
      "Dahyun Kim",
      "Eunseop Yoon",
      "Hee Yoon",
      "Junyeong Kim",
      "Chang Yoo"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.798": {
    "title": "Improving Consistency for Text Summarization with Energy Functions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Zeng",
      "Qingyu Yin",
      "Zheng Li",
      "Yifan Gao",
      "Sreyashi Nag",
      "Zhengyang Wang",
      "Bing Yin",
      "Heng Ji",
      "Chao Zhang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.799": {
    "title": "Defining a New NLP Playground",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sha Li",
      "Chi Han",
      "Pengfei Yu",
      "Carl Edwards",
      "Manling Li",
      "Xingyao Wang",
      "Yi Fung",
      "Charles Yu",
      "Joel Tetreault",
      "Eduard Hovy",
      "Heng Ji"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.800": {
    "title": "UPTON: Preventing Authorship Leakage from Public Text Release via Data Poisoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyao Wang",
      "Thai Le",
      "Dongwon Lee"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.801": {
    "title": "IAEval: A Comprehensive Evaluation of Instance Attribution on Natural Language Understanding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peijian Gu",
      "Yaozong Shen",
      "Lijie Wang",
      "Quan Wang",
      "Hua Wu",
      "Zhendong Mao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.802": {
    "title": "Scene Graph Enhanced Pseudo-Labeling for Referring Expression Comprehension",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cantao Wu",
      "Yi Cai",
      "Liuwu Li",
      "Jiexin Wang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.803": {
    "title": "Noisy Self-Training with Synthetic Queries for Dense Retrieval",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Jiang",
      "Tom Drummond",
      "Trevor Cohn"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.804": {
    "title": "Leveraging GPT-4 for Automatic Translation Post-Editing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vikas Raunak",
      "Amr Sharaf",
      "Yiren Wang",
      "Hany Awadalla",
      "Arul Menezes"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.805": {
    "title": "Uniform Complexity for Text Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joseph Imperial",
      "Harish Madabushi"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.806": {
    "title": "Cue-CoT: Chain-of-thought Prompting for Responding to In-depth Dialogue Questions with LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongru Wang",
      "Rui Wang",
      "Fei Mi",
      "Yang Deng",
      "Zezhong Wang",
      "Bin Liang",
      "Ruifeng Xu",
      "Kam-Fai Wong"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.807": {
    "title": "CONTRASTE: Supervised Contrastive Pre-training With Aspect-based Prompts For Aspect Sentiment Triplet Extraction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rajdeep Mukherjee",
      "Nithish Kannen",
      "Saurabh Pandey",
      "Pawan Goyal"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.808": {
    "title": "Towards Anytime Fine-tuning: Continually Pre-trained Language Models with Hypernetwork Prompts",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gangwei Jiang",
      "Caigao Jiang",
      "Siqiao Xue",
      "James Zhang",
      "Jun Zhou",
      "Defu Lian",
      "Ying Wei"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.809": {
    "title": "Language Guided Visual Question Answering: Elevate Your Multimodal Language Model Using Knowledge-Enriched Prompts",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Deepanway Ghosal",
      "Navonil Majumder",
      "Roy Lee",
      "Rada Mihalcea",
      "Soujanya Poria"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.810": {
    "title": "XLS-R fine-tuning on noisy word boundaries for unsupervised speech segmentation into words",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Robin Algayres",
      "Pablo Diego-Simon",
      "Benoît Sagot",
      "Emmanuel Dupoux"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.811": {
    "title": "Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kashun Shum",
      "Shizhe Diao",
      "Tong Zhang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.812": {
    "title": "What Makes it Ok to Set a Fire? Iterative Self-distillation of Contexts and Rationales for Disambiguating Defeasible Social and Moral Situations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kavel Rao",
      "Liwei Jiang",
      "Valentina Pyatkin",
      "Yuling Gu",
      "Niket Tandon",
      "Nouha Dziri",
      "Faeze Brahman",
      "Yejin Choi"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.813": {
    "title": "An Empirical Study on Multiple Knowledge from ChatGPT for Emotion Recognition in Conversations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Geng Tu",
      "Bin Liang",
      "Bing Qin",
      "Kam-Fai Wong",
      "Ruifeng Xu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.814": {
    "title": "Exploiting Contrastive Learning and Numerical Evidence for Confusing Legal Judgment Prediction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leilei Gan",
      "Baokui Li",
      "Kun Kuang",
      "Yating Zhang",
      "Lei Wang",
      "Anh Luu",
      "Yi Yang",
      "Fei Wu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.815": {
    "title": "One For All & All For One: Bypassing Hyperparameter Tuning with Model Averaging for Cross-Lingual Transfer",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fabian Schmidt",
      "Ivan Vulić",
      "Goran Glavaš"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.816": {
    "title": "Dimensions of Online Conflict: Towards Modeling Agonism",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matt Canute",
      "Mali Jin",
      "Hannah Holtzclaw",
      "Alberto Lusoli",
      "Philippa Adams",
      "Mugdha Pandya",
      "Maite Taboada",
      "Diana Maynard",
      "Wendy Hui Kyong Chun"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.817": {
    "title": "Learning under Label Proportions for Text Classification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jatin Chauhan",
      "Xiaoxuan Wang",
      "Wei Wang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.818": {
    "title": "MetaReVision: Meta-Learning with Retrieval for Visually Grounded Compositional Concept Acquisition",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangyue Xu",
      "Parisa Kordjamshidi",
      "Joyce Chai"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.819": {
    "title": "PR-MCS: Perturbation Robust Metric for MultiLingual Image Captioning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongil Kim",
      "Yerin Hwang",
      "Hyeongu Yun",
      "Seunghyun Yoon",
      "Trung Bui",
      "Kyomin Jung"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.820": {
    "title": "Pre-training Multi-task Contrastive Learning Models for Scientific Literature Understanding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Zhang",
      "Hao Cheng",
      "Zhihong Shen",
      "Xiaodong Liu",
      "Ye-Yi Wang",
      "Jianfeng Gao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.821": {
    "title": "BLM-s/lE: A structured dataset of English spray-load verb alternations for testing generalization in LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giuseppe Samo",
      "Vivi Nastase",
      "Chunyang Jiang",
      "Paola Merlo"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.822": {
    "title": "Efficiently Enhancing Zero-Shot Performance of Instruction Following Model via Retrieval of Soft Prompt",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seonghyeon Ye",
      "Joel Jang",
      "Doyoung Kim",
      "Yongrae Jo",
      "Minjoon Seo"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.823": {
    "title": "Geographical Erasure in Language Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pola Schwöbel",
      "Jacek Golebiowski",
      "Michele Donini",
      "Cedric Archambeau",
      "Danish Pruthi"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.824": {
    "title": "Can Foundation Models Watch, Talk and Guide You Step by Step to Make a Cake?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuwei Bao",
      "Keunwoo Yu",
      "Yichi Zhang",
      "Shane Storks",
      "Itamar Bar-Yossef",
      "Alex de la Iglesia",
      "Megan Su",
      "Xiao Zheng",
      "Joyce Chai"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.825": {
    "title": "Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Tay",
      "Mostafa Dehghani",
      "Samira Abnar",
      "Hyung Chung",
      "William Fedus",
      "Jinfeng Rao",
      "Sharan Narang",
      "Vinh Tran",
      "Dani Yogatama",
      "Donald Metzler"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.826": {
    "title": "Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyang Huang",
      "Tianyi Tang",
      "Dongdong Zhang",
      "Xin Zhao",
      "Ting Song",
      "Yan Xia",
      "Furu Wei"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.827": {
    "title": "DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinyan Su",
      "Terry Zhuo",
      "Di Wang",
      "Preslav Nakov"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.828": {
    "title": "From Complex to Simple: Unraveling the Cognitive Tree for Reasoning with Small Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Junbing",
      "Chengyu Wang",
      "Taolin Zhang",
      "Xiaofeng He",
      "Jun Huang",
      "Wei Zhang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.829": {
    "title": "Macedon: Minimizing Representation Coding Rate Reduction for Cross-Lingual Natural Language Understanding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Wang",
      "Yaqing Wang",
      "Huaxiu Yao",
      "Jing Gao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.830": {
    "title": "Adversarial Robustness for Large Language NER models using Disentanglement and Word Attributions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaomeng Jin",
      "Bhanukiran Vinzamuri",
      "Sriram Venkatapathy",
      "Heng Ji",
      "Pradeep Natarajan"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.831": {
    "title": "LLMs – the Good, the Bad or the Indispensable?: A Use Case on Legal Statute Prediction and Legal Judgment Prediction on Indian Court Cases",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaurya Vats",
      "Atharva Zope",
      "Somsubhra De",
      "Anurag Sharma",
      "Upal Bhattacharya",
      "Shubham Nigam",
      "Shouvik Guha",
      "Koustav Rudra",
      "Kripabandhu Ghosh"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.832": {
    "title": "You Are What You Annotate: Towards Better Models through Annotator Representations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Naihao Deng",
      "Xinliang Zhang",
      "Siyang Liu",
      "Winston Wu",
      "Lu Wang",
      "Rada Mihalcea"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.833": {
    "title": "Large Language Models Are Better Adversaries: Exploring Generative Clean-Label Backdoor Attacks Against Text Classifiers",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wencong You",
      "Zayd Hammoudeh",
      "Daniel Lowd"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.834": {
    "title": "Noise-Robust Fine-Tuning of Pretrained Language Models via External Guidance",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Song Wang",
      "Zhen Tan",
      "Ruocheng Guo",
      "Jundong Li"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.835": {
    "title": "Probabilistic Tree-of-thought Reasoning for Answering Knowledge-intensive Complex Questions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shulin Cao",
      "Jiajie Zhang",
      "Jiaxin Shi",
      "Xin Lv",
      "Zijun Yao",
      "Qi Tian",
      "Lei Hou",
      "Juanzi Li"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.836": {
    "title": "Ensemble-Instruct: Instruction Tuning Data Generation with a Heterogeneous Mixture of LMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Young-Suk Lee",
      "Md Sultan",
      "Yousef El-Kurdi",
      "Tahira Naseem",
      "Asim Munawar",
      "Radu Florian",
      "Salim Roukos",
      "Ramón Astudillo"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.837": {
    "title": "The Less the Merrier? Investigating Language Representation in Multilingual Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hellina Nigatu",
      "Atnafu Tonja",
      "Jugal Kalita"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.838": {
    "title": "SuperTweetEval: A Challenging, Unified and Heterogeneous Benchmark for Social Media NLP Research",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dimosthenis Antypas",
      "Asahi Ushio",
      "Francesco Barbieri",
      "Leonardo Neves",
      "Kiamehr Rezaee",
      "Luis Espinosa-Anke",
      "Jiaxin Pei",
      "Jose Camacho-Collados"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.839": {
    "title": "Enabling Unsupervised Neural Machine Translation with Word-level Visual Representations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengpeng Fu",
      "Xiaocheng Feng",
      "Yichong Huang",
      "Wenshuai Huo",
      "Hui Wang",
      "Bing Qin",
      "Ting Liu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.840": {
    "title": "Pragmatics in Language Grounding: Phenomena, Tasks, and Modeling Approaches",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Fried",
      "Nicholas Tomlin",
      "Jennifer Hu",
      "Roma Patel",
      "Aida Nematzadeh"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.841": {
    "title": "MISCA: A Joint Model for Multiple Intent Detection and Slot Filling with Intent-Slot Co-Attention",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thinh Pham",
      "Chi Tran",
      "Dat Quoc Nguyen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.842": {
    "title": "Enhancing Emotion Recognition in Conversation via Multi-view Feature Alignment and Memorization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guiyang Hou",
      "Yongliang Shen",
      "Wenqi Zhang",
      "Wei Xue",
      "Weiming Lu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.843": {
    "title": "Mandarin classifier systems optimize to accommodate communicative pressures",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yamei Wang",
      "Géraldine Walther"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.844": {
    "title": "Probing Representations for Document-level Event Extraction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Barry Wang",
      "Xinya Du",
      "Claire Cardie"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.845": {
    "title": "Cultural Compass: Predicting Transfer Learning Success in Offensive Language Detection with Cultural Features",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li Zhou",
      "Antonia Karamolegkou",
      "Wenyu Chen",
      "Daniel Hershcovich"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.846": {
    "title": "Linguistically Motivated Sign Language Segmentation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amit Moryossef",
      "Zifan Jiang",
      "Mathias Müller",
      "Sarah Ebling",
      "Yoav Goldberg"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.847": {
    "title": "Re-weighting Tokens: A Simple and Effective Active Learning Strategy for Named Entity Recognition",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haocheng Luo",
      "Wei Tan",
      "Ngoc Nguyen",
      "Lan Du"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.848": {
    "title": "Language-Agnostic Bias Detection in Language Models with Bias Probing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abdullatif Köksal",
      "Omer Yalcin",
      "Ahmet Akbiyik",
      "M. Kilavuz",
      "Anna Korhonen",
      "Hinrich Schuetze"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.849": {
    "title": "CompleQA: Benchmarking the Impacts of Knowledge Graph Completion Methods on Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Donghan Yu",
      "Yu Gu",
      "Chenyan Xiong",
      "Yiming Yang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.850": {
    "title": "Improving Multi-Criteria Chinese Word Segmentation through Learning Sentence Representation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chun Lin",
      "Ying-Jia Lin",
      "Chia-Jen Yeh",
      "Yi-Ting Li",
      "Ching Yang",
      "Hung-Yu Kao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.851": {
    "title": "A Joint Matrix Factorization Analysis of Multilingual Representations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheng Zhao",
      "Yftah Ziser",
      "Bonnie Webber",
      "Shay Cohen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.852": {
    "title": "Don't Add, don't Miss: Effective Content Preserving Generation from Pre-Selected Text Spans",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aviv Slobodkin",
      "Avi Caciularu",
      "Eran Hirsch",
      "Ido Dagan"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.853": {
    "title": "A Computational Interface to Translate Strategic Intent from Unstructured Language in a Low-Data Setting",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pradyumna Tambwekar",
      "Lakshita Dodeja",
      "Nathan Vaska",
      "Wei Xu",
      "Matthew Gombolay"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.854": {
    "title": "HFMRE: Constructing Huffman Tree in Bags to Find Excellent Instances for Distantly Supervised Relation Extraction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Min Li",
      "Cong Shao",
      "Gang Li",
      "Mingle Zhou"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.855": {
    "title": "DISCO: A Large Scale Human Annotated Corpus for Disfluency Correction in Indo-European Languages",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vineet Bhat",
      "Preethi Jyothi",
      "Pushpak Bhattacharyya"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.856": {
    "title": "Towards Being Parameter-Efficient: A Stratified Sparsely Activated Transformer with Dynamic Capacity",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoran Xu",
      "Maha Elbayad",
      "Kenton Murray",
      "Jean Maillard",
      "Vedanuj Goswami"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.857": {
    "title": "Misery Loves Complexity: Exploring Linguistic Complexity in the Context of Emotion Detection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pranaydeep Singh",
      "Luna De Bruyne",
      "Orphée De Clercq",
      "Els Lefever"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.858": {
    "title": "Probing the \"Creativity\" of Large Language Models: Can models produce divergent semantic association?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Honghua Chen",
      "Nai Ding"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.859": {
    "title": "Code-Switching with Word Senses for Pretraining in Neural Machine Translation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vivek Iyer",
      "Edoardo Barba",
      "Alexandra Birch",
      "Jeff Pan",
      "Roberto Navigli"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.860": {
    "title": "DiffusionSL: Sequence Labeling via Tag Diffusion Process",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyang Huang",
      "Pengfei Cao",
      "Jun Zhao",
      "Kang Liu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.861": {
    "title": "COMET-M: Reasoning about Multiple Events in Complex Sentences",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sahithya Ravi",
      "Raymond Ng",
      "Vered Shwartz"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.862": {
    "title": "On Event Individuation for Document-Level Information Extraction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "William Gantt",
      "Reno Kriz",
      "Yunmo Chen",
      "Siddharth Vashishtha",
      "Aaron White"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.863": {
    "title": "AniEE: A Dataset of Animal Experimental Literature for Event Extraction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dohee Kim",
      "Ra Yoo",
      "Soyoung Yang",
      "Hee Yang",
      "Jaegul Choo"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.864": {
    "title": "From Words to Wires: Generating Functioning Electronic Devices from Natural Language Descriptions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peter Jansen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.865": {
    "title": "Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhisong Zhang",
      "Emma Strubell",
      "Eduard Hovy"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.866": {
    "title": "Explicit Alignment and Many-to-many Entailment Based Reasoning for Conversational Machine Reading",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangyang Luo",
      "Shiyu Tian",
      "Caixia Yuan",
      "Xiaojie Wang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.867": {
    "title": "Harnessing Dataset Cartography for Improved Compositional Generalization in Transformers",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Osman İnce",
      "Tanin Zeraati",
      "Semih Yagcioglu",
      "Yadollah Yaghoobzadeh",
      "Erkut Erdem",
      "Aykut Erdem"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.868": {
    "title": "Roles of Scaling and Instruction Tuning in Language Perception: Model vs. Human Attention",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changjiang Gao",
      "Shujian Huang",
      "Jixing Li",
      "Jiajun Chen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.869": {
    "title": "Efficient Data Learning for Open Information Extraction with Pre-trained Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyuan Fan",
      "Shizhu He"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.870": {
    "title": "Survival of the Most Influential Prompts: Efficient Black-Box Prompt Search via Clustering and Pruning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Zhou",
      "Xingchen Wan",
      "Ivan Vulić",
      "Anna Korhonen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.871": {
    "title": "Towards Zero-shot Learning for End-to-end Cross-modal Translation Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jichen Yang",
      "Kai Fan",
      "Minpeng Liao",
      "Boxing Chen",
      "Zhongqiang Huang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.872": {
    "title": "LLMaAA: Making Large Language Models as Active Annotators",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruoyu Zhang",
      "Yanzeng Li",
      "Yongliang Ma",
      "Ming Zhou",
      "Lei Zou"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.873": {
    "title": "NLMs: Augmenting Negation in Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rituraj Singh",
      "Rahul Kumar",
      "Vivek Sridhar"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.874": {
    "title": "Parameter-Efficient Prompt Tuning Makes Generalized and Calibrated Neural Text Retrievers",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weng Tam",
      "Xiao Liu",
      "Kaixuan Ji",
      "Lilong Xue",
      "Jiahua Liu",
      "Tao Li",
      "Yuxiao Dong",
      "Jie Tang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.875": {
    "title": "X-SNS: Cross-Lingual Transfer Prediction through Sub-Network Similarity",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taejun Yun",
      "Jinhyeon Kim",
      "Deokyeong Kang",
      "Seonghoon Lim",
      "Jihoon Kim",
      "Taeuk Kim"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.876": {
    "title": "Noise-Robust Semi-Supervised Learning for Distantly Supervised Relation Extraction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Sun",
      "Qiang Liu",
      "Shu Wu",
      "Zilei Wang",
      "Liang Wang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.877": {
    "title": "Towards Concept-Aware Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Shani",
      "Jilles Vreeken",
      "Dafna Shahaf"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.878": {
    "title": "ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Viet Lai",
      "Nghia Ngo",
      "Amir Pouran Ben Veyseh",
      "Hieu Man",
      "Franck Dernoncourt",
      "Trung Bui",
      "Thien Nguyen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.879": {
    "title": "Subspace Chronicles: How Linguistic Information Emerges, Shifts and Interacts during Language Model Training",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Max Müller-Eberstein",
      "Rob van der Goot",
      "Barbara Plank",
      "Ivan Titov"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.880": {
    "title": "Not All Demonstration Examples are Equally Beneficial: Reweighting Demonstration Examples for In-Context Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhe Yang",
      "Damai Dai",
      "Peiyi Wang",
      "Zhifang Sui"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.881": {
    "title": "Difference-Masking: Choosing What to Mask in Continued Pretraining",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alex Wilf",
      "Syeda Akter",
      "Leena Mathur",
      "Paul Liang",
      "Sheryl Mathew",
      "Mengrou Shou",
      "Eric Nyberg",
      "Louis-Philippe Morency"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.882": {
    "title": "Learn From One Specialized Sub-Teacher: One-to-One Mapping for Feature-Based Knowledge Distillation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Khouloud Saadi",
      "Jelena Mitrović",
      "Michael Granitzer"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.883": {
    "title": "IMU2CLIP: Language-grounded Motion Sensor Translation with Multimodal Contrastive Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seungwhan Moon",
      "Andrea Madotto",
      "Zhaojiang Lin",
      "Aparajita Saraf",
      "Amy Bearman",
      "Babak Damavandi"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.884": {
    "title": "Conditioning on Dialog Acts improves Empathy Style Transfer",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Renyi Qu",
      "Lyle Ungar",
      "João Sedoc"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.885": {
    "title": "Systematic Assessment of Factual Knowledge in Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linhao Luo",
      "Trang Vu",
      "Dinh Phung",
      "Reza Haf"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.886": {
    "title": "From Speculation Detection to Trustworthy Relational Tuples in Information Extraction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kuicai Dong",
      "Aixin Sun",
      "Jung-jae Kim",
      "Xiaoli Li"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.887": {
    "title": "Tokenization Consistency Matters for Generative Models on Extractive NLP Tasks",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaiser Sun",
      "Peng Qi",
      "Yuhao Zhang",
      "Lan Liu",
      "William Wang",
      "Zhiheng Huang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.888": {
    "title": "Dialogue Medical Information Extraction with Medical-Item Graph and Dialogue-Status Enriched Representation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Gao",
      "Xinnan Zhang",
      "Xian Wu",
      "Shen Ge",
      "Yefeng Zheng"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.889": {
    "title": "LogicAttack: Adversarial Attacks for Evaluating Logical Consistency of Natural Language Inference",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mutsumi Nakamura",
      "Santosh Mashetty",
      "Mihir Parmar",
      "Neeraj Varshney",
      "Chitta Baral"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.890": {
    "title": "Decomposed Prompt Tuning via Low-Rank Reparameterization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yao Xiao",
      "Lu Xu",
      "Jiaxi Li",
      "Wei Lu",
      "Xiaoli Li"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.891": {
    "title": "SGP-TOD: Building Task Bots Effortlessly via Schema-Guided LLM Prompting",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoying Zhang",
      "Baolin Peng",
      "Kun Li",
      "Jingyan Zhou",
      "Helen Meng"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.892": {
    "title": "Ethical Reasoning over Moral Alignment: A Case and Framework for In-Context Ethical Policies in LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhinav Rao",
      "Aditi Khandelwal",
      "Kumar Tanmay",
      "Utkarsh Agarwal",
      "Monojit Choudhury"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.893": {
    "title": "Vector-Quantized Prompt Learning for Paraphrase Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haotian Luo",
      "Yixin Liu",
      "Peidong Liu",
      "Xianggen Liu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.894": {
    "title": "Rethinking the Construction of Effective Metrics for Understanding the Mechanisms of Pretrained Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "You Li",
      "Jinhui Yin",
      "Yuming Lin"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.895": {
    "title": "PARROT: Zero-Shot Narrative Reading Comprehension via Parallel Reading",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Zhao",
      "Anvesh Vijjini",
      "Snigdha Chaturvedi"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.896": {
    "title": "BioDEX: Large-Scale Biomedical Adverse Drug Event Extraction for Real-World Pharmacovigilance",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karel D’Oosterlinck",
      "François Remy",
      "Johannes Deleu",
      "Thomas Demeester",
      "Chris Develder",
      "Klim Zaporojets",
      "Aneiss Ghodsi",
      "Simon Ellershaw",
      "Jack Collins",
      "Christopher Potts"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.897": {
    "title": "Coarse-to-Fine Dual Encoders are Better Frame Identification Learners",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaikai An",
      "Ce Zheng",
      "Bofei Gao",
      "Haozhe Zhao",
      "Baobao Chang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.898": {
    "title": "Sound of Story: Multi-modal Storytelling with Audio",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaeyeon Bae",
      "Seokhoon Jeong",
      "Seokun Kang",
      "Namgi Han",
      "Jae-Yon Lee",
      "Hyounghun Kim",
      "Taehwan Kim"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.899": {
    "title": "Synthesize, if you do not have: Effective Synthetic Dataset Creation Strategies for Self-Supervised Opinion Summarization in E-commerce",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tejpalsingh Siledar",
      "Suman Banerjee",
      "Amey Patil",
      "Sudhanshu Singh",
      "Muthusamy Chelliah",
      "Nikesh Garera",
      "Pushpak Bhattacharyya"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.900": {
    "title": "Leveraging Contrastive Learning and Knowledge Distillation for Incomplete Modality Rumor Detection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Xu",
      "Pinyun Fu",
      "Qi Huang",
      "Bowei Zou",
      "AiTi Aw",
      "Mingwen Wang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.901": {
    "title": "Beyond Testers' Biases: Guiding Model Testing with Knowledge Bases using LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenyang Yang",
      "Rishabh Rustogi",
      "Rachel Brower-Sinning",
      "Grace Lewis",
      "Christian Kaestner",
      "Tongshuang Wu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.902": {
    "title": "CAR: Conceptualization-Augmented Reasoner for Zero-Shot Commonsense Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiqi Wang",
      "Tianqing Fang",
      "Wenxuan Ding",
      "Baixuan Xu",
      "Xin Liu",
      "Yangqiu Song",
      "Antoine Bosselut"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.903": {
    "title": "kNN-CM: A Non-parametric Inference-Phase Adaptation of Parametric Text Classifiers",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rishabh Bhardwaj",
      "Yingting Li",
      "Navonil Majumder",
      "Bo Cheng",
      "Soujanya Poria"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.904": {
    "title": "Cross-modality Data Augmentation for End-to-End Sign Language Translation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinhui Ye",
      "Wenxiang Jiao",
      "Xing Wang",
      "Zhaopeng Tu",
      "Hui Xiong"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.905": {
    "title": "Consistency is Key: On Data-Efficient Modality Transfer in Speech Translation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hojin Lee",
      "Changmin Lee",
      "Seung-won Hwang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.906": {
    "title": "Relation-Aware Question Answering for Heterogeneous Knowledge Graphs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haowei Du",
      "Quzhe Huang",
      "Chen Li",
      "Chen Zhang",
      "Yang Li",
      "Dongyan Zhao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.907": {
    "title": "InstOptima: Evolutionary Multi-objective Instruction Optimization via Large Language Model-based Instruction Operators",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heng Yang",
      "Ke Li"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.908": {
    "title": "Less than One-shot: Named Entity Recognition via Extremely Weak Supervision",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Letian Peng",
      "Zihan Wang",
      "Jingbo Shang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.909": {
    "title": "Focus on the Core: Efficient Attention via Pruned Token Compression for Document Classification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jungmin Yun",
      "Mihyeon Kim",
      "Youngbin Kim"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.910": {
    "title": "Semantic Decomposition of Question and SQL for Text-to-SQL Parsing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ben Eyal",
      "Moran Mahabi",
      "Ophir Haroche",
      "Amir Bachar",
      "Michael Elhadad"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.911": {
    "title": "Time-Aware Language Modeling for Historical Text Dating",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Ren",
      "Hai Wang",
      "Yajie Zhao",
      "Yafeng Ren"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.912": {
    "title": "A Read-and-Select Framework for Zero-shot Entity Linking",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenran Xu",
      "Yulin Chen",
      "Baotian Hu",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.913": {
    "title": "Multi-Task Learning of Query Generation and Classification for Generative Conversational Question Rewriting",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sarawoot Kongyoung",
      "Craig MacDonald",
      "Iadh Ounis"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.914": {
    "title": "DepNeCTI: Dependency-based Nested Compound Type Identification for Sanskrit",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jivnesh Sandhan",
      "Yaswanth Narsupalli",
      "Sreevatsa Muppirala",
      "Sriram Krishnan",
      "Pavankumar Satuluri",
      "Amba Kulkarni",
      "Pawan Goyal"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.915": {
    "title": "HeQ: a Large and Diverse Hebrew Reading Comprehension Benchmark",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amir Cohen",
      "Hilla Merhav-Fine",
      "Yoav Goldberg",
      "Reut Tsarfaty"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.916": {
    "title": "HANSEN: Human and AI Spoken Text Benchmark for Authorship Analysis",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nafis Tripto",
      "Adaku Uchendu",
      "Thai Le",
      "Mattia Setzu",
      "Fosca Giannotti",
      "Dongwon Lee"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.917": {
    "title": "Data Augmentation for Code Translation with Comparable Corpora and Multiple References",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiqing Xie",
      "Atharva Naik",
      "Daniel Fried",
      "Carolyn Rose"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.918": {
    "title": "Multilingual Generation and Answering of Questions from Texts and Knowledge Graphs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kelvin Han",
      "Claire Gardent"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.919": {
    "title": "InfoDiffusion: Information Entropy Aware Diffusion Process for Non-Autoregressive Text Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Renzhi Wang",
      "Jing Li",
      "Piji Li"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.920": {
    "title": "Enhancing Scalability of Pre-trained Language Models via Efficient Parameter Sharing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peiyu Liu",
      "Ze-Feng Gao",
      "Yushuo Chen",
      "Xin Zhao",
      "Ji-Rong Wen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.921": {
    "title": "Boosting Prompt-Based Self-Training With Mapping-Free Automatic Verbalizer for Multi-Class Classification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yookyung Kho",
      "Jaehee Kim",
      "Pilsung Kang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.922": {
    "title": "On the Impact of Cross-Domain Data on German Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amin Dada",
      "Aokun Chen",
      "Cheng Peng",
      "Kaleb Smith",
      "Ahmad Idrissi-Yaghir",
      "Constantin Seibold",
      "Jianning Li",
      "Lars Heiliger",
      "Christoph Friedrich",
      "Daniel Truhn",
      "Jan Egger",
      "Jiang Bian",
      "Jens Kleesiek",
      "Yonghui Wu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.923": {
    "title": "Dialect-to-Standard Normalization: A Large-Scale Multilingual Evaluation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Olli Kuparinen",
      "Aleksandra Miletić",
      "Yves Scherrer"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.924": {
    "title": "Re-Examining Summarization Evaluation across Multiple Quality Criteria",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ori Ernst",
      "Ori Shapira",
      "Ido Dagan",
      "Ran Levy"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.925": {
    "title": "A Parallel Corpus for Vietnamese Central-Northern Dialect Text Transfer",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thang Le",
      "Anh Luu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.926": {
    "title": "A Comprehensive Evaluation of Tool-Assisted Generation Strategies",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alon Jacovi",
      "Avi Caciularu",
      "Jonathan Herzig",
      "Roee Aharoni",
      "Bernd Bohnet",
      "Mor Geva"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.927": {
    "title": "InheritSumm: A General, Versatile and Compact Summarizer by Distilling from GPT",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yichong Xu",
      "Ruochen Xu",
      "Dan Iter",
      "Yang Liu",
      "Shuohang Wang",
      "Chenguang Zhu",
      "Michael Zeng"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.928": {
    "title": "Learning to love diligent trolls: Accounting for rater effects in the dialogue safety task",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Ilagan"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.929": {
    "title": "Can ChatGPT Perform Reasoning Using the IRAC Method in Analyzing Legal Scenarios Like a Lawyer?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoxi Kang",
      "Lizhen Qu",
      "Lay-Ki Soon",
      "Adnan Trakic",
      "Terry Zhuo",
      "Patrick Emerton",
      "Genevieve Grant"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.930": {
    "title": "Coverage-based Example Selection for In-Context Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shivanshu Gupta",
      "Matt Gardner",
      "Sameer Singh"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.931": {
    "title": "Are Structural Concepts Universal in Transformer Language Models? Towards Interpretable Cross-Lingual Generalization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ningyu Xu",
      "Qi Zhang",
      "Jingting Ye",
      "Menghan Zhang",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.932": {
    "title": "Thorny Roses: Investigating the Dual Use Dilemma in Natural Language Processing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucie-Aimée Kaffee",
      "Arnav Arora",
      "Zeerak Talat",
      "Isabelle Augenstein"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.933": {
    "title": "BYOC: Personalized Few-Shot Classification with Co-Authored Class Descriptions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arth Bohra",
      "Govert Verkes",
      "Artem Harutyunyan",
      "Pascal Weinberger",
      "Giovanni Campagna"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.934": {
    "title": "Approximating CKY with Transformers",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ghazal Khalighinejad",
      "Ollie Liu",
      "Sam Wiseman"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.935": {
    "title": "DialGuide: Aligning Dialogue Model Behavior with Developer Guidelines",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prakhar Gupta",
      "Yang Liu",
      "Di Jin",
      "Behnam Hedayatnia",
      "Spandana Gella",
      "Sijia Liu",
      "Patrick Lange",
      "Julia Hirschberg",
      "Dilek Hakkani-Tur"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.936": {
    "title": "RWKV: Reinventing RNNs for the Transformer Era",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Peng",
      "Eric Alcaide",
      "Quentin Anthony",
      "Alon Albalak",
      "Samuel Arcadinho",
      "Stella Biderman",
      "Huanqi Cao",
      "Xin Cheng",
      "Michael Chung",
      "Leon Derczynski",
      "Xingjian Du",
      "Matteo Grella",
      "Kranthi Gv",
      "Xuzheng He",
      "Haowen Hou",
      "Przemyslaw Kazienko",
      "Jan Kocon",
      "Jiaming Kong",
      "Bartłomiej Koptyra",
      "Hayden Lau",
      "Jiaju Lin",
      "Krishna Sri Ipsit Mantri",
      "Ferdinand Mom",
      "Atsushi Saito",
      "Guangyu Song",
      "Xiangru Tang",
      "Johan Wind",
      "Stanisław Woźniak",
      "Zhenyuan Zhang",
      "Qinghua Zhou",
      "Jian Zhu",
      "Rui-Jie Zhu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.937": {
    "title": "Who Wrote it and Why? Prompting Large-Language Models for Authorship Verification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chia-Yu Hung",
      "Zhiqiang Hu",
      "Yujia Hu",
      "Roy Lee"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.938": {
    "title": "Transitioning Representations between Languages for Cross-lingual Event Detection via Langevin Dynamics",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chien Nguyen",
      "Huy Nguyen",
      "Franck Dernoncourt",
      "Thien Nguyen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.939": {
    "title": "VISIT: Visualizing and Interpreting the Semantic Information Flow of Transformers",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shahar Katz",
      "Yonatan Belinkov"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.940": {
    "title": "Is Robustness Transferable across Languages in Multilingual Neural Machine Translation?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leiyu Pan",
      "Supryadi",
      "Deyi Xiong"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.941": {
    "title": "Arabic Mini-ClimateGPT : A Climate Change and Sustainability Tailored Arabic LLM",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sahal Mullappilly",
      "Abdelrahman Shaker",
      "Omkar Thawakar",
      "Hisham Cholakkal",
      "Rao Anwer",
      "Salman Khan",
      "Fahad Khan"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.942": {
    "title": "Interpreting Answers to Yes-No Questions in User-Generated Content",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shivam Mathur",
      "Keun Park",
      "Dhivya Chinnappa",
      "Saketh Kotamraju",
      "Eduardo Blanco"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.943": {
    "title": "Task-Aware Self-Supervised Framework for Dialogue Discourse Parsing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Li",
      "Luyao Zhu",
      "Wei Shao",
      "Zonglin Yang",
      "Erik Cambria"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.944": {
    "title": "Selective Demonstrations for Cross-domain Text-to-SQL",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuaichen Chang",
      "Eric Fosler-Lussier"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.945": {
    "title": "DocSplit: Simple Contrastive Pretraining for Large Document Embeddings",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujie Wang",
      "Mike Izbicki"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.946": {
    "title": "TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shubhra Kanti Karmaker Santu",
      "Dongji Feng"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.947": {
    "title": "IntenDD: A Unified Contrastive Learning Approach for Intent Detection and Discovery",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bhavuk Singhal",
      "Ashim Gupta",
      "V P Shivasankaran",
      "Amrith Krishna"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.948": {
    "title": "INarIG: Iterative Non-autoregressive Instruct Generation Model For Word-Level Auto Completion",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hengchao Shang",
      "Zongyao Li",
      "Daimeng Wei",
      "Jiaxin Guo",
      "Minghan Wang",
      "Xiaoyu Chen",
      "Lizhi Lei",
      "Hao Yang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.949": {
    "title": "Is the Answer in the Text? Challenging ChatGPT with Evidence Retrieval from Instructive Text",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sophie Henning",
      "Talita Anthonio",
      "Wei Zhou",
      "Heike Adel",
      "Mohsen Mesgar",
      "Annemarie Friedrich"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.950": {
    "title": "PaRaDe: Passage Ranking using Demonstrations with LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrew Drozdov",
      "Honglei Zhuang",
      "Zhuyun Dai",
      "Zhen Qin",
      "Razieh Rahimi",
      "Xuanhui Wang",
      "Dana Alon",
      "Mohit Iyyer",
      "Andrew McCallum",
      "Donald Metzler",
      "Kai Hui"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.951": {
    "title": "Learning Dynamic Representations for Discourse Dependency Parsing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyi Liu",
      "Yansong Feng",
      "Dongyan Zhao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.952": {
    "title": "K-HATERS: A Hate Speech Detection Corpus in Korean with Target-Specific Ratings",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaewon Park",
      "Soohwan Kim",
      "Kyubyong Park",
      "Kunwoo Park"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.953": {
    "title": "Mitigating Data Imbalance and Representation Degeneration in Multilingual Machine Translation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wen Lai",
      "Alexandra Chronopoulou",
      "Alexander Fraser"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.954": {
    "title": "BotPercent: Estimating Bot Populations in Twitter Communities",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoxuan Tan",
      "Shangbin Feng",
      "Melanie Sclar",
      "Herun Wan",
      "Minnan Luo",
      "Yejin Choi",
      "Yulia Tsvetkov"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.955": {
    "title": "The Locality and Symmetry of Positional Encodings",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lihu Chen",
      "Gael Varoquaux",
      "Fabian Suchanek"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.956": {
    "title": "Towards a Deep Understanding of Multilingual End-to-End Speech Translation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoran Sun",
      "Xiaohu Zhao",
      "Yikun Lei",
      "Shaolin Zhu",
      "Deyi Xiong"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.957": {
    "title": "An Empirical Investigation of Implicit and Explicit Knowledge-Enhanced Methods for Ad Hoc Dataset Retrieval",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiqing Luo",
      "Qiaosheng Chen",
      "Zhiyang Zhang",
      "Zixian Huang",
      "Gong Cheng"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.958": {
    "title": "A Multi-Modal Multilingual Benchmark for Document Image Classification",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yoshinari Fujinuma",
      "Siddharth Varia",
      "Nishant Sankaran",
      "Srikar Appalaraju",
      "Bonan Min",
      "Yogarshi Vyas"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.959": {
    "title": "Unnatural language processing: How do language models handle machine-generated prompts?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Corentin Kervadec",
      "Francesca Franzon",
      "Marco Baroni"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.960": {
    "title": "Investigating the Effectiveness of Multiple Expert Models Collaboration",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ikumi Ito",
      "Takumi Ito",
      "Jun Suzuki",
      "Kentaro Inui"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.961": {
    "title": "Gradually Excavating External Knowledge for Implicit Complex Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chang Liu",
      "Xiaoguang Li",
      "Lifeng Shang",
      "Xin Jiang",
      "Qun Liu",
      "Edmund Lam",
      "Ngai Wong"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.962": {
    "title": "Evaluating Subjective Cognitive Appraisals of Emotions from Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongli Zhan",
      "Desmond Ong",
      "Junyi Li"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.963": {
    "title": "Exploring Linguistic Properties of Monolingual BERTs with Typological Classification among Languages",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elena Ruzzetti",
      "Federico Ranaldi",
      "Felicia Logozzo",
      "Michele Mastromattei",
      "Leonardo Ranaldi",
      "Fabio Zanzotto"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.964": {
    "title": "Discourse Sense Flows: Modelling the Rhetorical Style of Documents across Various Domains",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rene Knaebel",
      "Manfred Stede"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.965": {
    "title": "HierarchicalContrast: A Coarse-to-Fine Contrastive Learning Framework for Cross-Domain Zero-Shot Slot Filling",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junwen Zhang",
      "Yin Zhang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.966": {
    "title": "A Confederacy of Models: a Comprehensive Evaluation of LLMs on Creative Writing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Carlos Gómez-Rodríguez",
      "Paul Williams"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.967": {
    "title": "1-PAGER: One Pass Answer Generation and Evidence Retrieval",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Palak Jain",
      "Livio Soares",
      "Tom Kwiatkowski"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.968": {
    "title": "Context-faithful Prompting for Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenxuan Zhou",
      "Sheng Zhang",
      "Hoifung Poon",
      "Muhao Chen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.969": {
    "title": "InfoCL: Alleviating Catastrophic Forgetting in Continual Text Classification from An Information Theoretic Perspective",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Song",
      "Peiyi Wang",
      "Weimin Xiong",
      "Dawei Zhu",
      "Tianyu Liu",
      "Zhifang Sui",
      "Sujian Li"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.970": {
    "title": "Sparse Frame Grouping Network with Action Centered for Untrimmed Video Paragraph Captioning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guorui Yu",
      "Yimin Hu",
      "Yuejie Zhang",
      "Rui Feng",
      "Tao Zhang",
      "Shang Gao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.971": {
    "title": "Unsupervised Binary Code Translation with Application to Code Clone Detection and Vulnerability Discovery",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Iftakhar Ahmad",
      "Lannan Luo"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.972": {
    "title": "Drilling Down into the Discourse Structure with LLMs for Long Document Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Inderjeet Nair",
      "Shwetha Somasundaram",
      "Apoorv Saxena",
      "Koustava Goswami"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.973": {
    "title": "Emergent Inabilities? Inverse Scaling Over the Course of Pretraining",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "James Michaelov",
      "Ben Bergen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.974": {
    "title": "Alignment Precedes Fusion: Open-Vocabulary Named Entity Recognition as Context-Type Semantic Matching",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuoran Jin",
      "Pengfei Cao",
      "Zhitao He",
      "Yubo Chen",
      "Kang Liu",
      "Jun Zhao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.975": {
    "title": "Representation Projection Invariance Mitigates Representation Collapse",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anastasia Razdaibiedina",
      "Ashish Khetan",
      "Zohar Karnin",
      "Daniel Khashabi",
      "Vivek Madan"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.976": {
    "title": "Tunable Soft Prompts are Messengers in Federated Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenhe Dong",
      "Yuexiang Xie",
      "Bolin Ding",
      "Ying Shen",
      "Yaliang Li"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.977": {
    "title": "Style-Aware Radiology Report Generation with RadGraph and Few-Shot Prompting",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Yan",
      "Ruochen Liu",
      "David Kuo",
      "Subathra Adithan",
      "Eduardo Reis",
      "Stephen Kwak",
      "Vasantha Venugopal",
      "Chloe O’Connell",
      "Agustina Saenz",
      "Pranav Rajpurkar",
      "Michael Moor"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.978": {
    "title": "Incorporating Probing Signals into Multimodal Machine Translation via Visual Question-Answering Pairs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxin Zuo",
      "Bei Li",
      "Chuanhao Lv",
      "Tong Zheng",
      "Tong Xiao",
      "JingBo Zhu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.979": {
    "title": "GenKIE: Robust Generative Multimodal Document Key Information Extraction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Panfeng Cao",
      "Ye Wang",
      "Qiang Zhang",
      "Zaiqiao Meng"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.980": {
    "title": "Improving Multimodal Sentiment Analysis: Supervised Angular margin-based Contrastive Learning for Enhanced Fusion Representation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cong-Duy Nguyen",
      "Thong Nguyen",
      "Duc Vu",
      "Anh Luu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.981": {
    "title": "Efficient Multilingual Language Model Compression through Vocabulary Trimming",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Asahi Ushio",
      "Yi Zhou",
      "Jose Camacho-Collados"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.982": {
    "title": "ICU: Conquering Language Barriers in Vision-and-Language Modeling by Dividing the Tasks into Image Captioning and Language Understanding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guojun Wu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.983": {
    "title": "GTA: Gated Toxicity Avoidance for LM Performance Preservation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heegyu Kim",
      "Hyunsouk Cho"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.984": {
    "title": "LMGQS: A Large-scale Dataset for Query-focused Summarization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruochen Xu",
      "Song Wang",
      "Yang Liu",
      "Shuohang Wang",
      "Yichong Xu",
      "Dan Iter",
      "Pengcheng He",
      "Chenguang Zhu",
      "Michael Zeng"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.985": {
    "title": "ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhipeng Chen",
      "Kun Zhou",
      "Beichen Zhang",
      "Zheng Gong",
      "Xin Zhao",
      "Ji-Rong Wen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.986": {
    "title": "Non-Autoregressive Document-Level Machine Translation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangsheng Bao",
      "Zhiyang Teng",
      "Hao Zhou",
      "Jianhao Yan",
      "Yue Zhang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.987": {
    "title": "Exploring the Effectiveness of Multi-Lingual Commonsense Knowledge-Aware Open-Domain Dialogue Response Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sixing Wu",
      "Jiong Yu",
      "Tianshi Che",
      "Yang Zhou",
      "Wei Zhou"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.988": {
    "title": "Mixture of Soft Prompts for Controllable Data Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Derek Chen",
      "Celine Lee",
      "Yunan Lu",
      "Domenic Rosati",
      "Zhou Yu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.989": {
    "title": "A Boundary Offset Prediction Network for Named Entity Recognition",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minghao Tang",
      "Yongquan He",
      "Yongxiu Xu",
      "Hongbo Xu",
      "Wenyuan Zhang",
      "Yang Lin"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.990": {
    "title": "Prefix-Tuning Based Unsupervised Text Style Transfer",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huiyu Mai",
      "Wenhao Jiang",
      "Zhi-Hong Deng"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.991": {
    "title": "Evaluating and Enhancing the Robustness of Code Pre-trained Models through Structure-Aware Adversarial Samples Generation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nuo Chen",
      "Qiushi Sun",
      "Jianing Wang",
      "Ming Gao",
      "Xiaoli Li",
      "Xiang Li"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.992": {
    "title": "Annotation Sensitivity: Training Data Collection Methods Affect Model Performance",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christoph Kern",
      "Stephanie Eckman",
      "Jacob Beck",
      "Rob Chew",
      "Bolei Ma",
      "Frauke Kreuter"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.993": {
    "title": "Qualitative Code Suggestion: A Human-Centric Approach to Qualitative Coding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cesare Piano",
      "Samira Rahimi",
      "Jackie Cheung"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.994": {
    "title": "D2TV: Dual Knowledge Distillation and Target-oriented Vision Modeling for Many-to-Many Multimodal Summarization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunlong Liang",
      "Fandong Meng",
      "Jiaan Wang",
      "Jinan Xu",
      "Yufeng Chen",
      "Jie Zhou"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.995": {
    "title": "Improving Input-label Mapping with Demonstration Replay for In-context Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuocheng Gong",
      "Jiahao Liu",
      "Qifan Wang",
      "Jingang Wang",
      "Xunliang Cai",
      "Dongyan Zhao",
      "Rui Yan"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.996": {
    "title": "Enhancing Text-to-SQL Capabilities of Large Language Models: A Study on Prompt Design Strategies",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linyong Nan",
      "Yilun Zhao",
      "Weijin Zou",
      "Narutatsu Ri",
      "Jaesung Tae",
      "Ellen Zhang",
      "Arman Cohan",
      "Dragomir Radev"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.997": {
    "title": "Cross-lingual Open-Retrieval Question Answering for African Languages",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Odunayo Ogundepo",
      "Tajuddeen Gwadabe",
      "Clara Rivera",
      "Jonathan Clark",
      "Sebastian Ruder",
      "David Adelani",
      "Bonaventure Dossou",
      "Abdou Diop",
      "Claytone Sikasote",
      "Gilles Hacheme",
      "Happy Buzaaba",
      "Ignatius Ezeani",
      "Rooweither Mabuya",
      "Salomey Osei",
      "Chris Emezue",
      "Albert Kahira",
      "Shamsuddeen Muhammad",
      "Akintunde Oladipo",
      "Abraham Owodunni",
      "Atnafu Tonja",
      "Iyanuoluwa Shode",
      "Akari Asai",
      "Anuoluwapo Aremu",
      "Ayodele Awokoya",
      "Bernard Opoku",
      "Chiamaka Chukwuneke",
      "Christine Mwase",
      "Clemencia Siro",
      "Stephen Arthur",
      "Tunde Ajayi",
      "Verrah Otiende",
      "Andre Rubungo",
      "Boyd Sinkala",
      "Daniel Ajisafe",
      "Emeka Onwuegbuzia",
      "Falalu Lawan",
      "Ibrahim Ahmad",
      "Jesujoba Alabi",
      "Chinedu Mbonu",
      "Mofetoluwa Adeyemi",
      "Mofya Phiri",
      "Orevaoghene Ahia",
      "Ruqayya Iro",
      "Sonia Adhiambo"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.998": {
    "title": "Viewing Knowledge Transfer in Multilingual Machine Translation Through a Representational Lens",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Stap",
      "Vlad Niculae",
      "Christof Monz"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.999": {
    "title": "Aligning Predictive Uncertainty with Clarification Questions in Grounded Dialog",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kata Naszadi",
      "Putra Manggala",
      "Christof Monz"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1000": {
    "title": "Cache me if you Can: an Online Cost-aware Teacher-Student framework to Reduce the Calls to Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ilias Stogiannidis",
      "Stavros Vassos",
      "Prodromos Malakasiotis",
      "Ion Androutsopoulos"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1001": {
    "title": "ParroT: Translating during Chat using Large Language Models tuned with Human Translation and Feedback",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenxiang Jiao",
      "Jen-tse Huang",
      "Wenxuan Wang",
      "Zhiwei He",
      "Tian Liang",
      "Xing Wang",
      "Shuming Shi",
      "Zhaopeng Tu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1002": {
    "title": "Dense Retrieval as Indirect Supervision for Large-space Decision Making",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nan Xu",
      "Fei Wang",
      "Mingtao Dong",
      "Muhao Chen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1003": {
    "title": "One-Model-Connects-All: A Unified Graph Pre-Training Model for Online Community Modeling",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruoxue Ma",
      "Jiarong Xu",
      "Xinnong Zhang",
      "Haozhe Zhang",
      "Zuyu Zhao",
      "Qi Zhang",
      "Xuanjing Huang",
      "Zhongyu Wei"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1004": {
    "title": "In-Image Neural Machine Translation with Segmented Pixel Sequence-to-Sequence Model",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanzhi Tian",
      "Xiang Li",
      "Zeming Liu",
      "Yuhang Guo",
      "Bin Wang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1005": {
    "title": "NarrativeXL: a Large-scale Dataset for Long-Term Memory Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arsenii Moskvichev",
      "Ky-Vinh Mai"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1006": {
    "title": "Dialogue Act-Aided Backchannel Prediction Using Multi-Task Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wencke Liermann",
      "Yo-Han Park",
      "Yong-Seok Choi",
      "Kong Lee"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1007": {
    "title": "mReFinED: An Efficient End-to-End Multilingual Entity Linking System",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peerat Limkonchotiwat",
      "Weiwei Cheng",
      "Christos Christodoulopoulos",
      "Amir Saffari",
      "Jens Lehmann"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1008": {
    "title": "Sub-network Discovery and Soft-masking for Continual Learning of Mixed Tasks",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixuan Ke",
      "Bing Liu",
      "Wenhan Xiong",
      "Asli Celikyilmaz",
      "Haoran Li"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1009": {
    "title": "PIVOINE: Instruction Tuning for Open-world Entity Profiling",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keming Lu",
      "Xiaoman Pan",
      "Kaiqiang Song",
      "Hongming Zhang",
      "Dong Yu",
      "Jianshu Chen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1010": {
    "title": "DiQAD: A Benchmark Dataset for Open-domain Dialogue Quality Assessment",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yukun Zhao",
      "Lingyong Yan",
      "Weiwei Sun",
      "Chong Meng",
      "Shuaiqiang Wang",
      "Zhicong Cheng",
      "Zhaochun Ren",
      "Dawei Yin"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1011": {
    "title": "Tuna: Instruction Tuning using Feedback from Large Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoran Li",
      "Yiran Liu",
      "Xingxing Zhang",
      "Wei Lu",
      "Furu Wei"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1012": {
    "title": "Emptying the Ocean with a Spoon: Should We Edit Models?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuval Pinter",
      "Michael Elhadad"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1013": {
    "title": "A Causal View of Entity Bias in (Large) Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fei Wang",
      "Wenjie Mo",
      "Yiwei Wang",
      "Wenxuan Zhou",
      "Muhao Chen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1014": {
    "title": "T5Score: Discriminative Fine-tuning of Generative Evaluation Metrics",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiwei Qin",
      "Weizhe Yuan",
      "Graham Neubig",
      "Pengfei Liu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1015": {
    "title": "T-Projection: High Quality Annotation Projection for Sequence Labeling Tasks",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Iker García-Ferrero",
      "Rodrigo Agerri",
      "German Rigau"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1016": {
    "title": "MTGER: Multi-view Temporal Graph Enhanced Temporal Reasoning over Time-Involved Document",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheng Chu",
      "Zekun Wang",
      "Jiafeng Liang",
      "Ming Liu",
      "Bing Qin"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1017": {
    "title": "MSCFFN: A New FFN with Multi-Space Cross to Accelerate Transformer",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tang Dongge",
      "Qing Yang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1018": {
    "title": "Dialect Transfer for Swiss German Speech Translation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Claudio Paonessa",
      "Yanick Schraner",
      "Jan Deriu",
      "Manuela Hürlimann",
      "Manfred Vogel",
      "Mark Cieliebak"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1019": {
    "title": "Masked Path Modeling for Vision-and-Language Navigation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zi-Yi Dou",
      "Feng Gao",
      "Nanyun Peng"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1020": {
    "title": "Learning Interpretable Style Embeddings via Prompting LLMs",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ajay Patel",
      "Delip Rao",
      "Ansh Kothary",
      "Kathleen McKeown",
      "Chris Callison-Burch"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1021": {
    "title": "Exploring Context-Aware Evaluation Metrics for Machine Translation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyu Hu",
      "Xunjian Yin",
      "Xiaojun Wan"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1022": {
    "title": "GRACE: Discriminator-Guided Chain-of-Thought Reasoning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Khalifa",
      "Lajanugen Logeswaran",
      "Moontae Lee",
      "Honglak Lee",
      "Lu Wang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1023": {
    "title": "QADYNAMICS: Training Dynamics-Driven Synthetic QA Diagnostic for Zero-Shot Commonsense Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haochen Shi",
      "Weiqi Wang",
      "Tianqing Fang",
      "Baixuan Xu",
      "Wenxuan Ding",
      "Xin Liu",
      "Yangqiu Song"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1024": {
    "title": "RexUIE: A Recursive Method with Explicit Schema Instructor for Universal Information Extraction",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengyuan Liu",
      "Fubang Zhao",
      "Yangyang Kang",
      "Jingyuan Zhang",
      "Xiang Zhou",
      "Changlong Sun",
      "Kun Kuang",
      "Fei Wu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1025": {
    "title": "PromptARA: Improving Deep Representation in Hybrid Automatic Readability Assessment with Prompt and Orthogonal Projection",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinshan Zeng",
      "Xianglong Yu",
      "Xianchao Tong",
      "Wenyan Xiao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1026": {
    "title": "Does Listener Gaze in Face-to-Face Interaction Follow the Entropy Rate Constancy Principle: An Empirical Study",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Wang",
      "Hendrik Buschmeier"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1027": {
    "title": "Incorporating Object-Level Visual Context for Multimodal Fine-Grained Entity Typing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ying Zhang",
      "Wenbo Fan",
      "Kehui Song",
      "Yu Zhao",
      "Xuhui Sui",
      "Xiaojie Yuan"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1028": {
    "title": "Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mubashara Akhtar",
      "Abhilash Shankarampeta",
      "Vivek Gupta",
      "Arpit Patil",
      "Oana Cocarascu",
      "Elena Simperl"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1029": {
    "title": "Assessing Privacy Risks in Language Models: A Case Study on Summarization Tasks",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruixiang Tang",
      "Gord Lueck",
      "Rodolfo Quispe",
      "Huseyin Inan",
      "Janardhan Kulkarni",
      "Xia Hu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1030": {
    "title": "BERT Has More to Offer: BERT Layers Combination Yields Better Sentence Embeddings",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "MohammadSaleh Hosseini",
      "Munawara Munia",
      "Latifur Khan"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1031": {
    "title": "Extrapolating Multilingual Understanding Models as Multilingual Generators",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bohong Wu",
      "Fei Yuan",
      "Hai Zhao",
      "Lei Li",
      "Jingjing Xu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1032": {
    "title": "SAC3: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxin Zhang",
      "Zhuohang Li",
      "Kamalika Das",
      "Bradley Malin",
      "Sricharan Kumar"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1033": {
    "title": "Test-Time Self-Adaptive Small Language Models for Question Answering",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soyeong Jeong",
      "Jinheon Baek",
      "Sukmin Cho",
      "Sung Hwang",
      "Jong Park"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1034": {
    "title": "ExpNote: Black-box Large Language Models are better Task Solvers with Experience Notebook",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wangtao Sun",
      "Xuanqing Yu",
      "Shizhu He",
      "Jun Zhao",
      "Kang Liu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1035": {
    "title": "Evaluating Parameter-Efficient Finetuning Approaches for Pre-trained Models on the Financial Domain",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Isabella Olariu",
      "Cedric Lothritz",
      "Jacques Klein",
      "Tegawendé Bissyandé",
      "Siwen Guo",
      "Shohreh Haddadan"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1036": {
    "title": "Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Parishad BehnamGhader",
      "Santiago Miret",
      "Siva Reddy"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1037": {
    "title": "BERTwich: Extending BERT's Capabilities to Model Dialectal and Noisy Text",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aarohi Srivastava",
      "David Chiang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1038": {
    "title": "Closed Boundary Learning for Classification Tasks with the Universum Class",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanzhang Zhou",
      "Zijian Feng",
      "Kezhi Mao"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1039": {
    "title": "Revisiting Entropy Rate Constancy in Text",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vivek Verma",
      "Nicholas Tomlin",
      "Dan Klein"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1040": {
    "title": "Calibrated Seq2seq Models for Efficient and Generalizable Ultra-fine Entity Typing",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanlin Feng",
      "Adithya Pratapa",
      "David Mortensen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1041": {
    "title": "Learning Semantic Role Labeling from Compatible Label Sequences",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Li",
      "Ghazaleh Kazeminejad",
      "Susan Brown",
      "Vivek Srikumar",
      "Martha Palmer"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1042": {
    "title": "QUADRo: Dataset and Models for QUestion-Answer Database Retrieval",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefano Campese",
      "Ivano Lauriola",
      "Alessandro Moschitti"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1043": {
    "title": "Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained Language Models",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paul Youssef",
      "Osman Koraş",
      "Meijie Li",
      "Jörg Schlötterer",
      "Christin Seifert"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1044": {
    "title": "Is ChatGPT the ultimate Data Augmentation Algorithm?",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Frédéric Piedboeuf",
      "Philippe Langlais"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1045": {
    "title": "Enhanced Simultaneous Machine Translation with Word-level Policies",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kang Kim",
      "Hankyu Cho"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1046": {
    "title": "Causal Intervention-based Few-Shot Named Entity Recognition",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Yang",
      "Yongbin Liu",
      "Chunping Ouyang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1047": {
    "title": "TADI: Topic-aware Attention and Powerful Dual-encoder Interaction for Recall in News Recommendation",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junxiang Jiang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1048": {
    "title": "Unveiling the Power of Argument Arrangement in Online Persuasive Discussions",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nailia Mirzakhmedova",
      "Johannes Kiesel",
      "Khalid Al-Khatib",
      "Benno Stein"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1049": {
    "title": "FFAEval: Evaluating Dialogue System via Free-For-All Ranking",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyao Ma",
      "Zijun Yao",
      "Jing Zhang",
      "Jifan Yu",
      "Xiaohan Zhang",
      "Juanzi Li",
      "Jie Tang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1050": {
    "title": "Orca: A Few-shot Benchmark for Chinese Conversational Machine Reading Comprehension",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nuo Chen",
      "Hongguang Li",
      "Junqing He",
      "Yinan Bao",
      "Xinshi Lin",
      "Qi Yang",
      "Jianfeng Liu",
      "Ruyi Gan",
      "Jiaxing Zhang",
      "Baoyuan Wang",
      "Jia Li"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1051": {
    "title": "VER: Unifying Verbalizing Entities and Relations",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Huang",
      "Kevin Chang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1052": {
    "title": "The Linearity of the Effect of Surprisal on Reading Times across Languages",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weijie Xu",
      "Jason Chon",
      "Tianran Liu",
      "Richard Futrell"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1053": {
    "title": "Adversarial Text Generation by Search and Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guoyi Li",
      "Bingkang Shi",
      "Zongzhen Liu",
      "Dehan Kong",
      "Yulei Wu",
      "Xiaodan Zhang",
      "Longtao Huang",
      "Honglei Lyu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1054": {
    "title": "Measuring Pointwise 𝒱-Usable Information In-Context-ly",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng Lu",
      "Shan Chen",
      "Yingya Li",
      "Danielle Bitterman",
      "Guergana Savova",
      "Iryna Gurevych"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1055": {
    "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dong Zhang",
      "Shimin Li",
      "Xin Zhang",
      "Jun Zhan",
      "Pengyu Wang",
      "Yaqian Zhou",
      "Xipeng Qiu"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1056": {
    "title": "Unleashing the Multilingual Encoder Potential: Boosting Zero-Shot Performance via Probability Calibration",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ercong Nie",
      "Helmut Schmid",
      "Hinrich Schuetze"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1057": {
    "title": "A Thorough Examination on Zero-shot Dense Retrieval",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruiyang Ren",
      "Yingqi Qu",
      "Jing Liu",
      "Xin Zhao",
      "Qifei Wu",
      "Yuchen Ding",
      "Hua Wu",
      "Haifeng Wang",
      "Ji-Rong Wen"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1058": {
    "title": "Contrastive Pre-training for Personalized Expert Finding",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiyao Peng",
      "Hongtao Liu",
      "Zhepeng Lv",
      "Qing Yang",
      "Wenjun Wang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1059": {
    "title": "Mitigating Intrinsic Named Entity-Related Hallucinations of Abstractive Text Summarization",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianbin Shen",
      "Junyu Xuan",
      "Christy Liang"
    ]
  },
  "https://aclanthology.org/2023.findings-emnlp.1060": {
    "title": "Towards Informative Few-Shot Prompt with Maximum Information Gain for In-Context Learning",
    "volume": "findings",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongfu Liu",
      "Ye Wang"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.1": {
    "title": "Violet: A Vision-Language Model for Arabic Image Captioning with Gemini Decoder",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abdelrahman Mohamed",
      "Fakhraddin Alwajih",
      "El Moatez Billah Nagoudi",
      "Alcides Inciarte",
      "Muhammad Abdul-Mageed"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.2": {
    "title": "Nâbra: Syrian Arabic Dialects with Morphological Annotations",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amal Nayouf",
      "Tymaa Hammouda",
      "Mustafa Jarrar",
      "Fadi Zaraket",
      "Mohamad-Bassam Kurdy"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.3": {
    "title": "HICMA: The Handwriting Identification for Calligraphy and Manuscripts in Arabic Dataset",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anis Ismail",
      "Zena Kamel",
      "Reem Mahmoud"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.4": {
    "title": "Automated De-Identification of Arabic Medical Records",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Veysel Kocaman",
      "Youssef Mellah",
      "Hasham Haq",
      "David Talby"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.5": {
    "title": "ArTST: Arabic Text and Speech Transformer",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hawau Toyin",
      "Amirbek Djanibekov",
      "Ajinkya Kulkarni",
      "Hanan Aldarmaki"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.6": {
    "title": "TARJAMAT: Evaluation of Bard and ChatGPT on Machine Translation of Ten Arabic Varieties",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karima Kadaoui",
      "Samar Magdy",
      "Abdul Waheed",
      "Md Tawkat Islam Khondaker",
      "Ahmed El-Shangiti",
      "El Moatez Billah Nagoudi",
      "Muhammad Abdul-Mageed"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.7": {
    "title": "Leveraging Domain Adaptation and Data Augmentation to Improve Qur'anic IR in English and Arabic",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vera Pavlova"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.8": {
    "title": "LANS: Large-scale Arabic News Summarization Corpus",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abdulaziz Alhamadani",
      "Xuchao Zhang",
      "Jianfeng He",
      "Aadyant Khatri",
      "Chang-Tien Lu"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.9": {
    "title": "Beyond English: Evaluating LLMs for Arabic Grammatical Error Correction",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sang Kwon",
      "Gagan Bhatia",
      "El Moatez Billah Nagoudi",
      "Muhammad Abdul-Mageed"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.10": {
    "title": "Aswat: Arabic Audio Dataset for Automatic Speech Recognition Using Speech-Representation Learning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lamya Alkanhal",
      "Abeer Alessa",
      "Elaf Almahmoud",
      "Rana Alaqil"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.11": {
    "title": "Analyzing Multilingual Competency of LLMs in Multi-Turn Instruction Following: A Case Study of Arabic",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sabri Boughorbel",
      "Majd Hawasly"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.12": {
    "title": "Cross-Dialectal Named Entity Recognition in Arabic",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Niama Elkhbir",
      "Urchade Zaratiana",
      "Nadi Tomeh",
      "Thierry Charnois"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.13": {
    "title": "Enhancing Arabic Machine Translation for E-commerce Product Information: Data Quality Challenges and Innovative Selection Approaches",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bryan Zhang",
      "Salah Danial",
      "Stephan Walter"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.14": {
    "title": "IDRISI-D: Arabic and English Datasets and Benchmarks for Location Mention Disambiguation over Disaster Microblogs",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Reem Suwaileh",
      "Tamer Elsayed",
      "Muhammad Imran"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.15": {
    "title": "CamelParser2.0: A State-of-the-Art Dependency Parser for Arabic",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ahmed Elshabrawy",
      "Muhammed AbuOdeh",
      "Go Inoue",
      "Nizar Habash"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.16": {
    "title": "GARI: Graph Attention for Relative Isomorphism of Arabic Word Embeddings",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Ali",
      "Maha Alshmrani",
      "Jianbin Qin",
      "Yan Hu",
      "Di Wang"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.17": {
    "title": "ArTrivia: Harvesting Arabic Wikipedia to Build A New Arabic Question Answering Dataset",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sultan Alrowili",
      "K Vijay-Shanker"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.18": {
    "title": "ArSarcasMoji Dataset: The Emoji Sentiment Roles in Arabic Ironic Contexts",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shatha Ali A. Hakami",
      "Robert Hendley",
      "Phillip Smith"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.19": {
    "title": "Performance Implications of Using Unrepresentative Corpora in Arabic Natural Language Processing",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saied Alshahrani",
      "Norah Alshahrani",
      "Soumyabrata Dey",
      "Jeanna Matthews"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.20": {
    "title": "Octopus: A Multitask Model and Toolkit for Arabic Natural Language Generation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "AbdelRahim Elmadany",
      "El Moatez Billah Nagoudi",
      "Muhammad Abdul-Mageed"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.21": {
    "title": "AlGhafa Evaluation Benchmark for Arabic Language Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ebtesam Almazrouei",
      "Ruxandra Cojocaru",
      "Michele Baldo",
      "Quentin Malartic",
      "Hamza Alobeidli",
      "Daniele Mazzotta",
      "Guilherme Penedo",
      "Giulia Campesan",
      "Mugariya Farooq",
      "Maitha Alhammadi",
      "Julien Launay",
      "Badreddine Noune"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.22": {
    "title": "ArBanking77: Intent Detection Neural Model and a New Dataset in Modern and Dialectical Arabic",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mustafa Jarrar",
      "Ahmet Birim",
      "Mohammed Khalilia",
      "Mustafa Erden",
      "Sana Ghanem"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.23": {
    "title": "ArabIcros: AI-Powered Arabic Crossword Puzzle Generation for Educational Applications",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kamyar Zeinalipour",
      "Mohamed Saad",
      "Marco Maggini",
      "Marco Gori"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.24": {
    "title": "Machine Translation of Omani Arabic Dialect from Social Media",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Khoula Al-Kharusi",
      "Abdurahman AAlAbdulsalam"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.25": {
    "title": "Arabic Fine-Grained Entity Recognition",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haneen Liqreina",
      "Mustafa Jarrar",
      "Mohammed Khalilia",
      "Ahmed El-Shangiti",
      "Muhammad Abdul-Mageed"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.26": {
    "title": "Investigating Zero-shot Cross-lingual Language Understanding for Arabic",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zaid Alyafeai",
      "Moataz Ahmed"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.27": {
    "title": "Evaluating ChatGPT and Bard AI on Arabic Sentiment Analysis",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abdulmohsen Al-Thubaity",
      "Sakhar Alkhereyf",
      "Hanan Murayshid",
      "Nouf Alshalawi",
      "Maha Omirah",
      "Raghad Alateeq",
      "Rawabi Almutairi",
      "Razan Alsuwailem",
      "Manal Alhassoun",
      "Imaan Alkhanen"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.28": {
    "title": "In-Context Meta-Learning vs. Semantic Score-Based Similarity: A Comparative Study in Arabic Short Answer Grading",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Menna Fateen",
      "Tsunenori Mina"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.29": {
    "title": "SALMA: Arabic Sense-Annotated Corpus and WSD Benchmarks",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mustafa Jarrar",
      "Sanad Malaysha",
      "Tymaa Hammouda",
      "Mohammed Khalilia"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.30": {
    "title": "Arabic dialect identification: An in-depth error analysis on the MADAR parallel corpus",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Helene Olsen",
      "Samia Touileb",
      "Erik Velldal"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.31": {
    "title": "Arabic Dialect Identification under Scrutiny: Limitations of Single-label Classification",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amr Keleg",
      "Walid Magdy"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.32": {
    "title": "Arabic Topic Classification in the Generative and AutoML Era",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Doha Albared",
      "Hadi Hamoud",
      "Fadi Zaraket"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.33": {
    "title": "On Enhancing Fine-Tuning for Pre-trained Language Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abir Betka",
      "Zeyd Ferhat",
      "Riyadh Barka",
      "Selma Boutiba",
      "Zineddine Kahhoul",
      "Tiar Lakhdar",
      "Ahmed Abdelali",
      "Habiba Dahmani"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.34": {
    "title": "Multi-Parallel Corpus of North Levantine Arabic",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mateusz Krubiński",
      "Hashem Sellat",
      "Shadi Saleh",
      "Adam Pospíšil",
      "Petr Zemánek",
      "Pavel Pecina"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.35": {
    "title": "Simplify: Automatic Arabic Sentence Simplification using Word Embeddings",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yousef SalahEldin",
      "Caroline Sabty"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.36": {
    "title": "Offensive Language Detection in Arabizi",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Imene Bensalem",
      "Meryem Mout",
      "Paolo Rosso"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.37": {
    "title": "Yet Another Model for Arabic Dialect Identification",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ajinkya Kulkarni",
      "Hanan Aldarmaki"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.38": {
    "title": "VoxArabica: A Robust Dialect-Aware Arabic Speech Recognition System",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abdul Waheed",
      "Bashar Talafha",
      "Peter Sullivan",
      "AbdelRahim Elmadany",
      "Muhammad Abdul-Mageed"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.39": {
    "title": "KSAA-RD Shared Task: Arabic Reverse Dictionary",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rawan Al-Matham",
      "Waad Alshammari",
      "Abdulrahman AlOsaimy",
      "Sarah Alhumoud",
      "Asma Wazrah",
      "Afrah Altamimi",
      "Halah Alharbi",
      "Abdullah Alaifi"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.40": {
    "title": "UWB at Arabic Reverse Dictionary shared task: Computing the meaning of a gloss",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stephen Taylor"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.41": {
    "title": "Qamosy at Arabic Reverse Dictionary shared task: Semi Decoder Architecture for Reverse Dictionary with SBERT Encoder",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Serry Sibaee",
      "Samar Ahmad",
      "Ibrahim Khurfan",
      "Vian Sabeeh",
      "Ahmed Bahaaulddin",
      "Hanan Belhaj",
      "Abdullah Alharbi"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.42": {
    "title": "Abed at KSAA-RD Shared Task: Enhancing Arabic Word Embedding with Modified BERT Multilingual",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abdelrahim Qaddoumi"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.43": {
    "title": "Rosetta Stone at KSAA-RD Shared Task: A Hop From Language Modeling To Word–Definition Alignment",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ahmed Elbakry",
      "Mohamed Gabr",
      "Muhammad ElNokrashy",
      "Badr AlKhamissi"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.44": {
    "title": "ArAIEval Shared Task: Persuasion Techniques and Disinformation Detection in Arabic Text",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maram Hasanain",
      "Firoj Alam",
      "Hamdy Mubarak",
      "Samir Abdaljalil",
      "Wajdi Zaghouani",
      "Preslav Nakov",
      "Giovanni Da San Martino",
      "Abed Freihat"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.45": {
    "title": "DetectiveRedasers at ArAIEval Shared Task: Leveraging Transformer Ensembles for Arabic Deception Detection",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bryan Tuck",
      "Fatima Qachfar",
      "Dainis Boumber",
      "Rakesh Verma"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.46": {
    "title": "HTE at ArAIEval Shared Task: Integrating Content Type Information in Binary Persuasive Technique Detection",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Khaldi Hadjer",
      "Taqiy Bouklouha"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.47": {
    "title": "USTHB at ArAIEval'23 Shared Task: Disinformation Detection System based on Linguistic Feature Concatenation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohamed Lichouri",
      "Khaled Lounnas",
      "Aicha Zitouni",
      "Houda Latrache",
      "Rachida Djeradi"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.48": {
    "title": "Mavericks at ArAIEval Shared Task: Towards a Safer Digital Space - Transformer Ensemble Models Tackling Deception and Persuasion",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sudeep Mangalvedhekar",
      "Kshitij Deshpande",
      "Yash Patwardhan",
      "Vedant Deshpande",
      "Ravindra Murumkar"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.49": {
    "title": "KnowTellConvince at ArAIEval Shared Task: Disinformation and Persuasion Detection in Arabic using Similar and Contrastive Representation Alignment",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hariram Veeramani",
      "Surendrabikram Thapa",
      "Usman Naseem"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.50": {
    "title": "PTUK-HULAT at ArAIEval Shared Task Fine-tuned Distilbert to Predict Disinformative Tweets",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Areej Jaber",
      "Paloma Martinez"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.51": {
    "title": "AraDetector at ArAIEval Shared Task: An Ensemble of Arabic-specific pre-trained BERT and GPT-4 for Arabic Disinformation Detection",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ahmed Bahaaulddin",
      "Vian Sabeeh",
      "Hanan Belhaj",
      "Serry Sibaee",
      "Samar Ahmad",
      "Ibrahim Khurfan",
      "Abdullah Alharbi"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.52": {
    "title": "rematchka at ArAIEval Shared Task: Prefix-Tuning & Prompt-tuning for Improved Detection of Propaganda and Disinformation in Arabic Social Media Content",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Reem Abdel-Salam"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.53": {
    "title": "Itri Amigos at ArAIEval Shared Task: Transformer vs. Compression-Based Models for Persuasion Techniques and Disinformation Detection",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jehad Oumer",
      "Nouman Ahmed",
      "Natalia Manrique"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.54": {
    "title": "ReDASPersuasion at ArAIEval Shared Task: Multilingual and Monolingual Models For Arabic Persuasion Detection",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fatima Qachfar",
      "Rakesh Verma"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.55": {
    "title": "UL & UM6P at ArAIEval Shared Task: Transformer-based model for Persuasion Techniques and Disinformation detection in Arabic",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Salima Lamsiyah",
      "Abdelkader Mahdaouy",
      "Hamza Alami",
      "Ismail Berrada",
      "Christoph Schommer"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.56": {
    "title": "AAST-NLP at ArAIEval Shared Task: Tackling Persuasion technique and Disinformation Detection using Pre-Trained Language Models On Imbalanced Datasets",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ahmed El-Sayed",
      "Omar Nasr",
      "Noureldin Elmadany"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.57": {
    "title": "PD-AR at ArAIEval Shared Task: A BERT-Centric Approach to Tackle Arabic Disinformation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pritam Deka",
      "Ashwathy Revi"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.58": {
    "title": "Nexus at ArAIEval Shared Task: Fine-Tuning Arabic Language Models for Propaganda and Disinformation Detection",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunze Xiao",
      "Firoj Alam"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.59": {
    "title": "Frank at ArAIEval Shared Task: Arabic Persuasion and Disinformation: The Power of Pretrained Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dilshod Azizov",
      "Jiyong Li",
      "Shangsong Liang"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.60": {
    "title": "Raphael at ArAIEval Shared Task: Understanding Persuasive Language and Tone, an LLM Approach",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Utsav Shukla",
      "Manan Vyas",
      "Shailendra Tiwari"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.61": {
    "title": "Legend at ArAIEval Shared Task: Persuasion Technique Detection using a Language-Agnostic Text Representation Model",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Olumide Ojo",
      "Olaronke Adebanji",
      "Hiram Calvo",
      "Damian Dieke",
      "Olumuyiwa Ojo",
      "Seye Akinsanya",
      "Tolulope Abiola",
      "Anna Feldman"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.62": {
    "title": "NADI 2023: The Fourth Nuanced Arabic Dialect Identification Shared Task",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Abdul-Mageed",
      "AbdelRahim Elmadany",
      "Chiyu Zhang",
      "El Moatez Billah Nagoudi",
      "Houda Bouamor",
      "Nizar Habash"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.63": {
    "title": "DialectNLU at NADI 2023 Shared Task: Transformer Based Multitask Approach Jointly Integrating Dialect and Machine Translation Tasks in Arabic",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hariram Veeramani",
      "Surendrabikram Thapa",
      "Usman Naseem"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.64": {
    "title": "UoT at NADI 2023 shared task: Automatic Arabic Dialect Identification is Made Possible",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abduslam F A Nwesri",
      "Nabila A S Shinbir",
      "Hassan Ebrahem"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.65": {
    "title": "SANA at NADI 2023 shared task: Ensemble of Layer-Wise BERT-based models for Dialectal Arabic Identification",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nada Almarwani",
      "Samah Aloufi"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.66": {
    "title": "ISL-AAST at NADI 2023 shared task: Enhancing Arabic Dialect Identification in the Era of Globalization and Technological Progress",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shorouk Adel",
      "Noureldin Elmadany"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.67": {
    "title": "Frank at NADI 2023 Shared Task: Trio-Based Ensemble Approach for Arabic Dialect Identification",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dilshod Azizov",
      "Jiyong Li",
      "Shangsong Liang"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.68": {
    "title": "NLPeople at NADI 2023 Shared Task: Arabic Dialect Identification with Augmented Context and Multi-Stage Tuning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohab Elkaref",
      "Movina Moses",
      "Shinnosuke Tanaka",
      "James Barry",
      "Geeth Mel"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.69": {
    "title": "USTHB at NADI 2023 shared task: Exploring Preprocessing and Feature Engineering Strategies for Arabic Dialect Identification",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohamed Lichouri",
      "Khaled Lounnas",
      "Aicha Zitouni",
      "Houda Latrache",
      "Rachida Djeradi"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.70": {
    "title": "rematchka at NADI 2023 shared task: Parameter Efficient tuning for Dialect Identification and Dialect Machine Translation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Reem Abdel-Salam"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.71": {
    "title": "UniManc at NADI 2023 Shared Task: A Comparison of Various T5-based Models for Translating Arabic Dialectical Text to Modern Standard Arabic",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abdullah Khered",
      "Ingy Abdelhalim",
      "Nadine Abdelhalim",
      "Ahmed Soliman",
      "Riza Batista-Navarro"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.72": {
    "title": "IUNADI at NADI 2023 shared task: Country-level Arabic Dialect Classification in Tweets for the Shared Task NADI 2023",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yash Hatekar",
      "Muhammad Abdo"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.73": {
    "title": "The Helsinki-NLP Submissions at NADI 2023 Shared Task: Walking the Baseline",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yves Scherrer",
      "Aleksandra Miletić",
      "Olli Kuparinen"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.74": {
    "title": "Mavericks at NADI 2023 Shared Task: Unravelling Regional Nuances through Dialect Identification using Transformer-based Approach",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vedant Deshpande",
      "Yash Patwardhan",
      "Kshitij Deshpande",
      "Sudeep Mangalvedhekar",
      "Ravindra Murumkar"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.75": {
    "title": "ANLP-RG at NADI 2023 shared task: Machine Translation of Arabic Dialects: A Comparative Study of Transformer Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wiem Derouich",
      "Sameh Kchaou",
      "Rahma Boujelbane"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.76": {
    "title": "Qur'an QA 2023 Shared Task: Overview of Passage Retrieval and Reading Comprehension Tasks over the Holy Qur'an",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rana Malhas",
      "Watheq Mansour",
      "Tamer Elsayed"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.77": {
    "title": "AHJL at Qur'an QA 2023 Shared Task: Enhancing Passage Retrieval using Sentence Transformer and Translation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hessa Alawwad",
      "Lujain Alawwad",
      "Jamilah Alharbi",
      "Abdullah Alharbi"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.78": {
    "title": "LowResContextQA at Qur'an QA 2023 Shared Task: Temporal and Sequential Representation Augmented Question Answering Span Detection in Arabic",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hariram Veeramani",
      "Surendrabikram Thapa",
      "Usman Naseem"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.79": {
    "title": "GYM at Qur'an QA 2023 Shared Task: Multi-Task Transfer Learning for Quranic Passage Retrieval and Question Answering with Large Language Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ghazaleh Mahmoudi",
      "Yeganeh Morshedzadeh",
      "Sauleh Eetemadi"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.80": {
    "title": "LKAU23 at Qur'an QA 2023: Using Transformer Models for Retrieving Passages and Finding Answers to Questions from the Qur'an",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sarah Alnefaie",
      "Abdullah Alsaleh",
      "Eric Atwell",
      "Mohammad Alsalka",
      "Abdulrahman Altahhan"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.81": {
    "title": "TCE at Qur'an QA 2023 Shared Task: Low Resource Enhanced Transformer-based Ensemble Approach for Qur'anic QA",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammed Elkomy",
      "Amany Sarhan"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.82": {
    "title": "Al-Jawaab at Qur'an QA 2023 Shared Task: Exploring Embeddings and GPT Models for Passage Retrieval and Reading Comprehension",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abdulrezzak Zekiye",
      "Fadi Amroush"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.83": {
    "title": "WojoodNER 2023: The First Arabic Named Entity Recognition Shared Task",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mustafa Jarrar",
      "Muhammad Abdul-Mageed",
      "Mohammed Khalilia",
      "Bashar Talafha",
      "AbdelRahim Elmadany",
      "Nagham Hamad",
      "Alaa’ Omar"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.84": {
    "title": "ELYADATA at WojoodNER Shared Task: Data and Model-centric Approaches for Arabic Flat and Nested NER",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Imen Laouirine",
      "Haroun Elleuch",
      "Fethi Bougares"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.85": {
    "title": "Lotus at WojoodNER Shared Task: Multilingual Transformers: Unveiling Flat and Nested Entity Recognition",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiyong Li",
      "Dilshod Azizov",
      "Hilal AlQuabeh",
      "Shangsong Liang"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.86": {
    "title": "AlexU-AIC at WojoodNER shared task: Sequence Labeling vs MRC and SWA for Arabic Named Entity Recognition",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shereen Elkordi",
      "Noha Adly",
      "Marwan Torki"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.87": {
    "title": "UM6P & UL at WojoodNER shared task: Improving Multi-Task Learning for Flat and Nested Arabic Named Entity Recognition",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abdelkader Mahdaouy",
      "Salima Lamsiyah",
      "Hamza Alami",
      "Christoph Schommer",
      "Ismail Berrada"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.88": {
    "title": "AlphaBrains at WojoodNER shared task: Arabic Named Entity Recognition by Using Character-based Context-Sensitive Word Representations",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Toqeer Ehsan",
      "Amjad Ali",
      "Ala Al-Fuqaha"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.89": {
    "title": "LIPN at WojoodNER shared task: A Span-Based Approach for Flat and Nested Arabic Named Entity Recognition",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Niama Elkhbir",
      "Urchade Zaratiana",
      "Nadi Tomeh",
      "Thierry Charnois"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.90": {
    "title": "Alex-U 2023 NLP at WojoodNER shared task: AraBINDER (Bi-encoder for Arabic Named Entity Recognition)",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mariam Hussein",
      "Sarah Khaled",
      "Marwan Torki",
      "Nagwa El-Makky"
    ]
  },
  "https://aclanthology.org/2023.arabicnlp-1.91": {
    "title": "El-Kawaref at WojoodNER shared task: StagedNER for Arabic Named Entity Recognition",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nehal Elkaref",
      "Mohab Elkaref"
    ]
  },
  "https://aclanthology.org/2023.argmining-1.1": {
    "title": "Detecting Argumentative Fallacies in the Wild: Problems and Limitations of Large Language Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ramon Ruiz-Dolz",
      "John Lawrence"
    ]
  },
  "https://aclanthology.org/2023.argmining-1.2": {
    "title": "Using Masked Language Model Probabilities of Connectives for Stance Detection in English Discourse",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Regina Stodden",
      "Laura Kallmeyer",
      "Lea Kawaletz",
      "Heidrun Dorgeloh"
    ]
  },
  "https://aclanthology.org/2023.argmining-1.3": {
    "title": "Teach Me How to Argue: A Survey on NLP Feedback Systems in Argumentation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Camelia Guerraoui",
      "Paul Reisert",
      "Naoya Inoue",
      "Farjana Sultana Mim",
      "Keshav Singh",
      "Jungmin Choi",
      "Irfan Robbani",
      "Shoichi Naito",
      "Wenzhi Wang",
      "Kentaro Inui"
    ]
  },
  "https://aclanthology.org/2023.argmining-1.4": {
    "title": "Constituency Tree Representation for Argument Unit Recognition",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel Guilluy",
      "Florian Mehats",
      "Billal Chouli"
    ]
  },
  "https://aclanthology.org/2023.argmining-1.5": {
    "title": "Stance-Aware Re-Ranking for Non-factual Comparative Queries",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jan Heinrich Reimer",
      "Alexander Bondarenko",
      "Maik Fröbe",
      "Matthias Hagen"
    ]
  },
  "https://aclanthology.org/2023.argmining-1.6": {
    "title": "Legal Argument Extraction from Court Judgements using Integer Linear Programming",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Basit Ali",
      "Sachin Pawar",
      "Girish Palshikar",
      "Anindita Sinha Banerjee",
      "Dhirendra Singh"
    ]
  },
  "https://aclanthology.org/2023.argmining-1.7": {
    "title": "Argument Detection in Student Essays under Resource Constraints",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Omid Kashefi",
      "Sophia Chan",
      "Swapna Somasundaran"
    ]
  },
  "https://aclanthology.org/2023.argmining-1.8": {
    "title": "Towards Fine-Grained Argumentation Strategy Analysis in Persuasive Essays",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Robin Schaefer",
      "René Knaebel",
      "Manfred Stede"
    ]
  },
  "https://aclanthology.org/2023.argmining-1.9": {
    "title": "Dimensionality Reduction for Machine Learning-based Argument Mining",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrés Segura-Tinoco",
      "Iván Cantador"
    ]
  },
  "https://aclanthology.org/2023.argmining-1.10": {
    "title": "On the Impact of Reconstruction and Context for Argument Prediction in Natural Debate",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zlata Kikteva",
      "Alexander Trautsch",
      "Patrick Katzer",
      "Mirko Oest",
      "Steffen Herbold",
      "Annette Hautli-Janisz"
    ]
  },
  "https://aclanthology.org/2023.argmining-1.11": {
    "title": "Unsupervised argument reframing with a counterfactual-based approach",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Philipp Heinisch",
      "Dimitry Mindlin",
      "Philipp Cimiano"
    ]
  },
  "https://aclanthology.org/2023.argmining-1.12": {
    "title": "Overview of ImageArg-2023: The First Shared Task in Multimodal Argument Mining",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhexiong Liu",
      "Mohamed Elaraby",
      "Yang Zhong",
      "Diane Litman"
    ]
  },
  "https://aclanthology.org/2023.argmining-1.13": {
    "title": "IUST at ImageArg: The First Shared Task in Multimodal Argument Mining",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Melika Nobakhtian",
      "Ghazal Zamaninejad",
      "Erfan Moosavi Monazzah",
      "Sauleh Eetemadi"
    ]
  },
  "https://aclanthology.org/2023.argmining-1.14": {
    "title": "TILFA: A Unified Framework for Text, Image, and Layout Fusion in Argument Mining",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qing Zong",
      "Zhaowei Wang",
      "Baixuan Xu",
      "Tianshi Zheng",
      "Haochen Shi",
      "Weiqi Wang",
      "Yangqiu Song",
      "Ginny Wong",
      "Simon See"
    ]
  },
  "https://aclanthology.org/2023.argmining-1.15": {
    "title": "A General Framework for Multimodal Argument Persuasiveness Classification of Tweets",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Soltani",
      "Julia Romberg"
    ]
  },
  "https://aclanthology.org/2023.argmining-1.16": {
    "title": "Webis @ ImageArg 2023: Embedding-based Stance and Persuasiveness Classification",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Islam Torky",
      "Simon Ruth",
      "Shashi Sharma",
      "Mohamed Salama",
      "Krishna Chaitanya",
      "Tim Gollub",
      "Johannes Kiesel",
      "Benno Stein"
    ]
  },
  "https://aclanthology.org/2023.argmining-1.17": {
    "title": "GC-Hunter at ImageArg Shared Task: Multi-Modal Stance and Persuasiveness Learning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Shokri",
      "Sarah Ita Levitan"
    ]
  },
  "https://aclanthology.org/2023.argmining-1.18": {
    "title": "Argumentative Stance Prediction: An Exploratory Study on Multimodality and Few-Shot Learning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arushi Sharma",
      "Abhibha Gupta",
      "Maneesh Bilalpur"
    ]
  },
  "https://aclanthology.org/2023.argmining-1.19": {
    "title": "SPLIT: Stance and Persuasion Prediction with Multi-modal on Image and Textual Information",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Zhang",
      "Shaojun Yu",
      "Xuan Li",
      "Jia Geng",
      "Zhiyuan Zheng",
      "Joyce Ho"
    ]
  },
  "https://aclanthology.org/2023.argmining-1.20": {
    "title": "Semantists at ImageArg-2023: Exploring Cross-modal Contrastive and Ensemble Models for Multimodal Stance and Persuasiveness Classification",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kanagasabai Rajaraman",
      "Hariram Veeramani",
      "Saravanan Rajamanickam",
      "Adam Maciej Westerski",
      "Jung-Jae Kim"
    ]
  },
  "https://aclanthology.org/2023.argmining-1.21": {
    "title": "Overview of PragTag-2023: Low-Resource Multi-Domain Pragmatic Tagging of Peer Reviews",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nils Dycke",
      "Ilia Kuznetsov",
      "Iryna Gurevych"
    ]
  },
  "https://aclanthology.org/2023.argmining-1.22": {
    "title": "CATALPA_EduNLP at PragTag-2023",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuning Ding",
      "Marie Bexte",
      "Andrea Horbach"
    ]
  },
  "https://aclanthology.org/2023.argmining-1.23": {
    "title": "DeepBlueAI at PragTag-2023:Ensemble-based Text Classification Approaches under Limited Data Resources",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhipeng Luo",
      "Jiahui Wang",
      "Yihao Guo"
    ]
  },
  "https://aclanthology.org/2023.argmining-1.24": {
    "title": "MILAB at PragTag-2023: Enhancing Cross-Domain Generalization through Data Augmentation with Reduced Uncertainty",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yoonsang Lee",
      "Dongryeol Lee",
      "Kyomin Jung"
    ]
  },
  "https://aclanthology.org/2023.argmining-1.25": {
    "title": "NUS-IDS at PragTag-2023: Improving Pragmatic Tagging of Peer Reviews through Unlabeled Data",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sujatha Das Gollapalli",
      "Yixin Huang",
      "See-Kiong Ng"
    ]
  },
  "https://aclanthology.org/2023.argmining-1.26": {
    "title": "SuryaKiran at PragTag 2023 - Benchmarking Domain Adaptation using Masked Language Modeling in Natural Language Processing For Specialized Data",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kunal Suri",
      "Prakhar Mishra",
      "Albert Nanda"
    ]
  },
  "https://aclanthology.org/2023.blackboxnlp-1.1": {
    "title": "Knowledge-Grounded Natural Language Recommendation Explanation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anthony Colas",
      "Jun Araki",
      "Zhengyu Zhou",
      "Bingqing Wang",
      "Zhe Feng"
    ]
  },
  "https://aclanthology.org/2023.blackboxnlp-1.2": {
    "title": "Emergent Linear Representations in World Models of Self-Supervised Sequence Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Neel Nanda",
      "Andrew Lee",
      "Martin Wattenberg"
    ]
  },
  "https://aclanthology.org/2023.blackboxnlp-1.3": {
    "title": "Explaining Data Patterns in Natural Language with Language Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chandan Singh",
      "John X. Morris",
      "Jyoti Aneja",
      "Alexander Rush",
      "Jianfeng Gao"
    ]
  },
  "https://aclanthology.org/2023.blackboxnlp-1.4": {
    "title": "Probing Quantifier Comprehension in Large Language Models: Another Example of Inverse Scaling",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akshat Gupta"
    ]
  },
  "https://aclanthology.org/2023.blackboxnlp-1.5": {
    "title": "Disentangling the Linguistic Competence of Privacy-Preserving BERT",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefan Arnold",
      "Nils Kemmerzell",
      "Annika Schreiner"
    ]
  },
  "https://aclanthology.org/2023.blackboxnlp-1.6": {
    "title": "Honey, Tell Me What's Wrong\", Global Explanation of Textual Discriminative Models through Cooperative Generation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antoine Chaffin",
      "Julien Delaunay"
    ]
  },
  "https://aclanthology.org/2023.blackboxnlp-1.7": {
    "title": "Self-Consistency of Large Language Models under Ambiguity",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Henning Bartsch",
      "Ole Jorgensen",
      "Domenic Rosati",
      "Jason Hoelscher-Obermaier",
      "Jacob Pfau"
    ]
  },
  "https://aclanthology.org/2023.blackboxnlp-1.8": {
    "title": "Character-Level Chinese Backpack Language Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Sun",
      "John Hewitt"
    ]
  },
  "https://aclanthology.org/2023.blackboxnlp-1.9": {
    "title": "Unveiling Multilinguality in Transformer Models: Exploring Language Specificity in Feed-Forward Networks",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sunit Bhattacharya",
      "Ondřej Bojar"
    ]
  },
  "https://aclanthology.org/2023.blackboxnlp-1.10": {
    "title": "Why Bother with Geometry? On the Relevance of Linear Decompositions of Transformer Embeddings",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Timothee Mickus",
      "Raúl Vázquez"
    ]
  },
  "https://aclanthology.org/2023.blackboxnlp-1.11": {
    "title": "Investigating Semantic Subspaces of Transformer Sentence Embeddings through Linear Structural Probing",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dmitry Nikolaev",
      "Sebastian Padó"
    ]
  },
  "https://aclanthology.org/2023.blackboxnlp-1.12": {
    "title": "Causal Abstraction for Chain-of-Thought Reasoning in Arithmetic Word Problems",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juanhe (TJ) Tan"
    ]
  },
  "https://aclanthology.org/2023.blackboxnlp-1.13": {
    "title": "Enhancing Interpretability Using Human Similarity Judgements to Prune Word Embeddings",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Natalia Flechas Manrique",
      "Wanqian Bao",
      "Aurelie Herbelot",
      "Uri Hasson"
    ]
  },
  "https://aclanthology.org/2023.blackboxnlp-1.14": {
    "title": "When Your Language Model Cannot Even Do Determiners Right: Probing for Anti-Presuppositions and the Maximize Presupposition! Principle",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Judith Sieker",
      "Sina Zarrieß"
    ]
  },
  "https://aclanthology.org/2023.blackboxnlp-1.15": {
    "title": "Introducing VULCAN: A Visualization Tool for Understanding Our Models and Data by Example",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonas Groschwitz"
    ]
  },
  "https://aclanthology.org/2023.blackboxnlp-1.16": {
    "title": "The Self-Contained Negation Test Set",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Kletz",
      "Pascal Amsili",
      "Marie Candito"
    ]
  },
  "https://aclanthology.org/2023.blackboxnlp-1.17": {
    "title": "Investigating the Effect of Discourse Connectives on Transformer Surprisal: Language Models Understand Connectives, Even So They Are Surprised",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Cong",
      "Emmanuele Chersoni",
      "Yu-Yin Hsu",
      "Philippe Blache"
    ]
  },
  "https://aclanthology.org/2023.blackboxnlp-1.18": {
    "title": "METAPROBE: A Representation- and Task-Agnostic Probe",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yichu Zhou",
      "Vivek Srikumar"
    ]
  },
  "https://aclanthology.org/2023.blackboxnlp-1.19": {
    "title": "How Much Consistency Is Your Accuracy Worth?",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jacob K. Johnson",
      "Ana Marasović"
    ]
  },
  "https://aclanthology.org/2023.blackboxnlp-1.20": {
    "title": "Investigating the Encoding of Words in BERT's Neurons Using Feature Textualization",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tanja Baeumel",
      "Soniya Vijayakumar",
      "Josef van Genabith",
      "Guenter Neumann",
      "Simon Ostermann"
    ]
  },
  "https://aclanthology.org/2023.blackboxnlp-1.21": {
    "title": "Evaluating Transformer's Ability to Learn Mildly Context-Sensitive Languages",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shunjie Wang",
      "Shane Steinert-Threlkeld"
    ]
  },
  "https://aclanthology.org/2023.blackboxnlp-1.22": {
    "title": "Layered Bias: Interpreting Bias in Pretrained Large Language Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nirmalendu Prakash",
      "Roy Ka-Wei Lee"
    ]
  },
  "https://aclanthology.org/2023.blackboxnlp-1.23": {
    "title": "Not Wacky vs. Definitely Wacky: A Study of Scalar Adverbs in Pretrained Language Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Isabelle Lorge",
      "Janet B. Pierrehumbert"
    ]
  },
  "https://aclanthology.org/2023.blackboxnlp-1.24": {
    "title": "Rigorously Assessing Natural Language Explanations of Neurons",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Huang",
      "Atticus Geiger",
      "Karel D’Oosterlinck",
      "Zhengxuan Wu",
      "Christopher Potts"
    ]
  },
  "https://aclanthology.org/2023.blackboxnlp-1.25": {
    "title": "NPIs Aren't Exactly Easy: Variation in Licensing across Large Language Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Deanna DeCarlo",
      "William Palmer",
      "Michael Wilson",
      "Bob Frank"
    ]
  },
  "https://aclanthology.org/2023.blackboxnlp-1.26": {
    "title": "Memory Injections: Correcting Multi-Hop Reasoning Failures During Inference in Transformer-Based Language Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mansi Sakarvadia",
      "Aswathy Ajith",
      "Arham Khan",
      "Daniel Grzenda",
      "Nathaniel Hudson",
      "André Bauer",
      "Kyle Chard",
      "Ian Foster"
    ]
  },
  "https://aclanthology.org/2023.blackboxnlp-1.27": {
    "title": "Systematic Generalization by Finetuning? Analyzing Pretrained Language Models Using Constituency Tests",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aishik Chakraborty",
      "Jackie CK Cheung",
      "Timothy J. O’Donnell"
    ]
  },
  "https://aclanthology.org/2023.blackboxnlp-1.28": {
    "title": "On Quick Kisses and How to Make Them Count: A Study on Event Construal in Light Verb Constructions with BERT",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenxin Liu",
      "Emmanuele Chersoni"
    ]
  },
  "https://aclanthology.org/2023.blackboxnlp-1.29": {
    "title": "Identifying and Adapting Transformer-Components Responsible for Gender Bias in an English Language Model",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhijith Chintam",
      "Rahel Beloch",
      "Willem Zuidema",
      "Michael Hanna",
      "Oskar van der Wal"
    ]
  },
  "https://aclanthology.org/2023.calcs-1.1": {
    "title": "TongueSwitcher: Fine-Grained Identification of German-English Code-Switching",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Igor Sterner",
      "Simone Teufel"
    ]
  },
  "https://aclanthology.org/2023.calcs-1.2": {
    "title": "Towards Real-World Streaming Speech Translation for Code-Switched Speech",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Belen Alastruey",
      "Matthias Sperber",
      "Christian Gollan",
      "Dominic Telaar",
      "Tim Ng",
      "Aashish Agarwal"
    ]
  },
  "https://aclanthology.org/2023.calcs-1.3": {
    "title": "Language Preference for Expression of Sentiment for Nepali-English Bilingual Speakers on Social Media",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Niraj Pahari",
      "Kazutaka Shimada"
    ]
  },
  "https://aclanthology.org/2023.calcs-1.4": {
    "title": "Text-Derived Language Identity Incorporation for End-to-End Code-Switching Speech Recognition",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qinyi Wang",
      "Haizhou Li"
    ]
  },
  "https://aclanthology.org/2023.calcs-1.5": {
    "title": "Prompting Multilingual Large Language Models to Generate Code-Mixed Texts: The Case of South East Asian Languages",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheng Xin Yong",
      "Ruochen Zhang",
      "Jessica Forde",
      "Skyler Wang",
      "Arjun Subramonian",
      "Holy Lovenia",
      "Samuel Cahyawijaya",
      "Genta Winata",
      "Lintang Sutawika",
      "Jan Christian Blaise Cruz",
      "Yin Lin Tan",
      "Long Phan",
      "Long Phan",
      "Rowena Garcia",
      "Thamar Solorio",
      "Alham Aji"
    ]
  },
  "https://aclanthology.org/2023.calcs-1.6": {
    "title": "CONFLATOR: Incorporating Switching Point based Rotatory Positional Encodings for Code-Mixed Language Modeling",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohsin Mohammed",
      "Sai Kandukuri",
      "Neeharika Gupta",
      "Parth Patwa",
      "Anubhab Chatterjee",
      "Vinija Jain",
      "Aman Chadha",
      "Amitava Das"
    ]
  },
  "https://aclanthology.org/2023.calcs-1.7": {
    "title": "Unified Model for Code-Switching Speech Recognition and Language Identification Based on Concatenated Tokenizer",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kunal Dhawan",
      "KDimating Rekesh",
      "Boris Ginsburg"
    ]
  },
  "https://aclanthology.org/2023.calcs-1.8": {
    "title": "Multilingual self-supervised speech representations improve the speech recognition of low-resource African languages with codeswitching",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tolulope Ogunremi",
      "Christopher Manning",
      "Dan Jurafsky"
    ]
  },
  "https://aclanthology.org/2023.conll-1.1": {
    "title": "Can Language Models Be Tricked by Language Illusions? Easier with Syntax, Harder with Semantics",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhan Zhang",
      "Edward Gibson",
      "Forrest Davis"
    ]
  },
  "https://aclanthology.org/2023.conll-1.2": {
    "title": "ToMChallenges: A Principle-Guided Dataset and Diverse Evaluation Tasks for Exploring Theory of Mind",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaomeng Ma",
      "Lingyu Gao",
      "Qihui Xu"
    ]
  },
  "https://aclanthology.org/2023.conll-1.3": {
    "title": "The Zipfian Challenge: Learning the statistical fingerprint of natural languages",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christian Bentz"
    ]
  },
  "https://aclanthology.org/2023.conll-1.4": {
    "title": "On the Effects of Structural Modeling for Neural Semantic Parsing",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Zhang",
      "Shizhu He",
      "Kang Liu",
      "Jun Zhao"
    ]
  },
  "https://aclanthology.org/2023.conll-1.5": {
    "title": "Humans and language models diverge when predicting repeating text",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditya Vaidya",
      "Javier Turek",
      "Alexander Huth"
    ]
  },
  "https://aclanthology.org/2023.conll-1.6": {
    "title": "Investigating the Nature of Disagreements on Mid-Scale Ratings: A Case Study on the Abstractness-Concreteness Continuum",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Urban Knupleš",
      "Diego Frassinelli",
      "Sabine Schulte im Walde"
    ]
  },
  "https://aclanthology.org/2023.conll-1.7": {
    "title": "ArchBERT: Bi-Modal Understanding of Neural Architectures and Natural Languages",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Akbari",
      "Saeed Ranjbar Alvar",
      "Behnam Kamranian",
      "Amin Banitalebi-Dehkordi",
      "Yong Zhang"
    ]
  },
  "https://aclanthology.org/2023.conll-1.8": {
    "title": "A Comparative Study on Textual Saliency of Styles from Eye Tracking, Annotations, and Language Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karin de Langis",
      "Dongyeop Kang"
    ]
  },
  "https://aclanthology.org/2023.conll-1.9": {
    "title": "PROPRES: Investigating the Projectivity of Presupposition with Various Triggers and Environments",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daiki Asami",
      "Saku Sugawara"
    ]
  },
  "https://aclanthology.org/2023.conll-1.10": {
    "title": "A Minimal Approach for Natural Language Action Space in Text-based Games",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongwon Ryu",
      "Meng Fang",
      "Gholamreza Haffari",
      "Shirui Pan",
      "Ehsan Shareghi"
    ]
  },
  "https://aclanthology.org/2023.conll-1.11": {
    "title": "Structural Ambiguity and its Disambiguation in Language Model Based Parsers: the Case of Dutch Clause Relativization",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gijs Wijnholds",
      "Michael Moortgat"
    ]
  },
  "https://aclanthology.org/2023.conll-1.12": {
    "title": "On the utility of enhancing BERT syntactic bias with Token Reordering Pretraining",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yassir El Mesbahi",
      "Atif Mahmud",
      "Abbas Ghaddar",
      "Mehdi Rezagholizadeh",
      "Phillippe Langlais",
      "Prasanna Parthasarathi"
    ]
  },
  "https://aclanthology.org/2023.conll-1.13": {
    "title": "Quirk or Palmer: A Comparative Study of Modal Verb Frameworks with Annotated Datasets",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Risako Owan",
      "Maria Gini",
      "Dongyeop Kang"
    ]
  },
  "https://aclanthology.org/2023.conll-1.14": {
    "title": "Quantifying Information of Tokens for Simple and Flexible Simultaneous Machine Translation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "DongHyun Lee",
      "Minkyung Park",
      "Byung-Jun Lee"
    ]
  },
  "https://aclanthology.org/2023.conll-1.15": {
    "title": "Enhancing Code-mixed Text Generation Using Synthetic Data Filtering in Neural Machine Translation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dama Sravani",
      "Radhika Mamidi"
    ]
  },
  "https://aclanthology.org/2023.conll-1.16": {
    "title": "Towards Better Evaluation of Instruction-Following: A Case-Study in Summarization",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ondrej Skopek",
      "Rahul Aralikatte",
      "Sian Gooding",
      "Victor Carbune"
    ]
  },
  "https://aclanthology.org/2023.conll-1.17": {
    "title": "Syntactic Inductive Bias in Transformer Language Models: Especially Helpful for Low-Resource Languages?",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luke Gessler",
      "Nathan Schneider"
    ]
  },
  "https://aclanthology.org/2023.conll-1.18": {
    "title": "Attribution and Alignment: Effects of Local Context Repetition on Utterance Production and Comprehension in Dialogue",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aron Molnar",
      "Jaap Jumelet",
      "Mario Giulianelli",
      "Arabella Sinclair"
    ]
  },
  "https://aclanthology.org/2023.conll-1.19": {
    "title": "The Validity of Evaluation Results: Assessing Concurrence Across Compositionality Benchmarks",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaiser Sun",
      "Adina Williams",
      "Dieuwke Hupkes"
    ]
  },
  "https://aclanthology.org/2023.conll-1.20": {
    "title": "Mind the instructions: a holistic evaluation of consistency and interactions in prompt-based learning",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucas Weber",
      "Elia Bruni",
      "Dieuwke Hupkes"
    ]
  },
  "https://aclanthology.org/2023.conll-1.21": {
    "title": "Med-HALT: Medical Domain Hallucination Test for Large Language Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ankit Pal",
      "Logesh Kumar Umapathi",
      "Malaikannan Sankarasubbu"
    ]
  },
  "https://aclanthology.org/2023.conll-1.22": {
    "title": "Revising with a Backward Glance: Regressions and Skips during Reading as Cognitive Signals for Revision Policies in Incremental Processing",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brielen Madureira",
      "Pelin Çelikkol",
      "David Schlangen"
    ]
  },
  "https://aclanthology.org/2023.conll-1.23": {
    "title": "ChiSCor: A Corpus of Freely-Told Fantasy Stories by Dutch Children for Computational Linguistics and Cognitive Science",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bram van Dijk",
      "Max van Duijn",
      "Suzan Verberne",
      "Marco Spruit"
    ]
  },
  "https://aclanthology.org/2023.conll-1.24": {
    "title": "HNC: Leveraging Hard Negative Captions towards Models with Fine-Grained Visual-Linguistic Comprehension Capabilities",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Esra Dönmez",
      "Pascal Tilli",
      "Hsiu-Yu Yang",
      "Ngoc Thang Vu",
      "Carina Silberer"
    ]
  },
  "https://aclanthology.org/2023.conll-1.25": {
    "title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Max van Duijn",
      "Bram van Dijk",
      "Tom Kouwenhoven",
      "Werner de Valk",
      "Marco Spruit",
      "Peter vanderPutten"
    ]
  },
  "https://aclanthology.org/2023.conll-1.26": {
    "title": "A Block Metropolis-Hastings Sampler for Controllable Energy-based Text Generation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jarad Forristal",
      "Fatemehsadat Mireshghallah",
      "Greg Durrett",
      "Taylor Berg-Kirkpatrick"
    ]
  },
  "https://aclanthology.org/2023.conll-1.27": {
    "title": "How Fragile is Relation Extraction under Entity Replacements?",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiwei Wang",
      "Bryan Hooi",
      "Fei Wang",
      "Yujun Cai",
      "Yuxuan Liang",
      "Wenxuan Zhou",
      "Jing Tang",
      "Manjuan Duan",
      "Muhao Chen"
    ]
  },
  "https://aclanthology.org/2023.conll-1.28": {
    "title": "JaSPICE: Automatic Evaluation Metric Using Predicate-Argument Structures for Image Captioning Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuiga Wada",
      "Kanta Kaneda",
      "Komei Sugiura"
    ]
  },
  "https://aclanthology.org/2023.conll-1.29": {
    "title": "MuLER: Detailed and Scalable Reference-based Evaluation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taelin Karidi",
      "Leshem Choshen",
      "Gal Patel",
      "Omri Abend"
    ]
  },
  "https://aclanthology.org/2023.conll-1.30": {
    "title": "The Impact of Familiarity on Naming Variation: A Study on Object Naming in Mandarin Chinese",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunke He",
      "Xixian Liao",
      "Jialing Liang",
      "Gemma Boleda"
    ]
  },
  "https://aclanthology.org/2023.conll-1.31": {
    "title": "PSST! Prosodic Speech Segmentation with Transformers",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nathan Roll",
      "Calbert Graham",
      "Simon Todd"
    ]
  },
  "https://aclanthology.org/2023.conll-1.32": {
    "title": "Alignment via Mutual Information",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shinjini Ghosh",
      "Yoon Kim",
      "Ramon Fernandez Astudillo",
      "Tahira Naseem",
      "Jacob Andreas"
    ]
  },
  "https://aclanthology.org/2023.conll-1.33": {
    "title": "Challenging the \"One Single Vector per Token\" Assumption",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mathieu Dehouck"
    ]
  },
  "https://aclanthology.org/2023.conll-1.34": {
    "title": "Strategies to Improve Low-Resource Agglutinative Languages Morphological Inflection",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gulinigeer Abudouwaili",
      "Wayit Ablez",
      "Kahaerjiang Abiderexiti",
      "Aishan Wumaier",
      "Nian Yi"
    ]
  },
  "https://aclanthology.org/2023.conll-1.35": {
    "title": "Exploring Transformers as Compact, Data-efficient Language Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Clayton Fields",
      "Casey Kennington"
    ]
  },
  "https://aclanthology.org/2023.conll-1.36": {
    "title": "Tree-shape Uncertainty for Analyzing the Inherent Branching Bias of Unsupervised Parsing Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taiga Ishii",
      "Yusuke Miyao"
    ]
  },
  "https://aclanthology.org/2023.conll-1.37": {
    "title": "Future Lens: Anticipating Subsequent Tokens from a Single Hidden State",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Koyena Pal",
      "Jiuding Sun",
      "Andrew Yuan",
      "Byron Wallace",
      "David Bau"
    ]
  },
  "https://aclanthology.org/2023.conll-1.38": {
    "title": "Cross-Document Event Coreference Resolution: Instruct Humans or Instruct GPT?",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jin Zhao",
      "Nianwen Xue",
      "Bonan Min"
    ]
  },
  "https://aclanthology.org/2023.conll-1.39": {
    "title": "Implications of Annotation Artifacts in Edge Probing Test Datasets",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sagnik Ray Choudhury",
      "Jushaan Kalra"
    ]
  },
  "https://aclanthology.org/2023.conll-1.40": {
    "title": "REFER: An End-to-end Rationale Extraction Framework for Explanation Regularization",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Reza Ghasemi Madani",
      "Pasquale Minervini"
    ]
  },
  "https://aclanthology.org/2023.crac-main.1": {
    "title": "Filling in the Gaps: Efficient Event Coreference Resolution using Graph Autoencoder Networks",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Loic De Langhe",
      "Orphee De Clercq",
      "Veronique Hoste"
    ]
  },
  "https://aclanthology.org/2023.crac-main.2": {
    "title": "CAW-coref: Conjunction-Aware Word-level Coreference Resolution",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karel D’Oosterlinck",
      "Semere Kiros Bitew",
      "Brandon Papineau",
      "Christopher Potts",
      "Thomas Demeester",
      "Chris Develder"
    ]
  },
  "https://aclanthology.org/2023.crac-main.3": {
    "title": "Towards Transparency in Coreference Resolution: A Quantum-Inspired Approach",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hadi Wazni",
      "Mehrnoosh Sadrzadeh"
    ]
  },
  "https://aclanthology.org/2023.crac-main.4": {
    "title": "Scalar Anaphora: Annotating Degrees of Coreference in Text",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bingyang Ye",
      "Jingxuan Tu",
      "James Pustejovsky"
    ]
  },
  "https://aclanthology.org/2023.crac-main.5": {
    "title": "Better Handling Coreference Resolution in Aspect Level Sentiment Classification by Fine-Tuning Language Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dhruv Mullick",
      "Bilal Ghanem",
      "Alona Fyshe"
    ]
  },
  "https://aclanthology.org/2023.crac-main.6": {
    "title": "The pragmatics of characters' mental perspectives in pronominal reference resolution",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tiana Simovic",
      "Craig Chambers"
    ]
  },
  "https://aclanthology.org/2023.crac-main.7": {
    "title": "MARRS: Multimodal Reference Resolution System",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Halim Cagri Ates",
      "Shruti Bhargava",
      "Site Li",
      "Jiarui Lu",
      "Siddhardha Maddula",
      "Joel Ruben Antony Moniz",
      "Anil Kumar Nalamalapu",
      "Roman Hoang Nguyen",
      "Melis Ozyildirim",
      "Alkesh Patel",
      "Dhivya Piraviperumal",
      "Vincent Renkens",
      "Ankit Samal",
      "Thy Tran",
      "Bo-Hsiang Tseng",
      "Hong Yu",
      "Yuan Zhang",
      "Shirley Zou"
    ]
  },
  "https://aclanthology.org/2023.crac-main.8": {
    "title": "Towards Harmful Erotic Content Detection through Coreference-Driven Contextual Analysis",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Inez Okulska",
      "Emilia Wisnios"
    ]
  },
  "https://aclanthology.org/2023.crac-main.9": {
    "title": "Integrated Annotation of Event Structure, Object States, and Entity Coreference",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyeongmin Rim",
      "James Pustejovsky"
    ]
  },
  "https://aclanthology.org/2023.crac-sharedtask.1": {
    "title": "Findings of the Second Shared Task on Multilingual Coreference Resolution",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zdeněk Žabokrtský",
      "Miloslav Konopik",
      "Anna Nedoluzhko",
      "Michal Novák",
      "Maciej Ogrodniczuk",
      "Martin Popel",
      "Ondrej Prazak",
      "Jakub Sido",
      "Daniel Zeman"
    ]
  },
  "https://aclanthology.org/2023.crac-sharedtask.2": {
    "title": "Multilingual coreference resolution: Adapt and Generate",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Natalia Skachkova",
      "Tatiana Anikina",
      "Anna Mokhova"
    ]
  },
  "https://aclanthology.org/2023.crac-sharedtask.3": {
    "title": "Neural End-to-End Coreference Resolution using Morphological Information",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tuğba Pamay Arslan",
      "Kutay Acar",
      "Gülşen Eryiğit"
    ]
  },
  "https://aclanthology.org/2023.crac-sharedtask.4": {
    "title": "ÚFAL CorPipe at CRAC 2023: Larger Context Improves Multilingual Coreference Resolution",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Milan Straka"
    ]
  },
  "https://aclanthology.org/2023.crac-sharedtask.5": {
    "title": "McGill at CRAC 2023: Multilingual Generalization of Entity-Ranking Coreference Resolution Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ian Porada",
      "Jackie Chi Kit Cheung"
    ]
  },
  "https://aclanthology.org/2023.genbench-1.1": {
    "title": "90% F1 Score in Relation Triple Extraction: Is it Real?",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pratik Saini",
      "Samiran Pal",
      "Tapas Nayak",
      "Indrajit Bhattacharya"
    ]
  },
  "https://aclanthology.org/2023.genbench-1.2": {
    "title": "GenCodeSearchNet: A Benchmark Test Suite for Evaluating Generalization in Programming Language Understanding",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andor Diera",
      "Abdelhalim Dahou",
      "Lukas Galke",
      "Fabian Karl",
      "Florian Sihler",
      "Ansgar Scherp"
    ]
  },
  "https://aclanthology.org/2023.genbench-1.3": {
    "title": "Adapt and Decompose: Efficient Generalization of Text-to-SQL via Domain Adapted Least-To-Most Prompting",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aseem Arora",
      "Shabbirhussain Bhaisaheb",
      "Harshit Nigam",
      "Manasi Patwardhan",
      "Lovekesh Vig",
      "Gautam Shroff"
    ]
  },
  "https://aclanthology.org/2023.genbench-1.4": {
    "title": "Evaluating Neural Language Models as Cognitive Models of Language Acquisition",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hector Javier Vazquez Martinez",
      "Annika Lea Heuser",
      "Charles Yang",
      "Jordan Kodner"
    ]
  },
  "https://aclanthology.org/2023.genbench-1.5": {
    "title": "Robust Code Summarization",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Debanjan Mondal",
      "Abhilasha Lodha",
      "Ankita Sahoo",
      "Beena Kumari"
    ]
  },
  "https://aclanthology.org/2023.genbench-1.6": {
    "title": "Temporal Generalizability in Multimodal Misinformation Detection",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nataliya Stepanova",
      "Björn Ross"
    ]
  },
  "https://aclanthology.org/2023.genbench-1.7": {
    "title": "Robust Generalization Strategies for Morpheme Glossing in an Endangered Language Documentation Context",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Ginn",
      "Alexis Palmer"
    ]
  },
  "https://aclanthology.org/2023.genbench-1.8": {
    "title": "Walking a Tightrope – Evaluating Large Language Models in High-Risk Domains",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chia-Chien Hung",
      "Wiem Ben Rim",
      "Lindsay Frost",
      "Lars Bruckner",
      "Carolin Lawrence"
    ]
  },
  "https://aclanthology.org/2023.genbench-1.9": {
    "title": "Latent Feature-based Data Splits to Improve Generalisation Evaluation: A Hate Speech Detection Case Study",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maike Züfle",
      "Verna Dankers",
      "Ivan Titov"
    ]
  },
  "https://aclanthology.org/2023.genbench-1.10": {
    "title": "Syntax-Guided Transformers: Elevating Compositional Generalization and Grounding in Multimodal Environments",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Danial Kamali",
      "Parisa Kordjamshidi"
    ]
  },
  "https://aclanthology.org/2023.genbench-1.11": {
    "title": "mSCAN: A Dataset for Multilingual Compositional Generalisation Evaluation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amélie Reymond",
      "Shane Steinert-Threlkeld"
    ]
  },
  "https://aclanthology.org/2023.genbench-1.12": {
    "title": "Inductive Bias Is in the Eye of the Beholder",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Wilson",
      "Robert Frank"
    ]
  },
  "https://aclanthology.org/2023.genbench-1.13": {
    "title": "Blackbird Language Matrices Tasks for Generalization",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paola Merlo",
      "Chunyang Jiang",
      "Giuseppe Samo",
      "Vivi Nastase"
    ]
  },
  "https://aclanthology.org/2023.genbench-1.14": {
    "title": "In-Context Learning for Text Classification with Many Labels",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aristides Milios",
      "Siva Reddy",
      "Dzmitry Bahdanau"
    ]
  },
  "https://aclanthology.org/2023.genbench-1.15": {
    "title": "GQG: Generalized Quantifier Generalization - A Dataset for Evaluating Quantifier Semantics Understanding in Language Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leroy Zhifei Wang",
      "Shane Steinert-Threlkeld"
    ]
  },
  "https://aclanthology.org/2023.genbench-1.16": {
    "title": "Cross-Lingual Data Augmentation For Thai Question-Answering",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Parinthapat Pengpun",
      "Can Udomcharoenchaikit",
      "Weerayut Buaphet",
      "Peerat Limkonchotiwat"
    ]
  },
  "https://aclanthology.org/2023.genbench-1.17": {
    "title": "On using distribution-based compositionality assessment to evaluate compositional generalisation in machine translation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anssi Moisio",
      "Mathias Creutz",
      "Mikko Kurimo"
    ]
  },
  "https://aclanthology.org/2023.genbench-1.18": {
    "title": "Shifted PAUQ: Distribution shift in text-to-SQL",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oleg Somov",
      "Elena Tutubalina"
    ]
  },
  "https://aclanthology.org/2023.lchange-1.1": {
    "title": "Literary Intertextual Semantic Change Detection: Application and Motivation for Evaluating Models on Small Corpora",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jackson Ehrenworth",
      "Katherine Keith"
    ]
  },
  "https://aclanthology.org/2023.lchange-1.2": {
    "title": "Domain-Adapting BERT for Attributing Manuscript, Century and Region in Pre-Modern Slavic Texts",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Piroska Lendvai",
      "Uwe Reichel",
      "Anna Jouravel",
      "Achim Rabus",
      "Elena Renje"
    ]
  },
  "https://aclanthology.org/2023.lchange-1.3": {
    "title": "Representing and Computing Uncertainty in Phonological Reconstruction",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Johann-Mattis List",
      "Nathan Hill",
      "Robert Forkel",
      "Frederic Blum"
    ]
  },
  "https://aclanthology.org/2023.lchange-1.4": {
    "title": "GHisBERT – Training BERT from scratch for lexical semantic investigations across historical German language stages",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christin Beck",
      "Marisa Köllner"
    ]
  },
  "https://aclanthology.org/2023.lchange-1.5": {
    "title": "A longitudinal study about gradual changes in the Iranian Online Public Sphere pre and post of ‘Mahsa Moment': Focusing on Twitter",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sadegh Jafari",
      "Amin Fathi",
      "Abolfazl Hajizadegan",
      "Amirmohammad Kazemeini",
      "Sauleh Eetemadi"
    ]
  },
  "https://aclanthology.org/2023.lchange-1.6": {
    "title": "Political dogwhistles and community divergence in semantic change",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Max Boholm",
      "Asad Sayeed"
    ]
  },
  "https://aclanthology.org/2023.lchange-1.7": {
    "title": "EvoSem: A database of polysemous cognate sets",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mathieu Dehouck",
      "Alex François",
      "Siva Kalyan",
      "Martial Pastor",
      "David Kletz"
    ]
  },
  "https://aclanthology.org/2023.lchange-1.8": {
    "title": "Multi-lect automatic detection of Swadesh list items from raw corpus data in East Slavic languages",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ilia Afanasev"
    ]
  },
  "https://aclanthology.org/2023.lchange-1.9": {
    "title": "Anchors in Embedding Space: A Simple Concept Tracking Approach to Support Conceptual History Research",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jetske Adams",
      "Martha Larson",
      "Jaap Verheul",
      "Michael Boyden"
    ]
  },
  "https://aclanthology.org/2023.lchange-1.10": {
    "title": "ChiWUG: A Graph-based Evaluation Dataset for Chinese Lexical Semantic Change Detection",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Chen",
      "Emmanuele Chersoni",
      "Dominik Schlechtweg",
      "Jelena Prokic",
      "Chu-Ren Huang"
    ]
  },
  "https://aclanthology.org/2023.lchange-1.11": {
    "title": "Towards Detecting Lexical Change of Hate Speech in Historical Data",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanne Hoeken",
      "Sophie Spliethoff",
      "Silke Schwandt",
      "Sina Zarrieß",
      "Özge Alacam"
    ]
  },
  "https://aclanthology.org/2023.lchange-1.12": {
    "title": "Changing usage of Low Saxon auxiliary and modal verbs",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Janine Siewert",
      "Martijn Wieling",
      "Yves Scherrer"
    ]
  },
  "https://aclanthology.org/2023.lchange-1.13": {
    "title": "Semantic Shifts in Mental Health-Related Concepts",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Naomi Baes",
      "Nick Haslam",
      "Ekaterina Vylomova"
    ]
  },
  "https://aclanthology.org/2023.lchange-1.14": {
    "title": "Automating Sound Change Prediction for Phylogenetic Inference: A Tukanoan Case Study",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kalvin Chang",
      "Nathaniel Robinson",
      "Anna Cai",
      "Ting Chen",
      "Annie Zhang",
      "David Mortensen"
    ]
  },
  "https://aclanthology.org/2023.lchange-1.15": {
    "title": "Scent and Sensibility: Perception Shifts in the Olfactory Domain",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Teresa Paccosi",
      "Stefano Menini",
      "Elisa Leonardelli",
      "Ilaria Barzon",
      "Sara Tonelli"
    ]
  },
  "https://aclanthology.org/2023.lchange-1.16": {
    "title": "From Diachronic to Contextual Lexical Semantic Change: Introducing Semantic Difference Keywords (SDKs) for Discourse Studies",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Isabelle Gribomont"
    ]
  },
  "https://aclanthology.org/2023.mrl-1.1": {
    "title": "UniBriVL: Robust Audio Representation and Generation of Audio Driven Diffusion Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sen Fang",
      "Bowen Gao",
      "Yangjian Wu",
      "TeikToe Teoh"
    ]
  },
  "https://aclanthology.org/2023.mrl-1.2": {
    "title": "Meta-learning For Vision-and-language Cross-lingual Transfer",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanxu Hu",
      "Frank Keller"
    ]
  },
  "https://aclanthology.org/2023.mrl-1.3": {
    "title": "Counterfactually Probing Language Identity in Multilingual Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anirudh Srinivasan",
      "Venkata Subrahmanyan Govindarajan",
      "Kyle Mahowald"
    ]
  },
  "https://aclanthology.org/2023.mrl-1.4": {
    "title": "A General-Purpose Multilingual Document Encoder",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Onur Galoğlu Robert Litschko",
      "Robert Litschko",
      "Goran Glavaš"
    ]
  },
  "https://aclanthology.org/2023.mrl-1.5": {
    "title": "Zero-Shot Cross-Lingual Sentiment Classification under Distribution Shift: an Exploratory Study",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maarten De Raedt",
      "Semere Kiros Bitew",
      "Fréderic Godin",
      "Thomas Demeester",
      "Chris Develder"
    ]
  },
  "https://aclanthology.org/2023.mrl-1.6": {
    "title": "To token or not to token: A Comparative Study of Text Representations for Cross-Lingual Transfer",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Mushfiqur Rahman",
      "Fardin Ahsan Sakib",
      "Fahim Faisal",
      "Antonios Anastasopoulos"
    ]
  },
  "https://aclanthology.org/2023.mrl-1.7": {
    "title": "Adapt and Prune Strategy for Multilingual Speech Foundational Model on Low-resourced Languages",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyeon Soo Kim",
      "Chung Hyeon Cho",
      "Hyejin Won",
      "Kyung Ho Park"
    ]
  },
  "https://aclanthology.org/2023.mrl-1.8": {
    "title": "Multilingual Word Embeddings for Low-Resource Languages using Anchors and a Chain of Related Languages",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Viktor Hangya",
      "Silvia Severini",
      "Radoslav Ralev",
      "Alexander Fraser",
      "Hinrich Schütze"
    ]
  },
  "https://aclanthology.org/2023.mrl-1.9": {
    "title": "TalaMT: Multilingual Machine Translation for Cabécar-Bribri-Spanish",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alex Jones",
      "Rolando Coto-Solano",
      "Guillermo González Campos"
    ]
  },
  "https://aclanthology.org/2023.mrl-1.10": {
    "title": "Mergen: The First Manchu-Korean Machine Translation Model Trained on Augmented Data",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jean Seo",
      "Sungjoo Byun",
      "Minha Kang",
      "Sangah Lee"
    ]
  },
  "https://aclanthology.org/2023.mrl-1.11": {
    "title": "Improving Cross-Lingual Transfer for Open Information Extraction with Linguistic Feature Projection",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youmi Ma",
      "Bhushan Kotnis",
      "Carolin Lawrence",
      "Goran Glavaš",
      "Naoaki Okazaki"
    ]
  },
  "https://aclanthology.org/2023.mrl-1.12": {
    "title": "Geographic and Geopolitical Biases of Language Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fahim Faisal",
      "Antonios Anastasopoulos"
    ]
  },
  "https://aclanthology.org/2023.mrl-1.13": {
    "title": "Task-Based MoE for Multitask Multilingual Machine Translation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hai Pham",
      "Young Jin Kim",
      "Subhabrata Mukherjee",
      "David P. Woodruff",
      "Barnabas Poczos",
      "Hany Hassan"
    ]
  },
  "https://aclanthology.org/2023.mrl-1.14": {
    "title": "Does the English Matter? Elicit Cross-lingual Abilities of Large Language Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leonardo Ranaldi",
      "Giulia Pucci"
    ]
  },
  "https://aclanthology.org/2023.mrl-1.15": {
    "title": "CAPIVARA: Cost-Efficient Approach for Improving Multilingual CLIP Performance on Low-Resource Languages",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gabriel Oliveira dos Santos",
      "Diego Alysson Braga Moreira",
      "Alef Iury Ferreira",
      "Jhessica Silva",
      "Luiz Pereira",
      "Pedro Bueno",
      "Thiago Sousa",
      "Helena Maia",
      "Nádia Da Silva",
      "Esther Colombini",
      "Helio Pedrini",
      "Sandra Avila"
    ]
  },
  "https://aclanthology.org/2023.mrl-1.16": {
    "title": "Code-switching as a cross-lingual Training Signal: an Example with Unsupervised Bilingual Embedding",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Felix Gaschi",
      "Ilias El-Baamrani",
      "Barbara Gendron",
      "Parisa Rastin",
      "Yannick Toussaint"
    ]
  },
  "https://aclanthology.org/2023.mrl-1.17": {
    "title": "Learning to translate by learning to communicate",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "C.m. Downey",
      "Xuhui Zhou",
      "Zeyu Liu",
      "Shane Steinert-Threlkeld"
    ]
  },
  "https://aclanthology.org/2023.mrl-1.18": {
    "title": "Contrastive Learning for Universal Zero-Shot NLI with Cross-Lingual Sentence Embeddings",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Kowsher",
      "Md. Shohanur Islam Sobuj",
      "Nusrat Jahan Prottasha",
      "Mohammad Shamsul Arefin",
      "Yasuhiko Morimoto"
    ]
  },
  "https://aclanthology.org/2023.mrl-1.19": {
    "title": "UD-MULTIGENRE – a UD-Based Dataset Enriched with Instance-Level Genre Annotations",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vera Danilova",
      "Sara Stymne"
    ]
  },
  "https://aclanthology.org/2023.mrl-1.20": {
    "title": "Embedding Structure Matters: Comparing Methods to Adapt Multilingual Vocabularies to New Languages",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "C.m. Downey",
      "Terra Blevins",
      "Nora Goldfine",
      "Shane Steinert-Threlkeld"
    ]
  },
  "https://aclanthology.org/2023.mrl-1.21": {
    "title": "Multi-EuP: The Multilingual European Parliament Dataset for Analysis of Bias in Information Retrieval",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinrui Yang",
      "Timothy Baldwin",
      "Trevor Cohn"
    ]
  },
  "https://aclanthology.org/2023.mrl-1.22": {
    "title": "Generating Continuations in Multilingual Idiomatic Contexts",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rhitabrat Pokharel",
      "Ameeta Agrawal"
    ]
  },
  "https://aclanthology.org/2023.mrl-1.23": {
    "title": "CUNI Submission to MRL 2023 Shared Task on Multi-lingual Multi-task Information Retrieval",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jindřich Helcl",
      "Jindřich Libovický"
    ]
  },
  "https://aclanthology.org/2023.mrl-1.24": {
    "title": "Findings of the 1st Shared Task on Multi-lingual Multi-task Information Retrieval at MRL 2023",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesco Tinner",
      "David Ifeoluwa Adelani",
      "Chris Emezue",
      "Mammad Hajili",
      "Omer Goldman",
      "Muhammad Farid Adilazuarda",
      "Muhammad Dehan Al Kautsar",
      "Aziza Mirsaidova",
      "Müge Kural",
      "Dylan Massey",
      "Chiamaka Chukwuneke",
      "Chinedu Mbonu",
      "Damilola Oluwaseun Oloyede",
      "Kayode Olaleye",
      "Jonathan Atala",
      "Benjamin A. Ajibade",
      "Saksham Bassi",
      "Rahul Aralikatte",
      "Najoung Kim",
      "Duygu Ataman"
    ]
  },
  "https://aclanthology.org/2023.newsum-1.1": {
    "title": "Is ChatGPT a Good NLG Evaluator? A Preliminary Study",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaan Wang",
      "Yunlong Liang",
      "Fandong Meng",
      "Zengkui Sun",
      "Haoxiang Shi",
      "Zhixu Li",
      "Jinan Xu",
      "Jianfeng Qu",
      "Jie Zhou"
    ]
  },
  "https://aclanthology.org/2023.newsum-1.2": {
    "title": "Zero-Shot Cross-Lingual Summarization via Large Language Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaan Wang",
      "Yunlong Liang",
      "Fandong Meng",
      "Beiqi Zou",
      "Zhixu Li",
      "Jianfeng Qu",
      "Jie Zhou"
    ]
  },
  "https://aclanthology.org/2023.newsum-1.3": {
    "title": "SimCSum: Joint Learning of Simplification and Cross-lingual Summarization for Cross-lingual Science Journalism",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mehwish Fatima",
      "Tim Kolber",
      "Katja Markert",
      "Michael Strube"
    ]
  },
  "https://aclanthology.org/2023.newsum-1.4": {
    "title": "Extract, Select and Rewrite: A Modular Sentence Summarization Method",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuo Guan",
      "Vishakh Padmakumar"
    ]
  },
  "https://aclanthology.org/2023.newsum-1.5": {
    "title": "Summarization-based Data Augmentation for Document Classification",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yueguan Wang",
      "Naoki Yoshinaga"
    ]
  },
  "https://aclanthology.org/2023.newsum-1.6": {
    "title": "In-context Learning of Large Language Models for Controlled Dialogue Summarization: A Holistic Benchmark and Empirical Analysis",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuting Tang",
      "Ratish Puduppully",
      "Zhengyuan Liu",
      "Nancy Chen"
    ]
  },
  "https://aclanthology.org/2023.newsum-1.7": {
    "title": "From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Griffin Adams",
      "Alex Fabbri",
      "Faisal Ladhak",
      "Eric Lehman",
      "Noémie Elhadad"
    ]
  },
  "https://aclanthology.org/2023.newsum-1.8": {
    "title": "Generating Extractive and Abstractive Summaries in Parallel from Scientific Articles Incorporating Citing Statements",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sudipta Singha Roy",
      "Robert E. Mercer"
    ]
  },
  "https://aclanthology.org/2023.newsum-1.9": {
    "title": "Supervising the Centroid Baseline for Extractive Multi-Document Summarization",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simão Gonçalves",
      "Gonçalo Correia",
      "Diogo Pernes",
      "Afonso Mendes"
    ]
  },
  "https://aclanthology.org/2023.newsum-1.10": {
    "title": "DebateKG – Automatic Policy Debate Case Creation with Semantic Knowledge Graphs",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Allen Roush",
      "David Mezzetti"
    ]
  },
  "https://aclanthology.org/2023.newsum-1.11": {
    "title": "Unsupervised Opinion Summarization Using Approximate Geodesics",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Somnath Basu Roy Chowdhury",
      "Nicholas Monath",
      "Kumar Dubey",
      "Amr Ahmed",
      "Snigdha Chaturvedi"
    ]
  },
  "https://aclanthology.org/2023.newsum-1.12": {
    "title": "Analyzing Multi-Sentence Aggregation in Abstractive Summarization via the Shapley Value",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyi He",
      "Meng Cao",
      "Jackie Chi Kit Cheung"
    ]
  },
  "https://aclanthology.org/2023.newsum-1.13": {
    "title": "Improving Multi-Stage Long Document Summarization with Enhanced Coarse Summarizer",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinhyeong Lim",
      "Hyun-Je Song"
    ]
  },
  "https://aclanthology.org/2023.nllp-1.1": {
    "title": "Anthropomorphization of AI: Opportunities and Risks",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ameet Deshpande",
      "Tanmay Rajpurohit",
      "Karthik Narasimhan",
      "Ashwin Kalyan"
    ]
  },
  "https://aclanthology.org/2023.nllp-1.2": {
    "title": "NOMOS: Navigating Obligation Mining in Official Statutes",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrea Pennisi",
      "Elvira González Hernández",
      "Nina Koivula"
    ]
  },
  "https://aclanthology.org/2023.nllp-1.3": {
    "title": "Long Text Classification using Transformers with Paragraph Selection Strategies",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohit Tuteja",
      "Daniel González Juclà"
    ]
  },
  "https://aclanthology.org/2023.nllp-1.4": {
    "title": "Do Language Models Learn about Legal Entity Types during Pretraining?",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Claire Barale",
      "Michael Rovatsos",
      "Nehal Bhuta"
    ]
  },
  "https://aclanthology.org/2023.nllp-1.5": {
    "title": "Pretrained Language Models v. Court Ruling Predictions: A Case Study on a Small Dataset of French Court of Appeal Rulings",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Olivia Vaudaux",
      "Caroline Bazzoli",
      "Maximin Coavoux",
      "Géraldine Vial",
      "Étienne Vergès"
    ]
  },
  "https://aclanthology.org/2023.nllp-1.6": {
    "title": "Italian Legislative Text Classification for Gazzetta Ufficiale",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marco Rovera",
      "Alessio Palmero Aprosio",
      "Francesco Greco",
      "Mariano Lucchese",
      "Sara Tonelli",
      "Antonio Antetomaso"
    ]
  },
  "https://aclanthology.org/2023.nllp-1.7": {
    "title": "Mixed-domain Language Modeling for Processing Long Legal Documents",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenyue Hua",
      "Yuchen Zhang",
      "Zhe Chen",
      "Josie Li",
      "Melanie Weber"
    ]
  },
  "https://aclanthology.org/2023.nllp-1.8": {
    "title": "Questions about Contracts: Prompt Templates for Structured Answer Generation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adam Roegiest",
      "Radha Chitta",
      "Jonathan Donnelly",
      "Maya Lash",
      "Alexandra Vtyurina",
      "Francois Longtin"
    ]
  },
  "https://aclanthology.org/2023.nllp-1.9": {
    "title": "Legal Judgment Prediction: If You Are Going to Do It, Do It Right",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Masha Medvedeva",
      "Pauline Mcbride"
    ]
  },
  "https://aclanthology.org/2023.nllp-1.10": {
    "title": "Beyond The Text: Analysis of Privacy Statements through Syntactic and Semantic Role Labeling",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Shvartzshanider",
      "Ananth Balashankar",
      "Thomas Wies",
      "Lakshminarayanan Subramanian"
    ]
  },
  "https://aclanthology.org/2023.nllp-1.11": {
    "title": "Towards Mitigating Perceived Unfairness in Contracts from a Non-Legal Stakeholder's Perspective",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anmol Singhal",
      "Preethu Rose Anish",
      "Shirish Karande",
      "Smita Ghaisas"
    ]
  },
  "https://aclanthology.org/2023.nllp-1.12": {
    "title": "Connecting Symbolic Statutory Reasoning with Legal Information Extraction",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nils Holzenberger",
      "Benjamin Van Durme"
    ]
  },
  "https://aclanthology.org/2023.nllp-1.13": {
    "title": "Retrieval-based Evaluation for LLMs: A Case Study in Korean Legal QA",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheol Ryu",
      "Seolhwa Lee",
      "Subeen Pang",
      "Chanyeol Choi",
      "Hojun Choi",
      "Myeonggee Min",
      "Jy-Yong Sohn"
    ]
  },
  "https://aclanthology.org/2023.nllp-1.14": {
    "title": "Legal NLP Meets MiCAR: Advancing the Analysis of Crypto White Papers",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Carolina Camassa"
    ]
  },
  "https://aclanthology.org/2023.nllp-1.15": {
    "title": "Low-Resource Deontic Modality Classification in EU Legislation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kristina Minkova",
      "Shashank Chakravarthy",
      "Gijs Dijck"
    ]
  },
  "https://aclanthology.org/2023.nllp-1.16": {
    "title": "Automatic Anonymization of Swiss Federal Supreme Court Rulings",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joel Niklaus",
      "Robin Mamié",
      "Matthias Stürmer",
      "Daniel Brunner",
      "Marcel Gygli"
    ]
  },
  "https://aclanthology.org/2023.nllp-1.17": {
    "title": "Exploration of Open Large Language Models for eDiscovery",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sumit Pai",
      "Sounak Lahiri",
      "Ujjwal Kumar",
      "Krishanu Baksi",
      "Elijah Soba",
      "Michael Suesserman",
      "Nirmala Pudota",
      "Jon Foster",
      "Edward Bowen",
      "Sanmitra Bhattacharya"
    ]
  },
  "https://aclanthology.org/2023.nllp-1.18": {
    "title": "Retrieval-Augmented Chain-of-Thought in Semi-structured Domains",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vaibhav Mavi",
      "Abulhair Saparov",
      "Chen Zhao"
    ]
  },
  "https://aclanthology.org/2023.nllp-1.19": {
    "title": "Joint Learning for Legal Text Retrieval and Textual Entailment: Leveraging the Relationship between Relevancy and Affirmation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nguyen Hai Long",
      "Thi Hai Yen Vuong",
      "Ha Thanh Nguyen",
      "Xuan-Hieu Phan"
    ]
  },
  "https://aclanthology.org/2023.nllp-1.20": {
    "title": "Super-SCOTUS: A multi-sourced dataset for the Supreme Court of the US",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Biaoyan Fang",
      "Trevor Cohn",
      "Timothy Baldwin",
      "Lea Frermann"
    ]
  },
  "https://aclanthology.org/2023.nllp-1.21": {
    "title": "Transferring Legal Natural Language Inference Model from a US State to Another: What Makes It So Hard?",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alice Kwak",
      "Gaetano Forte",
      "Derek Bambauer",
      "Mihai Surdeanu"
    ]
  },
  "https://aclanthology.org/2023.nllp-1.22": {
    "title": "Large Language Models are legal but they are not: Making the case for a powerful LegalLLM",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thanmay Jayakumar",
      "Fauzan Farooqui",
      "Luqman Farooqui"
    ]
  },
  "https://aclanthology.org/2023.nllp-1.23": {
    "title": "On the Potential and Limitations of Few-Shot In-Context Learning to Generate Metamorphic Specifications for Tax Preparation Software",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dananjay Srinivas",
      "Rohan Das",
      "Saeid Tizpaz-Niari",
      "Ashutosh Trivedi",
      "Maria Leonor Pacheco"
    ]
  },
  "https://aclanthology.org/2023.nllp-1.24": {
    "title": "AsyLex: A Dataset for Legal Language Processing of Refugee Claims",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Claire Barale",
      "Mark Klaisoongnoen",
      "Pasquale Minervini",
      "Michael Rovatsos",
      "Nehal Bhuta"
    ]
  },
  "https://aclanthology.org/2023.nllp-1.25": {
    "title": "A Comparative Study of Prompting Strategies for Legal Text Classification",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Hakimi Parizi",
      "Yuyang Liu",
      "Prudhvi Nokku",
      "Sina Gholamian",
      "David Emerson"
    ]
  },
  "https://aclanthology.org/2023.nllp-1.26": {
    "title": "Tracing Influence at Scale: A Contrastive Learning Approach to Linking Public Comments and Regulator Responses",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linzi Xing",
      "Brad Hackinen",
      "Giuseppe Carenini"
    ]
  },
  "https://aclanthology.org/2023.nlposs-1.1": {
    "title": "calamanCy: A Tagalog Natural Language Processing Toolkit",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lester James Miranda"
    ]
  },
  "https://aclanthology.org/2023.nlposs-1.2": {
    "title": "Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Günther",
      "Georgios Mastrapas",
      "Bo Wang",
      "Han Xiao",
      "Jonathan Geuter"
    ]
  },
  "https://aclanthology.org/2023.nlposs-1.3": {
    "title": "Deepparse : An Extendable, and Fine-Tunable State-Of-The-Art Library for Parsing Multinational Street Addresses",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Beauchemin"
    ]
  },
  "https://aclanthology.org/2023.nlposs-1.4": {
    "title": "PyThaiNLP: Thai Natural Language Processing in Python",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wannaphong Phatthiyaphaibun",
      "Korakot Chaovavanich",
      "Charin Polpanumas",
      "Arthit Suriyawongkul",
      "Lalita Lowphansirikul",
      "Pattarawat Chormai",
      "Peerat Limkonchotiwat",
      "Thanathip Suntorntip",
      "Can Udomcharoenchaikit"
    ]
  },
  "https://aclanthology.org/2023.nlposs-1.5": {
    "title": "Empowering Knowledge Discovery from Scientific Literature: A novel approach to Research Artifact Analysis",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Petros Stavropoulos",
      "Ioannis Lyris",
      "Natalia Manola",
      "Ioanna Grypari",
      "Haris Papageorgiou"
    ]
  },
  "https://aclanthology.org/2023.nlposs-1.6": {
    "title": "Zelda Rose: a tool for hassle-free training of transformer models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Loïc Grobol"
    ]
  },
  "https://aclanthology.org/2023.nlposs-1.7": {
    "title": "GPT4All: An Ecosystem of Open Source Compressed Language Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuvanesh Anand",
      "Zach Nussbaum",
      "Adam Treat",
      "Aaron Miller",
      "Richard Guo",
      "Benjamin Schmidt",
      "Brandon Duderstadt",
      "Andriy Mulyar"
    ]
  },
  "https://aclanthology.org/2023.nlposs-1.8": {
    "title": "Kani: A Lightweight and Highly Hackable Framework for Building Language Model Applications",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrew Zhu",
      "Liam Dugan",
      "Alyssa Hwang",
      "Chris Callison-Burch"
    ]
  },
  "https://aclanthology.org/2023.nlposs-1.9": {
    "title": "Beyond the Repo: A Case Study on Open Source Integration with GECToR",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanjna Kashyap",
      "Zhaoyang Xie",
      "Kenneth Steimel",
      "Nitin Madnani"
    ]
  },
  "https://aclanthology.org/2023.nlposs-1.10": {
    "title": "Two Decades of the ACL Anthology: Development, Impact, and Open Challenges",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marcel Bollmann",
      "Nathan Schneider",
      "Arne Köhn",
      "Matt Post"
    ]
  },
  "https://aclanthology.org/2023.nlposs-1.11": {
    "title": "nanoT5: Fast & Simple Pre-training and Fine-tuning of T5 Models with Limited Resources",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Piotr Nawrot"
    ]
  },
  "https://aclanthology.org/2023.nlposs-1.12": {
    "title": "AWARE-TEXT: An Android Package for Mobile Phone Based Text Collection and On-Device Processing",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Salvatore Giorgi",
      "Garrick Sherman",
      "Douglas Bellew",
      "Sharath Chandra Guntuku",
      "Lyle Ungar",
      "Brenda Curtis"
    ]
  },
  "https://aclanthology.org/2023.nlposs-1.13": {
    "title": "SOTASTREAM: A Streaming Approach to Machine Translation Training",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matt Post",
      "Thamme Gowda",
      "Roman Grundkiewicz",
      "Huda Khayrallah",
      "Rohit Jain",
      "Marcin Junczys-Dowmunt"
    ]
  },
  "https://aclanthology.org/2023.nlposs-1.14": {
    "title": "An Open-source Web-based Application for Development of Resources and Technologies in Underresourced Languages",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siddharth Singh",
      "Shyam Ratan",
      "Neerav Mathur",
      "Ritesh Kumar"
    ]
  },
  "https://aclanthology.org/2023.nlposs-1.15": {
    "title": "Rumour Detection in the Wild: A Browser Extension for Twitter",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrej Jovanovic",
      "Björn Ross"
    ]
  },
  "https://aclanthology.org/2023.nlposs-1.16": {
    "title": "DeepZensols: A Deep Learning Natural Language Processing Framework for Experimentation and Reproducibility",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paul Landes",
      "Barbara Di Eugenio",
      "Cornelia Caragea"
    ]
  },
  "https://aclanthology.org/2023.nlposs-1.17": {
    "title": "Improving NER Research Workflows with SeqScore",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Constantine Lignos",
      "Maya Kruse",
      "Andrew Rueda"
    ]
  },
  "https://aclanthology.org/2023.nlposs-1.18": {
    "title": "torchdistill Meets Hugging Face Libraries for Reproducible, Coding-Free Deep Learning Studies: A Case Study on NLP",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yoshitomo Matsubara"
    ]
  },
  "https://aclanthology.org/2023.nlposs-1.19": {
    "title": "Using Captum to Explain Generative Language Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vivek Miglani",
      "Aobo Yang",
      "Aram Markosyan",
      "Diego Garcia-Olano",
      "Narine Kokhlikyan"
    ]
  },
  "https://aclanthology.org/2023.nlposs-1.20": {
    "title": "nerblackbox: A High-level Library for Named Entity Recognition in Python",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Felix Stollenwerk"
    ]
  },
  "https://aclanthology.org/2023.nlposs-1.21": {
    "title": "News Signals: An NLP Library for Text and Time Series",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chris Hokamp",
      "Demian Ghalandari",
      "Parsa Ghaffari"
    ]
  },
  "https://aclanthology.org/2023.nlposs-1.22": {
    "title": "PyTAIL: An Open Source Tool for Interactive and Incremental Learning of NLP Models with Human in the Loop for Online Data",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shubhanshu Mishra",
      "Jana Diesner"
    ]
  },
  "https://aclanthology.org/2023.nlposs-1.23": {
    "title": "Antarlekhaka: A Comprehensive Tool for Multi-task Natural Language Annotation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hrishikesh Terdalkar",
      "Arnab Bhattacharya"
    ]
  },
  "https://aclanthology.org/2023.nlposs-1.24": {
    "title": "GPTCache: An Open-Source Semantic Cache for LLM Applications Enabling Faster Answers and Cost Savings",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fu Bang"
    ]
  },
  "https://aclanthology.org/2023.nlposs-1.25": {
    "title": "The Vault: A Comprehensive Multilingual Dataset for Advancing Code Understanding and Generation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dung Nguyen Manh",
      "Nam Le Hai",
      "Anh T. V. Dau",
      "Anh Minh Nguyen",
      "Khanh Nghiem",
      "Jin Guo",
      "Nghi D. Q. Bui"
    ]
  },
  "https://aclanthology.org/2023.nlposs-1.26": {
    "title": "SEA-LION (Southeast Asian Languages In One Network): A Family of Southeast Asian Language Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "William Tjhi",
      "David Ong",
      "Peerat Limkonchotiwat"
    ]
  },
  "https://aclanthology.org/2023.nlposs-1.27": {
    "title": "trlX: A Framework for Large Scale Open Source RLHF",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Louis Castricato"
    ]
  },
  "https://aclanthology.org/2023.nlposs-1.28": {
    "title": "Towards Explainable and Accessible AI",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brandon Duderstadt",
      "Yuvanesh Anand"
    ]
  },
  "https://aclanthology.org/2023.pandl-1.1": {
    "title": "Nearest Neighbor Search over Vectorized Lexico-Syntactic Patterns for Relation Extraction from Financial Documents",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pawan Rajpoot",
      "Ankur Parikh"
    ]
  },
  "https://aclanthology.org/2023.pandl-1.2": {
    "title": "LEAF: Linguistically Enhanced Event Temporal Relation Framework",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stanley Lim",
      "Da Yin",
      "Nanyun Peng"
    ]
  },
  "https://aclanthology.org/2023.pandl-1.3": {
    "title": "A Graph-Guided Reasoning Approach for Open-ended Commonsense Question Answering",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Han",
      "Yue Feng",
      "Mingming Sun"
    ]
  },
  "https://aclanthology.org/2023.pandl-1.4": {
    "title": "Generating Irish Text with a Flexible Plug-and-Play Architecture",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simon Mille",
      "Elaine Uí Dhonnchadha",
      "Lauren Cassidy",
      "Brian Davis",
      "Stamatia Dasiopoulou",
      "Anya Belz"
    ]
  },
  "https://aclanthology.org/2023.pandl-1.5": {
    "title": "Symbolic Planning and Code Generation for Grounded Dialogue",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Justin Chiu",
      "Wenting Zhao",
      "Derek Chen",
      "Saujas Vaduguru",
      "Alexander Rush",
      "Daniel Fried"
    ]
  },
  "https://aclanthology.org/2023.pandl-1.6": {
    "title": "Towards Zero-Shot Frame Semantic Parsing with Task Agnostic Ontologies and Simple Labels",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Danilo Neves Ribeiro",
      "Jack Goetz",
      "Omid Abdar",
      "Mike Ross",
      "Annie Dong",
      "Kenneth Forbus",
      "Ahmed Mohamed"
    ]
  },
  "https://aclanthology.org/2023.pandl-1.7": {
    "title": "Co-evolving data-driven and NLU-driven Synthesizers for Generating Code in Domain Growth and Data Scarcity",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiasheng Gu",
      "Zifan Nan",
      "Zhiyuan Peng",
      "Xipeng Shen",
      "Dongkuan Xu"
    ]
  },
  "https://aclanthology.org/2023.pandl-1.8": {
    "title": "Complementary Roles of Inference and Language Models in QA",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liang Cheng",
      "Mohammad Javad Hosseini",
      "Mark Steedman"
    ]
  },
  "https://aclanthology.org/2023.pandl-1.9": {
    "title": "Controlled Data Augmentation for Training Task-Oriented Dialog Systems with Low Resource Data",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Steindl",
      "Ulrich Schäfer",
      "Bernd Ludwig"
    ]
  },
  "https://aclanthology.org/2023.pandl-1.10": {
    "title": "A Hybrid of Rule-based and Transformer-based Approaches for Relation Extraction in Biodiversity Literature",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roselyn Gabud",
      "Portia Lapitan",
      "Vladimir Mariano",
      "Eduardo Mendoza",
      "Nelson Pampolina",
      "Maria Art Antonette Clariño",
      "Riza Batista-Navarro"
    ]
  },
  "https://aclanthology.org/2023.splurobonlp-1.1": {
    "title": "Dialogue-based generation of self-driving simulation scenarios using Large Language Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antonio Valerio Miceli Barone",
      "Craig Innes",
      "Alex Lascarides"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.1": {
    "title": "Findings of the 2023 Conference on Machine Translation (WMT23): LLMs Are Here but Not Quite There Yet",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tom Kocmi",
      "Eleftherios Avramidis",
      "Rachel Bawden",
      "Ondřej Bojar",
      "Anton Dvorkovich",
      "Christian Federmann",
      "Mark Fishel",
      "Markus Freitag",
      "Thamme Gowda",
      "Roman Grundkiewicz",
      "Barry Haddow",
      "Philipp Koehn",
      "Benjamin Marie",
      "Christof Monz",
      "Makoto Morishita",
      "Kenton Murray",
      "Makoto Nagata",
      "Toshiaki Nakazawa",
      "Martin Popel",
      "Maja Popović",
      "Mariya Shmatova"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.2": {
    "title": "Findings of the WMT 2023 Biomedical Translation Shared Task: Evaluation of ChatGPT 3.5 as a Comparison System",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mariana Neves",
      "Antonio Jimeno Yepes",
      "Aurélie Névéol",
      "Rachel Bawden",
      "Giorgio Maria Di Nunzio",
      "Roland Roller",
      "Philippe Thomas",
      "Federica Vezzani",
      "Maika Vicente Navarro",
      "Lana Yeganova",
      "Dina Wiemann",
      "Cristian Grozea"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.3": {
    "title": "Findings of the WMT 2023 Shared Task on Discourse-Level Literary Translation: A Fresh Orb in the Cosmos of LLMs",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Longyue Wang",
      "Zhaopeng Tu",
      "Yan Gu",
      "Siyou Liu",
      "Dian Yu",
      "Qingsong Ma",
      "Chenyang Lyu",
      "Liting Zhou",
      "Chao-Hong Liu",
      "Yufeng Ma",
      "Weiyu Chen",
      "Yvette Graham",
      "Bonnie Webber",
      "Philipp Koehn",
      "Andy Way",
      "Yulin Yuan",
      "Shuming Shi"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.4": {
    "title": "Findings of the Second WMT Shared Task on Sign Language Translation (WMT-SLT23)",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mathias Müller",
      "Malihe Alikhani",
      "Eleftherios Avramidis",
      "Richard Bowden",
      "Annelies Braffort",
      "Necati Cihan Camgöz",
      "Sarah Ebling",
      "Cristina España-Bonet",
      "Anne Göhring",
      "Roman Grundkiewicz",
      "Mert Inan",
      "Zifan Jiang",
      "Oscar Koller",
      "Amit Moryossef",
      "Annette Rios",
      "Dimitar Shterionov",
      "Sandra Sidler-Miserez",
      "Katja Tissi",
      "Davy Van Landuyt"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.5": {
    "title": "Findings of the WMT 2023 Shared Task on Parallel Data Curation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Steve Sloto",
      "Brian Thompson",
      "Huda Khayrallah",
      "Tobias Domhan",
      "Thamme Gowda",
      "Philipp Koehn"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.6": {
    "title": "Samsung R&D Institute Philippines at WMT 2023",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jan Christian Blaise Cruz"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.7": {
    "title": "NAIST-NICT WMT'23 General MT Task Submission",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hiroyuki Deguchi",
      "Kenji Imamura",
      "Yuto Nishida",
      "Yusuke Sakai",
      "Justin Vasselli",
      "Taro Watanabe"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.8": {
    "title": "CUNI at WMT23 General Translation Task: MT and a Genetic Algorithm",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Josef Jon",
      "Martin Popel",
      "Ondřej Bojar"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.9": {
    "title": "SKIM at WMT 2023 General Translation Task",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keito Kudo",
      "Takumi Ito",
      "Makoto Morishita",
      "Jun Suzuki"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.10": {
    "title": "KYB General Machine Translation Systems for WMT23",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ben Li",
      "Yoko Matsuzaki",
      "Shivam Kalkar"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.11": {
    "title": "Yishu: Yishu at WMT2023 Translation Task",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luo Min",
      "Yixin Tan",
      "Qiulin Chen"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.12": {
    "title": "PROMT Systems for WMT23 Shared General Translation Task",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Molchanov",
      "Vladislav Kovalenko"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.13": {
    "title": "AIST AIRC Submissions to the WMT23 Shared Task",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matiss Rikters",
      "Makoto Miwa"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.14": {
    "title": "MUNI-NLP Submission for Czech-Ukrainian Translation Task at WMT23",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pavel Rychly",
      "Yuliia Teslia"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.15": {
    "title": "Exploring Prompt Engineering with GPT Language Models for Document-Level Machine Translation: Insights and Findings",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangjian Wu",
      "Gang Hu"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.16": {
    "title": "Treating General MT Shared Task as a Multi-Domain Adaptation Problem: HW-TSC's Submission to the WMT23 General MT Shared Task",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhanglin Wu",
      "Daimeng Wei",
      "Zongyao Li",
      "Zhengzhe Yu",
      "Shaojun Li",
      "Xiaoyu Chen",
      "Hengchao Shang",
      "Jiaxin Guo",
      "Yuhao Xie",
      "Lizhi Lei",
      "Hao Yang",
      "Yanfei Jiang"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.17": {
    "title": "UvA-MT's Participation in the WMT 2023 General Translation Shared Task",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Di Wu",
      "Shaomu Tan",
      "David Stap",
      "Ali Araabi",
      "Christof Monz"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.18": {
    "title": "Achieving State-of-the-Art Multilingual Translation Model with Minimal Data and Parameters",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hui Zeng"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.19": {
    "title": "IOL Research Machine Translation Systems for WMT23 General Machine Translation Shared Task",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenbo Zhang"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.20": {
    "title": "GTCOM and DLUT's Neural Machine Translation Systems for WMT23",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Zong"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.21": {
    "title": "RoCS-MT: Robustness Challenge Set for Machine Translation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rachel Bawden",
      "Benoît Sagot"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.22": {
    "title": "Multifaceted Challenge Set for Evaluating Machine Translation Performance",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyu Chen",
      "Daimeng Wei",
      "Zhanglin Wu",
      "Ting Zhu",
      "Hengchao Shang",
      "Zongyao Li",
      "Jiaxin Guo",
      "Ning Xie",
      "Lizhi Lei",
      "Hao Yang",
      "Yanfei Jiang"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.23": {
    "title": "Linguistically Motivated Evaluation of the 2023 State-of-the-art Machine Translation: Can ChatGPT Outperform NMT?",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shushen Manakhimova",
      "Eleftherios Avramidis",
      "Vivien Macketanz",
      "Ekaterina Lapshinova-Koltunski",
      "Sergei Bagdasarov",
      "Sebastian Möller"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.24": {
    "title": "IIIT HYD's Submission for WMT23 Test-suite Task",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ananya Mukherjee",
      "Manish Shrivastava"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.25": {
    "title": "Test Suites Task: Evaluation of Gender Fairness in MT with MuST-SHE and INES",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Beatrice Savoldi",
      "Marco Gaido",
      "Matteo Negri",
      "Luisa Bentivogli"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.26": {
    "title": "Biomedical Parallel Sentence Retrieval Using Large Language Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheema Firdous",
      "Sadaf Abdul Rauf"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.27": {
    "title": "The Path to Continuous Domain Adaptation Improvements by HW-TSC for the WMT23 Biomedical Translation Shared Task",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhanglin Wu",
      "Daimeng Wei",
      "Zongyao Li",
      "Zhengzhe Yu",
      "Shaojun Li",
      "Xiaoyu Chen",
      "Hengchao Shang",
      "Jiaxin Guo",
      "Yuhao Xie",
      "Lizhi Lei",
      "Hao Yang",
      "Yanfei Jiang"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.28": {
    "title": "Investigating Techniques for a Deeper Understanding of Neural Machine Translation (NMT) Systems through Data Filtering and Fine-tuning Strategies",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lichao Zhu",
      "Maria Zimina",
      "Maud Bénard",
      "Behnoosh Namdar",
      "Nicolas Ballier",
      "Guillaume Wisniewski",
      "Jean-Baptiste Yunès"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.29": {
    "title": "MAX-ISI System at WMT23 Discourse-Level Literary Translation Task",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li An",
      "Linghao Jin",
      "Xuezhe Ma"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.30": {
    "title": "The MAKE-NMTVIZ System Description for the WMT23 Literary Task",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fabien Lopez",
      "Gabriela González",
      "Damien Hansen",
      "Mariam Nakhle",
      "Behnoosh Namdarzadeh",
      "Nicolas Ballier",
      "Marco Dinarelli",
      "Emmanuelle Esperança-Rodier",
      "Sui He",
      "Sadaf Mohseni",
      "Caroline Rossi",
      "Didier Schwab",
      "Jun Yang",
      "Jean-Baptiste Yunès",
      "Lichao Zhu"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.31": {
    "title": "DUTNLP System for the WMT2023 Discourse-Level Literary Translation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anqi Zhao",
      "Kaiyu Huang",
      "Hao Yu",
      "Degen Huang"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.32": {
    "title": "HW-TSC's Submissions to the WMT23 Discourse-Level Literary Translation Shared Task",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhao Xie",
      "Zongyao Li",
      "Zhanglin Wu",
      "Daimeng Wei",
      "Xiaoyu Chen",
      "Zhiqiang Rao",
      "Shaojun Li",
      "Hengchao Shang",
      "Jiaxin Guo",
      "Lizhi Lei",
      "Hao Yang",
      "Yanfei Jiang"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.33": {
    "title": "TJUNLP:System Description for the WMT23 Literary Task in Chinese to English Translation Direction",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaolin Zhu",
      "Deyi Xiong"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.34": {
    "title": "Machine Translation for Nko: Tools, Corpora, and Baseline Results",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Moussa Doumbouya",
      "Baba Mamadi Diané",
      "Solo Farabado Cissé",
      "Djibrila Diané",
      "Abdoulaye Sow",
      "Séré Moussa Doumbouya",
      "Daouda Bangoura",
      "Fodé Moriba Bayo",
      "Ibrahima Sory 2. Conde",
      "Kalo Mory Diané",
      "Chris Piech",
      "Christopher Manning"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.35": {
    "title": "TTIC's Submission to WMT-SLT 23",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marcelo Sandoval-Castaneda",
      "Yanhong Li",
      "Bowen Shi",
      "Diane Brentari",
      "Karen Livescu",
      "Gregory Shakhnarovich"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.36": {
    "title": "KnowComp Submission for WMT23 Sign Language Translation Task",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baixuan Xu",
      "Haochen Shi",
      "Tianshi Zheng",
      "Qing Zong",
      "Weiqi Wang",
      "Zhaowei Wang",
      "Yangqiu Song"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.37": {
    "title": "A Fast Method to Filter Noisy Parallel Data WMT2023 Shared Task on Parallel Data Curation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nguyen-Hoang Minh-Cong",
      "Nguyen Van Vinh",
      "Nguyen Le-Minh"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.38": {
    "title": "A Sentence Alignment Approach to Document Alignment and Multi-faceted Filtering for Curating Parallel Sentence Pairs from Web-crawled Data",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Steinthor Steingrimsson"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.39": {
    "title": "Document-Level Language Models for Machine Translation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Frithjof Petrick",
      "Christian Herold",
      "Pavel Petrushkov",
      "Shahram Khadivi",
      "Hermann Ney"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.40": {
    "title": "ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nathaniel Robinson",
      "Perez Ogayo",
      "David R. Mortensen",
      "Graham Neubig"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.41": {
    "title": "Large Language Models Effectively Leverage Document-level Context for Literary Translation, but Critical Errors Persist",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marzena Karpinska",
      "Mohit Iyyer"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.42": {
    "title": "Identifying Context-Dependent Translations for Evaluation Set Production",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rachel Wicks",
      "Matt Post"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.43": {
    "title": "Machine Translation with Large Language Models: Prompting, Few-shot Learning, and Fine-tuning with QLoRA",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan Zhang",
      "Navid Rajabi",
      "Kevin Duh",
      "Philipp Koehn"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.44": {
    "title": "Towards Effective Disambiguation for Machine Translation with Large Language Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vivek Iyer",
      "Pinzhen Chen",
      "Alexandra Birch"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.45": {
    "title": "A Closer Look at Transformer Attention for Multilingual Translation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyi Zhang",
      "Gerard de Melo",
      "Hongfei Xu",
      "Kehai Chen"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.46": {
    "title": "Bridging the Gap between Position-Based and Content-Based Self-Attention for Neural Machine Translation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Felix Schmidt",
      "Mattia Di Gangi"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.47": {
    "title": "Visual Prediction Improves Zero-Shot Cross-Modal Machine Translation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tosho Hirasawa",
      "Emanuele Bugliarello",
      "Desmond Elliott",
      "Mamoru Komachi"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.48": {
    "title": "The Gender-GAP Pipeline: A Gender-Aware Polyglot Pipeline for Gender Characterisation in 55 Languages",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Muller",
      "Belen Alastruey",
      "Prangthip Hansanti",
      "Elahe Kalbassi",
      "Christophe Ropers",
      "Eric Smith",
      "Adina Williams",
      "Luke Zettlemoyer",
      "Pierre Andrews",
      "Marta R. Costa-jussà"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.49": {
    "title": "Towards Better Evaluation for Formality-Controlled English-Japanese Machine Translation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Edison Marrese-Taylor",
      "Pin Chen Wang",
      "Yutaka Matsuo"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.50": {
    "title": "There's No Data like Better Data: Using QE Metrics for MT Data Filtering",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jan-Thorsten Peter",
      "David Vilar",
      "Daniel Deutsch",
      "Mara Finkelstein",
      "Juraj Juraska",
      "Markus Freitag"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.51": {
    "title": "Results of WMT23 Metrics Shared Task: Metrics Might Be Guilty but References Are Not Innocent",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Markus Freitag",
      "Nitika Mathur",
      "Chi-kiu Lo",
      "Eleftherios Avramidis",
      "Ricardo Rei",
      "Brian Thompson",
      "Tom Kocmi",
      "Frederic Blain",
      "Daniel Deutsch",
      "Craig Stewart",
      "Chrysoula Zerva",
      "Sheila Castilho",
      "Alon Lavie",
      "George Foster"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.52": {
    "title": "Findings of the WMT 2023 Shared Task on Quality Estimation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Frederic Blain",
      "Chrysoula Zerva",
      "Ricardo Ribeiro",
      "Nuno M. Guerreiro",
      "Diptesh Kanojia",
      "José G. C. de Souza",
      "Beatriz Silva",
      "Tânia Vaz",
      "Yan Jingxuan",
      "Fatemeh Azadi",
      "Constantin Orasan",
      "André Martins"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.53": {
    "title": "Findings of the Word-Level AutoCompletion Shared Task in WMT 2023",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lemao Liu",
      "Francisco Casacuberta",
      "George Foster",
      "Guoping Huang",
      "Philipp Koehn",
      "Geza Kovacs",
      "Shuming Shi",
      "Taro Watanabe",
      "Chengqing Zong"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.54": {
    "title": "Findings of the WMT 2023 Shared Task on Machine Translation with Terminologies",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kirill Semenov",
      "Vilém Zouhar",
      "Tom Kocmi",
      "Dongdong Zhang",
      "Wangchunshu Zhou",
      "Yuchen Eleanor Jiang"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.55": {
    "title": "Findings of the WMT 2023 Shared Task on Automatic Post-Editing",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pushpak Bhattacharyya",
      "Rajen Chatterjee",
      "Markus Freitag",
      "Diptesh Kanojia",
      "Matteo Negri",
      "Marco Turchi"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.56": {
    "title": "Findings of the WMT 2023 Shared Task on Low-Resource Indic Language Translation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Santanu Pal",
      "Partha Pakray",
      "Sahinur Rahman Laskar",
      "Lenin Laitonjam",
      "Vanlalmuansangi Khenglawt",
      "Sunita Warjri",
      "Pankaj Kundan Dadure",
      "Sandeep Kumar Dash"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.57": {
    "title": "ACES: Translation Accuracy Challenge Sets at WMT 2023",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chantal Amrhein",
      "Nikita Moghe",
      "Liane Guillou"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.58": {
    "title": "Challenging the State-of-the-art Machine Translation Metrics from a Linguistic Perspective",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eleftherios Avramidis",
      "Shushen Manakhimova",
      "Vivien Macketanz",
      "Sebastian Möller"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.59": {
    "title": "Tokengram_F, a Fast and Accurate Token-based chrF++ Derivative",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sören Dreano",
      "Derek Molloy",
      "Noel Murphy"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.60": {
    "title": "Embed_Llama: Using LLM Embeddings for the Metrics Shared Task",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sören Dreano",
      "Derek Molloy",
      "Noel Murphy"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.61": {
    "title": "eBLEU: Unexpectedly Good Machine Translation Evaluation Using Simple Word Embeddings",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad ElNokrashy",
      "Tom Kocmi"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.62": {
    "title": "Cometoid: Distilling Strong Reference-based Machine Translation Metrics into Even Stronger Quality Estimation Metrics",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thamme Gowda",
      "Tom Kocmi",
      "Marcin Junczys-Dowmunt"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.63": {
    "title": "MetricX-23: The Google Submission to the WMT 2023 Metrics Shared Task",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juraj Juraska",
      "Mara Finkelstein",
      "Daniel Deutsch",
      "Aditya Siddhant",
      "Mehdi Mirzazadeh",
      "Markus Freitag"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.64": {
    "title": "GEMBA-MQM: Detecting Translation Quality Error Spans with GPT-4",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tom Kocmi",
      "Christian Federmann"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.65": {
    "title": "Metric Score Landscape Challenge (MSLC23): Understanding Metrics' Performance on a Wider Landscape of Translation Quality",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chi-kiu Lo",
      "Samuel Larkin",
      "Rebecca Knowles"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.66": {
    "title": "MEE4 and XLsim : IIIT HYD's Submissions' for WMT23 Metrics Shared Task",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ananya Mukherjee",
      "Manish Shrivastava"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.67": {
    "title": "Quality Estimation Using Minimum Bayes Risk",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Subhajit Naskar",
      "Daniel Deutsch",
      "Markus Freitag"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.68": {
    "title": "Evaluating Metrics for Document-context Evaluation in Machine Translation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vikas Raunak",
      "Tom Kocmi",
      "Matt Post"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.69": {
    "title": "Semantically-Informed Regressive Encoder Score",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vasiliy Viskov",
      "George Kokush",
      "Daniil Larionov",
      "Steffen Eger",
      "Alexander Panchenko"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.70": {
    "title": "Empowering a Metric with LLM-assisted Named Entity Annotation: HW-TSC's Submission to the WMT23 Metrics Shared Task",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhanglin Wu",
      "Yilun Liu",
      "Min Zhang",
      "Xiaofeng Zhao",
      "Junhao Zhu",
      "Ming Zhu",
      "Xiaosong Qiao",
      "Jingfei Zhang",
      "Ma Miaomiao",
      "Zhao Yanqing",
      "Song Peng",
      "Shimin Tao",
      "Hao Yang",
      "Yanfei Jiang"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.71": {
    "title": "Unify Word-level and Span-level Tasks: NJUNLP's Participation for the WMT2023 Quality Estimation Shared Task",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Geng",
      "Zhejian Lai",
      "Yu Zhang",
      "Shimin Tao",
      "Hao Yang",
      "Jiajun Chen",
      "Shujian Huang"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.72": {
    "title": "HW-TSC 2023 Submission for the Quality Estimation Shared Task",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuang Li",
      "Chang Su",
      "Ming Zhu",
      "Mengyao Piao",
      "Xinglin Lyu",
      "Min Zhang",
      "Hao Yang"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.73": {
    "title": "Scaling up CometKiwi: Unbabel-IST 2023 Submission for the Quality Estimation Shared Task",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ricardo Rei",
      "Nuno M. Guerreiro",
      "JosÃ© Pombal",
      "Daan van Stigt",
      "Marcos Treviso",
      "Luisa Coheur",
      "José G. C. de Souza",
      "André Martins"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.74": {
    "title": "SurreyAI 2023 Submission for the Quality Estimation Shared Task",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Archchana Sindhujan",
      "Diptesh Kanojia",
      "Constantin Orasan",
      "Tharindu Ranasinghe"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.75": {
    "title": "MMT's Submission for the WMT 2023 Quality Estimation Shared Task",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yulong Wu",
      "Viktor Schlegel",
      "Daniel Beck",
      "Riza Batista-Navarro"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.76": {
    "title": "IOL Research's Submission for WMT 2023 Quality Estimation Shared Task",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyu Yan"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.77": {
    "title": "SJTU-MTLAB's Submission to the WMT23 Word-Level Auto Completion Task",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyu Chen",
      "Rui Wang"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.78": {
    "title": "PRHLT's Submission to WLAC 2023",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Angel Navarro",
      "Miguel Domingo",
      "Francisco Casacuberta"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.79": {
    "title": "KnowComp Submission for WMT23 Word-Level AutoCompletion Task",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Wu",
      "Haochen Shi",
      "Weiqi Wang",
      "Yangqiu Song"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.80": {
    "title": "Terminology-Aware Translation with Constrained Decoding and Large Language Model Prompting",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikolay Bogoychev",
      "Pinzhen Chen"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.81": {
    "title": "Lingua Custodia's Participation at the WMT 2023 Terminology Shared Task",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingshu Liu",
      "Mariam Nakhlé",
      "Gaëtan Caillout",
      "Raheel Qadar"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.82": {
    "title": "Domain Terminology Integration into Machine Translation: Leveraging Large Language Models",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yasmin Moslem",
      "Gianfranco Romani",
      "Mahdi Molaei",
      "John D. Kelleher",
      "Rejwanul Haque",
      "Andy Way"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.83": {
    "title": "OPUS-CAT Terminology Systems for the WMT23 Terminology Shared Task",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tommi Nieminen"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.84": {
    "title": "VARCO-MT: NCSOFT's WMT'23 Terminology Shared Task Submission",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Geon Woo Park",
      "Junghwa Lee",
      "Meiying Ren",
      "Allison Shindell",
      "Yeonsoo Lee"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.85": {
    "title": "HW-TSC's Participation in the WMT 2023 Automatic Post Editing Shared Task",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawei Yu",
      "Min Zhang",
      "Zhao Yanqing",
      "Xiaofeng Zhao",
      "Yuang Li",
      "Su Chang",
      "Yinglu Li",
      "Ma Miaomiao",
      "Shimin Tao",
      "Hao Yang"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.86": {
    "title": "Neural Machine Translation for English - Manipuri and English - Assamese",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Goutam Agrawal",
      "Rituraj Das",
      "Anupam Biswas",
      "Dalton Meitei Thounaojam"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.87": {
    "title": "GUIT-NLP's Submission to Shared Task: Low Resource Indic Language Translation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mazida Ahmed",
      "Kuwali Talukdar",
      "Parvez Boruah",
      "Prof. Shikhar Kumar Sarma",
      "Kishore Kashyap"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.88": {
    "title": "NICT-AI4B's Submission to the Indic MT Shared Task in WMT 2023",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raj Dabre",
      "Jay Gala",
      "Pranjal Chitale"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.89": {
    "title": "Machine Translation Advancements for Low-Resource Indian Languages in WMT23: CFILT-IITB's Effort for Bridging the Gap",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pranav Gaikwad",
      "Meet Doshi",
      "Sourabh Deoghare",
      "Pushpak Bhattacharyya"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.90": {
    "title": "Low-Resource Machine Translation Systems for Indic Languages",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ivana Kvapilíková",
      "Ondřej Bojar"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.91": {
    "title": "MUNI-NLP Systems for Low-resource Indic Machine Translation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Edoardo Signoroni",
      "Pavel Rychly"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.92": {
    "title": "NITS-CNLP Low-Resource Neural Machine Translation Systems of English-Manipuri Language Pair",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kshetrimayum Boynao Singh",
      "Avichandra Singh Ningthoujam",
      "Loitongbam Sanayai Meetei",
      "Sivaji Bandyopadhyay",
      "Thoudam Doren Singh"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.93": {
    "title": "IACS-LRILT: Machine Translation for Low-Resource Indic Languages",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dhairya Suman",
      "Atanu Mandal",
      "Santanu Pal",
      "Sudip Naskar"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.94": {
    "title": "IOL Research Machine Translation Systems for WMT23 Low-Resource Indic Language Translation Shared Task",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenbo Zhang"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.95": {
    "title": "Trained MT Metrics Learn to Cope with Machine-translated References",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jannis Vamvas",
      "Tobias Domhan",
      "Sony Trenous",
      "Rico Sennrich",
      "Eva Hasler"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.96": {
    "title": "Training and Meta-Evaluating Machine Translation Evaluation Metrics at the Paragraph Level",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Deutsch",
      "Juraj Juraska",
      "Mara Finkelstein",
      "Markus Freitag"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.97": {
    "title": "Automating Behavioral Testing in Machine Translation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Javier Ferrando",
      "Matthias Sperber",
      "Hendra Setiawan",
      "Dominic Telaar",
      "Saša Hasan"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.98": {
    "title": "One Wide Feedforward Is All You Need",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Telmo Pires",
      "António Vilarinho Lopes",
      "Yannick Assogba",
      "Hendra Setiawan"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.99": {
    "title": "A Benchmark for Evaluating Machine Translation Metrics on Dialects without Standard Orthography",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Noëmi Aepli",
      "Chantal Amrhein",
      "Florian Schottmann",
      "Rico Sennrich"
    ]
  },
  "https://aclanthology.org/2023.wmt-1.100": {
    "title": "The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation",
    "volume": "workshop",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Patrick Fernandes",
      "Daniel Deutsch",
      "Mara Finkelstein",
      "Parker Riley",
      "André Martins",
      "Graham Neubig",
      "Ankush Garg",
      "Jonathan Clark",
      "Markus Freitag",
      "Orhan Firat"
    ]
  }
}