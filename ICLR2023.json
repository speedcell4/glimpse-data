{
  "https://openreview.net/forum?id=7YfHla7IxBJ": {
    "title": "Encoding Recurrence into Transformers",
    "volume": "oral",
    "abstract": "This paper novelly breaks down with ignorable loss an RNN layer into a sequence of simple RNNs, each of which can be further rewritten into a lightweight positional encoding matrix of a self-attention, named the Recurrence Encoding Matrix (REM). Thus, recurrent dynamics introduced by the RNN layer can be encapsulated into the positional encodings of a multihead self-attention, and this makes it possible to seamlessly incorporate these recurrent dynamics into a Transformer, leading to a new module, Self-Attention with Recurrence (RSA). The proposed module can leverage the recurrent inductive bias of REMs to achieve a better sample efficiency than its corresponding baseline Transformer, while the self-attention is used to model the remaining non-recurrent signals. The relative proportions of these two components are controlled by a data-driven gated mechanism, and the effectiveness of RSA modules are demonstrated by four sequential learning tasks",
    "checked": true,
    "id": "ccf84c100fa78c599d0e901a3754ff044aa6bd9e",
    "semantic_title": "encoding recurrence into transformers",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=l6CpxixmUg": {
    "title": "Modeling content creator incentives on algorithm-curated platforms",
    "volume": "oral",
    "abstract": "Content creators compete for user attention. Their reach crucially depends on algorithmic choices made by developers on online platforms. To maximize exposure, many creators adapt strategically, as evidenced by examples like the sprawling search engine optimization industry. This begets competition for the finite user attention pool. We formalize these dynamics in what we call an exposure game, a model of incentives induced by modern algorithms including factorization and (deep) two-tower architectures. We prove that seemingly innocuous algorithmic choices—e.g., non-negative vs. unconstrained factorization—significantly affect the existence and character of (Nash) equilibria in exposure games. We proffer use of creator behavior models like ours for an (ex-ante) pre-deployment audit. Such an audit can identify misalignment between desirable and incentivized content, and thus complement post-hoc measures like content filtering and moderation. To this end, we propose tools for numerically finding equilibria in exposure games, and illustrate results of an audit on the MovieLens and LastFM datasets. Among else, we find that the strategically produced content exhibits strong dependence between algorithmic exploration and content diversity, and between model expressivity and bias towards gender-based user and creator groups",
    "checked": true,
    "id": "740dbd216cb102c8e2816bdbb08365ecc3340525",
    "semantic_title": "modeling content creator incentives on algorithm-curated platforms",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=paGvsrl4Ntr": {
    "title": "Transfer NAS with Meta-learned Bayesian Surrogates",
    "volume": "oral",
    "abstract": "While neural architecture search (NAS) is an intensely-researched area, approaches typically still suffer from either (i) high computational costs or (ii) lack of robustness across datasets and experiments. Furthermore, most methods start searching for an optimal architecture from scratch, ignoring prior knowledge. This is in contrast to the manual design process by researchers and engineers that leverage previous deep learning experiences by, e.g., transferring architectures from previously solved, related problems. We propose to adopt this human design strategy and introduce a novel surrogate for NAS, that is meta-learned across prior architecture evaluations across different datasets. We utilizes Bayesian Optimization (BO) with deep-kernel Gaussian Processes, graph neural networks for the architecture embeddings and a transformer-based set encoder of datasets. As a result, our method consistently achieves state-of-the-art results on six computer vision datasets, while being as fast as one-shot NAS methods",
    "checked": true,
    "id": "05a4880bc44ee91b846541a330ba5aa7a874cdf0",
    "semantic_title": "transfer nas with meta-learned bayesian surrogates",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=067CGykiZTS": {
    "title": "Scaling Up Probabilistic Circuits by Latent Variable Distillation",
    "volume": "oral",
    "abstract": "Probabilistic Circuits (PCs) are a unified framework for tractable probabilistic models that support efficient computation of various probabilistic queries (e.g., marginal probabilities). One key challenge is to scale PCs to model large and high-dimensional real-world datasets: we observe that as the number of parameters in PCs increases, their performance immediately plateaus. This phenomenon suggests that the existing optimizers fail to exploit the full expressive power of large PCs. We propose to overcome such bottleneck by latent variable distillation: we leverage the less tractable but more expressive deep generative models to provide extra supervision over the latent variables of PCs. Specifically, we extract information from Transformer-based generative models to assign values to latent variables of PCs, providing guidance to PC optimizers. Experiments on both image and language modeling benchmarks (e.g., ImageNet and WikiText-2) show that latent variable distillation substantially boosts the performance of large PCs compared to their counterparts without latent variable distillation. In particular, on the image modeling benchmarks, PCs achieve competitive performance against some of the widely-used deep generative models, including variational autoencoders and flow-based models, opening up new avenues for tractable generative modeling. Our code can be found at https://github.com/UCLA-StarAI/LVD",
    "checked": true,
    "id": "af6e8ef9352ed2a82b168032c6d35655afc54f57",
    "semantic_title": "scaling up probabilistic circuits by latent variable distillation",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=6H_uOfcwiVh": {
    "title": "A Kernel Perspective of Skip Connections in Convolutional Networks",
    "volume": "oral",
    "abstract": "Over-parameterized residual networks (ResNets) are amongst the most successful convolutional neural architectures for image processing. Here we study their properties through their Gaussian Process and Neural Tangent kernels. We derive explicit formulas for these kernels, analyze their spectra, and provide bounds on their implied condition numbers. Our results indicate that (1) with ReLU activation, the eigenvalues of these residual kernels decay polynomially at a similar rate compared to the same kernels when skip connections are not used, thus maintaining a similar frequency bias; (2) however, residual kernels are more locally biased. Our analysis further shows that the matrices obtained by these residual kernels yield favorable condition numbers at finite depths than those obtained without the skip connections, enabling therefore faster convergence of training with gradient descent",
    "checked": true,
    "id": "9a3b2b0755d1a53ceaeda57e1a843912b74e6aad",
    "semantic_title": "a kernel perspective of skip connections in convolutional networks",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=vaxnu-Utr4l": {
    "title": "WikiWhy: Answering and Explaining Cause-and-Effect Questions",
    "volume": "oral",
    "abstract": "As large language models (LLMs) grow larger and more sophisticated, assessing their \"reasoning\" capabilities in natural language grows more challenging. Recent question answering (QA) benchmarks that attempt to assess reasoning are often limited by a narrow scope of covered situations and subject matters. We introduce WikiWhy, a QA dataset built around a novel auxiliary task: explaining why an answer is true in natural language. WikiWhy contains over 9,000 \"why\" question-answer-rationale triples, grounded on Wikipedia facts across a diverse set of topics. Each rationale is a set of supporting statements connecting the question to the answer. WikiWhy serves as a benchmark for the reasoning capabilities of LLMs because it demands rigorous explicit rationales for each answer to demonstrate the acquisition of implicit commonsense knowledge, which is unlikely to be easily memorized. GPT-3 baselines achieve only 38.7% human-evaluated correctness in the end-to-end answer & explain condition, leaving significant room for future improvements",
    "checked": true,
    "id": "8345d757e9127eff382d5285fef99312eaf283cd",
    "semantic_title": "wikiwhy: answering and explaining cause-and-effect questions",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=CQsmMYmlP5T": {
    "title": "Git Re-Basin: Merging Models modulo Permutation Symmetries",
    "volume": "oral",
    "abstract": "The success of deep learning is due in large part to our ability to solve certain massive non-convex optimization problems with relative ease. Though non-convex optimization is NP-hard, simple algorithms -- often variants of stochastic gradient descent -- exhibit surprising effectiveness in fitting large neural networks in practice. We argue that neural network loss landscapes often contain (nearly) a single basin after accounting for all possible permutation symmetries of hidden units a la Entezari et al. 2021. We introduce three algorithms to permute the units of one model to bring them into alignment with a reference model in order to merge the two models in weight space. This transformation produces a functionally equivalent set of weights that lie in an approximately convex basin near the reference model. Experimentally, we demonstrate the single basin phenomenon across a variety of model architectures and datasets, including the first (to our knowledge) demonstration of zero-barrier linear mode connectivity between independently trained ResNet models on CIFAR-10. Additionally, we identify intriguing phenomena relating model width and training time to mode connectivity. Finally, we discuss shortcomings of the linear mode connectivity hypothesis, including a counterexample to the single basin theory",
    "checked": true,
    "id": "a9e20180153f6c139a4b6f2791b535fa6ffc3959",
    "semantic_title": "git re-basin: merging models modulo permutation symmetries",
    "citation_count": 354,
    "authors": []
  },
  "https://openreview.net/forum?id=LQIjzPdDt3q": {
    "title": "The Role of Coverage in Online Reinforcement Learning",
    "volume": "oral",
    "abstract": "Coverage conditions---which assert that the data logging distribution adequately covers the state space---play a fundamental role in determining the sample complexity of offline reinforcement learning. While such conditions might seem irrelevant to online reinforcement learning at first glance, we establish a new connection by showing---somewhat surprisingly---that the mere existence of a data distribution with good coverage can enable sample-efficient online RL. Concretely, we show that coverability---that is, existence of a data distribution that satisfies a ubiquitous coverage condition called concentrability---can be viewed as a structural property of the underlying MDP, and can be exploited by standard algorithms for sample-efficient exploration, even when the agent does not know said distribution. We complement this result by proving that several weaker notions of coverage, despite being sufficient for offline RL, are insufficient for online RL. We also show that existing complexity measures for online RL, including Bellman rank and Bellman-Eluder dimension, fail to optimally capture coverability, and propose a new complexity measure, the self-normalized coefficient, to provide a unification",
    "checked": true,
    "id": "3d444eafdd2cd17af73cc88cdbe35e6e21795ff2",
    "semantic_title": "the role of coverage in online reinforcement learning",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=FZdJQgy05rz": {
    "title": "Is the Performance of My Deep Network Too Good to Be True? A Direct Approach to Estimating the Bayes Error in Binary Classification",
    "volume": "oral",
    "abstract": "There is a fundamental limitation in the prediction performance that a machine learning model can achieve due to the inevitable uncertainty of the prediction target. In classification problems, this can be characterized by the Bayes error, which is the best achievable error with any classifier. The Bayes error can be used as a criterion to evaluate classifiers with state-of-the-art performance and can be used to detect test set overfitting. We propose a simple and direct Bayes error estimator, where we just take the mean of the labels that show \\emph{uncertainty} of the class assignments. Our flexible approach enables us to perform Bayes error estimation even for weakly supervised data. In contrast to others, our method is model-free and even instance-free. Moreover, it has no hyperparameters and gives a more accurate estimate of the Bayes error than several baselines empirically. Experiments using our method suggest that recently proposed deep networks such as the Vision Transformer may have reached, or is about to reach, the Bayes error for benchmark datasets. Finally, we discuss how we can study the inherent difficulty of the acceptance/rejection decision for scientific articles, by estimating the Bayes error of the ICLR papers from 2017 to 2023",
    "checked": true,
    "id": "0d564d688e4e25bf640adf46387f0baf31beefbb",
    "semantic_title": "is the performance of my deep network too good to be true? a direct approach to estimating the bayes error in binary classification",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=4-k7kUavAj": {
    "title": "Offline Q-learning on Diverse Multi-Task Data Both Scales And Generalizes",
    "volume": "oral",
    "abstract": "The potential of offline reinforcement learning (RL) is that high-capacity models trained on large, heterogeneous datasets can lead to agents that generalize broadly, analogously to similar advances in vision and NLP. However, recent works argue that offline RL methods encounter unique challenges to scaling up model capacity. Drawing on the learnings from these works, we re-examine previous design choices and find that with appropriate choices: ResNets, cross-entropy based distributional backups, and feature normalization, offline Q-learning algorithms exhibit strong performance that scales with model capacity. Using multi-task Atari as a testbed for scaling and generalization, we train a single policy on 40 games with near-human performance using up-to 80 million parameter networks, finding that model performance scales favorably with capacity. In contrast to prior work, we extrapolate beyond dataset performance even when trained entirely on a large (400M transitions) but highly suboptimal dataset (51% human-level performance). Compared to return-conditioned supervised approaches, offline Q-learning scales similarly with model capacity and has better performance, especially when the dataset is suboptimal. Finally, we show that offline Q-learning with a diverse dataset is sufficient to learn powerful representations that facilitate rapid transfer to novel games and fast online learning on new variations of a training game, improving over existing state-of-the-art representation learning approaches",
    "checked": true,
    "id": "9bdad7f737c1303f199384c5b65dc67da7e5c4d8",
    "semantic_title": "offline q-learning on diverse multi-task data both scales and generalizes",
    "citation_count": 54,
    "authors": []
  },
  "https://openreview.net/forum?id=0g0X4H8yN4I": {
    "title": "What learning algorithm is in-context learning? Investigations with linear models",
    "volume": "oral",
    "abstract": "Neural sequence models, especially transformers, exhibit a remarkable capacity for in-context learning. They can construct new predictors from sequences of labeled examples $(x, f(x))$ presented in the input without further parameter updates. We investigate the hypothesis that transformer-based in-context learners implement standard learning algorithms implicitly, by encoding context-specific parametric models in their hidden representations, and updating these implicit models as new examples appear in the context. Using linear regression as a model problem, we offer three sources of evidence for this hypothesis. First, we prove by construction that transformers can implement learning algorithms for linear models based on gradient descent and closed-form computation of regression parameters. Second, we show that trained in-context learners closely match the predictors computed by gradient descent, ridge regression, and exact least-squares regression, transitioning between different predictors as transformer depth and dataset noise vary. Third, we present preliminary evidence that in-context learners share algorithmic features with these predictors: learners' late layers encode weight vectors and moment matrices. These results suggest that in-context learning is understandable in algorithmic terms, and that (at least in the linear case) learners may work by rediscovering standard estimation algorithms",
    "checked": true,
    "id": "7aa801b907b59b8ee4cfb1296d9dac22c5164c5d",
    "semantic_title": "what learning algorithm is in-context learning? investigations with linear models",
    "citation_count": 509,
    "authors": []
  },
  "https://openreview.net/forum?id=Uuf2q9TfXGA": {
    "title": "Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning",
    "volume": "oral",
    "abstract": "We formally study how \\emph{ensemble} of deep learning models can improve test accuracy, and how the superior performance of ensemble can be distilled into a single model using \\emph{knowledge distillation}. We consider the challenging case where the ensemble is simply an average of the outputs of a few independently trained neural networks with the \\emph{same} architecture, trained using the \\emph{same} algorithm on the \\emph{same} data set, and they only differ by the random seeds used in the initialization. We show that ensemble/knowledge distillation in \\emph{deep learning} works very differently from traditional learning theory (such as boosting or NTKs). We develop a theory showing that when data has a structure we refer to as ``multi-view'', then ensemble of independently trained neural networks can provably improve test accuracy, and such superior test accuracy can also be provably distilled into a single model. Our result sheds light on how ensemble works in deep learning in a way that is completely different from traditional theorems, and how the ``dark knowledge'' is hidden in the outputs of the ensemble and can be used in distillation",
    "checked": false,
    "id": "d4838211d7f65628f56b9f6faab30a95ff7b51f8",
    "semantic_title": "for prediction city region re-weighting",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KRLUvxh8uaX": {
    "title": "When and Why Vision-Language Models Behave like Bags-Of-Words, and What to Do About It?",
    "volume": "oral",
    "abstract": "Despite the success of large vision and language models (VLMs) in many downstream applications, it is unclear how well they encode the compositional relationships between objects and attributes. Here, we create the Attribution, Relation, and Order (ARO) benchmark to systematically evaluate the ability of VLMs to understand different types of relationships, attributes, and order information. ARO consists of \\emph{Visual Genome Attribution}, to test the understanding of objects' properties; \\emph{Visual Genome Relation}, to test for relational understanding; and \\emph{COCO-Order \\& Flickr30k-Order}, to test for order sensitivity in VLMs. ARO is orders of magnitude larger than previous benchmarks of compositionality, with more than 50,000 test cases. We present the settings where state-of-the-art VLMs behave like bags-of-words---i.e. when they have poor relational understanding, can blunder when linking objects to their attributes, and demonstrate a severe lack of order sensitivity. VLMs are predominantly trained and evaluated on large scale datasets with rich compositional structure in the images and captions. Yet, training on these datasets has not been enough to address the lack of compositional understanding, and evaluating on these datasets has failed to surface this deficiency. To understand why these limitations emerge and are not represented in the standard tests, we zoom into the evaluation and training procedures. We demonstrate that it is possible to perform well on image-text retrieval over existing datasets without using the composition and order information. This further motivates the value of using ARO to benchmark VLMs. Given that contrastive pretraining optimizes for retrieval on large datasets with similar shortcuts, we hypothesize that this can explain why the models do not need to learn to represent compositional information. This finding suggests a natural solution: composition-aware hard negative mining. We show that a simple-to-implement modification of contrastive learning significantly improves the performance on tasks requiring understanding of order and compositionality",
    "checked": true,
    "id": "10667c1ae4b49808772b5a377c5b52196701267f",
    "semantic_title": "when and why vision-language models behave like bags-of-words, and what to do about it?",
    "citation_count": 430,
    "authors": []
  },
  "https://openreview.net/forum?id=Zeb5mTuqT5": {
    "title": "Confidence-Conditioned Value Functions for Offline Reinforcement Learning",
    "volume": "oral",
    "abstract": "Offline reinforcement learning (RL) promises the ability to learn effective policies solely using existing, static datasets, without any costly online interaction. To do so, offline RL methods must handle distributional shift between the dataset and the learned policy. The most common approach is to learn conservative, or lower-bound, value functions, which underestimate the return of OOD actions. However, such methods exhibit one notable drawback: policies optimized on such value functions can only behave according to a fixed, possibly suboptimal, degree of conservatism. However, this can be alleviated if we instead are able to learn policies for varying degrees of conservatism at training time and devise a method to dynamically choose one of them during evaluation. To do so, in this work, we propose learning value functions that additionally condition on the degree of conservatism, which we dub confidence-conditioned value functions. We derive a new form of a Bellman backup that simultaneously learns Q-values for any degree of confidence with high probability. By conditioning on confidence, our value functions enable adaptive strategies during online evaluation by controlling for confidence level using the history of observations thus far. This approach can be implemented in practice by conditioning the Q-function from existing conservative algorithms on the confidence. We theoretically show that our learned value functions produce conservative estimates of the true value at any desired confidence. Finally, we empirically show that our algorithm outperforms existing conservative offline RL algorithms on multiple discrete control domains",
    "checked": true,
    "id": "e9565e0242aed311888bf4dcc6fef06faf6fc61a",
    "semantic_title": "confidence-conditioned value functions for offline reinforcement learning",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=hJqGbUpDGV": {
    "title": "On the Sensitivity of Reward Inference to Misspecified Human Models",
    "volume": "oral",
    "abstract": "Inferring reward functions from human behavior is at the center of value alignment – aligning AI objectives with what we, humans, actually want. But doing so relies on models of how humans behave given their objectives. After decades of research in cognitive science, neuroscience, and behavioral economics, obtaining accurate human models remains an open research topic. This begs the question: how accurate do these models need to be in order for the reward inference to be accurate? On the one hand, if small errors in the model can lead to catastrophic error in inference, the entire framework of reward learning seems ill-fated, as we will never have perfect models of human behavior. On the other hand, if as our models improve, we can have a guarantee that reward accuracy also improves, this would show the benefit of more work on the modeling side. We study this question both theoretically and empirically. We do show that it is unfortunately possible to construct small adversarial biases in behavior that lead to arbitrarily large errors in the inferred reward. However, and arguably more importantly, we are also able to identify reasonable assumptions under which the reward inference error can be bounded linearly in the error in the human model. Finally, we verify our theoretical insights in discrete and continuous control tasks with simulated and human data",
    "checked": true,
    "id": "aef62643b561991d7db70ff29b009822065641d8",
    "semantic_title": "on the sensitivity of reward inference to misspecified human models",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=H3HcEJA2Um": {
    "title": "Time Will Tell: New Outlooks and A Baseline for Temporal Multi-View 3D Object Detection",
    "volume": "oral",
    "abstract": "While recent camera-only 3D detection methods leverage multiple timesteps, the limited history they use significantly hampers the extent to which temporal fusion can improve object perception. Observing that existing works' fusion of multi-frame images are instances of temporal stereo matching, we find that performance is hindered by the interplay between 1) the low granularity of matching resolution and 2) the sub-optimal multi-view setup produced by limited history usage. Our theoretical and empirical analysis demonstrates that the optimal temporal difference between views varies significantly for different pixels and depths, making it necessary to fuse many timesteps over long-term history. Building on our investigation, we propose to generate a cost volume from a long history of image observations, compensating for the coarse but efficient matching resolution with a more optimal multi-view matching setup. Further, we augment the per-frame monocular depth predictions used for long-term, coarse matching with short-term, fine-grained matching and find that long and short term temporal fusion are highly complementary. While maintaining high efficiency, our framework sets new state-of-the-art on nuScenes, achieving first place on the test set and outperforming previous best art by 5.2% mAP and 3.7% NDS on the validation set. Code will be released here: https://github.com/Divadi/SOLOFusion",
    "checked": true,
    "id": "ebde9fbe26c8d3a04a3d159ad7d127ce2df7305c",
    "semantic_title": "time will tell: new outlooks and a baseline for temporal multi-view 3d object detection",
    "citation_count": 167,
    "authors": []
  },
  "https://openreview.net/forum?id=DEGjDDV22pI": {
    "title": "Dichotomy of Control: Separating What You Can Control from What You Cannot",
    "volume": "oral",
    "abstract": "Future- or return-conditioned supervised learning is an emerging paradigm for offline reinforcement learning (RL), in which the future outcome (i.e., return) associated with a sequence of actions in an offline dataset is used as input to a policy trained to imitate those same actions. While return-conditioning is at the heart of popular algorithms such as decision transformer (DT), these methods tend to perform poorly in highly stochastic environments, where an occasional high return associated with a sequence of actions may be due more to the randomness of the environment than to the actions themselves. Such situations can lead to a learned policy that is inconsistent with its conditioning inputs; i.e., using the policy – while conditioned on a specific desired return – to act in the environment can lead to a distribution of real returns that is wildly different than desired. In this work, we propose the dichotomy of control (DoC), a future-conditioned supervised learning framework that separates mechanisms within a policy's control (actions) from those outside of a policy's control (environment stochasticity). We achieve this by conditioning the policy on a latent variable representation of the future and designing a mutual information constraint that removes any future information from the latent variable that is only due to randomness of the environment. Theoretically, we show that DoC yields policies that are consistent with their conditioning inputs, ensuring that conditioning a learned policy on a desired high-return future outcome will correctly induce high-return behavior. Empirically, we show that DoC is able to achieve significantly better performance than DT on environments with highly stochastic rewards (e.g., Bandit) and transitions (e.g., FrozenLake)",
    "checked": true,
    "id": "834c8c95ff1129eb197bfdfa18f6bdf3c11c205c",
    "semantic_title": "dichotomy of control: separating what you can control from what you cannot",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=en9V5F8PR-": {
    "title": "Learning where and when to reason in neuro-symbolic inference",
    "volume": "oral",
    "abstract": "The integration of hard constraints on neural network outputs is a very desirable capability. This allows to instill trust in AI by guaranteeing the sanity of that neural network predictions with respect to domain knowledge. Recently, this topic has received a lot of attention. However, all the existing methods usually either impose the constraints in a \"weak\" form at training time, with no guarantees at inference, or fail to provide a general framework that supports different tasks and constraint types. We tackle this open problem from a neuro-symbolic perspective. Our pipeline enhances a conventional neural predictor with (1) a symbolic reasoning module capable of correcting structured prediction errors and (2) a neural attention module that learns to direct the reasoning effort to focus on potential prediction errors, while keeping other outputs unchanged. This framework provides an appealing trade-off between the efficiency of constraint-free neural inference and the prohibitive cost of exhaustive reasoning at inference time. We show that our method outperforms the state of the art on visual-Sudoku, and can also benefit visual scene graph prediction. Furthermore, it can improve the performance of existing neuro-symbolic systems that lack our explicit reasoning during inference",
    "checked": true,
    "id": "2e48527e9ea8d8fe02050af2c355d010a990d5be",
    "semantic_title": "learning where and when to reason in neuro-symbolic inference",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=kDEL91Dufpa": {
    "title": "On the duality between contrastive and non-contrastive self-supervised learning",
    "volume": "oral",
    "abstract": "Recent approaches in self-supervised learning of image representations can be categorized into different families of methods and, in particular, can be divided into contrastive and non-contrastive approaches. While differences between the two families have been thoroughly discussed to motivate new approaches, we focus more on the theoretical similarities between them. By designing contrastive and covariance based non-contrastive criteria that can be related algebraically and shown to be equivalent under limited assumptions, we show how close those families can be. We further study popular methods and introduce variations of them, allowing us to relate this theoretical result to current practices and show the influence (or lack thereof) of design choices on downstream performance. Motivated by our equivalence result, we investigate the low performance of SimCLR and show how it can match VICReg's with careful hyperparameter tuning, improving significantly over known baselines. We also challenge the popular assumption that non-contrastive methods need large output dimensions. Our theoretical and quantitative results suggest that the numerical gaps between contrastive and non-contrastive methods in certain regimes can be closed given better network design choices and hyperparameter tuning. The evidence shows that unifying different SOTA methods is an important direction to build a better understanding of self-supervised learning",
    "checked": true,
    "id": "11c16254f7b61687b5d9b7637de032461a6ebb5f",
    "semantic_title": "on the duality between contrastive and non-contrastive self-supervised learning",
    "citation_count": 96,
    "authors": []
  },
  "https://openreview.net/forum?id=FjNys5c7VyY": {
    "title": "DreamFusion: Text-to-3D using 2D Diffusion",
    "volume": "oral",
    "abstract": "Recent breakthroughs in text-to-image synthesis have been driven by diffusion models trained on billions of image-text pairs. Adapting this approach to 3D synthesis would require large-scale datasets of labeled 3D or multiview data and efficient architectures for denoising 3D data, neither of which currently exist. In this work, we circumvent these limitations by using a pretrained 2D text-to-image diffusion model to perform text-to-3D synthesis. We introduce a loss based on probability density distillation that enables the use of a 2D diffusion model as a prior for optimization of a parametric image generator. Using this loss in a DeepDream-like procedure, we optimize a randomly-initialized 3D model (a Neural Radiance Field, or NeRF) via gradient descent such that its 2D renderings from random angles achieve a low loss. The resulting 3D model of the given text can be viewed from any angle, relit by arbitrary illumination, or composited into any 3D environment. Our approach requires no 3D training data and no modifications to the image diffusion model, demonstrating the effectiveness of pretrained image diffusion models as priors",
    "checked": true,
    "id": "4c94d04afa4309ec2f06bdd0fe3781f91461b362",
    "semantic_title": "dreamfusion: text-to-3d using 2d diffusion",
    "citation_count": 2666,
    "authors": []
  },
  "https://openreview.net/forum?id=zyLVMgsZ0U_": {
    "title": "Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions",
    "volume": "oral",
    "abstract": "We provide theoretical convergence guarantees for score-based generative models (SGMs) such as denoising diffusion probabilistic models (DDPMs), which constitute the backbone of large-scale real-world generative models such as DALL$\\cdot$E 2. Our main result is that, assuming accurate score estimates, such SGMs can efficiently sample from essentially any realistic data distribution. In contrast to prior works, our results (1) hold for an $L^2$-accurate score estimate (rather than $L^\\infty$-accurate); (2) do not require restrictive functional inequality conditions that preclude substantial non-log-concavity; (3) scale polynomially in all relevant problem parameters; and (4) match state-of-the-art complexity guarantees for discretization of the Langevin diffusion, provided that the score error is sufficiently small. We view this as strong theoretical justification for the empirical success of SGMs. We also examine SGMs based on the critically damped Langevin diffusion (CLD). Contrary to conventional wisdom, we provide evidence that the use of the CLD does *not* reduce the complexity of SGMs",
    "checked": true,
    "id": "7309bf7607f4b4339f4ae288f3ad4fc36d139b5a",
    "semantic_title": "sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions",
    "citation_count": 297,
    "authors": []
  },
  "https://openreview.net/forum?id=88nT0j5jAn": {
    "title": "Universal Few-shot Learning of Dense Prediction Tasks with Visual Token Matching",
    "volume": "oral",
    "abstract": "Dense prediction tasks are a fundamental class of problems in computer vision. As supervised methods suffer from high pixel-wise labeling cost, a few-shot learning solution that can learn any dense task from a few labeled images is desired. Yet, current few-shot learning methods target a restricted set of tasks such as semantic segmentation, presumably due to challenges in designing a general and unified model that is able to flexibly and efficiently adapt to arbitrary tasks of unseen semantics. We propose Visual Token Matching (VTM), a universal few-shot learner for arbitrary dense prediction tasks. It employs non-parametric matching on patch-level embedded tokens of images and labels that encapsulates all tasks. Also, VTM flexibly adapts to any task with a tiny amount of task-specific parameters that modulate the matching algorithm. We implement VTM as a powerful hierarchical encoder-decoder architecture involving ViT backbones where token matching is performed at multiple feature hierarchies. We experiment VTM on a challenging variant of Taskonomy dataset and observe that it robustly few-shot learns various unseen dense prediction tasks. Surprisingly, it is competitive with fully supervised baselines using only 10 labeled examples of novel tasks ($0.004\\%$ of full supervision) and sometimes outperforms using $0.1\\%$ of full supervision. Codes are available at https://github.com/GitGyun/visual_token_matching",
    "checked": true,
    "id": "f17119316ada03dd7f05511457c4a15c69f7e866",
    "semantic_title": "universal few-shot learning of dense prediction tasks with visual token matching",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=dLAYGdKTi2": {
    "title": "Mitigating Gradient Bias in Multi-objective Learning: A Provably Convergent Approach",
    "volume": "oral",
    "abstract": "Many machine learning problems today have multiple objective functions. They appear either in learning with multiple criteria where learning has to make a trade-off between multiple performance metrics such as fairness, safety and accuracy; or, in multi-task learning where multiple tasks are optimized jointly, sharing inductive bias between them. This problems are often tackled by the multi-objective optimization framework. However, existing stochastic multi-objective gradient methods and its variants (e.g., MGDA, PCGrad, CAGrad, etc.) all adopt a biased noisy gradient direction, which leads to degraded empirical performance. To this end, we develop a stochastic multi-objective gradient correction (MoCo) method for multi-objective optimization. The unique feature of our method is that it can guarantee convergence without increasing the batch size even in the nonconvex setting. Simulations on multi-task supervised and reinforcement learning demonstrate the effectiveness of our method relative to the state-of-the-art methods",
    "checked": true,
    "id": "3b1dcf6d1a0e3b37641d751471bef5398f2ee3ac",
    "semantic_title": "mitigating gradient bias in multi-objective learning: a provably convergent approach",
    "citation_count": 42,
    "authors": []
  },
  "https://openreview.net/forum?id=WE_vluYUL-X": {
    "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
    "volume": "oral",
    "abstract": "While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples",
    "checked": true,
    "id": "99832586d55f540f603637e458a292406a0ed75d",
    "semantic_title": "react: synergizing reasoning and acting in language models",
    "citation_count": 3470,
    "authors": []
  },
  "https://openreview.net/forum?id=ayPPc0SyLv1": {
    "title": "Do We Really Need Complicated Model Architectures For Temporal Networks?",
    "volume": "oral",
    "abstract": "Recurrent neural network (RNN) and self-attention mechanism (SAM) are the de facto methods to extract spatial-temporal information for temporal graph learning. Interestingly, we found that although both RNN and SAM could lead to a good performance, in practice neither of them is always necessary. In this paper, we propose GraphMixer, a conceptually and technically simple architecture that consists of three components: (1) a link-encoder that is only based on multi-layer perceptrons (MLP) to summarize the information from temporal links, (2) a node-encoder that is only based on neighbor mean-pooling to summarize node information, and (3) an MLP-based link classifier that performs link prediction based on the outputs of the encoders. Despite its simplicity, GraphMixer attains an outstanding performance on temporal link prediction benchmarks with faster convergence and better generalization performance. These results motivate us to rethink the importance of simpler model architecture",
    "checked": true,
    "id": "ed23b5871365c873f3958210f2accce353c20db6",
    "semantic_title": "do we really need complicated model architectures for temporal networks?",
    "citation_count": 128,
    "authors": []
  },
  "https://openreview.net/forum?id=sP1fo2K9DFG": {
    "title": "Is Conditional Generative Modeling all you need for Decision Making?",
    "volume": "oral",
    "abstract": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone. We investigate whether these methods can directly address the problem of sequential decision-making. We view decision-making not through the lens of reinforcement learning (RL), but rather through conditional generative modeling. To our surprise, we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks. By modeling a policy as a return-conditional generative model, we avoid the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL. We further demonstrate the advantages of modeling policies as conditional generative models by considering two other conditioning variables: constraints and skills. Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills. Our results illustrate that conditional generative modeling is a powerful tool for decision-making",
    "checked": false,
    "id": "f19dfc360088922cf1d423c538662aae8d542c28",
    "semantic_title": "is conditional generative modeling all you need for decision-making?",
    "citation_count": 427,
    "authors": []
  },
  "https://openreview.net/forum?id=JL7Va5Vy15J": {
    "title": "The Lie Derivative for Measuring Learned Equivariance",
    "volume": "oral",
    "abstract": "Equivariance guarantees that a model's predictions capture key symmetries in data. When an image is translated or rotated, an equivariant model's representation of that image will translate or rotate accordingly. The success of convolutional neural networks has historically been tied to translation equivariance directly encoded in their architecture. The rising success of vision transformers, which have no explicit architectural bias towards equivariance, challenges this narrative and suggests that augmentations and training data might also play a significant role in their performance. In order to better understand the role of equivariance in recent vision models, we apply the Lie derivative, a method for measuring equivariance with strong mathematical foundations and minimal hyperparameters. Using the Lie derivative, we study the equivariance properties of hundreds of pretrained models, spanning CNNs, transformers, and Mixer architectures. The scale of our analysis allows us to separate the impact of architecture from other factors like model size or training method. Surprisingly, we find that many violations of equivariance can be linked to spatial aliasing in ubiquitous network layers, such as pointwise non-linearities, and that as models get larger and more accurate they tend to display more equivariance, regardless of architecture. For example, transformers can be more equivariant than convolutional neural networks after training",
    "checked": true,
    "id": "4721210fb0359d43edcea934d0ecb21aa9d9e98e",
    "semantic_title": "the lie derivative for measuring learned equivariance",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=K7CbYQbyYhY": {
    "title": "Agree to Disagree: Diversity through Disagreement for Better Transferability",
    "volume": "oral",
    "abstract": "Gradient-based learning algorithms have an implicit \\emph{simplicity bias} which in effect can limit the diversity of predictors being sampled by the learning procedure. This behavior can hinder the transferability of trained models by (i) favoring the learning of simpler but spurious features --- present in the training data but absent from the test data --- and (ii) by only leveraging a small subset of predictive features. Such an effect is especially magnified when the test distribution does not exactly match the train distribution---referred to as the Out of Distribution (OOD) generalization problem. However, given only the training data, it is not always possible to apriori assess if a given feature is spurious or transferable. Instead, we advocate for learning an ensemble of models which capture a diverse set of predictive features. Towards this, we propose a new algorithm D-BAT (Diversity-By-disAgreement Training), which enforces agreement among the models on the training data, but disagreement on the OOD data. We show how D-BAT naturally emerges from the notion of generalized discrepancy, as well as demonstrate in multiple experiments how the proposed method can mitigate shortcut-learning, enhance uncertainty and OOD detection, as well as improve transferability",
    "checked": true,
    "id": "962466ca8a3bf432a2d45b656ab5dbcc9caf5b16",
    "semantic_title": "agree to disagree: diversity through disagreement for better transferability",
    "citation_count": 76,
    "authors": []
  },
  "https://openreview.net/forum?id=dJruFeSRym1": {
    "title": "Efficient Conditionally Invariant Representation Learning",
    "volume": "oral",
    "abstract": "We introduce the Conditional Independence Regression CovariancE (CIRCE), a measure of conditional independence for multivariate continuous-valued variables. CIRCE applies as a regularizer in settings where we wish to learn neural features $\\varphi(X)$ of data $X$ to estimate a target $Y$, while being conditionally independent of a distractor $Z$ given $Y$. Both $Z$ and $Y$ are assumed to be continuous-valued but relatively low dimensional, whereas $X$ and its features may be complex and high dimensional. Relevant settings include domain-invariant learning, fairness, and causal learning. The procedure requires just a single ridge regression from $Y$ to kernelized features of $Z$, which can be done in advance. It is then only necessary to enforce independence of $\\varphi(X)$ from residuals of this regression, which is possible with attractive estimation properties and consistency guarantees. By contrast, earlier measures of conditional feature dependence require multiple regressions for each step of feature learning, resulting in more severe bias and variance, and greater computational cost. When sufficiently rich features are used, we establish that CIRCE is zero if and only if $\\varphi(X) \\perp \\!\\!\\! \\perp Z \\mid Y$. In experiments, we show superior performance to previous methods on challenging benchmarks, including learning conditionally invariant image features. Code for image data experiments is available at github.com/namratadeka/circe",
    "checked": true,
    "id": "422b68799bfc5cae91febc5c33d59e28b067a368",
    "semantic_title": "efficient conditionally invariant representation learning",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=SMYdcXjJh1q": {
    "title": "Aligning Model and Macaque Inferior Temporal Cortex Representations Improves Model-to-Human Behavioral Alignment and Adversarial Robustness",
    "volume": "oral",
    "abstract": "While some state-of-the-art artificial neural network systems in computer vision are strikingly accurate models of the corresponding primate visual processing, there are still many discrepancies between these models and the behavior of primates on object recognition tasks. Many current models suffer from extreme sensitivity to adversarial attacks and often do not align well with the image-by-image behavioral error patterns observed in humans. Previous research has provided strong evidence that primate object recognition behavior can be very accurately predicted by neural population activity in the inferior temporal (IT) cortex, a brain area in the late stages of the visual processing hierarchy. Therefore, here we directly test whether making the late stage representations of models more similar to that of macaque IT produces new models that exhibit more robust, primate-like behavior. We conducted chronic, large-scale multi-electrode recordings across the IT cortex in six non-human primates (rhesus macaques). We then use these data to fine-tune (end-to-end) the model \"IT\" representations such that they are more aligned with the biological IT representations, while preserving accuracy on object recognition tasks. We generate a cohort of models with a range of IT similarity scores validated on held-out animals across two image sets with distinct statistics. Across a battery of optimization conditions, we observed a strong correlation between the models' IT-likeness and alignment with human behavior, as well as an increase in its adversarial robustness. We further assessed the limitations of this approach and find that the improvements in behavioral alignment and adversarial robustness generalize across different image statistics, but not to object categories outside of those covered in our IT training set. Taken together, our results demonstrate that building models that are more aligned with the primate brain leads to more robust and human-like behavior, and call for larger neural data-sets to further augment these gains",
    "checked": true,
    "id": "4ce6d229d5f44239c948fd56ad744c012aae22e0",
    "semantic_title": "aligning model and macaque inferior temporal cortex representations improves model-to-human behavioral alignment and adversarial robustness",
    "citation_count": 43,
    "authors": []
  },
  "https://openreview.net/forum?id=De4FYqjFueZ": {
    "title": "Transformers Learn Shortcuts to Automata",
    "volume": "oral",
    "abstract": "Algorithmic reasoning requires capabilities which are most naturally understood through recurrent models of computation, like the Turing machine. However, Transformer models, while lacking recurrence, are able to perform such reasoning using far fewer layers than the number of reasoning steps. This raises the question: what solutions are these shallow and non-recurrent models finding? We investigate this question in the setting of learning automata, discrete dynamical systems naturally suited to recurrent modeling and expressing algorithmic tasks. Our theoretical results completely characterize shortcut solutions, whereby a shallow Transformer with only $o(T)$ layers can exactly replicate the computation of an automaton on an input sequence of length $T$. By representing automata using the algebraic structure of their underlying transformation semigroups, we obtain $O(\\log T)$-depth simulators for all automata and $O(1)$-depth simulators for all automata whose associated groups are solvable. Empirically, we perform synthetic experiments by training Transformers to simulate a wide variety of automata, and show that shortcut solutions can be learned via standard training. We further investigate the brittleness of these solutions and propose potential mitigations",
    "checked": true,
    "id": "e82e3f4347674b75c432cb80604d38ee630d4bf6",
    "semantic_title": "transformers learn shortcuts to automata",
    "citation_count": 195,
    "authors": []
  },
  "https://openreview.net/forum?id=hy0a5MMPUv": {
    "title": "In-context Reinforcement Learning with Algorithm Distillation",
    "volume": "oral",
    "abstract": "We propose Algorithm Distillation (AD), a method for distilling reinforcement learning (RL) algorithms into neural networks by modeling their training histories with a causal sequence model. Algorithm Distillation treats learning to reinforcement learn as an across-episode sequential prediction problem. A dataset of learning histories is generated by a source RL algorithm, and then a causal transformer is trained by autoregressively predicting actions given their preceding learning histories as context. Unlike sequential policy prediction architectures that distill post-learning or expert sequences, AD is able to improve its policy entirely in-context without updating its network parameters. We demonstrate that AD can reinforcement learn in-context in a variety of environments with sparse rewards, combinatorial task structure, and pixel-based observations, and find that AD learns a more data-efficient RL algorithm than the one that generated the source data",
    "checked": true,
    "id": "860bc4f071f35d6d8529a52c2c1858d030779a6a",
    "semantic_title": "in-context reinforcement learning with algorithm distillation",
    "citation_count": 141,
    "authors": []
  },
  "https://openreview.net/forum?id=3Pf3Wg6o-A4": {
    "title": "Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning",
    "volume": "oral",
    "abstract": "Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 46 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5-shot generalisation setting, with no fine-tuning, yields a performance improvement of over 100% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system",
    "checked": true,
    "id": "d48b29889241551e1ee6622fa78c3fa4159255dd",
    "semantic_title": "selection-inference: exploiting large language models for interpretable logical reasoning",
    "citation_count": 374,
    "authors": []
  },
  "https://openreview.net/forum?id=Y5SEe3dfniJ": {
    "title": "Compressing multidimensional weather and climate data into neural networks",
    "volume": "oral",
    "abstract": "Weather and climate simulations produce petabytes of high-resolution data that are later analyzed by researchers in order to understand climate change or severe weather. We propose a new method of compressing this multidimensional weather and climate data: a coordinate-based neural network is trained to overfit the data, and the resulting parameters are taken as a compact representation of the original grid-based data. While compression ratios range from 300x to more than 3,000x, our method outperforms the state-of-the-art compressor SZ3 in terms of weighted RMSE, MAE. It can faithfully preserve important large scale atmosphere structures and does not introduce significant artifacts. When using the resulting neural network as a 790x compressed dataloader to train the WeatherBench forecasting model, its RMSE increases by less than 2%. The three orders of magnitude compression democratizes access to high-resolution climate data and enables numerous new research directions",
    "checked": true,
    "id": "21ae428fe2742641f4af6c46e805ce359203d836",
    "semantic_title": "compressing multidimensional weather and climate data into neural networks",
    "citation_count": 37,
    "authors": []
  },
  "https://openreview.net/forum?id=iIfDQVyuFD": {
    "title": "Confidential-PROFITT: Confidential PROof of FaIr Training of Trees",
    "volume": "oral",
    "abstract": "Post hoc auditing of model fairness suffers from potential drawbacks: (1) auditing may be highly sensitive to the test samples chosen; (2) the model and/or its training data may need to be shared with an auditor thereby breaking confidentiality. We address these issues by instead providing a certificate that demonstrates that the learning algorithm itself is fair, and hence, as a consequence, so too is the trained model. We introduce a method to provide a confidential proof of fairness for training, in the context of widely used decision trees, which we term Confidential-PROFITT. We propose novel fair decision tree learning algorithms along with customized zero-knowledge proof protocols to obtain a proof of fairness that can be audited by a third party. Using zero-knowledge proofs enables us to guarantee confidentiality of both the model and its training data. We show empirically that bounding the information gain of each node with respect to the sensitive attributes reduces the unfairness of the final tree. In extensive experiments on the COMPAS, Communities and Crime, Default Credit, and Adult datasets, we demonstrate that a company can use Confidential-PROFITT to certify the fairness of their decision tree to an auditor in less than 2 minutes, thus indicating the applicability of our approach. This is true for both the demographic parity and equalized odds definitions of fairness. Finally, we extend Confidential-PROFITT to apply to ensembles of trees",
    "checked": true,
    "id": "283cefcba35b032b7286be90ea892700e201b0dc",
    "semantic_title": "confidential-profitt: confidential proof of fair training of trees",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=Nc1ZkRW8Vde": {
    "title": "Near-optimal Coresets for Robust Clustering",
    "volume": "oral",
    "abstract": "We consider robust clustering problems in $\\mathbb{R}^d$, specifically $k$-clustering problems (e.g., $k$-Median and $k$-Means) with $m$ \\emph{outliers}, where the cost for a given center set $C \\subset \\mathbb{R}^d$ aggregates the distances from $C$ to all but the furthest $m$ data points, instead of all points as in classical clustering. We focus on the $\\epsilon$-coreset for robust clustering, a small proxy of the dataset that preserves the clustering cost within $\\epsilon$-relative error for all center sets. Our main result is an $\\epsilon$-coreset of size $O(m + \\mathrm{poly}(k \\epsilon^{-1}))$ that can be constructed in near-linear time. This significantly improves previous results, which either suffers an exponential dependence on $(m + k)$ [Feldman and Schulman, SODA'12], or has a weaker bi-criteria guarantee [Huang et al., FOCS'18]. Furthermore, we show this dependence in $m$ is nearly-optimal, and the fact that it is isolated from other factors may be crucial for dealing with large number of outliers. We construct our coresets by adapting to the outlier setting a recent framework [Braverman et al., FOCS'22] which was designed for capacity-constrained clustering, overcoming a new challenge that the participating terms in the cost, particularly the excluded $m$ outlier points, are dependent on the center set $C$. We validate our coresets on various datasets, and we observe a superior size-accuracy tradeoff compared with popular baselines including uniform sampling and sensitivity sampling. We also achieve a significant speedup of existing approximation algorithms for robust clustering using our coresets",
    "checked": true,
    "id": "0aec562c8cc3d1004ea86f601b6b42734454466b",
    "semantic_title": "near-optimal coresets for robust clustering",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=0Ij9_q567Ma": {
    "title": "Targeted Hyperparameter Optimization with Lexicographic Preferences Over Multiple Objectives",
    "volume": "oral",
    "abstract": "Motivated by various practical applications, we propose a novel and general formulation of targeted multi-objective hyperparameter optimization. Our formulation allows a clear specification of an automatable optimization goal using lexicographic preference over multiple objectives. We then propose a randomized directed search method named LexiFlow to solve this problem. We demonstrate the strong empirical performance of the proposed algorithm in multiple hyperparameter optimization tasks",
    "checked": true,
    "id": "c90b845f86dd8389d971a176184e6ad6b00ad7b8",
    "semantic_title": "targeted hyperparameter optimization with lexicographic preferences over multiple objectives",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=F61FwJTZhb": {
    "title": "Mastering the Game of No-Press Diplomacy via Human-Regularized Reinforcement Learning and Planning",
    "volume": "oral",
    "abstract": "No-press Diplomacy is a complex strategy game involving both cooperation and competition that has served as a benchmark for multi-agent AI research. While self-play reinforcement learning has resulted in numerous successes in purely adversarial games like chess, Go, and poker, self-play alone is insufficient for achieving optimal performance in domains involving cooperation with humans. We address this shortcoming by first introducing a planning algorithm we call DiL-piKL that regularizes a reward-maximizing policy toward a human imitation-learned policy. We prove that this is a no-regret learning algorithm under a modified utility function. We then show that DiL-piKL can be extended into a self-play reinforcement learning algorithm we call RL-DiL-piKL that provides a model of human play while simultaneously training an agent that responds well to this human model. We used RL-DiL-piKL to train an agent we name Diplodocus. In a 200-game no-press Diplomacy tournament involving 62 human participants spanning skill levels from beginner to expert, two Diplodocus agents both achieved a higher average score than all other participants who played more than two games, and ranked first and third according to an Elo ratings model",
    "checked": true,
    "id": "7842368bf1afde87deb63333871760ae848f01f9",
    "semantic_title": "mastering the game of no-press diplomacy via human-regularized reinforcement learning and planning",
    "citation_count": 50,
    "authors": []
  },
  "https://openreview.net/forum?id=G-uNfHKrj46": {
    "title": "Efficient Attention via Control Variates",
    "volume": "oral",
    "abstract": "Random-feature-based attention (RFA) is an efficient approximation of softmax attention with linear runtime and space complexity. However, the approximation gap between RFA and conventional softmax attention is not well studied. Built upon previous progress of RFA, we characterize this gap through the lens of control variates and show that RFA can be decomposed into a sum of multiple control variate estimators for each element in the sequence. This new framework reveals that exact softmax attention can be recovered from RFA by manipulating each control variate. Besides, it allows us to develop a more flexible form of control variates, resulting in a novel attention mechanism that significantly reduces the approximation gap while maintaining linear complexity. Extensive experiments demonstrate that our model outperforms state-of-the-art efficient attention mechanisms on both vision and language tasks",
    "checked": true,
    "id": "ac608a4a6b19b3208e560eee5daadb3cc18638a2",
    "semantic_title": "efficient attention via control variates",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=k4fevFqSQcX": {
    "title": "SAM as an Optimal Relaxation of Bayes",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "a872902f3e216419fc16b3702d4669ee1c03c9a0",
    "semantic_title": "sam as an optimal relaxation of bayes",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=q0nmYciuuZN": {
    "title": "Learning on Large-scale Text-attributed Graphs via Variational Inference",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "8bb37e8ae7dd6fa8cab2407f63a61f697152717f",
    "semantic_title": "learning on large-scale text-attributed graphs via variational inference",
    "citation_count": 150,
    "authors": []
  },
  "https://openreview.net/forum?id=SJ0Lde3tRL": {
    "title": "Extreme Q-Learning: MaxEnt RL without Entropy",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "2c2180fbe7f38e88b1123e5fab43785b66814e5d",
    "semantic_title": "extreme q-learning: maxent rl without entropy",
    "citation_count": 83,
    "authors": []
  },
  "https://openreview.net/forum?id=mjzm6btqgV": {
    "title": "Efficiently Computing Nash Equilibria in Adversarial Team Markov Games",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "7ba026ef54575acf209b381cee4cfe2ad5646eb2",
    "semantic_title": "efficiently computing nash equilibria in adversarial team markov games",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=Ai8Hw3AXqks": {
    "title": "Simplified State Space Layers for Sequence Modeling",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "6d7d141c75af752ffc0d8a6184cca3f9323d6c74",
    "semantic_title": "simplified state space layers for sequence modeling",
    "citation_count": 608,
    "authors": []
  },
  "https://openreview.net/forum?id=vmjctNUSWI": {
    "title": "Moving Forward by Moving Backward: Embedding Action Impact over Action Semantics",
    "volume": "oral",
    "abstract": "A common assumption when training embodied agents is that the impact of taking an action is stable; for instance, executing the ``move ahead'' action will always move the agent forward by a fixed distance, perhaps with some small amount of actuator-induced noise. This assumption is limiting; an agent may encounter settings that dramatically alter the impact of actions: a move ahead action on a wet floor may send the agent twice as far as it expects and using the same action with a broken wheel might transform the expected translation into a rotation. Instead of relying that the impact of an action stably reflects its pre-defined semantic meaning, we propose to model the impact of actions on-the-fly using latent embeddings. By combining these latent action embeddings with a novel, transformer-based, policy head, we design an Action Adaptive Policy (AAP). We evaluate our AAP on two challenging visual navigation tasks in the AI2-THOR and Habitat environments and show that our AAP is highly performant even when faced, at inference-time, with missing actions and, previously unseen, perturbed action spaces. Moreover, we observe significant improvement in robustness against these actions when evaluating in real-world scenarios",
    "checked": true,
    "id": "cecd400cb0bdd14aa4b2568015b197d6862ede63",
    "semantic_title": "moving forward by moving backward: embedding action impact over action semantics",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=EKpMeEV0hOo": {
    "title": "SimPer: Simple Self-Supervised Learning of Periodic Targets",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "c0d116bea6af35c0cf1f9180a754b44512bd1cda",
    "semantic_title": "simper: simple self-supervised learning of periodic targets",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=mWVoBz4W0u": {
    "title": "PaLI: A Jointly-Scaled Multilingual Language-Image Model",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "28630034bb29760df01ab033b743e30b37f336ae",
    "semantic_title": "pali: a jointly-scaled multilingual language-image model",
    "citation_count": 771,
    "authors": []
  },
  "https://openreview.net/forum?id=OpC-9aBBVJe": {
    "title": "Sample-Efficient Reinforcement Learning by Breaking the Replay Ratio Barrier",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "33d84b1531f88d2bd2e516e1574f22e139133065",
    "semantic_title": "sample-efficient reinforcement learning by breaking the replay ratio barrier",
    "citation_count": 124,
    "authors": []
  },
  "https://openreview.net/forum?id=Wc5bmZZU9cy": {
    "title": "Dr.Spider: A Diagnostic Evaluation Benchmark towards Text-to-SQL Robustness",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "7195ed3c7f11220f29634cecb68b1d39db2e36d9",
    "semantic_title": "dr.spider: a diagnostic evaluation benchmark towards text-to-sql robustness",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=sWOsRj4nT1n": {
    "title": "Temporal Domain Generalization with Drift-Aware Dynamic Neural Networks",
    "volume": "oral",
    "abstract": "",
    "checked": false,
    "id": "85a7c8523f5e1688896e9da7f36f081d5db21876",
    "semantic_title": "temporal domain generalization with drift-aware dynamic neural network",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=SMa9EAovKMC": {
    "title": "Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "7de36d6b14aadc8cdb6ad1340b9ca64b15375bca",
    "semantic_title": "draft, sketch, and prove: guiding formal theorem provers with informal proofs",
    "citation_count": 193,
    "authors": []
  },
  "https://openreview.net/forum?id=uVcDssQff_": {
    "title": "REVISITING PRUNING AT INITIALIZATION THROUGH THE LENS OF RAMANUJAN GRAPH",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "fda9a8f0664456dc4accb4018cfad2e6fde2d460",
    "semantic_title": "revisiting pruning at initialization through the lens of ramanujan graph",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=5N0wtJZ89r9": {
    "title": "Embedding Fourier for Ultra-High-Definition Low-Light Image Enhancement",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "fe6ccdd4688d0cfc5a921c8a345a71feb3852c4d",
    "semantic_title": "embedding fourier for ultra-high-definition low-light image enhancement",
    "citation_count": 105,
    "authors": []
  },
  "https://openreview.net/forum?id=YnkGMIh0gvX": {
    "title": "A Call to Reflect on Evaluation Practices for Failure Detection in Image Classification",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "4b6f6f69d6400d4f0daa6a174b33925bb76696c1",
    "semantic_title": "a call to reflect on evaluation practices for failure detection in image classification",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=7JsGYvjE88d": {
    "title": "Fast and Precise: Adjusting Planning Horizon with Adaptive Subgoal Search",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "67da27828fcea2edb09ea7e9c316864244db9b92",
    "semantic_title": "fast and precise: adjusting planning horizon with adaptive subgoal search",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=N9Pk5iSCzAn": {
    "title": "Towards Open Temporal Graph Neural Networks",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "e9b973e5fb4ab649e0d229cd346919a9dbe6e7dc",
    "semantic_title": "towards open temporal graph neural networks",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=SrC-nwieGJ": {
    "title": "Relative representations enable zero-shot latent space communication",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "f56d363635bc378a196bae6d886ddd2d2899a220",
    "semantic_title": "relative representations enable zero-shot latent space communication",
    "citation_count": 118,
    "authors": []
  },
  "https://openreview.net/forum?id=FkSp8VW8RjH": {
    "title": "Language Modelling with Pixels",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "23f4b6432b74e5db05da04e354341807f5044f7e",
    "semantic_title": "language modelling with pixels",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=M95oDwJXayG": {
    "title": "Addressing Parameter Choice Issues in Unsupervised Domain Adaptation by Aggregation",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "9064ead1b2fa00cef3fbcc50d2b5e3fedc246b87",
    "semantic_title": "addressing parameter choice issues in unsupervised domain adaptation by aggregation",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=ZTK3SefE8_Z": {
    "title": "Symbolic Physics Learner: Discovering governing equations via Monte Carlo tree search",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "925d82383ba96485ed1d85c41cbe63e329cb4c18",
    "semantic_title": "symbolic physics learner: discovering governing equations via monte carlo tree search",
    "citation_count": 56,
    "authors": []
  },
  "https://openreview.net/forum?id=rFQfjDC9Mt": {
    "title": "Clean-image Backdoor: Attacking Multi-label Models with Poisoned Labels Only",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "a2c7b25b2646aa2b6a2b80cdfe9f94c895cf6fd4",
    "semantic_title": "clean-image backdoor: attacking multi-label models with poisoned labels only",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=m1oqEOAozQU": {
    "title": "Graph Neural Networks for Link Prediction with Subgraph Sketching",
    "volume": "oral",
    "abstract": "Many Graph Neural Networks (GNNs) perform poorly compared to simple heuristics on Link Prediction (LP) tasks. This is due to limitations in expressive power such as the inability to count triangles (the backbone of most LP heuristics) and because they can not distinguish automorphic nodes (those having identical structural roles). Both expressiveness issues can be alleviated by learning link (rather than node) representations and incorporating structural features such as triangle counts. Since explicit link representations are often prohibitively expensive, recent works resorted to subgraph-based methods, which have achieved state-of-the-art performance for LP, but suffer from poor efficiency due to high levels of redundancy between subgraphs. We analyze the components of subgraph GNN (SGNN) methods for link prediction. Based on our analysis, we propose a novel full-graph GNN called ELPH (Efficient Link Prediction with Hashing) that passes subgraph sketches as messages to approximate the key components of SGNNs without explicit subgraph construction. ELPH is provably more expressive than Message Passing GNNs (MPNNs). It outperforms existing SGNN models on many standard LP benchmarks while being orders of magnitude faster. However, it shares the common GNN limitation that it is only efficient when the dataset fits in GPU memory. Accordingly, we develop a highly scalable model, called BUDDY, which uses feature precomputation to circumvent this limitation without sacrificing predictive performance. Our experiments show that BUDDY also outperforms SGNNs on standard LP benchmarks while being highly scalable and faster than ELPH",
    "checked": true,
    "id": "68baa11061a8da3a9e6c6cd0ff075bd5cc72376d",
    "semantic_title": "graph neural networks for link prediction with subgraph sketching",
    "citation_count": 95,
    "authors": []
  },
  "https://openreview.net/forum?id=_2bDpAtr7PI": {
    "title": "Image to Sphere: Learning Equivariant Features for Efficient Pose Prediction",
    "volume": "oral",
    "abstract": "Predicting the pose of objects from a single image is an important but difficult computer vision problem. Methods that predict a single point estimate do not predict the pose of objects with symmetries well and cannot represent uncertainty. Alternatively, some works predict a distribution over orientations in $\\mathrm{SO}(3)$. However, training such models can be computation- and sample-inefficient. Instead, we propose a novel mapping of features from the image domain to the 3D rotation manifold. Our method then leverages $\\mathrm{SO}(3)$ equivariant layers, which are more sample efficient, and outputs a distribution over rotations that can be sampled at arbitrary resolution. We demonstrate the effectiveness of our method at object orientation prediction, and achieve state-of-the-art performance on the popular PASCAL3D+ dataset. Moreover, we show that our method can model complex object symmetries, without any modifications to the parameters or loss function. Code is available at \\url{https://dmklee.github.io/image2sphere}",
    "checked": true,
    "id": "326f04f657dff3f3ec9ff8d1dc686e7695b342a0",
    "semantic_title": "image to sphere: learning equivariant features for efficient pose prediction",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=zt53IDUR1U": {
    "title": "MICN: Multi-scale Local and Global Context Modeling for Long-term Series Forecasting",
    "volume": "oral",
    "abstract": "Recently, Transformer-based methods have achieved surprising performance in the field of long-term series forecasting, but the attention mechanism for computing global correlations entails high complexity. And they do not allow for targeted modeling of local features as CNN structures do. To solve the above problems, we propose to combine local features and global correlations to capture the overall view of time series (e.g., fluctuations, trends). To fully exploit the underlying information in the time series, a multi-scale branch structure is adopted to model different potential patterns separately. Each pattern is extracted with down-sampled convolution and isometric convolution for local features and global correlations, respectively. In addition to being more effective, our proposed method, termed as Multi-scale Isometric Convolution Network (MICN), is more efficient with linear complexity about the sequence length with suitable convolution kernels. Our experiments on six benchmark datasets show that compared with state-of-the-art methods, MICN yields 17.2% and 21.6% relative improvements for multivariate and univariate time series, respectively",
    "checked": true,
    "id": "6f370b047624dafea4e59df20ac4cbec538dc44a",
    "semantic_title": "micn: multi-scale local and global context modeling for long-term series forecasting",
    "citation_count": 277,
    "authors": []
  },
  "https://openreview.net/forum?id=SXZr8aDKia": {
    "title": "Personalized Federated Learning with Feature Alignment and Classifier Collaboration",
    "volume": "oral",
    "abstract": "Data heterogeneity is one of the most challenging issues in federated learning, which motivates a variety of approaches to learn personalized models for participating clients. One such approach in deep neural networks based tasks is employing a shared feature representation and learning a customized classifier head for each client. However, previous works do not utilize the global knowledge during local representation learning and also neglect the fine-grained collaboration between local classifier heads, which limits the model generalization ability. In this work, we conduct explicit local-global feature alignment by leveraging global semantic knowledge for learning a better representation. Moreover, we quantify the benefit of classifier combination for each client as a function of the combining weights and derive an optimization problem for estimating optimal weights. Finally, extensive evaluation results on benchmark datasets with various heterogeneous data scenarios demonstrate the effectiveness of our proposed method",
    "checked": true,
    "id": "f52e2e84bb1dbae264da4c51419f899a770dd47a",
    "semantic_title": "personalized federated learning with feature alignment and classifier collaboration",
    "citation_count": 123,
    "authors": []
  },
  "https://openreview.net/forum?id=c7rM7F7jQjN": {
    "title": "From Play to Policy: Conditional Behavior Generation from Uncurated Robot Data",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "9b5f4aab169fba588e214c010345232053f8ae76",
    "semantic_title": "from play to policy: conditional behavior generation from uncurated robot data",
    "citation_count": 100,
    "authors": []
  },
  "https://openreview.net/forum?id=jlAjNL8z5cs": {
    "title": "Visual Classification via Description from Large Language Models",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "a42b091adaf29b06a092b67192ac07cb93312f2a",
    "semantic_title": "visual classification via description from large language models",
    "citation_count": 323,
    "authors": []
  },
  "https://openreview.net/forum?id=w0QXrZ3N-s": {
    "title": "The Modality Focusing Hypothesis: Towards Understanding Crossmodal Knowledge Distillation",
    "volume": "oral",
    "abstract": "Crossmodal knowledge distillation (KD) extends traditional knowledge distillation to the area of multimodal learning and demonstrates great success in various applications. To achieve knowledge transfer across modalities, a pretrained network from one modality is adopted as the teacher to provide supervision signals to a student network learning from the other modality. In contrast to the empirical success reported in prior works, the working mechanism of crossmodal KD remains a mystery. In this paper, we present a thorough understanding of crossmodal KD. We begin by providing two failure cases and demonstrate that KD is not a universal cure in crossmodal knowledge transfer. We then present the modality Venn diagram to understand modality relationships and the modality focusing hypothesis revealing the decisive factor in the efficacy of crossmodal KD. Experimental results on 6 multimodal datasets help justify our hypothesis, diagnose failure cases, and point directions to improve crossmodal knowledge transfer in the future",
    "checked": true,
    "id": "af54c8a1b08b15cb77db1e6231827c3d173c9317",
    "semantic_title": "the modality focusing hypothesis: towards understanding crossmodal knowledge distillation",
    "citation_count": 45,
    "authors": []
  },
  "https://openreview.net/forum?id=OJ8aSjCaMNK": {
    "title": "Multi-Rate VAE: Train Once, Get the Full Rate-Distortion Curve",
    "volume": "oral",
    "abstract": "Variational autoencoders (VAEs) are powerful tools for learning latent representations of data used in a wide range of applications. In practice, VAEs usually require multiple training rounds to choose the amount of information the latent variable should retain. This trade-off between the reconstruction error (distortion) and the KL divergence (rate) is typically parameterized by a hyperparameter $\\beta$. In this paper, we introduce Multi-Rate VAE (MR-VAE), a computationally efficient framework for learning optimal parameters corresponding to various $\\beta$ in a single training run. The key idea is to explicitly formulate a response function using hypernetworks that maps $\\beta$ to the optimal parameters. MR-VAEs construct a compact response hypernetwork where the pre-activations are conditionally gated based on $\\beta$. We justify the proposed architecture by analyzing linear VAEs and showing that it can represent response functions exactly for linear VAEs. With the learned hypernetwork, MR-VAEs can construct the rate-distortion curve without additional training and can be deployed with significantly less hyperparameter tuning. Empirically, our approach is competitive and often exceeds the performance of multiple $\\beta$-VAEs training with minimal computation and memory overheads",
    "checked": true,
    "id": "d10b6a8a4543af29380cac09cd8b277c25066e6b",
    "semantic_title": "multi-rate vae: train once, get the full rate-distortion curve",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=3OR2tbtnYC-": {
    "title": "Near-optimal Policy Identification in Active Reinforcement Learning",
    "volume": "oral",
    "abstract": "Many real-world reinforcement learning tasks require control of complex dynamical systems that involve both costly data acquisition processes and large state spaces. In cases where the expensive transition dynamics can be readily evaluated at specified states (e.g., via a simulator), agents can operate in what is often referred to as planning with a \\emph{generative model}. We propose the AE-LSVI algorithm for best policy identification, a novel variant of the kernelized least-squares value iteration (LSVI) algorithm that combines optimism with pessimism for active exploration (AE). AE-LSVI provably identifies a near-optimal policy \\emph{uniformly} over an entire state space and achieves polynomial sample complexity guarantees that are independent of the number of states. When specialized to the recently introduced offline contextual Bayesian optimization setting, our algorithm achieves improved sample complexity bounds. Experimentally, we demonstrate that AE-LSVI outperforms other RL algorithms in a variety of environments when robustness to the initial state is required",
    "checked": true,
    "id": "445f4e1d4c9fcbab2e1aaf89c27599ddfaad82f5",
    "semantic_title": "near-optimal policy identification in active reinforcement learning",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=LFHFQbjxIiP": {
    "title": "Conditional Antibody Design as 3D Equivariant Graph Translation",
    "volume": "oral",
    "abstract": "Antibody design is valuable for therapeutic usage and biological research. Existing deep-learning-based methods encounter several key issues: 1) incomplete context for Complementarity-Determining Regions (CDRs) generation; 2) incapability of capturing the entire 3D geometry of the input structure; 3) inefficient prediction of the CDR sequences in an autoregressive manner. In this paper, we propose Multi-channel Equivariant Attention Network (MEAN) to co-design 1D sequences and 3D structures of CDRs. To be specific, MEAN formulates antibody design as a conditional graph translation problem by importing extra components including the target antigen and the light chain of the antibody. Then, MEAN resorts to E(3)-equivariant message passing along with a proposed attention mechanism to better capture the geometrical correlation between different components. Finally, it outputs both the 1D sequences and 3D structure via a multi-round progressive full-shot scheme, which enjoys more efficiency and precision against previous autoregressive approaches. Our method significantly surpasses state-of-the-art models in sequence and structure modeling, antigen-binding CDR design, and binding affinity optimization. Specifically, the relative improvement to baselines is about 23\\% in antigen-binding CDR design and 34\\% for affinity optimization",
    "checked": true,
    "id": "071e69ad14880d1de98855cb3e1773db7c41977a",
    "semantic_title": "conditional antibody design as 3d equivariant graph translation",
    "citation_count": 93,
    "authors": []
  },
  "https://openreview.net/forum?id=DeG07_TcZvT": {
    "title": "Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task",
    "volume": "oral",
    "abstract": "Language models show a surprising range of capabilities, but the source of their apparent competence is unclear. Do these networks just memorize a collection of surface statistics, or do they rely on internal representations of the process that generates the sequences they see? We investigate this question by applying a variant of the GPT model to the task of predicting legal moves in a simple board game, Othello. Although the network has no a priori knowledge of the game or its rules, we uncover evidence of an emergent nonlinear internal representation of the board state. Interventional experiments indicate this representation can be used to control the output of the network and create \"latent saliency maps\" that can help explain predictions in human terms",
    "checked": true,
    "id": "d5295f7ddcf281f3d30a7579d5ce482036a8e27c",
    "semantic_title": "emergent world representations: exploring a sequence model trained on a synthetic task",
    "citation_count": 309,
    "authors": []
  },
  "https://openreview.net/forum?id=VELL0PlWfc": {
    "title": "Tailoring Language Generation Models under Total Variation Distance",
    "volume": "oral",
    "abstract": "The standard paradigm of neural language generation adopts maximum likelihood estimation (MLE) as the optimizing method. From a distributional view, MLE in fact minimizes the Kullback-Leibler divergence (KLD) between the distribution of the real data and that of the model. However, this approach forces the model to distribute non-zero (sometimes large) probability mass to all training samples regardless of their quality. Moreover, in the attempt to cover the low-probability regions in the data distribution, the model systematically overestimates the probability of corrupted text sequences, which we conjecture is one of the main reasons for text degeneration during autoregressive decoding. To remedy this problem, we leverage the total variation distance (TVD) with its robustness to outliers, and develop practical bounds to apply it to language generation. Then, we introduce the TaiLr objective that balances the tradeoff of estimating TVD. Intuitively, TaiLr downweights real data samples that have low model probabilities with tunable penalization intensity. Experimental results show that our method alleviates the overestimation of degenerated sequences without sacrificing diversity and improves generation quality on a wide range of text generation tasks",
    "checked": true,
    "id": "14a26bd4d9213e40325e2a04f136279115bf248d",
    "semantic_title": "tailoring language generation models under total variation distance",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=vhFu1Acb0xb": {
    "title": "Transformers are Sample-Efficient World Models",
    "volume": "oral",
    "abstract": "Deep reinforcement learning agents are notoriously sample inefficient, which considerably limits their application to real-world problems. Recently, many model-based methods have been designed to address this issue, with learning in the imagination of a world model being one of the most prominent approaches. However, while virtually unlimited interaction with a simulated environment sounds appealing, the world model has to be accurate over extended periods of time. Motivated by the success of Transformers in sequence modeling tasks, we introduce IRIS, a data-efficient agent that learns in a world model composed of a discrete autoencoder and an autoregressive Transformer. With the equivalent of only two hours of gameplay in the Atari 100k benchmark, IRIS achieves a mean human normalized score of 1.046, and outperforms humans on 10 out of 26 games, setting a new state of the art for methods without lookahead search. To foster future research on Transformers and world models for sample-efficient reinforcement learning, we release our code and models at https://github.com/eloialonso/iris",
    "checked": false,
    "id": "235303a8bc1e4892efd525a38ead657422d8a519",
    "semantic_title": "transformers are sample efficient world models",
    "citation_count": 201,
    "authors": []
  },
  "https://openreview.net/forum?id=TD7AnQjNzR6": {
    "title": "Statistical Efficiency of Score Matching: The View from Isoperimetry",
    "volume": "oral",
    "abstract": "Deep generative models parametrized up to a normalizing constant (e.g. energy-based models) are difficult to train by maximizing the likelihood of the data because the likelihood and/or gradients thereof cannot be explicitly or efficiently written down. Score matching is a training method, whereby instead of fitting the likelihood $\\log p(x)$ for the training data, we instead fit the score function $\\nabla_x \\log p(x)$ --- obviating the need to evaluate the partition function. Though this estimator is known to be consistent, its unclear whether (and when) its statistical efficiency is comparable to that of maximum likelihood --- which is known to be (asymptotically) optimal. We initiate this line of inquiry in this paper, and show a tight connection between statistical efficiency of score matching and the isoperimetric properties of the distribution being estimated --- i.e. the Poincar\\'e, log-Sobolev and isoperimetric constant --- quantities which govern the mixing time of Markov processes like Langevin dynamics. Roughly, we show that the score matching estimator is statistically comparable to the maximum likelihood when the distribution has a small isoperimetric constant. Conversely, if the distribution has a large isoperimetric constant --- even for simple families of distributions like exponential families with rich enough sufficient statistics --- score matching will be substantially less efficient than maximum likelihood. We suitably formalize these results both in the finite sample regime, and in the asymptotic regime. Finally, we identify a direct parallel in the discrete setting, where we connect the statistical properties of pseudolikelihood estimation with approximate tensorization of entropy and the Glauber dynamics",
    "checked": true,
    "id": "bdc52aeaaa366316642b558b7495991676541201",
    "semantic_title": "statistical efficiency of score matching: the view from isoperimetry",
    "citation_count": 53,
    "authors": []
  },
  "https://openreview.net/forum?id=0ypGZvm0er0": {
    "title": "View Synthesis with Sculpted Neural Points",
    "volume": "oral",
    "abstract": "We address the task of view synthesis, generating novel views of a scene given a set of images as input. In many recent works such as NeRF (Mildenhall et al., 2020), the scene geometry is parameterized using neural implicit representations (i.e., MLPs). Implicit neural representations have achieved impressive visual quality but have drawbacks in computational efficiency. In this work, we propose a new approach that performs view synthesis using point clouds. It is the first point-based method that achieves better visual quality than NeRF while being 100× faster in rendering speed. Our approach builds on existing works on differentiable point-based rendering but introduces a novel technique we call \"Sculpted Neural Points (SNP)\", which significantly improves the robustness to errors and holes in the reconstructed point cloud. We further propose to use view-dependent point features based on spherical harmonics to capture non-Lambertian surfaces, and new designs in the point-based rendering pipeline that further boost the performance. Finally, we show that our system supports fine-grained scene editing. Code is available at https://github.com/princeton-vl/SNP",
    "checked": true,
    "id": "ab0ead190ad6e19d82cfa8a43365e842c8ee62a1",
    "semantic_title": "view synthesis with sculpted neural points",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=GcM7qfl5zY": {
    "title": "AutoGT: Automated Graph Transformer Architecture Search",
    "volume": "oral",
    "abstract": "Although Transformer architectures have been successfully applied to graph data with the advent of Graph Transformer, current design of Graph Transformer still heavily relies on human labor and expertise knowledge to decide proper neural architectures and suitable graph encoding strategies at each Transformer layer. In literature, there have been some works on automated design of Transformers focusing on non-graph data such as texts and images without considering graph encoding strategies, which fail to handle the non-euclidean graph data. In this paper, we study the problem of automated graph Transformer, for the first time. However, solving these problems poses the following challenges: i) how can we design a unified search space for graph Transformer, and ii) how to deal with the coupling relations between Transformer architectures and the graph encodings of each Transformer layer. To address these challenges, we propose Automated Graph Transformer (AutoGT), a neural architecture search framework that can automatically discover the optimal graph Transformer architectures by joint optimization of Transformer architecture and graph encoding strategies. Specifically, we first propose a unified graph Transformer formulation that can represent most of state-of-the-art graph Transformer architectures. Based upon the unified formulation, we further design the graph Transformer search space that includes both candidate architectures and various graph encodings. To handle the coupling relations, we propose a novel encoding-aware performance estimation strategy by gradually training and splitting the supernets according to the correlations between graph encodings and architectures. The proposed strategy can provide a more consistent and fine-grained performance prediction when evaluating the jointly optimized graph encodings and architectures. Extensive experiments and ablation studies show that our proposed AutoGT gains sufficient improvement over state-of-the-art hand-crafted baselines on all datasets, demonstrating its effectiveness and wide applicability",
    "checked": true,
    "id": "5d00e8f305d20ad937938fa4db054a33186626f7",
    "semantic_title": "autogt: automated graph transformer architecture search",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=vSVLM2j9eie": {
    "title": "Crossformer: Transformer Utilizing Cross-Dimension Dependency for Multivariate Time Series Forecasting",
    "volume": "oral",
    "abstract": "Recently many deep models have been proposed for multivariate time series (MTS) forecasting. In particular, Transformer-based models have shown great potential because they can capture long-term dependency. However, existing Transformer-based models mainly focus on modeling the temporal dependency (cross-time dependency) yet often omit the dependency among different variables (cross-dimension dependency), which is critical for MTS forecasting. To fill the gap, we propose Crossformer, a Transformer-based model utilizing cross-dimension dependency for MTS forecasting. In Crossformer, the input MTS is embedded into a 2D vector array through the Dimension-Segment-Wise (DSW) embedding to preserve time and dimension information. Then the Two-Stage Attention (TSA) layer is proposed to efficiently capture the cross-time and cross-dimension dependency. Utilizing DSW embedding and TSA layer, Crossformer establishes a Hierarchical Encoder-Decoder (HED) to use the information at different scales for the final forecasting. Extensive experimental results on six real-world datasets show the effectiveness of Crossformer against previous state-of-the-arts",
    "checked": true,
    "id": "fb45d31cc89207aec392dbac8908cc24db2df871",
    "semantic_title": "crossformer: transformer utilizing cross-dimension dependency for multivariate time series forecasting",
    "citation_count": 537,
    "authors": []
  },
  "https://openreview.net/forum?id=LV_MeMS38Q9": {
    "title": "Betty: An Automatic Differentiation Library for Multilevel Optimization",
    "volume": "oral",
    "abstract": "Gradient-based multilevel optimization (MLO) has gained attention as a framework for studying numerous problems, ranging from hyperparameter optimization and meta-learning to neural architecture search and reinforcement learning. However, gradients in MLO, which are obtained by composing best-response Jacobians via the chain rule, are notoriously difficult to implement and memory/compute intensive. We take an initial step towards closing this gap by introducing Betty, a software library for large-scale MLO. At its core, we devise a novel dataflow graph for MLO, which allows us to (1) develop efficient automatic differentiation for MLO that reduces the computational complexity from $\\mathcal{O}(d^3)$ to $\\mathcal{O}(d^2)$, (2) incorporate systems support such as mixed-precision and data-parallel training for scalability, and (3) facilitate implementation of MLO programs of arbitrary complexity while allowing a modular interface for diverse algorithmic and systems design choices. We empirically demonstrate that Betty can be used to implement an array of MLO programs, while also observing up to 11% increase in test accuracy, 14% decrease in GPU memory usage, and 20% decrease in training wall time over existing implementations on multiple benchmarks. We also showcase that Betty enables scaling MLO to models with hundreds of millions of parameters. We open-source the code at https://github.com/leopard-ai/betty",
    "checked": true,
    "id": "5be4f2074e4811fec280248e10e2f7b2faee54dd",
    "semantic_title": "betty: an automatic differentiation library for multilevel optimization",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=ueYYgo2pSSU": {
    "title": "Offline RL with No OOD Actions: In-Sample Learning via Implicit Value Regularization",
    "volume": "oral",
    "abstract": "Most offline reinforcement learning (RL) methods suffer from the trade-off between improving the policy to surpass the behavior policy and constraining the policy to limit the deviation from the behavior policy as computing $Q$-values using out-of-distribution (OOD) actions will suffer from errors due to distributional shift. The recent proposed \\textit{In-sample Learning} paradigm (i.e., IQL), which improves the policy by quantile regression using only data samples, shows great promise because it learns an optimal policy without querying the value function of any unseen actions. However, it remains unclear how this type of method handles the distributional shift in learning the value function. In this work, we make a key finding that the in-sample learning paradigm arises under the \\textit{Implicit Value Regularization} (IVR) framework. This gives a deeper understanding of why the in-sample learning paradigm works, i.e., it applies implicit value regularization to the policy. Based on the IVR framework, we further propose two practical algorithms, Sparse $Q$-learning (SQL) and Exponential $Q$-learning (EQL), which adopt the same value regularization used in existing works, but in a complete in-sample manner. Compared with IQL, we find that our algorithms introduce sparsity in learning the value function, making them more robust in noisy data regimes. We also verify the effectiveness of SQL and EQL on D4RL benchmark datasets and show the benefits of in-sample learning by comparing them with CQL in small data regimes. Code is available at \\url{https://github.com/ryanxhr/SQL}",
    "checked": true,
    "id": "18d82f2a4aa1e2c1c4b447876c95b8f7e717e1a1",
    "semantic_title": "offline rl with no ood actions: in-sample learning via implicit value regularization",
    "citation_count": 88,
    "authors": []
  },
  "https://openreview.net/forum?id=CPdc77SQfQ5": {
    "title": "Win: Weight-Decay-Integrated Nesterov Acceleration for Adaptive Gradient Algorithms",
    "volume": "oral",
    "abstract": "Training deep networks on large-scale datasets is computationally challenging. In this work, we explore the problem of ``\\textit{how to accelerate adaptive gradient algorithms in a general manner}\", and aim to provide practical efficiency-boosting insights. To this end, we propose an effective and general {Weight-decay-Integrated Nesterov acceleration} (Win) to accelerate adaptive algorithms. Taking AdamW and Adam as examples, we minimize a dynamical loss per iteration which combines the vanilla training loss and a dynamic regularizer inspired by proximal point method (PPM) to improve the convexity of the problem. To introduce Nesterov-alike-acceleration into AdamW and Adam, we respectively use the first- and second-order Taylor approximations of vanilla loss to update the variable twice. In this way, we arrive at our Win acceleration for AdamW and Adam that uses a conservative step and a reckless step to update twice and then linearly combines these two updates for acceleration. Next, we extend Win acceleration to LAMB and SGD. Our transparent acceleration derivation could provide insights for other accelerated methods and their integration into adaptive algorithms. Besides, we prove the convergence of Win-accelerated adaptive algorithms and justify their convergence superiority over their non-accelerated counterparts by taking AdamW and Adam as examples. Experimental results testify to the faster convergence speed and superior performance of our Win-accelerated AdamW, Adam, LAMB and SGD over their non-accelerated counterparts on vision classification tasks and language modeling tasks with both CNN and Transformer backbones. We hope Win shall be a default acceleration option for popular optimizers in deep learning community to improve the training efficiency. Code will be released at \\url{https://github.com/sail-sg/win}",
    "checked": true,
    "id": "c99be6b5cd24ae05f60256989efbefc7252c7717",
    "semantic_title": "win: weight-decay-integrated nesterov acceleration for adaptive gradient algorithms",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=g2YraF75Tj": {
    "title": "Towards Stable Test-time Adaptation in Dynamic Wild World",
    "volume": "oral",
    "abstract": "Test-time adaptation (TTA) has shown to be effective at tackling distribution shifts between training and testing data by adapting a given model on test samples. However, the online model updating of TTA may be unstable and this is often a key obstacle preventing existing TTA methods from being deployed in the real world. Specifically, TTA may fail to improve or even harm the model performance when test data have: 1) mixed distribution shifts, 2) small batch sizes, and 3) online imbalanced label distribution shifts, which are quite common in practice. In this paper, we investigate the unstable reasons and find that the batch norm layer is a crucial factor hindering TTA stability. Conversely, TTA can perform more stably with batch-agnostic norm layers, i.e., group or layer norm. However, we observe that TTA with group and layer norms does not always succeed and still suffers many failure cases. By digging into the failure cases, we find that certain noisy test samples with large gradients may disturb the model adaption and result in collapsed trivial solutions, i.e., assigning the same class label for all samples. To address the above collapse issue, we propose a sharpness-aware and reliable entropy minimization method, called SAR, for further stabilizing TTA from two aspects: 1) remove partial noisy samples with large gradients, 2) encourage model weights to go to a flat minimum so that the model is robust to the remaining noisy samples. Promising results demonstrate that SAR performs more stably than prior methods and is computationally efficient under the above wild test scenarios",
    "checked": true,
    "id": "f849b873e94f28e1f2a3e1dc4d7bef17eb64adab",
    "semantic_title": "towards stable test-time adaptation in dynamic wild world",
    "citation_count": 300,
    "authors": []
  },
  "https://openreview.net/forum?id=2QGJXyMNoPz": {
    "title": "MocoSFL: enabling cross-client collaborative self-supervised learning",
    "volume": "oral",
    "abstract": "Existing collaborative self-supervised learning (SSL) schemes are not suitable for cross-client applications because of their expensive computation and large local data requirements. To address these issues, we propose MocoSFL, a collaborative SSL framework based on Split Federated Learning (SFL) and Momentum Contrast (MoCo). In MocoSFL, the large backbone model is split into a small client-side model and a large server-side model, and only the small client-side model is processed locally on the client's local devices. MocoSFL has three key components: (i) vector concatenation which enables the use of small batch size and reduces computation and memory requirements by orders of magnitude; (ii) feature sharing that helps achieve high accuracy regardless of the quality and volume of local data; (iii) frequent synchronization that helps achieve better non-IID performance because of smaller local model divergence. For a 1,000-client case with non-IID data (each client only has data from 2 random classes of CIFAR-10), MocoSFL can achieve over 84% accuracy with ResNet-18 model. Next we present TAResSFL module that significantly improves the resistance to privacy threats and communication overhead with small sacrifice in accuracy for a MocoSFL system. On a Raspberry Pi 4B device, the MocoSFL-based scheme requires less than 1MB of memory and less than 40MB of communication, and consumes less than 5W power. The code is available at https://github.com/SonyAI/MocoSFL",
    "checked": true,
    "id": "15af33590302d09cf0b5a90e3f3fac26cc2bd6a6",
    "semantic_title": "mocosfl: enabling cross-client collaborative self-supervised learning",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=1NAzMofMnWl": {
    "title": "DaxBench: Benchmarking Deformable Object Manipulation with Differentiable Physics",
    "volume": "oral",
    "abstract": "Deformable object manipulation (DOM) is a long-standing challenge in robotics and has attracted significant interest recently. This paper presents DaXBench, a differentiable simulation framework for DOM. While existing work often focuses on a specific type of deformable objects, DaXBench supports fluid, rope, cloth ...; it provides a general-purpose benchmark to evaluate widely different DOM methods, including planning, imitation learning, and reinforcement learning. DaXBench combines recent advances in deformable object simulation with JAX, a high-performance computational framework. All DOM tasks in DaXBench are wrapped with the OpenAI Gym API for easy integration with DOM algorithms. We hope that DaXBench provides to the research community a comprehensive, standardized benchmark and a valuable tool to support the development and evaluation of new DOM methods. The code and video are available online",
    "checked": false,
    "id": "fc715a1aac98fd5f5609e8fae16905ec3b85a057",
    "semantic_title": "benchmarking deformable object manipulation with differentiable physics",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=U2WjB9xxZ9q": {
    "title": "3D generation on ImageNet",
    "volume": "oral",
    "abstract": "All existing 3D-from-2D generators are designed for well-curated single-category datasets, where all the objects have (approximately) the same scale, 3D location, and orientation, and the camera always points to the center of the scene. This makes them inapplicable to diverse, in-the-wild datasets of non-alignable scenes rendered from arbitrary camera poses. In this work, we develop a 3D generator with Generic Priors (3DGP): a 3D synthesis framework with more general assumptions about the training data, and show that it scales to very challenging datasets, like ImageNet. Our model is based on three new ideas. First, we incorporate an inaccurate off-the-shelf depth estimator into 3D GAN training via a special depth adaptation module to handle the imprecision. Then, we create a flexible camera model and a regularization strategy for it to learn its distribution parameters during training. Finally, we extend the recent ideas of transferring knowledge from pretrained classifiers into GANs for patch-wise trained models by employing a simple distillation-based technique on top of the discriminator. It achieves more stable training than the existing methods and speeds up the convergence by at least 40%. We explore our model on four datasets: SDIP Dogs $256^2$, SDIP Elephants $256^2$, LSUN Horses $256^2$, and ImageNet $256^2$ and demonstrate that 3DGP outperforms the recent state-of-the-art in terms of both texture and geometry quality. Code and visualizations: https://snap-research.github.io/3dgp",
    "checked": true,
    "id": "e750fd59b0239788c811af8809969285071ee71c",
    "semantic_title": "3d generation on imagenet",
    "citation_count": 57,
    "authors": []
  },
  "https://openreview.net/forum?id=r9hNv76KoT3": {
    "title": "Rethinking the Expressive Power of GNNs via Graph Biconnectivity",
    "volume": "oral",
    "abstract": "Designing expressive Graph Neural Networks (GNNs) is a central topic in learning graph-structured data. While numerous approaches have been proposed to improve GNNs with respect to the Weisfeiler-Lehman (WL) test, for most of them, there is still a lack of deep understanding of what additional power they can systematically and provably gain. In this paper, we take a fundamentally different perspective to study the expressive power of GNNs beyond the WL test. Specifically, we introduce a novel class of expressivity metrics via graph biconnectivity and highlight their importance in both theory and practice. As biconnectivity can be easily calculated using simple algorithms that have linear computational costs, it is natural to expect that popular GNNs can learn it easily as well. However, after a thorough review of prior GNN architectures, we surprisingly find that most of them are not expressive for any of these metrics. The only exception is the ESAN framework (Bevilacqua et al., 2022), for which we give a theoretical justification of its power. We proceed to introduce a principled and more efficient approach, called the Generalized Distance Weisfeiler-Lehman (GD-WL), which is provably expressive for all biconnectivity metrics. Practically, we show GD-WL can be implemented by a Transformer-like architecture that preserves expressiveness and enjoys full parallelizability. A set of experiments on both synthetic and real datasets demonstrates that our approach can consistently outperform prior GNN architectures",
    "checked": true,
    "id": "e3d1175f5b522220c31f96c5c6753a0757aae471",
    "semantic_title": "rethinking the expressive power of gnns via graph biconnectivity",
    "citation_count": 128,
    "authors": []
  },
  "https://openreview.net/forum?id=RecZ9nB9Q4": {
    "title": "Sparse Mixture-of-Experts are Domain Generalizable Learners",
    "volume": "oral",
    "abstract": "Human visual perception can easily generalize to out-of-distributed visual data, which is far beyond the capability of modern machine learning models. Domain generalization (DG) aims to close this gap, with existing DG methods mainly focusing on the loss function design. In this paper, we propose to explore an orthogonal direction, i.e., the design of the backbone architecture. It is motivated by an empirical finding that transformer-based models trained with empirical risk minimization (ERM) outperform CNN-based models employing state-of-the-art (SOTA) DG algorithms on multiple DG datasets. We develop a formal framework to characterize a network's robustness to distribution shifts by studying its architecture's alignment with the correlations in the dataset. This analysis guides us to propose a novel DG model built upon vision transformers, namely \\emph{Generalizable Mixture-of-Experts (GMoE)}. Extensive experiments on DomainBed demonstrate that GMoE trained with ERM outperforms SOTA DG baselines by a large margin. Moreover, GMoE is complementary to existing DG methods and its performance is substantially improved when trained with DG algorithms",
    "checked": true,
    "id": "9c08d8fca57bac1998b79235f773cde27319a209",
    "semantic_title": "sparse mixture-of-experts are domain generalizable learners",
    "citation_count": 69,
    "authors": []
  },
  "https://openreview.net/forum?id=JroZRaRw7Eu": {
    "title": "Token Merging: Your ViT But Faster",
    "volume": "oral",
    "abstract": "We introduce Token Merging (ToMe), a simple method to increase the throughput of existing ViT models without needing to train. ToMe gradually combines similar tokens in a transformer using a general and light-weight matching algorithm that is as fast as pruning while being more accurate. Off-the-shelf, ToMe can 2x the throughput of state-of-the-art ViT-L @ 512 and ViT-H @ 518 models on images and 2.2x the throughput of ViT-L on video with only a 0.2-0.3% accuracy drop in each case. ToMe can also easily be applied during training, improving in practice training speed up to 2x for MAE fine-tuning on video. Training with ToMe further minimizes accuracy drop, leading to 2x the throughput of ViT-B on audio for only a 0.4% mAP drop. Qualitatively, we find that ToMe merges object parts into one token, even over multiple frames of video. Overall, ToMe's accuracy and speed are competitive with state-of-the-art on images, video, and audio",
    "checked": true,
    "id": "1dff6b1b35e2d45d4db57c8b4e4395486c3e365f",
    "semantic_title": "token merging: your vit but faster",
    "citation_count": 520,
    "authors": []
  },
  "https://openreview.net/forum?id=FeWvD0L_a4": {
    "title": "Learnable Behavior Control: Breaking Atari Human World Records via Sample-Efficient Behavior Selection",
    "volume": "oral",
    "abstract": "The exploration problem is one of the main challenges in deep reinforcement learning (RL). Recent promising works tried to handle the problem with population-based methods, which collect samples with diverse behaviors derived from a population of different exploratory policies. Adaptive policy selection has been adopted for behavior control. However, the behavior selection space is largely limited by the predefined policy population, which further limits behavior diversity. In this paper, we propose a general framework called Learnable Behavioral Control (LBC) to address the limitation, which a) enables a significantly enlarged behavior selection space via formulating a hybrid behavior mapping from all policies; b) constructs a unified learnable process for behavior selection. We introduce LBC into distributed off-policy actor-critic methods and achieve behavior control via optimizing the selection of the behavior mappings with bandit-based meta-controllers. Our agents have achieved 10077.52% mean human normalized score and surpassed 24 human world records within 1B training frames in the Arcade Learning Environment, which demonstrates our significant state-of-the-art (SOTA) performance without degrading the sample efficiency",
    "checked": true,
    "id": "bb1a3404a677154c99f6309e2ae709f44589c4cf",
    "semantic_title": "learnable behavior control: breaking atari human world records via sample-efficient behavior selection",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=awnvqZja69": {
    "title": "Image as Set of Points",
    "volume": "oral",
    "abstract": "What is an image, and how to extract latent features? Convolutional Networks (ConvNets) consider an image as organized pixels in a rectangular shape and extract features via convolutional operation in a local region; Vision Transformers (ViTs) treat an image as a sequence of patches and extract features via attention mechanism in a global range. In this work, we introduce a straightforward and promising paradigm for visual representation, which is called Context Clusters. Context clusters (CoCs) view an image as a set of unorganized points and extract features via a simplified clustering algorithm. In detail, each point includes the raw feature (e.g., color) and positional information (e.g., coordinates), and a simplified clustering algorithm is employed to group and extract deep features hierarchically. Our CoCs are convolution- and attention-free, only relying on clustering algorithm for spatial interaction. Owing to the simple design, we show CoCs endow gratifying interpretability via the visualization of the clustering process. Our CoCs aim at providing a new perspective on image and visual representation, which may enjoy broad applications in different domains and exhibit profound insights. Even though we are not targeting SOTA performance, COCs still achieve comparable or even better performance than ConvNets or ViTs on several benchmarks",
    "checked": true,
    "id": "72862e6e858c44ebd94d6ba90dc420963d9d0f39",
    "semantic_title": "image as set of points",
    "citation_count": 56,
    "authors": []
  },
  "https://openreview.net/forum?id=N_g8TT9Cy7f": {
    "title": "Human-Guided Fair Classification for Natural Language Processing",
    "volume": "spotlight",
    "abstract": "Text classifiers have promising applications in high-stake tasks such as resume screening and content moderation. These classifiers must be fair and avoid discriminatory decisions by being invariant to perturbations of sensitive attributes such as gender or ethnicity. However, there is a gap between human intuition about these perturbations and the formal similarity specifications capturing them. While existing research has started to address this gap, current methods are based on hardcoded word replacements, resulting in specifications with limited expressivity or ones that fail to fully align with human intuition (e.g., in cases of asymmetric counterfactuals). This work proposes novel methods for bridging this gap by discovering expressive and intuitive individual fairness specifications. We show how to leverage unsupervised style transfer and GPT-3's zero-shot capabilities to automatically generate expressive candidate pairs of semantically similar sentences that differ along sensitive attributes. We then validate the generated pairs via an extensive crowdsourcing study, which confirms that a lot of these pairs align with human intuition about fairness in the context of toxicity classification. Finally, we show how limited amounts of human feedback can be leveraged to learn a similarity specification that can be used to train downstream fairness-aware models",
    "checked": true,
    "id": "4ae163ad7dac91bae435eff844d0fd084f0399ec",
    "semantic_title": "human-guided fair classification for natural language processing",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=X5ZMzRYqUjB": {
    "title": "Humanly Certifying Superhuman Classifiers",
    "volume": "spotlight",
    "abstract": "This paper addresses a key question in current machine learning research: if we believe that a model's predictions might be better than those given by human experts, how can we (humans) verify these beliefs? In some cases, this ``superhuman'' performance is readily demonstrated; for example by defeating top-tier human players in traditional two player games. On the other hand, it can be challenging to evaluate classification models that potentially surpass human performance. Indeed, human annotations are often treated as a ground truth, which implicitly assumes the superiority of the human over any models trained on human annotations. In reality, human annotators are subjective and can make mistakes. Evaluating the performance with respect to a genuine oracle is more objective and reliable, even when querying the oracle is more expensive or sometimes impossible. In this paper, we first raise the challenge of evaluating the performance of both humans and models with respect to an oracle which is $\\textit{unobserved}$. We develop a theory for estimating the accuracy compared to the oracle, using only imperfect human annotations for reference. Our analysis provides an executable recipe for detecting and certifying superhuman performance in this setting, which we believe will assist in understanding the stage of current research on classification. We validate the convergence of the bounds and the assumptions of our theory on carefully designed toy experiments with known oracles. Moreover, we demonstrate the utility of our theory by meta-analyzing large-scale natural language processing tasks, for which an oracle does not exist, and show that under our mild assumptions a number of models from recent years have already achieved superhuman performance with high probability---suggesting that our new oracle based performance evaluation metrics are overdue as an alternative to the widely used accuracy metrics that are naively based on imperfect human annotations",
    "checked": true,
    "id": "2bf280fb550a852e6cb40c31bb75d200f731f388",
    "semantic_title": "humanly certifying superhuman classifiers",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4F1gvduDeL": {
    "title": "Few-Shot Domain Adaptation For End-to-End Communication",
    "volume": "spotlight",
    "abstract": "The problem of end-to-end learning of a communication system using an autoencoder -- consisting of an encoder, channel, and decoder modeled using neural networks -- has recently been shown to be an effective approach. A challenge faced in the practical adoption of this learning approach is that under changing channel conditions (e.g. a wireless link), it requires frequent retraining of the autoencoder in order to maintain a low decoding error rate. Since retraining is both time consuming and requires a large number of samples, it becomes impractical when the channel distribution is changing quickly. We propose to address this problem using a fast and sample-efficient (few-shot) domain adaptation method that does not change the encoder and decoder networks. Different from conventional training-time unsupervised or semi-supervised domain adaptation, here we have a trained autoencoder from a source distribution that we want to adapt (at test time) to a target distribution using only a small labeled dataset, and no unlabeled data. We focus on a generative channel model based on the Gaussian mixture density network (MDN), and propose a regularized, parameter-efficient adaptation of the MDN using a set of affine transformations. The learned affine transformations are then used to design an optimal transformation at the decoder input to compensate for the distribution shift, and effectively present to the decoder inputs close to the source distribution. Experiments on many simulated distribution changes common to the wireless setting, and a real mmWave FPGA testbed demonstrate the effectiveness of our method at adaptation using very few target domain samples~\\footnote{Code for our work: \\url{https://github.com/jayaram-r/domain-adaptation-autoencoder}}",
    "checked": true,
    "id": "d85ad141f2d0e800dd8a151857193d06ef2d5c1a",
    "semantic_title": "few-shot domain adaptation for end-to-end communication",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=688hNNMigVX": {
    "title": "Learning a Data-Driven Policy Network for Pre-Training Automated Feature Engineering",
    "volume": "spotlight",
    "abstract": "Feature engineering is widely acknowledged to be pivotal in tabular data analysis and prediction. Automated feature engineering (AutoFE) emerged to automate this process managed by experienced data scientists and engineers conventionally. In this area, most — if not all — prior work adopted an identical framework from the neural architecture search (NAS) method. While feasible, we posit that the NAS framework very much contradicts the way how human experts cope with the data since the inherent Markov decision process (MDP) setup differs. We point out that its data-unobserved setup consequentially results in an incapability to generalize across different datasets as well as also high computational cost. This paper proposes a novel AutoFE framework Feature Set Data-Driven Search (FETCH), a pipeline mainly for feature generation and selection. Notably, FETCH is built on a brand-new data-driven MDP setup using the tabular dataset as the state fed into the policy network. Further, we posit that the crucial merit of FETCH is its transferability where the yielded policy network trained on a variety of datasets is indeed capable to enact feature engineering on unseen data, without requiring additional exploration. To the best of our knowledge, this is a pioneer attempt to build a tabular data pre-training paradigm via AutoFE. Extensive experiments show that FETCH systematically surpasses the current state-of-the-art AutoFE methods and validates the transferability of AutoFE pre-training",
    "checked": true,
    "id": "352361e1531c9859e4c0b12d61551e458ccefc34",
    "semantic_title": "learning a data-driven policy network for pre-training automated feature engineering",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=75O7S_L4oY": {
    "title": "Learning Group Importance using the Differentiable Hypergeometric Distribution",
    "volume": "spotlight",
    "abstract": "Partitioning a set of elements into subsets of a priori unknown sizes is essential in many applications. These subset sizes are rarely explicitly learned - be it the cluster sizes in clustering applications or the number of shared versus independent generative latent factors in weakly-supervised learning. Probability distributions over correct combinations of subset sizes are non-differentiable due to hard constraints, which prohibit gradient-based optimization. In this work, we propose the differentiable hypergeometric distribution. The hypergeometric distribution models the probability of different group sizes based on their relative importance. We introduce reparameterizable gradients to learn the importance between groups and highlight the advantage of explicitly learning the size of subsets in two typical applications: weakly-supervised learning and clustering. In both applications, we outperform previous approaches, which rely on suboptimal heuristics to model the unknown size of groups",
    "checked": true,
    "id": "459aaba88972ecf483df51521f16b82595c9eadb",
    "semantic_title": "learning group importance using the differentiable hypergeometric distribution",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=oiwXWPDTyNk": {
    "title": "Concept-level Debugging of Part-Prototype Networks",
    "volume": "spotlight",
    "abstract": "Part-prototype Networks (ProtoPNets) are concept-based classifiers designed to achieve the same performance as black-box models without compromising transparency. ProtoPNets compute predictions based on similarity to class-specific part-prototypes learned to recognize parts of training examples, making it easy to faithfully determine what examples are responsible for any target prediction and why. However, like other models, they are prone to picking up confounders and shortcuts from the data, thus suffering from compromised prediction accuracy and limited generalization. We propose ProtoPDebug, an effective concept-level debugger for ProtoPNets in which a human supervisor, guided by the model's explanations, supplies feedback in the form of what part-prototypes must be forgotten or kept, and the model is fine-tuned to align with this supervision. Our experimental evaluation shows that ProtoPDebug outperforms state-of-the-art debuggers for a fraction of the annotation cost. An online experiment with laypeople confirms the simplicity of the feedback requested to the users and the effectiveness of the collected feedback for learning confounder-free part-prototypes. ProtoPDebug is a promising tool for trustworthy interactive learning in critical applications, as suggested by a preliminary evaluation on a medical decision making task",
    "checked": true,
    "id": "1557669e5b43c7dbffe7a9f23b80264098cdc7ea",
    "semantic_title": "concept-level debugging of part-prototype networks",
    "citation_count": 54,
    "authors": []
  },
  "https://openreview.net/forum?id=6BHlZgyPOZY": {
    "title": "Neuroevolution is a Competitive Alternative to Reinforcement Learning for Skill Discovery",
    "volume": "spotlight",
    "abstract": "Deep Reinforcement Learning (RL) has emerged as a powerful paradigm for training neural policies to solve complex control tasks. However, these policies tend to be overfit to the exact specifications of the task and environment they were trained on, and thus do not perform well when conditions deviate slightly or when composed hierarchically to solve even more complex tasks. Recent work has shown that training a mixture of policies, as opposed to a single one, that are driven to explore different regions of the state-action space can address this shortcoming by generating a diverse set of behaviors, referred to as skills, that can be collectively used to great effect in adaptation tasks or for hierarchical planning. This is typically realized by including a diversity term - often derived from information theory - in the objective function optimized by RL. However these approaches often require careful hyperparameter tuning to be effective. In this work, we demonstrate that less widely-used neuroevolution methods, specifically Quality Diversity (QD), are a competitive alternative to information-theory-augmented RL for skill discovery. Through an extensive empirical evaluation comparing eight state-of-the-art algorithms (four flagship algorithms from each line of work) on the basis of (i) metrics directly evaluating the skills' diversity, (ii) the skills' performance on adaptation tasks, and (iii) the skills' performance when used as primitives for hierarchical planning; QD methods are found to provide equal, and sometimes improved, performance whilst being less sensitive to hyperparameters and more scalable. As no single method is found to provide near-optimal performance across all environments, there is a rich scope for further research which we support by proposing future directions and providing optimized open-source implementations",
    "checked": true,
    "id": "2cd4233bf08e45760645f1e79be0acbd4262163e",
    "semantic_title": "neuroevolution is a competitive alternative to reinforcement learning for skill discovery",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=JpbLyEI5EwW": {
    "title": "Implicit Bias in Leaky ReLU Networks Trained on High-Dimensional Data",
    "volume": "spotlight",
    "abstract": "The implicit biases of gradient-based optimization algorithms are conjectured to be a major factor in the success of modern deep learning. In this work, we investigate the implicit bias of gradient flow and gradient descent in two-layer fully-connected neural networks with leaky ReLU activations when the training data are nearly-orthogonal, a common property of high-dimensional data. For gradient flow, we leverage recent work on the implicit bias for homogeneous neural networks to show that asymptotically, gradient flow produces a neural network with rank at most two. Moreover, this network is an $\\ell_2$-max-margin solution (in parameter space), and has a linear decision boundary that corresponds to an approximate-max-margin linear predictor. For gradient descent, provided the random initialization variance is small enough, we show that a single step of gradient descent suffices to drastically reduce the rank of the network, and that the rank remains small throughout training. We provide experiments which suggest that a small initialization scale is important for finding low-rank neural networks with gradient descent",
    "checked": true,
    "id": "a5dad5a2343a1e48790c39b377dfa7f7e0da9e2d",
    "semantic_title": "implicit bias in leaky relu networks trained on high-dimensional data",
    "citation_count": 45,
    "authors": []
  },
  "https://openreview.net/forum?id=O5rKg7IRQIO": {
    "title": "Guarded Policy Optimization with Imperfect Online Demonstrations",
    "volume": "spotlight",
    "abstract": "The Teacher-Student Framework (TSF) is a reinforcement learning setting where a teacher agent guards the training of a student agent by intervening and providing online demonstrations. Assuming optimal, the teacher policy has the perfect timing and capability to intervene in the learning process of the student agent, providing safety guarantee and exploration guidance. Nevertheless, in many real-world settings it is expensive or even impossible to obtain a well-performing teacher policy. In this work, we relax the assumption of a well-performing teacher and develop a new method that can incorporate arbitrary teacher policies with modest or inferior performance. We instantiate an Off-Policy Reinforcement Learning algorithm, termed Teacher-Student Shared Control (TS2C), which incorporates teacher intervention based on trajectory-based value estimation. Theoretical analysis validates that the proposed TS2C algorithm attains efficient exploration and substantial safety guarantee without being affected by the teacher's own performance. Experiments on various continuous control tasks show that our method can exploit teacher policies at different performance levels while maintaining a low training cost. Moreover, the student policy surpasses the imperfect teacher policy in terms of higher accumulated reward in held-out testing environments. Code is available at https://metadriverse.github.io/TS2C",
    "checked": true,
    "id": "c28c30a846d97f7e9c0722fbc35bb88aba3c6b04",
    "semantic_title": "guarded policy optimization with imperfect online demonstrations",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=M2unceRvqhh": {
    "title": "Learning with Logical Constraints but without Shortcut Satisfaction",
    "volume": "spotlight",
    "abstract": "Recent studies have started to explore the integration of logical knowledge into deep learning via encoding logical constraints as an additional loss function. However, existing approaches tend to vacuously satisfy logical constraints through shortcuts, failing to fully exploit the knowledge. In this paper, we present a new framework for learning with logical constraints. Specifically, we address the shortcut satisfaction issue by introducing dual variables for logical connectives, encoding how the constraint is satisfied. We further propose a variational framework where the encoded logical constraint is expressed as a distributional loss that is compatible with the model's original training loss. The theoretical analysis shows that the proposed approach bears some nice properties, and the experimental evaluations demonstrate its superior performance in both model generalizability and constraint satisfaction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7oFuxtJtUMH": {
    "title": "Certified Training: Small Boxes are All You Need",
    "volume": "spotlight",
    "abstract": "To obtain, deterministic guarantees of adversarial robustness, specialized training methods are used. We propose, SABR, a novel such certified training method, based on the key insight that propagating interval bounds for a small but carefully selected subset of the adversarial input region is sufficient to approximate the worst-case loss over the whole region while significantly reducing approximation errors. We show in an extensive empirical evaluation that SABR outperforms existing certified defenses in terms of both standard and certifiable accuracies across perturbation magnitudes and datasets, pointing to a new class of certified training methods promising to alleviate the robustness-accuracy trade-off",
    "checked": true,
    "id": "a2f519580930421aec9501d5cbbc365893877750",
    "semantic_title": "certified training: small boxes are all you need",
    "citation_count": 49,
    "authors": []
  },
  "https://openreview.net/forum?id=dKkMnCWfVmm": {
    "title": "Multi-Objective Online Learning",
    "volume": "spotlight",
    "abstract": "This paper presents a systematic study of multi-objective online learning. We first formulate the framework of Multi-Objective Online Convex Optimization, which encompasses a novel multi-objective regret. This regret is built upon a sequence-wise extension of the commonly used discrepancy metric Pareto suboptimality gap in zero-order multi-objective bandits. We then derive an equivalent form of the regret, making it amenable to be optimized via first-order iterative methods. To motivate the algorithm design, we give an explicit example in which equipping OMD with the vanilla min-norm solver for gradient composition will incur a linear regret, which shows that merely regularizing the iterates, as in single-objective online learning, is not enough to guarantee sublinear regrets in the multi-objective setting. To resolve this issue, we propose a novel min-regularized-norm solver that regularizes the composite weights. Combining min-regularized-norm with OMD results in the Doubly Regularized Online Mirror Multiple Descent algorithm. We further derive the multi-objective regret bound for the proposed algorithm, which matches the optimal bound in the single-objective setting. Extensive experiments on several real-world datasets verify the effectiveness of the proposed algorithm",
    "checked": true,
    "id": "fbb265bb6bc64b61defd8f4b79981a6a3be191f2",
    "semantic_title": "multi-objective online learning",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=3ULaIHxn9u7": {
    "title": "Seeing Differently, Acting Similarly: Heterogeneously Observable Imitation Learning",
    "volume": "spotlight",
    "abstract": "In many real-world imitation learning tasks, the demonstrator and the learner have to act under different observation spaces. This situation brings significant obstacles to existing imitation learning approaches, since most of them learn policies under homogeneous observation spaces. On the other hand, previous studies under different observation spaces have strong assumptions that these two observation spaces coexist during the entire learning process. However, in reality, the observation coexistence will be limited due to the high cost of acquiring expert observations. In this work, we study this challenging problem with limited observation coexistence under heterogeneous observations: Heterogeneously Observable Imitation Learning (HOIL). We identify two underlying issues in HOIL: the dynamics mismatch and the support mismatch, and further propose the Importance Weighting with REjection (IWRE) algorithm based on importance weighting and learning with rejection to solve HOIL problems. Experimental results show that IWRE can solve various HOIL tasks, including the challenging tasks of transforming the vision-based demonstrations to random access memory (RAM)-based policies in the Atari domain, even with limited visual observations",
    "checked": false,
    "id": "9d0d494c4ab60b4d8e72b761c6972f0d980d6b5b",
    "semantic_title": "seeing differently, acting similarly: imitation learning with heterogeneous observations",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=wkg_b4-IwTZ": {
    "title": "A Closer Look at Model Adaptation using Feature Distortion and Simplicity Bias",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "22faadbcfe2c2886899c68dbe2d4e88d8860e54b",
    "semantic_title": "a closer look at model adaptation using feature distortion and simplicity bias",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=WzGdBqcBicl": {
    "title": "Understanding and Adopting Rational Behavior by Bellman Score Estimation",
    "volume": "spotlight",
    "abstract": "We are interested in solving a class of problems that seek to understand and adopt rational behavior from demonstrations. We may broadly classify these problems into four categories of reward identification, counterfactual analysis, behavior imitation, and behavior transfer. In this work, we make a key observation that knowing how changes in the underlying rewards affect the optimal behavior allows one to solve a variety of aforementioned problems. To a local approximation, this quantity is precisely captured by what we term the Bellman score, i.e gradient of log probabilities of the optimal policy with respect to the reward. We introduce the Bellman score operator which provably converges to the gradient of the infinite-horizon optimal Q-values with respect to the reward which can then be used to directly estimate the score. Guided by our theory, we derive a practical score-learning algorithm which can be used for score estimation in high-dimensional state-actions spaces. We show that score-learning can be used to reliably identify rewards, perform counterfactual predictions, achieve state-of-the-art behavior imitation, and transfer policies across environments",
    "checked": true,
    "id": "2a733c6c5af9aa58f1ff58605e9f9ac830412640",
    "semantic_title": "understanding and adopting rational behavior by bellman score estimation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=_xlsjehDvlY": {
    "title": "STUNT: Few-shot Tabular Learning with Self-generated Tasks from Unlabeled Tables",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "b50a12cce08050c49f5cda13a76fd32cab2d07ce",
    "semantic_title": "stunt: few-shot tabular learning with self-generated tasks from unlabeled tables",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=bhUPJnS2g0X": {
    "title": "Ask Me Anything: A simple strategy for prompting language models",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "fb49e88c6bd676516898e911e42b4f8479e6f1bf",
    "semantic_title": "ask me anything: a simple strategy for prompting language models",
    "citation_count": 226,
    "authors": []
  },
  "https://openreview.net/forum?id=cP2QVK-uygd": {
    "title": "On Representing Linear Programs by Graph Neural Networks",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "e0fe6efa4e0046f6b03d83e5bad4516277c22cee",
    "semantic_title": "on representing linear programs by graph neural networks",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=VZ5EaTI6dqa": {
    "title": "Scale-invariant Bayesian Neural Networks with Connectivity Tangent Kernel",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "ae5b0395a1b2d41aebe18d8d91c86b3284b0309b",
    "semantic_title": "scale-invariant bayesian neural networks with connectivity tangent kernel",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=nN_nBVKAhhD": {
    "title": "Minimalistic Unsupervised Representation Learning with the Sparse Manifold Transform",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "0380271e23c5a3348f9f8ad1906b692b22f8b75e",
    "semantic_title": "minimalistic unsupervised representation learning with the sparse manifold transform",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=aKcS3xojnwY": {
    "title": "GEASS: Neural causal feature selection for high-dimensional biological data",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "944b76bcdf4618b047245c6dc324ee2fbbb24e8c",
    "semantic_title": "geass: neural causal feature selection for high-dimensional biological data",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=i9UlAr1T_xl": {
    "title": "SmartFRZ: An Efficient Training Framework using Attention-Based Layer Freezing",
    "volume": "spotlight",
    "abstract": "There has been a proliferation of artificial intelligence applications, where model training is key to promising high-quality services for these applications. However, the model training process is both time-intensive and energy-intensive, inevitably affecting the user's demand for application efficiency. Layer freezing, an efficient model training technique, has been proposed to improve training efficiency. Although existing layer freezing methods demonstrate the great potential to reduce model training costs, they still remain shortcomings such as lacking generalizability and compromised accuracy. For instance, existing layer freezing methods either require the freeze configurations to be manually defined before training, which does not apply to different networks, or use heuristic freezing criteria that is hard to guarantee decent accuracy in different scenarios. Therefore, there lacks a generic and smart layer freezing method that can automatically perform ``in-situation'' layer freezing for different networks during training processes. To this end, we propose a generic and efficient training framework (SmartFRZ). The core proposed technique in SmartFRZ is attention-guided layer freezing, which can automatically select the appropriate layers to freeze without compromising accuracy. Experimental results show that SmartFRZ effectively reduces the amount of computation in training and achieves significant training acceleration, and outperforms the state-of-the-art layer freezing approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u-RuvyDYqCM": {
    "title": "The In-Sample Softmax for Offline Reinforcement Learning",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "488fb301e6a18df78bf3e3cb6a79a8a776296ab2",
    "semantic_title": "the in-sample softmax for offline reinforcement learning",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=TdTGGj7fYYJ": {
    "title": "Unsupervised Meta-learning via Few-shot Pseudo-supervised Contrastive Learning",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "704fac2b3a69da67b7e742147f1c4cf4b315bd4d",
    "semantic_title": "unsupervised meta-learning via few-shot pseudo-supervised contrastive learning",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=CZmHHj9MgkP": {
    "title": "Guiding Energy-based Models via Contrastive Latent Variables",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "7f74086d2772cdb40196ece48830b3644f0a7ae2",
    "semantic_title": "guiding energy-based models via contrastive latent variables",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=ZzdBhtEH9yB": {
    "title": "Implicit regularization in Heavy-ball momentum accelerated stochastic gradient descent",
    "volume": "spotlight",
    "abstract": "It is well known that the finite step-size ($h$) in Gradient descent (GD) implicitly regularizes solutions to flatter minimas. A natural question to ask is \\textit{Does the momentum parameter $\\beta$ (say) play a role in implicit regularization in Heavy-ball (H.B) momentum accelerated gradient descent (GD+M)?}. To answer this question, first, we show that the trajectory traced by discrete H.B momentum update (GD+M) is $O(h^2)$ close to a continuous trajectory induced by a modified loss, which consists of an original loss and an implicit regularizer. This implicit regularizer for (GD+M) is indeed stronger than that of (GD) by factor of $(\\frac{1+\\beta}{1-\\beta})$, thus explaining why (GD+M) shows better generalization performance and higher test accuracy than (GD). Furthermore, we extend our analysis to stochastic version of gradient descent with momentum (SGD+M) and propose a deterministic continuous trajectory that is $O(h^2)$ close to the discrete update of (SGD+M) in a strong approximation sense. We explore the implicit regularization in (SGD+M) and (GD+M) through a series of experiments validating our theory",
    "checked": true,
    "id": "bed75393db4c3216470f93d7ec21803ec40b3c72",
    "semantic_title": "implicit regularization in heavy-ball momentum accelerated stochastic gradient descent",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=M_MvkWgQSt": {
    "title": "Real-time variational method for learning neural trajectory and its dynamics",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "ca22a7638c775dbeaac338773e8aa3f45a518b9e",
    "semantic_title": "real-time variational method for learning neural trajectory and its dynamics",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=ZMz-sW6gCLF": {
    "title": "Energy-Inspired Self-Supervised Pretraining for Vision Models",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "07ba6023e5aed51a68dd178a46e79e74ee009e1a",
    "semantic_title": "energy-inspired self-supervised pretraining for vision models",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=lH1PV42cbF": {
    "title": "Binding Language Models in Symbolic Languages",
    "volume": "spotlight",
    "abstract": "Though end-to-end neural approaches have recently been dominating NLP tasks in both performance and ease-of-use, they lack interpretability and robustness. We propose Binder, a training-free neural-symbolic framework that maps the task input to a program, which (1) allows binding a unified API of language model (LM) functionalities to a programming language (e.g., SQL, Python) to extend its grammar coverage and thus tackle more diverse questions, (2) adopts an LM as both the program parser and the underlying model called by the API during execution, and (3) requires only a few in-context exemplar annotations. Specifically, we employ GPT-3 Codex as the LM. In the parsing stage, with only a few in-context exemplars, Codex is able to identify the part of the task input that cannot be answerable by the original programming language, correctly generate API calls to prompt Codex to solve the unanswerable part, and identify where to place the API calls while being compatible with the original grammar. In the execution stage, Codex can perform versatile functionalities (e.g., commonsense QA, information extraction) given proper prompts in the API calls. Binder achieves state-of-the-art results on WikiTableQuestions and TabFact datasets, with explicit output programs that benefit human debugging. Note that previous best systems are all finetuned on tens of thousands of task-specific samples, while Binder only uses dozens of annotations as in-context exemplars without any training. Our code is available at anonymized",
    "checked": true,
    "id": "f58ca7ba4a08b7082e86b7a5989b4b0fda2107ab",
    "semantic_title": "binding language models in symbolic languages",
    "citation_count": 230,
    "authors": []
  },
  "https://openreview.net/forum?id=Z4s73sJYQM": {
    "title": "Evolve Smoothly, Fit Consistently: Learning Smooth Latent Dynamics For Advection-Dominated Systems",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "f5b6f673726e6c3347329180f20cd6cc6490b3ed",
    "semantic_title": "evolve smoothly, fit consistently: learning smooth latent dynamics for advection-dominated systems",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=Ovnwe_sDQW": {
    "title": "BC-IRL: Learning Generalizable Reward Functions from Demonstrations",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "d8c0b2fbacdade8bde676feff9da9aaeda244839",
    "semantic_title": "bc-irl: learning generalizable reward functions from demonstrations",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=z9C5dGip90": {
    "title": "Phase2vec: dynamical systems embedding with a physics-informed convolutional network",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "1f2848abe8ebbc1ef5d227831d793c93fb4e1e07",
    "semantic_title": "phase2vec: dynamical systems embedding with a physics-informed convolutional network",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=1hKE9qjvz-": {
    "title": "gDDIM: Generalized denoising diffusion implicit models",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "670bab7b71be5e432b0dc60f406a6115cf6c0633",
    "semantic_title": "gddim: generalized denoising diffusion implicit models",
    "citation_count": 128,
    "authors": []
  },
  "https://openreview.net/forum?id=IPrzNbddXV": {
    "title": "FedExP: Speeding Up Federated Averaging via Extrapolation",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "2b04a0c88f4533c12d107e1c06234a95120bb0e0",
    "semantic_title": "fedexp: speeding up federated averaging via extrapolation",
    "citation_count": 63,
    "authors": []
  },
  "https://openreview.net/forum?id=T-qVtA3pAxG": {
    "title": "Serving Graph Compression for Graph Neural Networks",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "145c76206afac4958b4eb768e1f648ad980a2abb",
    "semantic_title": "serving graph compression for graph neural networks",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=Cs3r5KLdoj": {
    "title": "Learning MLPs on Graphs: A Unified View of Effectiveness, Robustness, and Efficiency",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "889ea8c9210f49ff6e1690f5c06e626bbb1d17b0",
    "semantic_title": "learning mlps on graphs: a unified view of effectiveness, robustness, and efficiency",
    "citation_count": 58,
    "authors": []
  },
  "https://openreview.net/forum?id=QPtMRyk5rb": {
    "title": "Contrastive Audio-Visual Masked Autoencoder",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "3fa1505ec327b82416987a7db3dadab8b12601ea",
    "semantic_title": "contrastive audio-visual masked autoencoder",
    "citation_count": 136,
    "authors": []
  },
  "https://openreview.net/forum?id=IM4xp7kGI5V": {
    "title": "The Asymmetric Maximum Margin Bias of Quasi-Homogeneous Neural Networks",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "c716f71a74ec79caade004da6090489256bd1ceb",
    "semantic_title": "the asymmetric maximum margin bias of quasi-homogeneous neural networks",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=MhuFzFsrfvH": {
    "title": "Optimal Transport for Offline Imitation Learning",
    "volume": "spotlight",
    "abstract": "With the advent of large datasets, offline reinforcement learning is a promising framework for learning good decision-making policies without the need to interact with the real environment. However, offline RL requires the dataset to be reward-annotated, which presents practical challenges when reward engineering is difficult or when obtaining reward annotations is labor-intensive. In this paper, we introduce Optimal Transport Relabeling (OTR), an imitation learning algorithm that can automatically relabel offline data of mixed and unknown quality with rewards from a few good demonstrations. OTR's key idea is to use optimal transport to compute an optimal alignment between an unlabeled trajectory in the dataset and an expert demonstration to obtain a similarity measure that can be interpreted as a reward, which can then be used by an offline RL algorithm to learn the policy. OTR is easy to implement and computationally efficient. On D4RL benchmarks, we demonstrate that OTR with a single demonstration can consistently match the performance of offline RL with ground-truth rewards",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8aHzds2uUyB": {
    "title": "Is Reinforcement Learning (Not) for Natural Language Processing: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization",
    "volume": "spotlight",
    "abstract": "We tackle the problem of aligning pre-trained large language models (LMs) with human preferences. If we view text generation as a sequential decision-making problem, reinforcement learning (RL) appears to be a natural conceptual framework. However, using RL for LM-based generation faces empirical challenges, including training instability due to the combinatorial action space, as well as a lack of open-source libraries and benchmarks customized for LM alignment. Thus, a question rises in the research community: is RL a practical paradigm for NLP? To help answer this, we first introduce an open-source modular library, $RL4LMs$ (Reinforcement Learning for Language Models), for optimizing language generators with RL. The library consists of on-policy RL algorithms that can be used to train any encoder or encoder-decoder LM in the HuggingFace library (Wolf et al. 2020) with an arbitrary reward function. Next, we present the $GRUE$ (General Reinforced-language Understanding Evaluation) benchmark, a set of 6 language generation tasks which are supervised not by target strings, but by reward functions which capture automated measures of human preference.GRUE is the first leaderboard-style evaluation of RL algorithms for NLP tasks. Finally, we introduce an easy-to-use, performant RL algorithm, $NLPO$ (Natural Language Policy Optimization)} that learns to effectively reduce the combinatorial action space in language generation. We show 1) that RL techniques are generally better than supervised methods at aligning LMs to human preferences; and 2) that NLPO exhibits greater stability and performance than previous policy gradient methods (e.g., PPO (Schulman et al. 2017)), based on both automatic and human evaluations",
    "checked": true,
    "id": "aa46002c3c36a4b474da65793d57efe4a57d2116",
    "semantic_title": "is reinforcement learning (not) for natural language processing: benchmarks, baselines, and building blocks for natural language policy optimization",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=VZX2I_VVJKH": {
    "title": "Learning multi-scale local conditional probability models of images",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "aeab6599b9203e9f4a4f14313c326aed28954e4d",
    "semantic_title": "learning multi-scale local conditional probability models of images",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=9Z_GfhZnGH": {
    "title": "Disentanglement with Biological Constraints: A Theory of Functional Cell Types",
    "volume": "spotlight",
    "abstract": "Neurons in the brain are often finely tuned for specific task variables. Moreover, such disentangled representations are highly sought after in machine learning. Here we mathematically prove that simple biological constraints on neurons, namely nonnegativity and energy efficiency in both activity and weights, promote such sought after disentangled representations by enforcing neurons to become selective for single factors of task variation. We demonstrate these constraints lead to disentanglement in a variety of tasks and architectures, including variational autoencoders. We also use this theory to explain why the brain partitions its cells into distinct cell types such as grid and object-vector cells, and also explain when the brain instead entangles representations in response to entangled task factors. Overall, this work provides a mathematical understanding of why single neurons in the brain often represent single human-interpretable factors, and steps towards an understanding task structure shapes the structure of brain representation",
    "checked": true,
    "id": "43694ae3de01d0ebd730485b5890ed84094f7ad1",
    "semantic_title": "disentanglement with biological constraints: a theory of functional cell types",
    "citation_count": 54,
    "authors": []
  },
  "https://openreview.net/forum?id=J7Uh781A05p": {
    "title": "Learning rigid dynamics with face interaction graph networks",
    "volume": "spotlight",
    "abstract": "Simulating rigid collisions among arbitrary shapes is notoriously difficult due to complex geometry and the strong non-linearity of the interactions. While graph neural network (GNN)-based models are effective at learning to simulate complex physical dynamics, such as fluids, cloth and articulated bodies, they have been less effective and efficient on rigid-body physics, except with very simple shapes. Existing methods that model collisions through the meshes' nodes are often inaccurate because they struggle when collisions occur on faces far from nodes. Alternative approaches that represent the geometry densely with many particles are prohibitively expensive for complex shapes. Here we introduce the ``Face Interaction Graph Network'' (FIGNet) which extends beyond GNN-based methods, and computes interactions between mesh faces, rather than nodes. Compared to learned node- and particle-based methods, FIGNet is around 4x more accurate in simulating complex shape interactions, while also 8x more computationally efficient on sparse, rigid meshes. Moreover, FIGNet can learn frictional dynamics directly from real-world data, and can be more accurate than analytical solvers given modest amounts of training data. FIGNet represents a key step forward in one of the few remaining physical domains which have seen little competition from learned simulators, and offers allied fields such as robotics, graphics and mechanical design a new tool for simulation and model-based planning",
    "checked": true,
    "id": "d6fdd8fc0c5fc052d040687e72638fb4297661cc",
    "semantic_title": "learning rigid dynamics with face interaction graph networks",
    "citation_count": 42,
    "authors": []
  },
  "https://openreview.net/forum?id=6iDHce-0B-a": {
    "title": "Implicit Bias of Large Depth Networks: a Notion of Rank for Nonlinear Functions",
    "volume": "spotlight",
    "abstract": "We show that the representation cost of fully connected neural networks with homogeneous nonlinearities - which describes the implicit bias in function space of networks with $L_2$-regularization or with losses such as the cross-entropy - converges as the depth of the network goes to infinity to a notion of rank over nonlinear functions. We then inquire under which conditions the global minima of the loss recover the `true' rank of the data: we show that for too large depths the global minimum will be approximately rank 1 (underestimating the rank); we then argue that there is a range of depths which grows with the number of datapoints where the true rank is recovered. Finally, we discuss the effect of the rank of a classifier on the topology of the resulting class boundaries and show that autoencoders with optimal nonlinear rank are naturally denoising",
    "checked": true,
    "id": "58135342c0300d54760f735123092c3559d4c3c8",
    "semantic_title": "implicit bias of large depth networks: a notion of rank for nonlinear functions",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=uzFQpkEzOo": {
    "title": "Depth Separation with Multilayer Mean-Field Networks",
    "volume": "spotlight",
    "abstract": "Depth separation—why a deeper network is more powerful than a shallow one—has been a major problem in deep learning theory. Previous results often focus on representation power, for example, Safran et al. (2019) constructed a function that is easy to approximate using a 3-layer network but not approximable by any 2-layer network. In this paper, we show that this separation is in fact algorithmic: one can learn the function constructed by Safran et al. (2019) using an overparametrized network with polynomially many neurons efﬁciently. Our result relies on a new way of extending the mean-ﬁeld limit to multilayer networks, and a decomposition of loss that factors out the error introduced by the discretization of inﬁnite-width mean-ﬁeld networks",
    "checked": true,
    "id": "9364149ffcb3156ad2ad265be64b55f4fc0c9af3",
    "semantic_title": "depth separation with multilayer mean-field networks",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=i_1rbq8yFWC": {
    "title": "Rhino: Deep Causal Temporal Relationship Learning with History-dependent Noise",
    "volume": "spotlight",
    "abstract": "Discovering causal relationships between different variables from time series data has been a long-standing challenge for many domains. For example, in stock markets, the announcement of acquisitions from leading companies may have immediate effects on stock prices and increase the uncertainty of the future market due to this past action. To discover causal relations in such case, the model needs to consider non-linear relations between variables, instantaneous effect and the change of noise distribution due to past actions. We name the latter as history-dependent noise. However, previous works do not offer a solution addressing all these problems together. In this paper, we propose a structural equation model, called Rhino, which combines vector auto-regression, deep learning and variational inference to model non-linear relationships with instantaneous effects while allowing the noise distribution to be modulated by history observations. Theoretically, we prove the structural identifiability of Rhino. Our empirical results from extensive synthetic experiments and two real-world benchmarks demonstrate better discovery performance compared to relevant baselines, with ablation studies revealing its robustness under model misspecification",
    "checked": true,
    "id": "87d2722c58c25fe9878ab4b29def9bb27675541d",
    "semantic_title": "rhino: deep causal temporal relationship learning with history-dependent noise",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=VD-AYtP0dve": {
    "title": "Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation",
    "volume": "spotlight",
    "abstract": "We introduce a method to measure uncertainty in large language models. For tasks like question answering, it is essential to know when we can trust the natural language outputs of foundation models. We show that measuring uncertainty in natural language is challenging because of \"semantic equivalence\"—different sentences can mean the same thing. To overcome these challenges we introduce semantic entropy—an entropy which incorporates linguistic invariances created by shared meanings. Our method is unsupervised, uses only a single model, and requires no modifications to off-the-shelf language models. In comprehensive ablation studies we show that the semantic entropy is more predictive of model accuracy on question answering data sets than comparable baselines",
    "checked": true,
    "id": "507465f8d46489a68a527cb5304d76bdb6c31ed9",
    "semantic_title": "semantic uncertainty: linguistic invariances for uncertainty estimation in natural language generation",
    "citation_count": 343,
    "authors": []
  },
  "https://openreview.net/forum?id=cMJo1FTwBTQ": {
    "title": "DINO as a von Mises-Fisher mixture model",
    "volume": "spotlight",
    "abstract": "Self-distillation methods using Siamese networks are popular for self-supervised pre-training. DINO is one such method based on a cross-entropy loss between $K$-dimensional probability vectors, obtained by applying a softmax function to the dot product between representations and learnt prototypes. Given the fact that the learned representations are $L^2$-normalized, we show that DINO and its derivatives, such as iBOT, can be interpreted as a mixture model of von Mises-Fisher components. With this interpretation, DINO assumes equal precision for all components when the prototypes are also $L^2$-normalized. Using this insight we propose DINO-vMF, that adds appropriate normalization constants when computing the cluster assignment probabilities. Unlike DINO, DINO-vMF is stable also for the larger ViT-Base model with unnormalized prototypes. We show that the added flexibility of the mixture model is beneficial in terms of better image representations. The DINO-vMF pre-trained model consistently performs better than DINO on a range of downstream tasks. We obtain similar improvements for iBOT-vMF vs iBOT and thereby show the relevance of our proposed modification also for other methods derived from DINO",
    "checked": false,
    "id": "df183cedd2990f966cc768ecf9b9e0064e15a122",
    "semantic_title": "trajectory poisson multi-bernoulli mixture filter for traffic monitoring using a drone",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=ZCStthyW-TD": {
    "title": "Associative Memory Augmented Asynchronous Spatiotemporal Representation Learning for Event-based Perception",
    "volume": "spotlight",
    "abstract": "We propose $\\textit{EventFormer}$, a computationally efficient event-based representation learning framework for asynchronously processing event camera data. EventFormer treats sparse input events as a spatially unordered set and models their spatial interactions using self-attention mechanism. An associative memory-augmented recurrent module is used to correlate with the stored representation computed from past events. A memory addressing mechanism is proposed to store and retrieve the latent states only $\\textit{where}$ these events occur and update them only $\\textit{when}$ they occur. The representation learning shift from input space to the latent memory space resulting in reduced computation cost for processing each event. We show that EventFormer achieves 0.5$\\%$ and 9$\\%$ better accuracy with 30000$\\times$ and 200$\\times$ less computation compared to the state-of-the-art dense and event-based method, respectively, on event-based object recognition datasets",
    "checked": true,
    "id": "24fbb2b0da93c92f3553994f2e5b5fbf04a1441f",
    "semantic_title": "associative memory augmented asynchronous spatiotemporal representation learning for event-based perception",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=9piH3Hg8QEf": {
    "title": "SMART: Self-supervised Multi-task pretrAining with contRol Transformers",
    "volume": "spotlight",
    "abstract": "Self-supervised pretraining has been extensively studied in language and vision domains, where a unified model can be easily adapted to various downstream tasks by pretraining representations without explicit labels. When it comes to sequential decision-making tasks, however, it is difficult to properly design such a pretraining approach that can cope with both high-dimensional perceptual information and the complexity of sequential control over long interaction horizons. The challenge becomes combinatorially more complex if we want to pretrain representations amenable to a large variety of tasks. To tackle this problem, in this work, we formulate a general pretraining-finetuning pipeline for sequential decision making, under which we propose a generic pretraining framework \\textit{Self-supervised Multi-task pretrAining with contRol Transformer (SMART)}. By systematically investigating pretraining regimes, we carefully design a Control Transformer (CT) coupled with a novel control-centric pretraining objective in a self-supervised manner. SMART encourages the representation to capture the common essential information relevant to short-term control and long-term control, which is transferrable across tasks. We show by extensive experiments in DeepMind Control Suite that SMART significantly improves the learning efficiency among seen and unseen downstream tasks and domains under different learning scenarios including Imitation Learning (IL) and Reinforcement Learning (RL). Benefiting from the proposed control-centric objective, SMART is resilient to distribution shift between pretraining and finetuning, and even works well with low-quality pretraining datasets that are randomly collected. The codebase, pretrained models and datasets are provided at https://github.com/microsoft/smart",
    "checked": true,
    "id": "abba9a6f99d877fdd1b8412ddfcc26fdac6163dc",
    "semantic_title": "smart: self-supervised multi-task pretraining with control transformers",
    "citation_count": 42,
    "authors": []
  },
  "https://openreview.net/forum?id=gSHyqBijPFO": {
    "title": "TEMPERA: Test-Time Prompt Editing via Reinforcement Learning",
    "volume": "spotlight",
    "abstract": "Careful prompt design is critical to the use of large language models in zero-shot or few-shot learning. As a consequence, there is a growing interest in automated methods to design optimal prompts. In this work, we propose Test-time Prompt Editing using Reinforcement learning (TEMPERA). In contrast to prior prompt generation methods, TEMPERA can efficiently leverage prior knowledge, is adaptive to different queries and provides an interpretable prompt for every query. To achieve this, we design a novel action space that allows flexible editing of the initial prompts covering a wide set of commonly-used components like instructions, few-shot exemplars, and verbalizers. The proposed method achieves significant gains compared with recent SoTA approaches like prompt tuning, AutoPrompt, and RLPrompt, across a variety of tasks including sentiment analysis, topic classification, natural language inference, and reading comprehension. Our method achieves 5.33x on average improvement in sample efficiency when compared to the traditional fine-tuning methods",
    "checked": false,
    "id": "66c5a0fbde7e06bf0ef179e660b6a211bcd80aac",
    "semantic_title": "tempera: test-time prompting via reinforcement learning",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=ThXqBsRI-cY": {
    "title": "Provable Defense Against Geometric Transformations",
    "volume": "spotlight",
    "abstract": "Geometric image transformations that arise in the real world, such as scaling and rotation, have been shown to easily deceive deep neural networks (DNNs). Hence, training DNNs to be certifiably robust to these perturbations is critical. However, no prior work has been able to incorporate the objective of deterministic certified robustness against geometric transformations into the training procedure, as existing verifiers are exceedingly slow. To address these challenges, we propose the first provable defense for deterministic certified geometric robustness. Our framework leverages a novel GPU-optimized verifier that can certify images between 60$\\times$ to 42,600$\\times$ faster than existing geometric robustness verifiers, and thus unlike existing works, is fast enough for use in training. Across multiple datasets, our results show that networks trained via our framework consistently achieve state-of-the-art deterministic certified geometric robustness and clean accuracy. Furthermore, for the first time, we verify the geometric robustness of a neural network for the challenging, real-world setting of autonomous driving",
    "checked": true,
    "id": "32e5d216816786c8cdaedeb64097ef3265661f21",
    "semantic_title": "provable defense against geometric transformations",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=Zb6c8A-Fghk": {
    "title": "Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations",
    "volume": "spotlight",
    "abstract": "Neural network classifiers can largely rely on simple spurious features, such as image backgrounds, to make predictions. However, even in these cases, we show that they still often learn core features associated with the desired attributes of the data, contrary to recent findings. Inspired by this insight, we demonstrate that simple last layer retraining can match or outperform state-of-the-art approaches on spurious correlation benchmarks, but with profoundly lower complexity and computational expenses. Moreover, we show that last layer retraining on large ImageNet-trained models can also significantly reduce reliance on background and texture information, improving robustness to covariate shift, after only minutes of training on a single GPU",
    "checked": true,
    "id": "14a3aae8060338e3fbefc2af694890b019874d4f",
    "semantic_title": "last layer re-training is sufficient for robustness to spurious correlations",
    "citation_count": 355,
    "authors": []
  },
  "https://openreview.net/forum?id=hWwY_Jq0xsN": {
    "title": "Towards Interpretable Deep Reinforcement Learning with Human-Friendly Prototypes",
    "volume": "spotlight",
    "abstract": "Despite recent success of deep learning models in research settings, their application in sensitive domains remains limited because of their opaque decision-making processes. Taking to this challenge, people have proposed various eXplainable AI (XAI) techniques designed to calibrate trust and understandability of black-box models, with the vast majority of work focused on supervised learning. Here, we focus on making an \"interpretable-by-design\" deep reinforcement learning agent which is forced to use human-friendly prototypes in its decisions, thus making its reasoning process clear. Our proposed method, dubbed Prototype-Wrapper Network (PW-Net), wraps around any neural agent backbone, and results indicate that it does not worsen performance relative to black-box models. Most importantly, we found in a user study that PW-Nets supported better trust calibration and task performance relative to standard interpretability approaches and black-boxes",
    "checked": true,
    "id": "c0de7a5e50976bbd56a89a5de21c0e2c76cfa03f",
    "semantic_title": "towards interpretable deep reinforcement learning with human-friendly prototypes",
    "citation_count": 45,
    "authors": []
  },
  "https://openreview.net/forum?id=P4MUGRM4Acu": {
    "title": "The Surprising Effectiveness of Equivariant Models in Domains with Latent Symmetry",
    "volume": "spotlight",
    "abstract": "Extensive work has demonstrated that equivariant neural networks can significantly improve sample efficiency and generalization by enforcing an inductive bias in the network architecture. These applications typically assume that the domain symmetry is fully described by explicit transformations of the model inputs and outputs. However, many real-life applications contain only latent or partial symmetries which cannot be easily described by simple transformations of the input. In these cases, it is necessary to learn symmetry in the environment instead of imposing it mathematically on the network architecture. We discover, surprisingly, that imposing equivariance constraints that do not exactly match the domain symmetry is very helpful in learning the true symmetry in the environment. We differentiate between extrinsic and incorrect symmetry constraints and show that while imposing incorrect symmetry can impede the model's performance, imposing extrinsic symmetry can actually improve performance. We demonstrate that an equivariant model can significantly outperform non-equivariant methods on domains with latent symmetries both in supervised learning and in reinforcement learning for robotic manipulation and control problems",
    "checked": true,
    "id": "19b8f213ba5c70afc9bf5dd5945a0771e12f9ef5",
    "semantic_title": "the surprising effectiveness of equivariant models in domains with latent symmetry",
    "citation_count": 37,
    "authors": []
  },
  "https://openreview.net/forum?id=j8IiQUM33s": {
    "title": "Task-customized Masked Autoencoder via Mixture of Cluster-conditional Experts",
    "volume": "spotlight",
    "abstract": "Masked Autoencoder (MAE) is a prevailing self-supervised learning method that achieves promising results in model pre-training. However, when the various downstream tasks have data distributions different from the pre-training data, the semantically irrelevant pre-training information might result in negative transfer, impeding MAE's scalability. To address this issue, we propose a novel MAE-based pre-training paradigm, Mixture of Cluster-conditional Experts (MoCE), which can be trained once but provides customized pre-training models for diverse downstream tasks. Different from the mixture of experts (MoE), our MoCE trains each expert only with semantically relevant images by using cluster-conditional gates. Thus, each downstream task can be allocated to its customized model pre-trained with data most similar to the downstream data. Experiments on a collection of 11 downstream tasks show that MoCE outperforms the vanilla MAE by 2.45\\% on average. It also obtains new state-of-the-art self-supervised learning results on detection and segmentation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uyqks-LILZX": {
    "title": "Modeling the Data-Generating Process is Necessary for Out-of-Distribution Generalization",
    "volume": "spotlight",
    "abstract": "Recent empirical studies on domain generalization (DG) have shown that DG algorithms that perform well on some distribution shifts fail on others, and no state-of-the-art DG algorithm performs consistently well on all shifts. Moreover, real-world data often has multiple distribution shifts over different attributes; hence we introduce multi-attribute distribution shift datasets and find that the accuracy of existing DG algorithms falls even further. To explain these results, we provide a formal characterization of generalization under multi-attribute shifts using a canonical causal graph. Based on the relationship between spurious attributes and the classification label, we obtain realizations of the canonical causal graph that characterize common distribution shifts and show that each shift entails different independence constraints over observed variables. As a result, we prove that any algorithm based on a single, fixed constraint cannot work well across all shifts, providing theoretical evidence for mixed empirical results on DG algorithms. Based on this insight, we develop Causally Adaptive Constraint Minimization (CACM), an algorithm that uses knowledge about the data-generating process to adaptively identify and apply the correct independence constraints for regularization. Results on fully synthetic, MNIST, small NORB, and Waterbirds datasets, covering binary and multi-valued attributes and labels, show that adaptive dataset-dependent constraints lead to the highest accuracy on unseen domains whereas incorrect constraints fail to do so. Our results demonstrate the importance of modeling the causal relationships inherent in the data-generating process",
    "checked": true,
    "id": "bf908f36468fc3869e7969b3ffdbd19b967debea",
    "semantic_title": "modeling the data-generating process is necessary for out-of-distribution generalization",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=eR2dG8yjnQ": {
    "title": "Using Language to Extend to Unseen Domains",
    "volume": "spotlight",
    "abstract": "It is expensive to collect training data for every possible domain that a vision model may encounter when deployed. We instead consider how simply $\\textit{verbalizing}$ the training domain (e.g.``photos of birds'') as well as domains we want to extend to but do not have data for (e.g.``paintings of birds'') can improve robustness. Using a multimodal model with a joint image and language embedding space, our method $\\textit{LADS}$ learns a transformation of the image embeddings from the source domain to each target domain, while preserving task relevant information. Without using any images from the target domain, we show that over the $\\textit{extended}$ domain containing both source and target, $\\textit{LADS}$ outperforms standard fine-tuning and ensemble approaches over a suite of 4 benchmarks targeting domain adaptation and dataset bias",
    "checked": true,
    "id": "762b4fc658bfd36115c3d693ed398c5094db759b",
    "semantic_title": "using language to extend to unseen domains",
    "citation_count": 37,
    "authors": []
  },
  "https://openreview.net/forum?id=eQzLwwGyQrb": {
    "title": "Can We Find Nash Equilibria at a Linear Rate in Markov Games?",
    "volume": "spotlight",
    "abstract": "We study decentralized learning in two-player zero-sum discounted Markov games where the goal is to design a policy optimization algorithm for either agent satisfying two properties. First, the player does not need to know the policy of the opponent to update its policy. Second, when both players adopt the algorithm, their joint policy converges to a Nash equilibrium of the game. To this end, we construct a meta-algorithm, dubbed as $\\texttt{Homotopy-PO}$, which provably finds a Nash equilibrium at a global linear rate. In particular, $\\texttt{Homotopy-PO}$ interweaves two base algorithms $\\texttt{Local-Fast}$ and $\\texttt{Global-Slow}$ via homotopy continuation. $\\texttt{Local-Fast}$ is an algorithm that enjoys local linear convergence while $\\texttt{Global-Slow}$ is an algorithm that converges globally but at a slower sublinear rate. By switching between these two base algorithms, $\\texttt{Global-Slow}$ essentially serves as a ``guide'' which identifies a benign neighborhood where $\\texttt{Local-Fast}$ enjoys fast convergence. However, since the exact size of such a neighborhood is unknown, we apply a doubling trick to switch between these two base algorithms. The switching scheme is delicately designed so that the aggregated performance of the algorithm is driven by $\\texttt{Local-Fast}$. Furthermore, we prove that $\\texttt{Local-Fast}$ and $\\texttt{Global-Slow}$ can both be instantiated by variants of optimistic gradient descent/ascent (OGDA) method, which is of independent interest",
    "checked": true,
    "id": "8428cff0746ac0b9ad8477f0ac057058f9b9c965",
    "semantic_title": "can we find nash equilibria at a linear rate in markov games?",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=8gd4M-_Rj1": {
    "title": "Hebbian Deep Learning Without Feedback",
    "volume": "spotlight",
    "abstract": "Recent approximations to backpropagation (BP) have mitigated many of BP's computational inefficiencies and incompatibilities with biology, but important limitations still remain. Moreover, the approximations significantly decrease accuracy in benchmarks, suggesting that an entirely different approach may be more fruitful. Here, grounded on recent theory for Hebbian learning in soft winner-take-all networks, we present multilayer SoftHebb, i.e. an algorithm that trains deep neural networks, without any feedback, target, or error signals. As a result, it achieves efficiency by avoiding weight transport, non-local plasticity, time-locking of layer updates, iterative equilibria, and (self-) supervisory or other feedback signals – which were necessary in other approaches. Its increased efficiency and biological compatibility do not trade off accuracy compared to state-of-the-art bio-plausible learning, but rather improve it. With up to five hidden layers and an added linear classifier, accuracies on MNIST, CIFAR-10, STL-10, and ImageNet, respectively reach 99.4%, 80.3%, 76.2%, and 27.3%. In conclusion, SoftHebb shows with a radically different approach from BP that Deep Learning over few layers may be plausible in the brain and increases the accuracy of bio-plausible machine learning. Code is available at https://github.com/NeuromorphicComputing/SoftHebb",
    "checked": true,
    "id": "dee626dcedf3686399294cb1ce707f1ecda89e29",
    "semantic_title": "hebbian deep learning without feedback",
    "citation_count": 56,
    "authors": []
  },
  "https://openreview.net/forum?id=kt-dcBQcSA": {
    "title": "A probabilistic framework for task-aligned intra- and inter-area neural manifold estimation",
    "volume": "spotlight",
    "abstract": "Latent manifolds provide a compact characterization of neural population activity and of shared co-variability across brain areas. Nonetheless, existing statistical tools for extracting neural manifolds face limitations in terms of interpretability of latents with respect to task variables, and can be hard to apply to datasets with no trial repeats. Here we propose a novel probabilistic framework that allows for interpretable partitioning of population variability within and across areas in the context of naturalistic behavior. Our approach for task aligned manifold estimation (TAME-GP) explicitly partitions variability into private and shared sources which can themselves be subdivided in task-relevant and task irrelevant components, uses a realistic Poisson noise model, and introduces temporal smoothing of latent trajectories in the form of a Gaussian Process prior. This TAME-GP graphical model allows for robust estimation of task-relevant variability in local population responses, and of shared co-variability between brain areas. We demonstrate the efficiency of our estimator on within model and biologically motivated simulated data. We also apply it to several datasets of neural population recordings during behavior. Overall, our results demonstrate the capacity of TAME-GP to capture meaningful intra- and inter-area neural variability with single trial resolution",
    "checked": true,
    "id": "e00d7f6a030d96830b3534b5c0ee1e6b827d52ab",
    "semantic_title": "a probabilistic framework for task-aligned intra- and inter-area neural manifold estimation",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=PvLnIaJbt9": {
    "title": "Metadata Archaeology: Unearthing Data Subsets by Leveraging Training Dynamics",
    "volume": "spotlight",
    "abstract": "Modern machine learning research relies on relatively few carefully curated datasets. Even in these datasets, and typically in `untidy' or raw data, practitioners are faced with significant issues of data quality and diversity which can be prohibitively labor intensive to address. Existing methods for dealing with these challenges tend to make strong assumptions about the particular issues at play, and often require a priori knowledge or metadata such as domain labels. Our work is orthogonal to these methods: we instead focus on providing a unified and efficient framework for Metadata Archaeology -- uncovering and inferring metadata of examples in a dataset. We curate different subsets of data that might exist in a dataset (e.g. mislabeled, atypical, or out-of-distribution examples) using simple transformations, and leverage differences in learning dynamics between these probe suites to infer metadata of interest. Our method is on par with far more sophisticated mitigation methods across different tasks: identifying and correcting mislabeled examples, classifying minority-group samples, prioritizing points relevant for training and enabling scalable human auditing of relevant examples",
    "checked": true,
    "id": "33fd110d1e4ca5f91d1b7ca7ff24ce1e9335359e",
    "semantic_title": "metadata archaeology: unearthing data subsets by leveraging training dynamics",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=gm0VZ-h-hPy": {
    "title": "Proposal-Contrastive Pretraining for Object Detection from Fewer Data",
    "volume": "spotlight",
    "abstract": "The use of pretrained deep neural networks represents an attractive way to achieve strong results with few data available. When specialized in dense problems such as object detection, learning local rather than global information in images has proven to be more efficient. However, for unsupervised pretraining, the popular contrastive learning requires a large batch size and, therefore, a lot of resources. To address this problem, we are interested in transformer-based object detectors that have recently gained traction in the community with good performance and with the particularity of generating many diverse object proposals. In this work, we present Proposal Selection Contrast (ProSeCo), a novel unsupervised overall pretraining approach that leverages this property. ProSeCo uses the large number of object proposals generated by the detector for contrastive learning, which allows the use of a smaller batch size, combined with object-level features to learn local information in the images. To improve the effectiveness of the contrastive loss, we introduce the object location information in the selection of positive examples to take into account multiple overlapping object proposals. When reusing pretrained backbone, we advocate for consistency in learning local information between the backbone and the detection head. We show that our method outperforms state of the art in unsupervised pretraining for object detection on standard and novel benchmarks in learning with fewer data",
    "checked": true,
    "id": "dbeeb2eb58a77a2175104e455186b739d6674948",
    "semantic_title": "proposal-contrastive pretraining for object detection from fewer data",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=HXz7Vcm3VgM": {
    "title": "ImageNet-X: Understanding Model Mistakes with Factor of Variation Annotations",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "32c1408d4b50f1ae5cb76fb3089372b0d4f6c708",
    "semantic_title": "imagenet-x: understanding model mistakes with factor of variation annotations",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=b7SBTEBFnC": {
    "title": "Canary in a Coalmine: Better Membership Inference with Ensembled Adversarial Queries",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "049e016d5241eb2eab9c7e015394aa7dc8bf2b40",
    "semantic_title": "canary in a coalmine: better membership inference with ensembled adversarial queries",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PhkWyijGi5b": {
    "title": "Choreographer: Learning and Adapting Skills in Imagination",
    "volume": "spotlight",
    "abstract": "Unsupervised skill learning aims to learn a rich repertoire of behaviors without external supervision, providing artificial agents with the ability to control and influence the environment. However, without appropriate knowledge and exploration, skills may provide control only over a restricted area of the environment, limiting their applicability. Furthermore, it is unclear how to leverage the learned skill behaviors for adapting to downstream tasks in a data-efficient manner. We present Choreographer, a model-based agent that exploits its world model to learn and adapt skills in imagination. Our method decouples the exploration and skill learning processes, being able to discover skills in the latent state space of the model. During adaptation, the agent uses a meta-controller to evaluate and adapt the learned skills efficiently by deploying them in parallel in imagination. Choreographer is able to learn skills both from offline data, and by collecting data simultaneously with an exploration policy. The skills can be used to effectively adapt to downstream tasks, as we show in the URL benchmark, where we outperform previous approaches from both pixels and states inputs. The skills also explore the environment thoroughly, finding sparse rewards more frequently, as shown in goal-reaching tasks from the DMC Suite and Meta-World. Project website: https://skillchoreographer.github.io/",
    "checked": true,
    "id": "cfd232ade1fdee8f00d90a0c1e6148b8ee530e29",
    "semantic_title": "choreographer: learning and adapting skills in imagination",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=sKc6fgce1zs": {
    "title": "Learning About Progress From Experts",
    "volume": "spotlight",
    "abstract": "Many important tasks involve some notion of long-term progress in multiple phases: e.g. to clean a shelf it must be cleared of items, cleaning products applied, and then the items placed back on the shelf. In this work, we explore the use of expert demonstrations in long-horizon tasks to learn a monotonically increasing function that summarizes progress. This function can then be used to aid agent exploration in environments with sparse rewards. As a case study we consider the NetHack environment, which requires long-term progress at a variety of scales and is far from being solved by existing approaches. In this environment, we demonstrate that by learning a model of long-term progress from expert data containing only observations, we can achieve efficient exploration in challenging sparse tasks, well beyond what is possible with current state-of-the-art approaches. We have made the curated gameplay dataset used in this work available at https://github.com/deepmind/nao_top10",
    "checked": true,
    "id": "10c045a0e1e9b68a3e8a6cc2adb66bec96659984",
    "semantic_title": "learning about progress from experts",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=1_OGWcP1s9w": {
    "title": "Learning Fair Graph Representations via Automated Data Augmentations",
    "volume": "spotlight",
    "abstract": "We consider fair graph representation learning via data augmentations. While this direction has been explored previously, existing methods invariably rely on certain assumptions on the properties of fair graph data in order to design fixed strategies on data augmentations. Nevertheless, the exact properties of fair graph data may vary significantly in different scenarios. Hence, heuristically designed augmentations may not always generate fair graph data in different application scenarios. In this work, we propose a method, known as Graphair, to learn fair representations based on automated graph data augmentations. Such fairness-aware augmentations are themselves learned from data. Our Graphair is designed to automatically discover fairness-aware augmentations from input graphs in order to circumvent sensitive information while preserving other useful information. Experimental results demonstrate that our Graphair consistently outperforms many baselines on multiple node classification datasets in terms of fairness-accuracy trade-off performance. In addition, results indicate that Graphair can automatically learn to generate fair graph data without prior knowledge on fairness-relevant graph properties",
    "checked": true,
    "id": "adcd41e985a168530921d5e871e01788fbcc25d1",
    "semantic_title": "learning fair graph representations via automated data augmentations",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=lTt4KjHSsyl": {
    "title": "Emergence of Maps in the Memories of Blind Navigation Agents",
    "volume": "spotlight",
    "abstract": "Animal navigation research posits that organisms build and maintain internal spa- tial representations, or maps, of their environment. We ask if machines – specifically, artificial intelligence (AI) navigation agents – also build implicit (or ‘mental') maps. A positive answer to this question would (a) explain the surprising phenomenon in recent literature of ostensibly map-free neural-networks achieving strong performance, and (b) strengthen the evidence of mapping as a fundamental mechanism for navigation by intelligent embodied agents, whether they be biological or artificial. Unlike animal navigation, we can judiciously design the agent's perceptual system and control the learning paradigm to nullify alternative navigation mechanisms. Specifically, we train ‘blind' agents – with sensing limited to only egomotion and no other sensing of any kind – to perform PointGoal navigation (‘go to $\\Delta$x, $\\Delta$y') via reinforcement learning. Our agents are composed of navigation-agnostic components (fully-connected and recurrent neural networks), and our experimental setup provides no inductive bias towards mapping. Despite these harsh conditions, we find that blind agents are (1) surprisingly effective navigators in new environments (∼95% success); (2) they utilize memory over long horizons (remembering ∼1,000 steps of past experience in an episode); (3) this memory enables them to exhibit intelligent behavior (following walls, detecting collisions, taking shortcuts); (4) there is emergence of maps and collision detection neurons in the representations of the environment built by a blind agent as it navigates; and (5) the emergent maps are selective and task dependent (e.g. the agent ‘forgets' exploratory detours). Overall, this paper presents no new techniques for the AI audience, but a surprising finding, an insight, and an explanation",
    "checked": true,
    "id": "e5774f164dbf5ad3642a4feee61cf9fad8a6bd36",
    "semantic_title": "emergence of maps in the memories of blind navigation agents",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=DjzBCrMBJ_p": {
    "title": "Spectral Augmentation for Self-Supervised Learning on Graphs",
    "volume": "spotlight",
    "abstract": "Graph contrastive learning (GCL), as an emerging self-supervised learning technique on graphs, aims to learn representations via instance discrimination. Its performance heavily relies on graph augmentation to reflect invariant patterns that are robust to small perturbations; yet it still remains unclear about what graph invariance GCL should capture. Recent studies mainly perform topology augmentations in a uniformly random manner in the spatial domain, ignoring its influence on the intrinsic structural properties embedded in the spectral domain. In this work, we aim to find a principled way for topology augmentations by exploring the invariance of graphs from the spectral perspective. We develop spectral augmentation which guides topology augmentations by maximizing the spectral change. Extensive experiments on both graph and node classification tasks demonstrate the effectiveness of our method in self-supervised representation learning. The proposed method also brings promising generalization capability in transfer learning, and is equipped with intriguing robustness property under adversarial attacks. Our study sheds light on a general principle for graph topology augmentation",
    "checked": true,
    "id": "4fac6a8d86261484983729f3c7d466677d4cf359",
    "semantic_title": "spectral augmentation for self-supervised learning on graphs",
    "citation_count": 52,
    "authors": []
  },
  "https://openreview.net/forum?id=WOquZTLCBO1": {
    "title": "VIPeR: Provably Efficient Algorithm for Offline RL with Neural Function Approximation",
    "volume": "spotlight",
    "abstract": "We propose a novel algorithm for offline reinforcement learning called Value Iteration with Perturbed Rewards (VIPeR), which amalgamates the pessimism principle with random perturbations of the value function. Most current offline RL algorithms explicitly construct statistical confidence regions to obtain pessimism via lower confidence bounds (LCB), which cannot easily scale to complex problems where a neural network is used to estimate the value functions. Instead, VIPeR implicitly obtains pessimism by simply perturbing the offline data multiple times with carefully-designed i.i.d. Gaussian noises to learn an ensemble of estimated state-action {value functions} and acting greedily with respect to the minimum of the ensemble. The estimated state-action values are obtained by fitting a parametric model (e.g., neural networks) to the perturbed datasets using gradient descent. As a result, VIPeR only needs $\\mathcal{O}(1)$ time complexity for action selection, while LCB-based algorithms require at least $\\Omega(K^2)$, where $K$ is the total number of trajectories in the offline data. We also propose a novel data-splitting technique that helps remove a factor involving the log of the covering number in our bound. We prove that VIPeR yields a provable uncertainty quantifier with overparameterized neural networks and enjoys a bound on sub-optimality of $\\tilde{\\mathcal{O}}( { \\kappa H^{5/2} \\tilde{d} }/{\\sqrt{K}})$, where $\\tilde{d}$ is the effective dimension, $H$ is the horizon length and $\\kappa$ measures the distributional shift. We corroborate the statistical and computational efficiency of VIPeR with an empirical evaluation on a wide set of synthetic and real-world datasets. To the best of our knowledge, VIPeR is the first algorithm for offline RL that is provably efficient for general Markov decision processes (MDPs) with neural network function approximation",
    "checked": true,
    "id": "7d200b868cb92657a68ac64c112a2cd0a4045f87",
    "semantic_title": "viper: provably efficient algorithm for offline rl with neural function approximation",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=8uu6JStuYm": {
    "title": "Self-supervised learning with rotation-invariant kernels",
    "volume": "spotlight",
    "abstract": "We introduce a regularization loss based on kernel mean embeddings with rotation-invariant kernels on the hypersphere (also known as dot-product kernels) for self-supervised learning of image representations. Besides being fully competitive with the state of the art, our method significantly reduces time and memory complexity for self-supervised training, making it implementable for very large embedding dimensions on existing devices and more easily adjustable than previous methods to settings with limited resources. Our work follows the major paradigm where the model learns to be invariant to some predefined image transformations (cropping, blurring, color jittering, etc.), while avoiding a degenerate solution by regularizing the embedding distribution. Our particular contribution is to propose a loss family promoting the embedding distribution to be close to the uniform distribution on the hypersphere, with respect to the maximum mean discrepancy pseudometric. We demonstrate that this family encompasses several regularizers of former methods, including uniformity-based and information-maximization methods, which are variants of our flexible regularization loss with different kernels. Beyond its practical consequences for state of the art self-supervised learning with limited resources, the proposed generic regularization approach opens perspectives to leverage more widely the literature on kernel methods in order to improve self-supervised learning methods",
    "checked": true,
    "id": "cde535c74312f61d71a7649afb474259c600433d",
    "semantic_title": "self-supervised learning with rotation-invariant kernels",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=QubsmJT_A0": {
    "title": "Neuromechanical Autoencoders: Learning to Couple Elastic and Neural Network Nonlinearity",
    "volume": "spotlight",
    "abstract": "Intelligent biological systems are characterized by their embodiment in a complex environment and the intimate interplay between their nervous systems and the nonlinear mechanical properties of their bodies. This coordination, in which the dynamics of the motor system co-evolved to reduce the computational burden on the brain, is referred to as \"mechanical intelligence\" or \"morphological computation\". In this work, we seek to develop machine learning analogs of this process, in which we jointly learn the morphology of complex nonlinear elastic solids along with a deep neural network to control it. By using a specialized differentiable simulator of elastic mechanics coupled to conventional deep learning architectures---which we refer to as neuromechanical autoencoders---we are able to learn to perform morphological computation via gradient descent. Key to our approach is the use of mechanical metamaterials---cellular solids, in particular---as the morphological substrate. Just as deep neural networks provide flexible and massively-parametric function approximators for perceptual and control tasks, cellular solid metamaterials are promising as a rich and learnable space for approximating a variety of actuation tasks. In this work we take advantage of these complementary computational concepts to co-design materials and neural network controls to achieve nonintuitive mechanical behavior. We demonstrate in simulation how it is possible to achieve translation, rotation, and shape matching, as well as a \"digital MNIST\" task. We additionally manufacture and evaluate one of the designs to verify its real-world behavior",
    "checked": true,
    "id": "7d882f565384d54043f3f2f61ca4530f14ff1018",
    "semantic_title": "neuromechanical autoencoders: learning to couple elastic and neural network nonlinearity",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=YJ7o2wetJ2": {
    "title": "VIP: Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training",
    "volume": "spotlight",
    "abstract": "Reward and representation learning are two long-standing challenges for learning an expanding set of robot manipulation skills from sensory observations. Given the inherent cost and scarcity of in-domain, task-specific robot data, learning from large, diverse, offline human videos has emerged as a promising path towards acquiring a generally useful visual representation for control; however, how these human videos can be used for general-purpose reward learning remains an open question. We introduce $\\textbf{V}$alue-$\\textbf{I}$mplicit $\\textbf{P}$re-training (VIP), a self-supervised pre-trained visual representation capable of generating dense and smooth reward functions for unseen robotic tasks. VIP casts representation learning from human videos as an offline goal-conditioned reinforcement learning problem and derives a self-supervised dual goal-conditioned value-function objective that does not depend on actions, enabling pre-training on unlabeled human videos. Theoretically, VIP can be understood as a novel implicit time contrastive objective that generates a temporally smooth embedding, enabling the value function to be implicitly defined via the embedding distance, which can then be used to construct the reward for any goal-image specified downstream task. Trained on large-scale Ego4D human videos and without any fine-tuning on in-domain, task-specific data, VIP can provide dense visual reward for an extensive set of simulated and $\\textbf{real-robot}$ tasks, enabling diverse reward-based visual control methods and significantly outperforming all prior pre-trained representations. Notably, VIP can enable simple, few-shot offline RL on a suite of real-world robot tasks with as few as 20 trajectories",
    "checked": true,
    "id": "3fbe2e8413df0207c26ff393c9aaa8488e3ca4c3",
    "semantic_title": "vip: towards universal visual reward and representation via value-implicit pre-training",
    "citation_count": 316,
    "authors": []
  },
  "https://openreview.net/forum?id=74A-FDAyiL": {
    "title": "Subquadratic Algorithms for Kernel Matrices via Kernel Density Estimation",
    "volume": "spotlight",
    "abstract": "Kernel matrices, as well as weighted graphs represented by them, are ubiquitous objects in machine learning, statistics and other related fields. The main drawback of using kernel methods (learning and inference using kernel matrices) is efficiency -- given $n$ input points, most kernel-based algorithms need to materialize the full $n \\times n$ kernel matrix before performing any subsequent computation, thus incurring $\\Omega(n^2)$ runtime. Breaking this quadratic barrier for various problems has therefore, been a subject of extensive research efforts. We break the quadratic barrier and obtain \\emph{subquadratic} time algorithms for several fundamental linear-algebraic and graph processing primitives, including approximating the top eigenvalue and eigenvector, spectral sparsification, solving linear systems, local clustering, low-rank approximation, arboricity estimation and counting weighted triangles. We build on the recently developed Kernel Density Estimation framework, which (after preprocessing in time subquadratic in $n$) can return estimates of row/column sums of the kernel matrix. In particular, we develop efficient reductions from \\emph{weighted vertex} and \\emph{weighted edge sampling} on kernel graphs, \\emph{simulating random walks} on kernel graphs, and \\emph{importance sampling} on matrices to Kernel Density Estimation and show that we can generate samples from these distributions in \\emph{sublinear} (in the support of the distribution) time. Our reductions are the central ingredient in each of our applications and we believe they may be of independent interest. We empirically demonstrate the efficacy of our algorithms on low-rank approximation (LRA) and spectral sparsification, where we observe a $\\textbf{9x}$ decrease in the number of kernel evaluations over baselines for LRA and a $\\textbf{41x}$ reduction in the graph size for spectral sparsification",
    "checked": false,
    "id": "c7ce06fad686f60649f0ebe38702ad16ab3353bb",
    "semantic_title": "sub-quadratic algorithms for kernel matrices via kernel density estimation",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=aMXD8gqsIiC": {
    "title": "A Higher Precision Algorithm for Computing the $1$-Wasserstein Distance",
    "volume": "spotlight",
    "abstract": "We consider the problem of computing the $1$-Wasserstein distance $\\mathcal{W}(\\mu,\\nu)$ between two $d$-dimensional discrete distributions $\\mu$ and $\\nu$ whose support lie within the unit hypercube. There are several algorithms that estimate $\\mathcal{W}(\\mu,\\nu)$ within an additive error of $\\varepsilon$. However, when $\\mathcal{W}(\\mu,\\nu)$ is small, the additive error $\\varepsilon$ dominates, leading to noisy results. Consider any additive approximation algorithm with execution time $T(n,\\varepsilon)$. We propose an algorithm that runs in $O(T(n,\\varepsilon/d) \\log n)$ time and boosts the accuracy of estimating $\\mathcal{W}(\\mu,\\nu)$ from $\\varepsilon$ to an expected additive error of $\\min\\{\\varepsilon, (d\\log_{\\sqrt{d}/\\varepsilon} n)\\mathcal{W}(\\mu,\\nu)\\}$. For the special case where every point in the support of $\\mu$ and $\\nu$ has a mass of $1/n$ (also called the Euclidean Bipartite Matching problem), we describe an algorithm to boost the accuracy of any additive approximation algorithm from $\\varepsilon$ to an expected additive error of $\\min\\{\\varepsilon, (d\\log\\log n)\\mathcal{W}(\\mu,\\nu)\\}$ in $O(T(n, \\varepsilon/d)\\log\\log n)$ time",
    "checked": true,
    "id": "495d7bae37257bf844e6b3e52de6241d89b02a3f",
    "semantic_title": "a higher precision algorithm for computing the $1$-wasserstein distance",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=HPdxC1THU8T": {
    "title": "Revisiting adapters with adversarial training",
    "volume": "spotlight",
    "abstract": "While adversarial training is generally used as a defense mechanism, recent works show that it can also act as a regularizer. By co-training a neural network on clean and adversarial inputs, it is possible to improve classification accuracy on the clean, non-adversarial inputs. We demonstrate that, contrary to previous findings, it is not necessary to separate batch statistics when co-training on clean and adversarial inputs, and that it is sufficient to use adapters with few domain-specific parameters for each type of input. We establish that using the classification token of a Vision Transformer (ViT) as an adapter is enough to match the classification performance of dual normalization layers, while using significantly less additional parameters. First, we improve upon the top-1 accuracy of a non-adversarially trained ViT-B16 model by +1.12% on ImageNet (reaching 83.76% top-1 accuracy). Second, and more importantly, we show that training with adapters enables model soups through linear combinations of the clean and adversarial tokens. These model soups, which we call adversarial model soups, allow us to trade-off between clean and robust accuracy without sacrificing efficiency. Finally, we show that we can easily adapt the resulting models in the face of distribution shifts. Our ViT-B16 obtains top-1 accuracies on ImageNet variants that are on average +4.00% better than those obtained with Masked Autoencoders",
    "checked": true,
    "id": "7bbd4e4d8405ffce1c58d1a33a8ae5b69e755529",
    "semantic_title": "revisiting adapters with adversarial training",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=Mj7K4lglGyj": {
    "title": "UNICORN: A Unified Backdoor Trigger Inversion Framework",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "984db0dc4dea4a6bd2cd48f5b497027492266761",
    "semantic_title": "unicorn: a unified backdoor trigger inversion framework",
    "citation_count": 49,
    "authors": []
  },
  "https://openreview.net/forum?id=xkev3_np08z": {
    "title": "ExpressivE: A Spatio-Functional Embedding For Knowledge Graph Completion",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "ee5ceab9fa5f3bad231469923a03ad16184b51b9",
    "semantic_title": "expressive: a spatio-functional embedding for knowledge graph completion",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=-k7Lvk0GpBl": {
    "title": "Localized Randomized Smoothing for Collective Robustness Certification",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "7ee017cf2cbd83fe23f94280b0931bbf5416b1f0",
    "semantic_title": "localized randomized smoothing for collective robustness certification",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=cXMHQD-xQas": {
    "title": "Learning Probabilistic Topological Representations Using Discrete Morse Theory",
    "volume": "spotlight",
    "abstract": "Accurate delineation of fine-scale structures is a very important yet challenging problem. Existing methods use topological information as an additional training loss, but are ultimately making pixel-wise predictions. In this paper, we propose a novel deep learning based method to learn topological/structural. We use discrete Morse theory and persistent homology to construct a one-parameter family of structures as the topological/structural representation space. Furthermore, we learn a probabilistic model that can perform inference tasks in such a topological/structural representation space. Our method generates true structures rather than pixel-maps, leading to better topological integrity in automatic segmentation tasks. It also facilitates semi-automatic interactive annotation/proofreading via the sampling of structures and structure-aware uncertainty",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Vk-34OQ7rFo": {
    "title": "Model-based Causal Bayesian Optimization",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "b8dc2c6f9ca7c0d55b0379d5067f67b3e834ee80",
    "semantic_title": "model-based causal bayesian optimization",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=KzkLAE49H9b": {
    "title": "Training language models to summarize narratives improves brain alignment",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "357569763e23ebeab6ca1da8f33cde493e05dbc0",
    "semantic_title": "training language models to summarize narratives improves brain alignment",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=hhvkdRdWt1F": {
    "title": "Dual Algorithmic Reasoning",
    "volume": "spotlight",
    "abstract": "Neural Algorithmic Reasoning is an emerging area of machine learning which seeks to infuse algorithmic computation in neural networks, typically by training neural models to approximate steps of classical algorithms. In this context, much of the current work has focused on learning reachability and shortest path graph algorithms, showing that joint learning on similar algorithms is beneficial for generalisation. However, when targeting more complex problems, such \"similar\" algorithms become more difficult to find. Here, we propose to learn algorithms by exploiting duality of the underlying algorithmic problem. Many algorithms solve optimisation problems. We demonstrate that simultaneously learning the dual definition of these optimisation problems in algorithmic learning allows for better learning and qualitatively better solutions. Specifically, we exploit the max-flow min-cut theorem to simultaneously learn these two algorithms over synthetically generated graphs, demonstrating the effectiveness of the proposed approach. We then validate the real-world utility of our dual algorithmic reasoner by deploying it on a challenging brain vessel classification task, which likely depends on the vessels' flow properties. We demonstrate a clear performance gain when using our model within such a context, and empirically show that learning the max-flow and min-cut algorithms together is critical for achieving such a result",
    "checked": true,
    "id": "d996bfdcb5664c370293bd965080ce5178d59cc3",
    "semantic_title": "dual algorithmic reasoning",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=U_T8-5hClV": {
    "title": "A Primal-Dual Framework for Transformers and Neural Networks",
    "volume": "spotlight",
    "abstract": "Self-attention is key to the remarkable success of transformers in sequence modeling tasks including many applications in natural language processing and computer vision. Like neural network layers, these attention mechanisms are often developed by heuristics and experience. To provide a principled framework for constructing attention layers in transformers, we show that the self-attention corresponds to the support vector expansion derived from a support vector regression problem, whose primal formulation has the form of a neural network layer. Using our framework, we derive popular attention layers used in practice and propose two new attentions: 1) the Batch Normalized Attention (Attention-BN) derived from the batch normalization layer and 2) the Attention with Scaled Head (Attention-SH) derived from using less training data to fit the SVR model. We empirically demonstrate the advantages of the Attention-BN and Attention-SH in reducing head redundancy, increasing the model's accuracy, and improving the model's efficiency in a variety of practical applications including image and time-series classification",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c9lAOPvQHS": {
    "title": "Fisher-Legendre (FishLeg) optimization of deep neural networks",
    "volume": "spotlight",
    "abstract": "Incorporating second-order gradient information (curvature) into optimization can dramatically reduce the number of iterations required to train machine learning models. In natural gradient descent, such information comes from the Fisher information matrix which yields a number of desirable properties. As exact natural gradient updates are intractable for large models, successful methods such as KFAC and sequels approximate the Fisher in a structured form that can easily be inverted. However, this requires model/layer-specific tensor algebra and certain approximations that are often difficult to justify. Here, we use ideas from Legendre-Fenchel duality to learn a direct and efficiently evaluated model for the product of the inverse Fisher with any vector, in an online manner, leading to natural gradient steps that get progressively more accurate over time despite noisy gradients. We prove that the resulting \"Fisher-Legendre\" (FishLeg) optimizer converges to a (global) minimum of non-convex functions satisfying the PL condition, which applies in particular to deep linear networks. On standard auto-encoder benchmarks, we show empirically that FishLeg outperforms standard first-order optimization methods, and performs on par with or better than other second-order methods, especially when using small batches. Thanks to its generality, we expect our approach to facilitate the handling of a variety neural network layers in future work",
    "checked": true,
    "id": "522d806be48fba2be02eeb11b7fbd9d0a88ab3b2",
    "semantic_title": "fisher-legendre (fishleg) optimization of deep neural networks",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=0Vv4H4Ch0la": {
    "title": "Capturing the Motion of Every Joint: 3D Human Pose and Shape Estimation with Independent Tokens",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "79139657e26e4dca31405b36ac79ba69823c836a",
    "semantic_title": "capturing the motion of every joint: 3d human pose and shape estimation with independent tokens",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=lJdOlWg8td": {
    "title": "Efficient recurrent architectures through activity sparsity and sparse back-propagation through time",
    "volume": "spotlight",
    "abstract": "Recurrent neural networks (RNNs) are well suited for solving sequence tasks in resource-constrained systems due to their expressivity and low computational requirements. However, there is still a need to bridge the gap between what RNNs are capable of in terms of efficiency and performance and real-world application requirements. The memory and computational requirements arising from propagating the activations of all the neurons at every time step to every connected neuron, together with the sequential dependence of activations, contribute to the inefficiency of training and using RNNs. We propose a solution inspired by biological neuron dynamics that makes the communication between RNN units sparse and discrete. This makes the backward pass with backpropagation through time (BPTT) computationally sparse and efficient as well. We base our model on the gated recurrent unit (GRU), extending it with units that emit discrete events for communication triggered by a threshold so that no information is communicated to other units in the absence of events. We show theoretically that the communication between units, and hence the computation required for both the forward and backward passes, scales with the number of events in the network. Our model achieves efficiency without compromising task performance, demonstrating competitive performance compared to state-of-the-art recurrent network models in real-world tasks, including language modeling. The dynamic activity sparsity mechanism also makes our model well suited for novel energy-efficient neuromorphic hardware. Code is available at https://github.com/KhaleelKhan/EvNN/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XVjTT1nw5z": {
    "title": "Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow",
    "volume": "spotlight",
    "abstract": "We present rectified flow, a simple approach to learning (neural) ordinary differential equation (ODE) models to transport between two empirically observed distributions $\\pi_0$ and $\\pi_1$, hence providing a unified solution to generative modeling and domain transfer, among various other tasks involving distribution transport. The idea of rectified flow is to learn the ODE to follow the straight paths connecting the points drawn from $\\pi_0$ and $\\pi_1$ as much as possible. This is achieved by solving a straightforward nonlinear least squares optimization problem, which can be easily scaled to large models without introducing extra parameters beyond standard supervised learning. The straight paths are the shortest paths between two points, and can be simulated exactly without time discretization and hence yield computationally efficient models. We show that, by learning a rectified flow from data, we effectively turn an arbitrary coupling of $\\pi_0$ and $\\pi_1$ to a new deterministic coupling with provably non-increasing convex transport costs. In addition, with a ``reflow\" procedure that iteratively learns a new rectified flow from the data bootstrapped from the previous one, we obtain a sequence of flows with increasingly straight paths, which can be simulated accurately with coarse time discretization in the inference phase. In empirical studies, we show that rectified flow performs superbly on image generation, image-to-image translation, and domain adaptation. In particular, on image generation and translation, our method yields nearly straight flows that give high quality results even with \\emph{a single Euler discretization step}. Code is available at \\url{https://github.com/gnobitab/RectifiedFlow}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4t9q35BxGr": {
    "title": "Inequality phenomenon in $l_{\\infty}$-adversarial training, and its unrealized threats",
    "volume": "spotlight",
    "abstract": "The appearance of adversarial examples raises attention from both academia and industry. Along with the attack-defense arms race, adversarial training is the most effective against adversarial examples. However, we find inequality phenomena occur during the $l_{\\infty}$-adversarial training, that few features dominate the prediction made by the adversarially trained model. We systematically evaluate such inequality phenomena by extensive experiments and find such phenomena become more obvious when performing adversarial training with increasing adversarial strength (evaluated by $\\epsilon$). We hypothesize such inequality phenomena make $l_{\\infty}$-adversarially trained model less reliable than the standard trained model when few ``important features\" are influenced. To validate our hypothesis, we proposed two simple attacks that either perturb or replace important features with noise or occlusion. Experiments show that $l_{\\infty}$-adversarially trained model can be easily attacked when the few important features are influenced. Our work shed light on the limitation of the practicality of $l_{\\infty}$-adversarial training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WH1yCa0TbB": {
    "title": "Learning Diffusion Bridges on Constrained Domains",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "13fd50a1d3455501703176c7aec614886a2dfb42",
    "semantic_title": "learning diffusion bridges on constrained domains",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=1_jFneF07YC": {
    "title": "Unsupervised Semantic Segmentation with Self-supervised Object-centric Representations",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "c399b8d44dac36982b0d0b2b037c74740fa3dca7",
    "semantic_title": "unsupervised semantic segmentation with self-supervised object-centric representations",
    "citation_count": 54,
    "authors": []
  },
  "https://openreview.net/forum?id=f0a_dWEYg-Td": {
    "title": "Indiscriminate Poisoning Attacks on Unsupervised Contrastive Learning",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "491eb37da128ef8c266636acbc7e1ba780e5c34f",
    "semantic_title": "indiscriminate poisoning attacks on unsupervised contrastive learning",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=lKOfilXucGB": {
    "title": "Decompositional Generation Process for Instance-Dependent Partial Label Learning",
    "volume": "spotlight",
    "abstract": "Partial label learning (PLL) is a typical weakly supervised learning problem, where each training example is associated with a set of candidate labels among which only one is true. Most existing PLL approaches assume that the incorrect labels in each training example are randomly picked as the candidate labels and model the generation process of the candidate labels in a simple way. However, these approaches usually do not perform as well as expected due to the fact that the generation process of the candidate labels is always instance-dependent. Therefore, it deserves to be modeled in a refined way. In this paper, we consider instance-dependent PLL and assume that the generation process of the candidate labels could decompose into two sequential parts, where the correct label emerges first in the mind of the annotator but then the incorrect labels related to the feature are also selected with the correct label as candidate labels due to uncertainty of labeling. Motivated by this consideration, we propose a novel PLL method that performs Maximum A Posterior(MAP) based on an explicitly modeled generation process of candidate labels via decomposed probability distribution models. Extensive experiments on manually corrupted benchmark datasets and real-world datasets validate the effectiveness of the proposed method",
    "checked": false,
    "id": "bbda9fb21942552d84d4d8ac70ee2236693bfc7a",
    "semantic_title": "decomposition-based generation process for instance-dependent partial label learning",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=UKr0MwZM6fL": {
    "title": "Building a Subspace of Policies for Scalable Continual Learning",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "bdcd8dd1c2051f063b651e3e94b47596c9827a3b",
    "semantic_title": "building a subspace of policies for scalable continual learning",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=KGV-GBh8fb": {
    "title": "Not All Tasks Are Born Equal: Understanding Zero-Shot Generalization",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "ab5f4f8ed92549a1765c89d1c467ab8d8b1992be",
    "semantic_title": "not all tasks are born equal: understanding zero-shot generalization",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=RQY2AXFMRiu": {
    "title": "Solving Constrained Variational Inequalities via a First-order Interior Point-based Method",
    "volume": "spotlight",
    "abstract": "We develop an interior-point approach to solve constrained variational inequality (cVI) problems. Inspired by the efficacy of the alternating direction method of multipliers (ADMM) method in the single-objective context, we generalize ADMM to derive a first-order method for cVIs, that we refer to as ADMM-based interior-point method for constrained VIs (ACVI). We provide convergence guarantees for ACVI in two general classes of problems: (i) when the operator is $\\xi$-monotone, and (ii) when it is monotone, some constraints are active and the game is not purely rotational. When the operator is in addition L-Lipschitz for the latter case, we match known lower bounds on rates for the gap function of $\\mathcal{O}(1/\\sqrt{K})$ and $\\mathcal{O}(1/K)$ for the last and average iterate, respectively. To the best of our knowledge, this is the first presentation of a first-order interior-point method for the general cVI problem that has a global convergence guarantee. Moreover, unlike previous work in this setting, ACVI provides a means to solve cVIs when the constraints are nontrivial. Empirical analyses demonstrate clear advantages of ACVI over common first-order methods. In particular, (i) cyclical behavior is notably reduced as our methods approach the solution from the analytic center, and (ii) unlike projection-based methods that zigzag when near a constraint, ACVI efficiently handles the constraints",
    "checked": true,
    "id": "b25fa22af7c3570342bd7ae30f114cdeee4fd0d2",
    "semantic_title": "solving constrained variational inequalities via a first-order interior point-based method",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=K96AogLDT2K": {
    "title": "Symmetric Pruning in Quantum Neural Networks",
    "volume": "spotlight",
    "abstract": "Many fundamental properties of a quantum system are captured by its Hamiltonian and ground state. Despite the significance, ground states preparation (GSP) is classically intractable for large-scale Hamiltonians. Quantum neural networks (QNNs), which exert the power of modern quantum machines, have emerged as a leading protocol to conquer this issue. As such, the performance enhancement of QNNs becomes the core in GSP. Empirical evidence showed that QNNs with handcraft symmetric ans\\\"atze generally experience better trainability than those with asymmetric ans\\\"atze, while theoretical explanations remain vague. To fill this knowledge gap, here we propose the effective quantum neural tangent kernel (EQNTK) and connect this concept with over-parameterization theory to quantify the convergence of QNNs towards the global optima. We uncover that the advance of symmetric ans\\\"atze attributes to their large EQNTK value with low effective dimension, which requests few parameters and quantum circuit depth to reach the over-parameterization regime permitting a benign loss landscape and fast convergence. Guided by EQNTK, we further devise a symmetric pruning (SP) scheme to automatically tailor a symmetric ansatz from an over-parameterized and asymmetric one to greatly improve the performance of QNNs when the explicit symmetry information of Hamiltonian is unavailable. Extensive numerical simulations are conducted to validate the analytical results of EQNTK and the effectiveness of SP",
    "checked": true,
    "id": "ff1e394c63e4487dff4e3a3b9d94b0dba7d82792",
    "semantic_title": "symmetric pruning in quantum neural networks",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=vuD2xEtxZcj": {
    "title": "Minimum Variance Unbiased N:M Sparsity for the Neural Gradients",
    "volume": "spotlight",
    "abstract": "",
    "checked": false,
    "id": "b96fc157c6035978188ac8bb24c469fcd24ab5d5",
    "semantic_title": "minimum variance unbiased n: m sparsity for the neural gradients",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=a2jNdqE2102": {
    "title": "Knowledge-in-Context: Towards Knowledgeable Semi-Parametric Language Models",
    "volume": "spotlight",
    "abstract": "Fully-parametric language models generally require a huge number of model parameters to store the necessary knowledge for solving multiple natural language tasks in zero/few-shot settings. In addition, it is hard to adapt to the evolving world knowledge without the costly model re-training. In this paper, we develop a novel semi-parametric language model architecture, Knowledge-in-Context (KiC), which empowers a parametric text-to-text language model with a knowledge-rich external memory. Specifically, the external memory contains six different types of knowledge: entity, dictionary, commonsense, event, script, and causality knowledge. For each input instance, the KiC model adaptively selects a knowledge type and retrieves the most helpful pieces of knowledge. The input instance along with its knowledge augmentation is fed into a text-to-text model (e.g., T5) to generate the output answer, where both the input and the output are in natural language forms after prompting. Interestingly, we find that KiC can be identified as a special mixture-of-experts (MoE) model, where the knowledge selector plays the role of a router that is used to determine the sequence-to-expert assignment in MoE. This key observation inspires us to develop a novel algorithm for training KiC with an instance-adaptive knowledge selector. As a knowledge-rich semi-parametric language model, KiC only needs a much smaller parametric part to achieve superior zero-shot performance on unseen tasks. By evaluating on 40+ different tasks, we show that KiC-Large with 770M parameters easily outperforms large language models that are 4-39x larger. In addition, KiC also exhibits emergent abilities at a much smaller model scale compared to the fully-parametric models",
    "checked": true,
    "id": "7ffb3a27a2a4da5c35472bd3a3a4dee8d40a6d86",
    "semantic_title": "knowledge-in-context: towards knowledgeable semi-parametric language models",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=JAezPMehaUu": {
    "title": "Mosaic Representation Learning for Self-supervised Visual Pre-training",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "a31c4426423e8e61bf53a196c8c0e851b0a2c3b4",
    "semantic_title": "mosaic representation learning for self-supervised visual pre-training",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=Cp-io_BoFaE": {
    "title": "FluidLab: A Differentiable Environment for Benchmarking Complex Fluid Manipulation",
    "volume": "spotlight",
    "abstract": "Humans manipulate various kinds of fluids in their everyday life: creating latte art, scooping floating objects from water, rolling an ice cream cone, etc. Using robots to augment or replace human labors in these daily settings remain as a challenging task due to the multifaceted complexities of fluids. Previous research in robotic fluid manipulation mostly consider fluids governed by an ideal, Newtonian model in simple task settings (e.g., pouring water into a container). However, the vast majority of real-world fluid systems manifest their complexities in terms of the fluid's complex material behaviors (e.g., elastoplastic deformation) and multi-component interactions (e.g. coffee and frothed milk when making latte art), both of which were well beyond the scope of the current literature. To evaluate robot learning algorithms on understanding and interacting with such complex fluid systems, a comprehensive virtual platform with versatile simulation capabilities and well-established tasks is needed. In this work, we introduce FluidLab, a simulation environment with a diverse set of manipulation tasks involving complex fluid dynamics. These tasks address interactions between solid and fluid as well as among multiple fluids. At the heart of our platform is a fully differentiable physics simulator, FluidEngine, providing GPU-accelerated simulations and gradient calculations for various material types and their couplings, extending the scope of the existing differentiable simulation engines. We identify several challenges for fluid manipulation learning by evaluating a set of reinforcement learning and trajectory optimization methods on our platform. To address these challenges, we propose several domain-specific optimization schemes coupled with differentiable physics, which are empirically shown to be effective in tackling optimization problems featured by fluid system's non-convex and non-smooth properties. Furthermore, we demonstrate reasonable sim-to-real transfer by deploying optimized trajectories in real-world settings. FluidLab is publicly available at: https://fluidlab2023.github.io",
    "checked": true,
    "id": "73003cb9f5c1b30cd495f31f70323b1b17b24ff3",
    "semantic_title": "fluidlab: a differentiable environment for benchmarking complex fluid manipulation",
    "citation_count": 49,
    "authors": []
  },
  "https://openreview.net/forum?id=PqvMRDCJT9t": {
    "title": "Flow Matching for Generative Modeling",
    "volume": "spotlight",
    "abstract": "We introduce a new paradigm for generative modeling built on Continuous Normalizing Flows (CNFs), allowing us to train CNFs at unprecedented scale. Specifically, we present the notion of Flow Matching (FM), a simulation-free approach for training CNFs based on regressing vector fields of fixed conditional probability paths. Flow Matching is compatible with a general family of Gaussian probability paths for transforming between noise and data samples---which subsumes existing diffusion paths as specific instances. Interestingly, we find that employing FM with diffusion paths results in a more robust and stable alternative for training diffusion models. Furthermore, Flow Matching opens the door to training CNFs with other, non-diffusion probability paths. An instance of particular interest is using Optimal Transport (OT) displacement interpolation to define the conditional probability paths. These paths are more efficient than diffusion paths, provide faster training and sampling, and result in better generalization. Training CNFs using Flow Matching on ImageNet leads to consistently better performance than alternative diffusion-based methods in terms of both likelihood and sample quality, and allows fast and reliable sample generation using off-the-shelf numerical ODE solvers",
    "checked": true,
    "id": "af68f10ab5078bfc519caae377c90ee6d9c504e9",
    "semantic_title": "flow matching for generative modeling",
    "citation_count": 1674,
    "authors": []
  },
  "https://openreview.net/forum?id=tVkrbkz42vc": {
    "title": "PAC-NeRF: Physics Augmented Continuum Neural Radiance Fields for Geometry-Agnostic System Identification",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "fa430898844074e44f7a855565378f8bd203ea8d",
    "semantic_title": "pac-nerf: physics augmented continuum neural radiance fields for geometry-agnostic system identification",
    "citation_count": 97,
    "authors": []
  },
  "https://openreview.net/forum?id=iPWiwWHc1V": {
    "title": "CLIP-Dissect: Automatic Description of Neuron Representations in Deep Vision Networks",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "57b064db445c8c1e6b08bc7a8499e6f2c8b67dbc",
    "semantic_title": "clip-dissect: automatic description of neuron representations in deep vision networks",
    "citation_count": 95,
    "authors": []
  },
  "https://openreview.net/forum?id=27uBgHuoSQ": {
    "title": "Data Continuity Matters: Improving Sequence Modeling with Lipschitz Regularizer",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "4df628b6811ee63340ada6f687124941069f4881",
    "semantic_title": "data continuity matters: improving sequence modeling with lipschitz regularizer",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=iaYcJKpY2B_": {
    "title": "CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "38115e80d805fb0fb8f090dc88ced4b24be07878",
    "semantic_title": "codegen: an open large language model for code with multi-turn program synthesis",
    "citation_count": 1103,
    "authors": []
  },
  "https://openreview.net/forum?id=xYlJRpzZtsY": {
    "title": "ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "fe4e77394faf9bcc9d2b8ef16b42a75b2e2dd389",
    "semantic_title": "roscoe: a suite of metrics for scoring step-by-step reasoning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WUWJIV2Yxtp": {
    "title": "Re-calibrating Feature Attributions for Model Interpretation",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "723022abbc57e8dd7c6749610b16f1e2e2ab02e4",
    "semantic_title": "re-calibrating feature attributions for model interpretation",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=uLE3WF3-H_5": {
    "title": "Adversarial Diversity in Hanabi",
    "volume": "spotlight",
    "abstract": "Many Dec-POMDPs admit a qualitatively diverse set of ''reasonable'' joint policies, where reasonableness is indicated by symmetry equivariance, non-sabotaging behaviour and the graceful degradation of performance when paired with ad-hoc partners. Some of the work in diversity literature is concerned with generating these policies. Unfortunately, existing methods fail to produce teams of agents that are simultaneously diverse, high performing, and reasonable. In this work, we propose a novel approach, adversarial diversity (ADVERSITY), which is designed for turn-based Dec-POMDPs with public actions. ADVERSITY relies on off-belief learning to encourage reasonableness and skill, and on ''repulsive'' fictitious transitions to encourage diversity. We use this approach to generate new agents with distinct but reasonable play styles for the card game Hanabi and open-source our agents to be used for future research on (ad-hoc) coordination",
    "checked": true,
    "id": "09f6d1afea9ff7879725cd814cd027c55e56dec3",
    "semantic_title": "adversarial diversity in hanabi",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=ZsvWb6mJnMv": {
    "title": "Optimal Conservative Offline RL with General Function Approximation via Augmented Lagrangian",
    "volume": "spotlight",
    "abstract": "Offline reinforcement learning (RL), which aims at learning good policies from historical data, has received significant attention over the past years. Much effort has focused on improving offline RL practicality by addressing the prevalent issue of partial data coverage through various forms of conservative policy learning. While the majority of algorithms do not have finite- sample guarantees, several provable conservative offline RL algorithms are designed and analyzed within the single-policy concentrability framework that handles partial coverage. Yet, in the nonlinear function approximation setting where confidence intervals are difficult to obtain, existing provable algorithms suffer from computational intractability, prohibitively strong assumptions, and suboptimal statistical rates. In this paper, we leverage the marginalized importance sampling (MIS) formulation of RL and present the first set of offline RL algorithms that are statistically optimal and practical under general function approximation and single-policy concentrability, bypassing the need for uncertainty quantification. We identify that the key to successfully solving the sample-based approximation of the MIS problem is ensuring that certain occupancy validity constraints are nearly satisfied. We enforce these constraints by a novel application of the augmented Lagrangian method and prove the following result: with the MIS formulation, augmented Lagrangian is enough for statistically optimal offline RL. In stark contrast to prior algorithms that induce additional conservatism through methods such as behavior regularization, our approach provably eliminates this need and reinterprets regularizers as \"enforcers of occupancy validity\" than \"promoters of conservatism",
    "checked": true,
    "id": "63888b247d2e42c9b9a64e15bbf4a89877fd797f",
    "semantic_title": "optimal conservative offline rl with general function approximation via augmented lagrangian",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=ZTCxT2t2Ru": {
    "title": "DocPrompting: Generating Code by Retrieving the Docs",
    "volume": "spotlight",
    "abstract": "Publicly available source-code libraries are continuously growing and changing. This makes it impossible for models of code to keep current with all available APIs by simply training these models on existing code repositories. Thus, existing models inherently cannot generalize to using unseen functions and libraries, because these would never appear in the training data. In contrast, when human programmers use functions and libraries for the first time, they frequently refer to textual resources such as code manuals and documentation, to explore and understand the available functionality. Inspired by this observation, we introduce DocPrompting: a natural-language-to-code generation approach that explicitly leverages documentation by (1) retrieving the relevant documentation pieces given an NL intent, and (2) generating code based on the NL intent and the retrieved documentation. DocPrompting is general: it can be applied to any programming language and is agnostic to the underlying neural model. We demonstrate that DocPrompting consistently improves NL-to-code models: DocPrompting improves strong base models such as CodeT5 by 2.85% in pass@1 (52% relative gain) and 4.39% in pass@10 (30% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark; on a new Bash dataset tldr, DocPrompting improves CodeT5 and GPT-Neo1.3B by up to absolute 6.9% exact match",
    "checked": true,
    "id": "0a39442979d6e678dd36bb443ad529c14e86a86e",
    "semantic_title": "docprompting: generating code by retrieving the docs",
    "citation_count": 152,
    "authors": []
  },
  "https://openreview.net/forum?id=HcUf-QwZeFh": {
    "title": "A System for Morphology-Task Generalization via Unified Representation and Behavior Distillation",
    "volume": "spotlight",
    "abstract": "The rise of generalist large-scale models in natural language and vision has made us expect that a massive data-driven approach could achieve broader generalization in other domains such as continuous control. In this work, we explore a method for learning a single policy that manipulates various forms of agents to solve various tasks by distilling a large amount of proficient behavioral data. In order to align input-output (IO) interface among multiple tasks and diverse agent morphologies while preserving essential 3D geometric relations, we introduce morphology-task graph, which treats observations, actions and goals/task in a unified graph representation. We also develop MxT-Bench for fast large-scale behavior generation, which supports procedural generation of diverse morphology-task combinations with a minimal blueprint and hardware-accelerated simulator. Through efficient representation and architecture selection on MxT-Bench, we find out that a morphology-task graph representation coupled with Transformer architecture improves the multi-task performances compared to other baselines including recent discrete tokenization, and provides better prior knowledge for zero-shot transfer or sample efficiency in downstream multi-task imitation learning. Our work suggests large diverse offline datasets, unified IO representation, and policy representation and architecture selection through supervised learning form a promising approach for studying and advancing morphology-task generalization",
    "checked": true,
    "id": "8745c5b9522c11818418f64fdc880894faeaed16",
    "semantic_title": "a system for morphology-task generalization via unified representation and behavior distillation",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=9XFSbDPmdW": {
    "title": "Progress measures for grokking via mechanistic interpretability",
    "volume": "spotlight",
    "abstract": "Neural networks often exhibit emergent behavior in which qualitatively new capabilities that arise from scaling up the number of parameters, training data, or even the number of steps. One approach to understanding emergence is to find the continuous \\textit{progress measures} that underlie the seemingly discontinuous qualitative changes. In this work, we argue that progress measures can be found via mechanistic interpretability---that is, by reverse engineering learned models into components and measuring the progress of each component over the course of training. As a case study, we study small transformers trained on a modular arithmetic tasks with emergent grokking behavior. We fully reverse engineer the algorithm learned by these networks, which uses discrete fourier transforms and trigonometric identities to convert addition to rotation about a circle. After confirming the algorithm via ablation, we then use our understanding of the algorithm to define progress measures that precede the grokking phase transition on this task. We see our result as demonstrating both that it is possible to fully reverse engineer trained networks, and that doing so can be invaluable to understanding their training dynamics",
    "checked": true,
    "id": "f680d47a51a0e470fcb228bf0110c026535ead1b",
    "semantic_title": "progress measures for grokking via mechanistic interpretability",
    "citation_count": 484,
    "authors": []
  },
  "https://openreview.net/forum?id=oMsN9TYwJ0j": {
    "title": "PiFold: Toward effective and efficient protein inverse folding",
    "volume": "spotlight",
    "abstract": "How can we design protein sequences folding into the desired structures effectively and efficiently? AI methods for structure-based protein design have attracted increasing attention in recent years; however, few methods can simultaneously improve the accuracy and efficiency due to the lack of expressive features and autoregressive sequence decoder. To address these issues, we propose PiFold, which contains a novel residue featurizer and PiGNN layers to generate protein sequences in a one-shot way with improved recovery. Experiments show that PiFold could achieve 51.66\\% recovery on CATH 4.2, while the inference speed is 70 times faster than the autoregressive competitors. In addition, PiFold achieves 58.72\\% and 60.42\\% recovery scores on TS50 and TS500, respectively. We conduct comprehensive ablation studies to reveal the role of different types of protein features and model designs, inspiring further simplification and improvement. The PyTorch code is available at \\href{https://github.com/A4Bio/PiFold}{GitHub}",
    "checked": true,
    "id": "61faa3d2d74dc98a098579008f64d35ada55a07a",
    "semantic_title": "pifold: toward effective and efficient protein inverse folding",
    "citation_count": 121,
    "authors": []
  },
  "https://openreview.net/forum?id=6qeBuZSo7Pr": {
    "title": "Planning Goals for Exploration",
    "volume": "spotlight",
    "abstract": "Dropped into an unknown environment, what should an agent do to quickly learn about the environment and how to accomplish diverse tasks within it? We address this question within the goal-conditioned reinforcement learning paradigm, by identifying how the agent should set its goals at training time to maximize exploration. We propose \"Planning Exploratory Goals\" (PEG), a method that sets goals for each training episode to directly optimize an intrinsic exploration reward. PEG first chooses goal commands such that the agent's goal-conditioned policy, at its current level of training, will end up in states with high exploration potential. It then launches an exploration policy starting at those promising states. To enable this direct optimization, PEG learns world models and adapts sampling-based planning algorithms to \"plan goal commands\". In challenging simulated robotics environments including a multi-legged ant robot in a maze, and a robot arm on a cluttered tabletop, PEG exploration enables more efficient and effective training of goal-conditioned policies relative to baselines and ablations. Our ant successfully navigates a long maze, and the robot arm successfully builds a stack of three blocks upon command. Website: https://sites.google.com/view/exploratory-goals",
    "checked": true,
    "id": "5045c2a64a4ea41d8da9ee8eb279498db1ff3d78",
    "semantic_title": "planning goals for exploration",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=Do9MOlwWHu0": {
    "title": "Learning Sparse Group Models Through Boolean Relaxation",
    "volume": "spotlight",
    "abstract": "We introduce an efficient algorithmic framework for learning sparse group models formulated as the natural convex relaxation of a cardinality-constrained program with Boolean variables. We provide theoretical techniques to characterize the equivalent condition when the relaxation achieves the exact integral optimal solution, as well as a rounding algorithm to produce a feasible integral solution once the optimal relaxation solution is fractional. We demonstrate the power of our equivalent condition by applying it to two ensembles of random problem instances that are challenging and popularly used in literature and prove that our method achieves exactness with overwhelming probability and nearly optimal sample complexity. Empirically, we use synthetic datasets to demonstrate that our proposed method significantly outperforms the state-of-the-art group sparse learning models in terms of individual and group support recovery when the number of samples is small. Furthermore, we show the out-performance of our method in cancer drug response prediction",
    "checked": true,
    "id": "819942d9ca2d5aa00b9a280c3a20a5ea8e20d0de",
    "semantic_title": "learning sparse group models through boolean relaxation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0cpM2ApF9p6": {
    "title": "MeshDiffusion: Score-based Generative 3D Mesh Modeling",
    "volume": "spotlight",
    "abstract": "We consider the task of generating realistic 3D shapes, which is useful for a variety of applications such as automatic scene generation and physical simulation. Compared to other 3D representations like voxels and point clouds, meshes are more desirable in practice, because (1) they enable easy and arbitrary manipulation of shapes for relighting and simulation, and (2) they can fully leverage the power of modern graphics pipelines which are mostly optimized for meshes. Previous scalable methods for generating meshes typically rely on sub-optimal post-processing, and they tend to produce overly-smooth or noisy surfaces without fine-grained geometric details. To overcome these shortcomings, we take advantage of the graph structure of meshes and use a simple yet very effective generative modeling method to generate 3D meshes. Specifically, we represent meshes with deformable tetrahedral grids, and then train a diffusion model on this direct parameterization. We demonstrate the effectiveness of our model on multiple generative tasks",
    "checked": true,
    "id": "b7a783e3897baed760fb91cd1289dd0e353377f5",
    "semantic_title": "meshdiffusion: score-based generative 3d mesh modeling",
    "citation_count": 170,
    "authors": []
  },
  "https://openreview.net/forum?id=n05upKp02kQ": {
    "title": "Partially Observable RL with B-Stability: Unified Structural Condition and Sharp Sample-Efficient Algorithms",
    "volume": "spotlight",
    "abstract": "Partial Observability---where agents can only observe partial information about the true underlying state of the system---is ubiquitous in real-world applications of Reinforcement Learning (RL). Theoretically, learning a near-optimal policy under partial observability is known to be hard in the worst case due to an exponential sample complexity lower bound. Recent work has identified several tractable subclasses that are learnable with polynomial samples, such as Partially Observable Markov Decision Processes (POMDPs) with certain revealing or decodability conditions. However, this line of research is still in its infancy, where (1) unified structural conditions enabling sample-efficient learning are lacking; (2) existing sample complexities for known tractable subclasses are far from sharp; and (3) fewer sample-efficient algorithms are available than in fully observable RL. This paper advances all three aspects above for Partially Observable RL in the general setting of Predictive State Representations (PSRs). First, we propose a natural and unified structural condition for PSRs called \\emph{B-stability}. B-stable PSRs encompasses the vast majority of known tractable subclasses such as weakly revealing POMDPs, low-rank future-sufficient POMDPs, decodable POMDPs, and regular PSRs. Next, we show that any B-stable PSR can be learned with polynomial samples in relevant problem parameters. When instantiated in the aforementioned subclasses, our sample complexities improve substantially over the current best ones. Finally, our results are achieved by three algorithms simultaneously: Optimistic Maximum Likelihood Estimation, Estimation-to-Decisions, and Model-Based Optimistic Posterior Sampling. The latter two algorithms are new for sample-efficient learning of POMDPs/PSRs. We additionally design a variant of the Estimation-to-Decisions algorithm to perform sample-efficient \\emph{all-policy model estimation} for B-stable PSRs, which also yields guarantees for reward-free learning as an implication",
    "checked": true,
    "id": "7d23c472507450c1ac386cc2b1c20419d86da780",
    "semantic_title": "partially observable rl with b-stability: unified structural condition and sharp sample-efficient algorithms",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=fk7RbGibe1": {
    "title": "Domain Generalization via Heckman-type Selection Models",
    "volume": "spotlight",
    "abstract": "The domain generalization (DG) setup considers the problem where models are trained on data sampled from multiple domains and evaluated on test domains unseen during training. In this paper, we formulate DG as a sample selection problem where each domain is sampled from a common underlying population through non-random sampling probabilities that correlate with both the features and the outcome. Under this setting, the fundamental iid assumption of the empirical risk minimization (ERM) is violated, so it often performs worse on test domains whose non-random sampling probabilities differ from the domains in the training dataset. We propose a Selection-Guided DG (SGDG) framework to learn the selection probability of each domain and the joint distribution of the outcome and domain selection variables. The proposed SGDG is domain generalizable as it intends to minimize the risk under the population distribution. We theoretically proved that, under certain regular conditions, SGDG can achieve smaller risk than ERM. Furthermore, we present a class of parametric SGDG (HeckmanDG) estimators applicable to continuous, binary, and multinomial outcomes. We also demonstrated its efficacy empirically through simulations and experiments on a set of benchmark datasets comparing with other well-known DG methods",
    "checked": true,
    "id": "f445045eeb96f322aa219616382dd2b61398104d",
    "semantic_title": "domain generalization via heckman-type selection models",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=mbxz9Cjehr": {
    "title": "A CMDP-within-online framework for Meta-Safe Reinforcement Learning",
    "volume": "spotlight",
    "abstract": "Meta-reinforcement learning has widely been used as a learning-to-learn framework to solve unseen tasks with limited experience. However, the aspect of constraint violations has not been adequately addressed in the existing works, making their application restricted in real-world settings. In this paper, we study the problem of meta-safe reinforcement learning (meta-SRL) through the CMDP-within-online framework. We obtain task-averaged regret guarantees for the reward maximization (optimality gap) and constraint violations using gradient-based meta-learning and show that the task-averaged optimality gap and constraint satisfaction improve with task-similarity in the static environment, or task-relatedness in the changing environment. Several technical challenges arise when making this framework practical while still having strong theoretical guarantees. To address these challenges, we propose a meta-algorithm that performs inexact online learning on the upper bounds of intra-task optimality gap and constraint violations estimated by off-policy stationary distribution corrections. Furthermore, we enable the learning rates to be adapted for every task and extend our approach to settings with the dynamically changing task environments. Finally, experiments are conducted to demonstrate the effectiveness of our approach. The proposed theoretical framework is the first to handle the nonconvexity and stochastic nature of within-task CMDPs, while exploiting inter-task dependency for multi-task safe learning",
    "checked": false,
    "id": "d27ec858f86a12cae3d6addf3426ecd2b3539344",
    "semantic_title": "provable guarantees for meta-safe reinforcement learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=P-73JPgRs0R": {
    "title": "Effects of Graph Convolutions in Multi-layer Networks",
    "volume": "spotlight",
    "abstract": "Graph Convolutional Networks (GCNs) are one of the most popular architectures that are used to solve classification problems accompanied by graphical information. We present a rigorous theoretical understanding of the effects of graph convolutions in multi-layer networks. We study these effects through the node classification problem of a non-linearly separable Gaussian mixture model coupled with a stochastic block model. First, we show that a single graph convolution expands the regime of the distance between the means where multi-layer networks can classify the data by a factor of at least $1/\\sqrt[4]{\\rm deg}$, where ${\\rm deg}$ denotes the expected degree of a node. Second, we show that with a slightly stronger graph density, two graph convolutions improve this factor to at least $1/\\sqrt[4]{n}$, where $n$ is the number of nodes in the graph. Finally, we provide both theoretical and empirical insights into the performance of graph convolutions placed in different combinations among the layers of a neural network, concluding that the performance is mutually similar for all combinations of the placement. We present extensive experiments on both synthetic and real-world data that illustrate our results",
    "checked": true,
    "id": "373d3b20dc5728d73d6e4323235bec50418a0395",
    "semantic_title": "effects of graph convolutions in multi-layer networks",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=nA5AZ8CEyow": {
    "title": "Post-hoc Concept Bottleneck Models",
    "volume": "spotlight",
    "abstract": "Concept Bottleneck Models (CBMs) map the inputs onto a set of interpretable concepts (``the bottleneck'') and use the concepts to make predictions. A concept bottleneck enhances interpretability since it can be investigated to understand what concepts the model \"sees\" in an input and which of these concepts are deemed important. However, CBMs are restrictive in practice as they require dense concept annotations in the training data to learn the bottleneck. Moreover, CBMs often do not match the accuracy of an unrestricted neural network, reducing the incentive to deploy them in practice. In this work, we address these limitations of CBMs by introducing Post-hoc Concept Bottleneck models (PCBMs). We show that we can turn any neural network into a PCBM without sacrificing model performance while still retaining the interpretability benefits. When concept annotations are not available on the training data, we show that PCBM can transfer concepts from other datasets or from natural language descriptions of concepts via multimodal models. A key benefit of PCBM is that it enables users to quickly debug and update the model to reduce spurious correlations and improve generalization to new distributions. PCBM allows for global model edits, which can be more efficient than previous works on local interventions that fix a specific prediction. Through a model-editing user study, we show that editing PCBMs via concept-level feedback can provide significant performance gains without using data from the target domain or model retraining",
    "checked": true,
    "id": "8545e249ab7a49f4a5abcfade395b90ffadb687a",
    "semantic_title": "post-hoc concept bottleneck models",
    "citation_count": 205,
    "authors": []
  },
  "https://openreview.net/forum?id=u2Pd6x794I": {
    "title": "When Source-Free Domain Adaptation Meets Learning with Noisy Labels",
    "volume": "spotlight",
    "abstract": "Recent state-of-the-art source-free domain adaptation (SFDA) methods have focused on learning meaningful cluster structures in the feature space, which have succeeded in adapting the knowledge from source domain to unlabeled target domain without accessing the private source data. However, existing methods rely on the pseudo-labels generated by source models that can be noisy due to domain shift. In this paper, we study SFDA from the perspective of learning with label noise (LLN). Unlike the label noise in the conventional LLN scenario, we prove that the label noise in SFDA follows a different distribution assumption. We also prove that such a difference makes existing LLN methods that rely on their distribution assumptions unable to address the label noise in SFDA. Empirical evidence suggests that only marginal improvements are achieved when applying the existing LLN methods to solve the SFDA problem. On the other hand, although there exists a fundamental difference between the label noise in the two scenarios, we demonstrate theoretically that the early-time training phenomenon (ETP), which has been previously observed in conventional label noise settings, can also be observed in the SFDA problem. Extensive experiments demonstrate significant improvements to existing SFDA algorithms by leveraging ETP to address the label noise in SFDA",
    "checked": true,
    "id": "549f026dfe0dcd86ff8f081584f3385e5696fd8e",
    "semantic_title": "when source-free domain adaptation meets learning with noisy labels",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=6taykzqcPD": {
    "title": "Neural Networks Efficiently Learn Low-Dimensional Representations with SGD",
    "volume": "spotlight",
    "abstract": "We study the problem of training a two-layer neural network (NN) of arbitrary width using stochastic gradient descent (SGD) where the input $\\boldsymbol{x}\\in \\mathbb{R}^d$ is Gaussian and the target $y \\in \\mathbb{R}$ follows a multiple-index model, i.e., $y=g(\\langle\\boldsymbol{u_1},\\boldsymbol{x}\\rangle,...,\\langle\\boldsymbol{u_k},\\boldsymbol{x}\\rangle)$ with a noisy link function $g$. We prove that the first-layer weights in the NN converge to the $k$-dimensional principal subspace spanned by the vectors $\\boldsymbol{u_1},...,\\boldsymbol{u_k}$ of the true model, when online SGD with weight decay is used for training. This phenomenon has several important consequences when $k \\ll d$. First, by employing uniform convergence on this smaller subspace, we establish a generalization error bound of $\\mathcal{O}(\\sqrt{{kd}/{T}})$ after $T$ iterations of SGD, which is independent of the width of the NN. We further demonstrate that, by recovering the principal direction, SGD-trained ReLU NNs can learn a single-index target of the form $y=f(\\langle\\boldsymbol{u},\\boldsymbol{x}\\rangle) + \\epsilon$ with a sample complexity linear in $d$ (up to log factors), where $f$ is a monotonic function with at most polynomial growth, and $\\epsilon$ is the noise. This is in contrast to the known $d^{\\Omega(p)}$ samples required to learn any degree $p$ polynomial in the kernel regime, and shows that SGD-trained NNs can outperform the Neural Tangent Kernel at initialization. Finally, we establish compressibility guarantees for NNs using that SGD produces an approximately rank-$k$ first-layer weight matrix",
    "checked": true,
    "id": "60b43c4ba92f14cd5a8a4c68ff232b592d9b19cd",
    "semantic_title": "neural networks efficiently learn low-dimensional representations with sgd",
    "citation_count": 51,
    "authors": []
  },
  "https://openreview.net/forum?id=MYEap_OcQI": {
    "title": "Does Zero-Shot Reinforcement Learning Exist?",
    "volume": "spotlight",
    "abstract": "A zero-shot RL agent is an agent that can solve any RL task in a given environment, instantly with no additional planning or learning, after an initial reward-free learning phase. This marks a shift from the reward-centric RL paradigm towards controllable agents that can follow arbitrary instructions in an environment. Current RL agents can solve families of related tasks at best, or require planning anew for each task. Strategies for approximate zero-shot RL have been suggested using successor features (SFs) (Borsa et al., 2018) or forward-backward (FB) representations (Touati & Ollivier, 2021), but testing has been limited. After clarifying the relationships between these schemes, we introduce improved losses and new SF models, and test the viability of zero-shot RL schemes systematically on tasks from the Unsupervised RL benchmark (Laskin et al., 2021). To disentangle universal representation learning from exploration, we work in an offline setting and repeat the tests on several existing replay buffers. SFs appear to suffer from the choice of the elementary state features. SFs with Laplacian eigenfunctions do well, while SFs based on auto-encoders, inverse curiosity, transition models, low-rank transition matrix, contrastive learning, or diversity (APS), perform unconsistently. In contrast, FB representations jointly learn the elementary and successor features from a single, principled criterion. They perform best and consistently across the board, reaching $85\\%$ of supervised RL performance with a good replay buffer, in a zero-shot manner",
    "checked": true,
    "id": "ac134b312d64b0bf01ccf15e60be4fa3016ee101",
    "semantic_title": "does zero-shot reinforcement learning exist?",
    "citation_count": 50,
    "authors": []
  },
  "https://openreview.net/forum?id=TfBHFLgv77": {
    "title": "Hyperbolic Deep Reinforcement Learning",
    "volume": "spotlight",
    "abstract": "In deep reinforcement learning (RL), useful information about the state is inherently tied to its possible future successors. Consequently, encoding features that capture the hierarchical relationships between states into the model's latent representations is often conducive to recovering effective policies. In this work, we study a new class of deep RL algorithms that promote encoding such relationships by using hyperbolic space to model latent representations. However, we find that a naive application of existing methodology from the hyperbolic deep learning literature leads to fatal instabilities due to the non-stationarity and variance characterizing common gradient estimators in RL. Hence, we design a new general method that directly addresses such optimization challenges and enables stable end-to-end learning with deep hyperbolic representations. We empirically validate our framework by applying it to popular on-policy and off-policy RL algorithms on the Procgen and Atari 100K benchmarks, attaining near universal performance and generalization benefits. Given its natural fit, we hope this work will inspire future RL research to consider hyperbolic representations as a standard tool",
    "checked": true,
    "id": "34b977f14fbe8e37d09eb64e88e92386ba42c36d",
    "semantic_title": "hyperbolic deep reinforcement learning",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=PbfgkZ2HdbE": {
    "title": "Learning Controllable Adaptive Simulation for Multi-resolution Physics",
    "volume": "spotlight",
    "abstract": "Simulating the time evolution of physical systems is pivotal in many scientific and engineering problems. An open challenge in simulating such systems is their multi-resolution dynamics: a small fraction of the system is extremely dynamic, and requires very fine-grained resolution, while a majority of the system is changing slowly and can be modeled by coarser spatial scales. Typical learning-based surrogate models use a uniform spatial scale, which needs to resolve to the finest required scale and can waste a huge compute to achieve required accuracy. In this work, we introduce Learning controllable Adaptive simulation for Multi-resolution Physics (LAMP) as the first full deep learning-based surrogate model that jointly learns the evolution model and optimizes appropriate spatial resolutions that devote more compute to the highly dynamic regions. LAMP consists of a Graph Neural Network (GNN) for learning the forward evolution, and a GNN-based actor-critic for learning the policy of spatial refinement and coarsening. We introduce learning techniques that optimizes LAMP with weighted sum of error and computational cost as objective, allowing LAMP to adapt to varying relative importance of error vs. computation tradeoff at inference time. We evaluate our method in a 1D benchmark of nonlinear PDEs and a challenging 2D mesh-based simulation. We demonstrate that our LAMP outperforms state-of-the-art deep learning surrogate models, and can adaptively trade-off computation to improve long-term prediction error: it achieves an average of 33.7% error reduction for 1D nonlinear PDEs, and outperforms MeshGraphNets + classical Adaptive Mesh Refinement (AMR) in 2D mesh-based simulations. Project website with data and code can be found at: http://snap.stanford.edu/lamp",
    "checked": true,
    "id": "75cb343edaaa6a5322588bf6f3ef160742e9fbc0",
    "semantic_title": "learning controllable adaptive simulation for multi-resolution physics",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=Mpa3tRJFBb": {
    "title": "Where to Begin? On the Impact of Pre-Training and Initialization in Federated Learning",
    "volume": "spotlight",
    "abstract": "An oft-cited challenge of federated learning is the presence of heterogeneity. \\emph{Data heterogeneity} refers to the fact that data from different clients may follow very different distributions. \\emph{System heterogeneity} refers to client devices having different system capabilities. A considerable number of federated optimization methods address this challenge. In the literature, empirical evaluations usually start federated training from random initialization. However, in many practical applications of federated learning, the server has access to proxy data for the training task that can be used to pre-train a model before starting federated training. Using four standard federated learning benchmark datasets, we empirically study the impact of starting from a pre-trained model in federated learning. Unsurprisingly, starting from a pre-trained model reduces the training time required to reach a target error rate and enables the training of more accurate models (up to 40\\%) than is possible when starting from random initialization. Surprisingly, we also find that starting federated learning from a pre-trained initialization reduces the effect of both data and system heterogeneity. We recommend future work proposing and evaluating federated optimization methods to evaluate the performance when starting from random and pre-trained initializations. This study raises several questions for further work on understanding the role of heterogeneity in federated optimization",
    "checked": true,
    "id": "fc721f4fd6260ed3b86c64eaa204375e18863aad",
    "semantic_title": "where to begin? on the impact of pre-training and initialization in federated learning",
    "citation_count": 81,
    "authors": []
  },
  "https://openreview.net/forum?id=F_EhNDSamN": {
    "title": "Parametrizing Product Shape Manifolds by Composite Networks",
    "volume": "spotlight",
    "abstract": "Parametrizations of data manifolds in shape spaces can be computed using the rich toolbox of Riemannian geometry. This, however, often comes with high computational costs, which raises the question if one can learn an efficient neural network approximation. We show that this is indeed possible for shape spaces with a special product structure, namely those smoothly approximable by a direct sum of low-dimensional manifolds. Our proposed architecture leverages this structure by separately learning approximations for the low-dimensional factors and a subsequent combination. After developing the approach as a general framework, we apply it to a shape space of triangular surfaces. Here, typical examples of data manifolds are given through datasets of articulated models and can be factorized, for example, by a Sparse Principal Geodesic Analysis (SPGA). We demonstrate the effectiveness of our proposed approach with experiments on synthetic data as well as manifolds extracted from data via SPGA",
    "checked": true,
    "id": "f7346fbddd4d3a433dbbd14f964905226427cdc1",
    "semantic_title": "parametrizing product shape manifolds by composite networks",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zKvm1ETDOq": {
    "title": "Is Adversarial Training Really a Silver Bullet for Mitigating Data Poisoning?",
    "volume": "spotlight",
    "abstract": "Indiscriminate data poisoning can decrease the clean test accuracy of a deep learning model by slightly perturbing its training samples. There is a consensus that such poisons can hardly harm adversarially-trained (AT) models when the adversarial training budget is no less than the poison budget, i.e., $\\epsilon_\\mathrm{adv}\\geq\\epsilon_\\mathrm{poi}$. This consensus, however, is challenged in this paper based on our new attack strategy that induces \\textit{entangled features} (EntF). The existence of entangled features makes the poisoned data become less useful for training a model, no matter if AT is applied or not. We demonstrate that for attacking a CIFAR-10 AT model under a reasonable setting with $\\epsilon_\\mathrm{adv}=\\epsilon_\\mathrm{poi}=8/255$, our EntF yields an accuracy drop of $13.31\\%$, which is $7\\times$ better than existing methods and equal to discarding $83\\%$ training data. We further show the generalizability of EntF to more challenging settings, e.g., higher AT budgets, partial poisoning, unseen model architectures, and stronger (ensemble or adaptive) defenses. We finally provide new insights into the distinct roles of non-robust vs. robust features in poisoning standard vs. AT models and demonstrate the possibility of using a hybrid attack to poison standard and AT models simultaneously. Our code is available at~\\url{https://github.com/WenRuiUSTC/EntF}",
    "checked": true,
    "id": "2418bc3f97d1ee8d0caec7656af78343b83cabe3",
    "semantic_title": "is adversarial training really a silver bullet for mitigating data poisoning?",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=P3PJokAqGW": {
    "title": "Learning with Stochastic Orders",
    "volume": "spotlight",
    "abstract": "Learning high-dimensional distributions is often done with explicit likelihood modeling or implicit modeling via minimizing integral probability metrics (IPMs). In this paper, we expand this learning paradigm to stochastic orders, namely, the convex or Choquet order between probability measures. Towards this end, exploiting the relation between convex orders and optimal transport, we introduce the Choquet-Toland distance between probability measures, that can be used as a drop-in replacement for IPMs. We also introduce the Variational Dominance Criterion (VDC) to learn probability measures with dominance constraints, that encode the desired stochastic order between the learned measure and a known baseline. We analyze both quantities and show that they suffer from the curse of dimensionality and propose surrogates via input convex maxout networks (ICMNs), that enjoy parametric rates. We provide a min-max framework for learning with stochastic orders and validate it experimentally on synthetic and high-dimensional image generation, with promising results. Finally, our ICMNs class of convex functions and its derived Rademacher Complexity are of independent interest beyond their application in convex orders. Code to reproduce experimental results is available at https://github.com/yair-schiff/stochastic-orders-ICMN",
    "checked": true,
    "id": "873faa4e9c3aefd5990746bb7380102d0f039820",
    "semantic_title": "learning with stochastic orders",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=6ve2CkeQe5S": {
    "title": "MEDFAIR: Benchmarking Fairness for Medical Imaging",
    "volume": "spotlight",
    "abstract": "A multitude of work has shown that machine learning-based medical diagnosis systems can be biased against certain subgroups of people. This has motivated a growing number of bias mitigation algorithms that aim to address fairness issues in machine learning. However, it is difficult to compare their effectiveness in medical imaging for two reasons. First, there is little consensus on the criteria to assess fairness. Second, existing bias mitigation algorithms are developed under different settings, e.g., datasets, model selection strategies, backbones, and fairness metrics, making a direct comparison and evaluation based on existing results impossible. In this work, we introduce MEDFAIR, a framework to benchmark the fairness of machine learning models for medical imaging. MEDFAIR covers eleven algorithms from various categories, ten datasets from different imaging modalities, and three model selection criteria. Through extensive experiments, we find that the under-studied issue of model selection criterion can have a significant impact on fairness outcomes; while in contrast, state-of-the-art bias mitigation algorithms do not significantly improve fairness outcomes over empirical risk minimization (ERM) in both in-distribution and out-of-distribution settings. We evaluate fairness from various perspectives and make recommendations for different medical application scenarios that require different ethical principles. Our framework provides a reproducible and easy-to-use entry point for the development and evaluation of future bias mitigation algorithms in deep learning. Code is available at https://github.com/ys-zong/MEDFAIR",
    "checked": true,
    "id": "bca9c389bdcc185eefc6cb2b57f5d93a2e4250c7",
    "semantic_title": "medfair: benchmarking fairness for medical imaging",
    "citation_count": 68,
    "authors": []
  },
  "https://openreview.net/forum?id=TUBpc5rqGA": {
    "title": "Neural Design for Genetic Perturbation Experiments",
    "volume": "spotlight",
    "abstract": "The problem of how to genetically modify cells in order to maximize a certain cellular phenotype has taken center stage in drug development over the last few years (with, for example, genetically edited CAR-T, CAR-NK, and CAR-NKT cells entering cancer clinical trials). Exhausting the search space for all possible genetic edits (perturbations) or combinations thereof is infeasible due to cost and experimental limitations. This work provides a theoretically sound framework for iteratively exploring the space of perturbations in pooled batches in order to maximize a target phenotype under an experimental budget. Inspired by this application domain, we study the problem of batch query bandit optimization and introduce the Optimistic Arm Elimination ($\\mathrm{OAE}$) principle designed to find an almost optimal arm under different functional relationships between the queries (arms) and the outputs (rewards). We analyze the convergence properties of $\\mathrm{OAE}$ by relating it to the Eluder dimension of the algorithm's function class and validate that $\\mathrm{OAE}$ outperforms other strategies in finding optimal actions in experiments on simulated problems, public datasets well-studied in bandit contexts, and in genetic perturbation datasets when the regression model is a deep neural network. OAE also outperforms the benchmark algorithms in 3 of 4 datasets in the GeneDisco experimental planning challenge",
    "checked": true,
    "id": "f52de3bba5becb383834ccaf03aba7790d87b793",
    "semantic_title": "neural design for genetic perturbation experiments",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=R98ZfMt-jE": {
    "title": "Efficient Discrete Multi Marginal Optimal Transport Regularization",
    "volume": "spotlight",
    "abstract": "Optimal transport has emerged as a powerful tool for a variety of problems in machine learning, and it is frequently used to enforce distributional constraints. In this context, existing methods often use either a Wasserstein metric, or else they apply concurrent barycenter approaches when more than two distributions are considered. In this paper, we leverage multi-marginal optimal transport (MMOT), where we take advantage of a procedure that computes a generalized earth mover's distance as a sub-routine. We show that not only is our algorithm computationally more efficient compared to other barycentric-based distance methods, but it has the additional advantage that gradients used for backpropagation can be efficiently computed during the forward pass computation itself, which leads to substantially faster model training. We provide technical details about this new regularization term and its properties, and we present experimental demonstrations of faster runtimes when compared to standard Wasserstein-style methods. Finally, on a range of experiments designed to assess effectiveness at enforcing fairness, we demonstrate our method compares well with alternatives",
    "checked": true,
    "id": "0d18cc7327eea01378a07b52dcfca09b61ce743b",
    "semantic_title": "efficient discrete multi marginal optimal transport regularization",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=xSsW2Am-ukZ": {
    "title": "Unmasking the Lottery Ticket Hypothesis: What's Encoded in a Winning Ticket's Mask?",
    "volume": "spotlight",
    "abstract": "As neural networks get larger and costlier, it is important to find sparse networks that require less compute and memory but can be trained to the same accuracy as the full network (i.e. matching). Iterative magnitude pruning (IMP) is a state of the art algorithm that can find such highly sparse matching subnetworks, known as winning tickets. IMP iterates through cycles of training, pruning a fraction of smallest magnitude weights, rewinding unpruned weights back to an early training point, and repeating. Despite its simplicity, the principles underlying when and how IMP finds winning tickets remain elusive. In particular, what useful information does an IMP mask found at the end of training convey to a rewound network near the beginning of training? How does SGD allow the network to extract this information? And why is iterative pruning needed, i.e. why can't we prune to very high sparsities in one shot? We investigate these questions through the lens of the geometry of the error landscape. First, we find that—at higher sparsities—pairs of pruned networks at successive pruning iterations are connected by a linear path with zero error barrier if and only if they are matching. This indicates that masks found at the end of training convey to the rewind point the identity of an axial subspace that intersects a desired linearly connected mode of a matching sublevel set. Second, we show SGD can exploit this information due to a strong form of robustness: it can return to this mode despite strong perturbations early in training. Third, we show how the flatness of the error landscape at the end of training limits the fraction of weights that can be pruned at each iteration of IMP. This analysis yields a new quantitative link between IMP performance and the Hessian eigenspectrum. Finally, we show that the role of retraining in IMP is to find a network with new small weights to prune. Overall, these results make progress toward demystifying the existence of winning tickets by revealing the fundamental role of error landscape geometry in the algorithms used to find them",
    "checked": true,
    "id": "3de645f0c1993cd3f3374ad747640a1aa6658a82",
    "semantic_title": "unmasking the lottery ticket hypothesis: what's encoded in a winning ticket's mask?",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=TatRHT_1cK": {
    "title": "Quantifying Memorization Across Neural Language Models",
    "volume": "spotlight",
    "abstract": "Large language models (LMs) have been shown to memorize parts of their training data, and when prompted appropriately, they will emit the memorized training data verbatim. This is undesirable because memorization violates privacy (exposing user data), degrades utility (repeated easy-to-memorize text is often low quality), and hurts fairness (some texts are memorized over others). We describe three log-linear relationships that quantify the degree to which LMs emit memorized training data. Memorization significantly grows as we increase (1) the capacity of a model, (2) the number of times an example has been duplicated, and (3) the number of tokens of context used to prompt the model. Surprisingly, we find the situation becomes complicated when generalizing these results across model families. On the whole, we find that memorization in LMs is more prevalent than previously believed and will likely get worse as models continues to scale, at least without active mitigations",
    "checked": true,
    "id": "28c7e583d90ccfc5c3078dfc1d6b80a9ad90248d",
    "semantic_title": "quantifying memorization across neural language models",
    "citation_count": 663,
    "authors": []
  },
  "https://openreview.net/forum?id=AWZgXGmsbA": {
    "title": "Powderworld: A Platform for Understanding Generalization via Rich Task Distributions",
    "volume": "spotlight",
    "abstract": "One of the grand challenges of reinforcement learning is the ability to generalize to new tasks. However, general agents require a set of rich, diverse tasks to train on. Designing a `foundation environment' for such tasks is tricky -- the ideal environment would support a range of emergent phenomena, an expressive task space, and fast runtime. To take a step towards addressing this research bottleneck, this work presents Powderworld, a lightweight yet expressive simulation environment running directly on the GPU. Within Powderworld, two motivating task distributions are presented, one for world-modelling and one for reinforcement learning. Each contains hand-designed test tasks to examine generalization. Experiments indicate that increasing the environment's complexity improves generalization for world models, yet causes reinforcement learning agents to struggle. Powderworld aims to support the study of generalization by providing a source of diverse tasks arising from the same core rules",
    "checked": true,
    "id": "2d41c403c8e9cff035778961e6be45b5e8182d1d",
    "semantic_title": "powderworld: a platform for understanding generalization via rich task distributions",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=kJUS5nD0vPB": {
    "title": "Out-of-Distribution Detection and Selective Generation for Conditional Language Models",
    "volume": "spotlight",
    "abstract": "Machine learning algorithms typically assume independent and identically distributed samples in training and at test time (IID). Much work has shown that high-performing ML classifiers can degrade significantly and provide overly-confident, wrong classification predictions, particularly for out-of-distribution (OOD) inputs. Conditional language models (CLMs) are predominantly trained to classify the next token in an output sequence, and may suffer even worse degradation on OOD inputs as the prediction is done auto-regressively over many steps. Furthermore, the space of potential low-quality outputs is larger as arbitrary text can be generated and it is important to know when to trust the generated output. We present a highly accurate and lightweight OOD detection method for CLMs, and demonstrate its effectiveness on abstractive summarization and translation. We also show how our method can be used under the common and realistic setting of distribution shift for selective generation (analogous to selective prediction for classification) of high-quality outputs, while automatically abstaining from low-quality ones, enabling safer deployment of generative language models",
    "checked": true,
    "id": "94b6f6822f364cf7b1a3a9984667c009e2ec6a65",
    "semantic_title": "out-of-distribution detection and selective generation for conditional language models",
    "citation_count": 126,
    "authors": []
  },
  "https://openreview.net/forum?id=3UHoYrglYkG": {
    "title": "Differentially Private $L_2$-Heavy Hitters in the Sliding Window Model",
    "volume": "spotlight",
    "abstract": "The data management of large companies often prioritize more recent data, as a source of higher accuracy prediction than outdated data. For example, the Facebook data policy retains user search histories for $6$ months while the Google data retention policy states that browser information may be stored for up to $9$ months. These policies are captured by the sliding window model, in which only the most recent $W$ statistics form the underlying dataset. In this paper, we consider the problem of privately releasing the $L_2$-heavy hitters in the sliding window model, which include $L_p$-heavy hitters for $p\\le 2$ and in some sense are the strongest possible guarantees that can be achieved using polylogarithmic space, but cannot be handled by existing techniques due to the sub-additivity of the $L_2$ norm. Moreover, existing non-private sliding window algorithms use the smooth histogram framework, which has high sensitivity. To overcome these barriers, we introduce the first differentially private algorithm for $L_2$-heavy hitters in the sliding window model by initiating a number of $L_2$-heavy hitter algorithms across the stream with significantly lower threshold. Similarly, we augment the algorithms with an approximate frequency tracking algorithm with significantly higher accuracy. We then use smooth sensitivity and statistical distance arguments to show that we can add noise proportional to an estimation of the $L_2$ norm. To the best of our knowledge, our techniques are the first to privately release statistics that are related to a sub-additive function in the sliding window model, and may be of independent interest to future differentially private algorithmic design in the sliding window model",
    "checked": false,
    "id": "53bf4fafae1fcfe1621e5b00b15139f521ea3bcf",
    "semantic_title": "differentially private l2-heavy hitters in the sliding window model",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=ApF0dmi1_9K": {
    "title": "NTFields: Neural Time Fields for Physics-Informed Robot Motion Planning",
    "volume": "spotlight",
    "abstract": "Neural Motion Planners (NMPs) have emerged as a promising tool for solving robot navigation tasks in complex environments. However, these methods often require expert data for learning, which limits their application to scenarios where data generation is time-consuming. Recent developments have also led to physics-informed deep neural models capable of representing complex dynamical Partial Differential Equations (PDEs). Inspired by these developments, we propose Neural Time Fields (NTFields) for robot motion planning in cluttered scenarios. Our framework represents a wave propagation model generating continuous arrival time to find path solutions informed by a nonlinear first-order PDE called Eikonal Equation. We evaluate our method in various cluttered 3D environments, including the Gibson dataset, and demonstrate its ability to solve motion planning problems for 4-DOF and 6-DOF robot manipulators where the traditional grid-based Eikonal planners often face the curse of dimensionality. Furthermore, the results show that our method exhibits high success rates and significantly lower computational times than the state-of-the-art methods, including NMPs that require training data from classical planners",
    "checked": true,
    "id": "7f26566298dd9b89eb3446cc0f4bd2d2fa8aceed",
    "semantic_title": "ntfields: neural time fields for physics-informed robot motion planning",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=rwo-ls5GqGn": {
    "title": "ZiCo: Zero-shot NAS via inverse Coefficient of Variation on Gradients",
    "volume": "spotlight",
    "abstract": "Neural Architecture Search (NAS) is widely used to automatically obtain the neural network with the best performance among a large number of candidate architectures. To reduce the search time, zero-shot NAS aims at designing training-free proxies that can predict the test performance of a given architecture. However, as shown recently, none of the zero-shot proxies proposed to date can actually work consistently better than a naive proxy, namely, the number of network parameters (#Params). To improve this state of affairs, as the main theoretical contribution, we first reveal how some specific gradient properties across different samples impact the convergence rate and generalization capacity of neural networks. Based on this theoretical analysis, we propose a new zero-shot proxy, ZiCo, the first proxy that works consistently better than #Params. We demonstrate that ZiCo works better than State-Of-The-Art (SOTA) proxies on several popular NAS-Benchmarks (NASBench101, NATSBench-SSS/TSS, TransNASBench-101) for multiple applications (e.g., image classification/reconstruction and pixel-level prediction). Finally, we demonstrate that the optimal architectures found via ZiCo are as competitive as the ones found by one-shot and multi-shot NAS methods, but with much less search time. For example, ZiCo-based NAS can find optimal architectures with 78.1%, 79.4%, and 80.4% test accuracy under inference budgets of 450M, 600M, and 1000M FLOPs, respectively, on ImageNet within 0.4 GPU days. Our code is available at https://github.com/SLDGroup/ZiCo",
    "checked": true,
    "id": "ef174c117cbe749517e5b282bc68b0e4e567bcba",
    "semantic_title": "zico: zero-shot nas via inverse coefficient of variation on gradients",
    "citation_count": 70,
    "authors": []
  },
  "https://openreview.net/forum?id=hQ9V5QN27eS": {
    "title": "Pink Noise Is All You Need: Colored Noise Exploration in Deep Reinforcement Learning",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "4a06ec48e4b413ab2563273981018df82faac32e",
    "semantic_title": "pink noise is all you need: colored noise exploration in deep reinforcement learning",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=1mNssCWt_v": {
    "title": "STaSy: Score-based Tabular data Synthesis",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "6349332c9226c14561f9eb82162a198142cb2965",
    "semantic_title": "stasy: score-based tabular data synthesis",
    "citation_count": 70,
    "authors": []
  },
  "https://openreview.net/forum?id=k71IGLC8cfc": {
    "title": "A Unified Algebraic Perspective on Lipschitz Neural Networks",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "897759be42a5b59ab7c01c8021ec6a2e1079e043",
    "semantic_title": "a unified algebraic perspective on lipschitz neural networks",
    "citation_count": 56,
    "authors": []
  },
  "https://openreview.net/forum?id=nZ2NtpolC5-": {
    "title": "The Influence of Learning Rule on Representation Dynamics in Wide Neural Networks",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "28553431f0da6329a6d3722036e2511c2d8a1da2",
    "semantic_title": "the influence of learning rule on representation dynamics in wide neural networks",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=sCYXJr3QJM8": {
    "title": "Few-shot Cross-domain Image Generation via Inference-time Latent-code Learning",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "ba68c0997556ae7b78ae31b2302e1a2740ef367e",
    "semantic_title": "few-shot cross-domain image generation via inference-time latent-code learning",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=DJEEqoAq7to": {
    "title": "RLx2: Training a Sparse Deep Reinforcement Learning Model from Scratch",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "b831edf994dcfba95706bb93790c7e2cf697ab80",
    "semantic_title": "rlx2: training a sparse deep reinforcement learning model from scratch",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=J6F3lLg4Kdp": {
    "title": "Sparsity May Cry: Let Us Fail (Current) Sparse Neural Networks Together!",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "fdacdbc6a00eeb42efe7f81848b0bc09be5ca997",
    "semantic_title": "sparsity may cry: let us fail (current) sparse neural networks together!",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=w1hwFUb_81": {
    "title": "Sparse MoE as the New Dropout: Scaling Dense and Self-Slimmable Transformers",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "1462a0e5b7db47301bb0995db56426e1f4a0ac7d",
    "semantic_title": "sparse moe as the new dropout: scaling dense and self-slimmable transformers",
    "citation_count": 51,
    "authors": []
  },
  "https://openreview.net/forum?id=LfdEuhjR5GV": {
    "title": "Adversarial Training of Self-supervised Monocular Depth Estimation against Physical-World Attacks",
    "volume": "spotlight",
    "abstract": "Monocular Depth Estimation (MDE) is a critical component in applications such as autonomous driving. There are various attacks against MDE networks. These attacks, especially the physical ones, pose a great threat to the security of such systems. Traditional adversarial training method requires ground-truth labels and hence cannot be directly applied to self-supervised MDE that does not have depth ground truth. Some self-supervised model hardening technique (e.g., contrastive learning) ignores the domain knowledge of MDE and can hardly achieve optimal performance. In this work, we propose a novel adversarial training method for self-supervised MDE models based on view synthesis without using the depth ground truth. We improve adversarial robustness against physical-world attacks using $L_0$-norm-bounded perturbation in training. We compare our method with supervised learning-based and contrastive learning-based methods that are tailored for MDE. Results on two representative MDE networks show that we achieve better robustness against various adversarial attacks with nearly no benign performance degradation",
    "checked": true,
    "id": "32e07cc454af5249d34f005e277c307fdc2f638d",
    "semantic_title": "adversarial training of self-supervised monocular depth estimation against physical-world attacks",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=yHY9NbQJ5BP": {
    "title": "Sparsity-Constrained Optimal Transport",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "a27d5e6140269f292da02c3e88b4235c83f49e9e",
    "semantic_title": "sparsity-constrained optimal transport",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=mMNimwRb7Gr": {
    "title": "Turning the Curse of Heterogeneity in Federated Learning into a Blessing for Out-of-Distribution Detection",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "195d86d6b6a8420e9553fdbfc67cdfa4c87179aa",
    "semantic_title": "turning the curse of heterogeneity in federated learning into a blessing for out-of-distribution detection",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=j6zUzrapY3L": {
    "title": "DIFFormer: Scalable (Graph) Transformers Induced by Energy Constrained Diffusion",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "9b4f564e5d33625fa88fc4e1045e9d5681fa0cca",
    "semantic_title": "difformer: scalable (graph) transformers induced by energy constrained diffusion",
    "citation_count": 94,
    "authors": []
  },
  "https://openreview.net/forum?id=d3QNWD_pcFv": {
    "title": "Neural Lagrangian Schr\\\"{o}dinger Bridge: Diffusion Modeling for Population Dynamics",
    "volume": "spotlight",
    "abstract": "Population dynamics is the study of temporal and spatial variation in the size of populations of organisms and is a major part of population ecology. One of the main difficulties in analyzing population dynamics is that we can only obtain observation data with coarse time intervals from fixed-point observations due to experimental costs or measurement constraints. Recently, modeling population dynamics by using continuous normalizing flows (CNFs) and dynamic optimal transport has been proposed to infer the sample trajectories from a fixed-point observed population. While the sample behavior in CNFs is deterministic, the actual sample in biological systems moves in an essentially random yet directional manner. Moreover, when a sample moves from point A to point B in dynamical systems, its trajectory typically follows the principle of least action in which the corresponding action has the smallest possible value. To satisfy these requirements of the sample trajectories, we formulate the Lagrangian Schrödinger bridge (LSB) problem and propose to solve it approximately by modeling the advection-diffusion process with regularized neural SDE. We also develop a model architecture that enables faster computation of the loss function. Experimental results show that the proposed method can efficiently approximate the population-level dynamics even for high-dimensional data and that using the prior knowledge introduced by the Lagrangian enables us to estimate the sample-level dynamics with stochastic behavior",
    "checked": false,
    "id": "a48678440842cdf7681cb1560d5ad2971b9d4052",
    "semantic_title": "neural lagrangian schrödinger bridge",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=QC10RmRbZy9": {
    "title": "Loss Landscapes are All You Need: Neural Network Generalization Can Be Explained Without the Implicit Bias of Gradient Descent",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "e4f0ab1408504c3873a849d341ec63fbca899534",
    "semantic_title": "loss landscapes are all you need: neural network generalization can be explained without the implicit bias of gradient descent",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=h5OpjGd_lo6": {
    "title": "Self-Guided Noise-Free Data Generation for Efficient Zero-Shot Learning",
    "volume": "spotlight",
    "abstract": "There is a rising interest in further exploring the zero-shot learning potential of large pre-trained language models (PLMs). A new paradigm called data-generation-based zero-shot learning has achieved impressive success. In this paradigm, the synthesized data from the PLM acts as the carrier of knowledge, which is used to train a task-specific model with orders of magnitude fewer parameters than the PLM, achieving both higher performance and efficiency than prompt-based zero-shot learning methods on PLMs. The main hurdle of this approach is that the synthesized data from PLM usually contains a significant portion of low-quality samples. Fitting on such data will greatly hamper the performance of the task-specific model, making it unreliable for deployment. Previous methods remedy this issue mainly by filtering synthetic data using heuristic metrics(e.g., output confidence), or refining the data with the help of a human expert, which comes with excessive manual tuning or expensive costs. In this paper, we propose a novel noise-robust re-weighting framework SunGen to automatically construct high-quality data for zero-shot classification problems. Our framework features the ability to learn the sample weights indicating data quality without requiring any human annotation. We theoretically and empirically verify the ability of our method to help construct good-quality synthetic datasets. Notably, SunGen-LSTM yields a 9.8% relative improvement than the baseline on average accuracy across eight different established text classification tasks",
    "checked": true,
    "id": "6f7e03e4ccd26c762090e25dc5d2eb1e1f8c641d",
    "semantic_title": "self-guided noise-free data generation for efficient zero-shot learning",
    "citation_count": 51,
    "authors": []
  },
  "https://openreview.net/forum?id=aBWnqqsuot7": {
    "title": "D4FT: A Deep Learning Approach to Kohn-Sham Density Functional Theory",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "3991a235714f622aeaa2f1a5036398fcaf7d0fa2",
    "semantic_title": "d4ft: a deep learning approach to kohn-sham density functional theory",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=kPLzOfPfA2l": {
    "title": "Warping the Space: Weight Space Rotation for Class-Incremental Few-Shot Learning",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "a49d885d13d3aa763f0966acdb947708751a3750",
    "semantic_title": "warping the space: weight space rotation for class-incremental few-shot learning",
    "citation_count": 43,
    "authors": []
  },
  "https://openreview.net/forum?id=tYIMtogyee": {
    "title": "Pre-training via Denoising for Molecular Property Prediction",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "082aa5c92202719a5740c8e2edee65cc4bf3ccfa",
    "semantic_title": "pre-training via denoising for molecular property prediction",
    "citation_count": 132,
    "authors": []
  },
  "https://openreview.net/forum?id=-9PVqZ-IR_": {
    "title": "Martingale Posterior Neural Processes",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "f63d6770bc9a7ee235b791f839cfc1edfbfe2c9b",
    "semantic_title": "martingale posterior neural processes",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=bvpkw7UIRdU": {
    "title": "On the Usefulness of Embeddings, Clusters and Strings for Text Generation Evaluation",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "8ce3f729c24c50882d5e4fc2e3f7de882c99308a",
    "semantic_title": "on the usefulness of embeddings, clusters and strings for text generation evaluation",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=C-xa_D3oTj6": {
    "title": "DEP-RL: Embodied Exploration for Reinforcement Learning in Overactuated and Musculoskeletal Systems",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "04615a9955bce148aa7ba29e864389c26e10523a",
    "semantic_title": "dep-rl: embodied exploration for reinforcement learning in overactuated and musculoskeletal systems",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=PEgBEB74JjB": {
    "title": "The Symmetric Generalized Eigenvalue Problem as a Nash Equilibrium",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "63f11a67a80a6795f7c3ce18d2761a3f42faac1e",
    "semantic_title": "the symmetric generalized eigenvalue problem as a nash equilibrium",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n-bvaLSCC78": {
    "title": "EA-HAS-Bench: Energy-aware Hyperparameter and Architecture Search Benchmark",
    "volume": "spotlight",
    "abstract": "The energy consumption for training deep learning models is increasing at an alarming rate due to the growth of training data and model scale, resulting in a negative impact on carbon neutrality. Energy consumption is an especially pressing issue for AutoML algorithms because it usually requires repeatedly training large numbers of computationally intensive deep models to search for optimal configurations. This paper takes one of the most essential steps in developing energy-aware (EA) NAS methods, by providing a benchmark that makes EA-NAS research more reproducible and accessible. Specifically, we present the first large-scale energy-aware benchmark that allows studying AutoML methods to achieve better trade-offs between performance and search energy consumption, named EA-HAS-Bench. EA-HAS-Bench provides a large-scale architecture/hyperparameter joint search space, covering diversified configurations related to energy consumption. Furthermore, we propose a novel surrogate model specially designed for large joint search space, which proposes a Bezier curve-based model to predict learning curves with unlimited shape and length. Based on the proposed dataset, we new energy-aware AutoML method that arms existing AutoML algorithms to consider the search energy consumption, and our experiments show that the modified energy-aware AutoML methods achieve a better trade-off between energy consumption and model performance",
    "checked": true,
    "id": "4863d3546ee8cd591d2e18f4b1bfaf492de4ea3e",
    "semantic_title": "ea-has-bench: energy-aware hyperparameter and architecture search benchmark",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=WAgXmT8BeRj": {
    "title": "MARS: Meta-learning as Score Matching in the Function Space",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "a6b51458ebfa5160c0d292a413c93ec83a228ffd",
    "semantic_title": "mars: meta-learning as score matching in the function space",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=KDhFkA6MQsW": {
    "title": "Faster Gradient-Free Methods for Escaping Saddle Points",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "825e35abd5c40969132e6b643cde5e6cb4f92046",
    "semantic_title": "faster gradient-free methods for escaping saddle points",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=xjxUjHa_Wpa": {
    "title": "VA-DepthNet: A Variational Approach to Single Image Depth Prediction",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "4983b3555f3abf02c2bb00808f2b0d48881eaf32",
    "semantic_title": "va-depthnet: a variational approach to single image depth prediction",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=_CDixzkzeyb": {
    "title": "Prompt-to-Prompt Image Editing with Cross-Attention Control",
    "volume": "spotlight",
    "abstract": "",
    "checked": false,
    "id": "04e541391e8dce14d099d00fb2c21dbbd8afe87f",
    "semantic_title": "prompt-to-prompt image editing with cross attention control",
    "citation_count": 1898,
    "authors": []
  },
  "https://openreview.net/forum?id=3lge0p5o-M-": {
    "title": "DiffEdit: Diffusion-based semantic image editing with mask guidance",
    "volume": "spotlight",
    "abstract": "Image generation has recently seen tremendous advances, with diffusion models allowing to synthesize convincing images for a large variety of text prompts. In this article, we propose DiffEdit, a method to take advantage of text-conditioned diffusion models for the task of semantic image editing, where the goal is to edit an image based on a text query. Semantic image editing is an extension of image generation, with the additional constraint that the generated image should be as similar as possible to a given input image. Current editing methods based on diffusion models usually require to provide a mask, making the task much easier by treating it as a conditional inpainting task. In contrast, our main contribution is able to automatically generate a mask highlighting regions of the input image that need to be edited, by contrasting predictions of a diffusion model conditioned on different text prompts. Moreover, we rely on latent inference to preserve content in those regions of interest and show excellent synergies with mask-based diffusion. DiffEdit achieves state-of-the-art editing performance on ImageNet. In addition, we evaluate semantic image editing in more challenging settings, using images from the COCO dataset as well as text-based generated images",
    "checked": true,
    "id": "064ccebc03d3afabaae30fe29a457c1cfcdff7e3",
    "semantic_title": "diffedit: diffusion-based semantic image editing with mask guidance",
    "citation_count": 543,
    "authors": []
  },
  "https://openreview.net/forum?id=JTGimap_-F": {
    "title": "Rarity Score : A New Metric to Evaluate the Uncommonness of Synthesized Images",
    "volume": "spotlight",
    "abstract": "Evaluation metrics in image synthesis play a key role to measure performances of generative models. However, most metrics mainly focus on image fidelity. Existing diversity metrics are derived by comparing distributions, and thus they cannot quantify the diversity or rarity degree of each generated image. In this work, we propose a new evaluation metric, called `rarity score', to measure both image-wise uncommonness and model-wise diversified generation performance. We first show empirical observation that typical samples are close to each other and distinctive samples are far from each other in nearest-neighbor distances on latent spaces represented by feature extractor networks such as VGG16. We then show that one can effectively filter typical or distinctive samples with the proposed metric. We also use our metric to demonstrate that the extent to which different generative models produce rare images can be effectively compared. Further, our metric can be used to compare rarities between datasets that share the same concept such as CelebA-HQ and FFHQ. Finally, we analyze the use of metrics in different designs of feature extractors to better understand the relationship between feature spaces and resulting high-rarity images. Code will be publicly available for the research community",
    "checked": true,
    "id": "d181656b2e40f17c34b04d6ee69a3015e2e56479",
    "semantic_title": "rarity score : a new metric to evaluate the uncommonness of synthesized images",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=09hVcSDkea": {
    "title": "Corrupted Image Modeling for Self-Supervised Visual Pre-Training",
    "volume": "spotlight",
    "abstract": "We introduce Corrupted Image Modeling (CIM) for self-supervised visual pre-training. CIM uses an auxiliary generator with a small trainable BEiT to corrupt the input image instead of using artificial [MASK] tokens, where some patches are randomly selected and replaced with plausible alternatives sampled from the BEiT output distribution. Given this corrupted image, an enhancer network learns to either recover all the original image pixels, or predict whether each visual token is replaced by a generator sample or not. The generator and the enhancer are simultaneously trained and synergistically updated. After pre-training, the enhancer can be used as a high-capacity visual encoder for downstream tasks. CIM is a general and flexible visual pre-training framework that is suitable for various network architectures. For the first time, CIM demonstrates that both ViT and CNN can learn rich visual representations using a unified, non-Siamese framework. Experimental results show that our approach achieves compelling results in vision benchmarks, such as ImageNet classification and ADE20K semantic segmentation",
    "checked": true,
    "id": "9d054fa2ce6b0b3160aa52712f44f7967057205b",
    "semantic_title": "corrupted image modeling for self-supervised visual pre-training",
    "citation_count": 90,
    "authors": []
  },
  "https://openreview.net/forum?id=sd90a2ytrt": {
    "title": "Semi-Implicit Variational Inference via Score Matching",
    "volume": "spotlight",
    "abstract": "Semi-implicit variational inference (SIVI) greatly enriches the expressiveness of variational families by considering implicit variational distributions defined in a hierarchical manner. However, due to the intractable densities of variational distributions, current SIVI approaches often use surrogate evidence lower bounds (ELBOs) or employ expensive inner-loop MCMC runs for unbiased ELBOs for training. In this paper, we propose SIVI-SM, a new method for SIVI based on an alternative training objective via score matching. Leveraging the hierarchical structure of semi-implicit variational families, the score matching objective allows a minimax formulation where the intractable variational densities can be naturally handled with denoising score matching. We show that SIVI-SM closely matches the accuracy of MCMC and outperforms ELBO-based SIVI methods in a variety of Bayesian inference tasks",
    "checked": true,
    "id": "aa02c9b676c9aedc8d6693a73ac7c7af02d182af",
    "semantic_title": "semi-implicit variational inference via score matching",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=fxjzKOdw9wb": {
    "title": "Exploring Temporally Dynamic Data Augmentation for Video Recognition",
    "volume": "spotlight",
    "abstract": "Data augmentation has recently emerged as an essential component of modern training recipes for visual recognition tasks. However, data augmentation for video recognition has been rarely explored despite its effectiveness. Few existing augmentation recipes for video recognition naively extend the image augmentation methods by applying the same operations to the whole video frames. Our main idea is that the magnitude of augmentation operations for each frame needs to be changed over time to capture the real-world video's temporal variations. These variations should be generated as diverse as possible using fewer additional hyper-parameters during training. Through this motivation, we propose a simple yet effective video data augmentation framework, DynaAugment. The magnitude of augmentation operations on each frame is changed by an effective mechanism, Fourier Sampling that parameterizes diverse, smooth, and realistic temporal variations. DynaAugment also includes an extended search space suitable for video for automatic data augmentation methods. DynaAugment experimentally demonstrates that there are additional performance rooms to be improved from static augmentations on diverse video models. Specifically, we show the effectiveness of DynaAugment on various video datasets and tasks: large-scale video recognition (Kinetics-400 and Something-Something-v2), small-scale video recognition (UCF-101 and HMDB-51), fine-grained video recognition (Diving-48 and FineGym), video action segmentation on Breakfast, video action localization on THUMOS'14, and video object detection on MOT17Det",
    "checked": true,
    "id": "5ca0180552cc0e1c74dd755ff11851bde207c42a",
    "semantic_title": "exploring temporally dynamic data augmentation for video recognition",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=dqITIpZ5Z4b": {
    "title": "A General Framework for Sample-Efficient Function Approximation in Reinforcement Learning",
    "volume": "spotlight",
    "abstract": "With the increasing need for handling large state and action spaces, general function approximation has become a key technique in reinforcement learning (RL). In this paper, we propose a general framework that unifies model-based and model-free RL, and an Admissible Bellman Characterization (ABC) class that subsumes nearly all Markov decision process (MDP) models in the literature for tractable RL. We propose a novel estimation function with decomposable structural properties for optimization-based exploration and the functional Eluder dimension as a complexity measure of the ABC class. Under our framework, a new sample-efficient algorithm namely OPtimization-based ExploRation with Approximation (OPERA) is proposed, achieving regret bounds that match or improve over the best-known results for a variety of MDP models. In particular, for MDPs with low Witness rank, under a slightly stronger assumption, OPERA improves the state-of-the-art sample complexity results by a factor of $dH$. Our framework provides a generic interface to design and analyze new RL models and algorithms",
    "checked": true,
    "id": "6aafc286baae51ecce2ee54330eb8f26d8d44867",
    "semantic_title": "a general framework for sample-efficient function approximation in reinforcement learning",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=bBpT6dEjeRG": {
    "title": "Adversarial Attacks on Adversarial Bandits",
    "volume": "spotlight",
    "abstract": "We study a security threat to adversarial multi-armed bandit, in which an attacker perturbs the loss or reward signal to control the behavior of the victim bandit player. We show that the attacker is able to mislead any no-regret adversarial bandit algorithm into selecting a suboptimal target action in every but sublinear (T−o(T )) number of rounds, while incurring only sublinear (o(T)) cumulative attack cost. This result implies critical security concern in real-world bandit-based systems, e.g., in online recommendation, an attacker might be able to hijack the recommender system and promote a desired product. Our proposed attack algorithms require knowledge of only the regret rate, thus are agnostic to the concrete bandit algorithm employed by the victim player. We also derived a theoretical lower bound on the cumulative attack cost that any victim-agnostic attack algorithm must incur. The lower bound matches the upper bound achieved by our attack, which shows that our attack is asymptotically optimal",
    "checked": true,
    "id": "b2eb421f668be678a9c4314223717b86a255d336",
    "semantic_title": "adversarial attacks on adversarial bandits",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=QVcDQJdFTG": {
    "title": "Ensuring DNN Solution Feasibility for Optimization Problems with Linear Constraints",
    "volume": "spotlight",
    "abstract": "We propose preventive learning as the first framework to guarantee Deep Neural Network (DNN) solution feasibility for optimization problems with linear constraints without post-processing, upon satisfying a mild condition on constraint calibration. Without loss of generality, we focus on problems with only inequality constraints. We systematically calibrate the inequality constraints used in training, thereby anticipating DNN prediction errors and ensuring the obtained solutions remain feasible. We characterize the calibration rate and a critical DNN size, based on which we can directly construct a DNN with provable solution feasibility guarantee. We further propose an Adversarial-Sample Aware training algorithm to improve its optimality performance. We apply the framework to develop DeepOPF+ for solving essential DC optimal power flow problems in grid operation. Simulation results over IEEE test cases show that it outperforms existing strong DNN baselines in ensuring 100\\% feasibility and attaining consistent optimality loss (<0.19%) and speedup (up to x228) in both light-load and heavy-load regimes, as compared to a state-of-the-art solver. We also apply our framework to a non-convex problem and show its performance advantage over existing schemes",
    "checked": true,
    "id": "57f3e6511cd0b6c95904ed2f17f81439636c9892",
    "semantic_title": "ensuring dnn solution feasibility for optimization problems with linear constraints",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=FKXVK9dyMM": {
    "title": "LightGCL: Simple Yet Effective Graph Contrastive Learning for Recommendation",
    "volume": "spotlight",
    "abstract": "Graph neural network (GNN) is a powerful learning approach for graph-based recommender systems. Recently, GNNs integrated with contrastive learning have shown superior performance in recommendation with their data augmentation schemes, aiming at dealing with highly sparse data. Despite their success, most existing graph contrastive learning methods either perform stochastic augmentation (e.g., node/edge perturbation) on the user-item interaction graph, or rely on the heuristic-based augmentation techniques (e.g., user clustering) for generating contrastive views. We argue that these methods cannot well preserve the intrinsic semantic structures and are easily biased by the noise perturbation. In this paper, we propose a simple yet effective graph contrastive learning paradigm LightGCL that mitigates these issues impairing the generality and robustness of CL-based recommenders. Our model exclusively utilizes singular value decomposition for contrastive augmentation, which enables the unconstrained structural refinement with global collaborative relation modeling. Experiments conducted on several benchmark datasets demonstrate the significant improvement in performance of our model over the state-of-the-arts. Further analyses demonstrate the superiority of LightGCL's robustness against data sparsity and popularity bias. The source code of our model is available at https://github.com/HKUDS/LightGCL",
    "checked": true,
    "id": "1b06e507f7e2af3dc7cf28086ac34083709c3150",
    "semantic_title": "lightgcl: simple yet effective graph contrastive learning for recommendation",
    "citation_count": 241,
    "authors": []
  },
  "https://openreview.net/forum?id=j9m-mVnndbm": {
    "title": "MIMT: Masked Image Modeling Transformer for Video Compression",
    "volume": "spotlight",
    "abstract": "Deep learning video compression outperforms its hand-craft counterparts with enhanced flexibility and capacity. One key component of the learned video codec is the autoregressive entropy model conditioned on spatial and temporal priors. Operating autoregressive on raster scanning order naively treats the context as unidirectional. This is neither efficient nor optimal, considering that conditional information probably locates at the end of the sequence. We thus introduce an entropy model based on a masked image modeling transformer (MIMT) to learn the spatial-temporal dependencies. Video frames are first encoded into sequences of tokens and then processed with the transformer encoder as priors. The transformer decoder learns the probability mass functions (PMFs) \\emph{conditioned} on the priors and masked inputs. Then it is capable of selecting optimal decoding orders without a fixed direction. During training, MIMT aims to predict the PMFs of randomly masked tokens by attending to tokens in all directions. This allows MIMT to capture the temporal dependencies from encoded priors and the spatial dependencies from the unmasked tokens, i.e., decoded tokens. At inference time, the model begins with generating PMFs of all masked tokens in parallel and then decodes the frame iteratively from the previously-selected decoded tokens (i.e., with high confidence). In addition, we improve the overall performance with more techniques, e.g., manifold conditional priors accumulating a long range of information, shifted window attention to reduce complexity. Extensive experiments demonstrate the proposed MIMT framework equipped with the new transformer entropy model achieves state-of-the-art performance on HEVC, UVG, and MCL-JCV datasets, generally outperforming the VVC in terms of PSNR and SSIM",
    "checked": true,
    "id": "1054668810e6925ed0530aa0978430f73dde937b",
    "semantic_title": "mimt: masked image modeling transformer for video compression",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=COZDy0WYGg": {
    "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
    "volume": "spotlight",
    "abstract": "State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4$\\times$ faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark",
    "checked": true,
    "id": "5a77b508302771fc083bf24e0bcda8553c9b5421",
    "semantic_title": "hungry hungry hippos: towards language modeling with state space models",
    "citation_count": 438,
    "authors": []
  },
  "https://openreview.net/forum?id=4fZc_79Lrqs": {
    "title": "ACMP: Allen-Cahn Message Passing with Attractive and Repulsive Forces for Graph Neural Networks",
    "volume": "spotlight",
    "abstract": "Neural message passing is a basic feature extraction unit for graph-structured data considering neighboring node features in network propagation from one layer to the next. We model such process by an interacting particle system with attractive and repulsive forces and the Allen-Cahn force arising in the modeling of phase transition. The dynamics of the system is a reaction-diffusion process which can separate particles without blowing up. This induces an Allen-Cahn message passing (ACMP) for graph neural networks where the numerical iteration for the particle system solution constitutes the message passing propagation. ACMP which has a simple implementation with a neural ODE solver can propel the network depth up to one hundred of layers with theoretically proven strictly positive lower bound of the Dirichlet energy. It thus provides a deep model of GNNs circumventing the common GNN problem of oversmoothing. GNNs with ACMP achieve state of the art performance for real-world node classification tasks on both homophilic and heterophilic datasets. Codes are available at https://github.com/ykiiiiii/ACMP",
    "checked": true,
    "id": "fea03f338db94581753f9588bd07937996aee63d",
    "semantic_title": "acmp: allen-cahn message passing with attractive and repulsive forces for graph neural networks",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=cFuMmbWiN6": {
    "title": "Relational Attention: Generalizing Transformers for Graph-Structured Tasks",
    "volume": "spotlight",
    "abstract": "Transformers flexibly operate over sets of real-valued vectors representing task-specific entities and their attributes, where each vector might encode one word-piece token and its position in a sequence, or some piece of information that carries no position at all. As set processors, transformers are at a disadvantage in reasoning over more general graph-structured data where nodes represent entities and edges represent relations between entities. To address this shortcoming, we generalize transformer attention to consider and update edge vectors in each transformer layer. We evaluate this relational transformer on a diverse array of graph-structured tasks, including the large and challenging CLRS Algorithmic Reasoning Benchmark. There, it dramatically outperforms state-of-the-art graph neural networks expressly designed to reason over graph-structured data. Our analysis demonstrates that these gains are attributable to relational attention's inherent ability to leverage the greater expressivity of graphs over sets",
    "checked": true,
    "id": "a84b33c93222d525abcee154698aa937e110fb77",
    "semantic_title": "relational attention: generalizing transformers for graph-structured tasks",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=99RpBVpLiX": {
    "title": "Distilling Model Failures as Directions in Latent Space",
    "volume": "spotlight",
    "abstract": "Existing methods for isolating hard subpopulations and spurious correlations in datasets often require human intervention. This can make these methods labor-intensive and dataset-specific. To address these shortcomings, we present a scalable method for automatically distilling a model's failure modes. Specifically, we harness linear classifiers to identify consistent error patterns, and, in turn, induce a natural representation of these failure modes as directions within the feature space. We demonstrate that this framework allows us to discover and automatically caption challenging subpopulations within the training dataset. Moreover, by combining our framework with off-the-shelf diffusion models, we can generate images that are especially challenging for the analyzed model, and thus can be used to perform synthetic data augmentation that helps remedy the model's failure modes",
    "checked": true,
    "id": "b21d7d2f52a06380a18e272863efebf77dc69f90",
    "semantic_title": "distilling model failures as directions in latent space",
    "citation_count": 91,
    "authors": []
  },
  "https://openreview.net/forum?id=8qjSA5QACb40": {
    "title": "Combinatorial-Probabilistic Trade-Off: P-Values of Community Properties Test in the Stochastic Block Models",
    "volume": "spotlight",
    "abstract": "We propose an inferential framework testing the general community combinatorial properties of the stochastic block model. We aim to test the hypothesis on whether a certain community property is satisfied, e.g., whether a given set of nodes belong to the same community, and provide p-values for uncertainty quantification. Our framework is applicable to all symmetric community properties. To ease the challenges caused by the combinatorial nature of community properties, we develop a novel shadowing bootstrap method. Utilizing the symmetry, our method can find a shadowing representative of the true assignment and the number of tested assignments in the alternative is largely reduced. In theory, we introduce a combinatorial distance between two community classes and show a combinatorial-probabilistic trade-off phenomenon. Our test is honest as long as the product of the combinatorial distance between two communities and the probabilistic distance between two connection probabilities is sufficiently large. Besides, we show that such trade-off also exists in the information-theoretic lower bound. We also implement numerical experiments to show the validity of our method",
    "checked": false,
    "id": "04685cd6cb60e2b0de16410eab4d0e1a7c7e1692",
    "semantic_title": "combinatorial-probabilistic trade-off: p-values of community property test in the stochastic block models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=yYbhKqdi7Hz": {
    "title": "Continuized Acceleration for Quasar Convex Functions in Non-Convex Optimization",
    "volume": "spotlight",
    "abstract": "Quasar convexity is a condition that allows some first-order methods to efficiently minimize a function even when the optimization landscape is non-convex. Previous works develop near-optimal accelerated algorithms for minimizing this class of functions, however, they require a subroutine of binary search which results in multiple calls to gradient evaluations in each iteration, and consequently the total number of gradient evaluations does not match a known lower bound. In this work, we show that a recently proposed continuized Nesterov acceleration can be applied to minimizing quasar convex functions and achieves the optimal bound with a high probability. Furthermore, we find that the objective functions of training generalized linear models (GLMs) satisfy quasar convexity, which broadens the applicability of the relevant algorithms, while known practical examples of quasar convexity in non-convex learning are sparse in the literature. We also show that if a smooth and one-point strongly convex, Polyak-Lojasiewicz, or quadratic-growth function satisfies quasar convexity, then attaining an accelerated linear rate for minimizing the function is possible under certain conditions, while acceleration is not known in general for these classes of functions",
    "checked": true,
    "id": "d1d6c791badca60b285163ce3342aec135b519fb",
    "semantic_title": "continuized acceleration for quasar convex functions in non-convex optimization",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=8sSnD78NqTN": {
    "title": "Learning Soft Constraints From Constrained Expert Demonstrations",
    "volume": "spotlight",
    "abstract": "Inverse reinforcement learning (IRL) methods assume that the expert data is generated by an agent optimizing some reward function. However, in many settings, the agent may optimize a reward function subject to some constraints, where the constraints induce behaviors that may be otherwise difficult to express with just a reward function. We consider the setting where the reward function is given, and the constraints are unknown, and propose a method that is able to recover these constraints satisfactorily from the expert data. While previous work has focused on recovering hard constraints, our method can recover cumulative soft constraints that the agent satisfies on average per episode. In IRL fashion, our method solves this problem by adjusting the constraint function iteratively through a constrained optimization procedure, until the agent behavior matches the expert behavior. We demonstrate our approach on synthetic environments, robotics environments and real world highway driving scenarios",
    "checked": true,
    "id": "bec89199faf9a7aa6b1605f1d506c8d215f036f1",
    "semantic_title": "learning soft constraints from constrained expert demonstrations",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=cDYRS5iZ16f": {
    "title": "Learning to Grow Pretrained Models for Efficient Transformer Training",
    "volume": "spotlight",
    "abstract": "Scaling transformers has led to significant breakthroughs in many domains, leading to a paradigm in which larger versions of existing models are trained and released on a periodic basis. New instances of such models are typically trained completely from scratch, despite the fact that they are often just scaled-up versions of their smaller counterparts. How can we use the implicit knowledge in the parameters of smaller, extant models to enable faster training of newer, larger models? This paper describes an approach for accelerating transformer training by learning to grow pretrained transformers, where we learn to linearly map the parameters of the smaller model to initialize the larger model. For tractable learning, we factorize the linear transformation as a composition of (linear) width- and depth-growth operators, and further employ a Kronecker factorization of these growth operators to encode architectural knowledge. Extensive experiments across both language and vision transformers demonstrate that our learned Linear Growth Operator (LiGO) can save up to 50% computational cost of training from scratch, while also consistently outperforming strong baselines that also reuse smaller pretrained models to initialize larger models",
    "checked": true,
    "id": "a7a40b35b6f37c554f1c5c2038892ed70c693a64",
    "semantic_title": "learning to grow pretrained models for efficient transformer training",
    "citation_count": 59,
    "authors": []
  },
  "https://openreview.net/forum?id=hQwb-lbM6EL": {
    "title": "InCoder: A Generative Model for Code Infilling and Synthesis",
    "volume": "spotlight",
    "abstract": "Code is seldom written in a single left-to-right pass and is instead repeatedly edited and refined. We introduce InCoder, a unified generative model that can perform program synthesis (via left-to-right generation) as well as editing (via masking and infilling). InCoder is trained to generate code files from a large corpus of permissively licensed code, where regions of code have been randomly masked and moved to the end of each file, allowing code infilling with bidirectional context. Our model is the first large generative code model that is able to infill arbitrary regions of code, which we evaluate in a zero-shot setting on challenging tasks such as type inference, comment generation, and variable re-naming. We find that the ability to condition on bidirectional context substantially improves performance on these tasks, while still performing comparably on standard program synthesis benchmarks in comparison to left-to-right only models pretrained at similar scale. Our models and code will be publicly released",
    "checked": true,
    "id": "5288b9f3a9f575543f44c39e1d3b78b3ca4c99da",
    "semantic_title": "incoder: a generative model for code infilling and synthesis",
    "citation_count": 692,
    "authors": []
  },
  "https://openreview.net/forum?id=E01k9048soZ": {
    "title": "UNIFIED-IO: A Unified Model for Vision, Language, and Multi-modal Tasks",
    "volume": "spotlight",
    "abstract": "We propose Unified-IO, a model that performs a large variety of AI tasks spanning classical computer vision tasks, including pose estimation, object detection, depth estimation and image generation, vision-and-language tasks such as region captioning and referring expression, to natural language processing tasks such as question answering and paraphrasing. Developing a single unified model for such a large variety of tasks poses unique challenges due to the heterogeneous inputs and outputs pertaining to each task, including RGB images, per-pixel maps, binary masks, bounding boxes, and language. We achieve this unification by homogenizing every supported input and output into a sequence of discrete vocabulary tokens. This common representation across all tasks allows us to train a single transformer-based architecture, jointly on over 90 diverse datasets in the vision and language fields. Unified-IO is the first model capable of performing all 7 tasks on the GRIT benchmark and produces strong results across 16 diverse benchmarks like NYUv2-Depth, ImageNet, VQA2.0, OK-VQA, Swig, VizWizGround, BoolQ, and SciTail, with no task-specific fine-tuning. Code and pre-trained models will be made publicly available",
    "checked": true,
    "id": "8b5eab31e1c5689312fff3181a75bfbf5c13e51c",
    "semantic_title": "unified-io: a unified model for vision, language, and multi-modal tasks",
    "citation_count": 430,
    "authors": []
  },
  "https://openreview.net/forum?id=3k5CUGDLNdd": {
    "title": "Benchmarking Offline Reinforcement Learning on Real-Robot Hardware",
    "volume": "spotlight",
    "abstract": "Learning policies from previously recorded data is a promising direction for real-world robotics tasks, as online learning is often infeasible. Dexterous manipulation in particular remains an open problem in its general form. The combination of offline reinforcement learning with large diverse datasets, however, has the potential to lead to a breakthrough in this challenging domain analogously to the rapid progress made in supervised learning in recent years. To coordinate the efforts of the research community toward tackling this problem, we propose a benchmark including: i) a large collection of data for offline learning from a dexterous manipulation platform on two tasks, obtained with capable RL agents trained in simulation; ii) the option to execute learned policies on a real-world robotic system and a simulation for efficient debugging. We evaluate prominent open-sourced offline reinforcement learning algorithms on the datasets and provide a reproducible experimental setup for offline reinforcement learning on real systems",
    "checked": true,
    "id": "d91591feb96936237167c9e569b8f74e0b2bfcc3",
    "semantic_title": "benchmarking offline reinforcement learning on real-robot hardware",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=RgUPdudkWlN": {
    "title": "CUDA: Curriculum of Data Augmentation for Long-tailed Recognition",
    "volume": "spotlight",
    "abstract": "Class imbalance problems frequently occur in real-world tasks, and conventional deep learning algorithms are well known for performance degradation on imbalanced training datasets. To mitigate this problem, many approaches have aimed to balance among given classes by re-weighting or re-sampling training samples. These re-balancing methods increase the impact of minority classes and reduce the influence of majority classes on the output of models. However, the extracted representations may be of poor quality owing to the limited number of minority samples. To handle this restriction, several methods have been developed that increase the representations of minority samples by leveraging the features of the majority samples. Despite extensive recent studies, no deep analysis has been conducted on determination of classes to be augmented and strength of augmentation has been conducted. In this study, we first investigate the correlation between the degree of augmentation and class-wise performance, and find that the proper degree of augmentation must be allocated for each class to mitigate class imbalance problems. Motivated by this finding, we propose a simple and efficient novel curriculum, which is designed to find the appropriate per-class strength of data augmentation, called CUDA: CUrriculum of Data Augmentation for long-tailed recognition. CUDA can simply be integrated into existing long-tailed recognition methods. We present the results of experiments showing that CUDA effectively achieves better generalization performance compared to the state-of-the-art method on various imbalanced datasets such as CIFAR-100-LT, ImageNet-LT, and iNaturalist 2018",
    "checked": true,
    "id": "fe47aebdcad5a934dfd44f383c2d0f314699b934",
    "semantic_title": "cuda: curriculum of data augmentation for long-tailed recognition",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=5ktFNz_pJLK": {
    "title": "Learning to Estimate Shapley Values with Vision Transformers",
    "volume": "spotlight",
    "abstract": "Transformers have become a default architecture in computer vision, but understanding what drives their predictions remains a challenging problem. Current explanation approaches rely on attention values or input gradients, but these provide a limited view of a model's dependencies. Shapley values offer a theoretically sound alternative, but their computational cost makes them impractical for large, high-dimensional models. In this work, we aim to make Shapley values practical for vision transformers (ViTs). To do so, we first leverage an attention masking approach to evaluate ViTs with partial information, and we then develop a procedure to generate Shapley value explanations via a separate, learned explainer model. Our experiments compare Shapley values to many baseline methods (e.g., attention rollout, GradCAM, LRP), and we find that our approach provides more accurate explanations than existing methods for ViTs",
    "checked": true,
    "id": "a71b9fc58d8df11d4ae725568308fb5ace3da24a",
    "semantic_title": "learning to estimate shapley values with vision transformers",
    "citation_count": 43,
    "authors": []
  },
  "https://openreview.net/forum?id=Iuubb9W6Jtk": {
    "title": "A framework for benchmarking Class-out-of-distribution detection and its application to ImageNet",
    "volume": "spotlight",
    "abstract": "When deployed for risk-sensitive tasks, deep neural networks must be able to detect instances with labels from outside the distribution for which they were trained. In this paper we present a novel framework to benchmark the ability of image classifiers to detect class-out-of-distribution instances (i.e., instances whose true labels do not appear in the training distribution) at various levels of detection difficulty. We apply this technique to ImageNet, and benchmark 525 pretrained, publicly available, ImageNet-1k classifiers. The code for generating a benchmark for any ImageNet-1k classifier, along with the benchmarks prepared for the above-mentioned 525 models is available at https://github.com/mdabbah/COOD_benchmarking. The usefulness of the proposed framework and its advantage over alternative existing benchmarks is demonstrated by analyzing the results obtained for these models, which reveals numerous novel observations including: (1) knowledge distillation consistently improves class-out-of-distribution (C-OOD) detection performance; (2) a subset of ViTs performs better C-OOD detection than any other model; (3) the language–-vision CLIP model achieves good zero-shot detection performance, with its best instance outperforming 96% of all other models evaluated; (4) accuracy and in-distribution ranking are positively correlated to C-OOD detection; and (5) we compare various confidence functions for C-OOD detection. Our companion paper, also published in ICLR 2023 (What Can We Learn From The Selective Prediction And Uncertainty Estimation Performance Of 523 Imagenet Classifiers), examines the uncertainty estimation performance (ranking, calibration, and selective prediction performance) of these classifiers in an in-distribution setting",
    "checked": true,
    "id": "85e3375ff47be23ad3cee66f379284c416a57e2c",
    "semantic_title": "a framework for benchmarking class-out-of-distribution detection and its application to imagenet",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=vDFA1tpuLvk": {
    "title": "Retrieval-based Controllable Molecule Generation",
    "volume": "spotlight",
    "abstract": "Generating new molecules with specified chemical and biological properties via generative models has emerged as a promising direction for drug discovery. However, existing methods require extensive training/fine-tuning with a large dataset, often unavailable in real-world generation tasks. In this work, we propose a new retrieval-based framework for controllable molecule generation. We use a small set of exemplar molecules, i.e., those that (partially) satisfy the design criteria, to steer the pre-trained generative model towards synthesizing molecules that satisfy the given design criteria. We design a retrieval mechanism that retrieves and fuses the exemplar molecules with the input molecule, which is trained by a new self-supervised objective that predicts the nearest neighbor of the input molecule. We also propose an iterative refinement process to dynamically update the generated molecules and retrieval database for better generalization. Our approach is agnostic to the choice of generative models and requires no task-specific fine-tuning. On various tasks ranging from simple design criteria to a challenging real-world scenario for designing lead compounds that bind to the SARS-CoV-2 main protease, we demonstrate our approach extrapolates well beyond the retrieval database, and achieves better performance and wider applicability than previous methods",
    "checked": true,
    "id": "ea4ff70180e4563fe12b51dfce0f57b36639aacf",
    "semantic_title": "retrieval-based controllable molecule generation",
    "citation_count": 43,
    "authors": []
  },
  "https://openreview.net/forum?id=_s1N-DnxdyT": {
    "title": "Stochastic Multi-Person 3D Motion Forecasting",
    "volume": "spotlight",
    "abstract": "This paper aims to deal with the ignored real-world complexities in prior work on human motion forecasting, emphasizing the social properties of multi-person motion, the diversity of motion and social interactions, and the complexity of articulated motion. To this end, we introduce a novel task of stochastic multi-person 3D motion forecasting. We propose a dual-level generative modeling framework that separately models independent individual motion at the local level and social interactions at the global level. Notably, this dual-level modeling mechanism can be achieved within a shared generative model, through introducing learnable latent codes that represent intents of future motion and switching the codes' modes of operation at different levels. Our framework is general; we instantiate it with different generative models, including generative adversarial networks and diffusion models, and various multi-person forecasting models. Extensive experiments on CMU-Mocap, MuPoTS-3D, and SoMoF benchmarks show that our approach produces diverse and accurate multi-person predictions, significantly outperforming the state of the art",
    "checked": true,
    "id": "800eb5ca1b94097837fd768879944c558fafdb8c",
    "semantic_title": "stochastic multi-person 3d motion forecasting",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=Q-UHqMorzil": {
    "title": "Sign and Basis Invariant Networks for Spectral Graph Representation Learning",
    "volume": "spotlight",
    "abstract": "We introduce SignNet and BasisNet---new neural architectures that are invariant to two key symmetries displayed by eigenvectors: (i) sign flips, since if v is an eigenvector then so is -v; and (ii) more general basis symmetries, which occur in higher dimensional eigenspaces with infinitely many choices of basis eigenvectors. We prove that under certain conditions our networks are universal, i.e., they can approximate any continuous function of eigenvectors with the desired invariances. When used with Laplacian eigenvectors, our networks are provably more expressive than existing spectral methods on graphs; for instance, they subsume all spectral graph convolutions, certain spectral graph invariants, and previously proposed graph positional encodings as special cases. Experiments show that our networks significantly outperform existing baselines on molecular graph regression, learning expressive graph representations, and learning neural fields on triangle meshes. Our code is available at https://github.com/cptq/SignNet-BasisNet",
    "checked": true,
    "id": "eb984b142db9965b10a3b5ae5813eeb3e0f6e676",
    "semantic_title": "sign and basis invariant networks for spectral graph representation learning",
    "citation_count": 153,
    "authors": []
  },
  "https://openreview.net/forum?id=7C9aRX2nBf2": {
    "title": "Sequential Latent Variable Models for Few-Shot High-Dimensional Time-Series Forecasting",
    "volume": "spotlight",
    "abstract": "Modern applications increasingly require learning and forecasting latent dynamics from high-dimensional time-series. Compared to univariate time-series forecasting, this adds a new challenge of reasoning about the latent dynamics of an unobserved abstract state. Sequential latent variable models (LVMs) present an attractive solution, although existing works either struggle with long-term forecasting or have difficulty learning across diverse dynamics. In this paper, we first present a conceptual framework of sequential LVMs to unify existing works, contrast their fundamental limitations, and identify an intuitive solution to long-term forecasting for diverse dynamics via meta-learning. We then present the first framework of few-shot forecasting for high-dimensional time-series: instead of learning a single dynamic function, we leverage data of diverse dynamics and learn to adapt latent dynamic functions to few-shot support series. This is realized via Bayesian meta-learning underpinned by: 1) a latent dynamic function conditioned on knowledge derived from few-shot support series, and 2) a meta-model that learns to extract such dynamic-specific knowledge via feed-forward embedding of support set. We compared the presented framework with a comprehensive set of baseline models trained 1) globally on the large meta-training set with diverse dynamics, and 2) individually on single dynamics, both with and without fine-tuning to k-shot support series used by the meta-models. We demonstrated that the presented framework is agnostic to the latent dynamic function of choice and, at meta-test time, is able to forecast for new dynamics given variable-shot of support series",
    "checked": true,
    "id": "72ce406751efe050681eecc3862c5681f7374aef",
    "semantic_title": "sequential latent variable models for few-shot high-dimensional time-series forecasting",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=XomEU3eNeSQ": {
    "title": "Code Translation with Compiler Representations",
    "volume": "spotlight",
    "abstract": "In this paper, we leverage low-level compiler intermediate representations (IR) code translation. Traditional transpilers rely on syntactic information and handcrafted rules, which limits their applicability and produces unnatural-looking code. Applying neural machine translation (NMT) approaches to code has successfully broadened the set of programs on which one can get a natural-looking translation. However, they treat the code as sequences of text tokens, and still do not differentiate well enough between similar pieces of code which have different semantics in different languages. The consequence is low quality translation, reducing the practicality of NMT, and stressing the need for an approach significantly increasing its accuracy. Here we propose to augment code translation with IRs, specifically LLVM IR, with results on the C++, Java, Rust, and Go languages. Our method improves upon the state of the art for unsupervised code translation, increasing the number of correct translations by 11% on average, and up to 79% for the Java → Rust pair with greedy decoding. With beam search, it increases the number of correct translations by 5.5% in average. We extend previous test sets for code translation, by adding hundreds of Go and Rust functions. Additionally, we train models with high performance on the problem of IR decompilation, generating programming source code from IR, and study using IRs as intermediary pivot for translation",
    "checked": true,
    "id": "593edb7f66bf6ec3eef943691c18aec9f976bc51",
    "semantic_title": "code translation with compiler representations",
    "citation_count": 66,
    "authors": []
  },
  "https://openreview.net/forum?id=zDiHoIWa0q1": {
    "title": "Omnigrok: Grokking Beyond Algorithmic Data",
    "volume": "spotlight",
    "abstract": "Grokking, the unusual phenomenon for algorithmic datasets where generalization happens long after overfitting the training data, has remained elusive. We aim to understand grokking by analyzing the loss landscapes of neural networks, identifying the mismatch between training and test losses as the cause for grokking. We refer to this as the \"LU mechanism\" because training and test losses (against model weight norm) typically resemble \"L\" and \"U\", respectively. This simple mechanism can nicely explain many aspects of grokking: data size dependence, weight decay dependence, the emergence of representations, etc. Guided by the intuitive picture, we are able to induce grokking on tasks involving images, language and molecules, although the grokking signals are sometimes less dramatic. We attribute the dramatic nature of grokking for algorithmic datasets to representation learning",
    "checked": true,
    "id": "ad065eed8e1f727a8b0d8675802e4ffb1fcb87b4",
    "semantic_title": "omnigrok: grokking beyond algorithmic data",
    "citation_count": 91,
    "authors": []
  },
  "https://openreview.net/forum?id=XCTVFJwS9LJ": {
    "title": "Flow Annealed Importance Sampling Bootstrap",
    "volume": "spotlight",
    "abstract": "Normalizing flows are tractable density models that can approximate complicated target distributions, e.g. Boltzmann distributions of physical systems. However, current methods for training flows either suffer from mode-seeking behavior, use samples from the target generated beforehand by expensive MCMC methods, or use stochastic losses that have high variance. To avoid these problems, we augment flows with annealed importance sampling (AIS) and minimize the mass-covering $\\alpha$-divergence with $\\alpha=2$, which minimizes importance weight variance. Our method, Flow AIS Bootstrap (FAB), uses AIS to generate samples in regions where the flow is a poor approximation of the target, facilitating the discovery of new modes. We apply FAB to multimodal targets and show that we can approximate them very accurately where previous methods fail. To the best of our knowledge, we are the first to learn the Boltzmann distribution of the alanine dipeptide molecule using only the unnormalized target density, without access to samples generated via Molecular Dynamics (MD) simulations: FAB produces better results than training via maximum likelihood on MD samples while using 100 times fewer target evaluations. After reweighting the samples, we obtain unbiased histograms of dihedral angles that are almost identical to the ground truth",
    "checked": true,
    "id": "201940ec4311c65a98bdb794297ef07ba2b7bc24",
    "semantic_title": "flow annealed importance sampling bootstrap",
    "citation_count": 105,
    "authors": []
  },
  "https://openreview.net/forum?id=ih0uFRFhaZZ": {
    "title": "Continual Unsupervised Disentangling of Self-Organizing Representations",
    "volume": "spotlight",
    "abstract": "Limited progress has been made in continual unsupervised learning of representations, especially in reusing, expanding, and continually disentangling learned semantic factors across data environments. We argue that this is because existing approaches treat continually-arrived data independently, without considering how they are related based on the underlying semantic factors. We address this by a new generative model describing a topologically-connected mixture of spike-and-slab distributions in the latent space, learned end-to-end in a continual fashion via principled variational inference. The learned mixture is able to automatically discover the active semantic factors underlying each data environment and to accumulate their relational structure based on that. This distilled knowledge of different data environments can further be used for generative replay and guiding continual disentangling of new semantic factors. We tested the presented method on a split version of 3DShapes to provide the first quantitative disentanglement evaluation of continually learned representations, and further demonstrated its ability to continually disentangle new representations in benchmark datasets",
    "checked": true,
    "id": "7dbbe26585fa0349b9afaa4659a6dd8dbc97d37b",
    "semantic_title": "continual unsupervised disentangling of self-organizing representations",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=5VBBA91N6n": {
    "title": "LMC: Fast Training of GNNs via Subgraph Sampling with Provable Convergence",
    "volume": "spotlight",
    "abstract": "The message passing-based graph neural networks (GNNs) have achieved great success in many real-world applications. However, training GNNs on large-scale graphs suffers from the well-known neighbor explosion problem, i.e., the exponentially increasing dependencies of nodes with the number of message passing layers. Subgraph-wise sampling methods---a promising class of mini-batch training techniques---discard messages outside the mini-batches in backward passes to avoid the neighbor explosion problem at the expense of gradient estimation accuracy. This poses significant challenges to their convergence analysis and convergence speeds, which seriously limits their reliable real-world applications. To address this challenge, we propose a novel subgraph-wise sampling method with a convergence guarantee, namely Local Message Compensation (LMC). To the best of our knowledge, LMC is the {\\it first} subgraph-wise sampling method with provable convergence. The key idea of LMC is to retrieve the discarded messages in backward passes based on a message passing formulation of backward passes. By efficient and effective compensations for the discarded messages in both forward and backward passes, LMC computes accurate mini-batch gradients and thus accelerates convergence. We further show that LMC converges to first-order stationary points of GNNs. Experiments on large-scale benchmark tasks demonstrate that LMC significantly outperforms state-of-the-art subgraph-wise sampling methods in terms of efficiency",
    "checked": true,
    "id": "36ce239097627ffc1bcf4a45f62bb0c2579fc837",
    "semantic_title": "lmc: fast training of gnns via subgraph sampling with provable convergence",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=rZ-wylY5VI": {
    "title": "Programmatically Grounded, Compositionally Generalizable Robotic Manipulation",
    "volume": "spotlight",
    "abstract": "Robots operating in the real world require both rich manipulation skills as well as the ability to semantically reason about when to apply those skills. Towards this goal, recent works have integrated semantic representations from large-scale pretrained vision-language (VL) models into manipulation models, imparting them with more general reasoning capabilities. However, we show that the conventional {\\it pretraining-finetuning} pipeline for integrating such representations entangles the learning of domain-specific action information and domain-general visual information, leading to less data-efficient training and poor generalization to unseen objects and tasks. To this end, we propose \\ours, a {\\it modular} approach to better leverage pretrained VL models by exploiting the syntactic and semantic structures of language instructions. Our framework uses a semantic parser to recover an executable program, composed of functional modules grounded on vision and action across different modalities. Each functional module is realized as a combination of deterministic computation and learnable neural networks. Program execution produces parameters to general manipulation primitives for a robotic end-effector. The entire modular network can be trained with end-to-end imitation learning objectives. Experiments show that our model successfully disentangles action and perception, translating to improved zero-shot and compositional generalization in a variety of manipulation behaviors. Project webpage at: \\url{https://progport.github.io}",
    "checked": true,
    "id": "da3b810e10638507be9901e1b5cb94db126c8166",
    "semantic_title": "programmatically grounded, compositionally generalizable robotic manipulation",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=4eJ43EN2g6l": {
    "title": "SketchKnitter: Vectorized Sketch Generation with Diffusion Models",
    "volume": "spotlight",
    "abstract": "We show vectorized sketch generation can be identified as a reversal of the stroke deformation process. This relationship was established by means of a diffusion model that learns data distributions over the stroke-point locations and pen states of real human sketches. Given randomly scattered stroke-points, sketch generation becomes a process of deformation-based denoising, where the generator rectifies positions of stroke points at each timestep to converge at a recognizable sketch. A key innovation was to embed recognizability into the reverse time diffusion process. It was observed that the estimated noise during the reversal process is strongly correlated with sketch classification accuracy. An auxiliary recurrent neural network (RNN) was consequently used to quantify recognizability during data sampling. It follows that, based on the recognizability scores, a sampling shortcut function can also be devised that renders better quality sketches with fewer sampling steps. Finally it is shown that the model can be easily extended to a conditional generation framework, where given incomplete and unfaithful sketches, it yields one that is more visually appealing and with higher recognizability",
    "checked": true,
    "id": "142062cc51352367c07afc48021db8f5f0bf0855",
    "semantic_title": "sketchknitter: vectorized sketch generation with diffusion models",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=S07feAlQHgM": {
    "title": "A Model or 603 Exemplars: Towards Memory-Efficient Class-Incremental Learning",
    "volume": "spotlight",
    "abstract": "Real-world applications require the classification model to adapt to new classes without forgetting old ones. Correspondingly, Class-Incremental Learning (CIL) aims to train a model with limited memory size to meet this requirement. Typical CIL methods tend to save representative exemplars from former classes to resist forgetting, while recent works find that storing models from history can substantially boost the performance. However, the stored models are not counted into the memory budget, which implicitly results in unfair comparisons. We find that when counting the model size into the total budget and comparing methods with aligned memory size, saving models do not consistently work, especially for the case with limited memory budgets. As a result, we need to holistically evaluate different CIL methods at different memory scales and simultaneously consider accuracy and memory size for measurement. On the other hand, we dive deeply into the construction of the memory buffer for memory efficiency. By analyzing the effect of different layers in the network, we find that shallow and deep layers have different characteristics in CIL. Motivated by this, we propose a simple yet effective baseline, denoted as MEMO for Memory-efficient Expandable MOdel. MEMO extends specialized layers based on the shared generalized representations, efficiently extracting diverse representations with modest cost and maintaining representative exemplars. Extensive experiments on benchmark datasets validate MEMO's competitive performance. Code is available at: https://github.com/wangkiw/ICLR23-MEMO",
    "checked": true,
    "id": "09bdd06f70f509b6a1513565cb6974076ec7d791",
    "semantic_title": "a model or 603 exemplars: towards memory-efficient class-incremental learning",
    "citation_count": 148,
    "authors": []
  },
  "https://openreview.net/forum?id=IxmWsm4xrua": {
    "title": "Toeplitz Neural Network for Sequence Modeling",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "f35f5aedc30e2c5ded210d9c91ba6e84bd029425",
    "semantic_title": "toeplitz neural network for sequence modeling",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=isiQ5KIXbjj": {
    "title": "QuAnt: Quantum Annealing with Learnt Couplings",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "73531f551386d469271c06f5b19ffed5ab848518",
    "semantic_title": "quant: quantum annealing with learnt couplings",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=q3F0UBAruO": {
    "title": "Towards Effective and Interpretable Human-Agent Collaboration in MOBA Games: A Communication Perspective",
    "volume": "spotlight",
    "abstract": "MOBA games, e.g., Dota2 and Honor of Kings, have been actively used as the testbed for the recent AI research on games, and various AI systems have been developed at the human level so far. However, these AI systems mainly focus on how to compete with humans, less on exploring how to collaborate with humans. To this end, this paper makes the first attempt to investigate human-agent collaboration in MOBA games. In this paper, we propose to enable humans and agents to collaborate through explicit communication by designing an efficient and interpretable Meta-Command Communication-based framework, dubbed MCC, for accomplishing effective human-agent collaboration in MOBA games. The MCC framework consists of two pivotal modules: 1) an interpretable communication protocol, i.e., the Meta-Command, to bridge the communication gap between humans and agents; 2) a meta-command value estimator, i.e., the Meta-Command Selector, to select a valuable meta-command for each agent to achieve effective human-agent collaboration. Experimental results in Honor of Kings demonstrate that MCC agents can collaborate reasonably well with human teammates and even generalize to collaborate with different levels and numbers of human teammates. Videos are available at https://sites.google.com/view/mcc-demo",
    "checked": true,
    "id": "70f1e2291235c53e5676512964976d3993f46569",
    "semantic_title": "towards effective and interpretable human-agent collaboration in moba games: a communication perspective",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=uqg3FhRZaq": {
    "title": "On the complexity of nonsmooth automatic differentiation",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "2121052a7337cc9a668c0b2c9fce8cbb56afa2ee",
    "semantic_title": "on the complexity of nonsmooth automatic differentiation",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=OnD9zGAGT0k": {
    "title": "Diffusion Posterior Sampling for General Noisy Inverse Problems",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "61e46884567be7cad12e999365b16a8d3414b678",
    "semantic_title": "diffusion posterior sampling for general noisy inverse problems",
    "citation_count": 934,
    "authors": []
  },
  "https://openreview.net/forum?id=MkbcAHIYgyS": {
    "title": "Mass-Editing Memory in a Transformer",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "2fe1ac0b09cc0f50eb83eef6c7c6b45ac8b12413",
    "semantic_title": "mass-editing memory in a transformer",
    "citation_count": 639,
    "authors": []
  },
  "https://openreview.net/forum?id=iV9Cs8s8keU": {
    "title": "Learning the Positions in CountSketch",
    "volume": "spotlight",
    "abstract": "We consider sketching algorithms which first compress data by multiplication with a random sketch matrix, and then apply the sketch to quickly solve an optimization problem, e.g., low-rank approximation and regression. In the learning-based sketching paradigm proposed by Indyk et al., the sketch matrix is found by choosing a random sparse matrix, e.g., CountSketch, and then the values of its non-zero entries are updated by running gradient descent on a training data set. Despite the growing body of work on this paradigm, a noticeable omission is that the locations of the non-zero entries of previous algorithms were fixed, and only their values were learned. In this work, we propose the first learning-based algorithms that also optimize the locations of the non-zero entries. Our first proposed algorithm is based on a greedy algorithm. However, one drawback of the greedy algorithm is its slower training time. We fix this issue and propose approaches for learning a sketching matrix for both low-rank approximation and Hessian approximation for second-order optimization. The latter is helpful for a range of constrained optimization problems, such as LASSO and matrix estimation with a nuclear norm constraint. Both approaches achieve good accuracy with a fast running time. Moreover, our experiments suggest that our algorithm can still reduce the error significantly even if we only have a very limited number of training matrices",
    "checked": false,
    "id": "500994f747554cafa7d601a92a6e3905b50f1833",
    "semantic_title": "bi-matching mechanism to combat the long tail of word sense disambiguation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v69itrHLEu": {
    "title": "Outcome-directed Reinforcement Learning by Uncertainty \\& Temporal Distance-Aware Curriculum Goal Generation",
    "volume": "spotlight",
    "abstract": "",
    "checked": false,
    "id": "b093a3fa79512c48524f81c754bddec7b16afb17",
    "semantic_title": "outcome-directed reinforcement learning by uncertainty & temporal distance-aware curriculum goal generation",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=Mvetq8DO05O": {
    "title": "A Laplace-inspired Distribution on SO(3) for Probabilistic Rotation Estimation",
    "volume": "spotlight",
    "abstract": "Estimating the 3DoF rotation from a single RGB image is an important yet challenging problem. Probabilistic rotation regression has raised more and more attention with the benefit of expressing uncertainty information along with the prediction. Though modeling noise using Gaussian-resembling Bingham distribution and matrix Fisher distribution is natural, they are shown to be sensitive to outliers for the nature of quadratic punishment to deviations. In this paper, we draw inspiration from multivariate Laplace distribution and propose a novel Rotation Laplace distribution on SO(3). Rotation Laplace distribution is robust to the disturbance of outliers and enforces much gradient to the low-error region, resulting in a better convergence. Our extensive experiments show that our proposed distribution achieves state-of-the-art performance for rotation regression tasks over both probabilistic and non-probabilistic baselines. Our project page is at pku-epic.github.io/RotationLaplace",
    "checked": true,
    "id": "cc755f586df6ca5f509b1e99753e3065b57638c2",
    "semantic_title": "a laplace-inspired distribution on so(3) for probabilistic rotation estimation",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=3F6I-0-57SC": {
    "title": "HiViT: A Simpler and More Efficient Design of Hierarchical Vision Transformer",
    "volume": "spotlight",
    "abstract": "There has been a debate on the choice of plain vs. hierarchical vision transformers, where researchers often believe that the former (e.g., ViT) has a simpler design but the latter (e.g., Swin) enjoys higher recognition accuracy. Recently, the emerge of masked image modeling (MIM), a self-supervised visual pre-training method, raised a new challenge to vision transformers in terms of flexibility, i.e., part of image patches or tokens are to be discarded, which seems to claim the advantages of plain vision transformers. In this paper, we delve deep into the comparison between ViT and Swin, revealing that (i) the performance gain of Swin is mainly brought by a deepened backbone and relative positional encoding, (ii) the hierarchical design of Swin can be simplified into hierarchical patch embedding (proposed in this work), and (iii) other designs such as shifted-window attentions can be removed. By removing the unnecessary operations, we come up with a new architecture named HiViT (short for hierarchical ViT), which is simpler and more efficient than Swin yet further improves its performance on fully-supervised and self-supervised visual representation learning. In particular, after pre-trained using masked autoencoder (MAE) on ImageNet-1K, HiViT-B reports a 84.6% accuracy on ImageNet-1K classification, a 53.3% box AP on COCO detection, and a 52.8% mIoU on ADE20K segmentation, significantly surpassing the baseline. Code is available at https://github.com/zhangxiaosong18/hivit",
    "checked": true,
    "id": "689bc24f71f8f22784534c764d59baa93a62c2e0",
    "semantic_title": "hivit: a simpler and more efficient design of hierarchical vision transformer",
    "citation_count": 74,
    "authors": []
  },
  "https://openreview.net/forum?id=kIPyTuEZuAK": {
    "title": "A Minimalist Dataset for Systematic Generalization of Perception, Syntax, and Semantics",
    "volume": "spotlight",
    "abstract": "Inspired by humans' exceptional ability to master arithmetic and generalize to new problems, we present a new dataset, HINT, to examine machines' capability of learning generalizable concepts at three levels: perception, syntax, and semantics. In HINT, machines are tasked with learning how concepts are perceived from raw signals such as images (i.e., perception), how multiple concepts are structurally combined to form a valid expression (i.e., syntax), and how concepts are realized to afford various reasoning tasks (i.e., semantics), all in a weakly supervised manner. Focusing on systematic generalization, we carefully design a five-fold test set to evaluate both the interpolation and the extrapolation of learned concepts w.r.t the three levels. Further, we design a few-shot learning split to determine whether or not models can rapidly learn new concepts and generalize them to more complex scenarios. To comprehend existing models' limitations, we undertake extensive experiments with various sequence-to-sequence models, including RNNs, Transformers, and GPT-3 (with the chain of thought prompting). The results indicate that current models struggle to extrapolate to long-range syntactic dependency and semantics. Models exhibit a considerable gap toward human-level generalization when evaluated with new concepts in a few-shot setting. Moreover, we discover that it is infeasible to solve HINT by merely scaling up the dataset and the model size; this strategy contributes little to the extrapolation of syntax and semantics. Finally, in zero-shot GPT-3 experiments, the chain of thought prompting exhibits impressive results and significantly boosts the test accuracy. We believe the HINT dataset and the experimental findings are of great interest to the learning community on systematic generalization.%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gOZ_pKANaPW": {
    "title": "Unsupervised Model Selection for Time Series Anomaly Detection",
    "volume": "spotlight",
    "abstract": "Anomaly detection in time-series has a wide range of practical applications. While numerous anomaly detection methods have been proposed in the literature, a recent survey concluded that no single method is the most accurate across various datasets. To make matters worse, anomaly labels are scarce and rarely available in practice. The practical problem of selecting the most accurate model for a given dataset without labels has received little attention in the literature. This paper answers this question \\textit{i.e.} Given an unlabeled dataset and a set of candidate anomaly detectors, how can we select the most accurate model? To this end, we identify three classes of surrogate (unsupervised) metrics, namely, \\textit{prediction error}, \\textit{model centrality}, and \\textit{performance on injected synthetic anomalies}, and show that some metrics are highly correlated with standard supervised anomaly detection performance metrics such as the $F_1$ score, but to varying degrees. We formulate metric combination with multiple imperfect surrogate metrics as a robust rank aggregation problem. We then provide theoretical justification behind the proposed approach. Large-scale experiments on multiple real-world datasets demonstrate that our proposed unsupervised approach is as effective as selecting the most accurate model based on partially labeled data",
    "checked": false,
    "id": "0265761ce77c5a05485e543ef376d57b2da0ac7e",
    "semantic_title": "unsupervised model selection for time-series anomaly detection",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=vtVDI3w_BLL": {
    "title": "AANG : Automating Auxiliary Learning",
    "volume": "spotlight",
    "abstract": "",
    "checked": false,
    "id": "55b901b3f5a0ec6788cbad0c0acdd4aa0f35c72f",
    "semantic_title": "aang: automating auxiliary learning",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=9gfir3fSy3J": {
    "title": "NeRN: Learning Neural Representations for Neural Networks",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "aa5cc9b82b862c528ab1189b674a13990aa55a50",
    "semantic_title": "nern: learning neural representations for neural networks",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=-P7G-8dmSh4": {
    "title": "Formal Mathematics Statement Curriculum Learning",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "916a06a6d51aa93de27aac2f3e14faed08dd6706",
    "semantic_title": "formal mathematics statement curriculum learning",
    "citation_count": 134,
    "authors": []
  },
  "https://openreview.net/forum?id=6fuPIe9tbnC": {
    "title": "Multifactor Sequential Disentanglement via Structured Koopman Autoencoders",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "76bd4ab6432228c73f78110be6e0093eedc0d3c5",
    "semantic_title": "multifactor sequential disentanglement via structured koopman autoencoders",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=XXTyv1zD9zD": {
    "title": "Packed Ensembles for efficient uncertainty estimation",
    "volume": "spotlight",
    "abstract": "Deep Ensembles (DE) are a prominent approach for achieving excellent performance on key metrics such as accuracy, calibration, uncertainty estimation, and out-of-distribution detection. However, hardware limitations of real-world systems constrain to smaller ensembles and lower-capacity networks, significantly deteriorating their performance and properties. We introduce Packed-Ensembles (PE), a strategy to design and train lightweight structured ensembles by carefully modulating the dimension of their encoding space. We leverage grouped convolutions to parallelize the ensemble into a single shared backbone and forward pass to improve training and inference speeds. PE is designed to operate within the memory limits of a standard neural network. Our extensive research indicates that PE accurately preserves the properties of DE, such as diversity, and performs equally well in terms of accuracy, calibration, out-of-distribution detection, and robustness to distribution shift. We make our code available at https://github.com/ENSTA-U2IS/torch-uncertainty",
    "checked": false,
    "id": "00961426bef856073e7a57d785883b1b0a2f6050",
    "semantic_title": "packed-ensembles for efficient uncertainty estimation",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=9y0HFvaAYD6": {
    "title": "Hidden Markov Transformer for Simultaneous Machine Translation",
    "volume": "spotlight",
    "abstract": "Simultaneous machine translation (SiMT) outputs the target sequence while receiving the source sequence, and hence learning when to start translating each target token is the core challenge for SiMT task. However, it is non-trivial to learn the optimal moment among many possible moments of starting translating, as the moments of starting translating always hide inside the model and can only be supervised with the observed target sequence. In this paper, we propose a Hidden Markov Transformer (HMT), which treats the moments of starting translating as hidden events and the target sequence as the corresponding observed events, thereby organizing them as a hidden Markov model. HMT explicitly models multiple moments of starting translating as the candidate hidden events, and then selects one to generate the target token. During training, by maximizing the marginal likelihood of the target sequence over multiple moments of starting translating, HMT learns to start translating at the moments that target tokens can be generated more accurately. Experiments on multiple SiMT benchmarks show that HMT outperforms strong baselines and achieves state-of-the-art performance",
    "checked": true,
    "id": "9ce604efe716d9f85bb3af198bf63d9ceacf1deb",
    "semantic_title": "hidden markov transformer for simultaneous machine translation",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=U2g8OGONA_V": {
    "title": "Multi-domain image generation and translation with identifiability guarantees",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "6fef47eb674d445835e9126655075b522c08c2cc",
    "semantic_title": "multi-domain image generation and translation with identifiability guarantees",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=Zy350cRstc6": {
    "title": "Continual evaluation for lifelong learning: Identifying the stability gap",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "58fc91ecbedf05e663495ff8f92b97279b9c2e3c",
    "semantic_title": "continual evaluation for lifelong learning: identifying the stability gap",
    "citation_count": 46,
    "authors": []
  },
  "https://openreview.net/forum?id=pxStyaf2oJ5": {
    "title": "Domain-Indexing Variational Bayes: Interpretable Domain Index for Domain Adaptation",
    "volume": "spotlight",
    "abstract": "Previous studies have shown that leveraging \"domain index\" can significantly boost domain adaptation performance (Wang et al., 2020; Xu et al., 2022). However, such domain indices are not always available. To address this challenge, we first provide a formal definition of domain index from the probabilistic perspective, and then propose an adversarial variational Bayesian framework that infers domain indices from multi-domain data, thereby providing additional insight on domain relations and improving domain adaptation performance. Our theoretical analysis shows that our adversarial variational Bayesian framework finds the optimal domain index at equilibrium. Empirical results on both synthetic and real data verify that our model can produce interpretable domain indices which enable us to achieve superior performance compared to state-of-the-art domain adaptation methods. Code is available at https://github.com/Wang-ML-Lab/VDI",
    "checked": true,
    "id": "ef018596c581fa37e83e9544b6412beeb15e31fa",
    "semantic_title": "domain-indexing variational bayes: interpretable domain index for domain adaptation",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=p7G8t5FVn2h": {
    "title": "One-Pixel Shortcut: On the Learning Preference of Deep Neural Networks",
    "volume": "spotlight",
    "abstract": "Unlearnable examples (ULEs) aim to protect data from unauthorized usage for training DNNs. Existing work adds $\\ell_\\infty$-bounded perturbations to the original sample so that the trained model generalizes poorly. Such perturbations, however, are easy to eliminate by adversarial training and data augmentations. In this paper, we resolve this problem from a novel perspective by perturbing only one pixel in each image. Interestingly, such a small modification could effectively degrade model accuracy to almost an untrained counterpart. Moreover, our produced \\emph{One-Pixel Shortcut (OPS)} could not be erased by adversarial training and strong augmentations. To generate OPS, we perturb in-class images at the same position to the same target value that could mostly and stably deviate from all the original images. Since such generation is only based on images, OPS needs significantly less computation cost than the previous methods using DNN generators. Based on OPS, we introduce an unlearnable dataset called CIFAR-10-S, which is indistinguishable from CIFAR-10 by humans but induces the trained model to extremely low accuracy. Even under adversarial training, a ResNet-18 trained on CIFAR-10-S has only 10.61% accuracy, compared to 83.02% by the existing error-minimizing method",
    "checked": true,
    "id": "3d4a385e9fe28e69a20b09471e22adf05979ccda",
    "semantic_title": "one-pixel shortcut: on the learning preference of deep neural networks",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=g8wBdhnstYz": {
    "title": "Deterministic training of generative autoencoders using invertible layers",
    "volume": "spotlight",
    "abstract": "In this work, we provide a deterministic alternative to the stochastic variational training of generative autoencoders. We refer to these new generative autoencoders as AutoEncoders within Flows (AEF), since the encoder and decoder are defined as affine layers of an overall invertible architecture. This results in a deterministic encoding of the data, as opposed to the stochastic encoding of VAEs. The paper introduces two related families of AEFs. The first family relies on a partition of the ambient space and is trained by exact maximum-likelihood. The second family exploits a deterministic expansion of the ambient space and is trained by maximizing the log-probability in this extended space. This latter case leaves complete freedom in the choice of encoder, decoder and prior architectures, making it a drop-in replacement for the training of existing VAEs and VAE-style models. We show that these AEFs can have strikingly higher performance than architecturally identical VAEs in terms of log-likelihood and sample quality, especially for low dimensional latent spaces. Importantly, we show that AEF samples are substantially sharper than VAE samples",
    "checked": true,
    "id": "944ca0b8eeda90274478e79352a54d056950c4d0",
    "semantic_title": "deterministic training of generative autoencoders using invertible layers",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=aFzaXRImWE": {
    "title": "A Holistic View of Label Noise Transition Matrix in Deep Learning and Beyond",
    "volume": "spotlight",
    "abstract": "In this paper, we explore learning statistically consistent classifiers under label noise by estimating the noise transition matrix T. We first provide a holistic view of existing T-estimation methods including those with or without anchor point assumptions. We unified them into the Minimum Geometric Envelope Operator (MGEO) framework, which tries to find the smallest T (in terms of a certain metric) that elicits a convex hull to enclose the posteriors of all the training data. Although MGEO methods show appealing theoretical properties and empirical results, we find them prone to failing when the noisy posterior estimation is imperfect, which is inevitable in practice. Specifically, we show that MGEO methods are in-consistent even with infinite samples if the noisy posterior is not estimated accurately. In view of this, we make the first effort to address this issue by proposing a novel T-estimation framework via the lens of bilevel optimization, and term it RObust Bilevel OpTimzation (ROBOT). ROBOT paves a new road beyond MGEO framework, which enjoys strong theoretical properties: identifibility, consistency and finite-sample generalization guarantees. Notably, ROBOT neither requires the perfect posterior estimation nor assumes the existence of anchor points. We further theoretically demonstrate that ROBOT is more robust in the case where MGEO methods fail. Experimentally, our framework also shows superior performance across multiple benchmarks",
    "checked": true,
    "id": "cee36fe5bf94d6351de2d29b947b4a36fcdf8d4b",
    "semantic_title": "a holistic view of label noise transition matrix in deep learning and beyond",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=ZTMuZ68B1g": {
    "title": "Active Learning in Bayesian Neural Networks with Balanced Entropy Learning Principle",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "26b5963bcdac9abf5941357acd60fdbe25560e29",
    "semantic_title": "active learning in bayesian neural networks with balanced entropy learning principle",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=i9ogGQHYbkY": {
    "title": "Near-Optimal Adversarial Reinforcement Learning with Switching Costs",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "272ec7c5cd1b1c1f550a55cdbbea1a21096f8035",
    "semantic_title": "near-optimal adversarial reinforcement learning with switching costs",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=IowKt5rYWsK": {
    "title": "GPViT: A High Resolution Non-Hierarchical Vision Transformer with Group Propagation",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "79086146a558905f16570f72a82f99446da0f9b8",
    "semantic_title": "gpvit: a high resolution non-hierarchical vision transformer with group propagation",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=d8CBRlWNkqH": {
    "title": "Neural Optimal Transport",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "2272b7d70951426e8d2dba2307f9b4d07c049d7b",
    "semantic_title": "neural optimal transport",
    "citation_count": 104,
    "authors": []
  },
  "https://openreview.net/forum?id=4WM4cy42B81": {
    "title": "Dirichlet-based Uncertainty Calibration for Active Domain Adaptation",
    "volume": "spotlight",
    "abstract": "Active domain adaptation (DA) aims to maximally boost the model adaptation on a new target domain by actively selecting limited target data to annotate, whereas traditional active learning methods may be less effective since they do not consider the domain shift issue. Despite active DA methods address this by further proposing targetness to measure the representativeness of target domain characteristics, their predictive uncertainty is usually based on the prediction of deterministic models, which can easily be miscalibrated on data with distribution shift. Considering this, we propose a Dirichlet-based Uncertainty Calibration (DUC) approach for active DA, which simultaneously achieves the mitigation of miscalibration and the selection of informative target samples. Specifically, we place a Dirichlet prior on the prediction and interpret the prediction as a distribution on the probability simplex, rather than a point estimate like deterministic models. This manner enables us to consider all possible predictions, mitigating the miscalibration of unilateral prediction. Then a two-round selection strategy based on different uncertainty origins is designed to select target samples that are both representative of target domain and conducive to discriminability. Extensive experiments on cross-domain image classification and semantic segmentation validate the superiority of DUC",
    "checked": true,
    "id": "a34225690ce8eccdda3e81f6a533771401d76894",
    "semantic_title": "dirichlet-based uncertainty calibration for active domain adaptation",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=IloMJ5rqfnt": {
    "title": "Accurate Image Restoration with Attention Retractable Transformer",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "a1f7f5597fdbc54e58f1f2a1a640bf355e87a978",
    "semantic_title": "accurate image restoration with attention retractable transformer",
    "citation_count": 109,
    "authors": []
  },
  "https://openreview.net/forum?id=C2fsSj3ZGiU": {
    "title": "Neural Episodic Control with State Abstraction",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "4875b7cdb20e23c4ada3ef58d48389b0c76052e3",
    "semantic_title": "neural episodic control with state abstraction",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=4oXTQ6m_ws8": {
    "title": "The Role of ImageNet Classes in Fréchet Inception Distance",
    "volume": "spotlight",
    "abstract": "Fréchet Inception Distance (FID) is the primary metric for ranking models in data-driven generative modeling. While remarkably successful, the metric is known to sometimes disagree with human judgement. We investigate a root cause of these discrepancies, and visualize what FID \"looks at\" in generated images. We show that the feature space that FID is (typically) computed in is so close to the ImageNet classifications that aligning the histograms of Top-$N$ classifications between sets of generated and real images can reduce FID substantially — without actually improving the quality of results. Thus, we conclude that FID is prone to intentional or accidental distortions. As a practical example of an accidental distortion, we discuss a case where an ImageNet pre-trained FastGAN achieves a FID comparable to StyleGAN2, while being worse in terms of human evaluation",
    "checked": true,
    "id": "75348e1f3c800fcdce2d4d64b5a6aa3c69a1fb7d",
    "semantic_title": "the role of imagenet classes in fréchet inception distance",
    "citation_count": 222,
    "authors": []
  },
  "https://openreview.net/forum?id=pd1P2eUBVfq": {
    "title": "Diffusion Models Already Have A Semantic Latent Space",
    "volume": "spotlight",
    "abstract": "Diffusion models achieve outstanding generative performance in various domains. Despite their great success, they lack semantic latent space which is essential for controlling the generative process. To address the problem, we propose asymmetric reverse process (Asyrp) which discovers the semantic latent space in frozen pretrained diffusion models. Our semantic latent space, named h-space, has nice properties for accommodating semantic image manipulation: homogeneity, linearity, robustness, and consistency across timesteps. In addition, we measure editing strength and quality deficiency of a generative process at timesteps to provide a principled design of the process for versatility and quality improvements. Our method is applicable to various architectures (DDPM++, iDDPM, and ADM) and datasets (CelebA-HQ, AFHQ-dog, LSUN-church, LSUN-bedroom, and METFACES)",
    "checked": true,
    "id": "a02313d56a6f71be9aafe43628e0f3a1d0cb858e",
    "semantic_title": "diffusion models already have a semantic latent space",
    "citation_count": 283,
    "authors": []
  },
  "https://openreview.net/forum?id=mRieQgMtNTQ": {
    "title": "Zero-Shot Image Restoration Using Denoising Diffusion Null-Space Model",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "3a75ed3e9e81c9db573ef73d20e2c66c12aaedf8",
    "semantic_title": "zero-shot image restoration using denoising diffusion null-space model",
    "citation_count": 486,
    "authors": []
  },
  "https://openreview.net/forum?id=CrfhZAsJDsZ": {
    "title": "Nonlinear Reconstruction for Operator Learning of PDEs with Discontinuities",
    "volume": "spotlight",
    "abstract": "Discontinuous solutions arise in a large class of hyperbolic and advection-dominated PDEs. This paper investigates, both theoretically and empirically, the operator learning of PDEs with discontinuous solutions. We rigorously prove, in terms of lower approximation bounds, that methods which entail a linear reconstruction step (e.g. DeepONets or PCA-Nets) fail to efficiently approximate the solution operator of such PDEs. In contrast, we show that certain methods employing a non-linear reconstruction mechanism can overcome these fundamental lower bounds and approximate the underlying operator efficiently. The latter class includes Fourier Neural Operators and a novel extension of DeepONets termed shift-DeepONets. Our theoretical findings are confirmed by empirical results for advection equations, inviscid Burgers' equation and the compressible Euler equations of gas dynamics",
    "checked": true,
    "id": "ace25e6421a2d21cb4519895e8e26ae1b73fcae1",
    "semantic_title": "nonlinear reconstruction for operator learning of pdes with discontinuities",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=k60XE_b0Ix6": {
    "title": "Learning Label Encodings for Deep Regression",
    "volume": "spotlight",
    "abstract": "Deep regression networks are widely used to tackle the problem of predicting a continuous value for a given input. Task-specialized approaches for training regression networks have shown significant improvement over generic approaches, such as direct regression. More recently, a generic approach based on regression by binary classification using binary-encoded labels has shown significant improvement over direct regression. The space of label encodings for regression is large. Lacking heretofore have been automated approaches to find a good label encoding for a given application. This paper introduces Regularized Label Encoding Learning (RLEL) for end-to-end training of an entire network and its label encoding. RLEL provides a generic approach for tackling regression. Underlying RLEL is our observation that the search space of label encodings can be constrained and efficiently explored by using a continuous search space of real-valued label encodings combined with a regularization function designed to encourage encodings with certain properties. These properties balance the probability of classification error in individual bits against error correction capability. Label encodings found by RLEL result in lower or comparable errors to manually designed label encodings. Applying RLEL results in $10.9\\%$ and $12.4\\%$ improvement in Mean Absolute Error (MAE) over direct regression and multiclass classification, respectively. Our evaluation demonstrates that RLEL can be combined with off-the-shelf feature extractors and is suitable across different architectures, datasets, and tasks. Code is available at \\url{https://github.com/ubc-aamodt-group/RLEL_regression}",
    "checked": true,
    "id": "d785bded41e43ea695466d917059e75b8562c671",
    "semantic_title": "learning label encodings for deep regression",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=Z3IClM_bzvP": {
    "title": "Multi-skill Mobile Manipulation for Object Rearrangement",
    "volume": "spotlight",
    "abstract": "We study a modular approach to tackle long-horizon mobile manipulation tasks for object rearrangement, which decomposes a full task into a sequence of subtasks. To tackle the entire task, prior work chains multiple stationary manipulation skills with a point-goal navigation skill, which are learned individually on subtasks. Although more effective than monolithic end-to-end RL policies, this framework suffers from compounding errors in skill chaining, e.g., navigating to a bad location where a stationary manipulation skill can not reach its target to manipulate. To this end, we propose that the manipulation skills should include mobility to have flexibility in interacting with the target object from multiple locations and at the same time the navigation skill could have multiple end points which lead to successful manipulation. We operationalize these ideas by implementing mobile manipulation skills rather than stationary ones and training a navigation skill trained with region goal instead of point goal. We evaluate our multi-skill mobile manipulation method M3 on 3 challenging long-horizon mobile manipulation tasks in the Home Assistant Benchmark (HAB), and show superior performance as compared to the baselines",
    "checked": true,
    "id": "344cba18285949fe409fc0b332831bc186cfeb17",
    "semantic_title": "multi-skill mobile manipulation for object rearrangement",
    "citation_count": 55,
    "authors": []
  },
  "https://openreview.net/forum?id=3RhuF8foyPW": {
    "title": "Single-shot General Hyper-parameter Optimization for Federated Learning",
    "volume": "spotlight",
    "abstract": "We address the problem of hyper-parameter optimization (HPO) for federated learning (FL-HPO). We introduce Federated Loss SuRface Aggregation (FLoRA), a general FL-HPO solution framework that can address use cases of tabular data and any Machine Learning (ML) model including gradient boosting training algorithms, SVMs, neural networks, among others and thereby further expands the scope of FL-HPO. FLoRA enables single-shot FL-HPO: identifying a single set of good hyper-parameters that are subsequently used in a single FL training. Thus, it enables FL-HPO solutions with minimal additional communication overhead compared to FL training without HPO. Utilizing standard smoothness assumptions, we theoretically characterize the optimality gap of FLoRA for any convex and non-convex loss functions, which explicitly accounts for the heterogeneous nature of the parties' local data distributions, a dominant characteristic of FL systems. Our empirical evaluation of FLoRA for multiple FL algorithms on seven OpenML datasets demonstrates significant model accuracy improvements over the baselines, and robustness to increasing number of parties involved in FL-HPO training",
    "checked": true,
    "id": "73d5f34e826440b956db1b071a43ad583e489cf8",
    "semantic_title": "single-shot general hyper-parameter optimization for federated learning",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=RWtGreRpovS": {
    "title": "Simplicial Embeddings in Self-Supervised Learning and Downstream Classification",
    "volume": "spotlight",
    "abstract": "Simplicial Embeddings (SEM) are representations learned through self-supervised learning (SSL), wherein a representation is projected into $L$ simplices of $V$ dimensions each using a \\texttt{softmax} operation. This procedure conditions the representation onto a constrained space during pretraining and imparts an inductive bias for group sparsity. For downstream classification, we formally prove that the SEM representation leads to better generalization than an unnormalized representation. Furthermore, we empirically demonstrate that SSL methods trained with SEMs have improved generalization on natural image datasets such as CIFAR-100 and ImageNet. Finally, when used in a downstream classification task, we show that SEM features exhibit emergent semantic coherence where small groups of learned features are distinctly predictive of semantically-relevant classes",
    "checked": true,
    "id": "f005116ddb524a4cc3df83b24a1d15bb3b4ef87a",
    "semantic_title": "simplicial embeddings in self-supervised learning and downstream classification",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=plKu2GByCNW": {
    "title": "Vision Transformer Adapter for Dense Predictions",
    "volume": "spotlight",
    "abstract": "This work investigates a simple yet powerful dense prediction task adapter for Vision Transformer (ViT). Unlike recently advanced variants that incorporate vision-specific inductive biases into their architectures, the plain ViT suffers inferior performance on dense predictions due to weak prior assumptions. To address this issue, we propose the ViT-Adapter, which allows plain ViT to achieve comparable performance to vision-specific transformers. Specifically, the backbone in our framework is a plain ViT that can learn powerful representations from large-scale multi-modal data. When transferring to downstream tasks, a pre-training-free adapter is used to introduce the image-related inductive biases into the model, making it suitable for these tasks. We verify ViT-Adapter on multiple dense prediction tasks, including object detection, instance segmentation, and semantic segmentation. Notably, without using extra detection data, our ViT-Adapter-L yields state-of-the-art 60.9 box AP and 53.0 mask AP on COCO test-dev. We hope that the ViT-Adapter could serve as an alternative for vision-specific transformers and facilitate future research. Code and models will be released at https://github.com/czczup/ViT-Adapter",
    "checked": true,
    "id": "c431408780586268e8bcf2483b01a80728d10960",
    "semantic_title": "vision transformer adapter for dense predictions",
    "citation_count": 612,
    "authors": []
  },
  "https://openreview.net/forum?id=hVrXUps3LFA": {
    "title": "Divide to Adapt: Mitigating Confirmation Bias for Domain Adaptation of Black-Box Predictors",
    "volume": "spotlight",
    "abstract": "Domain Adaptation of Black-box Predictors (DABP) aims to learn a model on an unlabeled target domain supervised by a black-box predictor trained on a source domain. It does not require access to both the source-domain data and the predictor parameters, thus addressing the data privacy and portability issues of standard domain adaptation methods. Existing DABP approaches mostly rely on knowledge distillation (KD) from the black-box predictor, i.e., training the model with its noisy target-domain predictions, which however inevitably introduces the confirmation bias accumulated from the prediction noises and leads to degrading performance. To mitigate such bias, we propose a new strategy, \\textit{divide-to-adapt}, that purifies cross-domain knowledge distillation by proper domain division. This is inspired by an observation we make for the first time in domain adaptation: the target domain usually contains easy-to-adapt and hard-to-adapt samples that have different levels of domain discrepancy w.r.t. the source domain, and deep models tend to fit easy-to-adapt samples first. Leveraging easy-to-adapt samples with less noise can help KD alleviate the negative effect of prediction noises from black-box predictors. In this sense, the target domain can be divided into an easy-to-adapt subdomain with less noise and a hard-to-adapt subdomain at the early stage of training. Then the adaptation is achieved by semi-supervised learning. We further reduce distribution discrepancy between subdomains and develop weak-strong augmentation strategy to filter the predictor errors progressively. As such, our method is a simple yet effective solution to reduce error accumulation in cross-domain knowledge distillation for DABP. Moreover, we prove that the target error of DABP is bounded by the noise ratio of two subdomains, i.e., the confirmation bias, which provides the theoretical justifications for our method. Extensive experiments demonstrate our method achieves state of the art on all DABP benchmarks, outperforming the existing best approach by 7.0\\% on VisDA-17, and is even comparable with the standard domain adaptation methods that use the source-domain data",
    "checked": true,
    "id": "db20ae433d93486c45d1beacefc01f8129c84b0e",
    "semantic_title": "divide to adapt: mitigating confirmation bias for domain adaptation of black-box predictors",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=zqwryBoXYnh": {
    "title": "PLOT: Prompt Learning with Optimal Transport for Vision-Language Models",
    "volume": "spotlight",
    "abstract": "With the increasing attention to large vision-language models such as CLIP, there has been a significant amount of effort dedicated to building efficient prompts. Unlike conventional methods of only learning one single prompt, we propose to learn multiple comprehensive prompts to describe diverse characteristics of categories such as intrinsic attributes or extrinsic contexts. However, directly matching each prompt to the same visual feature is problematic, as it pushes the prompts to converge to one point. To solve this problem, we propose to apply optimal transport to match the vision and text modalities. Specifically, we first model images and the categories with visual and textual feature sets. Then, we apply a two-stage optimization strategy to learn the prompts. In the inner loop, we optimize the optimal transport distance to align visual features and prompts by the Sinkhorn algorithm, while in the outer loop, we learn the prompts by this distance from the supervised data. Extensive experiments are conducted on the few-shot recognition task and the improvement demonstrates the superiority of our method. The code is available at https://github.com/CHENGY12/PLOT",
    "checked": false,
    "id": "0d5103378a9f4f6e08bfcd364da207f93b31b8b7",
    "semantic_title": "prompt learning with optimal transport for vision-language models",
    "citation_count": 66,
    "authors": []
  },
  "https://openreview.net/forum?id=VA1YpcNr7ul": {
    "title": "DASHA: Distributed Nonconvex Optimization with Communication Compression and Optimal Oracle Complexity",
    "volume": "spotlight",
    "abstract": "We develop and analyze DASHA: a new family of methods for nonconvex distributed optimization problems. When the local functions at the nodes have a finite-sum or an expectation form, our new methods, DASHA-PAGE, DASHA-MVR and DASHA-SYNC-MVR, improve the theoretical oracle and communication complexity of the previous state-of-the-art method MARINA by Gorbunov et al. (2020). In particular, to achieve an $\\varepsilon$-stationary point, and considering the random sparsifier Rand$K$ as an example, our methods compute the optimal number of gradients $\\mathcal{O}\\left(\\frac{\\sqrt{m}}{\\varepsilon\\sqrt{n}}\\right)$ and $\\mathcal{O}\\left(\\frac{\\sigma}{\\varepsilon^{3/2}n}\\right)$ in finite-sum and expectation form cases, respectively, while maintaining the SOTA communication complexity $\\mathcal{O}\\left(\\frac{d}{\\varepsilon \\sqrt{n}}\\right)$. Furthermore, unlike MARINA, the new methods DASHA, DASHA-PAGE and DASHA-MVR send compressed vectors only, which makes them more practical for federated learning. We extend our results to the case when the functions satisfy the Polyak-Lojasiewicz condition. Finally, our theory is corroborated in practice: we see a significant improvement in experiments with nonconvex classification and training of deep learning models",
    "checked": true,
    "id": "c760172c08c2aa6ecdb84a3915df445dde5a64cc",
    "semantic_title": "dasha: distributed nonconvex optimization with communication compression and optimal oracle complexity",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=JJuP86nBl4q": {
    "title": "LAVA: Data Valuation without Pre-Specified Learning Algorithms",
    "volume": "spotlight",
    "abstract": "Traditionally, data valuation is posed as a problem of equitably splitting the validation performance of a learning algorithm among the training data. As a result, the calculated data values depend on many design choices of the underlying learning algorithm. However, this dependence is undesirable for many use cases of data valuation, such as setting priorities over different data sources in a data acquisition process and informing pricing mechanisms in a data marketplace. In these scenarios, data needs to be valued before the actual analysis and the choice of the learning algorithm is still undetermined then. Another side-effect of the dependence is that to assess the value of individual points, one needs to re-run the learning algorithm with and without a point, which incurs a large computation burden. This work leapfrogs over the current limits of data valuation methods by introducing a new framework that can value training data in a way that is oblivious to the downstream learning algorithm. Our main results are as follows. $\\textbf{(1)}$ We develop a proxy for the validation performance associated with a training set based on a non-conventional $\\textit{class-wise}$ $\\textit{Wasserstein distance}$ between the training and the validation set. We show that the distance characterizes the upper bound of the validation performance for any given model under certain Lipschitz conditions. $\\textbf{(2)}$ We develop a novel method to value individual data based on the sensitivity analysis of the $\\textit{class-wise}$ Wasserstein distance. Importantly, these values can be directly obtained $\\textit{for free}$ from the output of off-the-shelf optimization solvers once the Wasserstein distance is computed. $\\textbf{(3) }$We evaluate our new data valuation framework over various use cases related to detecting low-quality data and show that, surprisingly, the learning-agnostic feature of our framework enables a $\\textit{significant improvement}$ over the state-of-the-art performance while being $\\textit{orders of magnitude faster.}$",
    "checked": true,
    "id": "4cc3fbdd9c07c16fd3c363363ac33e729b8021e6",
    "semantic_title": "lava: data valuation without pre-specified learning algorithms",
    "citation_count": 65,
    "authors": []
  },
  "https://openreview.net/forum?id=SEh5SfEQtqB": {
    "title": "Meta-prediction Model for Distillation-Aware NAS on Unseen Datasets",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "2fd5bb847862b29e1c5680ffeabe6737b71f58c5",
    "semantic_title": "meta-prediction model for distillation-aware nas on unseen datasets",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=rLwC0_MG-4w": {
    "title": "Denoising Diffusion Error Correction Codes",
    "volume": "spotlight",
    "abstract": "Error correction code (ECC) is an integral part of the physical communication layer, ensuring reliable data transfer over noisy channels. Recently, neural decoders have demonstrated their advantage over classical decoding techniques. However, recent state-of-the-art neural decoders suffer from high complexity and lack the important iterative scheme characteristic of many legacy decoders. In this work, we propose to employ denoising diffusion models for the soft decoding of linear codes at arbitrary block lengths. Our framework models the forward channel corruption as a series of diffusion steps that can be reversed iteratively. Three contributions are made: (i) a diffusion process suitable for the decoding setting is introduced, (ii) the neural diffusion decoder is conditioned on the number of parity errors, which indicates the level of corruption at a given step, (iii) a line search procedure based on the code's syndrome obtains the optimal reverse diffusion step size. The proposed approach demonstrates the power of diffusion models for ECC and is able to achieve state-of-the-art accuracy, outperforming the other neural decoders by sizable margins, even for a single reverse diffusion step",
    "checked": true,
    "id": "bd3ac574978fa0eefb2c68a19321660822de64c0",
    "semantic_title": "denoising diffusion error correction codes",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=2RwXVje1rAh": {
    "title": "Exploring Active 3D Object Detection from a Generalization Perspective",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "3b463a75c924affa60c65f7a27ef4be5b523ebc7",
    "semantic_title": "exploring active 3d object detection from a generalization perspective",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=iOc57X9KM54": {
    "title": "Neuro-Symbolic Procedural Planning with Commonsense Prompting",
    "volume": "spotlight",
    "abstract": "Procedural planning aims to implement complex high-level goals by decomposition into simpler low-level steps. Although procedural planning is a basic skill set for humans in daily life, it remains a challenge for large language models (LLMs) that lack a deep understanding of the cause-effect relations in procedures. Previous methods require manual exemplars to acquire procedural planning knowledge from LLMs in the zero-shot setting. However, such elicited pre-trained knowledge in LLMs induces spurious correlations between goals and steps, which impair the model generalization to unseen tasks. In contrast, this paper proposes a neuro-symbolic procedural PLANner (PLAN) that elicits procedural planning knowledge from the LLMs with commonsense-infused prompting. To mitigate spurious goal-step correlations, we use symbolic program executors on the latent procedural representations to formalize prompts from commonsense knowledge bases as a causal intervention toward the Structural Causal Model. Both automatic and human evaluations on WikiHow and RobotHow show the superiority of PLAN on procedural planning without further training or manual exemplars",
    "checked": true,
    "id": "418085c9726669bf53f3d66e0018f2b08ffc4ce6",
    "semantic_title": "neuro-symbolic procedural planning with commonsense prompting",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=urF_CBK5XC0": {
    "title": "Generative Augmented Flow Networks",
    "volume": "spotlight",
    "abstract": "The Generative Flow Network is a probabilistic framework where an agent learns a stochastic policy for object generation, such that the probability of generating an object is proportional to a given reward function. Its effectiveness has been shown in discovering high-quality and diverse solutions, compared to reward-maximizing reinforcement learning-based methods. Nonetheless, GFlowNets only learn from rewards of the terminal states, which can limit its applicability. Indeed, intermediate rewards play a critical role in learning, for example from intrinsic motivation to provide intermediate feedback even in particularly challenging sparse reward tasks. Inspired by this, we propose Generative Augmented Flow Networks (GAFlowNets), a novel learning framework to incorporate intermediate rewards into GFlowNets. We specify intermediate rewards by intrinsic motivation to tackle the exploration problem in sparse reward environments. GAFlowNets can leverage edge-based and state-based intrinsic rewards in a joint way to improve exploration. Based on extensive experiments on the GridWorld task, we demonstrate the effectiveness and efficiency of GAFlowNet in terms of convergence, performance, and diversity of solutions. We further show that GAFlowNet is scalable to a more complex and large-scale molecule generation domain, where it achieves consistent and significant performance improvement",
    "checked": true,
    "id": "1c6035ba208d64d119b7cf6a895f9e71d662b90c",
    "semantic_title": "generative augmented flow networks",
    "citation_count": 49,
    "authors": []
  },
  "https://openreview.net/forum?id=rvsbw2YthH_": {
    "title": "The Trade-off between Universality and Label Efficiency of Representations from Contrastive Learning",
    "volume": "spotlight",
    "abstract": "Pre-training representations (a.k.a. foundation models) has recently become a prevalent learning paradigm, where one first pre-trains a representation using large-scale unlabeled data, and then learns simple predictors on top of the representation using small labeled data from the downstream tasks. There are two key desiderata for the representation: label efficiency (the ability to learn an accurate classifier on top of the representation with a small amount of labeled data) and universality (usefulness across a wide range of downstream tasks). In this paper, we focus on one of the most popular instantiations of this paradigm: contrastive learning with linear probing, i.e., learning a linear predictor on the representation pre-trained by contrastive learning. We show that there exists a trade-off between the two desiderata so that one may not be able to achieve both simultaneously. Specifically, we provide analysis using a theoretical data model and show that, while more diverse pre-training data result in more diverse features for different tasks (improving universality), it puts less emphasis on task-specific features, giving rise to larger sample complexity for down-stream supervised tasks, and thus worse prediction performance. Guided by this analysis, we propose a contrastive regularization method to improve the trade-off. We validate our analysis and method empirically with systematic experiments using real-world datasets and foundation models",
    "checked": true,
    "id": "9d3463da77f6288da6fa16631034293a733bd719",
    "semantic_title": "the trade-off between universality and label efficiency of representations from contrastive learning",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=FUORz1tG8Og": {
    "title": "CROM: Continuous Reduced-Order Modeling of PDEs Using Implicit Neural Representations",
    "volume": "spotlight",
    "abstract": "The long runtime of high-fidelity partial differential equation (PDE) solvers makes them unsuitable for time-critical applications. We propose to accelerate PDE solvers using reduced-order modeling (ROM). Whereas prior ROM approaches reduce the dimensionality of discretized vector fields, our continuous reduced-order modeling (CROM) approach builds a low-dimensional embedding of the continuous vector fields themselves, not their discretization. We represent this reduced manifold using continuously differentiable neural fields, which may train on any and all available numerical solutions of the continuous system, even when they are obtained using diverse methods or discretizations. We validate our approach on an extensive range of PDEs with training data from voxel grids, meshes, and point clouds. Compared to prior discretization-dependent ROM methods, such as linear subspace proper orthogonal decomposition (POD) and nonlinear manifold neural-network-based autoencoders, CROM features higher accuracy, lower memory consumption, dynamically adaptive resolutions, and applicability to any discretization. For equal latent space dimension, CROM exhibits 79$\\times$ and 49$\\times$ better accuracy, and 39$\\times$ and 132$\\times$ smaller memory footprint, than POD and autoencoder methods, respectively. Experiments demonstrate 109$\\times$ and 89$\\times$ wall-clock speedups over unreduced models on CPUs and GPUs, respectively. Videos and codes are available on the project page: https://crom-pde.github.io",
    "checked": true,
    "id": "74d6a587d97bb13d0ca69bb326b454e2d87b9261",
    "semantic_title": "crom: continuous reduced-order modeling of pdes using implicit neural representations",
    "citation_count": 49,
    "authors": []
  },
  "https://openreview.net/forum?id=G2Q2Mh3avow": {
    "title": "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language",
    "volume": "spotlight",
    "abstract": "We investigate how multimodal prompt engineering can use language as the intermediate representation to combine complementary knowledge from different pretrained (potentially multimodal) language models for a variety of tasks. This approach is both distinct from and complementary to the dominant paradigm of joint multimodal training. It also recalls a traditional systems-building view as in classical NLP pipelines, but with prompting large pretrained multimodal models. We refer to these as Socratic Models (SMs): a modular class of systems in which multiple pretrained models may be composed zero-shot via multimodal-informed prompting to capture new multimodal capabilities, without additional finetuning. We show that these systems provide competitive state-of-the-art performance for zero-shot image captioning and video-to-text retrieval, and also enable new applications such as (i) answering free-form questions about egocentric video, (ii) engaging in multimodal assistive dialogue with people (e.g., for cooking recipes), and (iii) robot perception and planning. We hope this work provides (a) results for stronger zero-shot baseline performance with analysis also highlighting their limitations, (b) new perspectives for building multimodal systems powered by large pretrained models, and (c) practical application advantages in certain regimes limited by data scarcity, training compute, or model access",
    "checked": true,
    "id": "ada81a4de88a6ce474df2e2446ad11fea480616e",
    "semantic_title": "socratic models: composing zero-shot multimodal reasoning with language",
    "citation_count": 606,
    "authors": []
  },
  "https://openreview.net/forum?id=Bo7eeXm6An8": {
    "title": "Multi-lingual Evaluation of Code Generation Models",
    "volume": "spotlight",
    "abstract": "We present two new benchmarks, MBXP and Multilingual HumanEval, designed to evaluate code completion models in over 10 programming languages. These datasets are generated using a conversion framework that transpiles prompts and test cases from the original MBPP and HumanEval datasets into the corresponding data in the target language. By using these benchmarks, we are able to assess the performance of code generation models in a multi-lingual fashion, and discovered generalization ability of language models on out-of-domain languages, advantages of multi-lingual models over mono-lingual, the ability of few-shot prompting to teach the model new languages, and zero-shot translation abilities. In addition, we use our code generation model to perform large-scale bootstrapping to obtain synthetic canonical solutions in several languages, which can be used for other code-related evaluations such as code insertion, robustness, or summarization tasks",
    "checked": true,
    "id": "2577d053f8aab912d29b424e1f09133d83740fd2",
    "semantic_title": "multi-lingual evaluation of code generation models",
    "citation_count": 182,
    "authors": []
  },
  "https://openreview.net/forum?id=B_pCIsX8KL_": {
    "title": "GRACE-C: Generalized Rate Agnostic Causal Estimation via Constraints",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "9db8bfc43f7ee13d07e43e76c9b93ab177ce4b73",
    "semantic_title": "grace-c: generalized rate agnostic causal estimation via constraints",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=KwmPfARgOTD": {
    "title": "Equiformer: Equivariant Graph Attention Transformer for 3D Atomistic Graphs",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "1ede15a0aa5204de1f3013d7616a93dbd46fd849",
    "semantic_title": "equiformer: equivariant graph attention transformer for 3d atomistic graphs",
    "citation_count": 264,
    "authors": []
  },
  "https://openreview.net/forum?id=CWmvjOEhgH-": {
    "title": "MPCFORMER: FAST, PERFORMANT AND PRIVATE TRANSFORMER INFERENCE WITH MPC",
    "volume": "spotlight",
    "abstract": "Enabling private inference is crucial for many cloud inference services that are based on Transformer models. However, existing private inference solutions can increase the inference latency by more than 60$\\times$ or significantly compromise the inference quality. In this paper, we design the framework MPCFORMER as a practical solution, using Secure Multi-Party Computation (MPC) and Knowledge Distillation (KD). Through extensive evaluations, we show that MPCFORMER significantly speeds up Transformer inference in MPC settings while achieving similar ML performance to the input model. On the IMDb dataset, it achieves similar performance to $\\text{BERT}_\\text{BASE}$, while being 5.3$\\times$ faster. On the GLUE benchmark, it achieves 97% performance of $\\text{BERT}_\\text{BASE}$ with a 2.2$\\times$ speedup. MPCFORMER remains effective with different trained Transformer weights such as $\\text{ROBERTA}_\\text{BASE}$ and larger models including $\\text{BERT}_\\text{LARGE}$. Code is available at https://github.com/MccRee177/MPCFormer",
    "checked": true,
    "id": "2701d5b55ddacbdc82bc33901451f593a92127f1",
    "semantic_title": "mpcformer: fast, performant and private transformer inference with mpc",
    "citation_count": 90,
    "authors": []
  },
  "https://openreview.net/forum?id=qLOaeRvteqbx": {
    "title": "Disparate Impact in Differential Privacy from Gradient Misalignment",
    "volume": "spotlight",
    "abstract": "As machine learning becomes more widespread throughout society, aspects including data privacy and fairness must be carefully considered, and are crucial for deployment in highly regulated industries. Unfortunately, the application of privacy enhancing technologies can worsen unfair tendencies in models. In particular, one of the most widely used techniques for private model training, differentially private stochastic gradient descent (DPSGD), frequently intensifies disparate impact on groups within data. In this work we study the fine-grained causes of unfairness in DPSGD and identify gradient misalignment due to inequitable gradient clipping as the most significant source. This observation leads us to a new method for reducing unfairness by preventing gradient misalignment in DPSGD",
    "checked": true,
    "id": "e125db480c8a84d9b17f67fbbe577a8304dd9485",
    "semantic_title": "disparate impact in differential privacy from gradient misalignment",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=cp5PvcI6w8_": {
    "title": "TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second",
    "volume": "spotlight",
    "abstract": "We present TabPFN, a trained Transformer that can do supervised classification for small tabular datasets in less than a second, needs no hyperparameter tuning and is competitive with state-of-the-art classification methods. TabPFN is fully entailed in the weights of our network, which accepts training and test samples as a set-valued input and yields predictions for the entire test set in a single forward pass. TabPFN is a Prior-Data Fitted Network (PFN) and is trained offline once, to approximate Bayesian inference on synthetic datasets drawn from our prior. This prior incorporates ideas from causal reasoning: It entails a large space of structural causal models with a preference for simple structures. On the $18$ datasets in the OpenML-CC18 suite that contain up to 1000 training data points, up to 100 purely numerical features without missing values, and up to 10 classes, we show that our method clearly outperforms boosted trees and performs on par with complex state-of-the-art AutoML systems with up to $230\\times$ speedup. This increases to a $5\\,700\\times$ speedup when using a GPU. We also validate these results on an additional 67 small numerical datasets from OpenML. We provide all our code, the trained TabPFN, an interactive browser demo and a Colab notebook at https://github.com/automl/TabPFN",
    "checked": true,
    "id": "4c4f0fcf1ce04f12290d8c876abfbe57817de430",
    "semantic_title": "tabpfn: a transformer that solves small tabular classification problems in a second",
    "citation_count": 348,
    "authors": []
  },
  "https://openreview.net/forum?id=SJ1kSyO2jwu": {
    "title": "Human Motion Diffusion Model",
    "volume": "spotlight",
    "abstract": "Natural and expressive human motion generation is the holy grail of computer animation. It is a challenging task, due to the diversity of possible motion, human perceptual sensitivity to it, and the difficulty of accurately describing it. Therefore, current generative solutions are either low-quality or limited in expressiveness. Diffusion models are promising candidates for the human motion domain since they have already shown remarkable generative capabilities in other domains, and their many-to-many nature. In this paper, we introduce Motion Diffusion Model (MDM), a carefully adapted classifier-free diffusion-based generative model for human motion data. MDM is transformer-based, combining insights from motion generation literature. A notable design-choice is that it predicts the sample itself rather than the noise in each step to facilitate the use of established geometric losses on the locations and velocities of the motion, such as the foot contact loss. As we demonstrate, MDM is a generic approach, enabling different modes of conditioning, and different generation tasks. We show that our model is trained with lightweight resources and yet achieves state-of-the-art results on leading benchmarks for text-to-motion, action-to-motion, and unconditioned motion generation",
    "checked": true,
    "id": "15736f7c205d961c00378a938daffaacb5a0718d",
    "semantic_title": "human motion diffusion model",
    "citation_count": 816,
    "authors": []
  },
  "https://openreview.net/forum?id=CsKwavjr7A": {
    "title": "Visual Recognition with Deep Nearest Centroids",
    "volume": "spotlight",
    "abstract": "We devise deep nearest centroids (DNC), a conceptually elegant yet surprisingly effective network for large-scale visual recognition, by revisiting Nearest Centroids, one of the most classic and simple classifiers. Current deep models learn the classifier in a fully parametric manner, ignoring the latent data structure and lacking simplicity and explainability. DNC instead conducts nonparametric, case-based reasoning; it utilizes sub-centroids of training samples to describe class distributions and clearly explains the classification as the proximity of test data and the class sub-centroids in the feature space. Due to the distance-based nature, the network output dimensionality is flexible, and all the learnable parameters are only for data embedding. That means all the knowledge learnt for ImageNet classification can be completely transferred for pixel recognition learning, under the ‘pre-training and fine-tuning' paradigm. Apart from its nested simplicity and intuitive decision-making mechanism, DNC can even possess ad-hoc explainability when the sub-centroids are selected as actual training images that humans can view and inspect. Compared with parametric counterparts, DNC performs better on image classification (CIFAR-10, ImageNet) and greatly boots pixel recognition (ADE20K, Cityscapes), with improved transparency and fewer learnable parameters, using various network architectures (ResNet, Swin) and segmentation models (FCN, DeepLabV3, Swin). We feel this work brings fundamental insights into related fields. Our code is available at https://github.com/ChengHan111/DNC",
    "checked": true,
    "id": "5b1a2d2016fdb87572a07baf65fb0d64cffdf03c",
    "semantic_title": "visual recognition with deep nearest centroids",
    "citation_count": 98,
    "authors": []
  },
  "https://openreview.net/forum?id=B73niNjbPs": {
    "title": "Continuous PDE Dynamics Forecasting with Implicit Neural Representations",
    "volume": "spotlight",
    "abstract": "Effective data-driven PDE forecasting methods often rely on fixed spatial and / or temporal discretizations. This raises limitations in real-world applications like weather prediction where flexible extrapolation at arbitrary spatiotemporal locations is required. We address this problem by introducing a new data-driven approach, DINo, that models a PDE's flow with continuous-time dynamics of spatially continuous functions. This is achieved by embedding spatial observations independently of their discretization via Implicit Neural Representations in a small latent space temporally driven by a learned ODE. This separate and flexible treatment of time and space makes DINo the first data-driven model to combine the following advantages. It extrapolates at arbitrary spatial and temporal locations; it can learn from sparse irregular grids or manifolds; at test time, it generalizes to new grids or resolutions. DINo outperforms alternative neural PDE forecasters in a variety of challenging generalization scenarios on representative PDE systems",
    "checked": true,
    "id": "d39ad86d4617e069d89b6d62c760c2ba268a2b85",
    "semantic_title": "continuous pde dynamics forecasting with implicit neural representations",
    "citation_count": 58,
    "authors": []
  },
  "https://openreview.net/forum?id=3Y5Uhf5KgGK": {
    "title": "No Reason for No Supervision: Improved Generalization in Supervised Models",
    "volume": "spotlight",
    "abstract": "We consider the problem of training a deep neural network on a given classification task, e.g., ImageNet-1K (IN1K), so that it excels at both the training task as well as at other (future) transfer tasks. These two seemingly contradictory properties impose a trade-off between improving the model's generalization and maintaining its performance on the original task. Models trained with self-supervised learning tend to generalize better than their supervised counterparts for transfer learning; yet, they still lag behind supervised models on IN1K. In this paper, we propose a supervised learning setup that leverages the best of both worlds. We extensively analyze supervised training using multi-scale crops for data augmentation and an expendable projector head, and reveal that the design of the projector allows us to control the trade-off between performance on the training task and transferability. We further replace the last layer of class weights with class prototypes computed on the fly using a memory bank and derive two models: t-ReX that achieves a new state of the art for transfer learning and outperforms top methods such as DINO and PAWS on IN1K, and t-ReX* that matches the highly optimized RSB-A1 model on IN1K while performing better on transfer tasks. Code and pretrained models: https://europe.naverlabs.com/t-rex",
    "checked": true,
    "id": "f4d2644c8c03196212d00c3bee6d941e91678400",
    "semantic_title": "no reason for no supervision: improved generalization in supervised models",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=g7U9jD_2CUr": {
    "title": "EVA3D: Compositional 3D Human Generation from 2D Image Collections",
    "volume": "spotlight",
    "abstract": "Inverse graphics aims to recover 3D models from 2D observations. Utilizing differentiable rendering, recent 3D-aware generative models have shown impressive results of rigid object generation using 2D images. However, it remains challenging to generate articulated objects, like human bodies, due to their complexity and diversity in poses and appearances. In this work, we propose, EVA3D, an unconditional 3D human generative model learned from 2D image collections only. EVA3D can sample 3D humans with detailed geometry and render high-quality images (up to 512x256) without bells and whistles (e.g. super resolution). At the core of EVA3D is a compositional human NeRF representation, which divides the human body into local parts. Each part is represented by an individual volume. This compositional representation enables 1) inherent human priors, 2) adaptive allocation of network parameters, 3) efficient training and rendering. Moreover, to accommodate for the characteristics of sparse 2D human image collections (e.g. imbalanced pose distribution), we propose a pose-guided sampling strategy for better GAN learning. Extensive experiments validate that EVA3D achieves state-of-the-art 3D human generation performance regarding both geometry and texture quality. Notably, EVA3D demonstrates great potential and scalability to \"inverse-graphics\" diverse human bodies with a clean framework",
    "checked": true,
    "id": "8cb9f266e2a051301352493b3e8c480195c08424",
    "semantic_title": "eva3d: compositional 3d human generation from 2d image collections",
    "citation_count": 122,
    "authors": []
  },
  "https://openreview.net/forum?id=DSy8tP4WctmZ": {
    "title": "Voxurf: Voxel-based Efficient and Accurate Neural Surface Reconstruction",
    "volume": "spotlight",
    "abstract": "Neural surface reconstruction aims to reconstruct accurate 3D surfaces based on multi-view images. Previous methods based on neural volume rendering mostly train a fully implicit model with MLPs, which typically require hours of training for a single scene. Recent efforts explore the explicit volumetric representation to accelerate the optimization via memorizing significant information with learnable voxel grids. However, existing voxel-based methods often struggle in reconstructing fine-grained geometry, even when combined with an SDF-based volume rendering scheme. We reveal that this is because 1) the voxel grids tend to break the color-geometry dependency that facilitates fine-geometry learning, and 2) the under-constrained voxel grids lack spatial coherence and are vulnerable to local minima. In this work, we present Voxurf, a voxel-based surface reconstruction approach that is both efficient and accurate. Voxurf addresses the aforementioned issues via several key designs, including 1) a two-stage training procedure that attains a coherent coarse shape and recovers fine details successively, 2) a dual color network that maintains color-geometry dependency, and 3) a hierarchical geometry feature to encourage information propagation across voxels. Extensive experiments show that Voxurf achieves high efficiency and high quality at the same time. On the DTU benchmark, Voxurf achieves higher reconstruction quality with a 20x training speedup compared to previous fully implicit methods. Our code is publicly available at https://github.com/wutong16/Voxurf/",
    "checked": true,
    "id": "9ff6afe3ac617ab819dcf720282ea63561c4a524",
    "semantic_title": "voxurf: voxel-based efficient and accurate neural surface reconstruction",
    "citation_count": 104,
    "authors": []
  },
  "https://openreview.net/forum?id=UkU05GOH7_6": {
    "title": "Generating Diverse Cooperative Agents by Learning Incompatible Policies",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "15521311d6d47218e54c9b9ca0cda8283fb36c52",
    "semantic_title": "generating diverse cooperative agents by learning incompatible policies",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=KbYevcLjnc": {
    "title": "PEER: A Collaborative Language Model",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "a938ff4539b09a785a66669844f1a35f76169218",
    "semantic_title": "peer: a collaborative language model",
    "citation_count": 98,
    "authors": []
  },
  "https://openreview.net/forum?id=GMRodZ8OlVr": {
    "title": "ISS: Image as Stepping Stone for Text-Guided 3D Shape Generation",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "b8076c9f2daffeca047301ddb319dc4391a2272b",
    "semantic_title": "iss: image as stepping stone for text-guided 3d shape generation",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=1C_kSW1-k0": {
    "title": "STREET: A MULTI-TASK STRUCTURED REASONING AND EXPLANATION BENCHMARK",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "a3a241e9397fe29b37f96cb5e8f4b8bebed3d3da",
    "semantic_title": "street: a multi-task structured reasoning and explanation benchmark",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=y5W8tpojhtJ": {
    "title": "Neural Collapse Inspired Feature-Classifier Alignment for Few-Shot Class-Incremental Learning",
    "volume": "spotlight",
    "abstract": "",
    "checked": false,
    "id": "85fa07886672bc19429295ea7f69c3f2888fe80f",
    "semantic_title": "neural collapse inspired feature-classifier alignment for few-shot class incremental learning",
    "citation_count": 118,
    "authors": []
  },
  "https://openreview.net/forum?id=WbxHAzkeQcn": {
    "title": "Neural Networks and the Chomsky Hierarchy",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "c6d38add1b7bbc10f0da37a90e3f1b51ee5fb617",
    "semantic_title": "neural networks and the chomsky hierarchy",
    "citation_count": 162,
    "authors": []
  },
  "https://openreview.net/forum?id=D1Iqfm7WTkk": {
    "title": "Neural ePDOs: Spatially Adaptive Equivariant Partial Differential Operator Based Networks",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "bf172883d10dfe6fda9b3387e37b880eb8929302",
    "semantic_title": "neural epdos: spatially adaptive equivariant partial differential operator based networks",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=NAQvF08TcyG": {
    "title": "An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "5406129d9d7d00dc310671c43597101b0ee93629",
    "semantic_title": "an image is worth one word: personalizing text-to-image generation using textual inversion",
    "citation_count": 2015,
    "authors": []
  },
  "https://openreview.net/forum?id=nUmCcZ5RKF": {
    "title": "IS SYNTHETIC DATA FROM GENERATIVE MODELS READY FOR IMAGE RECOGNITION?",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "2a29e1bcbed17c588ffbae1fea2af3baaab924b8",
    "semantic_title": "is synthetic data from generative models ready for image recognition?",
    "citation_count": 315,
    "authors": []
  },
  "https://openreview.net/forum?id=k7p_YAO7yE": {
    "title": "MapTR: Structured Modeling and Learning for Online Vectorized HD Map Construction",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "e26ea26f3195cfc7675f5ae3795b4960b76b97d4",
    "semantic_title": "maptr: structured modeling and learning for online vectorized hd map construction",
    "citation_count": 276,
    "authors": []
  },
  "https://openreview.net/forum?id=zEn1BhaNYsC": {
    "title": "Minimax Optimal Kernel Operator Learning via Multilevel Training",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "c038c1a5f0e54b002521925ce6b40e426f0d1421",
    "semantic_title": "minimax optimal kernel operator learning via multilevel training",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=NRxydtWup1S": {
    "title": "Designing BERT for Convolutional Networks: Sparse and Hierarchical Masked Modeling",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "e45c0a415b01b13d25e7dbced9f651261be0feaf",
    "semantic_title": "designing bert for convolutional networks: sparse and hierarchical masked modeling",
    "citation_count": 113,
    "authors": []
  },
  "https://openreview.net/forum?id=RUzSobdYy0V": {
    "title": "Quantifying and Mitigating the Impact of Label Errors on Model Disparity Metrics",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e531451e6e2cd5d776cc0dd75a5a1068066ed2f7",
    "semantic_title": "quantifying and mitigating the impact of label errors on model disparity metrics",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=tmIiMPl4IPa": {
    "title": "Factorized Fourier Neural Operators",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "50bfb95187503caf0bd0daa6077f6df9de4c7456",
    "semantic_title": "factorized fourier neural operators",
    "citation_count": 179,
    "authors": []
  },
  "https://openreview.net/forum?id=mhnHqRqcjYU": {
    "title": "DFPC: Data flow driven pruning of coupled channels without data",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "57a2f56f2f44de8ab892c4e8c3f1744530a55399",
    "semantic_title": "dfpc: data flow driven pruning of coupled channels without data",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=sZI1Oj9KBKy": {
    "title": "TVSPrune - Pruning Non-discriminative filters via Total Variation separability of intermediate representations without fine tuning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "49215392c790076064c86c5d6ac301e951d8d42b",
    "semantic_title": "tvsprune - pruning non-discriminative filters via total variation separability of intermediate representations without fine tuning",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=I3HCE7Ro78H": {
    "title": "Finding Actual Descent Directions for Adversarial Training",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3e6da1517c593108cc3f1c0b858b3ad7fbbf85dc",
    "semantic_title": "finding actual descent directions for adversarial training",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=6iEoTr-jeB7": {
    "title": "Learning Continuous Normalizing Flows For Faster Convergence To Target Distribution via Ascent Regularizations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b8d2482005f6cb87d3b4b8a47b5d5cc7dfe67463",
    "semantic_title": "learning continuous normalizing flows for faster convergence to target distribution via ascent regularizations",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=HTJE5Krui0g": {
    "title": "Softened Symbol Grounding for Neuro-symbolic Systems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "104c0f48db425b01d0c3fd0fc4bf570e48f4da39",
    "semantic_title": "ymbol g rounding for n euro - symbolic",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jREF4bkfi_S": {
    "title": "Mini-batch k -means terminates within O ( d / ϵ ) iterations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "82aa0cf79eca312bde0ff92e489690a686140958",
    "semantic_title": "mini-batch k-means terminates within o(d/ε) iterations",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=pWVASryOyFw": {
    "title": "Learning Uncertainty for Unknown Domains with Zero-Target-Assumption",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b83308201eba0bfffd0c55396906fbf2c3c8f406",
    "semantic_title": "learning uncertainty for unknown domains with zero-target-assumption",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=ULzyv9M1j5": {
    "title": "Transformer-based model for symbolic regression via joint supervised learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e0e129b95b624e50778095f62f65e3f9ab4668d2",
    "semantic_title": "transformer-based model for symbolic regression via joint supervised learning",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=gNI4_85Cyve": {
    "title": "QAID: Question Answering Inspired Few-shot Intent Detection",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c27210604ad9626e4cf928c5073678493dd9c8fe",
    "semantic_title": "qaid: question answering inspired few-shot intent detection",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=ejR4E1jaH9k": {
    "title": "Solving stochastic weak Minty variational inequalities without increasing batch size",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c4020d85eb25b3782c634cb36dd5d00c4300b43c",
    "semantic_title": "solving stochastic weak minty variational inequalities without increasing batch size",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=r9fX833CsuN": {
    "title": "Curriculum-based Co-design of Morphology and Control of Voxel-based Soft Robots",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "599811e35d6218e7b7db74e06bfdf1e4e13cb2d5",
    "semantic_title": "curriculum-based co-design of morphology and control of voxel-based soft robots",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=tPKKXeW33YU": {
    "title": "WiNeRT: Towards Neural Ray Tracing for Wireless Channel Modelling and Differentiable Simulations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e113d13819cf35029c11d171ff039ab01e61226c",
    "semantic_title": "winert: towards neural ray tracing for wireless channel modelling and differentiable simulations",
    "citation_count": 46,
    "authors": []
  },
  "https://openreview.net/forum?id=o3Q4m8jg4BR": {
    "title": "LS-IQ: Implicit Reward Regularization for Inverse Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c8bd100b509367e831881c177a1ba41f4828d41f",
    "semantic_title": "ls-iq: implicit reward regularization for inverse reinforcement learning",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=oJpVVGXu9i": {
    "title": "Share Your Representation Only: Guaranteed Improvement of the Privacy-Utility Tradeoff in Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3ab8343bd8494a8370c1115007081f94e57ddca8",
    "semantic_title": "share your representation only: guaranteed improvement of the privacy-utility tradeoff in federated learning",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=eDLwjKmtYFt": {
    "title": "EquiMod: An Equivariance Module to Improve Visual Instance Discrimination",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c0241327a3ca4d2756dc64c5c50a89c4172d37da",
    "semantic_title": "equimod: an equivariance module to improve visual instance discrimination",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=-M0TNnyWFT5": {
    "title": "Task-Aware Information Routing from Common Representation Space in Lifelong Learning",
    "volume": "poster",
    "abstract": "Intelligent systems deployed in the real world suffer from catastrophic forgetting when exposed to a sequence of tasks. Humans, on the other hand, acquire, consolidate, and transfer knowledge between tasks that rarely interfere with the consolidated knowledge. Accompanied by self-regulated neurogenesis, continual learning in the brain is governed by the rich set of neurophysiological processes that harbor different types of knowledge which are then integrated by the conscious processing. Thus, inspired by Global Workspace Theory of conscious information access in the brain, we propose TAMiL, a continual learning method that entails task-attention modules to capture task-specific information from the common representation space. We employ simple, undercomplete autoencoders to create a communication bottleneck between the common representation space and the global workspace, allowing only the task-relevant information to the global workspace, thereby greatly reducing task interference. Experimental results show that our method outperforms state-of-the-art rehearsal-based and dynamic sparse approaches and bridges the gap between fixed capacity and parameter isolation approaches while being scalable. We also show that our method effectively mitigates catastrophic forgetting while being well-calibrated with reduced task-recency bias",
    "checked": true,
    "id": "7b64984a059c8b56feec5ebf4ecace0d71870326",
    "semantic_title": "task-aware information routing from common representation space in lifelong learning",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=htL4UZ344nF": {
    "title": "CodeBPE: Investigating Subtokenization Options for Large Language Model Pretraining on Source Code",
    "volume": "poster",
    "abstract": "Recent works have widely adopted large language model pretraining for source code, suggested source code-specific pretraining objectives and investigated the applicability of various Transformer-based language model architectures for source code. This work investigates another important aspect of such models, the effect of different subtokenization options, and aims at identifying most effective and length-efficient subtokenizations, taking into account source code specifics. We propose subtokenziation that reduces average length by 17--40% without downstream performance drop, and show that a carefully chosen subtokenization may improve quality by 0.5-2%, possibly with some length increase",
    "checked": true,
    "id": "e6b73466bab5e52ce0db19dd06d9353c26557dae",
    "semantic_title": "codebpe: investigating subtokenization options for large language model pretraining on source code",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=x-mXzBgCX3a": {
    "title": "FairGBM: Gradient Boosting with Fairness Constraints",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "814a5a6f75c1b47b5f2b6ae0c6a359711669975d",
    "semantic_title": "fairgbm: gradient boosting with fairness constraints",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=18XzeuYZh_": {
    "title": "Online Bias Correction for Task-Free Continual Learning",
    "volume": "poster",
    "abstract": "Task-free continual learning is the machine-learning setting where a model is trained online with data generated by a nonstationary stream. Conventional wisdom suggests that, in this setting, models are trained using an approach called experience replay, where the risk is computed both with respect to current stream observations and to a small subset of past observations. In this work, we explain both theoretically and empirically how experience replay biases the outputs of the model towards recent stream observations. Moreover, we propose a simple approach to mitigate this bias online, by changing how the output layer of the model is optimized. We show that our approach improves significantly the learning performance of experience-replay approaches over different datasets. Our findings suggest that, when performing experience replay, the output layer of the model should be optimized separately from the preceding layers",
    "checked": true,
    "id": "6009d662a2f4b220aec9ad0abdc7c4932c51e6b7",
    "semantic_title": "online bias correction for task-free continual learning",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=TN9gQ4x0Ep3": {
    "title": "Don't fear the unlabelled: safe semi-supervised learning via debiasing",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "eda9d680ea91462bdd8356285ef0d68ea131b4f9",
    "semantic_title": "don't fear the unlabelled: safe semi-supervised learning via debiasing",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=bjPPypbLre": {
    "title": "Making Substitute Models More Bayesian Can Enhance Transferability of Adversarial Examples",
    "volume": "poster",
    "abstract": "The transferability of adversarial examples across deep neural networks (DNNs) is the crux of many black-box attacks. Many prior efforts have been devoted to improving the transferability via increasing the diversity in inputs of some substitute models. In this paper, by contrast, we opt for the diversity in substitute models and advocate to attack a Bayesian model for achieving desirable transferability. Deriving from the Bayesian formulation, we develop a principled strategy for possible finetuning, which can be combined with many off-the-shelf Gaussian posterior approximations over DNN parameters. Extensive experiments have been conducted to verify the effectiveness of our method, on common benchmark datasets, and the results demonstrate that our method outperforms recent state-of-the-arts by large margins (roughly 19% absolute increase in average attack success rate on ImageNet), and, by combining with these recent methods, further performance gain can be obtained. Our code: https://github.com/qizhangli/MoreBayesian-attack",
    "checked": true,
    "id": "7a121f973165f96188049570be03933a4bb114c3",
    "semantic_title": "making substitute models more bayesian can enhance transferability of adversarial examples",
    "citation_count": 37,
    "authors": []
  },
  "https://openreview.net/forum?id=pvgEL1yS3Ql": {
    "title": "Cross-Layer Retrospective Retrieving via Layer Attention",
    "volume": "poster",
    "abstract": "More and more evidence has shown that strengthening layer interactions can enhance the representation power of a deep neural network, while self-attention excels at learning interdependencies by retrieving query-activated information. Motivated by this, we devise a cross-layer attention mechanism, called multi-head recurrent layer attention (MRLA), that sends a query representation of the current layer to all previous layers to retrieve query-related information from different levels of receptive fields. A light-weighted version of MRLA is also proposed to reduce the quadratic computation cost. The proposed layer attention mechanism can enrich the representation power of many state-of-the-art vision networks, including CNNs and vision transformers. Its effectiveness has been extensively evaluated in image classification, object detection and instance segmentation tasks, where improvements can be consistently observed. For example, our MRLA can improve 1.6% Top-1 accuracy on ResNet-50, while only introducing 0.16M parameters and 0.07B FLOPs. Surprisingly, it can boost the performances by a large margin of 3-4% box AP and mask AP in dense prediction tasks. Our code is available at https://github.com/joyfang1106/MRLA",
    "checked": true,
    "id": "db3d7316f72d6cc283c556e12db31a2d47727d7f",
    "semantic_title": "cross-layer retrospective retrieving via layer attention",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=kqHkCVS7wbj": {
    "title": "Decision S4: Efficient Sequence-Based RL via State Spaces Layers",
    "volume": "poster",
    "abstract": "Recently, sequence learning methods have been applied to the problem of off-policy Reinforcement Learning, including the seminal work on Decision Transformers, which employs transformers for this task. Since transformers are parameter-heavy, cannot benefit from history longer than a fixed window size, and are not computed using recurrence, we set out to investigate the suitability of the S4 family of models, which are based on state-space layers and have been shown to outperform transformers, especially in modeling long-range dependencies. In this work, we present two main algorithms: (i) an off-policy training procedure that works with trajectories, while still maintaining the training efficiency of the S4 model. (ii) An on-policy training procedure that is trained in a recurrent manner, benefits from long-range dependencies, and is based on a novel stable actor-critic mechanism. Our results indicate that our method outperforms multiple variants of decision transformers, as well as the other baseline methods on most tasks, while reducing the latency, number of parameters, and training time by several orders of magnitude, making our approach more suitable for real-world RL",
    "checked": true,
    "id": "56a78b4012f87587a81d17b060277047bfb3956c",
    "semantic_title": "decision s4: efficient sequence-based rl via state spaces layers",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=mnVf1W6ipGm": {
    "title": "Unveiling the sampling density in non-uniform geometric graphs",
    "volume": "poster",
    "abstract": "A powerful framework for studying graphs is to consider them as geometric graphs: nodes are randomly sampled from an underlying metric space, and any pair of nodes is connected if their distance is less than a specified neighborhood radius. Currently, the literature mostly focuses on uniform sampling and constant neighborhood radius. However, real-world graphs are likely to be better represented by a model in which the sampling density and the neighborhood radius can both vary over the latent space. For instance, in a social network communities can be modeled as densely sampled areas, and hubs as nodes with larger neighborhood radius. In this work, we first perform a rigorous mathematical analysis of this (more general) class of models, including derivations of the resulting graph shift operators. The key insight is that graph shift operators should be corrected in order to avoid potential distortions introduced by the non-uniform sampling. Then, we develop methods to estimate the unknown sampling density in a self-supervised fashion. Finally, we present exemplary applications in which the learnt density is used to 1) correct the graph shift operator and improve performance on a variety of tasks, 2) improve pooling, and 3) extract knowledge from networks. Our experimental findings support our theory and provide strong evidence for our model",
    "checked": true,
    "id": "cd9b3be09e6eb66d4a15ff9114561bd843b5174e",
    "semantic_title": "unveiling the sampling density in non-uniform geometric graphs",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=LNpMtk15AS4": {
    "title": "Boosting Causal Discovery via Adaptive Sample Reweighting",
    "volume": "poster",
    "abstract": "Under stringent model type and variable distribution assumptions, score-based causal discovery methods learn the directed acyclic graph (DAG) from observational data by evaluating candidate graphs over an averaged score function. Despite the great success in low-dimensional linear systems, it has been observed that these approaches overly exploits easier-to-fit samples, thus inevitably learning spurious edges. Worse still, the common homogeneity assumption of most causal discovery methods can be easily violated due to the widespread existence of heterogeneous data in the real world, resulting in performance vulnerability when noise distributions vary. We propose a simple yet effective model-agnostic framework to boost causal discovery performance by dynamically learning the adaptive weights for the Reweighted Score function, ReScore for short, where the learned weights tailors quantitatively to the important degree of each samples. Intuitively, we leverage the bilevel optimization scheme to alternatively train a standard DAG learner first, then upweight the samples that the DAG learner fails to fit well and downweight the samples that the DAG learner easily extracts the causation information from. Extensive experiments on both synthetic and real-world datasets are carried out to validate the effectiveness of ReScore. We observe consistent and significant boosts in structure learning performance. We further visualize that ReScore concurrently mitigates the influence of spurious edges and generalizes to heterogeneous data. Finally, we perform theoretical analysis to guarantee the structure identifiability and the weight adaptive properties of ReScore. Our codes are available at https://github.com/anzhang314/ReScore",
    "checked": true,
    "id": "cc9ff6df3d5a0a8199820e720ee604ad8bce8368",
    "semantic_title": "boosting causal discovery via adaptive sample reweighting",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=SEcSahl0Ql": {
    "title": "Iterative Circuit Repair Against Formal Specifications",
    "volume": "poster",
    "abstract": "We present a deep learning approach for repairing sequential circuits against formal specifications given in linear-time temporal logic (LTL). Given a defective circuit and its formal specification, we train Transformer models to output circuits that satisfy the corresponding specification. We propose a separated hierarchical Transformer for multimodal representation learning of the formal specification and the circuit. We introduce a data generation algorithm that enables generalization to more complex specifications and out-of-distribution datasets. In addition, our proposed repair mechanism significantly improves the automated synthesis of circuits from LTL specifications with Transformers. It improves the state-of-the-art by $6.8$ percentage points on held-out instances and $11.8$ percentage points on an out-of-distribution dataset from the annual reactive synthesis competition",
    "checked": true,
    "id": "757442d72dc516737abfb12ea9700aaac5c47316",
    "semantic_title": "iterative circuit repair against formal specifications",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=UazgYBMS9-W": {
    "title": "Can BERT Refrain from Forgetting on Sequential Tasks? A Probing Study",
    "volume": "poster",
    "abstract": "Large pre-trained language models have helped to achieve state of the art on a variety of NLP tasks, nevertheless, they still suffer from forgetting when incrementally learning a series of sequential tasks. To alleviate this problem, recent works propose several models enhanced by sparse experience replay and local adaption, which yield satisfactory performance. However, in this paper we find that pre-trained language models like BERT have a potential ability to learn sequentially, even without any sparse memory replay. To verify the ability of BERT to maintain old knowledge, we adopt and re-finetune single-layer probe networks with the parameters of BERT fixed. We investigate the models on two typical kinds of NLP tasks, text classification and extractive question answering. And our experiments reveal that BERT can actually generate high quality representations for previous tasks in a long term, under extremely sparse replay or even no replay. We further introduce a series of methods to interpret the mechanism of forgetting and how memory rehearsal plays a significant role in task incremental learning, which bridges the gap between our new discovery and previous studies about catastrophic forgetting. Additionally, we provide both quantified and visualized results demonstrating that the representation space of BERT is always topologically organised, which guarantees its performance",
    "checked": true,
    "id": "201047e827ed9587158fc71256c576c8544e3dfc",
    "semantic_title": "can bert refrain from forgetting on sequential tasks? a probing study",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=3c13LptpIph": {
    "title": "Behavior Proximal Policy Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b6b1b3b9ee2ef36a195b43477ba8bd690c07dd0c",
    "semantic_title": "behavior proximal policy optimization",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=xfqDe72zh41": {
    "title": "Actionable Neural Representations: Grid Cells from Minimal Constraints",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bc7417738a7f7186ce9b06933072c4ddd39ce4b1",
    "semantic_title": "actionable neural representations: grid cells from minimal constraints",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=jevY-DtiZTR": {
    "title": "Mole-BERT: Rethinking Pre-training Graph Neural Networks for Molecules",
    "volume": "poster",
    "abstract": "Recent years have witnessed the prosperity of pre-training graph neural networks (GNNs) for molecules. Typically, atom types as node attributes are randomly masked, and GNNs are then trained to predict masked types as in AttrMask \\citep{hu2020strategies}, following the Masked Language Modeling (MLM) task of BERT~\\citep{devlin2019bert}. However, unlike MLM with a large vocabulary, the AttrMask pre-training does not learn informative molecular representations due to small and unbalanced atom `vocabulary'. To amend this problem, we propose a variant of VQ-VAE~\\citep{van2017neural} as a context-aware tokenizer to encode atom attributes into chemically meaningful discrete codes. This can enlarge the atom vocabulary size and mitigate the quantitative divergence between dominant (e.g., carbons) and rare atoms (e.g., phosphorus). With the enlarged atom `vocabulary', we propose a novel node-level pre-training task, dubbed Masked Atoms Modeling (\\textbf{MAM}), to mask some discrete codes randomly and then pre-train GNNs to predict them. MAM also mitigates another issue of AttrMask, namely the negative transfer. It can be easily combined with various pre-training tasks to improve their performance. Furthermore, we propose triplet masked contrastive learning (\\textbf{TMCL}) for graph-level pre-training to model the heterogeneous semantic similarity between molecules for effective molecule retrieval. MAM and TMCL constitute a novel pre-training framework, \\textbf{Mole-BERT}, which can match or outperform state-of-the-art methods in a fully data-driven manner. We release the code at \\textcolor{magenta}{\\url{https://github.com/junxia97/Mole-BERT}}",
    "checked": true,
    "id": "f470ac3537339514bb9d88fcad9c075441906d45",
    "semantic_title": "mole-bert: rethinking pre-training graph neural networks for molecules",
    "citation_count": 150,
    "authors": []
  },
  "https://openreview.net/forum?id=_q7A0m3vXH0": {
    "title": "Geometrically regularized autoencoders for non-Euclidean data",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5726b2cfa9aedf18ad5b5008d3686179a93b25b3",
    "semantic_title": "geometrically regularized autoencoders for non-euclidean data",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=VBTJqqWjxMv": {
    "title": "A Message Passing Perspective on Learning Dynamics of Contrastive Learning",
    "volume": "poster",
    "abstract": "In recent years, contrastive learning achieves impressive results on self-supervised visual representation learning, but there still lacks a rigorous understanding of its learning dynamics. In this paper, we show that if we cast a contrastive objective equivalently into the feature space, then its learning dynamics admits an interpretable form. Specifically, we show that its gradient descent corresponds to a specific message passing scheme on the corresponding augmentation graph. Based on this perspective, we theoretically characterize how contrastive learning gradually learns discriminative features with the alignment update and the uniformity update. Meanwhile, this perspective also establishes an intriguing connection between contrastive learning and Message Passing Graph Neural Networks (MP-GNNs). This connection not only provides a unified understanding of many techniques independently developed in each community, but also enables us to borrow techniques from MP-GNNs to design new contrastive learning variants, such as graph attention, graph rewiring, jumpy knowledge techniques, etc. We believe that our message passing perspective not only provides a new theoretical understanding of contrastive learning dynamics, but also bridges the two seemingly independent areas together, which could inspire more interleaving studies to benefit from each other. The code is available at https://github.com/PKU-ML/Message-Passing-Contrastive-Learning",
    "checked": true,
    "id": "0ea5511cc66906b0142d8154d9a891c29ed22341",
    "semantic_title": "a message passing perspective on learning dynamics of contrastive learning",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=n1bLgxHW6jW": {
    "title": "Zeroth-Order Optimization with Trajectory-Informed Derivative Estimation",
    "volume": "poster",
    "abstract": "Zeroth-order (ZO) optimization, in which the derivative is unavailable, has recently succeeded in many important machine learning applications. Existing algorithms rely on finite difference (FD) methods for derivative estimation and gradient descent (GD)-based approaches for optimization. However, these algorithms suffer from query inefficiency because many additional function queries are required for derivative estimation in their every GD update, which typically hinders their deployment in real-world applications where every function query is expensive. To this end, we propose a trajectory-informed derivative estimation method which only employs the optimization trajectory (i.e., the history of function queries during optimization) and hence can eliminate the need for additional function queries to estimate a derivative. Moreover, based on our derivative estimation, we propose the technique of dynamic virtual updates, which allows us to reliably perform multiple steps of GD updates without reapplying derivative estimation. Based on these two contributions, we introduce the zeroth-order optimization with trajectory-informed derivative estimation (ZoRD) algorithm for query-efficient ZO optimization. We theoretically demonstrate that our trajectory-informed derivative estimation and our ZoRD algorithm improve over existing approaches, which is then supported by our real-world experiments such as black-box adversarial attack, non-differentiable metric optimization, and derivative-free reinforcement learning",
    "checked": true,
    "id": "af5cfa2fe0b3b4694dd5b86ea3eaa2a2bef1e9ea",
    "semantic_title": "zeroth-order optimization with trajectory-informed derivative estimation",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=_JScUk9TBUn": {
    "title": "Uniform-in-time propagation of chaos for the mean-field gradient Langevin dynamics",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3cc39cd6809669f0087e278968146b9e53954b3e",
    "semantic_title": "uniform-in-time propagation of chaos for the mean-field gradient langevin dynamics",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=_i0-12XqVJZ": {
    "title": "Asynchronous Distributed Bilevel Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2c5f6080a69efe76cf56d225808324fb5b692c1b",
    "semantic_title": "asynchronous distributed bilevel optimization",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=YPKBIILy-Kt": {
    "title": "Confidence-Based Feature Imputation for Graphs with Partially Known Features",
    "volume": "poster",
    "abstract": "This paper investigates a missing feature imputation problem for graph learning tasks. Several methods have previously addressed learning tasks on graphs with missing features. However, in cases of high rates of missing features, they were unable to avoid significant performance degradation. To overcome this limitation, we introduce a novel concept of channel-wise confidence in a node feature, which is assigned to each imputed channel feature of a node for reflecting the certainty of the imputation. We then design pseudo-confidence using the channel-wise shortest path distance between a missing-feature node and its nearest known-feature node to replace unavailable true confidence in an actual learning process. Based on the pseudo-confidence, we propose a novel feature imputation scheme that performs channel-wise inter-node diffusion and node-wise inter-channel propagation. The scheme can endure even at an exceedingly high missing rate (e.g., 99.5\\%) and it achieves state-of-the-art accuracy for both semi-supervised node classification and link prediction on various datasets containing a high rate of missing features. Codes are available at https://github.com/daehoum1/pcfi",
    "checked": true,
    "id": "132009e5f1d256b8806442bbf3924946b32a4b5a",
    "semantic_title": "confidence-based feature imputation for graphs with partially known features",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=WHlt5tLz12T": {
    "title": "LiftedCL: Lifting Contrastive Learning for Human-Centric Perception",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "617d80ab4307c5ea8a8178fe4cd108a52f51fb27",
    "semantic_title": "liftedcl: lifting contrastive learning for human-centric perception",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=JmC_Tld3v-f": {
    "title": "Individual Privacy Accounting with Gaussian Differential Privacy",
    "volume": "poster",
    "abstract": "Individual privacy accounting enables bounding differential privacy (DP) loss individually for each participant involved in the analysis. This can be informative as often the individual privacy losses are considerably smaller than those indicated by the DP bounds that are based on considering worst-case bounds at each data access. In order to account for the individual losses in a principled manner, we need a privacy accountant for adaptive compositions of mechanisms, where the loss incurred at a given data access is allowed to be smaller than the worst-case loss. This kind of analysis has been carried out for the Rényi differential privacy by Feldman and Zrnic (2021), however not yet for the so-called optimal privacy accountants. We make first steps in this direction by providing a careful analysis using the Gaussian differential privacy which gives optimal bounds for the Gaussian mechanism, one of the most versatile DP mechanisms. This approach is based on determining a certain supermartingale for the hockey-stick divergence and on extending the Rényi divergence-based fully adaptive composition results by Feldman and Zrnic (2021). We also consider measuring the individual $(\\varepsilon,\\delta)$-privacy losses using the so-called privacy loss distributions. Using the Blackwell theorem, we can then use the results of Feldman and Zrnic (2021) to construct an approximative individual $(\\varepsilon,\\delta)$-accountant. We also show how to speed up the FFT-based individual DP accounting using the Plancherel theorem",
    "checked": true,
    "id": "77d2d951c1a69bb7d7c6b4c1f8ebbfdfee15b544",
    "semantic_title": "individual privacy accounting with gaussian differential privacy",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=CBfYffLqWqb": {
    "title": "Evolving Populations of Diverse RL Agents with MAP-Elites",
    "volume": "poster",
    "abstract": "Quality Diversity (QD) has emerged as a powerful alternative optimization paradigm that aims at generating large and diverse collections of solutions, notably with its flagship algorithm MAP-ELITES (ME) which evolves solutions through mutations and crossovers. While very effective for some unstructured problems, early ME implementations relied exclusively on random search to evolve the population of solutions, rendering them notoriously sample-inefficient for high-dimensional problems, such as when evolving neural networks. Follow-up works considered exploiting gradient information to guide the search in order to address these shortcomings through techniques borrowed from either Black-Box Optimization (BBO) or Reinforcement Learning (RL). While mixing RL techniques with ME unlocked state-of-the-art performance for robotics control problems that require a good amount of exploration, it also plagued these ME variants with limitations common among RL algorithms that ME was free of, such as hyperparameter sensitivity, high stochasticity as well as training instability, including when the population size increases as some components are shared across the population in recent approaches. Furthermore, existing approaches mixing ME with RL tend to be tied to a specific RL algorithm, which effectively prevents their use on problems where the corresponding RL algorithm fails. To address these shortcomings, we introduce a flexible framework that allows the use of any RL algorithm and alleviates the aforementioned limitations by evolving populations of agents (whose definition include hyperparameters and all learnable parameters) instead of just policies. We demonstrate the benefits brought about by our framework through extensive numerical experiments on a number of robotics control problems, some of which with deceptive rewards, taken from the QD-RL literature. We open source an efficient JAX-based implementation of our algorithm in the QDax library",
    "checked": true,
    "id": "8b7d33373f018535adb24bdc80b312549c2f488f",
    "semantic_title": "evolving populations of diverse rl agents with map-elites",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=rmoMvptXK7M": {
    "title": "Gray-Box Gaussian Processes for Automated Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "91eda1866f77a31d40bc4a1a12124c3326508bb2",
    "semantic_title": "gray-box gaussian processes for automated reinforcement learning",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=pRCMXcfdihq": {
    "title": "Protein Sequence and Structure Co-Design with Equivariant Translation",
    "volume": "poster",
    "abstract": "Proteins are macromolecules that perform essential functions in all living organisms. Designing novel proteins with specific structures and desired functions has been a long-standing challenge in the field of bioengineering. Existing approaches generate both protein sequence and structure using either autoregressive models or diffusion models, both of which suffer from high inference costs. In this paper, we propose a new approach capable of protein sequence and structure co-design, which iteratively translates both protein sequence and structure into the desired state from random initialization, based on context features given a priori. Our model consists of a trigonometry-aware encoder that reasons geometrical constraints and interactions from context features, and a roto-translation equivariant decoder that translates protein sequence and structure interdependently. Notably, all protein amino acids are updated in one shot in each translation step, which significantly accelerates the inference process. Experimental results across multiple tasks show that our model outperforms previous state-of-the-art baselines by a large margin, and is able to design proteins of high fidelity as regards both sequence and structure, with running time orders of magnitude less than sampling-based methods",
    "checked": true,
    "id": "bc9185f8ae42c952edf84492721f27255ff2745d",
    "semantic_title": "protein sequence and structure co-design with equivariant translation",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=z0_V5O9cmNw": {
    "title": "Learning in temporally structured environments",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4d02858645b8be621eb8cc3f545164e2e907e51c",
    "semantic_title": "learning in temporally structured environments",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=cB4N3G5udUS": {
    "title": "RandProx: Primal-Dual Optimization Algorithms with Randomized Proximal Updates",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0e1fb583eb2a6ae20029c3e73404f0a73fe103eb",
    "semantic_title": "randprox: primal-dual optimization algorithms with randomized proximal updates",
    "citation_count": 37,
    "authors": []
  },
  "https://openreview.net/forum?id=NI7StoWHJPT": {
    "title": "Preserving Pre-trained Features Helps Calibrate Fine-tuned Language Models",
    "volume": "poster",
    "abstract": "Large pre-trained language models (PLMs) have demonstrated strong performance on natural language understanding (NLU) tasks through fine-tuning. However, fine-tuned models still suffer from overconfident predictions, especially in out-of-domain settings. In this paper, we tackle the problem of calibrating fine-tuned language models. We demonstrate that the PLMs are well-calibrated on the masked language modeling task with robust predictive confidence under domain shift, yet the fine-tuned models fail to retain such property due to catastrophic forgetting, which impacts the calibration on the downstream classification task. In light of these observations, we evaluate the calibration of several methods that preserve pre-trained features and show that preserving pre-trained features can improve the calibration of fine-tuned language models. Among these methods, our proposed method that encourages the fine-tuned model to learn generative representations with auxiliary language modeling objective achieves competitive accuracy and the lowest expected calibration error compared to several strong baselines under both in-domain and out-of-domain settings on three downstream NLU tasks",
    "checked": true,
    "id": "bd0d6a6bd10f80726b870450f6275f0530c24afb",
    "semantic_title": "preserving pre-trained features helps calibrate fine-tuned language models",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=UxqUgchwXkK": {
    "title": "Fast Nonlinear Vector Quantile Regression",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e89ba5d9bbe6c6e849708e8081c4e23884e61ed9",
    "semantic_title": "fast nonlinear vector quantile regression",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=yKbprarjc5B": {
    "title": "Leveraging Large Language Models for Multiple Choice Question Answering",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ed38c6b157c11476939c426ec6871c926f2f3524",
    "semantic_title": "leveraging large language models for multiple choice question answering",
    "citation_count": 207,
    "authors": []
  },
  "https://openreview.net/forum?id=h9O0wsmL-cT": {
    "title": "Regression with Label Differential Privacy",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f5e31faabc0d77fb85110e39af812a7edfecd7e8",
    "semantic_title": "regression with label differential privacy",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=fGG6vHp3W9W": {
    "title": "Hierarchical Abstraction for Combinatorial Generalization in Object Rearrangement",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3dc25449a9db4deecbcb50c8eaf6d4a59c245d70",
    "semantic_title": "hierarchical abstraction for combinatorial generalization in object rearrangement",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=tyZ1ChGZIKO": {
    "title": "Selective Frequency Network for Image Restoration",
    "volume": "poster",
    "abstract": "Image restoration aims to reconstruct the latent sharp image from its corrupted counterpart. Besides dealing with this long-standing task in the spatial domain, a few approaches seek solutions in the frequency domain in consideration of the large discrepancy between spectra of sharp/degraded image pairs. However, these works commonly utilize transformation tools, e.g., wavelet transform, to split features into several frequency parts, which is not flexible enough to select the most informative frequency component to recover. In this paper, we exploit a multi-branch and content-aware module to decompose features into separate frequency subbands dynamically and locally, and then accentuate the useful ones via channel-wise attention weights. In addition, to handle large-scale degradation blurs, we propose an extremely simple decoupling and modulation module to enlarge the receptive field via global and window-based average pooling. Integrating two developed modules into a U-Net backbone, the proposed Selective Frequency Network (SFNet) performs favorably against state-of-the-art algorithms on five image restoration tasks, including single-image defocus deblurring, image dehazing, image motion deblurring, image desnowing, and image deraining",
    "checked": true,
    "id": "6af728164e4424218967fd356b3448f5037acd88",
    "semantic_title": "selective frequency network for image restoration",
    "citation_count": 122,
    "authors": []
  },
  "https://openreview.net/forum?id=Tl8OmiibP99": {
    "title": "Improving Differentiable Neural Architecture Search by Encouraging Transferability",
    "volume": "poster",
    "abstract": "Differentiable neural architecture search methods are increasingly popular due to their computational efficiency. However, these methods have unsatisfactory generalizability and stability. Their searched architectures are often degenerate with a dominant number of skip connections and perform unsatisfactorily on test data. Existing methods for solving this problem have a variety of limitations, such as cannot prevent the happening of architecture degeneration, being excessively restrictive in setting the number of skip connections, etc. To address these limitations, we propose a new approach for improving the generalizability and stability of differentiable NAS, by developing a transferability-encouraging tri-level optimization framework which improves the architecture of a main model by encouraging good transferability to an auxiliary model. Our framework involves three stages performed end-to-end: 1) train network weights of a main model; 2) transfer knowledge from the main model to an auxiliary model; 3) optimize the architecture of the main model by maximizing its transferability to the auxiliary model. We propose a new knowledge transfer approach based on matching quadruple relative similarities. Experiments on several datasets demonstrate the effectiveness of our method",
    "checked": true,
    "id": "4a8a25aae3bc76b6ef2cb447ed9f3a71ec88ff34",
    "semantic_title": "improving differentiable neural architecture search by encouraging transferability",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=HtAfbHa7LAL": {
    "title": "MA-BERT: Towards Matrix Arithmetic-only BERT Inference by Eliminating Complex Non-Linear Functions",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "161f734370b15e4a6edf0e6db8850889f21be6a0",
    "semantic_title": "ma-bert: towards matrix arithmetic-only bert inference by eliminating complex non-linear functions",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=KyoVpYvWWnK": {
    "title": "Efficient Certified Training and Robustness Verification of Neural ODEs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b07ba82ede28d57e2e84f17fee8fef3edc24e451",
    "semantic_title": "efficient certified training and robustness verification of neural odes",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=d8mr8lKIZ3n": {
    "title": "Arbitrary Virtual Try-on Network: Characteristics Representation and Trade-off between Body and Clothing",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "57f0bd3d22966dea1cd6b92f99e3328397b1d9b9",
    "semantic_title": "arbitrary virtual try-on network: characteristics representation and trade-off between body and clothing",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6ruVLB727MC": {
    "title": "UL2: Unifying Language Learning Paradigms",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b21670e8061a06ab97e7d6052c9345a326e84ff8",
    "semantic_title": "ul2: unifying language learning paradigms",
    "citation_count": 319,
    "authors": []
  },
  "https://openreview.net/forum?id=SVl1w1u3InX": {
    "title": "CASR: Generating Complex Sequences with Autoregressive Self-Boost Refinement",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "16bab0d8e56e6d986e2e8baffdc0bc5f5b46b4e5",
    "semantic_title": "casr: generating complex sequences with autoregressive self-boost refinement",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2QzNuaRHn4Z": {
    "title": "Bitrate-Constrained DRO: Beyond Worst Case Robustness To Unknown Group Shifts",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5e4437c0ef2bcfa06102341938d63e68762527a6",
    "semantic_title": "bitrate-constrained dro: beyond worst case robustness to unknown group shifts",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=swEskiem99": {
    "title": "Feature selection and low test error in shallow low-rotation ReLU networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0512745dedebde62883f3a64e2aa331bf9cf5bd9",
    "semantic_title": "feature selection and low test error in shallow low-rotation relu networks",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=JZMR727O29": {
    "title": "Backpropagation through Combinatorial Algorithms: Identity with Projection Works",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b443a9c6f4c6ead57b03acdf8dc9a7bc0e12103c",
    "semantic_title": "backpropagation through combinatorial algorithms: identity with projection works",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=kIo_C6QmMOM": {
    "title": "Coupled Multiwavelet Operator Learning for Coupled Differential Equations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "eab89c05faa1ed6dc37875979d78424e761a147a",
    "semantic_title": "coupled multiwavelet operator learning for coupled differential equations",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=4oLK1_k71Tz": {
    "title": "Mid-Vision Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b15b76cbfcfa5b31c278dcbd7b4fe35e1e91b3fd",
    "semantic_title": "mid-vision feedback",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b39dQt_uffW": {
    "title": "Safe Reinforcement Learning From Pixels Using a Stochastic Latent Representation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "49d371e57e8f1ff63258a7b6ccc86ccd75db6244",
    "semantic_title": "safe reinforcement learning from pixels using a stochastic latent representation",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=ja4Lpp5mqc2": {
    "title": "TrojText: Test-time Invisible Textual Trojan Insertion",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "49a8ff23aa2a9d852aa8e3888d3c3d556e13d7cf",
    "semantic_title": "trojtext: test-time invisible textual trojan insertion",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=zqkfJA6R1-r": {
    "title": "Improved Training of Physics-Informed Neural Networks Using Energy-Based Priors: a Study on Electrical Impedance Tomography",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "224a29032669178b600b0e03abad6e95a8893ee9",
    "semantic_title": "improved training of physics-informed neural networks using energy-based priors: a study on electrical impedance tomography",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=wKPmPBHSnT6": {
    "title": "Ordered GNN: Ordering Message Passing to Deal with Heterophily and Over-smoothing",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "82bb898efad7e1db5e8aa41d55096da0fd269538",
    "semantic_title": "ordered gnn: ordering message passing to deal with heterophily and over-smoothing",
    "citation_count": 76,
    "authors": []
  },
  "https://openreview.net/forum?id=JknGeelZJpHP": {
    "title": "Sparse Distributed Memory is a Continual Learner",
    "volume": "poster",
    "abstract": "Continual learning is a problem for artificial neural networks that their biological counterparts are adept at solving. Building on work using Sparse Distributed Memory (SDM) to connect a core neural circuit with the powerful Transformer model, we create a modified Multi-Layered Perceptron (MLP) that is a strong continual learner. We find that every component of our MLP variant translated from biology is necessary for continual learning. Our solution is also free from any memory replay or task information, and introduces novel methods to train sparse networks that may be broadly applicable",
    "checked": true,
    "id": "a9c60602b9e0b268af152cca934eed40a7fb6937",
    "semantic_title": "sparse distributed memory is a continual learner",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=Xo2E217_M4n": {
    "title": "FLIP: A Provable Defense Framework for Backdoor Mitigation in Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "20916aae7121c798be32771b386878183fc538df",
    "semantic_title": "flip: a provable defense framework for backdoor mitigation in federated learning",
    "citation_count": 60,
    "authors": []
  },
  "https://openreview.net/forum?id=kXwdL1cWOAi": {
    "title": "UniMax: Fairer and More Effective Language Sampling for Large-Scale Multilingual Pretraining",
    "volume": "poster",
    "abstract": "Pretrained multilingual large language models have typically used heuristic temperature-based sampling to balance between different languages. However previous work has not systematically evaluated the efficacy of different pretraining language distributions across model scales. In this paper, we propose a new sampling method, UniMax, that delivers more uniform coverage of head languages while mitigating overfitting on tail languages by explicitly capping the number of repeats over each language's corpus. We perform an extensive series of ablations testing a range of sampling strategies on a suite of multilingual benchmarks, while varying model scale. We find that UniMax outperforms standard temperature-based sampling, and the benefits persist as scale increases. As part of our contribution, we release: (i) an improved and refreshed mC4 multilingual corpus consisting of 29 trillion characters across 107 languages, and (ii) a suite of pretrained umT5 model checkpoints trained with UniMax sampling",
    "checked": true,
    "id": "ba184d335a9a08c52c5d25eabd7f4a8ea987918b",
    "semantic_title": "unimax: fairer and more effective language sampling for large-scale multilingual pretraining",
    "citation_count": 63,
    "authors": []
  },
  "https://openreview.net/forum?id=rqq6Dh8t4d": {
    "title": "GNNInterpreter: A Probabilistic Generative Model-Level Explanation for Graph Neural Networks",
    "volume": "poster",
    "abstract": "Recently, Graph Neural Networks (GNNs) have significantly advanced the performance of machine learning tasks on graphs. However, this technological breakthrough makes people wonder: how does a GNN make such decisions, and can we trust its prediction with high confidence? When it comes to some critical fields, such as biomedicine, where making wrong decisions can have severe consequences, it is crucial to interpret the inner working mechanisms of GNNs before applying them. In this paper, we propose a model-agnostic model-level explanation method for different GNNs that follow the message passing scheme, GNNInterpreter, to explain the high-level decision-making process of the GNN model. More specifically, GNNInterpreter learns a probabilistic generative graph distribution that produces the most discriminative graph pattern the GNN tries to detect when making a certain prediction by optimizing a novel objective function specifically designed for the model-level explanation for GNNs. Compared to existing works, GNNInterpreter is more flexible and computationally efficient in generating explanation graphs with different types of node and edge features, without introducing another blackbox or requiring manually specified domain-specific rules. In addition, the experimental studies conducted on four different datasets demonstrate that the explanation graphs generated by GNNInterpreter match the desired graph pattern if the model is ideal; otherwise, potential model pitfalls can be revealed by the explanation",
    "checked": true,
    "id": "192067b0d238d54480d72d751cbd005e2ad2d2e4",
    "semantic_title": "gnninterpreter: a probabilistic generative model-level explanation for graph neural networks",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=OPGy07PojsZ": {
    "title": "Rethinking Symbolic Regression: Morphology and Adaptability in the Context of Evolutionary Algorithms",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "464d9f24014f1a15746979212acd712d2b91b245",
    "semantic_title": "rethinking symbolic regression: morphology and adaptability in the context of evolutionary algorithms",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=zaq4LV55xHl": {
    "title": "On Pre-training Language Model for Antibody",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a40f8e080f6b74780b9a8faa0ce02711699c5561",
    "semantic_title": "on pre-training language model for antibody",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=uR6x8Be7o_M": {
    "title": "Learning to reason over visual objects",
    "volume": "poster",
    "abstract": "A core component of human intelligence is the ability to identify abstract patterns inherent in complex, high-dimensional perceptual data, as exemplified by visual reasoning tasks such as Raven's Progressive Matrices (RPM). Motivated by the goal of designing AI systems with this capacity, recent work has focused on evaluating whether neural networks can learn to solve RPM-like problems. Previous work has generally found that strong performance on these problems requires the incorporation of inductive biases that are specific to the RPM problem format, raising the question of whether such models might be more broadly useful. Here, we investigated the extent to which a general-purpose mechanism for processing visual scenes in terms of objects might help promote abstract visual reasoning. We found that a simple model, consisting only of an object-centric encoder and a transformer reasoning module, achieved state-of-the-art results on both of two challenging RPM-like benchmarks (PGM and I-RAVEN), as well as a novel benchmark with greater visual complexity (CLEVR-Matrices). These results suggest that an inductive bias for object-centric processing may be a key component of abstract visual reasoning, obviating the need for problem-specific inductive biases",
    "checked": true,
    "id": "b816cdb27b28c1759f5060f5a1ed227fbac3efda",
    "semantic_title": "learning to reason over visual objects",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=6lUEy1J5R7p": {
    "title": "Imitating Graph-Based Planning with Goal-Conditioned Policies",
    "volume": "poster",
    "abstract": "Recently, graph-based planning algorithms have gained much attention to solve goal-conditioned reinforcement learning (RL) tasks: they provide a sequence of subgoals to reach the target-goal, and the agents learn to execute subgoal-conditioned policies. However, the sample-efficiency of such RL schemes still remains a challenge, particularly for long-horizon tasks. To address this issue, we present a simple yet effective self-imitation scheme which distills a subgoal-conditioned policy into the target-goal-conditioned policy. Our intuition here is that to reach a target-goal, an agent should pass through a subgoal, so target-goal- and subgoal- conditioned policies should be similar to each other. We also propose a novel scheme of stochastically skipping executed subgoals in a planned path, which further improves performance. Unlike prior methods that only utilize graph-based planning in an execution phase, our method transfers knowledge from a planner along with a graph into policy learning. We empirically show that our method can significantly boost the sample-efficiency of the existing goal-conditioned RL methods under various long-horizon control tasks",
    "checked": true,
    "id": "4a425aacde98c2099f721c8557a84f571af56ed7",
    "semantic_title": "imitating graph-based planning with goal-conditioned policies",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=AuEgNlEAmed": {
    "title": "A theoretical study of inductive biases in contrastive learning",
    "volume": "poster",
    "abstract": "Understanding self-supervised learning is important but challenging. Previous theoretical works study the role of pretraining losses, and view neural networks as general black boxes. However, the recent work of [Saunshi et al.] argues that the model architecture --- a component largely ignored by previous works --- also has significant influences on the downstream performance of self-supervised learning. In this work, we provide the first theoretical analysis of self-supervised learning that incorporates the effect of inductive biases originating from the model class. In particular, we focus on contrastive learning --- a popular self-supervised learning method that is widely used in the vision domain. We show that when the model has limited capacity, contrastive representations would recover certain special clustering structures that are compatible with the model architecture, but ignore many other clustering structures in the data distribution. As a result, our theory can capture the more realistic setting where contrastive representations have much lower dimensionality than the number of clusters in the data distribution. We instantiate our theory on several synthetic data distributions, and provide empirical evidence to support the theory",
    "checked": true,
    "id": "88788d73eb81dc0a1134f30a1ff815c727376681",
    "semantic_title": "a theoretical study of inductive biases in contrastive learning",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=pBBsrPzq7aF": {
    "title": "Combinatorial Pure Exploration of Causal Bandits",
    "volume": "poster",
    "abstract": "The combinatorial pure exploration of causal bandits is the following online learning task: given a causal graph with unknown causal inference distributions, in each round we choose a subset of variables to intervene or do no intervention, and observe the random outcomes of all random variables, with the goal that using as few rounds as possible, we can output an intervention that gives the best (or almost best) expected outcome on the reward variable $Y$ with probability at least $1-\\delta$, where $\\delta$ is a given confidence level. We provide the first gap-dependent and fully adaptive pure exploration algorithms on two types of causal models --- the binary generalized linear model (BGLM) and general graphs. For BGLM, our algorithm is the first to be designed specifically for this setting and achieves polynomial sample complexity, while all existing algorithms for general graphs have either sample complexity exponential to the graph size or some unreasonable assumptions. For general graphs, our algorithm provides a significant improvement on sample complexity, and it nearly matches the lower bound we prove. Our algorithms achieve such improvement by a novel integration of prior causal bandit algorithms and prior adaptive pure exploration algorithms, the former of which utilize the rich observational feedback in causal bandits but are not adaptive to reward gaps, while the latter of which have the issue in reverse",
    "checked": true,
    "id": "c023e52d78d7cf9eed549aee563b8bde4282a7b4",
    "semantic_title": "combinatorial pure exploration of causal bandits",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=C2ulri4duIs": {
    "title": "Computational Language Acquisition with Theory of Mind",
    "volume": "poster",
    "abstract": "Unlike current state-of-the-art language models, young children actively acquire language through interactions with their surrounding environment and caretakers. One mechanism that has been argued to be critical to language learning is the ability to infer the mental states of other agents in social environments, coined Theory of Mind (ToM) by Premack & Woodruff (1978). Drawing inspiration from the modern operationalized versions of ToM implemented in Rabinowitz et al. (2018) and Zhu et al. (2021), we build language-learning agents equipped with ToM, and measure its effects on the learning process. We model ToM by giving the speaker agent an internal listener model that is trained alongside the speaker and used to rerank potential utterances. We experiment with varying task difficulty, hypothesizing that models will acquire more complex language to adapt to stronger environmental pressures. We find that training speakers with a highly weighted ToM listener component leads to performance gains in our image referential game setting. We also find some evidence that increasing task difficulty in the training process results in more fluent and precise utterances in evaluation. This suggests the potential utility of further incorporating ToM, as well as other insights from child language acquisition, into computational models of language acquisition",
    "checked": true,
    "id": "e7b3b692b0816821aafc0d354749bc3802cbf6ac",
    "semantic_title": "computational language acquisition with theory of mind",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=esFxSb_0pSL": {
    "title": "Pareto Invariant Risk Minimization: Towards Mitigating the Optimization Dilemma in Out-of-Distribution Generalization",
    "volume": "poster",
    "abstract": "Recently, there has been a growing surge of interest in enabling machine learning systems to generalize well to Out-of-Distribution (OOD) data. Most efforts are devoted to advancing optimization objectives that regularize models to capture the underlying invariance; however, there often are compromises in the optimization process of these OOD objectives: i) Many OOD objectives have to be relaxed as penalty terms of Empirical Risk Minimization (ERM) for the ease of optimization, while the relaxed forms can weaken the robustness of the original objective; ii) The penalty terms also require careful tuning of the penalty weights due to the intrinsic conflicts between ERM and OOD objectives. Consequently, these compromises could easily lead to suboptimal performance of either the ERM or OOD objective. To address these issues, we introduce a multi-objective optimization (MOO) perspective to understand the OOD optimization process, and propose a new optimization scheme called PAreto Invariant Risk Minimization (PAIR). PAIR improves the robustness of OOD objectives by cooperatively optimizing with other OOD objectives, thereby bridging the gaps caused by the relaxations. Then PAIR approaches a Pareto optimal solution that trades off the ERM and OOD objectives properly. Extensive experiments on challenging benchmarks, WILDS, show that PAIR alleviates the compromises and yields top OOD performances",
    "checked": true,
    "id": "9a7a51cf95e5cb796847ec6d32c0b9ed95f1eec2",
    "semantic_title": "pareto invariant risk minimization: towards mitigating the optimization dilemma in out-of-distribution generalization",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=TGJSPbRpJX-": {
    "title": "What Makes Convolutional Models Great on Long Sequence Modeling?",
    "volume": "poster",
    "abstract": "Convolutional models have been widely used in multiple domains. However, most existing models only use local convolution, making the model unable to handle long-range dependencies efficiently. Attention overcomes this problem by aggregating global information based on the pair-wise attention score but also makes the computational complexity quadratic to the sequence length. Recently, Gu et al. proposed a model called S4 inspired by the state space model. S4 can be efficiently implemented as a global convolutional model whose kernel size equals the input sequence length. With Fast Fourier Transform, S4 can model much longer sequences than Transformers and achieve significant gains over SoTA on several long-range tasks. Despite its empirical success, S4 is involved. It requires sophisticated parameterization and initialization schemes that combine the wisdom from several prior works. As a result, S4 is less intuitive and hard to use for researchers with limited prior knowledge. Here we aim to demystify S4 and extract basic principles that contribute to the success of S4 as a global convolutional model. We focus on the structure of the convolution kernel and identify two critical but intuitive principles enjoyed by S4 that are sufficient to make up an effective global convolutional model: 1) The parameterization of the convolutional kernel needs to be efficient in the sense that the number of parameters should scale sub-linearly with sequence length. 2) The kernel needs to satisfy a decaying structure that the weights for convolving with closer neighbors are larger than the more distant ones. Based on the two principles, we propose a simple yet effective convolutional model called Structured Global Convolution (SGConv). SGConv exhibits strong empirical performance over several tasks: 1) With faster speed, SGConv surpasses the previous SoTA on Long Range Arena and Speech Command datasets. 2) When plugging SGConv into standard language and vision models, it shows the potential to improve both efficiency and performance",
    "checked": true,
    "id": "240300b1da360f22bf0b82c6817eacebba6deed4",
    "semantic_title": "what makes convolutional models great on long sequence modeling?",
    "citation_count": 101,
    "authors": []
  },
  "https://openreview.net/forum?id=6t0Kwf8-jrj": {
    "title": "Editing models with task arithmetic",
    "volume": "poster",
    "abstract": "Changing how pre-trained models behave---e.g., improving their performance on a downstream task or mitigating biases learned during pre-training---is a common practice when developing machine learning systems. In this work, we propose a new paradigm for steering the behavior of neural networks, centered around task vectors. A task vector specifies a direction in the weight space of a pre-trained model, such that movement in that direction improves performance on the task. We build task vectors by subtracting the weights of a pre-trained model from the weights of the same model after fine-tuning on a task. We show that these task vectors can be modified and combined together through arithmetic operations such as negation and addition, and the behavior of the resulting model is steered accordingly. Moreover, task vectors can be added together to improve performance on multiple tasks at once. Finally, when tasks are linked by an analogy relationship of the form ``A is to B as C is to D\", combining task vectors from three of the tasks can improve performance on the fourth, even when no data from the fourth task is used for training",
    "checked": true,
    "id": "71ba5f845bd22d42003675b7cea970ca9e590bcc",
    "semantic_title": "editing models with task arithmetic",
    "citation_count": 580,
    "authors": []
  },
  "https://openreview.net/forum?id=ZPHE4fht19t": {
    "title": "Neural Systematic Binder",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "78d398fb74d7f9c54ceb70343cd3f99c6a87add9",
    "semantic_title": "neural systematic binder",
    "citation_count": 37,
    "authors": []
  },
  "https://openreview.net/forum?id=PUIqjT4rzq7": {
    "title": "Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis",
    "volume": "poster",
    "abstract": "Large-scale diffusion models have achieved state-of-the-art results on text-to-image synthesis (T2I) tasks. Despite their ability to generate high-quality yet creative images, we observe that attribution-binding and compositional capabilities are still considered major challenging issues, especially when involving multiple objects. Attribute-binding requires the model to associate objects with the correct attribute descriptions, and compositional skills require the model to combine and generate multiple concepts into a single image. In this work, we improve these two aspects of T2I models to achieve more accurate image compositions. To do this, we incorporate linguistic structures with the diffusion guidance process based on the controllable properties of manipulating cross-attention layers in diffusion-based T2I models. We observe that keys and values in cross-attention layers have strong semantic meanings associated with object layouts and content. Therefore, by manipulating the cross-attention representations based on linguistic insights, we can better preserve the compositional semantics in the generated image. Built upon Stable Diffusion, a SOTA T2I model, our structured cross-attention design is efficient that requires no additional training samples. We achieve better compositional skills in qualitative and quantitative results, leading to a significant 5-8\\% advantage in head-to-head user comparison studies. Lastly, we conduct an in-depth analysis to reveal potential causes of incorrect image compositions and justify the properties of cross-attention layers in the generation process",
    "checked": true,
    "id": "25de00096c45121a06668bc501f91adec5d0aff9",
    "semantic_title": "training-free structured diffusion guidance for compositional text-to-image synthesis",
    "citation_count": 341,
    "authors": []
  },
  "https://openreview.net/forum?id=ipflrGaf7ry": {
    "title": "Can Agents Run Relay Race with Strangers? Generalization of RL to Out-of-Distribution Trajectories",
    "volume": "poster",
    "abstract": "In this paper, we evaluate and improve the generalization performance for reinforcement learning (RL) agents on the set of ``controllable'' states, where good policies exist on these states to achieve the goal. An RL agent that generally masters a task should reach its goal starting from any controllable state of the environment instead of memorizing a small set of trajectories. To practically evaluate this type of generalization, we propose relay evaluation, which starts the test agent from the middle of other independently well-trained stranger agents' trajectories. With extensive experimental evaluation, we show the prevalence of generalization failure on controllable states from stranger agents. For example, in the Humanoid environment, we observed that a well-trained Proximal Policy Optimization (PPO) agent, with only 3.9\\% failure rate during regular testing, failed on 81.6\\% of the states generated by well-trained stranger PPO agents. To improve \"relay generalization,\" we propose a novel method called Self-Trajectory Augmentation (STA), which will reset the environment to the agent's old states according to the Q function during training. After applying STA to the Soft Actor Critic's (SAC) training procedure, we reduced the failure rate of SAC under relay-evaluation by more than three times in most settings without impacting agent performance and increasing the needed number of environment interactions. Our code is available at https://github.com/lan-lc/STA",
    "checked": true,
    "id": "c7e4da026f47339697a522508893f2fa261b52ea",
    "semantic_title": "can agents run relay race with strangers? generalization of rl to out-of-distribution trajectories",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=NE2911Kq1sp": {
    "title": "CktGNN: Circuit Graph Neural Network for Electronic Design Automation",
    "volume": "poster",
    "abstract": "The electronic design automation of analog circuits has been a longstanding challenge in the integrated circuit field due to the huge design space and complex design trade-offs among circuit specifications. In the past decades, intensive research efforts have only been paid to automate the transistor sizing with a given circuit topology. By recognizing the graph nature of circuits, this paper presents a Circuit Graph Neural Network (CktGNN) that simultaneously automates the circuit topology generation and device sizing based on the encoder-dependent optimization subroutines. Particularly, CktGNN encodes circuit graphs using a two-level GNN framework (of nested GNN) where circuits are represented as combinations of subgraphs in a known subgraph basis. In this way, it significantly improves efficiency by reducing the number of subgraphs to perform message passing. Nonetheless, another critical roadblock to advancing learning-assisted circuit design automation is a lack of public benchmarks to perform canonical assessment and reproducible research. To tackle the challenge, we introduce Open Circuit Benchmark (OCB), an open-sourced dataset that contains $10$K distinct operational amplifiers with carefully-extracted circuit specifications from physical implementations. OCB also equips with communicative circuit generation and evaluation capabilities such that it can be used to generalize the applicability of CktGNN to design various analog circuits by efficiently producing corresponding datasets. Experiments on OCB show the extraordinary advantages of CktGNN through representation-based optimization frameworks over other recent powerful GNN baselines and manual design from human experts. Our work paves the way toward a learning-based open-sourced design automation flow for analog circuits",
    "checked": true,
    "id": "430a5e44fc2fc16e2376ef7197101816688f2ca7",
    "semantic_title": "cktgnn: circuit graph neural network for electronic design automation",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=0pdSt3oyJa1": {
    "title": "Specformer: Spectral Graph Neural Networks Meet Transformers",
    "volume": "poster",
    "abstract": "Spectral graph neural networks (GNNs) learn graph representations via spectral-domain graph convolutions. However, most existing spectral graph filters are scalar-to-scalar functions, i.e., mapping a single eigenvalue to a single filtered value, thus ignoring the global pattern of the spectrum. Furthermore, these filters are often constructed based on some fixed-order polynomials, which have limited expressiveness and flexibility. To tackle these issues, we introduce Specformer, which effectively encodes the set of all eigenvalues and performs self-attention in the spectral domain, leading to a learnable set-to-set spectral filter. We also design a decoder with learnable bases to enable non-local graph convolution. Importantly, Specformer is equivariant to permutation. By stacking multiple Specformer layers, one can build a powerful spectral GNN. On synthetic datasets, we show that our Specformer can better recover ground-truth spectral filters than other spectral GNNs. Extensive experiments of both node-level and graph-level tasks on real-world graph datasets show that our Specformer outperforms state-of-the-art GNNs and learns meaningful spectrum patterns. Code and data are available at https://github.com/bdy9527/Specformer",
    "checked": true,
    "id": "c193011099906126fe7b6cfcb04062cf4591ccf9",
    "semantic_title": "specformer: spectral graph neural networks meet transformers",
    "citation_count": 93,
    "authors": []
  },
  "https://openreview.net/forum?id=qFVVBzXxR2V": {
    "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a",
    "semantic_title": "language models are greedy reasoners: a systematic formal analysis of chain-of-thought",
    "citation_count": 331,
    "authors": []
  },
  "https://openreview.net/forum?id=5lgD4vU-l24s": {
    "title": "Recursive Time Series Data Augmentation",
    "volume": "poster",
    "abstract": "Time series observations can be seen as realizations of an underlying dynamical system governed by rules that we typically do not know. For time series learning tasks we create our model using available data. Training on available realizations, where data is limited, often induces severe over-fitting thereby preventing generalization. To address this issue, we introduce a general recursive framework for time series augmentation, which we call the Recursive Interpolation Method (RIM). New augmented time series are generated using a recursive interpolation function from the original time series for use in training. We perform theoretical analysis to characterize the proposed RIM and to guarantee its performance under certain conditions. We apply RIM to diverse synthetic and real-world time series cases to achieve strong performance over non-augmented data on a variety of learning tasks. Our method is also computationally more efficient and leads to better performance when compared to state of the art time series data augmentation",
    "checked": false,
    "id": "7d6ef99fc8d55a2244b936706323f1f1d32f9489",
    "semantic_title": "don't overfit the history - recursive time series data augmentation",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=JjCAdMUlu9v": {
    "title": "Auto-Encoding Goodness of Fit",
    "volume": "poster",
    "abstract": "For generative autoencoders to learn a meaningful latent representation for data generation, a careful balance must be achieved between reconstruction error and how close the distribution in the latent space is to the prior. However, this balance is challenging to achieve due to a lack of criteria that work both at the mini-batch (local) and aggregated posterior (global) level. In this work, we develop the Goodness of Fit Autoencoder (GoFAE), which incorporates hypothesis tests at two levels. At the mini-batch level, it uses GoF test statistics as regularization objectives. At a more global level, it selects a regularization coefficient based on higher criticism, i.e., a test on the uniformity of the local GoF p-values. We justify the use of GoF tests by providing a relaxed $L_2$-Wasserstein bound on the distance between the latent distribution and target prior. We propose to use GoF tests and prove that optimization based on these tests can be done with stochastic gradient (SGD) descent on a compact Riemannian manifold. Empirically, we show that our higher criticism parameter selection procedure balances reconstruction and generation using mutual information and uniformity of p-values respectively. Finally, we show that GoFAE achieves comparable FID scores and mean squared errors with competing deep generative models while retaining statistical indistinguishability from Gaussian in the latent space based on a variety of hypothesis tests",
    "checked": true,
    "id": "1df72e6e73be46d90307c8595ff9a3669f860b01",
    "semantic_title": "auto-encoding goodness of fit",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=WGApODQvwRg": {
    "title": "Understanding the Covariance Structure of Convolutional Filters",
    "volume": "poster",
    "abstract": "Neural network weights are typically initialized at random from univariate distributions, controlling just the variance of individual weights even in highly-structured operations like convolutions. Recent ViT-inspired convolutional networks such as ConvMixer and ConvNeXt use large-kernel depthwise convolutions whose learned filters have notable structure; this presents an opportunity to study their empirical covariances. In this work, we first observe that such learned filters have highly-structured covariance matrices, and moreover, we find that covariances calculated from small networks may be used to effectively initialize a variety of larger networks of different depths, widths, patch sizes, and kernel sizes, indicating a degree of model-independence to the covariance structure. Motivated by these findings, we then propose a learning-free multivariate initialization scheme for convolutional filters using a simple, closed-form construction of their covariance. Models using our initialization outperform those using traditional univariate initializations, and typically meet or exceed the performance of those initialized from the covariances of learned filters; in some cases, this improvement can be achieved without training the depthwise convolutional filters at all. Our code is available at https://github.com/locuslab/convcov",
    "checked": true,
    "id": "236a3805c01164d3319cf03fc2be083daaa65fb0",
    "semantic_title": "understanding the covariance structure of convolutional filters",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=mWRngkvIki3": {
    "title": "Masked Distillation with Receptive Tokens",
    "volume": "poster",
    "abstract": "Distilling from the feature maps can be fairly effective for dense prediction tasks since both the feature discriminability and localization information can be well transferred. However, not every pixel contributes equally to the performance, and a good student should learn from what really matters to the teacher. In this paper, we introduce a learnable embedding dubbed receptive token to locate the pixels of interests (PoIs) in the feature map, with a distillation mask generated via pixel-wise attention. Then the masked distillation will be performed via the pixel-wise reconstruction. In this way, a distillation mask refers to a pattern of pixel dependencies. We thus adopt multiple receptive tokens to investigate more sophisticated and informative pixel dependencies within feature maps to enhance the distillation. To obtain a group of masks, the receptive tokens are learned via the regular task loss but with teacher fixed, and we also leverage a Dice loss to enrich the diversity of obtained masks. Our method dubbed MasKD is simple and practical, and needs no priors of ground-truth labels, which can apply to various dense prediction tasks. Experiments show that our MasKD can achieve state-of-the-art performance consistently on object detection and semantic segmentation benchmarks",
    "checked": true,
    "id": "874fcbf4b82d292bf974094136d50763dd60833d",
    "semantic_title": "masked distillation with receptive tokens",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=ctmLBs8lITa": {
    "title": "Robust Multivariate Time-Series Forecasting: Adversarial Attacks and Defense Mechanisms",
    "volume": "poster",
    "abstract": "This work studies the threats of adversarial attack on multivariate probabilistic forecasting models and viable defense mechanisms. Our studies discover a new attack pattern that negatively impact the forecasting of a target time series via making strategic, sparse (imperceptible) modifications to the past observations of a small number of other time series. To mitigate the impact of such attack, we have developed two defense strategies. First, we extend a previously developed randomized smoothing technique in classification to multivariate forecasting scenarios. Second, we develop an adversarial training algorithm that learns to create adversarial examples and at the same time optimizes the forecasting model to improve its robustness against such adversarial simulation. Extensive experiments on real-world datasets confirm that our attack schemes are powerful and our defense algorithms are more effective compared with baseline defense mechanisms",
    "checked": true,
    "id": "84dc2a159c062ceedae62c4a3c682f18ead59812",
    "semantic_title": "robust multivariate time-series forecasting: adversarial attacks and defense mechanisms",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=xIWfWvKM7aQ": {
    "title": "TextShield: Beyond Successfully Detecting Adversarial Sentences in text classification",
    "volume": "poster",
    "abstract": "Adversarial attack serves as a major challenge for neural network models in NLP, which precludes the model's deployment in safety-critical applications. A recent line of work, detection-based defense, aims to distinguish adversarial sentences from benign ones. However, {the core limitation of previous detection methods is being incapable of giving correct predictions on adversarial sentences unlike defense methods from other paradigms.} To solve this issue, this paper proposes TextShield: (1) we discover a link between text attack and saliency information, and then we propose a saliency-based detector, which can effectively detect whether an input sentence is adversarial or not. (2) We design a saliency-based corrector, which converts the detected adversary sentences to benign ones. By combining the saliency-based detector and corrector, TextShield extends the detection-only paradigm to a detection-correction paradigm, thus filling the gap in the existing detection-based defense. Comprehensive experiments show that (a) TextShield consistently achieves higher or comparable performance than state-of-the-art defense methods across various attacks on different benchmarks. (b) our saliency-based detector outperforms existing detectors for detecting adversarial sentences",
    "checked": true,
    "id": "05d5a07d0688028e0cef5f9246e7949bb28dd01b",
    "semantic_title": "textshield: beyond successfully detecting adversarial sentences in text classification",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=14-kr46GvP-": {
    "title": "Efficient Deep Reinforcement Learning Requires Regulating Overfitting",
    "volume": "poster",
    "abstract": "Deep reinforcement learning algorithms that learn policies by trial-and-error must learn from limited amounts of data collected by actively interacting with the environment. While many prior works have shown that proper regularization techniques are crucial for enabling data-efficient RL, a general understanding of the bottlenecks in data-efficient RL has remained unclear. Consequently, it has been difficult to devise a universal technique that works well across all domains. In this paper, we attempt to understand the primary bottleneck in sample-efficient deep RL by examining several potential hypotheses such as non-stationarity, excessive action distribution shift, and overfitting. We perform thorough empirical analysis on state-based DeepMind control suite (DMC) tasks in a controlled and systematic way to show that high temporal-difference (TD) error on the validation set of transitions is the main culprit that severely affects the performance of deep RL algorithms, and prior methods that lead to good performance do in fact, control the validation TD error to be low. This observation gives us a robust principle for making deep RL efficient: we can hill-climb on the validation TD error by utilizing any form of regularization techniques from supervised learning. We show that a simple online model selection method that targets the validation TD error is effective across state-based DMC and Gym tasks",
    "checked": true,
    "id": "f04fe5f3f47f5b25e5295c29cdc8b109887f959c",
    "semantic_title": "efficient deep reinforcement learning requires regulating overfitting",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=6jfbOWzWTcE": {
    "title": "Offline Reinforcement Learning with Differentiable Function Approximation is Provably Efficient",
    "volume": "poster",
    "abstract": "Offline reinforcement learning, which aims at optimizing sequential decision-making strategies with historical data, has been extensively applied in real-life applications. State-Of-The-Art algorithms usually leverage powerful function approximators (e.g. neural networks) to alleviate the sample complexity hurdle for better empirical performances. Despite the successes, a more systematic under- standing of the statistical complexity for function approximation remains lacking. Towards bridging the gap, we take a step by considering offline reinforcement learning with differentiable function class approximation (DFA). This function class naturally incorporates a wide range of models with nonlinear/nonconvex structures. We show offline RL with differentiable function approximation is provably efficient by analyzing the pessimistic fitted Q-learning (PFQL) algorithm, and our results provide the theoretical basis for understanding a variety of practical heuristics that rely on Fitted Q-Iteration style design. In addition, we further im- prove our guarantee with a tighter instance-dependent characterization. We hope our work could draw interest in studying reinforcement learning with differentiable function approximation beyond the scope of current research",
    "checked": true,
    "id": "839fd6f8fb5f1cc3be573e448f11f459e06abd7d",
    "semantic_title": "offline reinforcement learning with differentiable function approximation is provably efficient",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oGDKSt9JrZi": {
    "title": "Proto-Value Networks: Scaling Representation Learning with Auxiliary Tasks",
    "volume": "poster",
    "abstract": "Auxiliary tasks improve the representations learned by deep reinforcement learning agents. Analytically, their effect is reasonably well-understood; in practice, how-ever, their primary use remains in support of a main learning objective, rather than as a method for learning representations. This is perhaps surprising given that many auxiliary tasks are defined procedurally, and hence can be treated as an essentially infinite source of information about the environment. Based on this observation, we study the effectiveness of auxiliary tasks for learning rich representations, focusing on the setting where the number of tasks and the size of the agent's network are simultaneously increased. For this purpose, we derive a new family of auxiliary tasks based on the successor measure. These tasks are easy to implement and have appealing theoretical properties. Combined with a suitable off-policy learning rule, the result is a representation learning algorithm that can be understood as extending Mahadevan & Maggioni (2007)'s proto-value functions to deep reinforcement learning – accordingly, we call the resulting object proto-value networks. Through a series of experiments on the Arcade Learning Environment, we demonstrate that proto-value networks produce rich features that may be used to obtain performance comparable to established algorithms, using only linear approximation and a small number (~4M) of interactions with the environment's reward function",
    "checked": true,
    "id": "a2b2abdb4566beee1e29f19bc6914ce67680acf3",
    "semantic_title": "proto-value networks: scaling representation learning with auxiliary tasks",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=I29Kt0RwChs": {
    "title": "Robust Algorithms on Adaptive Inputs from Bounded Adversaries",
    "volume": "poster",
    "abstract": "We study dynamic algorithms robust to adaptive input generated from sources with bounded capabilities, such as sparsity or limited interaction. For example, we consider robust linear algebraic algorithms when the updates to the input are sparse but given by an adversary with access to a query oracle. We also study robust algorithms in the standard centralized setting, where an adversary queries an algorithm in an adaptive manner, but the number of interactions between the adversary and the algorithm is bounded. We first recall a unified framework of (Hassidim et al., 2020; Beimel et al., 2022; Attias et al., 2023) for answering $Q$ adaptive queries that incurs $\\widetilde{\\mathcal{O}}(\\sqrt{Q})$ overhead in space, which is roughly a quadratic improvement over the na\\\"{i}ve implementation, and only incurs a logarithmic overhead in query time. Although the general framework has diverse applications in machine learning and data science, such as adaptive distance estimation, kernel density estimation, linear regression, range queries, point queries, and serves as a preliminary benchmark, we demonstrate even better algorithmic improvements for (1) reducing the pre-processing time for adaptive distance estimation and (2) permitting an unlimited number of adaptive queries for kernel density estimation. Finally, we complement our theoretical results with additional empirical evaluations",
    "checked": true,
    "id": "d10e3ef7fa49e13915d0e93c0877f2f2a26cd97a",
    "semantic_title": "robust algorithms on adaptive inputs from bounded adversaries",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=7jk5gWjC18M": {
    "title": "Chasing All-Round Graph Representation Robustness: Model, Training, and Optimization",
    "volume": "poster",
    "abstract": "Graph Neural Networks (GNNs) have achieved state-of-the-art results on a variety of graph learning tasks, however, it has been demonstrated that they are vulnerable to adversarial attacks, raising serious security concerns. A lot of studies have been developed to train GNNs in a noisy environment and increase their robustness against adversarial attacks. However, existing methods have not uncovered a principled difficulty: the convoluted mixture distribution between clean and attacked data samples, which leads to sub-optimal model design and limits their frameworks' robustness. In this work, we first begin by identifying the root cause of mixture distribution, then, for tackling it, we propose a novel method GAME - Graph Adversarial Mixture of Experts to enlarge the model capacity and enrich the representation diversity of adversarial samples, from three perspectives of model, training, and optimization. Specifically, we first propose a plug-and- play GAME layer that can be easily incorporated into any GNNs and enhance their adversarial learning capabilities. Second, we design a decoupling-based graph adversarial training in which the component of the model used to generate adversarial graphs is separated from the component used to update weights. Third, we introduce a graph diversity regularization that enables the model to learn diverse representation and further improves model performance. Extensive experiments demonstrate the effectiveness and advantages of GAME over the state-of-the-art adversarial training methods across various datasets given different attacks",
    "checked": true,
    "id": "89eb2fd47d93dd15354c84c41df599669c68b829",
    "semantic_title": "chasing all-round graph representation robustness: model, training, and optimization",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=4gc3MGZra1d": {
    "title": "On Representing Mixed-Integer Linear Programs by Graph Neural Networks",
    "volume": "poster",
    "abstract": "While Mixed-integer linear programming (MILP) is NP-hard in general, practical MILP has received roughly 100--fold speedup in the past twenty years. Still, many classes of MILPs quickly become unsolvable as their sizes increase, motivating researchers to seek new acceleration techniques for MILPs. With deep learning, they have obtained strong empirical results, and many results were obtained by applying graph neural networks (GNNs) to making decisions in various stages of MILP solution processes. This work discovers a fundamental limitation: there exist feasible and infeasible MILPs that all GNNs will, however, treat equally, indicating GNN's lacking power to express general MILPs. Then, we show that, by restricting the MILPs to unfoldable ones or by adding random features, there exist GNNs that can reliably predict MILP feasibility, optimal objective values, and optimal solutions up to prescribed precision. We conducted small-scale numerical experiments to validate our theoretical findings",
    "checked": true,
    "id": "078fc54e3311e18d2e407e927192760e2bb67ae5",
    "semantic_title": "on representing mixed-integer linear programs by graph neural networks",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fWWFv--P0xP": {
    "title": "On the Importance and Applicability of Pre-Training for Federated Learning",
    "volume": "poster",
    "abstract": "Pre-training is prevalent in nowadays deep learning to improve the learned model's performance. However, in the literature on federated learning (FL), neural networks are mostly initialized with random weights. These attract our interest in conducting a systematic study to explore pre-training for FL. Across multiple visual recognition benchmarks, we found that pre-training can not only improve FL, but also close its accuracy gap to the counterpart centralized learning, especially in the challenging cases of non-IID clients' data. To make our findings applicable to situations where pre-trained models are not directly available, we explore pre-training with synthetic data or even with clients' data in a decentralized manner, and found that they can already improve FL notably. Interestingly, many of the techniques we explore are complementary to each other to further boost the performance, and we view this as a critical result toward scaling up deep FL for real-world applications. We conclude our paper with an attempt to understand the effect of pre-training on FL. We found that pre-training enables the learned global models under different clients' data conditions to converge to the same loss basin, and makes global aggregation in FL more stable. Nevertheless, pre-training seems to not alleviate local model drifting, a fundamental problem in FL under non-IID data",
    "checked": true,
    "id": "393148d88541881b590c36eaf0effc4a6a823035",
    "semantic_title": "on the importance and applicability of pre-training for federated learning",
    "citation_count": 91,
    "authors": []
  },
  "https://openreview.net/forum?id=yVqC6gCNf4d": {
    "title": "Simple initialization and parametrization of sinusoidal networks via their kernel bandwidth",
    "volume": "poster",
    "abstract": "Neural networks with sinusoidal activations have been proposed as an alternative to networks with traditional activation functions. Despite their promise, particularly for learning implicit models, their training behavior is not yet fully understood, leading to a number of empirical design choices that are not well justified. In this work, we first propose a simplified version of such sinusoidal neural networks, which allows both for easier practical implementation and simpler theoretical analysis. We then analyze the behavior of these networks from the neural tangent kernel perspective and demonstrate that their kernel approximates a low-pass filter with an adjustable bandwidth. Finally, we utilize these insights to inform the sinusoidal network initialization, optimizing their performance for each of a series of tasks, including learning implicit models and solving differential equations",
    "checked": true,
    "id": "06074aeb225dcad22076bfa997e465c18a607141",
    "semantic_title": "simple initialization and parametrization of sinusoidal networks via their kernel bandwidth",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=29V3AWjVAFi": {
    "title": "The Best of Both Worlds: Accurate Global and Personalized Models through Federated Learning with Data-Free Hyper-Knowledge Distillation",
    "volume": "poster",
    "abstract": "Heterogeneity of data distributed across clients limits the performance of global models trained through federated learning, especially in the settings with highly imbalanced class distributions of local datasets. In recent years, personalized federated learning (pFL) has emerged as a potential solution to the challenges presented by heterogeneous data. However, existing pFL methods typically enhance performance of local models at the expense of the global model's accuracy. We propose FedHKD (Federated Hyper-Knowledge Distillation), a novel FL algorithm in which clients rely on knowledge distillation (KD) to train local models. In particular, each client extracts and sends to the server the means of local data representations and the corresponding soft predictions -- information that we refer to as ``hyper-knowledge\". The server aggregates this information and broadcasts it to the clients in support of local training. Notably, unlike other KD-based pFL methods, FedHKD does not rely on a public dataset nor it deploys a generative model at the server. We analyze convergence of FedHKD and conduct extensive experiments on visual datasets in a variety of scenarios, demonstrating that FedHKD provides significant improvement in both personalized as well as global model performance compared to state-of-the-art FL methods designed for heterogeneous data settings",
    "checked": true,
    "id": "ba2251da5893bb9407bb5050c279900019df160e",
    "semantic_title": "the best of both worlds: accurate global and personalized models through federated learning with data-free hyper-knowledge distillation",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=JmkjrlVE-DG": {
    "title": "Over-Training with Mixup May Hurt Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "99486d35169e1937b8ff9e5abd7227b6d1e901b9",
    "semantic_title": "over-training with mixup may hurt generalization",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=0eTTKOOOQkV": {
    "title": "HiCLIP: Contrastive Language-Image Pretraining with Hierarchy-aware Attention",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a935ba7ce7fd44fe372c6860504fbc164f012f03",
    "semantic_title": "hiclip: contrastive language-image pretraining with hierarchy-aware attention",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=p6jsTidUkPx": {
    "title": "Quantile Risk Control: A Flexible Framework for Bounding the Probability of High-Loss Predictions",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "722ee8624f376347a58d9948248227ab6d2eb3c8",
    "semantic_title": "quantile risk control: a flexible framework for bounding the probability of high-loss predictions",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=YlGsTZODyjz": {
    "title": "The Tilted Variational Autoencoder: Improving Out-of-Distribution Detection",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "20ca62aae323c8778a978300c682ac8e1d160e74",
    "semantic_title": "the tilted variational autoencoder: improving out-of-distribution detection",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=B4maZQLLW0_": {
    "title": "Stateful Active Facilitator: Coordination and Environmental Heterogeneity in Cooperative Multi-Agent Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NDWl9qcUpvy": {
    "title": "Learning Achievement Structure for Structured Exploration in Domains with Sparse Reward",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2f1fee5087d47e7a8c71763c22ac784c9565278c",
    "semantic_title": "learning achievement structure for structured exploration in domains with sparse reward",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=WBXbRs63oVu": {
    "title": "PINTO: Faithful Language Reasoning Using Prompt-Generated Rationales",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "deff814eefe597b2fe275bc3dd205ecb1cc09c4e",
    "semantic_title": "pinto: faithful language reasoning using prompt-generated rationales",
    "citation_count": 67,
    "authors": []
  },
  "https://openreview.net/forum?id=6doXHqwMayf": {
    "title": "Excess Risk of Two-Layer ReLU Neural Networks in Teacher-Student Settings and its Superiority to Kernel Methods",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8tYRqb05pVn": {
    "title": "Linearly Mapping from Image to Text Space",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c4cb3f7056f1216c1ddfbe4b9e55cbc07a1e43b9",
    "semantic_title": "linearly mapping from image to text space",
    "citation_count": 124,
    "authors": []
  },
  "https://openreview.net/forum?id=sAOOeI878Ns": {
    "title": "Characterizing intrinsic compositionality in transformers with Tree Projections",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5116af9e9265044d0919111f1e0e1eb283de3a1f",
    "semantic_title": "characterizing intrinsic compositionality in transformers with tree projections",
    "citation_count": 42,
    "authors": []
  },
  "https://openreview.net/forum?id=5vM51iamNeL": {
    "title": "Augmentation Component Analysis: Modeling Similarity via the Augmentation Overlaps",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a0c935bd39f512d4eddb19edde1b6dfee637e880",
    "semantic_title": "augmentation component analysis: modeling similarity via the augmentation overlaps",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=gcD2UtCGMc2": {
    "title": "Replicable Bandits",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "59a6933e06879f8f495629572a1117881b5c7238",
    "semantic_title": "replicable bandits",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=nJ3Vx78Nf7p": {
    "title": "Neural Bregman Divergences for Distance Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "19bb4d14e3e7d322b961b4e2df6e67e88a6ecf22",
    "semantic_title": "neural bregman divergences for distance learning",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=V7CYzdruWdm": {
    "title": "Bias Propagation in Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6ae73f371654b234372c22ea4d0df15159b6ccd0",
    "semantic_title": "bias propagation in federated learning",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=R0Xxvr_X3ZA": {
    "title": "Causal Confusion and Reward Misidentification in Preference-Based Reward Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6be327602deb674d0e9f3b606f3e6baf733bf266",
    "semantic_title": "causal confusion and reward misidentification in preference-based reward learning",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=Z63RvyAZ2Vh": {
    "title": "UniKGQA: Unified Retrieval and Reasoning for Solving Multi-hop Question Answering Over Knowledge Graph",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2d01da2c9ece0969d6ec56d22f78caf57050fc03",
    "semantic_title": "unikgqa: unified retrieval and reasoning for solving multi-hop question answering over knowledge graph",
    "citation_count": 95,
    "authors": []
  },
  "https://openreview.net/forum?id=bRwBpKrNzF7": {
    "title": "Faster Last-iterate Convergence of Policy Optimization in Zero-Sum Markov Games",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rB3zRN0lBYr": {
    "title": "Memorization Capacity of Neural Networks with Conditional Computation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e6f89cdc53ea40bbfc46c97623992ed80a600b56",
    "semantic_title": "memorization capacity of neural networks with conditional computation",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=YfUICnZMwk7": {
    "title": "Weighted Clock Logic Point Process",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ed1ed11dccc865f1448caa81ebc5a540184f1c05",
    "semantic_title": "weighted clock logic point process",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=NUl0ylt7SM": {
    "title": "Simple Emergent Action Representations from Multi-Task Policy Training",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a78248272e8b87e6f7650fdc29dbb77454c1a745",
    "semantic_title": "simple emergent action representations from multi-task policy training",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=JQc2VowqCzz": {
    "title": "Interaction-Based Disentanglement of Entities for Object-Centric World Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ac62d8e239eb90363a472681b09bac7bf4128f9a",
    "semantic_title": "interaction-based disentanglement of entities for object-centric world models",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=-ng-FXFlzgK": {
    "title": "Neural Image-based Avatars: Generalizable Radiance Fields for Human Avatar Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "12768e64c370ec7fcb9031df24bdc2e75f559d6b",
    "semantic_title": "neural image-based avatars: generalizable radiance fields for human avatar modeling",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=38m4h8HcNRL": {
    "title": "Federated Neural Bandits",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ad1bb40a89a889a87c5681749f2ddcbaf4292a6a",
    "semantic_title": "federated neural bandits",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=6axIMJA7ME3": {
    "title": "Compositional Task Representations for Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "66a8358bee061ed45ff49d08694f3a7c3abb4b38",
    "semantic_title": "compositional task representations for large language models",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=gU5sJ6ZggcX": {
    "title": "REPAIR: REnormalizing Permuted Activations for Interpolation Repair",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3a1f0963f57baf8021c434ac14e5d4132b6735bd",
    "semantic_title": "repair: renormalizing permuted activations for interpolation repair",
    "citation_count": 103,
    "authors": []
  },
  "https://openreview.net/forum?id=HZf7UbpWHuA": {
    "title": "Diffusion-GAN: Training GANs with Diffusion",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9c3ceae3cf605f934cc5f04a44feae23b5252faa",
    "semantic_title": "diffusion-gan: training gans with diffusion",
    "citation_count": 249,
    "authors": []
  },
  "https://openreview.net/forum?id=cWmtUcsYC3V": {
    "title": "Mind the Pool: Convolutional Neural Networks Can Overfit Input Size",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "97074d9b00a967c0c840dc170347d92f42dc3132",
    "semantic_title": "mind the pool: convolutional neural networks can overfit input size",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=Kpdewuy7RU6": {
    "title": "Reparameterization through Spatial Gradient Scaling",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9017cb3911c9dd3730e28689c2346e59d0c86e09",
    "semantic_title": "reparameterization through spatial gradient scaling",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=-ENYHCE8zBp": {
    "title": "Unsupervised Learning for Combinatorial Optimization Needs Meta Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r0BrY4BiEXO": {
    "title": "Decepticons: Corrupted Transformers Breach Privacy in Federated Learning for Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f2cd15c1925ef54d58b3d71506d7113d7911a8c2",
    "semantic_title": "decepticons: corrupted transformers breach privacy in federated learning for language models",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=zgVDqw9ZUES": {
    "title": "Adaptive Optimization in the $\\infty$-Width Limit",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sckjveqlCZ": {
    "title": "Broken Neural Scaling Laws",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "61f329722cd94291898c2c8131606a55f7a07219",
    "semantic_title": "broken neural scaling laws",
    "citation_count": 80,
    "authors": []
  },
  "https://openreview.net/forum?id=5BaqCFVh5qL": {
    "title": "Avoiding spurious correlations via logit correction",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c4ade0185969dca4b48e280b63cab54d71d7d492",
    "semantic_title": "avoiding spurious correlations via logit correction",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=wNUgn1n6esQ": {
    "title": "Safe Exploration Incurs Nearly No Additional Sample Complexity for Reward-Free RL",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "50911bce5f5aee1eb1d27ca2b17d483e9c3cf0c5",
    "semantic_title": "safe exploration incurs nearly no additional sample complexity for reward-free rl",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=Nayau9fwXU": {
    "title": "Diffusion-based Image Translation using disentangled style and content representation",
    "volume": "poster",
    "abstract": "Diffusion-based image translation guided by semantic texts or a single target image has enabled flexible style transfer which is not limited to the specific domains. Unfortunately, due to the stochastic nature of diffusion models, it is often difficult to maintain the original content of the image during the reverse diffusion. To address this, here we present a novel diffusion-based unsupervised image translation method, dubbed as DiffuseIT, using disentangled style and content representation. Specifically, inspired by the slicing Vision Transformer, we extract intermediate keys of multihead self attention layer from ViT model and used them as the content preservation loss. Then, an image guided style transfer is performed by matching the [CLS] classification token from the denoised samples and target image, whereas additional CLIP loss is used for the text-driven style transfer. To further accelerate the semantic change during the reverse diffusion, we also propose a novel semantic divergence loss and resampling strategy. Our experimental results show that the proposed method outperforms state-of-the-art baseline models in both text-guided and image-guided translation tasks",
    "checked": true,
    "id": "52472459ea81b6ebc65d16a0c80005f749542cba",
    "semantic_title": "diffusion-based image translation using disentangled style and content representation",
    "citation_count": 168,
    "authors": []
  },
  "https://openreview.net/forum?id=d7Q0vVfJ0wO": {
    "title": "Implicit Regularization for Group Sparsity",
    "volume": "poster",
    "abstract": "We study the implicit regularization of gradient descent towards structured sparsity via a novel neural reparameterization, which we call a diagonally grouped linear neural network. We show the following intriguing property of our reparameterization: gradient descent over the squared regression loss, without any explicit regularization, biases towards solutions with a group sparsity structure. In contrast to many existing works in understanding implicit regularization, we prove that our training trajectory cannot be simulated by mirror descent. We analyze the gradient dynamics of the corresponding regression problem in the general noise setting and obtain minimax-optimal error rates. Compared to existing bounds for implicit sparse regularization using diagonal linear networks, our analysis with the new reparameterization shows improved sample complexity. In the degenerate case of size-one groups, our approach gives rise to a new algorithm for sparse linear regression. Finally, we demonstrate the efficacy of our approach with several numerical experiments",
    "checked": true,
    "id": "6ed45c42f320d94ae5f209125b97ec84666bb211",
    "semantic_title": "implicit regularization for group sparsity",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=92gvk82DE-": {
    "title": "Large Language Models are Human-Level Prompt Engineers",
    "volume": "poster",
    "abstract": "By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the \"program,\" optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 21/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts",
    "checked": true,
    "id": "4610ffb1b016acaa82a2065ffd1a3adbae1ce722",
    "semantic_title": "large language models are human-level prompt engineers",
    "citation_count": 964,
    "authors": []
  },
  "https://openreview.net/forum?id=i-DleYh34BM": {
    "title": "Pruning Deep Neural Networks from a Sparsity Perspective",
    "volume": "poster",
    "abstract": "In recent years, deep network pruning has attracted significant attention in order to enable the rapid deployment of AI into small devices with computation and memory constraints. Pruning is often achieved by dropping redundant weights, neurons, or layers of a deep network while attempting to retain a comparable test performance. Many deep pruning algorithms have been proposed with impressive empirical success. However, existing approaches lack a quantifiable measure to estimate the compressibility of a sub-network during each pruning iteration and thus may under-prune or over-prune the model. In this work, we propose PQ Index (PQI) to measure the potential compressibility of deep neural networks and use this to develop a Sparsity-informed Adaptive Pruning (SAP) algorithm. Our extensive experiments corroborate the hypothesis that for a generic pruning procedure, PQI decreases first when a large model is being effectively regularized and then increases when its compressibility reaches a limit that appears to correspond to the beginning of underfitting. Subsequently, PQI decreases again when the model collapse and significant deterioration in the performance of the model start to occur. Additionally, our experiments demonstrate that the proposed adaptive pruning algorithm with proper choice of hyper-parameters is superior to the iterative pruning algorithms such as the lottery ticket-based pruning methods, in terms of both compression efficiency and robustness",
    "checked": true,
    "id": "4117f8b1aee5907cb8c0907f3cffbb11b27f28e0",
    "semantic_title": "pruning deep neural networks from a sparsity perspective",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=hCmjBJeGXcu": {
    "title": "Enhancing Meta Learning via Multi-Objective Soft Improvement Functions",
    "volume": "poster",
    "abstract": "Meta-learning tries to leverage information from similar learning tasks. In the commonly-used bilevel optimization formulation, the shared parameter is learned in the outer loop by minimizing the average loss over all tasks. However, the converged solution may be comprised in that it only focuses on optimizing on a small subset of tasks. To alleviate this problem, we consider meta-learning as a multi-objective optimization (MOO) problem, in which each task is an objective. However, existing MOO solvers need to access all the objectives' gradients in each iteration, and cannot scale to the huge number of tasks in typical meta-learning settings. To alleviate this problem, we propose a scalable gradient-based solver with the use of mini-batch. We provide theoretical guarantees on the Pareto optimality or Pareto stationarity of the converged solution. Empirical studies on various machine learning settings demonstrate that the proposed method is efficient, and achieves better performance than the baselines, particularly on improving the performance of the poorly-performing tasks and thus alleviating the compromising phenomenon",
    "checked": true,
    "id": "e841e0cd6b1c2677bdf6bae6e276f7fea0e1ea80",
    "semantic_title": "enhancing meta learning via multi-objective soft improvement functions",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=VM8batVBWvg": {
    "title": "Discrete Predictor-Corrector Diffusion Models for Image Synthesis",
    "volume": "poster",
    "abstract": "We introduce Discrete Predictor-Corrector diffusion models (DPC), extending predictor-corrector samplers in Gaussian diffusion models to the discrete case. Predictor-corrector samplers are a class of samplers for diffusion models, which improve on ancestral samplers by correcting the sampling distribution of intermediate diffusion states using MCMC methods. In DPC, the Langevin corrector, which does not have a direct counterpart in discrete space, is replaced with a discrete MCMC transition defined by a learned corrector kernel. The corrector kernel is trained to make the correction steps achieve asymptotic convergence, in distribution, to the correct marginal of the intermediate diffusion states. Equipped with DPC, we revisit recent transformer-based non-autoregressive generative models through the lens of discrete diffusion, and find that DPC can alleviate the compounding decoding error due to the parallel sampling of visual tokens. Our experiments show that DPC improves upon existing discrete latent space models for class-conditional image generation on ImageNet, and outperforms continuous diffusion models and GANs, according to standard metrics and user preference studies",
    "checked": true,
    "id": "30517693897bde78ee887bfc2904825f68ab65d0",
    "semantic_title": "discrete predictor-corrector diffusion models for image synthesis",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=tcbBPnfwxS": {
    "title": "OPTQ: Accurate Quantization for Generative Pre-trained Transformers",
    "volume": "poster",
    "abstract": "Generative Pre-trained Transformer models, known as GPT or OPT, set themselves apart through breakthrough performance across complex language modelling tasks, but also by their extremely high computational and storage costs. Specifically, due to their massive size, even inference for large, highly-accurate GPT models may require multiple performant GPUs, which limits the usability of such models. While there is emerging work on relieving this pressure via model compression, the applicability and performance of existing compression techniques is limited by the scale and complexity of GPT models. In this paper, we address this challenge, and propose OPTQ, a new one-shot weight quantization method based on approximate second-order information, that is both highly-accurate and highly-efficient. Specifically, OPTQ can quantize GPT models with 175 billion parameters in approximately four GPU hours, reducing the bitwidth down to 3 or 4 bits per weight, with negligible accuracy degradation relative to the uncompressed baseline. Our method more than doubles the compression gains relative to previously-proposed one-shot quantization methods, preserving accuracy, allowing us for the first time to execute an 175 billion-parameter model inside a single GPU for generative inference. Moreover, we also show that our method can still provide reasonable accuracy in the extreme quantization regime, in which weights are quantized to 2-bit or even ternary quantization levels. We show experimentally that these improvements can be leveraged for end-to-end inference speedups over FP16, of around 3.25x when using high-end GPUs (NVIDIA A100) and 4.5x when using more cost-effective ones (NVIDIA A6000). The implementation is available at https://github.com/IST-DASLab/gptq",
    "checked": true,
    "id": "363668677c459ebc0ff494655f993a93a0251009",
    "semantic_title": "optq: accurate quantization for generative pre-trained transformers",
    "citation_count": 274,
    "authors": []
  },
  "https://openreview.net/forum?id=bH-kCY6LdKg": {
    "title": "A new characterization of the edge of stability based on a sharpness measure aware of batch gradient distribution",
    "volume": "poster",
    "abstract": "For full-batch gradient descent (GD), it has been empirically shown that the sharpness, the top eigenvalue of the Hessian, increases and then hovers above $2/\\text{(learning rate)}$, and this is called ``the edge of stability'' phenomenon. However, it is unclear why the sharpness is somewhat larger than $2/\\text{(learning rate)}$ and how this can be extended to general mini-batch stochastic gradient descent (SGD). We propose a new sharpness measure (interaction-aware-sharpness) aware of the \\emph{interaction} between the batch gradient distribution and the loss landscape geometry. This leads to a more refined and general characterization of the edge of stability for SGD. Moreover, based on the analysis of a concentration measure of the batch gradient, we propose a more accurate scaling rule, Linear and Saturation Scaling Rule (LSSR), between batch size and learning rate",
    "checked": true,
    "id": "7db981dfaf7546e45947a65920ae756ea6cf239e",
    "semantic_title": "a new characterization of the edge of stability based on a sharpness measure aware of batch gradient distribution",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=RDy3IbvjMqT": {
    "title": "$\\mathrm{SE}(3)$-Equivariant Attention Networks for Shape Reconstruction in Function Space",
    "volume": "poster",
    "abstract": "We propose a method for 3D shape reconstruction from unoriented point clouds. Our method consists of a novel SE(3)-equivariant coordinate-based network (TF-ONet), that parametrizes the occupancy field of the shape and respects the inherent symmetries of the problem. In contrast to previous shape reconstruction methods that align the input to a regular grid, we operate directly on the irregular point cloud. Our architecture leverages equivariant attention layers that operate on local tokens. This mechanism enables local shape modelling, a crucial property for scalability to large scenes. Given an unoriented, sparse, noisy point cloud as input, we produce equivariant features for each point. These serve as keys and values for the subsequent equivariant cross-attention blocks that parametrize the occupancy field. By querying an arbitrary point in space, we predict its occupancy score. We show that our method outperforms previous SO(3)-equivariant methods, as well as non-equivariant methods trained on SO(3)-augmented datasets. More importantly, local modelling together with SE(3)-equivariance create an ideal setting for SE(3) scene reconstruction. We show that by training only on single, aligned objects and without any pre-segmentation, we can reconstruct novel scenes containing arbitrarily many objects in random poses without any performance loss",
    "checked": false,
    "id": "391488394770f211a7c6790a741286a1e777ed83",
    "semantic_title": "se(3)-equivariant attention networks for shape reconstruction in function space",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=m_GDIItaI3o": {
    "title": "Continual Pre-training of Language Models",
    "volume": "poster",
    "abstract": "Language models (LMs) have been instrumental for the rapid advance of natural language processing. This paper studies continual pre-training of LMs, in particular, continual domain-adaptive pre-training (or continual DAP-training). Existing research has shown that further pre-training an LM using a domain corpus to adapt the LM to the domain can improve the end-task performance in the domain. This paper proposes a novel method to continually DAP-train an LM with a sequence of unlabeled domain corpora to adapt the LM to these domains to improve their end-task performances. The key novelty of our method is a soft-masking mechanism that directly controls the update to the LM. A novel proxy is also proposed to preserve the general knowledge in the original LM. Additionally, it contrasts the representations of the previously learned domain knowledge (including the general knowledge in the pre-trained LM) and the knowledge from the current full network to achieve knowledge integration. The method not only overcomes catastrophic forgetting, but also achieves knowledge transfer to improve end-task performances. Empirical evaluation demonstrates the effectiveness of the proposed method",
    "checked": true,
    "id": "e3ec55e9e6720194a0ed5d4033d93a941c8a4f99",
    "semantic_title": "continual pre-training of language models",
    "citation_count": 151,
    "authors": []
  },
  "https://openreview.net/forum?id=PvDY71zKsvP": {
    "title": "Min-Max Multi-objective Bilevel Optimization with Applications in Robust Machine Learning",
    "volume": "poster",
    "abstract": "We consider a generic min-max multi-objective bilevel optimization problem with applications in robust machine learning such as representation learning and hyperparameter optimization. We design MORBiT, a novel single-loop gradient descent-ascent bilevel optimization algorithm, to solve the generic problem and present a novel analysis showing that MORBiT converges to the first-order stationary point at a rate of $\\widetilde{\\mathcal{O}}(n^{1/2} K^{-2/5})$ for a class of weakly convex problems with $n$ objectives upon $K$ iterations of the algorithm. Our analysis utilizes novel results to handle the non-smooth min-max multi-objective setup and to obtain a sublinear dependence in the number of objectives $n$. Experimental results on robust representation learning and robust hyperparameter optimization showcase (i) the advantages of considering the min-max multi-objective setup, and (ii) convergence properties of the proposed \\morbit",
    "checked": true,
    "id": "d6c868da77c4d9d22312541ab5af3aefda886932",
    "semantic_title": "min-max multi-objective bilevel optimization with applications in robust machine learning",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=7h5KSs2PCRi": {
    "title": "Forward Super-Resolution: How Can GANs Learn Hierarchical Generative Models for Real-World Distributions",
    "volume": "poster",
    "abstract": "Generative adversarial networks (GANs) are among the most successful models for learning high-complexity, real-world distributions. However, in theory, due to the highly non-convex, non-concave landscape of the minmax training objective, GAN remains one of the least understood deep learning models. In this work, we formally study how GANs can efficiently learn certain hierarchically generated distributions that are close to the distribution of real-life images. We prove that when a distribution has a structure that we refer to as \\emph{forward super-resolution}, then simply training generative adversarial networks using stochastic gradient descent ascent (SGDA) can learn this distribution efficiently, both in sample and time complexities. We also provide empirical evidence that our assumption ``forward super-resolution'' is very natural in practice, and the underlying learning mechanisms that we study in this paper (to allow us efficiently train GAN via GDA in theory) simulates the actual learning process of GANs on real-world problems",
    "checked": true,
    "id": "276c91fc2f41f95bfd14ab003e70dcbd0656bfc1",
    "semantic_title": "forward super-resolution: how can gans learn hierarchical generative models for real-world distributions",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=9yE2xEj0BH7": {
    "title": "Spotlight: Mobile UI Understanding using Vision-Language Models with a Focus",
    "volume": "poster",
    "abstract": "Mobile UI understanding is important for enabling various interaction tasks such as UI automation and accessibility. Previous mobile UI modeling often depends on the view hierarchy information of a screen, which directly provides the structural data of the UI, with the hope to bypass challenging tasks of visual modeling from screen pixels. However, view hierarchies are not always available, and are often corrupted with missing object descriptions or misaligned structure information. As a result, despite the use of view hierarchies could offer short-term gains, it may ultimately hinder the applicability and performance of the model. In this paper, we propose Spotlight, a vision-only approach for mobile UI understanding. Specifically, we enhance a vision-language model that only takes the screenshot of the UI and a region of interest on the screen---the focus---as the input. This general architecture of Spotlight is easily scalable and capable of performing a range of UI modeling tasks. Our experiments show that our model establishes SoTA results on several representative UI tasks and outperforms previous methods that use both screenshots and view hierarchies as inputs. Furthermore, we explore multi-task learning and few-shot prompting capacities of the proposed models, demonstrating promising results in the multi-task learning direction",
    "checked": true,
    "id": "9b9fb973e5d3b413baa90648d9eb0743bd889747",
    "semantic_title": "spotlight: mobile ui understanding using vision-language models with a focus",
    "citation_count": 70,
    "authors": []
  },
  "https://openreview.net/forum?id=rimcq1oIFeR": {
    "title": "A Control-Centric Benchmark for Video Prediction",
    "volume": "poster",
    "abstract": "Video is a promising source of knowledge for embodied agents to learn models of the world's dynamics. Large deep networks have become increasingly effective at modeling complex video data in a self-supervised manner, as evaluated by metrics based on human perceptual similarity or pixel-wise comparison. However, it remains unclear whether current metrics are accurate indicators of performance on downstream tasks. We find empirically that for planning robotic manipulation, existing metrics can be unreliable at predicting execution success. To address this, we propose a benchmark for action-conditioned video prediction in the form of a control benchmark that evaluates a given model for simulated robotic manipulation through sampling-based planning. Our benchmark, Video Prediction for Visual Planning ($\\text{VP}^2$), includes simulated environments with $11$ task categories and $310$ task instance definitions, a full planning implementation, and training datasets containing scripted interaction trajectories for each task category. A central design goal of our benchmark is to expose a simple interface -- a single forward prediction call -- so it is straightforward to evaluate almost any action-conditioned video prediction model. We then leverage our benchmark to study the effects of scaling model size, quantity of training data, and model ensembling by analyzing five highly-performant video prediction models, finding that while scale can improve perceptual quality when modelling visually diverse settings, other attributes such as uncertainty awareness can also aid planning performance",
    "checked": true,
    "id": "3be14893028ab620d0fc940becd4ce758ab96a40",
    "semantic_title": "a control-centric benchmark for video prediction",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=vsMyHUq_C1c": {
    "title": "A Stable and Scalable Method for Solving Initial Value PDEs with Neural Networks",
    "volume": "poster",
    "abstract": "Unlike conventional grid and mesh based methods for solving partial differential equations (PDEs), neural networks have the potential to break the curse of dimensionality, providing approximate solutions to problems where using classical solvers is difficult or impossible. While global minimization of the PDE residual over the network parameters works well for boundary value problems, catastrophic forgetting impairs applicability to initial value problems (IVPs). In an alternative local-in-time approach, the optimization problem can be converted into an ordinary differential equation (ODE) on the network parameters and the solution propagated forward in time; however, we demonstrate that current methods based on this approach suffer from two key issues. First, following the ODE produces an uncontrolled growth in the conditioning of the problem, ultimately leading to unacceptably large numerical errors. Second, as the ODE methods scale cubically with the number of model parameters, they are restricted to small neural networks, significantly limiting their ability to represent intricate PDE initial conditions and solutions. Building on these insights, we develop Neural-IVP, an ODE based IVP solver which prevents the network from getting ill-conditioned and runs in time linear in the number of parameters, enabling us to evolve the dynamics of challenging PDEs with neural networks",
    "checked": true,
    "id": "8371f8ca8ef8620cc72267763f5af2dd1b7c460b",
    "semantic_title": "a stable and scalable method for solving initial value pdes with neural networks",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=a65YK0cqH8g": {
    "title": "Noise Is Not the Main Factor Behind the Gap Between Sgd and Adam on Transformers, But Sign Descent Might Be",
    "volume": "poster",
    "abstract": "The success of the Adam optimizer on a wide array of architectures has made it the default in settings where stochastic gradient descent (SGD) performs poorly. However, our theoretical understanding of this discrepancy is lagging, preventing the development of significant improvements on either algorithm. Recent work advances the hypothesis that Adam and other heuristics like gradient clipping outperform SGD on language tasks because the distribution of the error induced by sampling has heavy tails. This suggests that Adam outperform SGD because it uses a more robust gradient estimate. We evaluate this hypothesis by varying the batch size, up to the entire dataset, to control for stochasticity. We present evidence that stochasticity and heavy-tailed noise are not major factors in the performance gap between SGD and Adam. Rather, Adam performs better as the batch size increases, while SGD is less effective at taking advantage of the reduction in noise. This raises the question as to why Adam outperforms SGD in the full-batch setting. Through further investigation of simpler variants of SGD, we find that the behavior of Adam with large batches is similar to sign descent with momentum",
    "checked": true,
    "id": "592e2a4c8bb3e72b1f6d671d6642907fa81b1782",
    "semantic_title": "noise is not the main factor behind the gap between sgd and adam on transformers, but sign descent might be",
    "citation_count": 84,
    "authors": []
  },
  "https://openreview.net/forum?id=li7qeBbCR1t": {
    "title": "Building Normalizing Flows with Stochastic Interpolants",
    "volume": "poster",
    "abstract": "A generative model based on a continuous-time normalizing flow between any pair of base and target probability densities is proposed. The velocity field of this flow is inferred from the probability current of a time-dependent density that interpolates between the base and the target in finite time. Unlike conventional normalizing flow inference methods based the maximum likelihood principle, which require costly backpropagation through ODE solvers, our interpolant approach leads to a simple quadratic loss for the velocity itself which is expressed in terms of expectations that are readily amenable to empirical estimation. The flow can be used to generate samples from either the base or target, and to estimate the likelihood at any time along the interpolant. In addition, the flow can be optimized to minimize the path length of the interpolant density, thereby paving the way for building optimal transport maps. In situations where the base is a Gaussian density, we also show that the velocity of our normalizing flow can also be used to construct a diffusion model to sample the target as well as estimate its score. However, our approach shows that we can bypass this diffusion completely and work at the level of the probability flow with greater simplicity, opening an avenue for methods based solely on ordinary differential equations as an alternative to those based on stochastic differential equations. Benchmarking on density estimation tasks illustrates that the learned flow can match and surpass conventional continuous flows at a fraction of the cost, and compares well with diffusions on image generation on CIFAR-10 and ImageNet $32 \\times 32$. The method scales ab-initio ODE flows to previously unreachable image resolutions, demonstrated up to $128\\times128$",
    "checked": true,
    "id": "4e6244baf4236f4635e85f7dfb941a9a0a6c4a11",
    "semantic_title": "building normalizing flows with stochastic interpolants",
    "citation_count": 451,
    "authors": []
  },
  "https://openreview.net/forum?id=VE1s3e5xriA": {
    "title": "Dual Student Networks for Data-Free Model Stealing",
    "volume": "poster",
    "abstract": "Data-free model stealing aims to replicate a target model without direct access to either the training data or the target model. To accomplish this, existing methods use a generator to produce samples in order to train a student model to match the target model outputs. To this end, the two main challenges are estimating gradients of the target model without access to its parameters, and generating a diverse set of training samples that thoroughly explores the input space. We propose a Dual Student method where two students are symmetrically trained in order to provide the generator a criterion to generate samples that the two students disagree on. On one hand, disagreement on a sample implies at least one student has classified the sample incorrectly when compared to the target model. This incentive towards disagreement implicitly encourages the generator to explore more diverse regions of the input space. On the other hand, our method utilizes gradients of student models to indirectly estimate gradients of the target model. We show that this novel training objective for the generator network is equivalent to optimizing a lower bound on the generator's loss if we had access to the target model gradients. In other words, our method alters the standard data-free model stealing paradigm by substituting the target model with a separate student model, thereby creating a lower bound which can be directly optimized without additional target model queries or separate synthetic datasets. We show that our new optimization framework provides more accurate gradient estimation of the target model and better accuracies on benchmark classification datasets. Additionally, our approach balances improved query efficiency with training computation cost. Finally, we demonstrate that our method serves as a better proxy model for transfer-based adversarial attacks than existing data-free model stealing methods",
    "checked": true,
    "id": "e562c8ee825ed4e1b3ec4037f6e9e1194d355f07",
    "semantic_title": "dual student networks for data-free model stealing",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=nWTzIsgrYNN": {
    "title": "Composite Slice Transformer: An Efficient Transformer with Composition of Multi-Scale Multi-Range Attentions",
    "volume": "poster",
    "abstract": "Since the introduction of Transformers, researchers have tackled the notoriously expensive quadratic complexity problem. While significant computational efficiency improvements have been achieved, they come at the cost of reduced accuracy trade-offs. In this paper, we propose Composite Slice Transformer (CST), a Transformer-based network equipped with a composition of multi-scale multi-range attentions, boosting both efficiency and modeling capability. After stacking fixed-length slices of the input sequence, each layer in CST performs a pair of fine-and-coarse-grained attentions with short-long ranges in a sequential manner, coupled with volatile instant positional embedding, enabling efficient token interactions {\\em and} improving expressiveness of the model. In addition to significantly reduced $O(NL+N^2/L^2)$ complexity for sequence length $N$ and slice length $L$, CST achieves superior performance on a variety of tasks. We show that CST surpasses recently published efficient Transformers on the Long Range Arena benchmark, demonstrating the bidirectional long-range dependency modeling capability of our model. It outperforms the standard Transformer by a margin of $6.9$\\% in average accuracy across the five classification tasks of the benchmark, while being of complexity comparable to other efficient transformers. Furthermore, on the word-level autoregressive language modeling task with the WikiText-103 dataset, CST performs competitively against the Transformer model with only $2$\\% gap in the test perplexity while outperforming other efficient Transformers",
    "checked": true,
    "id": "54983a66dc8bc15af89890bc9e1a63e056594179",
    "semantic_title": "composite slice transformer: an efficient transformer with composition of multi-scale multi-range attentions",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=dhYUMMy0_Eg": {
    "title": "Equal Improvability: A New Fairness Notion Considering the Long-term Impact",
    "volume": "poster",
    "abstract": "Devising a fair classifier that does not discriminate against different groups is an important problem in machine learning. Although researchers have proposed various ways of defining group fairness, most of them only focused on the immediate fairness, ignoring the long-term impact of a fair classifier under the dynamic scenario where each individual can improve its feature over time. Such dynamic scenarios happen in real world, e.g., college admission and credit loaning, where each rejected sample makes effort to change its features to get accepted afterwards. In this dynamic setting, the long-term fairness should equalize the samples' feature distribution across different groups after the rejected samples make some effort to improve. In order to promote long-term fairness, we propose a new fairness notion called Equal Improvability (EI), which equalizes the potential acceptance rate of the rejected samples across different groups assuming a bounded level of effort will be spent by each rejected sample. We analyze the properties of EI and its connections with existing fairness notions. To find a classifier that satisfies the EI requirement, we propose and study three different approaches that solve EI regularized optimization problems. Through experiments on both synthetic and real datasets, we demonstrate that the proposed EI-regularized algorithms encourage us to find a fair classifier in terms of EI. Finally, we provide experimental results on dynamic scenarios which highlight the advantages of our EI metric in achieving the long-term fairness. Codes are available in anonymous GitHub repository",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z9SIj-IM7tn": {
    "title": "Competitive Physics Informed Networks",
    "volume": "poster",
    "abstract": "Neural networks can be trained to solve partial differential equations (PDEs) by using the PDE residual as the loss function. This strategy is called \"physics-informed neural networks\" (PINNs), but it currently cannot produce high-accuracy solutions, typically attaining about $0.1\\%$ relative error. We present an adversarial approach that overcomes this limitation, which we call competitive PINNs (CPINNs). CPINNs train a discriminator that is rewarded for predicting mistakes the PINN makes. The discriminator and PINN participate in a zero-sum game with the exact PDE solution as an optimal strategy. This approach avoids squaring the large condition numbers of PDE discretizations, which is the likely reason for failures of previous attempts to decrease PINN errors even on benign problems. Numerical experiments on a Poisson problem show that CPINNs achieve errors four orders of magnitude smaller than the best-performing PINN. We observe relative errors on the order of single-precision accuracy, consistently decreasing with each epoch. To the authors' knowledge, this is the first time this level of accuracy and convergence behavior has been achieved. Additional experiments on the nonlinear Schr{\\\"o}dinger, Burgers', and Allen--Cahn equation show that the benefits of CPINNs are not limited to linear problems",
    "checked": true,
    "id": "f3a1c70d8dce3377e93fa0c623ce4d435b5e59aa",
    "semantic_title": "competitive physics informed networks",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=_nGgzQjzaRy": {
    "title": "Decomposed Prompting: A Modular Approach for Solving Complex Tasks",
    "volume": "poster",
    "abstract": "Few-shot prompting is a surprisingly powerful way to use Large Language Models (LLMs) to solve various tasks. However, this approach struggles as the task complexity increases or when the individual reasoning steps of the task themselves are hard to learn, especially when embedded in more complex tasks. To address this, we propose Decomposed Prompting, a new approach to solve complex tasks by decomposing them (via prompting) into simpler sub-tasks that can be delegated to a library of prompting-based LLMs dedicated to these sub-tasks. This modular structure allows each prompt to be optimized for its specific sub-task, further decomposed if necessary, and even easily replaced with more effective prompts, trained models, or symbolic functions if desired. We show that the flexibility and modularity of Decomposed Prompting allows it to outperform prior work on few-shot prompting using GPT3. On symbolic reasoning tasks, we can further decompose sub-tasks that are hard for LLMs into even simpler solvable sub-tasks. When the complexity comes from the input length, we can recursively decompose the task into the same task but with smaller inputs. We also evaluate our approach on textual multi-step reasoning tasks: on long-context multi-hop QA task, we can more effectively teach the sub-tasks via our separate sub-tasks prompts; and on open-domain multi-hop QA, we can incorporate a symbolic information retrieval within our decomposition framework, leading to improved performance on both tasks. Datasets, Code and Prompts available at https://github.com/allenai/DecomP",
    "checked": true,
    "id": "07955e96cbd778d0ae2a68f09d073b866dd84c2a",
    "semantic_title": "decomposed prompting: a modular approach for solving complex tasks",
    "citation_count": 490,
    "authors": []
  },
  "https://openreview.net/forum?id=9MO7bjoAfIA": {
    "title": "Self-Ensemble Protection: Training Checkpoints Are Good Data Protectors",
    "volume": "poster",
    "abstract": "As data becomes increasingly vital, a company would be very cautious about releasing data, because the competitors could use it to train high-performance models, thereby posing a tremendous threat to the company's commercial competence. To prevent training good models on the data, we could add imperceptible perturbations to it. Since such perturbations aim at hurting the entire training process, they should reflect the vulnerability of DNN training, rather than that of a single model. Based on this new idea, we seek perturbed examples that are always unrecognized (never correctly classified) in training. In this paper, we uncover them by model checkpoints' gradients, forming the proposed self-ensemble protection (SEP), which is very effective because (1) learning on examples ignored during normal training tends to yield DNNs ignoring normal examples; (2) checkpoints' cross-model gradients are close to orthogonal, meaning that they are as diverse as DNNs with different architectures. That is, our amazing performance of ensemble only requires the computation of training one model. By extensive experiments with 9 baselines on 3 datasets and 5 architectures, SEP is verified to be a new state-of-the-art, e.g., our small $\\ell_\\infty=2/255$ perturbations reduce the accuracy of a CIFAR-10 ResNet18 from 94.56% to 14.68%, compared to 41.35% by the best-known method. Code is available at https://github.com/Sizhe-Chen/SEP",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2EpjkjzdCAa": {
    "title": "Effectively Modeling Time Series with Simple Discrete State Spaces",
    "volume": "poster",
    "abstract": "Time series modeling is a well-established problem, which often requires that methods (1) expressively represent complicated dependencies, (2) forecast long horizons, and (3) efficiently train over long sequences. State-space models (SSMs) are classical models for time series, and prior works combine SSMs with deep learning layers for efficient sequence modeling. However, we find fundamental limitations with these prior approaches, proving their SSM representations cannot express autoregressive time series processes. We thus introduce SpaceTime, a new state-space time series architecture that improves all three criteria. For expressivity, we propose a new SSM parameterization based on the companion matrix---a canonical representation for discrete-time processes---which enables SpaceTime's SSM layers to learn desirable autoregressive processes. For long horizon forecasting, we introduce a \"closed-loop\" variation of the companion SSM, which enables SpaceTime to predict many future time-steps by generating its own layer-wise inputs. For efficient training and inference, we introduce an algorithm that reduces the memory and compute of a forward pass with the companion matrix. With sequence length $\\ell$ and state-space size $d$, we go from $\\tilde{O}(d \\ell)$ naïvely to $\\tilde{O}(d + \\ell)$. In experiments, our contributions lead to state-of-the-art results on extensive and diverse benchmarks, with best or second-best AUROC on 6 / 7 ECG and speech time series classification, and best MSE on 14 / 16 Informer forecasting tasks. Furthermore, we find SpaceTime (1) fits AR($p$) processes that prior deep SSMs fail on, (2) forecasts notably more accurately on longer horizons than prior state-of-the-art, and (3) speeds up training on real-world ETTh1 data by 73% and 80% relative wall-clock time over Transformers and LSTMs",
    "checked": true,
    "id": "a7d68b1702af08ce4dbbf2cd0b083e744ae5c6be",
    "semantic_title": "effectively modeling time series with simple discrete state spaces",
    "citation_count": 50,
    "authors": []
  },
  "https://openreview.net/forum?id=Jbdc0vTOcol": {
    "title": "A Time Series is Worth 64 Words: Long-term Forecasting with Transformers",
    "volume": "poster",
    "abstract": "We propose an efficient design of Transformer-based models for multivariate time series forecasting and self-supervised representation learning. It is based on two key components: (i) segmentation of time series into subseries-level patches which are served as input tokens to Transformer; (ii) channel-independence where each channel contains a single univariate time series that shares the same embedding and Transformer weights across all the series. Patching design naturally has three-fold benefit: local semantic information is retained in the embedding; computation and memory usage of the attention maps are quadratically reduced given the same look-back window; and the model can attend longer history. Our channel-independent patch time series Transformer (PatchTST) can improve the long-term forecasting accuracy significantly when compared with that of SOTA Transformer-based models. We also apply our model to self-supervised pre-training tasks and attain excellent fine-tuning performance, which outperforms supervised training on large datasets. Transferring of masked pre-training performed on one dataset to other datasets also produces SOTA forecasting accuracy",
    "checked": true,
    "id": "dad15404d372a23b4b3bf9a63b3124693df3c85e",
    "semantic_title": "a time series is worth 64 words: long-term forecasting with transformers",
    "citation_count": 1643,
    "authors": []
  },
  "https://openreview.net/forum?id=086pmarAris": {
    "title": "Fantastic Rewards and How to Tame Them: A Case Study on Reward Learning for Task-oriented Dialogue Systems",
    "volume": "poster",
    "abstract": "When learning task-oriented dialogue (ToD) agents, reinforcement learning (RL) techniques can naturally be utilized to train dialogue strategies to achieve user-specific goals. Prior works mainly focus on adopting advanced RL techniques to train the ToD agents, while the design of the reward function is not well studied. This paper aims at answering the question of how to efficiently learn and leverage a reward function for training end-to-end (E2E) ToD agents. Specifically, we introduce two generalized objectives for reward-function learning, inspired by the classical learning-to-rank literature. Further, we utilize the learned reward function to guide the training of the E2E ToD agent. With the proposed techniques, we achieve competitive results on the E2E response-generation task on the Multiwoz 2.0 dataset. Source code and checkpoints are publicly released at https://github.com/Shentao-YANG/Fantastic_Reward_ICLR2023",
    "checked": true,
    "id": "fa49e6f77ae14a9da2371d194ace3565376428ae",
    "semantic_title": "fantastic rewards and how to tame them: a case study on reward learning for task-oriented dialogue systems",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=8jU7wy7N7mA": {
    "title": "Supervision Complexity and its Role in Knowledge Distillation",
    "volume": "poster",
    "abstract": "Despite the popularity and efficacy of knowledge distillation, there is limited understanding of why it helps. In order to study the generalization behavior of a distilled student, we propose a new theoretical framework that leverages supervision complexity: a measure of alignment between teacher-provided supervision and the student's neural tangent kernel. The framework highlights a delicate interplay among the teacher's accuracy, the student's margin with respect to the teacher predictions, and the complexity of the teacher predictions. Specifically, it provides a rigorous justification for the utility of various techniques that are prevalent in the context of distillation, such as early stopping and temperature scaling. Our analysis further suggests the use of online distillation, where a student receives increasingly more complex supervision from teachers in different stages of their training. We demonstrate efficacy of online distillation and validate the theoretical findings on a range of image classification benchmarks and model architectures",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=-htnolWDLvP": {
    "title": "Transferable Unlearnable Examples",
    "volume": "poster",
    "abstract": "With more people publishing their personal data online, unauthorized data usage has become a serious concern. The unlearnable examples strategies have been introduced to prevent third parties from training on the data without permission. They add perturbations to the users' data before publishing, so as to make the models trained on the perturbed published dataset invalidated. These perturbations have been generated for a specific training setting and a target dataset. However, their unlearnable effects significantly decrease when used in other training settings or datasets. To tackle this issue, we propose a novel unlearnable strategy based on Class-wise Separability Discriminant (CSD), which boosts the transferability of the unlearnable perturbations by enhancing the linear separability. Extensive experiments demonstrate the transferability of the unlearnable examples crafted by our proposed method across training settings and datasets",
    "checked": true,
    "id": "d0d7afcb50d92a061d4351e7ed16dce2b26b1e23",
    "semantic_title": "transferable unlearnable examples",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=3pfNb4pZBNp": {
    "title": "Random Laplacian Features for Learning with Hyperbolic Space",
    "volume": "poster",
    "abstract": "Due to its geometric properties, hyperbolic space can support high-fidelity embeddings of tree- and graph-structured data, upon which various hyperbolic networks have been developed. Existing hyperbolic networks encode geometric priors not only for the input, but also at every layer of the network. This approach involves repeatedly mapping to and from hyperbolic space, which makes these networks complicated to implement, computationally expensive to scale, and numerically unstable to train. In this paper, we propose a simpler approach: learn a hyperbolic embedding of the input, then map once from it to Euclidean space using a mapping that encodes geometric priors by respecting the isometries of hyperbolic space, and finish with a standard Euclidean network. The key insight is to use a random feature mapping via the eigenfunctions of the Laplace operator, which we show can approximate any isometry-invariant kernel on hyperbolic space. Our method can be used together with any graph neural networks: using even a linear graph model yields significant improvements in both efficiency and performance over other hyperbolic baselines in both transductive and inductive tasks",
    "checked": true,
    "id": "b3f1594cfcb14a392feab64571691bd60f6bf308",
    "semantic_title": "random laplacian features for learning with hyperbolic space",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=SjzFVSJUt8S": {
    "title": "Replay Memory as An Empirical MDP: Combining Conservative Estimation with Experience Replay",
    "volume": "poster",
    "abstract": "Experience replay, which stores transitions in a replay memory for repeated use, plays an important role of improving sample efficiency in reinforcement learning. Existing techniques such as reweighted sampling, episodic learning and reverse sweep update further process the information in the replay memory to make experience replay more efficient. In this work, we further exploit the information in the replay memory by treating it as an empirical \\emph{Replay Memory MDP (RM-MDP)}. By solving it with dynamic programming, we learn a conservative value estimate that \\emph{only} considers transitions observed in the replay memory. Both value and policy regularizers based on this conservative estimate are developed and integrated with model-free learning algorithms. We design the metric \\textit{memory density} to measure the quality of RM-MDP. Our empirical studies quantitatively find a strong correlation between performance improvement and memory density. Our method combines \\emph{Conservative Estimation with Experience Replay (CEER)}, improving sample efficiency by a large margin, especially when the memory density is high. Even when the memory density is low, such a conservative estimate can still help to avoid suicidal actions and thereby improve performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vouQcZS8KfW": {
    "title": "Neural Causal Models for Counterfactual Identification and Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d356d612ac3e9d11b7a7778f03e6b1c5d4134489",
    "semantic_title": "neural causal models for counterfactual identification and estimation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vCJ9-Ri-6xU": {
    "title": "Momentum Stiefel Optimizer, with Applications to Suitably-Orthogonal Attention, and Optimal Transport",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f154ba64b77a0deeb64220e6a4c28f007b1816e8",
    "semantic_title": "momentum stiefel optimizer, with applications to suitably-orthogonal attention, and optimal transport",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=UvmDCdSPDOW": {
    "title": "Information-Theoretic Diffusion",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b72bef3c193d58c354a12ea0672ca5654d4a4db3",
    "semantic_title": "information-theoretic diffusion",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=GPJVuyX4p_h": {
    "title": "SIMPLE: A Gradient Estimator for k-Subset Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "24f89e3d55cd639a1f0f31c43e636d2b3bb66b2d",
    "semantic_title": "simple: a gradient estimator for k-subset sampling",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=gLPkzWjdhBN": {
    "title": "Learning Iterative Neural Optimizers for Image Steganography",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "46e760a8688fbcb4a863467a0d21e43af4bf6ab8",
    "semantic_title": "learning iterative neural optimizers for image steganography",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=3aQs3MCSexD": {
    "title": "How Much Data Are Augmentations Worth? An Investigation into Scaling Laws, Invariance, and Implicit Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4e2e78f75336240667687d16bf32ffdff7556f98",
    "semantic_title": "how much data are augmentations worth? an investigation into scaling laws, invariance, and implicit regularization",
    "citation_count": 45,
    "authors": []
  },
  "https://openreview.net/forum?id=qxRscesArBZ": {
    "title": "Robust Graph Dictionary Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9078452e8b927597bc0aef46c64b475ea06e2a72",
    "semantic_title": "robust graph dictionary learning",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=gpmL0D4VjN4": {
    "title": "Fundamental limits on the robustness of image classifiers",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ade34f95e9350477aa7f103e9607a8f770bffe6f",
    "semantic_title": "fundamental limits on the robustness of image classifiers",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=cxCEOSF99f": {
    "title": "Understanding Influence Functions and Datamodels via Harmonic Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5tKXUZil3X": {
    "title": "TextGrad: Advancing Robustness Evaluation in NLP by Gradient-Driven Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bQB6qozaBw": {
    "title": "Information Plane Analysis for Dropout Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9af80598cc32a8060fbf731e0954e3e6d3a754f8",
    "semantic_title": "information plane analysis for dropout neural networks",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=ySCL-NG_I3": {
    "title": "Learning Harmonic Molecular Representations on Riemannian Manifold",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eSQh8rG8Oa": {
    "title": "Greedy Actor-Critic: A New Conditional Cross-Entropy Method for Policy Improvement",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cyg2YXn_BqF": {
    "title": "Efficiently Controlling Multiple Risks with Pareto Testing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=loIfC8WHevK": {
    "title": "Characteristic Neural Ordinary Differential Equation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "17773066cf244e82503e7283dcf1981bfe4ef0a6",
    "semantic_title": "characteristic neural ordinary differential equations",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=Loek7hfb46P": {
    "title": "Fast Sampling of Diffusion Models with Exponential Integrator",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A9WQaxYsfx": {
    "title": "Panning for Gold in Federated Learning: Targeted Text Extraction under Arbitrarily Large-Scale Aggregation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e6c4955aa6410cfce3f7abdff22db9c2f6d96279",
    "semantic_title": "panning for gold in federated learning: targeted text extraction under arbitrarily large-scale aggregation",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=dBk3hsg-n6": {
    "title": "Artificial Neuronal Ensembles with Learned Context Dependent Gating",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rGeZuBRahju": {
    "title": "Learning Language Representations with Logical Inductive Bias",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4d1be5f81204f968a34a08975fece5f626f79618",
    "semantic_title": "learning language representations with logical inductive bias",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=Dzmd-Cc8OI": {
    "title": "How Does Semi-supervised Learning with Pseudo-labelers Work? A Case Study",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3ec356c38f0b2a0828e475c8d73e907691251b6f",
    "semantic_title": "how does semi-supervised learning with pseudo-labelers work? a case study",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=Lnxl5pr018": {
    "title": "Empowering Graph Representation Learning with Test-Time Graph Transformation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ee8333852cea80faec02a50d925f2316cf1c2e1f",
    "semantic_title": "empowering graph representation learning with test-time graph transformation",
    "citation_count": 66,
    "authors": []
  },
  "https://openreview.net/forum?id=HJFVrpCaGE": {
    "title": "Provable Robustness against Wasserstein Distribution Shifts via Input Randomization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YtntjusJV6": {
    "title": "Interpretations of Domain Adaptations via Layer Variational Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b421eb4550a682d8403d3af33589fe36b63bf42a",
    "semantic_title": "interpretations of domain adaptations via layer variational analysis",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=8pvnfTAbu1f": {
    "title": "Denoising Diffusion Samplers",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3ee60ee899941c7056fc10e1b69e71bac0f11c97",
    "semantic_title": "denoising diffusion samplers",
    "citation_count": 101,
    "authors": []
  },
  "https://openreview.net/forum?id=_nF5imFKQI": {
    "title": "How I Learned to Stop Worrying and Love Retraining",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6u7mf9s2A9": {
    "title": "Interpretable Geometric Deep Learning via Learnable Randomness Injection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fPVRcJqspu": {
    "title": "GOGGLE: Generative Modelling for Tabular Data by Learning Relational Structure",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d41c2ecc159e545ef02dac198a6be7b066d9563d",
    "semantic_title": "goggle: generative modelling for tabular data by learning relational structure",
    "citation_count": 68,
    "authors": []
  },
  "https://openreview.net/forum?id=UJTgQBc91_": {
    "title": "Progressive Prompts: Continual Learning for Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "86478f285356b5c8d27423e6b939634d9e010fba",
    "semantic_title": "progressive prompts: continual learning for language models",
    "citation_count": 155,
    "authors": []
  },
  "https://openreview.net/forum?id=_qVhsWyWB9": {
    "title": "Deep Learning From Crowdsourced Labels: Coupled Cross-Entropy Minimization, Identifiability, and Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f2f66f757b8df8a2f236b1500c77e2503cbec194",
    "semantic_title": "deep learning from crowdsourced labels: coupled cross-entropy minimization, identifiability, and regularization",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=yEsj8pGNl1": {
    "title": "Projective Proximal Gradient Descent for Nonconvex Nonsmooth Optimization: Fast Convergence Without Kurdyka-Lojasiewicz (KL) Property",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7wrq3vHcMM": {
    "title": "First Steps Toward Understanding the Extrapolation of Nonlinear Models to Unseen Domains",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=-Yzz6vlX7V-": {
    "title": "Compositionality with Variation Reliably Emerges in Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ac245daef84a6753f9e90abdc775c900eb9ba9e6",
    "semantic_title": "compositionality with variation reliably emerges in neural networks",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=k8_yVW3Wqln": {
    "title": "Systematic Rectification of Language Models via Dead-end Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "da5fcb26c830663b79c9aa1c550ae62e7725fcad",
    "semantic_title": "systematic rectification of language models via dead-end analysis",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=8efJYMBrNb": {
    "title": "Multiple sequence alignment as a sequence-to-sequence learning problem",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a8fefdd3aa68c054747eafb928ed383e88450f76",
    "semantic_title": "multiple sequence alignment as a sequence-to-sequence learning problem",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=4FBUihxz5nm": {
    "title": "A Mixture-of-Expert Approach to RL-based Dialogue Management",
    "volume": "poster",
    "abstract": "Despite recent advancements in language models (LMs), their application to dialogue management (DM) problems and ability to carry on rich conversations remain a challenge. We use reinforcement learning (RL) to develop a dialogue agent that avoids being short-sighted (outputting generic utterances) and maximizes overall user satisfaction. Most existing RL approaches to DM train the agent at the word-level, and thus, have to deal with a combinatorially complex action space even for a medium-size vocabulary. As a result, they struggle to produce a successful and engaging dialogue even if they are warm-started with a pre-trained LM. To address this issue, we develop a RL-based DM using a novel mixture of expert language model (MoE-LM) that consists of (i) a LM capable of learning diverse semantics for conversation histories, (ii) a number of specialized LMs (or experts) capable of generating utterances corresponding to a particular attribute or personality, and (iii) a RL-based DM that performs dialogue planning with the utterances generated by the experts. Our MoE approach provides greater flexibility to generate sensible utterances with different intents and allows RL to focus on conversational-level DM. We compare it with SOTA baselines on open-domain dialogues and demonstrate its effectiveness both in terms of the diversity and sensibility of the generated utterances and the overall DM performance",
    "checked": true,
    "id": "81de450838dfe072f3b4f773cc2a4ed37064af9b",
    "semantic_title": "a mixture-of-expert approach to rl-based dialogue management",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=iBdwKIsg4m": {
    "title": "f-DM: A Multi-stage Diffusion Model via Progressive Signal Transformation",
    "volume": "poster",
    "abstract": "Diffusion models (DMs) have recently emerged as SoTA tools for generative modeling in various domains. Standard DMs can be viewed as an instantiation of hierarchical variational autoencoders (VAEs) where the latent variables are inferred from input-centered Gaussian distributions with fixed scales and variances. Unlike VAEs, this formulation constrains DMs from changing the latent spaces and learning abstract representations. In this work, we propose f-DM, a generalized family of DMs which allows progressive signal transformation. More precisely, we extend DMs to incorporate a set of (hand-designed or learned) transformations, where the transformed input is the mean of each diffusion step. We propose a generalized formulation and derive the corresponding de-noising objective with a modified sampling algorithm. As a demonstration, we apply f-DM in image generation tasks with a range of functions, including down-sampling, blurring, and learned transformations based on the encoder of pretrained VAEs. In addition, we identify the importance of adjusting the noise levels whenever the signal is sub-sampled and propose a simple rescaling recipe. f-DM can produce high-quality samples on standard image generation benchmarks like FFHQ, AFHQ, LSUN, and ImageNet with better efficiency and semantic interpretation",
    "checked": true,
    "id": "19d39b1c13795c556cfaf0eb7b89aeb187200a43",
    "semantic_title": "f-dm: a multi-stage diffusion model via progressive signal transformation",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=nIMifqu2EO": {
    "title": "Backpropagation at the Infinitesimal Inference Limit of Energy-Based Models: Unifying Predictive Coding, Equilibrium Propagation, and Contrastive Hebbian Learning",
    "volume": "poster",
    "abstract": "How the brain performs credit assignment is a fundamental unsolved problem in neuroscience. Many `biologically plausible' algorithms have been proposed, which compute gradients that approximate those computed by backpropagation (BP), and which operate in ways that more closely satisfy the constraints imposed by neural circuitry. Many such algorithms utilize the framework of energy-based models (EBMs), in which all free variables in the model are optimized to minimize a global energy function. However, in the literature, these algorithms exist in isolation and no unified theory exists linking them together. Here, we provide a comprehensive theory of the conditions under which EBMs can approximate BP, which lets us unify many of the BP approximation results in the literature (namely, predictive coding, equilibrium propagation, and contrastive Hebbian learning) and demonstrate that their approximation to BP arises from a simple and general mathematical property of EBMs at free-phase equilibrium. This property can then be exploited in different ways with different energy functions, and these specific choices yield a family of BP-approximating algorithms, which both includes the known results in the literature and can be used to derive new ones",
    "checked": true,
    "id": "232e3ab355787bd61afeedc05de6ad4ae04070a6",
    "semantic_title": "backpropagation at the infinitesimal inference limit of energy-based models: unifying predictive coding, equilibrium propagation, and contrastive hebbian learning",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=ZCTvSF_uVM4": {
    "title": "A Theoretical Framework for Inference and Learning in Predictive Coding Networks",
    "volume": "poster",
    "abstract": "Predictive coding (PC) is an influential theory in computational neuroscience, which argues that the cortex forms unsupervised world models by implementing a hierarchical process of prediction error minimization. PC networks (PCNs) are trained in two phases. First, neural activities are updated to optimize the network's response to external stimuli. Second, synaptic weights are updated to consolidate this change in activity --- an algorithm called \\emph{prospective configuration}. While previous work has shown how in various limits, PCNs can be found to approximate backpropagation (BP), recent work has demonstrated that PCNs operating in this standard regime, which does not approximate BP, nevertheless obtain competitive training and generalization performance to BP-trained networks while outperforming them on various tasks. However, little is understood theoretically about the properties and dynamics of PCNs in this regime. In this paper, we provide a comprehensive theoretical analysis of the properties of PCNs trained with prospective configuration. We first derive analytical results concerning the inference equilibrium for PCNs and a previously unknown close connection relationship to target propagation (TP). Secondly, we provide a theoretical analysis of learning in PCNs as a variant of generalized expectation-maximization and use that to prove the convergence of PCNs to critical points of the BP loss function, thus showing that deep PCNs can, in theory, achieve the same generalization performance as BP, while maintaining their unique advantages",
    "checked": true,
    "id": "6f33194da2822dd84c2318d36c3f85a6ba93d73a",
    "semantic_title": "a theoretical framework for inference and learning in predictive coding networks",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=JLINxPOVTh7": {
    "title": "The Onset of Variance-Limited Behavior for Networks in the Lazy and Rich Regimes",
    "volume": "poster",
    "abstract": "For small training set sizes $P$, the generalization error of wide neural networks is well-approximated by the error of an infinite width neural network (NN), either in the kernel or mean-field/feature-learning regime. However, after a critical sample size $P^*$, we empirically find the finite-width network generalization becomes worse than that of the infinite width network. In this work, we empirically study the transition from infinite-width behavior to this \\textit{variance-limited} regime as a function of sample size $P$ and network width $N$. We find that finite-size effects can become relevant for very small dataset sizes on the order of $P^* \\sim \\sqrt{N}$ for polynomial regression with ReLU networks. We discuss the source of these effects using an argument based on the variance of the NN's final neural tangent kernel (NTK). This transition can be pushed to larger $P$ by enhancing feature learning or by ensemble averaging the networks. We find that the learning curve for regression with the final NTK is an accurate approximation of the NN learning curve. Using this, we provide a toy model which also exhibits $P^* \\sim \\sqrt{N}$ scaling and has $P$-dependent benefits from feature learning",
    "checked": true,
    "id": "4fe83e4a9aec5d057e15dc2f1aeab43b3dfea2ed",
    "semantic_title": "the onset of variance-limited behavior for networks in the lazy and rich regimes",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1C6nCCaRe6p": {
    "title": "A Simple Approach for Visual Room Rearrangement: 3D Mapping and Semantic Search",
    "volume": "poster",
    "abstract": "Physically rearranging objects is an important capability for embodied agents. Visual room rearrangement evaluates an agent's ability to rearrange objects in a room to a desired goal based solely on visual input. We propose a simple yet effective method for this problem: (1) search for and map which objects need to be rearranged, and (2) rearrange each object until the task is complete. Our approach consists of an off-the-shelf semantic segmentation model, voxel-based semantic map, and semantic search policy to efficiently find objects that need to be rearranged. Our method was the winning submission to the AI2-THOR Rearrangement Challenge in the 2022 Embodied AI Workshop at CVPR 2022, and improves on current state-of-the-art end-to-end reinforcement learning-based methods that learn visual room rearrangement policies from 0.53% correct rearrangement to 16.56%, using only 2.7% as many samples from the environment",
    "checked": true,
    "id": "6f436beb9b11748f1a76c945baad014202794d74",
    "semantic_title": "a simple approach for visual room rearrangement: 3d mapping and semantic search",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=H7M_5K5qKJV": {
    "title": "Progressive Mix-Up for Few-Shot Supervised Multi-Source Domain Transfer",
    "volume": "poster",
    "abstract": "This paper targets at a new and challenging setting of knowledge transfer from multiple source domains to a single target domain, where target data is few shot or even one shot with label. Traditional domain generalization or adaptation methods cannot directly work since there is no sufficient target domain distribution serving as the transfer object. The multi-source setting further prevents the transfer task as excessive domain gap introduced from all the source domains. To tackle this problem, we newly propose a progressive mix-up (P-Mixup) mechanism to introduce an intermediate mix-up domain, pushing both the source domains and the few-shot target domain aligned to this mix-up domain. Further by enforcing the mix-up domain to progressively move towards the source domains, we achieve the domain transfer from multi-source domains to the single one-shot target domain. Our P-Mixup is different from traditional mix-up that ours is with a progressive and adaptive mix-up ratio, following the curriculum learning spirit to better align the source and target domains. Moreover, our P-Mixup combines both pixel-level and feature-level mix-up to better enrich the data diversity. Experiments on two benchmarks show that our P-Mixup significantly outperforms the state-of-the-art methods, i.e., 6.0\\% and 6.8\\% improvements on Office-Home and DomainNet",
    "checked": true,
    "id": "b2472267838a706a7c0258d0c3a185b80d61ae83",
    "semantic_title": "progressive mix-up for few-shot supervised multi-source domain transfer",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=F8VKQyDgRVj": {
    "title": "Neural Compositional Rule Learning for Knowledge Graph Reasoning",
    "volume": "poster",
    "abstract": "Learning logical rules is critical to improving reasoning in KGs. This is due to their ability to provide logical and interpretable explanations when used for predictions, as well as their ability to generalize to other tasks, domains, and data. While recent methods have been proposed to learn logical rules, the majority of these methods are either restricted by their computational complexity and can not handle the large search space of large-scale KGs, or show poor generalization when exposed to data outside the training set. In this paper, we propose an end-to-end neural model for learning compositional logical rules called NCRL. NCRL detects the best compositional structure of a rule body, and breaks it into small compositions in order to infer the rule head. By recurrently merging compositions in the rule body with a recurrent attention unit, NCRL finally predicts a single rule head. Experimental results show that NCRL learns high-quality rules, as well as being generalizable. Specifically, we show that NCRL is scalable, efficient, and yields state-of-the-art results for knowledge graph completion on large-scale KGs. Moreover, we test NCRL for systematic generalization by learning to reason on small-scale observed graphs and evaluating on larger unseen ones",
    "checked": true,
    "id": "58820d7d68e12d2a531e771a5fcb7e461c25f5c6",
    "semantic_title": "neural compositional rule learning for knowledge graph reasoning",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=XC_yGI-0j9": {
    "title": "Efficient approximation of neural population structure and correlations with probabilistic circuits",
    "volume": "poster",
    "abstract": "We present a computationally efficient framework to model a wide range of population structures with high order correlations and a large number of neurons. Our method is based on a special type of Bayesian network that has linear inference time and is founded upon the concept of contextual independence. Moreover, we use an efficient architecture learning method for network selection to model large neural populations even with a small amount of data. Our framework is both fast and accurate in approximating neural population structures. Furthermore, our approach enables us to reliably quantify higher order neural correlations. We test our method on simulated neural populations commonly used to generate higher order correlations, as well as on publicly available large-scale neural recordings from the Allen Brain Observatory. Our approach significantly outperforms other models both in terms of statistical measures and alignment with experimental evidence",
    "checked": true,
    "id": "5f6781a009d3584c5fef903d732c69d6fa6b2f05",
    "semantic_title": "efficient approximation of neural population structure and correlations with probabilistic circuits",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4cOfD2qL6T": {
    "title": "Exploring perceptual straightness in learned visual representations",
    "volume": "poster",
    "abstract": "Humans have been shown to use a ''straightened'' encoding to represent the natural visual world as it evolves in time (Henaff et al. 2019). In the context of discrete video sequences, ''straightened'' means that changes between frames follow a more linear path in representation space at progressively deeper levels of processing. While deep convolutional networks are often proposed as models of human visual processing, many do not straighten natural videos. In this paper, we explore the relationship between network architecture, differing types of robustness, biologically-inspired filtering mechanisms, and representational straightness in response to time-varying input; we identify strengths and limitations of straightness as a useful way of evaluating neural network representations. We find that (1) adversarial training leads to straighter representations in both CNN and transformer-based architectures but (2) this effect is task-dependent, not generalizing to tasks such as segmentation and frame-prediction, where straight representations are not favorable for predictions; and nor to other types of robustness. In addition, (3) straighter representations impart temporal stability to class predictions, even for out-of-distribution data. Finally, (4) biologically-inspired elements increase straightness in the early stages of a network, but do not guarantee increased straightness in downstream layers of CNNs. We show that straightness is an easily computed measure of representational robustness and stability, as well as a hallmark of human representations with benefits for computer vision models",
    "checked": true,
    "id": "ef1e0be5660333544a4525bc125171c3529c519c",
    "semantic_title": "exploring perceptual straightness in learned visual representations",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=dL35lx-mTEs": {
    "title": "Is Forgetting Less a Good Inductive Bias for Forward Transfer?",
    "volume": "poster",
    "abstract": "One of the main motivations of studying continual learning is that the problem setting allows a model to accrue knowledge from past tasks to learn new tasks more efficiently. However, recent studies suggest that the key metric that continual learning algorithms optimize, reduction in catastrophic forgetting, does not correlate well with the forward transfer of knowledge. We believe that the conclusion previous works reached is due to the way they measure forward transfer. We argue that the measure of forward transfer to a task should not be affected by the restrictions placed on the continual learner in order to preserve knowledge of previous tasks. Instead, forward transfer should be measured by how easy it is to learn a new task given a set of representations produced by continual learning on previous tasks. Under this notion of forward transfer, we evaluate different continual learning algorithms on a variety of image classification benchmarks. Our results indicate that less forgetful representations lead to a better forward transfer suggesting a strong correlation between retaining past information and learning efficiency on new tasks. Further, we found less forgetful representations to be more diverse and discriminative compared to their forgetful counterparts",
    "checked": true,
    "id": "eaab8c5159fecd44687a1639d542e4fa4b2b0003",
    "semantic_title": "is forgetting less a good inductive bias for forward transfer?",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=7J-30ilaUZM": {
    "title": "Learning Structured Representations by Embedding Class Hierarchy",
    "volume": "poster",
    "abstract": "Existing models for learning representations in supervised classification problems are permutation invariant with respect to class labels. However, structured knowledge about the classes, such as hierarchical label structures, widely exists in many real-world datasets, e.g., the ImageNet and CIFAR benchmarks. How to learn representations that can preserve such structures among the classes remains an open problem. To approach this problem, given a tree of class hierarchy, we first define a tree metric between any pair of nodes in the tree to be the length of the shortest path connecting them. We then provide a method to learn the hierarchical relationship of class labels by approximately embedding the tree metric in the Euclidean space of features. More concretely, during supervised training, we propose to use the Cophenetic Correlation Coefficient (CPCC) as a regularizer for the cross-entropy loss to correlate the tree metric of classes and the Euclidean distance in the class-conditioned representations. Our proposed regularizer is computationally lightweight and easy to implement. Empirically, we demonstrate that this approach can help to learn more interpretable representations due to the preservation of the tree metric, and leads to better in-distribution generalization as well as under sub-population shifts over six real-world datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gmL46YMpu2J": {
    "title": "Promptagator: Few-shot Dense Retrieval From 8 Examples",
    "volume": "poster",
    "abstract": "Much recent research on information retrieval has focused on how to transfer from one task (typically with abundant supervised data) to various other retrieval tasks where supervision is limited, with the implicit assumption that it is possible to generalize from one task to all the rest. However, this overlooks the fact that there are many diverse and unique retrieval problems, each targeting different search intents, queries, and search domains. In this paper, we suggest to work on Few-shot Dense Retrieval, a setting where each task comes with a short description and a few examples. To address this, we introduce Prompt-based Query Generation forRetrieval (Promptagator): for each task, we feed the few-shot examples to a large language model (LLM) and prompt it to behave as a task-specific query generator. Using this, we can synthetically generate a large number of relevant queries for any document, yielding abundant data for training task-specific retrievers --- with no reliance on traditional resources such as Natural Questions (Kwiatkowskiet al., 2019) or MS MARCO (Nguyen et al., 2016). Surprisingly, Promptagator with only 8 annotated examples enables efficient dual encoder retrievers to outperform computationally more expensive models trained on MS MARCO such as ColBERT v2 (Santhanam et al., 2022) by more than 1.2 points nDCG@10 on average on 11 retrieval sets. Further training standard-size re-rankers using the same generated data yields another 5.0 points nDCG@10 improvement. Our studies show that synthetic query generation can be far more effective than previously observed, especially when a small amount of task-specific knowledge is given",
    "checked": true,
    "id": "e86009d9f9b1cdf083a48d087552bc4153784451",
    "semantic_title": "promptagator: few-shot dense retrieval from 8 examples",
    "citation_count": 255,
    "authors": []
  },
  "https://openreview.net/forum?id=mCmerkTCG2S": {
    "title": "Brain-like representational straightening of natural movies in robust feedforward neural networks",
    "volume": "poster",
    "abstract": "Representational straightening refers to a decrease in curvature of visual feature representations of a sequence of frames taken from natural movies. Prior work established straightening in neural representations of the primate primary visual cortex (V1) and perceptual straightening in human behavior as a hallmark of biological vision in contrast to artificial feedforward neural networks which did not demonstrate this phenomenon as they were not explicitly optimized to produce temporally predictable movie representations. Here, we show robustness to noise in the input image can produce representational straightening in feedforward neural networks. Both adversarial training (AT) and base classifiers for Random Smoothing (RS) induced remarkably straightened feature codes. Demonstrating their utility within the domain of natural movies, these codes could be inverted to generate intervening movie frames by linear interpolation in the feature space even though they were not trained on these trajectories. Demonstrating their biological utility, we found that AT and RS training improved predictions of neural data in primate V1 over baseline models providing a parsimonious, bio-plausible mechanism -- noise in the sensory input stages -- for generating representations in early visual cortex. Finally, we compared the geometric properties of frame representations in these networks to better understand how they produced representations that mimicked the straightening phenomenon from biology. Overall, this work elucidating emergent properties of robust neural networks demonstrates that it is not necessary to utilize predictive objectives or train directly on natural movie statistics to achieve models supporting straightened movie representations similar to human perception that also predict V1 neural responses",
    "checked": true,
    "id": "2fff703a2cfe081b86ea3dc877a0a068da9ae503",
    "semantic_title": "brain-like representational straightening of natural movies in robust feedforward neural networks",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=BT4N_v7CLrk": {
    "title": "FunkNN: Neural Interpolation for Functional Generation",
    "volume": "poster",
    "abstract": "Can we build continuous generative models which generalize across scales, can be evaluated at any coordinate, admit calculation of exact derivatives, and are conceptually simple? Existing MLP-based architectures generate worse samples than the grid-based generators with favorable convolutional inductive biases. Models that focus on generating images at different scales do better, but employ complex architectures not designed for continuous evaluation of images and derivatives. We take a signal-processing perspective and treat continuous signal generation as interpolation from samples. Indeed, correctly sampled discrete images contain all information about the low spatial frequencies. The question is then how to extrapolate the spectrum in a data-driven way while meeting the above design criteria. Our answer is FunkNN---a novel convolutional network which learns how to reconstruct continuous images at arbitrary coordinates and can be applied to any image dataset. Combined with a discrete generative model it becomes a functional generator which can act as a prior in continuous ill-posed inverse problems. We show that FunkNN generates high-quality continuous images and exhibits strong out-of-distribution performance thanks to its patch-based design. We further showcase its performance in several stylized inverse problems with exact spatial derivatives",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aCuFa-RRqtI": {
    "title": "Label Propagation with Weak Supervision",
    "volume": "poster",
    "abstract": "Semi-supervised learning and weakly supervised learning are important paradigms that aim to reduce the growing demand for labeled data in current machine learning applications. In this paper, we introduce a novel analysis of the classical label propagation algorithm (LPA) (Zhu & Ghahramani, 2002) that moreover takes advantage of useful prior information, specifically probabilistic hypothesized labels on the unlabeled data. We provide an error bound that exploits both the local geometric properties of the underlying graph and the quality of the prior information. We also propose a framework to incorporate multiple sources of noisy information. In particular, we consider the setting of weak supervision, where our sources of information are weak labelers. We demonstrate the ability of our approach on multiple benchmark weakly supervised classification tasks, showing improvements upon existing semi-supervised and weakly supervised methods",
    "checked": true,
    "id": "3846545fe2082a15e8075fbfd960eaf56ce779d7",
    "semantic_title": "label propagation with weak supervision",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=4TyNEhI2GdN": {
    "title": "TypeT5: Seq2seq Type Inference using Static Analysis",
    "volume": "poster",
    "abstract": "There has been growing interest in automatically predicting missing type annotations in programs written in Python and JavaScript. While prior methods have achieved impressive accuracy when predicting the most common types, they often perform poorly on rare or complex types. In this paper, we present a new type inference method that treats type prediction as a code infilling task by leveraging CodeT5, a state-of-the-art seq2seq pre-trained language model for code. Our method uses static analysis to construct dynamic contexts for each code element whose type signature is to be predicted by the model. We also propose an iterative decoding scheme that incorporates previous type predictions in the model's input context, allowing information exchange between related code elements. Our evaluation shows that the proposed approach, TypeT5, not only achieves a higher overall accuracy (particularly on rare and complex types) but also produces more coherent results with fewer type errors---while enabling easy user intervention",
    "checked": true,
    "id": "0b7509abcf9af50ad4e551be35f11b62ce7d2695",
    "semantic_title": "typet5: seq2seq type inference using static analysis",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=IrzkT99fDJH": {
    "title": "AGRO: Adversarial discovery of error-prone Groups for Robust Optimization",
    "volume": "poster",
    "abstract": "Models trained via empirical risk minimization (ERM) are known to rely on spurious correlations between labels and task-independent input features, resulting in poor generalization to distributional shifts. Group distributionally robust optimization (G-DRO) can alleviate this problem by minimizing the worst-case loss over a set of pre-defined groups over training data. G-DRO successfully improves performance of the worst group, where the correlation does not hold. However, G-DRO assumes that the spurious correlations and associated worst groups are known in advance, making it challenging to apply them to new tasks with potentially multiple unknown correlations. We propose AGRO---Adversarial Group discovery for Distributionally Robust Optimization---an end-to-end approach that jointly identifies error-prone groups and improves accuracy on them. AGRO equips G-DRO with an adversarial slicing model to find a group assignment for training examples which maximizes worst-case loss over the discovered groups. On the WILDS benchmark, AGRO results in 8\\% higher model performance on average on known worst-groups, compared to prior group discovery approaches used with G-DRO. AGRO also improves out-of-distribution performance on SST2, QQP, and MS-COCO---datasets where potential spurious correlations are as yet uncharacterized. Human evaluation of ARGO groups shows that they contain well-defined, yet previously unstudied spurious correlations that lead to model errors",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2b2s9vd7wYv": {
    "title": "LogicDP: Creating Labels for Graph Data via Inductive Logic Programming",
    "volume": "poster",
    "abstract": "Graph data, such as scene graphs and knowledge graphs, see wide use in AI systems. In real-world and large applications graph data are usually incomplete, motivating graph reasoning models for missing-fact or missing-relationship inference. While these models can achieve state-of-the-art performance, they require a large amount of training data. Recent years have witnessed the rising interest in label creation with data programming (DP) methods, which aim to generate training labels from heuristic labeling functions. However, existing methods typically focus on unstructured data and are not optimized for graphs. In this work, we propose LogicDP, a data programming framework for graph data. Unlike existing DP methods, (1) LogicDP utilizes the inductive logic programming (ILP) technique and automatically discovers the labeling functions from the graph data; (2) LogicDP employs a budget-aware framework to iteratively refine the functions by querying an oracle, which significantly reduces the human efforts in function creations. Experiments show that LogicDP achieves better data efficiency in both scene graph and knowledge graph reasoning tasks",
    "checked": true,
    "id": "73555a49203866198d503866d257acd0c75ecd8f",
    "semantic_title": "logicdp: creating labels for graph data via inductive logic programming",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j3GK3_xZydY": {
    "title": "Revisiting Intrinsic Reward for Exploration in Procedurally Generated Environments",
    "volume": "poster",
    "abstract": "Exploration under sparse rewards remains a key challenge in deep reinforcement learning. Recently, studying exploration in procedurally-generated environments has drawn increasing attention. Existing works generally combine lifelong intrinsic rewards and episodic intrinsic rewards to encourage exploration. Though various lifelong and episodic intrinsic rewards have been proposed, the individual contributions of the two kinds of intrinsic rewards to improving exploration are barely investigated. To bridge this gap, we disentangle these two parts and conduct ablative experiments. We consider lifelong and episodic intrinsic rewards used in prior works, and compare the performance of all lifelong-episodic combinations on the commonly used MiniGrid benchmark. Experimental results show that only using episodic intrinsic rewards can match or surpass prior state-of-the-art methods. On the other hand, only using lifelong intrinsic rewards hardly makes progress in exploration. This demonstrates that episodic intrinsic reward is more crucial than lifelong one in boosting exploration. Moreover, we find through experimental analysis that the lifelong intrinsic reward does not accurately reflect the novelty of states, which explains why it does not help much in improving exploration",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TdBaDGCpjly": {
    "title": "Transformer-based World Models Are Happy With 100k Interactions",
    "volume": "poster",
    "abstract": "Deep neural networks have been successful in many reinforcement learning settings. However, compared to human learners they are overly data hungry. To build a sample-efficient world model, we apply a transformer to real-world episodes in an autoregressive manner: not only the compact latent states and the taken actions but also the experienced or predicted rewards are fed into the transformer, so that it can attend flexibly to all three modalities at different time steps. The transformer allows our world model to access previous states directly, instead of viewing them through a compressed recurrent state. By utilizing the Transformer-XL architecture, it is able to learn long-term dependencies while staying computationally efficient. Our transformer-based world model (TWM) generates meaningful, new experience, which is used to train a policy that outperforms previous model-free and model-based reinforcement learning algorithms on the Atari 100k benchmark. Our code is available at https://github.com/jrobine/twm",
    "checked": true,
    "id": "f15a8105f5fb5444ef3685a893f85073af175bc4",
    "semantic_title": "transformer-based world models are happy with 100k interactions",
    "citation_count": 100,
    "authors": []
  },
  "https://openreview.net/forum?id=HVoJCRLByVk": {
    "title": "Can Neural Networks Learn Implicit Logic from Physical Reasoning?",
    "volume": "poster",
    "abstract": "Despite the success of neural network models in a range of domains, it remains an open question whether they can learn to represent abstract logical operators such as negation and disjunction. We test the hypothesis that neural networks without inherent inductive biases for logical reasoning can acquire an implicit representation of negation and disjunction. Here, implicit refers to limited, domain-specific forms of these operators, and work in psychology suggests these operators may be a precursor (developmentally and evolutionarily) to the type of abstract, domain-general logic that is characteristic of adult humans. To test neural networks, we adapt a test designed to diagnose the presence of negation and disjunction in animals and pre-verbal children, which requires inferring the location of a hidden object using constraints of the physical environment as well as implicit logic: if a ball is hidden in A or B, and shown not to be in A, can the subject infer that it is in B? Our results show that, despite the neural networks learning to track objects behind occlusion, they are unable to generalize to a task that requires implicit logic. We further show that models are unable to generalize to the test task even when they are trained directly on a logically identical (though visually dissimilar) task. However, experiments using transfer learning reveal that the models do recognize structural similarity between tasks which invoke the same logical reasoning pattern, suggesting that some desirable abstractions are learned, even if they are not yet sufficient to pass established tests of logical reasoning",
    "checked": true,
    "id": "fc80b1324187d5f7aa8466a7cb94d0d1640dfc2e",
    "semantic_title": "can neural networks learn implicit logic from physical reasoning?",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=35QyoZv8cKO": {
    "title": "ESCHER: Eschewing Importance Sampling in Games by Computing a History Value Function to Estimate Regret",
    "volume": "poster",
    "abstract": "Recent techniques for approximating Nash equilibria in very large games leverage neural networks to learn approximately optimal policies (strategies). One promis- ing line of research uses neural networks to approximate counterfactual regret minimization (CFR) or its modern variants. DREAM, the only current CFR-based neural method that is model free and therefore scalable to very large games, trains a neural network on an estimated regret target that can have extremely high variance due to an importance sampling term inherited from Monte Carlo CFR (MCCFR). In this paper we propose an unbiased model-free method that does not require any importance sampling. Our method, ESCHER, is principled and is guaranteed to converge to an approximate Nash equilibrium with high probability. We show that the variance of the estimated regret of ESCHER is orders of magnitude lower than DREAM and other baselines. We then show that ESCHER outperforms the prior state of the art—DREAM and neural fictitious self play (NFSP)—on a number of games and the difference becomes dramatic as game size increases. In the very large game of dark chess, ESCHER is able to beat DREAM and NFSP in a head-to-head competition over 90% of the time",
    "checked": true,
    "id": "f2df422a39f2cebb4cc7f74af04ccf23bfaacc3b",
    "semantic_title": "escher: eschewing importance sampling in games by computing a history value function to estimate regret",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=fVm3nZMZs9": {
    "title": "On Achieving Optimal Adversarial Test Error",
    "volume": "poster",
    "abstract": "We first elucidate various fundamental properties of optimal adversarial predictors: the structure of optimal adversarial convex predictors in terms of optimal adversarial zero-one predictors, bounds relating the adversarial convex loss to the adversarial zero-one loss, and the fact that continuous predictors can get arbitrarily close to the optimal adversarial error for both convex and zero-one losses. Applying these results along with new Rademacher complexity bounds for adversarial training near initialization, we prove that for general data distributions and perturbation sets, adversarial training on shallow networks with early stopping and an idealized optimal adversary is able to achieve optimal adversarial test error. By contrast, prior theoretical work either considered specialized data distributions or only provided training error guarantees",
    "checked": true,
    "id": "9637c9b57eb8fff1802bbabe6431310bb64ec13f",
    "semantic_title": "on achieving optimal adversarial test error",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=FJXf1FXN8C": {
    "title": "Towards Understanding GD with Hard and Conjugate Pseudo-labels for Test-Time Adaptation",
    "volume": "poster",
    "abstract": "We consider a setting that a model needs to adapt to a new domain under distribution shifts, given that only unlabeled test samples from the new domain are accessible at test time. A common idea in most of the related works is constructing pseudo-labels for the unlabeled test samples and applying gradient descent (GD) to a loss function with the pseudo-labels. Recently, Goyal et al. (2022) propose conjugate labels, which is a new kind of pseudo-labels for self-training at test time. They empirically show that the conjugate label outperforms other ways of pseudo-labeling on many domain adaptation benchmarks. However, provably showing that GD with conjugate labels learns a good classifier for test-time adaptation remains open. In this work, we aim at theoretically understanding GD with hard and conjugate labels for a binary classification problem. We show that for square loss, GD with conjugate labels converges to an $\\epsilon$-optimal predictor under a Gaussian model for any arbitrarily small $\\epsilon$, while GD with hard pseudo-labels fails in this task. We also analyze them under different loss functions for the update. Our results shed lights on understanding when and why GD with hard labels or conjugate labels works in test-time adaptation",
    "checked": true,
    "id": "4ab4a99fde6ffbfd1ca9a4313418c98d6a52c300",
    "semantic_title": "towards understanding gd with hard and conjugate pseudo-labels for test-time adaptation",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=6QkjC_cs03X": {
    "title": "A VAE for Transformers with Nonparametric Variational Information Bottleneck",
    "volume": "poster",
    "abstract": "We propose a Variational AutoEncoder (VAE) for Transformers by developing a Variational Information Bottleneck (VIB) regulariser for Transformer embeddings. We formalise such attention-based representations as mixture distributions, and use Bayesian nonparametrics to develop a Nonparametric VIB (NVIB) for them. The variable number of mixture components supported by nonparametrics captures the variable number of vectors supported by attention, and exchangeable distributions from nonparametrics capture the permutation invariance of attention. Our Transformer VAE (NVAE) uses NVIB to regularise the information passing from the Transformer encoder to the Transformer decoder. Evaluations of a NVAE, trained on natural language text, demonstrate that NVIB can regularise the number of mixture components in the induced embedding whilst maintaining generation quality and reconstruction capacity",
    "checked": true,
    "id": "c7ccf90e0c15e28b18f7ff517a07cc82adecd708",
    "semantic_title": "a vae for transformers with nonparametric variational information bottleneck",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=Fh97BDaR6I": {
    "title": "On The Specialization of Neural Modules",
    "volume": "poster",
    "abstract": "A number of machine learning models have been proposed with the goal of achieving systematic generalization: the ability to reason about new situations by combining aspects of previous experiences. These models leverage compositional architectures which aim to learn specialized modules dedicated to structures in a task that can be composed to solve novel problems with similar structures. While the compositionality of these architectures is guaranteed by design, the modules specializing is not. Here we theoretically study the ability of network modules to specialize to useful structures in a dataset and achieve systematic generalization. To this end we introduce a minimal space of datasets motivated by practical systematic generalization benchmarks. From this space of datasets we present a mathematical definition of systematicity and study the learning dynamics of linear neural modules when solving components of the task. Our results shed light on the difficulty of module specialization, what is required for modules to successfully specialize, and the necessity of modular architectures to achieve systematicity. Finally, we confirm that the theoretical results in our tractable setting generalize to more complex datasets and non-linear architectures",
    "checked": false,
    "id": "0be71190ec12789ce2c9c1173839dddd5060667a",
    "semantic_title": "dynamics of specialization in neural modules under resource constraints",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=D7srTrGhAs": {
    "title": "HomoDistil: Homotopic Task-Agnostic Distillation of Pre-trained Transformers",
    "volume": "poster",
    "abstract": "Knowledge distillation has been shown to be a powerful model compression approach to facilitate the deployment of pre-trained language models in practice. This paper focuses on task-agnostic distillation. It produces a compact pre-trained model that can be easily fine-tuned on various tasks with small computational costs and memory footprints. Despite the practical benefits, task-agnostic distillation is challenging. Since the teacher model has a significantly larger capacity and stronger representation power than the student model, it is very difficult for the student to produce predictions that match the teacher's over a massive amount of open-domain training data. Such a large prediction discrepancy often diminishes the benefits of knowledge distillation. To address this challenge, we propose Homotopic Distillation (HomoDistil), a novel task-agnostic distillation approach equipped with iterative pruning. Specifically, we initialize the student model from the teacher model, and iteratively prune the student's neurons until the target width is reached. Such an approach maintains a small discrepancy between the teacher's and student's predictions throughout the distillation process, which ensures the effectiveness of knowledge transfer. Extensive experiments demonstrate that HomoDistil achieves significant improvements on existing baselines. Our codes will be released",
    "checked": true,
    "id": "b835345c6168d7b179516700aa4460912a8857e9",
    "semantic_title": "homodistil: homotopic task-agnostic distillation of pre-trained transformers",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=4u42KCQxCn8": {
    "title": "Using Both Demonstrations and Language Instructions to Efficiently Learn Robotic Tasks",
    "volume": "poster",
    "abstract": "Demonstrations and natural language instructions are two common ways to specify and teach robots novel tasks. However, for many complex tasks, a demonstration or language instruction alone contains ambiguities, preventing tasks from being specified clearly. In such cases, a combination of both a demonstration and an instruction more concisely and effectively conveys the task to the robot than either modality alone. To instantiate this problem setting, we train a single multi-task policy on a few hundred challenging robotic pick-and-place tasks and propose DeL-TaCo (Joint Demo-Language Task Conditioning), a method for conditioning a robotic policy on task embeddings comprised of two components: a visual demonstration and a language instruction. By allowing these two modalities to mutually disambiguate and clarify each other during novel task specification, DeL-TaCo (1) substantially decreases the teacher effort needed to specify a new task and (2) achieves better generalization performance on novel objects and instructions over previous task-conditioning methods. To our knowledge, this is the first work to show that simultaneously conditioning a multi-task robotic manipulation policy on both demonstration and language embeddings improves sample efficiency and generalization over conditioning on either modality alone",
    "checked": true,
    "id": "11ba19641e12dc5f9cbe604fdeae2932f81f3056",
    "semantic_title": "using both demonstrations and language instructions to efficiently learn robotic tasks",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=NyR8OZFHw6i": {
    "title": "FIGARO: Controllable Music Generation using Learned and Expert Features",
    "volume": "poster",
    "abstract": "Recent symbolic music generative models have achieved significant improvements in the quality of the generated samples. Nevertheless, it remains hard for users to control the output in such a way that it matches their expectation. To address this limitation, high-level, human-interpretable conditioning is essential. In this work, we release FIGARO, a Transformer-based conditional model trained to generate symbolic music based on a sequence of high-level control codes. To this end, we propose description-to-sequence learning, which consists of automatically extracting fine-grained, human-interpretable features (the description) and training a sequence-to-sequence model to reconstruct the original sequence given only the description as input. FIGARO achieves state-of-the-art performance in multi-track symbolic music generation both in terms of style transfer and sample quality. We show that performance can be further improved by combining human-interpretable with learned features. Our extensive experimental evaluation shows that FIGARO is able to generate samples that closely adhere to the content of the input descriptions, even when they deviate significantly from the training distribution",
    "checked": true,
    "id": "31454071080f0593c7e6d3cde606bdef276d8152",
    "semantic_title": "figaro: controllable music generation using learned and expert features",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=fR3wGCk-IXp": {
    "title": "Language models are multilingual chain-of-thought reasoners",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "62f0db3a5ad5c795ec18fc7a6e7b01836809df57",
    "semantic_title": "language models are multilingual chain-of-thought reasoners",
    "citation_count": 405,
    "authors": []
  },
  "https://openreview.net/forum?id=-cqvvvb-NkI": {
    "title": "Recitation-Augmented Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ed99a2572fb5f4240aa6068e3bf274832e831306",
    "semantic_title": "recitation-augmented language models",
    "citation_count": 68,
    "authors": []
  },
  "https://openreview.net/forum?id=p0JSSa1AuV": {
    "title": "KwikBucks: Correlation Clustering with Cheap-Weak and Expensive-Strong Signals",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bcd42145cf7bf312484e94b86bd0c6895cb99975",
    "semantic_title": "kwikbucks: correlation clustering with cheap-weak and expensive-strong signals",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=10uNUgI5Kl": {
    "title": "Reward Design with Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KdwnGErdT6": {
    "title": "Calibrating the Rigged Lottery: Making All Tickets Reliable",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b1c88b29bb53f83abe89fa74b2873e1a9d4aaccd",
    "semantic_title": "calibrating the rigged lottery: making all tickets reliable",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=FUiDMCr_W4o": {
    "title": "A Statistical Framework for Personalized Federated Learning and Estimation: Theory, Algorithms, and Privacy",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5af70db6c56ebc39d0ff58e74f6aba6fce2d67cd",
    "semantic_title": "a statistical framework for personalized federated learning and estimation: theory, algorithms, and privacy",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=w9WUQkBvpI": {
    "title": "Subsampling in Large Graphs Using Ricci Curvature",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dNqxZgyjcYA": {
    "title": "Conservative Bayesian Model-Based Value Expansion for Offline Policy Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b70f24e1c189d3caa11792a759bc763af93125f2",
    "semantic_title": "conservative bayesian model-based value expansion for offline policy optimization",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=PYbe4MoHf32": {
    "title": "Scaling up and Stabilizing Differentiable Planning with Implicit Differentiation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "330bf00e4dab2f24c2fe30971a7f7f164370a37d",
    "semantic_title": "scaling up and stabilizing differentiable planning with implicit differentiation",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=BYWWwSY2G5s": {
    "title": "Score-based Continuous-time Discrete Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "33433e9103b00aa0c42597cbfe13a429fbf5abdf",
    "semantic_title": "score-based continuous-time discrete diffusion models",
    "citation_count": 93,
    "authors": []
  },
  "https://openreview.net/forum?id=NmZXv4467ai": {
    "title": "Decision Transformer under Random Frame Dropping",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bhfp5GlDtGe": {
    "title": "Adversarial Imitation Learning with Preferences",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hNyJBk3CwR": {
    "title": "Is Model Ensemble Necessary? Model-based RL via a Single Model with Lipschitz Regularized Value Function",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "53a058a02c16d31acd2176c1104ac95e544fdcfa",
    "semantic_title": "is model ensemble necessary? model-based rl via a single model with lipschitz regularized value function",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=Q120_4COf-K": {
    "title": "Synthetic Data Generation of Many-to-Many Datasets via Random Graph Generation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ef9c719078722ad00e36d58e9017d77f8d9c62af",
    "semantic_title": "synthetic data generation of many-to-many datasets via random graph generation",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=k9CF4h3muD": {
    "title": "Learning Low Dimensional State Spaces with Overparameterized Recurrent Neural Nets",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d1193538e3ffd2eae9c9ecbeb0862b615d853a5a",
    "semantic_title": "learning low dimensional state spaces with overparameterized recurrent neural nets",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=ddad0PNUvV": {
    "title": "Images as Weight Matrices: Sequential Image Generation Through Synaptic Learning Rules",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8f30c30f92fbe1c307ee3c7a68c80a2c0dc8d619",
    "semantic_title": "images as weight matrices: sequential image generation through synaptic learning rules",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=svCcui6Drl": {
    "title": "Why (and When) does Local SGD Generalize Better than SGD?",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4539dcc4672a86b8f1e0bc64b92790d33518f55d",
    "semantic_title": "why (and when) does local sgd generalize better than sgd?",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=89GT-S49mGd": {
    "title": "Function-space regularized Rényi divergences",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "01b5b907a58a798359d7fde28bb457b11c7da8b8",
    "semantic_title": "function-space regularized rényi divergences",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SRIQZTh0IK": {
    "title": "Analogy-Forming Transformers for Few-Shot 3D Parsing",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4bb44f4be38340d194d1d668de892e1e0f852166",
    "semantic_title": "analogy-forming transformers for few-shot 3d parsing",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=QWQM0ZwZdRS": {
    "title": "Fake It Until You Make It : Towards Accurate Near-Distribution Novelty Detection",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e7b7d5cee616906c3fc202cb4c156813372182d5",
    "semantic_title": "fake it until you make it : towards accurate near-distribution novelty detection",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=Pgtn4l6eKjv": {
    "title": "DySR: Adaptive Super-Resolution via Algorithm and System Co-design",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a9957da5ce2e890b2a74fcbadfee12013538c711",
    "semantic_title": "dysr: adaptive super-resolution via algorithm and system co-design",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=n7CPzMPKQl": {
    "title": "Integrating Symmetry into Differentiable Planning with Steerable Convolutions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "a807a1c5048bc4f6022ff9194fc39b1b54f16c8c",
    "semantic_title": "integrating symmetry into differentiable planning",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=dcN0CaXQhT": {
    "title": "Causal Reasoning in the Presence of Latent Confounders via Neural ADMG Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ac770ac2627c6592a95a5a8c04bdf8e3970abadf",
    "semantic_title": "causal reasoning in the presence of latent confounders via neural admg learning",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=VWqiPBB_EM": {
    "title": "$O(T^{-1})$ Convergence of Optimistic-Follow-the-Regularized-Leader in Two-Player Zero-Sum Markov Games",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "efc026ace73425a4f990ad5a521e8653df85d072",
    "semantic_title": "o(t-1 convergence of optimistic-follow-the-regularized-leader in two-player zero-sum markov games",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xnsg4pfKb7": {
    "title": "Bispectral Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5b7244ead54d1f13ea3eab5342d6e27d82758bba",
    "semantic_title": "bispectral neural networks",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=pOyi9KqE56b": {
    "title": "Beyond Lipschitz: Sharp Generalization and Excess Risk Bounds for Full-Batch GD",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f3d4ce6fc9c1acb93846768f6a76c0d54fa9a9fd",
    "semantic_title": "beyond lipschitz: sharp generalization and excess risk bounds for full-batch gd",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=AatUEvC-Wjv": {
    "title": "Hyper-Decision Transformer for Efficient Online Policy Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "01706dd038b959e92a93f3141bb98be8f1f048f0",
    "semantic_title": "hyper-decision transformer for efficient online policy adaptation",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=U5XOGxAgccS": {
    "title": "Solving Continuous Control via Q-learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "dd95d004827a6183146b2f497c9cbbd30803aeed",
    "semantic_title": "solving continuous control via q-learning",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=nJfylDvgzlq": {
    "title": "Make-A-Video: Text-to-Video Generation without Text-Video Data",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1e33716e8820b867d5a8aaebab44c2d3135ea4ac",
    "semantic_title": "make-a-video: text-to-video generation without text-video data",
    "citation_count": 1555,
    "authors": []
  },
  "https://openreview.net/forum?id=wGvzQWFyUB": {
    "title": "Personalized Reward Learning with Interaction-Grounded Learning (IGL)",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3b4344a2d52ab2ac257b86c4a96bcf60eacaa4e0",
    "semantic_title": "personalized reward learning with interaction-grounded learning (igl)",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=4BPFwvKOvo5": {
    "title": "Towards convergence to Nash equilibria in two-team zero-sum games",
    "volume": "poster",
    "abstract": "Contemporary applications of machine learning raise important and overlooked theoretical questions regarding optimization in two-team games. Formally, two-team zero-sum games are defined as multi-player games where players are split into two competing sets of agents, each experiencing a utility identical to that of their teammates and opposite to that of the opposing team. We focus on the solution concept of Nash equilibria and prove $\\textrm{CLS}$-hardness of computing them in this class of games. To further examine the capabilities of online learning algorithms in games with full-information feedback, we propose a benchmark of a simple ---yet nontrivial--- family of such games. These games do not enjoy the properties used to prove convergence for relevant algorithms. In particular, we use a dynamical systems perspective to demonstrate that gradient descent-ascent, its optimistic variant, optimistic multiplicative weights update, and extra gradient fail to converge (even locally) to a Nash equilibrium. On a brighter note, we propose a first-order method that leverages control theory techniques and under some conditions enjoys last-iterate local convergence to a Nash equilibrium. We also believe our proposed method is of independent interest for general min-max optimization",
    "checked": true,
    "id": "4bf419ef1b7414aca02d42b4840026ced70be99e",
    "semantic_title": "towards convergence to nash equilibria in two-team zero-sum games",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=mFDU0fP3EQH": {
    "title": "Discovering Evolution Strategies via Meta-Black-Box Optimization",
    "volume": "poster",
    "abstract": "Optimizing functions without access to gradients is the remit of black-box meth- ods such as evolution strategies. While highly general, their learning dynamics are often times heuristic and inflexible — exactly the limitations that meta-learning can address. Hence, we propose to discover effective update rules for evolution strategies via meta-learning. Concretely, our approach employs a search strategy parametrized by a self-attention-based architecture, which guarantees the update rule is invariant to the ordering of the candidate solutions. We show that meta-evolving this system on a small set of representative low-dimensional analytic optimization problems is sufficient to discover new evolution strategies capable of generalizing to unseen optimization problems, population sizes and optimization horizons. Furthermore, the same learned evolution strategy can outperform established neuroevolution baselines on supervised and continuous control tasks. As additional contributions, we ablate the individual neural network components of our method; reverse engineer the learned strategy into an explicit heuristic form, which remains highly competitive; and show that it is possible to self-referentially train an evolution strategy from scratch, with the learned update rule used to drive the outer meta-learning loop",
    "checked": true,
    "id": "dae69e513a0a5c4f10c11bb56e4e3fddedff714e",
    "semantic_title": "discovering evolution strategies via meta-black-box optimization",
    "citation_count": 49,
    "authors": []
  },
  "https://openreview.net/forum?id=p7hvOJ6Gq0i": {
    "title": "DensePure: Understanding Diffusion Models for Adversarial Robustness",
    "volume": "poster",
    "abstract": "Diffusion models have been recently employed to improve certified robustness through the process of denoising. However, the theoretical understanding of why diffusion models are able to improve the certified robustness is still lacking, preventing from further improvement. In this study, we close this gap by analyzing the fundamental properties of diffusion models and establishing the conditions under which they can enhance certified robustness. This deeper understanding allows us to propose a new method DensePure, designed to improve the certified robustness of a pretrained model (i.e. classifier). Given an (adversarial) input, DensePure consists of multiple runs of denoising via the reverse process of the diffusion model (with different random seeds) to get multiple reversed samples, which are then passed through the classifier, followed by majority voting of inferred labels to make the final prediction. This design of using multiple runs of denoising is informed by our theoretical analysis of the conditional distribution of the reversed sample. Specifically, when the data density of a clean sample is high, its conditional density under the reverse process in a diffusion model is also high; thus sampling from the latter conditional distribution can purify the adversarial example and return the corresponding clean sample with a high probability. By using the highest density point in the conditional distribution as the reversed sample, we identify the robust region of a given instance under the diffusion model's reverse process. We show that this robust region is a union of multiple convex sets, and is potentially much larger than the robust regions identified in previous works. In practice, DensePure can approximate the label of the high density region in the conditional distribution so that it can enhance certified robustness. We conduct extensive experiments to demonstrate the effectiveness of DensePure by evaluating its certified robustness given a standard model via randomized smoothing. We show that DensePure is consistently better than existing methods on ImageNet, with 7% improvement on average",
    "checked": true,
    "id": "11417522f57c13898e24d87ef22f9e45fa197cf8",
    "semantic_title": "densepure: understanding diffusion models for adversarial robustness",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=jsZsEd8VEY": {
    "title": "Grounding Graph Network Simulators using Physical Sensor Observations",
    "volume": "poster",
    "abstract": "Physical simulations that accurately model reality are crucial for many engineering disciplines such as mechanical engineering and robotic motion planning. In recent years, learned Graph Network Simulators produced accurate mesh-based simulations while requiring only a fraction of the computational cost of traditional simulators. Yet, the resulting predictors are confined to learning from data generated by existing mesh-based simulators and thus cannot include real world sensory information such as point cloud data. As these predictors have to simulate complex physical systems from only an initial state, they exhibit a high error accumulation for long-term predictions. In this work, we integrate sensory information to ground Graph Network Simulators on real world observations. In particular, we predict the mesh state of deformable objects by utilizing point cloud data. The resulting model allows for accurate predictions over longer time horizons, even under uncertainties in the simulation, such as unknown material properties. Since point clouds are usually not available for every time step, especially in online settings, we employ an imputation-based model. The model can make use of such additional information only when provided, and resorts to a standard Graph Network Simulator, otherwise. We experimentally validate our approach on a suite of prediction tasks for mesh-based interactions between soft and rigid bodies. Our method results in utilization of additional point cloud information to accurately predict stable simulations where existing Graph Network Simulators fail",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=osei3IzUia": {
    "title": "Where to Diffuse, How to Diffuse, and How to Get Back: Automated Learning for Multivariate Diffusions",
    "volume": "poster",
    "abstract": "Diffusion-based generative models (DBGMs) perturb data to a target noise distribution and reverse this process to generate samples. The choice of noising process, or inference diffusion process, affects both likelihoods and sample quality. For example, extending the inference process with auxiliary variables leads to improved sample quality. While there are many such multivariate diffusions to explore, each new one requires significant model-specific analysis, hindering rapid prototyping and evaluation. In this work, we study Multivariate Diffusion Models (MDMs). For any number of auxiliary variables, we provide a recipe for maximizing a lower-bound on the MDMs likelihood without requiring any model-specific analysis. We then demonstrate how to parameterize the diffusion for a specified target noise distribution; these two points together enable optimizing the inference diffusion process. Optimizing the diffusion expands easy experimentation from just a few well-known processes to an automatic search over all linear diffusions. To demonstrate these ideas, we introduce two new specific diffusions as well as learn a diffusion process on the MNIST, CIFAR10, and ImageNet32 datasets. We show learned MDMs match or surpass bits-per-dims (BPDs) relative to fixed choices of diffusions for a given dataset and model architecture",
    "checked": true,
    "id": "ffd9130570439d67a97cf3f6c19786db781ae231",
    "semantic_title": "where to diffuse, how to diffuse, and how to get back: automated learning for multivariate diffusions",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=eWKfMBL5to": {
    "title": "Contrastive Corpus Attribution for Explaining Representations",
    "volume": "poster",
    "abstract": "Despite the widespread use of unsupervised models, very few methods are designed to explain them. Most explanation methods explain a scalar model output. However, unsupervised models output representation vectors, the elements of which are not good candidates to explain because they lack semantic meaning. To bridge this gap, recent works defined a scalar explanation output: a dot product-based similarity in the representation space to the sample being explained (i.e., an explicand). Although this enabled explanations of unsupervised models, the interpretation of this approach can still be opaque because similarity to the explicand's representation may not be meaningful to humans. To address this, we propose contrastive corpus similarity, a novel and semantically meaningful scalar explanation output based on a reference corpus and a contrasting foil set of samples. We demonstrate that contrastive corpus similarity is compatible with many post-hoc feature attribution methods to generate COntrastive COrpus Attributions (COCOA) and quantitatively verify that features important to the corpus are identified. We showcase the utility of COCOA in two ways: (i) we draw insights by explaining augmentations of the same image in a contrastive learning setting (SimCLR); and (ii) we perform zero-shot object localization by explaining the similarity of image representations to jointly learned text representations (CLIP)",
    "checked": true,
    "id": "560f10749054035a4645b2732fb4404061e6eb9f",
    "semantic_title": "contrastive corpus attribution for explaining representations",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=PsIk0kO3hKd": {
    "title": "Spatio-temporal point processes with deep non-stationary kernels",
    "volume": "poster",
    "abstract": "Point process data are becoming ubiquitous in modern applications, such as social networks, health care, and finance. Despite the powerful expressiveness of the popular recurrent neural network (RNN) models for point process data, they may not successfully capture sophisticated non-stationary dependencies in the data due to their recurrent structures. Another popular type of deep model for point process data is based on representing the influence kernel (rather than the intensity function) by neural networks. We take the latter approach and develop a new deep non-stationary influence kernel that can model non-stationary spatio-temporal point processes. The main idea is to approximate the influence kernel with a novel and general low-rank decomposition, enabling efficient representation through deep neural networks and computational efficiency and better performance. We also take a new approach to maintain the non-negativity constraint of the conditional intensity by introducing a log-barrier penalty. We demonstrate our proposed method's good performance and computational efficiency compared with the state-of-the-art on simulated and real data",
    "checked": true,
    "id": "da8a2be8914712a04ba8562b65215c30a198ac6c",
    "semantic_title": "spatio-temporal point processes with deep non-stationary kernels",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=hDDV1lsRV8": {
    "title": "Federated Learning from Small Datasets",
    "volume": "poster",
    "abstract": "Federated learning allows multiple parties to collaboratively train a joint model without having to share any local data. It enables applications of machine learning in settings where data is inherently distributed and undisclosable, such as in the medical domain. Joint training is usually achieved by aggregating local models. When local datasets are small, locally trained models can vary greatly from a globally good model. Bad local models can arbitrarily deteriorate the aggregate model quality, causing federating learning to fail in these settings. We propose a novel approach that avoids this problem by interleaving model aggregation and permutation steps. During a permutation step we redistribute local models across clients through the server, while preserving data privacy, to allow each local model to train on a daisy chain of local datasets. This enables successful training in data-sparse domains. Combined with model aggregation, this approach enables effective learning even if the local datasets are extremely small, while retaining the privacy benefits of federated learning",
    "checked": true,
    "id": "8d20e3f4f11e757d99259e3bd1b2dfc4366d15f1",
    "semantic_title": "federated learning from small datasets",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=lGz9u1ubUXE": {
    "title": "Relative Behavioral Attributes: Filling the Gap between Symbolic Goal Specification and Reward Learning from Human Preferences",
    "volume": "poster",
    "abstract": "Generating complex behaviors that satisfy the preferences of non-expert users is a crucial requirement for AI agents. Interactive reward learning from trajectory comparisons (a.k.a. RLHF) is one way to allow non-expert users to convey complex objectives by expressing preferences over short clips of agent behaviors. Even though this parametric method can encode complex tacit knowledge present in the underlying tasks, it implicitly assumes that the human is unable to provide richer feedback than binary preference labels, leading to intolerably high feedback complexity and poor user experience. While providing a detailed symbolic closed-form specification of the objectives might be tempting, it is not always feasible even for an expert user. However, in most cases, humans are aware of how the agent should change its behavior along meaningful axes to fulfill their underlying purpose, even if they are not able to fully specify task objectives symbolically. Using this as motivation, we introduce the notion of Relative Behavioral Attributes, which allows the users to tweak the agent behavior through symbolic concepts (e.g., increasing the softness or speed of agents' movement). We propose two practical methods that can learn to model any kind of behavioral attributes from ordered behavior clips. We demonstrate the effectiveness of our methods on four tasks with nine different behavioral attributes, showing that once the attributes are learned, end users can produce desirable agent behaviors relatively effortlessly, by providing feedback just around ten times. This is over an order of magnitude less than that required by the popular learning-from-human-preferences baselines. The supplementary video and source code are available at: https://guansuns.github.io/pages/rba",
    "checked": true,
    "id": "8e2daf1f06d2d75bff9519c86382bf50fff6ae0f",
    "semantic_title": "relative behavioral attributes: filling the gap between symbolic goal specification and reward learning from human preferences",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=GRZtigJljLY": {
    "title": "Scalable Batch-Mode Deep Bayesian Active Learning via Equivalence Class Annealing",
    "volume": "poster",
    "abstract": "Active learning has demonstrated data efficiency in many fields. Existing active learning algorithms, especially in the context of batch-mode deep Bayesian active models, rely heavily on the quality of uncertainty estimations of the model, and are often challenging to scale to large batches. In this paper, we propose Batch-BALanCe, a scalable batch-mode active learning algorithm, which combines insights from decision-theoretic active learning, combinatorial information measure, and diversity sampling. At its core, Batch-BALanCe relies on a novel decision-theoretic acquisition function that facilitates differentiation among different equivalence classes. Intuitively, each equivalence class consists of hypotheses (e.g., posterior samples of deep neural networks) with similar predictions, and Batch-BALanCe adaptively adjusts the size of the equivalence classes as learning progresses. To scale up the computation of queries to large batches, we further propose an efficient batch-mode acquisition procedure, which aims to maximize a novel combinatorial information measure defined through the acquisition function. We show that our algorithm can effectively handle realistic multi-class classification tasks, and achieves compelling performance on several benchmark datasets for active learning under both low- and large-batch regimes",
    "checked": true,
    "id": "1c1047e1021285cbdb8edb8a404c51d9d748f94b",
    "semantic_title": "scalable batch-mode deep bayesian active learning via equivalence class annealing",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=FE99-fDrWd5": {
    "title": "Semi-Parametric Inducing Point Networks and Neural Processes",
    "volume": "poster",
    "abstract": "We introduce semi-parametric inducing point networks (SPIN), a general-purpose architecture that can query the training set at inference time in a compute-efficient manner. Semi-parametric architectures are typically more compact than parametric models, but their computational complexity is often quadratic. In contrast, SPIN attains linear complexity via a cross-attention mechanism between datapoints inspired by inducing point methods. Querying large training sets can be particularly useful in meta-learning, as it unlocks additional training signal, but often exceeds the scaling limits of existing models. We use SPIN as the basis of the Inducing Point Neural Process, a probabilistic model which supports large contexts in meta-learning and achieves high accuracy where existing models fail. In our experiments, SPIN reduces memory requirements, improves accuracy across a range of meta-learning tasks, and improves state-of-the-art performance on an important practical problem, genotype imputation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m9LCdYgN8-6": {
    "title": "DAG Learning on the Permutahedron",
    "volume": "poster",
    "abstract": "We propose a continuous optimization framework for discovering a latent directed acyclic graph (DAG) from observational data. Our approach optimizes over the polytope of permutation vectors, the so-called Permutahedron, to learn a topological ordering. Edges can be optimized jointly, or learned conditional on the ordering via a non-differentiable subroutine. Compared to existing continuous optimization approaches our formulation has a number of advantages including: 1. validity: optimizes over exact DAGs as opposed to other relaxations optimizing approximate DAGs; 2. modularity: accommodates any edge-optimization procedure, edge structural parameterization, and optimization loss; 3. end-to-end: either alternately iterates between node-ordering and edge-optimization, or optimizes them jointly; We demonstrate, on real-world data problems in protein-signaling and transcriptional network discovery, that our approach lies on the Pareto frontier of two key metrics, the SID and SHD",
    "checked": true,
    "id": "354180d79bac0380188089066de79cfea36b5bfc",
    "semantic_title": "dag learning on the permutahedron",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=9krnQ-ue9M": {
    "title": "Explicitly Minimizing the Blur Error of Variational Autoencoders",
    "volume": "poster",
    "abstract": "Variational autoencoders (VAEs) are powerful generative modelling methods, however they suffer from blurry generated samples and reconstructions compared to the images they have been trained on. Significant research effort has been spent to increase the generative capabilities by creating more flexible models but often flexibility comes at the cost of higher complexity and computational cost. Several works have focused on altering the reconstruction term of the evidence lower bound (ELBO), however, often at the expense of losing the mathematical link to maximizing the likelihood of the samples under the modeled distribution. Here we propose a new formulation of the reconstruction term for the VAE that specifically penalizes the generation of blurry images while at the same time still maximizing the ELBO under the modeled distribution. We show the potential of the proposed loss on three different data sets, where it outperforms several recently proposed reconstruction losses for VAEs",
    "checked": true,
    "id": "c426a0456c4bbd3cb1d5526f7654e9341e1cf91b",
    "semantic_title": "explicitly minimizing the blur error of variational autoencoders",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=kJqXEPXMsE0": {
    "title": "3D Equivariant Diffusion for Target-Aware Molecule Generation and Affinity Prediction",
    "volume": "poster",
    "abstract": "Rich data and powerful machine learning models allow us to design drugs for a specific protein target <em>in silico</em>. Recently, the inclusion of 3D structures during targeted drug design shows superior performance to other target-free models as the atomic interaction in the 3D space is explicitly modeled. However, current 3D target-aware models either rely on the voxelized atom densities or the autoregressive sampling process, which are not equivariant to rotation or easily violate geometric constraints resulting in unrealistic structures. In this work, we develop a 3D equivariant diffusion model to solve the above challenges. To achieve target-aware molecule design, our method learns a joint generative process of both continuous atom coordinates and categorical atom types with a SE(3)-equivariant network. Moreover, we show that our model can serve as an unsupervised feature extractor to estimate the binding affinity under proper parameterization, which provides an effective way for drug screening. To evaluate our model, we propose a comprehensive framework to evaluate the quality of sampled molecules from different dimensions. Empirical studies show our model could generate molecules with more realistic 3D structures and better affinities towards the protein targets, and improve binding affinity ranking and prediction without retraining",
    "checked": true,
    "id": "59713b444d3268528416f23fe860ba63bb03fc04",
    "semantic_title": "3d equivariant diffusion for target-aware molecule generation and affinity prediction",
    "citation_count": 193,
    "authors": []
  },
  "https://openreview.net/forum?id=EBC60mxBwyw": {
    "title": "How gradient estimator variance and bias impact learning in neural networks",
    "volume": "poster",
    "abstract": "There is growing interest in understanding how real brains may approximate gradients and how gradients can be used to train neuromorphic chips. However, neither real brains nor neuromorphic chips can perfectly follow the loss gradient, so parameter updates would necessarily use gradient estimators that have some variance and/or bias. Therefore, there is a need to understand better how variance and bias in gradient estimators impact learning dependent on network and task properties. Here, we show that variance and bias can impair learning on the training data, but some degree of variance and bias in a gradient estimator can be beneficial for generalization. We find that the ideal amount of variance and bias in a gradient estimator are dependent on several properties of the network and task: the size and activity sparsity of the network, the norm of the gradient, and the curvature of the loss landscape. As such, whether considering biologically-plausible learning algorithms or algorithms for training neuromorphic chips, researchers can analyze these properties to determine whether their approximation to gradient descent will be effective for learning given their network and task properties",
    "checked": true,
    "id": "1d62dcb3a27bf9c8b46e7cefe2d5e46d2bac914d",
    "semantic_title": "how gradient estimator variance and bias impact learning in neural networks",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=Fsd-6ax4T1m": {
    "title": "Evaluating Representations with Readout Model Switching",
    "volume": "poster",
    "abstract": "Although much of the success of Deep Learning builds on learning good representations, a rigorous method to evaluate their quality is lacking. In this paper, we treat the evaluation of representations as a model selection problem and propose to use the Minimum Description Length (MDL) principle to devise an evaluation metric. Contrary to the established practice of limiting the capacity of the readout model, we design a hybrid discrete and continuous-valued model space for the readout models and employ a switching strategy to combine their predictions. The MDL score takes model complexity, as well as data efficiency into account. As a result, the most appropriate model for the specific task and representation will be chosen, making it a unified measure for comparison. The proposed metric can be efficiently computed with an online method and we present results for pre-trained vision encoders of various architectures (ResNet and ViT) and objective functions (supervised and self-supervised) on a range of downstream tasks. We compare our methods with accuracy-based approaches and show that the latter are inconsistent when multiple readout models are used. Finally, we discuss important properties revealed by our evaluations such as model scaling, preferred readout model, and data efficiency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kPPVmUF6bM_": {
    "title": "Augmentation with Projection: Towards an Effective and Efficient Data Augmentation Paradigm for Distillation",
    "volume": "poster",
    "abstract": "Knowledge distillation is one of the primary methods of transferring knowledge from large to small models. However, it requires massive task-specific data, which may not be plausible in many real-world applications. Data augmentation methods such as representation interpolation, token replacement, or augmentation with models are applied to tackle this problem. However, these data augmentation methods either potentially cause shifts in decision boundaries (representation interpolation), are not expressive enough (token replacement), or introduce too much computational overhead (augmentation with models). To this end, we propose AugPro (Augmentation with Projection), an effective and efficient data augmentation method for distillation. Our method builds on top of representation interpolation augmentation methods to maintain the diversity of expressions and converts the augmented data to tokens to avoid shifting decision boundaries. It uses simple operations that come with little computational overhead. The results on multiple GLUE tasks show that our methods can improve distillation performance by a large margin at a low time cost",
    "checked": true,
    "id": "d829ca8ec95550b291339d8fd1d4ede8581e6896",
    "semantic_title": "augmentation with projection: towards an effective and efficient data augmentation paradigm for distillation",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=9_gsMA8MRKQ": {
    "title": "Pseudoinverse-Guided Diffusion Models for Inverse Problems",
    "volume": "poster",
    "abstract": "Diffusion models have become competitive candidates for solving various inverse problems. Models trained for specific inverse problems work well but are limited to their particular use cases, whereas methods that use problem-agnostic models are general but often perform worse empirically. To address this dilemma, we introduce Pseudoinverse-guided Diffusion Models ($\\Pi$GDM), an approach that uses problem-agnostic models to close the gap in performance. $\\Pi$GDM directly estimates conditional scores from the measurement model of the inverse problem without additional training. It can address inverse problems with noisy, non-linear, or even non-differentiable measurements, in contrast to many existing approaches that are limited to noiseless linear ones. We illustrate the empirical effectiveness of $\\Pi$GDM on several image restoration tasks, including super-resolution, inpainting and JPEG restoration. On ImageNet, $\\Pi$GDM is competitive with state-of-the-art diffusion models trained on specific tasks, and is the first to achieve this with problem-agnostic diffusion models. $\\Pi$GDM can also solve a wider set of inverse problems where the measurement processes are composed of several simpler ones",
    "checked": true,
    "id": "9c81be0c478bfc0a48eedb8769326fe289a11acc",
    "semantic_title": "pseudoinverse-guided diffusion models for inverse problems",
    "citation_count": 359,
    "authors": []
  },
  "https://openreview.net/forum?id=cVFD6qE8gnY": {
    "title": "Planning with Sequence Models through Iterative Energy Minimization",
    "volume": "poster",
    "abstract": "Recent works have shown that language modeling can be effectively used to train reinforcement learning (RL) policies. However, the success of applying existing language models to planning, in which we wish to obtain a trajectory of actions to reach some goal, is less straightforward. The typical autoregressive generation procedures of language models preclude sequential refinement of earlier steps, which limits the effectiveness of a predicted plan. In this paper, we suggest an approach towards integrating planning with language models based on the idea of iterative energy minimization, and illustrate how such a procedure leads to improved RL performance across different tasks. We train a masked language model to capture an implicit energy function over trajectories of actions, and formulate planning as finding a trajectory of actions with minimum energy. We illustrate how this procedure enables improved performance over recent approaches across BabyAI and Atari environments. We further demonstrate unique benefits of our iterative optimization procedure, involving new task generalization, test-time constraints adaptation, and the ability to compose plans together. Project webpage: https://hychen-naza.github.io/projects/LEAP/index.html",
    "checked": true,
    "id": "290e7e50c0c945485ab711fec7bc753d41a47491",
    "semantic_title": "planning with sequence models through iterative energy minimization",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=Rvee9CAX4fi": {
    "title": "Verifying the Union of Manifolds Hypothesis for Image Data",
    "volume": "poster",
    "abstract": "Deep learning has had tremendous success at learning low-dimensional representations of high-dimensional data. This success would be impossible if there was no hidden low-dimensional structure in data of interest; this existence is posited by the manifold hypothesis, which states that the data lies on an unknown manifold of low intrinsic dimension. In this paper, we argue that this hypothesis does not properly capture the low-dimensional structure typically present in image data. Assuming that data lies on a single manifold implies intrinsic dimension is identical across the entire data space, and does not allow for subregions of this space to have a different number of factors of variation. To address this deficiency, we consider the union of manifolds hypothesis, which states that data lies on a disjoint union of manifolds of varying intrinsic dimensions. We empirically verify this hypothesis on commonly-used image datasets, finding that indeed, observed data lies on a disconnected set and that intrinsic dimension is not constant. We also provide insights into the implications of the union of manifolds hypothesis in deep learning, both supervised and unsupervised, showing that designing models with an inductive bias for this structure improves performance across classification and generative modelling tasks. Our code is available at https://github.com/layer6ai-labs/UoMH",
    "checked": true,
    "id": "04abbf01d30993361c0e15f252046140c6fa5a76",
    "semantic_title": "verifying the union of manifolds hypothesis for image data",
    "citation_count": 49,
    "authors": []
  },
  "https://openreview.net/forum?id=zlbci7019Z3": {
    "title": "Error Sensitivity Modulation based Experience Replay: Mitigating Abrupt Representation Drift in Continual Learning",
    "volume": "poster",
    "abstract": "Humans excel at lifelong learning, as the brain has evolved to be robust to distribution shifts and noise in our ever-changing environment. Deep neural networks (DNNs), however, exhibit catastrophic forgetting and the learned representations drift drastically as they encounter a new task. This alludes to a different error-based learning mechanism in the brain. Unlike DNNs, where learning scales linearly with the magnitude of the error, the sensitivity to errors in the brain decreases as a function of their magnitude. To this end, we propose \"ESMER\" which employs a principled mechanism to modulate error sensitivity in a dual-memory rehearsal-based system. Concretely, it maintains a memory of past errors and uses it to modify the learning dynamics so that the model learns more from small consistent errors compared to large sudden errors. We also propose \"Error-Sensitive Reservoir Sampling\" to maintain episodic memory, which leverages the error history to pre-select low-loss samples as candidates for the buffer, which are better suited for retaining information. Empirical results show that ESMER effectively reduces forgetting and abrupt drift in representations at the task boundary by gradually adapting to the new task while consolidating knowledge. Remarkably, it also enables the model to learn under high levels of label noise, which is ubiquitous in real-world data streams",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=39z0zPZ0AvB": {
    "title": "Don't forget the nullspace! Nullspace occupancy as a mechanism for out of distribution failure",
    "volume": "poster",
    "abstract": "Out of distribution (OoD) generalization has received considerable interest in recent years. In this work, we identify a particular failure mode of OoD generalization for discriminative classifiers that is based on test data (from a new domain) lying in the nullspace of features learnt from source data. We demonstrate the existence of this failure mode across multiple networks trained across RotatedMNIST, PACS, TerraIncognita, DomainNet and ImageNet-R datasets. We then study different choices for characterizing the feature space and show that projecting intermediate representations onto the span of directions that obtain maximum training accuracy provides consistent improvements in OoD performance. Finally, we show that such nullspace behavior also provides an insight into neural networks trained on poisoned data. We hope our work galvanizes interest in the relationship between the nullspace occupancy failure mode and generalization",
    "checked": true,
    "id": "713a9d2fe1ce824a841df2ed5d0b090b5def27e9",
    "semantic_title": "don't forget the nullspace! nullspace occupancy as a mechanism for out of distribution failure",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=SM7XkJouWHm": {
    "title": "ContraNorm: A Contrastive Learning Perspective on Oversmoothing and Beyond",
    "volume": "poster",
    "abstract": "Oversmoothing is a common phenomenon in a wide range of Graph Neural Networks (GNNs) and Transformers, where performance degenerates as the layer goes deeper. Instead of characterizing oversmoothing from the view of complete collapse in which representations converge to a single point, we dive into a more general perspective dimensional collapse in which representations lie in a narrow cone. Accordingly, inspired by the power of contrastive learning in preventing dimensional collapse, we propose a novel normalization layer ContraNorm. Intuitively, ContraNorm implicitly shatters representations in the embedding space, leading to a more uniform distribution and slighter dimensional collapse. On the theoretical analysis, we prove that ContraNorm can alleviate both complete collapse and dimensional collapse under some conditions. Our proposed normalization layer can be easily inserted into GNNs and Transformers with negligible parameter overhead. Experiments on various real-world datasets verify the effectiveness of our method",
    "checked": true,
    "id": "519193aa26820af205e6040d4595ca30ebcebcb0",
    "semantic_title": "contranorm: a contrastive learning perspective on oversmoothing and beyond",
    "citation_count": 49,
    "authors": []
  },
  "https://openreview.net/forum?id=HRwN7IQLUKA": {
    "title": "Accelerated Single-Call Methods for Constrained Min-Max Optimization",
    "volume": "poster",
    "abstract": "We study first-order methods for constrained min-max optimization. Existing methods either require two gradient calls or two projections in each iteration, which may be costly in some applications. In this paper, we first show that a variant of the \\emph{Optimistic Gradient (OG)} method, a \\emph{single-call single-projection} algorithm, has $O(\\frac{1}{\\sqrt{T}})$ best-iterate convergence rate for inclusion problems with operators that satisfy the weak Minty variation inequality (MVI). Our second result is the first single-call single-projection algorithm -- the \\emph{Accelerated Reflected Gradient (ARG)} method that achieves the \\emph{optimal $O(\\frac{1}{T})$} last-iterate convergence rate for inclusion problems that satisfy negative comonotonicity. Both the weak MVI and negative comonotonicity are well-studied assumptions and capture a rich set of non-convex non-concave min-max optimization problems. Finally, we show that the \\emph{Reflected Gradient (RG)} method, another \\emph{single-call single-projection} algorithm, has $O(\\frac{1}{\\sqrt{T}})$ last-iterate convergence rate for constrained convex-concave min-max optimization, answering an open problem of [Hsieh et al., 2019]. Our convergence rates hold for standard measures such as the tangent residual and the natural residual",
    "checked": true,
    "id": "8e0db841036cf9121f2b3c7ec23db79b498b6545",
    "semantic_title": "accelerated single-call methods for constrained min-max optimization",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=b3itJyarLM0": {
    "title": "Distributed Extra-gradient with Optimal Complexity and Communication Guarantees",
    "volume": "poster",
    "abstract": "We consider monotone variational inequality (VI) problems in multi-GPU settings where multiple processors/workers/clients have access to local stochastic dual vectors. This setting includes a broad range of important problems from distributed convex minimization to min-max and games. Extra-gradient, which is a de facto algorithm for monotone VI problems, has not been designed to be communication-efficient. To this end, we propose a quantized generalized extra-gradient (Q-GenX), which is an unbiased and adaptive compression method tailored to solve VIs. We provide an adaptive step-size rule, which adapts to the respective noise profiles at hand and achieve a fast rate of ${\\cal O}(1/T)$ under relative noise, and an order-optimal ${\\cal O}(1/\\sqrt{T})$ under absolute noise and show distributed training accelerates convergence. Finally, we validate our theoretical results by providing real-world experiments and training generative adversarial networks on multiple GPUs",
    "checked": true,
    "id": "f58290a33ab20802d677c7b7c0cb006ff4892087",
    "semantic_title": "distributed extra-gradient with optimal complexity and communication guarantees",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=20gBzEzgtiI": {
    "title": "Performance Bounds for Model and Policy Transfer in Hidden-parameter MDPs",
    "volume": "poster",
    "abstract": "In the Hidden-Parameter MDP (HiP-MDP) framework, a family of reinforcement learning tasks is generated by varying hidden parameters specifying the dynamics and reward function for each individual task. HiP-MDP is a natural model for families of tasks in which meta- and lifelong-reinforcement learning approaches can succeed. Given a learned context encoder that infers the hidden parameters from previous experience, most existing algorithms fall into two categories: $\\textit{model transfer}$ and $\\textit{policy transfer}$, depending on which function the hidden parameters are used to parameterize. We characterize the robustness of model and policy transfer algorithms with respect to hidden parameter estimation error. We first show that the value function of HiP-MDPs is Lipschitz continuous under certain conditions. We then derive regret bounds for both settings through the lens of Lipschitz continuity. Finally, we empirically corroborate our theoretical analysis by experimentally varying the hyper-parameters governing the Lipschitz constants of two continuous control problems; the resulting performance is consistent with our predictions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DrtSx1z40Ib": {
    "title": "Composing Task Knowledge With Modular Successor Feature Approximators",
    "volume": "poster",
    "abstract": "Recently, the Successor Features and Generalized Policy Improvement (SF&GPI) framework has been proposed as a method for learning, composing and transferring predictive knowledge and behavior. SF&GPI works by having an agent learn predictive representations (SFs) that can be combined for transfer to new tasks with GPI. However, to be effective this approach requires state features that are useful to predict, and these state-features are typically hand-designed. In this work, we present a novel neural network architecture, \"Modular Successor Feature Approximators\" (MSFA), where modules both discover what is useful to predict, and learn their own predictive representations. We show that MSFA is able to better generalize compared to baseline architectures for learning SFs and a modular network that discovers factored state representations",
    "checked": true,
    "id": "b3cf06fd1ab90ad218df49d69f0a033d5bc0af0f",
    "semantic_title": "composing task knowledge with modular successor feature approximators",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=LIV7-_7pYPl": {
    "title": "DexDeform: Dexterous Deformable Object Manipulation with Human Demonstrations and Differentiable Physics",
    "volume": "poster",
    "abstract": "In this work, we aim to learn dexterous manipulation of deformable objects using multi-fingered hands. Reinforcement learning approaches for dexterous rigid object manipulation would struggle in this setting due to the complexity of physics interaction with deformable objects. At the same time, previous trajectory optimization approaches with differentiable physics for deformable manipulation would suffer from local optima caused by the explosion of contact modes from hand-object interactions. To address these challenges, we propose DexDeform, a principled framework that abstracts dexterous manipulation skills from human demonstration, and refines the learned skills with differentiable physics. Concretely, we first collect a small set of human demonstrations using teleoperation. And we then train a skill model using demonstrations for planning over action abstractions in imagination. To explore the goal space, we further apply augmentations to the existing deformable shapes in demonstrations and use a gradient optimizer to refine the actions planned by the skill model. Finally, we adopt the refined trajectories as new demonstrations for finetuning the skill model. To evaluate the effectiveness of our approach, we introduce a suite of six challenging dexterous deformable object manipulation tasks. Compared with baselines, DexDeform is able to better explore and generalize across novel goals unseen in the initial human demonstrations. Additional materials can be found at our project website: https://sites.google.com/view/dexdeform",
    "checked": true,
    "id": "26063b5f5efa71e1c3b726f6f8c792368ba43ce4",
    "semantic_title": "dexdeform: dexterous deformable object manipulation with human demonstrations and differentiable physics",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=QsCSLPP55Ku": {
    "title": "Effective passive membership inference attacks in federated learning against overparameterized models",
    "volume": "poster",
    "abstract": "This work considers the challenge of performing membership inference attacks in a federated learning setting ---for image classification--- where an adversary can only observe the communication between the central node and a single client (a passive white-box attack). Passive attacks are one of the hardest-to-detect attacks, since they can be performed without modifying how the behavior of the central server or its clients, and assumes *no access to private data instances*. The key insight of our method is empirically observing that, near parameters that generalize well in test, the gradient of large overparameterized neural network models statistically behave like high-dimensional independent isotropic random vectors. Using this insight, we devise two attacks that are often little impacted by existing and proposed defenses. Finally, we validated the hypothesis that our attack depends on the overparametrization by showing that increasing the level of overparametrization (without changing the neural network architecture) positively correlates with our attack effectiveness",
    "checked": true,
    "id": "94809c93b3ce812cef198e82f7f855b2dd070cee",
    "semantic_title": "effective passive membership inference attacks in federated learning against overparameterized models",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=9EAQVEINuum": {
    "title": "Optimizing Bi-Encoder for Named Entity Recognition via Contrastive Learning",
    "volume": "poster",
    "abstract": "We present a bi-encoder framework for named entity recognition (NER), which applies contrastive learning to map candidate text spans and entity types into the same vector representation space. Prior work predominantly approaches NER as sequence labeling or span classification. We instead frame NER as a representation learning problem that maximizes the similarity between the vector representations of an entity mention and its type. This makes it easy to handle nested and flat NER alike, and can better leverage noisy self-supervision signals. A major challenge to this bi-encoder formulation for NER lies in separating non-entity spans from entity mentions. Instead of explicitly labeling all non-entity spans as the same class $\\texttt{Outside}$ ($\\texttt{O}$) as in most prior methods, we introduce a novel dynamic thresholding loss, learned in conjunction with the standard contrastive loss. Experiments show that our method performs well in both supervised and distantly supervised settings, for nested and flat NER alike, establishing new state of the art across standard datasets in the general domain (e.g., ACE2004, ACE2005, CoNLL2003) and high-value verticals such as biomedicine (e.g., GENIA, NCBI, BC5CDR, JNLPBA). We release the code at https://github.com/microsoft/binder",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p_jIy5QFB7": {
    "title": "Taking a Step Back with KCal: Multi-Class Kernel-Based Calibration for Deep Neural Networks",
    "volume": "poster",
    "abstract": "Deep neural network (DNN) classifiers are often overconfident, producing miscalibrated class probabilities. In high-risk applications like healthcare, practitioners require fully calibrated probability predictions for decision-making. That is, conditioned on the prediction vector, every class' probability should be close to the predicted value. Most existing calibration methods either lack theoretical guarantees for producing calibrated outputs, reduce classification accuracy in the process, or only calibrate the predicted class. This paper proposes a new Kernel-based calibration method called KCal. Unlike existing calibration procedures, KCal does not operate directly on the logits or softmax outputs of the DNN. Instead, KCal learns a metric space on the penultimate-layer latent embedding and generates predictions using kernel density estimates on a calibration set. We first analyze KCal theoretically, showing that it enjoys a provable full calibration guarantee. Then, through extensive experiments across a variety of datasets, we show that KCal consistently outperforms baselines as measured by the calibration error and by proper scoring rules like the Brier Score",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TAVBJ4aHsWt": {
    "title": "SemPPL: Predicting Pseudo-Labels for Better Contrastive Representations",
    "volume": "poster",
    "abstract": "Learning from large amounts of unsupervised data and a small amount of supervision is an important open problem in computer vision. We propose a new semi-supervised learning method, Semantic Positives via Pseudo-Labels (SEMPPL), that combines labelled and unlabelled data to learn informative representations. Our method extends self-supervised contrastive learning—where representations are shaped by distinguishing whether two samples represent the same underlying datum (positives) or not (negatives)—with a novel approach to selecting positives. To enrich the set of positives, we leverage the few existing ground-truth labels to predict the missing ones through a k-nearest neighbors classifier by using the learned embeddings of the labelled data. We thus extend the set of positives with datapoints having the same pseudo-label and call these semantic positives. We jointly learn the representation and predict bootstrapped pseudo-labels. This creates a reinforcing cycle. Strong initial representations enable better pseudo-label predictions which then improve the selection of semantic positives and lead to even better representations. SEMPPL outperforms competing semi-supervised methods setting new state-of-the-art performance of 76% and 68.5% top-1accuracy when using a ResNet-50 and training on 10% and 1% of labels on ImageNet, respectively. Furthermore, when using selective kernels, SEMPPL significantly outperforms previous state-of-the-art achieving 72.3% and 78.3% top-1accuracy on ImageNet with 1% and 10% labels, respectively, which improves absolute +7.8% and +6.2% over previous work. SEMPPL also exhibits state-of-the-art performance over larger ResNet models as well as strong robustness, out-of-distribution and transfer performance. We release the checkpoints and the evaluation code at https://github.com/deepmind/semppl",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j1zQGmQQOX1": {
    "title": "Differentially Private Adaptive Optimization with Delayed Preconditioners",
    "volume": "poster",
    "abstract": "Privacy costs may negate the benefits of using adaptive optimizers in differentially private model training. Prior works typically address this issue by using auxiliary information (e.g., public data) to boost the effectiveness of adaptive optimization. In this work, we explore techniques to estimate and efficiently adapt to gradient geometry in private adaptive optimization without auxiliary data. Motivated by the observation that adaptive methods can tolerate stale preconditioners, we propose differentially private adaptive training with delayed preconditioners (DP^2), a simple method that constructs delayed but less noisy preconditioners to better realize the benefits of adaptivity. Theoretically, we provide convergence guarantees for our method for both convex and non-convex problems, and analyze trade-offs between delay and privacy noise reduction. Empirically, we explore DP^2 across several real-world datasets, demonstrating that it can improve convergence speed by as much as 4× relative to non-adaptive baselines and match the performance of state-of-the-art optimization methods that require auxiliary data",
    "checked": true,
    "id": "201146e9b26eb4a16a26808128765ba1c906ca48",
    "semantic_title": "differentially private adaptive optimization with delayed preconditioners",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=vOEXS39nOF": {
    "title": "Phenaki: Variable Length Video Generation from Open Domain Textual Descriptions",
    "volume": "poster",
    "abstract": "We present Phenaki, a model capable of realistic video synthesis given a sequence of textual prompts. Generating videos from text is particularly challenging due to the computational cost, limited quantities of high quality text-video data and variable length of videos. To address these issues, we introduce a new causal model for learning video representation which compresses the video to a small discrete tokens representation. This tokenizer is auto-regressive in time, which allows it to work with video representations of different length. To generate video tokens from text we are using a bidirectional masked transformer conditioned on pre-computed text tokens. The generated video tokens are subsequently de-tokenized to create the actual video. To address data issues, we demonstrate how joint training on a large corpus of image-text pairs as well as a smaller number of video-text examples can result in generalization beyond what is available in the video datasets. Compared to the previous video generation methods, Phenaki can generate arbitrary long videos conditioned on a sequence of prompts (i.e. time variable text or story) in open domain. To the best of our knowledge, this is the first time a paper studies generating videos from time variable prompts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5MkYIYCbva": {
    "title": "Long Range Language Modeling via Gated State Spaces",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=_geIwiOyUhZ": {
    "title": "Bayes-MIL: A New Probabilistic Perspective on Attention-based Multiple Instance Learning for Whole Slide Images",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sSt9fROSZRO": {
    "title": "Investigating Multi-task Pretraining and Generalization in Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PDG4-Y3aboN": {
    "title": "FIT: A Metric for Model Sensitivity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b0RuGUYo8pA": {
    "title": "Transfer Learning with Deep Tabular Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "96e22af70f9ca575ebfe648677aced03c6c8803d",
    "semantic_title": "transfer learning with deep tabular models",
    "citation_count": 68,
    "authors": []
  },
  "https://openreview.net/forum?id=_eTZBs-yedr": {
    "title": "CrAM: A Compression-Aware Minimizer",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1e5c62492b2fd7d7dc428d5d6a44c694d281b395",
    "semantic_title": "cram: a compression-aware minimizer",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=JVlyfHEEm0k": {
    "title": "Understanding Train-Validation Split in Meta-Learning with Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f1ed2af14839fdb7a83983442f3f20ab4816d159",
    "semantic_title": "understanding train-validation split in meta-learning with neural networks",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=h1o7Ry9Zctm": {
    "title": "Revisiting Robustness in Graph Machine Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=77lSWa-Tm3Z": {
    "title": "Variational Information Pursuit for Interpretable Predictions",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "13fba50e2c707b30e17defd93c040e837c29921e",
    "semantic_title": "variational information pursuit for interpretable predictions",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=T5nUQDrM4u": {
    "title": "Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5IND3TXJRb-": {
    "title": "Lossless Adaptation of Pretrained Vision Models For Robotic Manipulation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SoyOsp7i_l": {
    "title": "Logical Message Passing Networks with One-hop Inference on Atomic Formulas",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bAz2DBS35i": {
    "title": "Noise-Robust De-Duplication at Scale",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a70lGJ-rwy": {
    "title": "Few-shot Backdoor Attacks via Neural Tangent Kernels",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bc8d5957446b53f7798c209fb7ac6cfd45289bdf",
    "semantic_title": "few-shot backdoor attacks via neural tangent kernels",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=nAgdXgfmqj": {
    "title": "Hyperparameter Optimization through Neural Network Partitioning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "47975c425e03911ad2ac9e618bd9ae1b23e4c852",
    "semantic_title": "hyperparameter optimization through neural network partitioning",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=9ZpciCOunFb": {
    "title": "Symmetries, Flat Minima, and the Conserved Quantities of Gradient Flow",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d61999e6f9ecbca226f28649f2827ac437460ee6",
    "semantic_title": "symmetries, flat minima, and the conserved quantities of gradient flow",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=ooxDOe7ZtBe": {
    "title": "Summarization Programs: Interpretable Abstractive Summarization with Neural Modular Trees",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Lr8cOOtYbfL": {
    "title": "Planning with Large Language Models for Code Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a6rCdfABJXg": {
    "title": "Equivariance-aware Architectural Optimization of Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FbRY1XVfwK": {
    "title": "Accelerating Hamiltonian Monte Carlo via Chebyshev Integration Time",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q-neeWNVv1": {
    "title": "Order Matters: Agent-by-agent Policy Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "cbb494f682944c3c21d3772683742fc519a197e4",
    "semantic_title": "order matters: agent-by-agent policy optimization",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=ULnHxczCBaE": {
    "title": "On the Convergence of AdaGrad(Norm) on $\\mathbb{R}^d$: Beyond Convexity, Non-Asymptotic Rate and Acceleration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "ba29ac871948ddf64f85b6428530c005f7c4557b",
    "semantic_title": "on the convergence of adagrad(norm) on $\\r^{d}$: beyond convexity, non-asymptotic rate and acceleration",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=5mqFra2ZSuf": {
    "title": "SP2 : A Second Order Stochastic Polyak Method",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r8Mu7idxyF": {
    "title": "Making Better Decision by Directly Planning in Continuous Control",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ea6c9116edce14baeed7e36f72e6673d2929d6c4",
    "semantic_title": "making better decision by directly planning in continuous control",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=VuuDXDgujAc": {
    "title": "HiT-MDP: Learning the SMDP option framework on MDPs with Hidden Temporal Embeddings",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JLg5aHHv7j": {
    "title": "(Certified!!) Adversarial Robustness for Free!",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QIRtAqoXwj": {
    "title": "Heterogeneous Neuronal and Synaptic Dynamics for Spike-Efficient Unsupervised Learning: Theory and Design Principles",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "33f8433529e3f42a2892f9374a5a06936a6c835b",
    "semantic_title": "heterogeneous neuronal and synaptic dynamics for spike-efficient unsupervised learning: theory and design principles",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=sdQGxouELX": {
    "title": "MMVAE+: Enhancing the Generative Quality of Multimodal VAEs without Compromises",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "53608de216c4628528ff1b08517099a04694df17",
    "semantic_title": "mmvae+: enhancing the generative quality of multimodal vaes without compromises",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=T2Ncx_PN2K": {
    "title": "In-Situ Text-Only Adaptation of Speech Models with Low-Overhead Speech Imputations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=op-ceGueqc4": {
    "title": "Scaling Laws For Deep Learning Based Image Reconstruction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3oWo92cQyxL": {
    "title": "Meta Learning to Bridge Vision and Language Models for Multimodal Few-Shot Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "136d968598ab14715cec3393153355c3b535201e",
    "semantic_title": "meta learning to bridge vision and language models for multimodal few-shot learning",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=Xyme9p1rpZw": {
    "title": "SoftZoo: A Soft Robot Co-design Benchmark For Locomotion In Diverse Environments",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a24d419bd3ec59f16b9103a836b419e050497b38",
    "semantic_title": "softzoo: a soft robot co-design benchmark for locomotion in diverse environments",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=dCSFiAl_VO3": {
    "title": "Improved Learning-augmented Algorithms for k-means and k-medians Clustering",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c434a00107ad954f76846f7d4628e971f28f0357",
    "semantic_title": "improved learning-augmented algorithms for k-means and k-medians clustering",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=CMPIBjmhpo": {
    "title": "Neural Implicit Shape Editing using Boundary Sensitivity",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "732be0b93ca8b443ecc517832f6820b32bacd05e",
    "semantic_title": "neural implicit shape editing using boundary sensitivity",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=nXOhmfFu5n": {
    "title": "Amortised Invariance Learning for Contrastive Self-Supervision",
    "volume": "poster",
    "abstract": "Contrastive self-supervised learning methods famously produce high quality transferable representations by learning invariances to different data augmentations. Invariances established during pre-training can be interpreted as strong inductive biases. However these may or may not be helpful, depending on if they match the invariance requirements of downstream tasks or not. This has led to several attempts to learn task-specific invariances during pre-training, however, these methods are highly compute intensive and tedious to train. We introduce the notion of amortized invariance learning for contrastive self supervision. In the pre-training stage, we parameterize the feature extractor by differentiable invariance hyper-parameters that control the invariances encoded by the representation. Then, for any downstream task, both linear readout and task-specific invariance requirements can be efficiently and effectively learned by gradient-descent. We evaluate the notion of amortized invariances for contrastive learning over two different modalities: vision and audio, on two widely-used contrastive learning methods in vision: SimCLR and MoCo-v2 with popular architectures like ResNets and Vision Transformers, and SimCLR with ResNet-18 for audio. We show that our amortized features provide a reliable way to learn diverse downstream tasks with different invariance requirements, while using a single feature and avoiding task-specific pre-training. This provides an exciting perspective that opens up new horizons in the field of general purpose representation learning",
    "checked": true,
    "id": "b9362fc182f3161df3cad38737e54d54ebd3d4bf",
    "semantic_title": "amortised invariance learning for contrastive self-supervision",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=n-UHRIdPju": {
    "title": "Revisiting Populations in multi-agent Communication",
    "volume": "poster",
    "abstract": "Despite evidence from cognitive sciences that larger groups of speakers tend to develop more structured languages in human communication, scaling up to populations has failed to yield significant benefits in emergent multi-agent communication. In this paper we advocate for an alternate population-level training paradigm for referential games based on the idea of \"partitioning\" the agents into sender-receiver pairs and limiting co-adaptation across pairs. We show that this results in optimizing a different objective at the population level, where agents maximize (1) their respective \"internal\" communication accuracy and (2) some measure of alignment between agents. In experiments, we find that this leads to the emergence of languages that are significantly more compositional. Moreover, when agents are trained in populations that are not fully connected (ie. not all agent pairs interact at training time), this approach reduces multi-linguality and improves zero-shot communication with new agents (ie. agents are able to communicate successfully with other agents outside their training partners)",
    "checked": true,
    "id": "d2a1876392bc86982b5d206cddc01dc1d64b5fc0",
    "semantic_title": "revisiting populations in multi-agent communication",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=-lGvSmht7a": {
    "title": "Sequential Gradient Coding For Straggler Mitigation",
    "volume": "poster",
    "abstract": "In distributed computing, slower nodes (stragglers) usually become a bottleneck. Gradient Coding (GC), introduced by Tandon et al., is an efficient technique that uses principles of error-correcting codes to distribute gradient computation in the presence of stragglers. In this paper, we consider the distributed computation of a sequence of gradients $\\{g(1),g(2),\\ldots,g(J)\\}$, where processing of each gradient $g(t)$ starts in round-$t$ and finishes by round-$(t+T)$. Here $T\\geq 0$ denotes a delay parameter. For the GC scheme, coding is only across computing nodes and this results in a solution where $T=0$. On the other hand, having $T>0$ allows for designing schemes which exploit the temporal dimension as well. In this work, we propose two schemes that demonstrate improved performance compared to GC. Our first scheme combines GC with selective repetition of previously unfinished tasks and achieves improved straggler mitigation. In our second scheme, which constitutes our main contribution, we apply GC to a subset of the tasks and repetition for the remainder of the tasks. We then multiplex these two classes of tasks across workers and rounds in an adaptive manner, based on past straggler patterns. Using theoretical analysis, we demonstrate that our second scheme achieves significant reduction in the computational load. In our experiments, we study a practical setting of concurrently training multiple neural networks over an AWS Lambda cluster involving 256 worker nodes, where our framework naturally applies. We demonstrate that the latter scheme can yield a 16\\% improvement in runtime over the baseline GC scheme, in the presence of naturally occurring, non-simulated stragglers",
    "checked": true,
    "id": "f2f308c6e8847c1fe0544bb5dac0faedc36b38e3",
    "semantic_title": "sequential gradient coding for straggler mitigation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EQfeudmWLQ": {
    "title": "TTN: A Domain-Shift Aware Batch Normalization in Test-Time Adaptation",
    "volume": "poster",
    "abstract": "This paper proposes a novel batch normalization strategy for test-time adaptation. Recent test-time adaptation methods heavily rely on the modified batch normalization, i.e., transductive batch normalization (TBN), which calculates the mean and the variance from the current test batch rather than using the running mean and variance obtained from the source data, i.e., conventional batch normalization (CBN). Adopting TBN that employs test batch statistics mitigates the performance degradation caused by the domain shift. However, re-estimating normalization statistics using test data depends on impractical assumptions that a test batch should be large enough and be drawn from i.i.d. stream, and we observed that the previous methods with TBN show critical performance drop without the assumptions. In this paper, we identify that CBN and TBN are in a trade-off relationship and present a new test-time normalization (TTN) method that interpolates the statistics by adjusting the importance between CBN and TBN according to the domain-shift sensitivity of each BN layer. Our proposed TTN improves model robustness to shifted domains across a wide range of batch sizes and in various realistic evaluation scenarios. TTN is widely applicable to other test-time adaptation methods that rely on updating model parameters via backpropagation. We demonstrate that adopting TTN further improves their performance and achieves state-of-the-art performance in various standard benchmarks",
    "checked": true,
    "id": "f175795f1a93c62f3d42cf11bb24f70895839504",
    "semantic_title": "ttn: a domain-shift aware batch normalization in test-time adaptation",
    "citation_count": 100,
    "authors": []
  },
  "https://openreview.net/forum?id=OKcJhpQiGiX": {
    "title": "Disentanglement of Correlated Factors via Hausdorff Factorized Support",
    "volume": "poster",
    "abstract": "A grand goal in deep learning research is to learn representations capable of generalizing across distribution shifts. Disentanglement is one promising direction aimed at aligning a model's representation with the underlying factors generating the data (e.g. color or background). Existing disentanglement methods, however, rely on an often unrealistic assumption: that factors are statistically independent. In reality, factors (like object color and shape) are correlated. To address this limitation, we consider the use of a relaxed disentanglement criterion -- the Hausdorff Factorized Support (HFS) criterion -- that encourages only pairwise factorized support, rather than a factorial distribution, by minimizing a Hausdorff distance. This allows for arbitrary distributions of the factors over their support, including correlations between them. We show that the use of HFS consistently facilitates disentanglement and recovery of ground-truth factors across a variety of correlation settings and benchmarks, even under severe training correlations and correlation shifts, with in parts over +60% in relative improvement over existing disentanglement methods. In addition, we find that leveraging HFS for representation learning can even facilitate transfer to downstream tasks such as classification under distribution shifts. We hope our original approach and positive empirical results inspire further progress on the open problem of robust generalization. Code available at https://github.com/facebookresearch/disentangling-correlated-factors",
    "checked": true,
    "id": "1fb3198fbaa560a7e122c13a94a9344de79586be",
    "semantic_title": "disentanglement of correlated factors via hausdorff factorized support",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=hH36JeQZDaO": {
    "title": "Generating Sequences by Learning to Self-Correct",
    "volume": "poster",
    "abstract": "Sequence generation applications require satisfying semantic constraints, such as ensuring that programs are correct, using certain keywords, or avoiding undesirable content. Language models, whether fine-tuned or prompted with few-shot demonstrations, frequently violate these constraints, and lack a mechanism to iteratively revise their outputs. Moreover, some powerful language models are of extreme scale or inaccessible, making it inefficient, if not infeasible, to update their parameters for task-specific adaptation. We present Self-Correction, an approach that decouples an imperfect base generator (an off-the-shelf language model or supervised sequence-to-sequence model) from a separate corrector that learns to iteratively correct imperfect generations. To train the corrector, we propose an online training procedure that can use either scalar or natural language feedback on intermediate imperfect generations. We show that Self-Correction improves upon the base generator in three diverse generation tasks - mathematical program synthesis, lexically-constrained generation, and toxicity control - even when the corrector is much smaller than the base generator",
    "checked": true,
    "id": "538288d24bdad73d831dfed44b706958287ed318",
    "semantic_title": "generating sequences by learning to self-correct",
    "citation_count": 254,
    "authors": []
  },
  "https://openreview.net/forum?id=3mlITJRYYbs": {
    "title": "Interneurons accelerate learning dynamics in recurrent neural networks for statistical adaptation",
    "volume": "poster",
    "abstract": "Early sensory systems in the brain rapidly adapt to fluctuating input statistics, which requires recurrent communication between neurons. Mechanistically, such recurrent communication is often indirect and mediated by local interneurons. In this work, we explore the computational benefits of mediating recurrent communication via interneurons compared with direct recurrent connections. To this end, we consider two mathematically tractable recurrent neural networks that statistically whiten their inputs --- one with direct recurrent connections and the other with interneurons that mediate recurrent communication. By analyzing the corresponding continuous synaptic dynamics and numerically simulating the networks, we show that the network with interneurons is more robust to initialization than the network with direct recurrent connections in the sense that the convergence time for the synaptic dynamics in the network with interneurons (resp. direct recurrent connections) scales logarithmically (resp. linearly) with the spectrum of their initialization. Our results suggest that interneurons are computationally useful for rapid adaptation to changing input statistics. Interestingly, the network with interneurons is an overparameterized solution of the whitening objective for the network with direct recurrent connections, so our results can be viewed as a recurrent neural network analogue of the implicit acceleration phenomenon observed in overparameterized feedforward linear networks",
    "checked": true,
    "id": "d18387c070233a65c7ebfe7951f8f8db3eabbac5",
    "semantic_title": "interneurons accelerate learning dynamics in recurrent neural networks for statistical adaptation",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=6PIrhAx1j4i": {
    "title": "Understanding DDPM Latent Codes Through Optimal Transport",
    "volume": "poster",
    "abstract": "Diffusion models have recently outperformed alternative approaches to model the distribution of natural images. Such diffusion models allow for deterministic sampling via the probability flow ODE, giving rise to a latent space and an encoder map. While having important practical applications, such as the estimation of the likelihood, the theoretical properties of this map are not yet fully understood. In the present work, we partially address this question for the popular case of the VP-SDE (DDPM) approach. We show that, perhaps surprisingly, the DDPM encoder map coincides with the optimal transport map for common distributions; we support this claim by extensive numerical experiments using advanced tensor train solver for multidimensional Fokker-Planck equation. We provide additional theoretical evidence for the case of multivariate normal distributions",
    "checked": true,
    "id": "f638e576fddaa4f34fb7575046de83ad62269e6b",
    "semantic_title": "understanding ddpm latent codes through optimal transport",
    "citation_count": 56,
    "authors": []
  },
  "https://openreview.net/forum?id=moIlFZfj_1b": {
    "title": "Latent Neural ODEs with Sparse Bayesian Multiple Shooting",
    "volume": "poster",
    "abstract": "Training dynamic models, such as neural ODEs, on long trajectories is a hard problem that requires using various tricks, such as trajectory splitting, to make model training work in practice. These methods are often heuristics with poor theoretical justifications, and require iterative manual tuning. We propose a principled multiple shooting technique for neural ODEs that splits the trajectories into manageable short segments, which are optimized in parallel, while ensuring probabilistic control on continuity over consecutive segments. We derive variational inference for our shooting-based latent neural ODE models and propose amortized encodings of irregularly sampled trajectories with a transformer-based recognition network with temporal attention and relative positional encoding. We demonstrate efficient and stable training, and state-of-the-art performance on multiple large-scale benchmark datasets",
    "checked": true,
    "id": "e9ce0ca5f485e8ff4632693b76cdab3b64c34a84",
    "semantic_title": "latent neural odes with sparse bayesian multiple shooting",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=5cFfz6yMVPU": {
    "title": "$\\mathcal{O}$-GNN: incorporating ring priors into molecular modeling",
    "volume": "poster",
    "abstract": "Cyclic compounds that contain at least one ring play an important role in drug design. Despite the recent success of molecular modeling with graph neural networks (GNNs), few models explicitly take rings in compounds into consideration, consequently limiting the expressiveness of the models. In this work, we design a new variant of GNN, ring-enhanced GNN ($\\mathcal{O}$-GNN), that explicitly models rings in addition to atoms and bonds in compounds. In $\\mathcal{O}$-GNN, each ring is represented by a latent vector, which contributes to and is iteratively updated by atom and bond representations. Theoretical analysis shows that $\\mathcal{O}$-GNN is able to distinguish two isomorphic subgraphs lying on different rings using only one layer while conventional graph convolutional neural networks require multiple layers to distinguish, demonstrating that $\\mathcal{O}$-GNN is more expressive. Through experiments, $\\mathcal{O}$-GNN shows good performance on $\\bf{11}$ public datasets. In particular, it achieves state-of-the-art validation result on the PCQM4Mv1 benchmark (outperforming the previous KDDCup champion solution) and the drug-drug interaction prediction task on DrugBank. Furthermore, $\\mathcal{O}$-GNN outperforms strong baselines (without modeling rings) on the molecular property prediction and retrosynthesis prediction tasks",
    "checked": false,
    "id": "4cf145c765013f2369a782f9dc308b5a9c0f0f19",
    "semantic_title": "𝒪-gnn: incorporating ring priors into molecular modeling",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=CDlHZ78-Xzi": {
    "title": "MACTA: A Multi-agent Reinforcement Learning Approach for Cache Timing Attacks and Detection",
    "volume": "poster",
    "abstract": "Security vulnerabilities in computer systems raise serious concerns as computers process an unprecedented amount of private and sensitive data today. Cache timing attacks (CTA) pose an important practical threat as they can effectively breach many protection mechanisms in today's systems. However, the current detection techniques for cache timing attacks heavily rely on heuristics and expert knowledge, which can lead to brittleness and the inability to adapt to new attacks. To mitigate the CTA threat, we propose MACTA, a multi-agent reinforcement learning (MARL) approach that leverages population-based training to train both attackers and detectors. Following best practices, we develop a realistic simulated MARL environment, MA-AUTOCAT, which enables training and evaluation of cache-timing attackers and detectors. Our empirical results suggest that MACTA is an effective solution without any manual input from security experts. MACTA detectors can generalize to a heuristic attack not exposed in training with a 97.8% detection rate and reduce the attack bandwidth of adaptive attackers by 20% on average. In the meantime, MACTA attackers are qualitatively more effective than other attacks studied, and the average evasion rate of MACTA attackers against an unseen state-of-the-art detector can reach up to 99%. Furthermore, we found that agents equipped with a Transformer encoder can learn effective policies in situations when agents with multi-layer perceptron encoders do not in this environment, suggesting the potential of Transformer structures in CTA problems",
    "checked": true,
    "id": "321c8556d3c12a3eb505b578c86d9f66f97d9093",
    "semantic_title": "macta: a multi-agent reinforcement learning approach for cache timing attacks and detection",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=FVW7Mi2ph6C": {
    "title": "PAC Reinforcement Learning for Predictive State Representations",
    "volume": "poster",
    "abstract": "In this paper we study online Reinforcement Learning (RL) in partially observable dynamical systems. We focus on the Predictive State Representations (PSRs) model, which is an expressive model that captures other well-known models such as Partially Observable Markov Decision Processes (POMDP). PSR represents the states using a set of predictions of future observations and is defined entirely using observable quantities. We develop a novel model-based algorithm for PSRs that can learn a near optimal policy in sample complexity scaling polynomially with respect to all the relevant parameters of the systems. Our algorithm naturally works with function approximation to extend to systems with potentially large state and observation spaces. We show that given a realizable model class, the sample complexity of learning the near optimal policy only scales polynomially with respect to the statistical complexity of the model class, without any explicit polynomial dependence on the size of the state and observation spaces. Notably, our work is the first work that shows polynomial sample complexities to compete with the globally optimal policy in PSRs. Finally, we demonstrate how our general theorem can be directly used to derive sample complexity bounds for special models including $m$-step weakly revealing and $m$-step decodable tabular POMDPs, POMDPs with low-rank latent transition, and POMDPs with linear emission and latent transition",
    "checked": true,
    "id": "c9ffc6b13a3b9a3af9244b744b3a899a5ad1e96b",
    "semantic_title": "pac reinforcement learning for predictive state representations",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=bn0GZZdDfI1": {
    "title": "Decentralized Optimistic Hyperpolicy Mirror Descent: Provably No-Regret Learning in Markov Games",
    "volume": "poster",
    "abstract": "We study decentralized policy learning in Markov games where we control a single agent to play with nonstationary and possibly adversarial opponents. Our goal is to develop a no-regret online learning algorithm that (i) takes actions based on the local information observed by the agent and (ii) is able to find the best policy in hindsight. For such a problem, the nonstationary state transitions due to the varying opponent pose a significant challenge. In light of a recent hardness result (Liu et al., 2022), we focus on the setting where the opponent's previous policies are revealed to the agent for decision making. With such an information structure, we propose a new algorithm, Decentralized Optimistic hypeRpolicy mIrror deScent (DORIS), which achieves $\\sqrt{K}$-regret in the context of general function approximation, where $K$ is the number of episodes. Moreover, when all the agents adopt DORIS, we prove that their mixture policy constitutes an approximate coarse correlated equilibrium. In particular, DORIS maintains a hyperpolicy which is a distribution over the policy space. The hyperpolicy is updated via mirror descent, where the update direction is obtained by an optimistic variant of least-squares policy evaluation. Furthermore, to illustrate the power of our method, we apply DORIS to constrained and vector-valued MDPs, which can be formulated as zero-sum Markov games with a fictitious opponent",
    "checked": true,
    "id": "8fd7aae7826feb4733f2518cd6f8cc448be26b75",
    "semantic_title": "decentralized optimistic hyperpolicy mirror descent: provably no-regret learning in markov games",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=ZBUthI6wK9h": {
    "title": "Robust Scheduling with GFlowNets",
    "volume": "poster",
    "abstract": "Finding the best way to schedule operations in a computation graph is a classical NP-hard problem which is central to compiler optimization. However, evaluating the goodness of a schedule on the target hardware can be very time-consuming. Traditional approaches as well as previous machine learning ones typically optimize proxy metrics, which are fast to evaluate but can lead to bad schedules when tested on the target hardware. In this work, we propose a new approach to scheduling by sampling proportionally to the proxy metric using a novel GFlowNet method. We introduce a technique to control the trade-off between diversity and goodness of the proposed schedules at inference time and demonstrate empirically that the pure optimization baselines can lead to subpar performance with respect to our approach when tested on a target model. Furthermore, we show that conditioning the GFlowNet on the computation graph enables generalization to unseen scheduling problems for both synthetic and real-world compiler datasets",
    "checked": true,
    "id": "035b158f1f165930aaf7877fb54515ce29a17f9e",
    "semantic_title": "robust scheduling with gflownets",
    "citation_count": 59,
    "authors": []
  },
  "https://openreview.net/forum?id=OAsXFPBfTBh": {
    "title": "Autoregressive Conditional Neural Processes",
    "volume": "poster",
    "abstract": "Conditional neural processes (CNPs; Garnelo et al., 2018a) are attractive meta-learning models which produce well-calibrated predictions and are trainable via a simple maximum likelihood procedure. Although CNPs have many advantages, they are unable to model dependencies in their predictions. Various works propose solutions to this, but these come at the cost of either requiring approximate inference or being limited to Gaussian predictions. In this work, we instead propose to change how CNPs are deployed at test time, without any modifications to the model or training procedure. Instead of making predictions independently for every target point, we autoregressively define a joint predictive distribution using the chain rule of probability, taking inspiration from the neural autoregressive density estimator (NADE) literature. We show that this simple procedure allows factorised Gaussian CNPs to model highly dependent, non-Gaussian predictive distributions. Perhaps surprisingly, in an extensive range of tasks with synthetic and real data, we show that CNPs in autoregressive (AR) mode not only significantly outperform non-AR CNPs, but are also competitive with more sophisticated models that are significantly more computationally expensive and challenging to train. This performance is remarkable given that AR CNPs are not trained to model joint dependencies. Our work provides an example of how ideas from neural distribution estimation can benefit neural processes, and motivates research into the AR deployment of other neural process models",
    "checked": true,
    "id": "9d4521ed386eb5ebb657040e2ac89795642abd5b",
    "semantic_title": "autoregressive conditional neural processes",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=fe2S7736sNS": {
    "title": "$k$NN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference",
    "volume": "poster",
    "abstract": "In-Context Learning (ICL), which formulates target tasks as prompt completion conditioned on in-context demonstrations, has become the prevailing utilization of LLMs. In this paper, we first disclose an actual predicament for this typical usage that it can not scale up with training data due to context length restriction. Besides, existing works have shown that ICL also suffers from various biases and requires delicate calibration treatment. To address both challenges, we advocate a simple and effective solution, $k$NN Prompting, which first queries LLM with training data for distributed representations, then predicts test instances by simply referring to nearest neighbors. We conduct comprehensive experiments to demonstrate its two-fold superiority: 1) Calibration-Free: $k$NN Prompting does not directly align LLM output distribution with task-specific label space, instead leverages such distribution to align test and training instances. It significantly outperforms state-of-the-art calibration-based methods under comparable few-shot scenario. 2) Beyond-Context: $k$NN Prompting can further scale up effectively with as many training data as are available, continually bringing substantial improvements. The scaling trend holds across 10 orders of magnitude ranging from 2 shots to 1024 shots as well as different LLMs scales ranging from 0.8B to 30B. It successfully bridges data scaling into model scaling, and brings new potentials for the gradient-free paradigm of LLM deployment. Code is publicly available at https://github.com/BenfengXu/KNNPrompting",
    "checked": false,
    "id": "3ec5f0da304a606c5989de5b00e1246ee64b3e46",
    "semantic_title": "knn prompting: beyond-context learning with calibration-free nearest neighbor inference",
    "citation_count": 56,
    "authors": []
  },
  "https://openreview.net/forum?id=zVrw4OH1Lch": {
    "title": "FIFA: Making Fairness More Generalizable in Classifiers Trained on Imbalanced Data",
    "volume": "poster",
    "abstract": "Algorithmic fairness plays an important role in machine learning and imposing fairness constraints during learning is a common approach. However, many datasets are imbalanced in certain label classes (e.g. \"healthy\") and sensitive subgroups (e.g. \"older patients\"). Empirically, this imbalance leads to a lack of generalizability not only of classification but also of fairness properties, especially in over-parameterized models. For example, fairness-aware training may ensure equalized odds (EO) on the training data, but EO is far from being satisfied on new users. In this paper, we propose a theoretically-principled, yet {\\bf F}lexible approach that is {\\bf I}mbalance-{\\bf F}airness-{\\bf A}ware ({\\bf FIFA}). Specifically, FIFA encourages both classification and fairness generalization and can be flexibly combined with many existing fair learning methods with logits-based losses. While our main focus is on EO, FIFA can be directly applied to achieve equalized opportunity (EqOpt); and under certain conditions, it can also be applied to other fairness notions. We demonstrate the power of FIFA by combining it with a popular fair classification algorithm, and the resulting algorithm achieves significantly better fairness generalization on several real-world datasets",
    "checked": true,
    "id": "1abde3cba5355070bd41c51314e9ca4b41079dcf",
    "semantic_title": "fifa: making fairness more generalizable in classifiers trained on imbalanced data",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=7Cb7Faxa1OB": {
    "title": "Understanding The Robustness of Self-supervised Learning Through Topic Modeling",
    "volume": "poster",
    "abstract": "Self-supervised learning has significantly improved the performance of many NLP tasks. However, how can self-supervised learning discover useful features, and why is it better than traditional approaches such as probabilistic models are still largely unknown. In this paper, we focus on the context of topic modeling and highlight a key advantage of self-supervised learning - when applied to data generated by topic models, self-supervised learning can be oblivious to the specific model, and hence is less susceptible to model misspecification. In particular, we prove that commonly used self-supervised objectives based on reconstruction or contrastive samples can both recover useful posterior information for general topic models. Empirically, we show that the same objectives can perform on par with posterior inference using the correct model, while outperforming posterior inference using misspecified models",
    "checked": true,
    "id": "fb57a29cce4c6e4d6971ccc2f928289703566145",
    "semantic_title": "understanding the robustness of self-supervised learning through topic modeling",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=sPgP6aISLTD": {
    "title": "Temporal Disentanglement of Representations for Improved Generalisation in Reinforcement Learning",
    "volume": "poster",
    "abstract": "Reinforcement Learning (RL) agents are often unable to generalise well to environment variations in the state space that were not observed during training. This issue is especially problematic for image-based RL, where a change in just one variable, such as the background colour, can change many pixels in the image. The changed pixels can lead to drastic changes in the agent's latent representation of the image, causing the learned policy to fail. To learn more robust representations, we introduce TEmporal Disentanglement (TED), a self-supervised auxiliary task that leads to disentangled image representations exploiting the sequential nature of RL observations. We find empirically that RL algorithms utilising TED as an auxiliary task adapt more quickly to changes in environment variables with continued training compared to state-of-the-art representation learning methods. Since TED enforces a disentangled structure of the representation, our experiments also show that policies trained with TED generalise better to unseen values of variables irrelevant to the task (e.g. background colour) as well as unseen values of variables that affect the optimal policy (e.g. goal positions)",
    "checked": true,
    "id": "851cfaff3dd0421a9bc2775b135fe3007910eef0",
    "semantic_title": "temporal disentanglement of representations for improved generalisation in reinforcement learning",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=oze0clVGPeX": {
    "title": "Exploring the Limits of Differentially Private Deep Learning with Group-wise Clipping",
    "volume": "poster",
    "abstract": "Differentially private deep learning has recently witnessed advances in computational efficiency and privacy-utility trade-off. We explore whether further improvements along the two axes are possible and provide affirmative answers leveraging two instantiations of \\emph{group-wise clipping}. To reduce the compute time overhead of private learning, we show that \\emph{per-layer clipping}, where the gradient of each neural network layer is clipped separately, allows clipping to be performed in conjunction with backpropagation in differentially private optimization. This results in private learning that is as memory-efficient and almost as fast per training update as non-private learning for many workflows of interest. While per-layer clipping with constant thresholds tends to underperform standard flat clipping, per-layer clipping with adaptive thresholds matches or outperforms flat clipping under given training epoch constraints, hence attaining similar or better task performance within less wall time. To explore the limits of scaling (pretrained) models in differentially private deep learning, we privately fine-tune the 175 billion-parameter GPT-3. We bypass scaling challenges associated with clipping gradients that are distributed across multiple devices with \\emph{per-device clipping} that clips the gradient of each model piece separately on its host device. Privately fine-tuning GPT-3 with per-device clipping achieves a task performance at $\\epsilon=1$ better than what is attainable by non-privately fine-tuning the largest GPT-2 on a summarization task",
    "checked": true,
    "id": "0ba0091c60c0346493b9ffb46ac682eee5453a53",
    "semantic_title": "exploring the limits of differentially private deep learning with group-wise clipping",
    "citation_count": 53,
    "authors": []
  },
  "https://openreview.net/forum?id=7i6OZa7oij": {
    "title": "Strong inductive biases provably prevent harmless interpolation",
    "volume": "poster",
    "abstract": "Classical wisdom suggests that estimators should avoid fitting noise to achieve good generalization. In contrast, modern overparameterized models can yield small test error despite interpolating noise — a phenomenon often called \"benign overfitting\" or \"harmless interpolation\". This paper argues that the degree to which interpolation is harmless hinges upon the strength of an estimator's inductive bias, i.e., how heavily the estimator favors solutions with a certain structure: while strong inductive biases prevent harmless interpolation, weak inductive biases can even require fitting noise to generalize well. Our main theoretical result establishes tight non-asymptotic bounds for high-dimensional kernel regression that reflect this phenomenon for convolutional kernels, where the filter size regulates the strength of the inductive bias. We further provide empirical evidence of the same behavior for deep neural networks with varying filter sizes and rotational invariance",
    "checked": true,
    "id": "ee8cb5f62eaff0b766fb41d3821ee28786b46a08",
    "semantic_title": "strong inductive biases provably prevent harmless interpolation",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=b9tUk-f_aG": {
    "title": "Bridging the Gap to Real-World Object-Centric Learning",
    "volume": "poster",
    "abstract": "Humans naturally decompose their environment into entities at the appropriate level of abstraction to act in the world. Allowing machine learning algorithms to derive this decomposition in an unsupervised way has become an important line of research. However, current methods are restricted to simulated data or require additional information in the form of motion or depth in order to successfully discover objects. In this work, we overcome this limitation by showing that reconstructing features from models trained in a self-supervised manner is a sufficient training signal for object-centric representations to arise in a fully unsupervised way. Our approach, DINOSAUR, significantly out-performs existing object-centric learning models on simulated data and is the first unsupervised object-centric model that scales to real world-datasets such as COCO and PASCAL VOC. DINOSAUR is conceptually simple and shows competitive performance compared to more involved pipelines from the computer vision literature",
    "checked": true,
    "id": "6174d6498562f066526c7371edf8f548918d988c",
    "semantic_title": "bridging the gap to real-world object-centric learning",
    "citation_count": 169,
    "authors": []
  },
  "https://openreview.net/forum?id=cIbjyd2Vcy": {
    "title": "Towards a Unified Theoretical Understanding of Non-contrastive Learning via Rank Differential Mechanism",
    "volume": "poster",
    "abstract": "Recently, a variety of methods under the name of non-contrastive learning (like BYOL, SimSiam, SwAV, DINO) show that when equipped with some asymmetric architectural designs, aligning positive pairs alone is sufficient to attain good performance in self-supervised visual learning. Despite some understandings of some specific modules (like the predictor in BYOL), there is yet no unified theoretical understanding of how these seemingly different asymmetric designs can all avoid feature collapse, particularly considering methods that also work without the predictor (like DINO). In this work, we propose a unified theoretical understanding for existing variants of non-contrastive learning. Our theory named Rank Differential Mechanism (RDM) shows that all these asymmetric designs create a consistent rank difference in their dual-branch output features. This rank difference will provably lead to an improvement of effective dimensionality and alleviate either complete or dimensional feature collapse. Different from previous theories, our RDM theory is applicable to different asymmetric designs (with and without the predictor), and thus can serve as a unified understanding of existing non-contrastive learning methods. Besides, our RDM theory also provides practical guidelines for designing many new non-contrastive variants. We show that these variants indeed achieve comparable performance to existing methods on benchmark datasets, and some of them even outperform the baselines. Our code is available at \\url{https://github.com/PKU-ML/Rank-Differential-Mechanism}",
    "checked": true,
    "id": "40ff56355f530bd72010d482ae4f71260136df2d",
    "semantic_title": "towards a unified theoretical understanding of non-contrastive learning via rank differential mechanism",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=CtS2Rs_aYk": {
    "title": "Stay Moral and Explore: Learn to Behave Morally in Text-based Games",
    "volume": "poster",
    "abstract": "Reinforcement learning (RL) in text-based games has developed rapidly and achieved promising results. However, little effort has been expended to design agents that pursue objectives while behaving morally, which is a critical issue in the field of autonomous agents. In this paper, we propose a general framework named Moral Awareness Adaptive Learning (MorAL) that enhances the morality capacity of an agent using a plugin moral-aware learning model. The framework allows the agent to execute task learning and morality learning adaptively. The agent selects trajectories from past experiences during task learning. Meanwhile, the trajectories are used to conduct self-imitation learning with a moral-enhanced objective. In order to achieve the trade-off between morality and task progress, the agent uses the combination of task policy and moral policy for action selection. We evaluate on the Jiminy Cricket benchmark, a set of text-based games with various scenes and dense morality annotations. Our experiments demonstrate that, compared with strong contemporary value alignment approaches, the proposed framework improves task performance while reducing immoral behaviours in various games",
    "checked": true,
    "id": "dfbe976f9b445140fb59fed37c46d1af78d344fc",
    "semantic_title": "stay moral and explore: learn to behave morally in text-based games",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=9kBCMNb5mc": {
    "title": "Optimistic Exploration with Learned Features Provably Solves Markov Decision Processes with Neural Dynamics",
    "volume": "poster",
    "abstract": "Incorporated with the recent advances in deep learning, deep reinforcement learning (DRL) has achieved tremendous success in empirical study. However, analyzing DRL is still challenging due to the complexity of the neural network class. In this paper, we address such a challenge by analyzing the Markov decision process (MDP) with neural dynamics, which covers several existing models as special cases, including the kernelized nonlinear regulator (KNR) model and the linear MDP. We propose a novel algorithm that designs exploration incentives via learnable representations of the dynamics model by embedding the neural dynamics into a kernel space induced by the system noise. We further establish an upper bound on the sample complexity of the algorithm, which demonstrates the sample efficiency of the algorithm. We highlight that, unlike previous analyses of RL algorithms with function approximation, our bound on the sample complexity does not depend on the Eluder dimension of the neural network class, which is known to be exponentially large (Dong et al., 2021)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hp_RwhKDJ5": {
    "title": "Learning to Induce Causal Structure",
    "volume": "poster",
    "abstract": "The fundamental challenge in causal induction is to infer the underlying graph structure given observational and/or interventional data. Most existing causal induction algorithms operate by generating candidate graphs and evaluating them using either score-based methods (including continuous optimization) or independence tests. In our work, we instead treat the inference process as a black box and design a neural network architecture that learns the mapping from both observational and interventional data to graph structures via supervised training on synthetic graphs. The learned model generalizes to new synthetic graphs, is robust to train-test distribution shifts, and achieves state-of-the-art performance on naturalistic graphs for low sample complexity",
    "checked": true,
    "id": "b73266c0b61484107d1771491256adbae7d7bb58",
    "semantic_title": "learning to induce causal structure",
    "citation_count": 54,
    "authors": []
  },
  "https://openreview.net/forum?id=AHvFDPi-FA": {
    "title": "Diffusion Policies as an Expressive Policy Class for Offline Reinforcement Learning",
    "volume": "poster",
    "abstract": "Offline reinforcement learning (RL), which aims to learn an optimal policy using a previously collected static dataset, is an important paradigm of RL. Standard RL methods often perform poorly in this regime due to the function approximation errors on out-of-distribution actions. While a variety of regularization methods have been proposed to mitigate this issue, they are often constrained by policy classes with limited expressiveness that can lead to highly suboptimal solutions. In this paper, we propose representing the policy as a diffusion model, a recent class of highly-expressive deep generative models. We introduce Diffusion Q-learning (Diffusion-QL) that utilizes a conditional diffusion model to represent the policy. In our approach, we learn an action-value function and we add a term maximizing action-values into the training loss of the conditional diffusion model, which results in a loss that seeks optimal actions that are near the behavior policy. We show the expressiveness of the diffusion model-based policy, and the coupling of the behavior cloning and policy improvement under the diffusion model both contribute to the outstanding performance of Diffusion-QL. We illustrate the superiority of our method compared to prior works in a simple 2D bandit example with a multimodal behavior policy. We then show that our method can achieve state-of-the-art performance on the majority of the D4RL benchmark tasks",
    "checked": true,
    "id": "2cbea7615ebecea2c414d8fbad47d5d258a5c3b4",
    "semantic_title": "diffusion policies as an expressive policy class for offline reinforcement learning",
    "citation_count": 413,
    "authors": []
  },
  "https://openreview.net/forum?id=QTXKTXJKIh": {
    "title": "Achieving Near-Optimal Individual Regret & Low Communications in Multi-Agent Bandits",
    "volume": "poster",
    "abstract": "Cooperative multi-agent multi-armed bandits (CM2AB) study how distributed agents cooperatively play the same multi-armed bandit game. Most existing CM2AB works focused on maximizing the group performance of all agents---the accumulation of all agents' individual performance (i.e., individual reward). However, in many applications, the performance of the system is more sensitive to the ``bad'' agent---the agent with the worst individual performance. For example, in a drone swarm, a ``bad'' agent may crash into other drones and severely degrade the system performance. In that case, the key of the learning algorithm design is to coordinate computational and communicational resources among agents so to optimize the individual learning performance of the ``bad'' agent. In CM2AB, maximizing the group performance is equivalent to minimizing the group regret of all agents, and minimizing the individual performance can be measured by minimizing the maximum (worst) individual regret among agents. Minimizing the maximum individual regret was largely ignored in prior literature, and currently, there is little work on how to minimize this objective with a low communication overhead. In this paper, we propose a near-optimal algorithm on both individual and group regrets, in addition, we also propose a novel communication module in the algorithm, which only needs \\(O(\\log (\\log T))\\) communication times where \\(T\\) is the number of decision rounds. We also conduct simulations to illustrate the advantage of our algorithm by comparing it to other known baselines",
    "checked": true,
    "id": "261bd37a168e69b14c6bc957d3561e5a90f4270b",
    "semantic_title": "achieving near-optimal individual regret & low communications in multi-agent bandits",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=qco4ekz2Epm": {
    "title": "Online Boundary-Free Continual Learning by Scheduled Data Prior",
    "volume": "poster",
    "abstract": "Typical continual learning setup assumes that the dataset is split into multiple discrete tasks. We argue that it is less realistic as the streamed data would have no notion of task boundary in real-world data. Here, we take a step forward to investigate more realistic online continual learning – learning continuously changing data distribution without explicit task boundary, which we call boundary-free setup. As there is no clear boundary of tasks, it is not obvious when and what information in the past to be preserved as a better remedy for the stability-plasticity dilemma. To this end, we propose a scheduled transfer of previously learned knowledge. We further propose a data-driven balancing between the knowledge in the past and the present in learning objective. Moreover, since it is not straight-forward to use the previously proposed forgetting measure without task boundaries, we further propose a novel forgetting measure based on information theory that can capture forgetting. We empirically evaluate our method on a Gaussian data stream, its periodic extension, which assumes periodic data distribution frequently observed in real-life data, as well as the conventional disjoint task-split. Our method outperforms prior arts by large margins in various setups, using four popular benchmark datasets – CIFAR-10, CIFAR-100, TinyImageNet and ImageNet",
    "checked": true,
    "id": "3008cee71743197a6b68e8b76c6b512b61a6832d",
    "semantic_title": "online boundary-free continual learning by scheduled data prior",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=kUf4BcWXGJr": {
    "title": "HypeR: Multitask Hyper-Prompted Training Enables Large-Scale Retrieval Generalization",
    "volume": "poster",
    "abstract": "Recently, large-scale text retrieval has made impressive progress, facilitating both information retrieval and downstream knowledge-intensive tasks (e.g., open-domain QA and dialogue). With a moderate amount of data, a neural text retriever can outperform traditional methods such as BM25 by a large step. However, while being applied to out-of-domain data, the performance of a neural retriever degrades considerably. Therefore, how to enable a retriever to perform more robustly across different domains or tasks and even show strong zero-shot transfer ability is critical for building scalable IR systems. To this end, we propose HypeR, a hyper-prompted training mechanism to enable uniform retrieval across tasks of different domains. Specifically, our approach jointly trains the query encoder with a shared prompt-based parameter pool and a prompt synthesizer that dynamically composes hyper-prompt for encoding each query from different tasks or domains. Besides, to avoid the mode collapse of prompt attention distribution for different queries, we design a contrastive prompt regularization that promotes the mode of prompt attention to be aligned and uniform. Through multi-task hyper-prompted training, our retriever can master the ability to dynamically represent different types of queries and transfer knowledge across different domains and tasks. Extensive experiments show our model attains better retrieval performance across different tasks and better zero-shot transfer ability compared with various previous methods",
    "checked": true,
    "id": "6c79dc9d5ea8ef204e0543fd69a5e9eb80bbf612",
    "semantic_title": "hyper: multitask hyper-prompted training enables large-scale retrieval generalization",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=HjOo2k8lhFl": {
    "title": "Learning Rationalizable Equilibria in Multiplayer Games",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4637a1fd931b49a8119adc1c97a9d116474e5f28",
    "semantic_title": "learning rationalizable equilibria in multiplayer games",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=3dnrKbeVatv": {
    "title": "Energy-Based Test Sample Adaptation for Domain Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "af47a562da0a4d0a120a92fb7e0d48a58b09bd08",
    "semantic_title": "energy-based test sample adaptation for domain generalization",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=wCFB37bzud4": {
    "title": "Bidirectional Language Models Are Also Few-shot Learners",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b65b7f480a61d3dd31d8117b349cabc87c8ccf6c",
    "semantic_title": "bidirectional language models are also few-shot learners",
    "citation_count": 50,
    "authors": []
  },
  "https://openreview.net/forum?id=ytZIYmztET": {
    "title": "EPISODE: Episodic Gradient Clipping with Periodic Resampled Corrections for Federated Learning with Heterogeneous Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i8L9qoeZOS": {
    "title": "A Theory of Dynamic Benchmarks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "aa9e11e4f5b6f4cc7b7e6a4aa4c21ed19528ccb2",
    "semantic_title": "a theory of dynamic benchmarks",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=HWt4BBZjVW": {
    "title": "On the Trade-Off between Actionable Explanations and the Right to be Forgotten",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NeDc-Ak-H_": {
    "title": "Learning What and Where: Disentangling Location and Identity Tracking Without Supervision",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "173920346bde5deabdbe4c0e5950c8f5e5a4c20d",
    "semantic_title": "learning what and where: disentangling location and identity tracking without supervision",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=CN223OXgyb5": {
    "title": "BALTO: fast tensor program optimization with diversity-based active learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "464c72fe8a7873c909213a8a49ad9275ac84906d",
    "semantic_title": "balto: fast tensor program optimization with diversity-based active learning",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=gwcQajoXNF": {
    "title": "Computing all Optimal Partial Transports",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9OmCr1q54Z": {
    "title": "AE-FLOW: Autoencoders with Normalizing Flows for Medical Images Anomaly Detection",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "711dfa005ed3b3bd04751aec1b17c0169b342ff4",
    "semantic_title": "ae-flow: autoencoders with normalizing flows for medical images anomaly detection",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=xveTeHVlF7j": {
    "title": "A Self-Attention Ansatz for Ab-initio Quantum Chemistry",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sC-PmTsiTB": {
    "title": "Probabilistically Robust Recourse: Navigating the Trade-offs between Costs and Robustness in Algorithmic Recourse",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LiXDW7CF94J": {
    "title": "How robust is unsupervised representation learning to distribution shift?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eXkhH12DTD9": {
    "title": "Pseudo-label Training and Model Inertia in Neural Machine Translation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OAw6V3ZAhSd": {
    "title": "HyperDeepONet: learning operator with complex target function space using the limited resources via hypernetwork",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bf6750748589febbc04319f7e2471b45edbb6198",
    "semantic_title": "hyperdeeponet: learning operator with complex target function space using the limited resources via hypernetwork",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=qcJmsP3oE9": {
    "title": "Edge Guided GANs with Contrastive Learning for Semantic Image Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Kf7Yyf4O0u": {
    "title": "CANIFE: Crafting Canaries for Empirical Privacy Measurement in Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "37fc0cb77dfaf6cced0082f498912e19dc5666b5",
    "semantic_title": "canife: crafting canaries for empirical privacy measurement in federated learning",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=mMaInr0r0c": {
    "title": "A View From Somewhere: Human-Centric Face Representations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "20987b31af44ef816496bee3eb1ca54549c0a6c6",
    "semantic_title": "a view from somewhere: human-centric face representations",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=U_2kuqoTcB": {
    "title": "Identifiability Results for Multimodal Contrastive Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5be07671e7b1baeac5ce4ea7d3ee2bd0364aa701",
    "semantic_title": "identifiability results for multimodal contrastive learning",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=dZrQR7OR11": {
    "title": "Federated Learning as Variational Inference: A Scalable Expectation Propagation Approach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JLR_B7n_Wqr": {
    "title": "Latent Graph Inference using Product Manifolds",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e5c6d0f131945cf0ee1a2e8e1b4e4f91e5ed4d05",
    "semantic_title": "latent graph inference using product manifolds",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=lh-HRYxuoRr": {
    "title": "This Looks Like It Rather Than That: ProtoKNN For Similarity-Based Classifiers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uBKBoix9NXa": {
    "title": "Understanding weight-magnitude hyperparameters in training binary networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6d0c8934f62d605d7345de39759cd2b1df393e4d",
    "semantic_title": "understanding weight-magnitude hyperparameters in training binary networks",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pv1GPQzRrC8": {
    "title": "Imitating Human Behaviour with Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b43330013a5abcccd366d71f2f66c493c790abc6",
    "semantic_title": "imitating human behaviour with diffusion models",
    "citation_count": 229,
    "authors": []
  },
  "https://openreview.net/forum?id=6iVJOtr2zL2": {
    "title": "Contrastive Meta-Learning for Partially Observable Few-Shot Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "21419f1f1d6d5332308786b9c5fef1dcb81d271c",
    "semantic_title": "contrastive meta-learning for partially observable few-shot learning",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=ATLEl_izD87": {
    "title": "Enhancing the Inductive Biases of Graph Neural ODE for Modeling Physical Systems",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "aeb80ca92243aa1862f81caa58e8505c0c68255a",
    "semantic_title": "enhancing the inductive biases of graph neural ode for modeling physical systems",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=cA77NrVEuqn": {
    "title": "Efficient Planning in a Compact Latent Action Space",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "748c9aa5a31f279fa07b84238aa5ba748e9df40d",
    "semantic_title": "efficient planning in a compact latent action space",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=8JsaP7j1cL0": {
    "title": "Correlative Information Maximization Based Biologically Plausible Neural Networks for Correlated Source Separation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9c091005eedd74d73839b912b5266c4f499f21be",
    "semantic_title": "correlative information maximization based biologically plausible neural networks for correlated source separation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=9Nj_gNdvqYf": {
    "title": "Leveraging Importance Weights in Subset Selection",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2ecff5518f04f768b5450cb02d30783eec9d846d",
    "semantic_title": "leveraging importance weights in subset selection",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=CROlOA9Nd8C": {
    "title": "Copy is All You Need",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8b25d0065d30ed3c9e6a6cae94de53ef132d656d",
    "semantic_title": "copy is all you need",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=-CA8yFkPc7O": {
    "title": "Why adversarial training can hurt robust accuracy",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1dd1795f6fa368b61c78c3afccf194bbcf25ed3a",
    "semantic_title": "why adversarial training can hurt robust accuracy",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=xjb563TH-GH": {
    "title": "Representational Dissimilarity Metric Spaces for Stochastic Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6d2b13928cf9db71a0c245bbbdbdb7a07bd9247d",
    "semantic_title": "representational dissimilarity metric spaces for stochastic neural networks",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=dMMPUvNSYJr": {
    "title": "Sequential Learning of Neural Networks for Prequential MDL",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "39e4e6a4672c57495cb88b9c36137f397db11612",
    "semantic_title": "sequential learning of neural networks for prequential mdl",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=lIu-ixf-Tzf": {
    "title": "Learning topology-preserving data representations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e2f69d96c20acce19ca53a4086d9ac702237cead",
    "semantic_title": "learning topology-preserving data representations",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=4C8ChYvMYBn": {
    "title": "The Curious Case of Benign Memorization",
    "volume": "poster",
    "abstract": "Despite the empirical advances of deep learning across a variety of learning tasks, our theoretical understanding of its success is still very restricted. One of the key challenges is the overparametrized nature of modern models, enabling complete overfitting of the data even if the labels are randomized, i.e. networks can completely \\textit{memorize} all given patterns. While such a memorization capacity seems worrisome, in this work we show that under training protocols that include \\textit{data augmentation}, neural networks learn to memorize entirely random labels in a benign way, i.e. they learn embeddings that lead to highly non-trivial performance under nearest neighbour probing. We demonstrate that deep models have the surprising ability to separate noise from signal by distributing the task of memorization and feature learning to different layers. As a result, only the very last layers are used for memorization, while preceding layers encode performant features which remain largely unaffected by the label noise. We explore the intricate role of the augmentations used for training and identify a memorization-generalization trade-off in terms of their diversity, marking a clear distinction to all previous works. Finally, we give a first explanation for the emergence of benign memorization by showing that \\textit{malign} memorization under data augmentation is infeasible due to the insufficient capacity of the model for the increased sample size. As a consequence, the network is forced to leverage the correlated nature of the augmentations and as a result learns meaningful features. To complete the picture, a better theory of feature learning in deep neural networks is required to fully understand the origins of this phenomenon",
    "checked": true,
    "id": "d9cf800c64398382d13e6a59b986cd9748111676",
    "semantic_title": "the curious case of benign memorization",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=Ph5cJSfD2XN": {
    "title": "Unbiased Supervised Contrastive Learning",
    "volume": "poster",
    "abstract": "Many datasets are biased, namely they contain easy-to-learn features that are highly correlated with the target class only in the dataset but not in the true underlying distribution of the data. For this reason, learning unbiased models from biased data has become a very relevant research topic in the last years. In this work, we tackle the problem of learning representations that are robust to biases. We first present a margin-based theoretical framework that allows us to clarify why recent contrastive losses (InfoNCE, SupCon, etc.) can fail when dealing with biased data. Based on that, we derive a novel formulation of the supervised contrastive loss ($\\epsilon$-SupInfoNCE), providing more accurate control of the minimal distance between positive and negative samples. Furthermore, thanks to our theoretical framework, we also propose FairKL, a new debiasing regularization loss, that works well even with extremely biased data. We validate the proposed losses on standard vision datasets including CIFAR10, CIFAR100, and ImageNet, and we assess the debiasing capability of FairKL with $\\epsilon$-SupInfoNCE, reaching state-of-the-art performance on a number of biased datasets, including real instances of biases \"in the wild\"",
    "checked": true,
    "id": "50a5411bd2121d1bdc1b775887715c920f3f7dd8",
    "semantic_title": "unbiased supervised contrastive learning",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=mE91GkXYipg": {
    "title": "Compositional Prompt Tuning with Motion Cues for Open-vocabulary Video Relation Detection",
    "volume": "poster",
    "abstract": "Prompt tuning with large-scale pretrained vision-language models empowers open-vocabulary prediction trained on limited base categories, e.g., object classification and detection. In this paper, we propose compositional prompt tuning with motion cues: an extended prompt tuning paradigm for compositional predictions of video data. In particular, we present Relation Prompt (RePro) for Open-vocabulary Video Visual Relation Detection (Open-VidVRD), where conventional prompt tuning is easily biased to certain subject-object combinations and motion patterns. To this end, RePro addresses the two technical challenges of Open-VidVRD: 1) the prompt tokens should respect the two different semantic roles of subject and object, and 2) the tuning should account for the diverse spatiotemporal motion patterns of the subject-object compositions. Our RePro achieves a new state-of-the-art performance on two VidVRD benchmarks of not only the base training object and predicate categories, but also the unseen ones. Extensive ablations also demonstrate the effectiveness of the proposed compositional and multi-mode design of prompt. Code is available at https://github.com/Dawn-LX/OpenVoc-VidVRD",
    "checked": true,
    "id": "a4ecbaf6c57e08157b268ac22bba507abb6b24d5",
    "semantic_title": "compositional prompt tuning with motion cues for open-vocabulary video relation detection",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=fSa5IjNMmmi": {
    "title": "Multi-objective optimization via equivariant deep hypervolume approximation",
    "volume": "poster",
    "abstract": "Optimizing multiple competing objectives is a common problem across science and industry. The inherent inextricable trade-off between those objectives leads one to the task of exploring their Pareto front. A meaningful quantity for the purpose of the latter is the hypervolume indicator, which is used in Bayesian Optimization (BO) and Evolutionary Algorithms (EAs). However, the computational complexity for the calculation of the hypervolume scales unfavorably with increasing number of objectives and data points, which restricts its use in those common multi-objective optimization frameworks. To overcome these restrictions, previous work has focused on approximating the hypervolume using deep learning. In this work, we propose a novel deep learning architecture to approximate the hypervolume function, which we call DeepHV. For better sample efficiency and generalization, we exploit the fact that the hypervolume is scale equivariant in each of the objectives as well as permutation invariant w.r.t. both the objectives and the samples, by using a deep neural network that is equivariant w.r.t. the combined group of scalings and permutations. We show through an ablation study that including these symmetries leads to significantly improved model accuracy. We evaluate our method against exact, and approximate hypervolume methods in terms of accuracy, computation time, and generalization. We also apply and compare our methods to state-of-the-art multi-objective BO methods and EAs on a range of synthetic and real-world benchmark test cases. The results show that our methods are promising for such multi-objective optimization tasks",
    "checked": true,
    "id": "db1ef4f77340da13e0a3b87e348938acaeba7742",
    "semantic_title": "multi-objective optimization via equivariant deep hypervolume approximation",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=nG9RF9z1yy3": {
    "title": "DiffusER: Diffusion via Edit-based Reconstruction",
    "volume": "poster",
    "abstract": "In text generation, models that generate text from scratch one token at a time are currently the dominant paradigm. Despite being performant, these models lack the ability to revise existing text, which limits their usability in many practical scenarios. We look to address this, with DiffusER (Diffusion via Edit-based Reconstruction), a new edit-based generative model for text based on denoising diffusion models -- a class of models that use a Markov chain of denoising steps to incrementally generate data. DiffusER is not only a strong generative model in general, rivalling autoregressive models on several tasks spanning machine translation, summarization, and style transfer; it can also perform other varieties of generation that standard autoregressive models are not well-suited for. For instance, we demonstrate that DiffusER makes it possible for a user to condition generation on a prototype, or an incomplete sequence, and continue revising based on previous edit steps",
    "checked": true,
    "id": "0004c4e27e54f13577951416d8ed476e93f00ef9",
    "semantic_title": "diffuser: diffusion via edit-based reconstruction",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=7oPAgqxNb20": {
    "title": "DynaMS: Dyanmic Margin Selection for Efficient Deep Learning",
    "volume": "poster",
    "abstract": "The great success of deep learning is largely driven by training over-parameterized models on massive datasets. To avoid excessive computation, extracting and training only on the most informative subset is drawing increasing attention. Nevertheless, it is still an open question how to select such a subset on which the model trained generalizes on par with the full data. In this paper, we propose dynamic margin selection (DynaMS). DynaMS leverages the distance from candidate samples to the classification boundary to construct the subset, and the subset is dynamically updated during model training. We show that DynaMS converges with large probability, and for the first time show both in theory and practice that dynamically updating the subset can result in better generalization over previous works. To reduce the additional computation incurred by the selection, a light parameter sharing proxy (PSP) is designed. PSP is able to faithfully evaluate instances with respect to the current model, which is necessary for dynamic selection. Extensive analysis and experiments demonstrate the superiority of the proposed approach in data selection against many state-of-the-art counterparts on benchmark datasets",
    "checked": true,
    "id": "a30f8afa2d867fcf26c7ae504d0885a933589702",
    "semantic_title": "dynams: dyanmic margin selection for efficient deep learning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=n6H86gW8u0d": {
    "title": "TANGOS: Regularizing Tabular Neural Networks through Gradient Orthogonalization and Specialization",
    "volume": "poster",
    "abstract": "Despite their success with unstructured data, deep neural networks are not yet a panacea for structured tabular data. In the tabular domain, their efficiency crucially relies on various forms of regularization to prevent overfitting and provide strong generalization performance. Existing regularization techniques include broad modelling decisions such as choice of architecture, loss functions, and optimization methods. In this work, we introduce Tabular Neural Gradient Orthogonalization and Specialization (TANGOS), a novel framework for regularization in the tabular setting built on latent unit attributions. The gradient attribution of an activation with respect to a given input feature suggests how the neuron attends to that feature, and is often employed to interpret the predictions of deep networks. In TANGOS, we take a different approach and incorporate neuron attributions directly into training to encourage orthogonalization and specialization of latent attributions in a fully-connected network. Our regularizer encourages neurons to focus on sparse, non-overlapping input features and results in a set of diverse and specialized latent units. In the tabular domain, we demonstrate that our approach can lead to improved out-of-sample generalization performance, outperforming other popular regularization methods. We provide insight into why our regularizer is effective and demonstrate that TANGOS can be applied jointly with existing methods to achieve even greater generalization performance",
    "checked": true,
    "id": "b720dc316737256f71b2e2de35bb8836e79cf2e7",
    "semantic_title": "tangos: regularizing tabular neural networks through gradient orthogonalization and specialization",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=udNhDCr2KQe": {
    "title": "Learning to Solve Constraint Satisfaction Problems with Recurrent Transformer",
    "volume": "poster",
    "abstract": "Constraint satisfaction problems (CSPs) are about finding values of variables that satisfy the given constraints. We show that Transformer extended with recurrence is a viable approach to learning to solve CSPs in an end-to-end manner, having clear advantages over state-of-the-art methods such as Graph Neural Networks, SATNet, and some neuro-symbolic models. With the ability of Transformer to handle visual input, the proposed Recurrent Transformer can straightforwardly be applied to visual constraint reasoning problems while successfully addressing the symbol grounding problem. We also show how to leverage deductive knowledge of discrete constraints in the Transformer's inductive learning to achieve sample-efficient learning and semi-supervised learning for CSPs",
    "checked": true,
    "id": "6a74efe6cf4526058d1a7dd2289e1b3d4fbb418e",
    "semantic_title": "learning to solve constraint satisfaction problems with recurrent transformer",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=GrpU6dxFmMN": {
    "title": "Improving the imputation of missing data with Markov Blanket discovery",
    "volume": "poster",
    "abstract": "The process of imputation of missing data typically relies on generative and regression models. These approaches often operate on the unrealistic assumption that all of the data features are directly related with one another, and use all of the available features to impute missing values. In this paper, we propose a novel Markov Blanket discovery approach to determine the optimal feature set for a given variable by considering both observed variables and missingness of partially observed variables to account for systematic missingness. We then incorporate this method to the learning process of the state-of-the-art MissForest imputation algorithm, such that it informs MissForest which features to consider to impute missing values, depending on the variable the missing value belongs to. Experiments across different case studies and multiple imputation algorithms show that the proposed solution improves imputation accuracy, both under random and systematic missingness",
    "checked": true,
    "id": "588fac3a8154e447d2db3a594447991a49b74e48",
    "semantic_title": "improving the imputation of missing data with markov blanket discovery",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=kDSmxOspsXQ": {
    "title": "Boosting the Cycle Counting Power of Graph Neural Networks with I$^2$-GNNs",
    "volume": "poster",
    "abstract": "Message Passing Neural Networks (MPNNs) are a widely used class of Graph Neural Networks (GNNs). The limited representational power of MPNNs inspires the study of provably powerful GNN architectures. However, knowing one model is more powerful than another gives little insight about what functions they can or cannot express. It is still unclear whether these models are able to approximate specific functions such as counting certain graph substructures, which is essential for applications in biology, chemistry and social network analysis. Motivated by this, we propose to study the counting power of Subgraph MPNNs, a recent and popular class of powerful GNN models that extract rooted subgraphs for each node, assign the root node a unique identifier and encode the root node's representation within its rooted subgraph. Specifically, we prove that Subgraph MPNNs fail to count more-than-4-cycles at node level, implying that node representations cannot correctly encode the surrounding substructures like ring systems with more than four atoms. To overcome this limitation, we propose I$^2$-GNNs to extend Subgraph MPNNs by assigning different identifiers for the root node and its neighbors in each subgraph. I$^2$-GNNs' discriminative power is shown to be strictly stronger than Subgraph MPNNs and partially stronger than the 3-WL test. More importantly, I$^2$-GNNs are proven capable of counting all 3, 4, 5 and 6-cycles, covering common substructures like benzene rings in organic chemistry, while still keeping linear complexity. To the best of our knowledge, it is the first linear-time GNN model that can count 6-cycles with theoretical guarantees. We validate its counting power in cycle counting tasks and demonstrate its competitive performance in molecular prediction benchmarks",
    "checked": false,
    "id": "85aee44aecc922041ead3dc112aa7c323f2c3c05",
    "semantic_title": "boosting the cycle counting power of graph neural networks with i2-gnns",
    "citation_count": 49,
    "authors": []
  },
  "https://openreview.net/forum?id=WlbG820mRH-": {
    "title": "Fundamental Limits in Formal Verification of Message-Passing Neural Networks",
    "volume": "poster",
    "abstract": "Output reachability and adversarial robustness are among the most relevant safety properties of neural networks. We show that in the context of Message Passing Neural Networks (MPNN), a common Graph Neural Network (GNN) model, formal verification is impossible. In particular, we show that output reachability of graph-classifier MPNN, working over graphs of unbounded size, non-trivial degree and sufficiently expressive node labels, cannot be verified formally: there is no algorithm that answers correctly (with yes or no), given an MPNN, whether there exists some valid input to the MPNN such that the corresponding output satisfies a given specification. However, we also show that output reachability and adversarial robustness of node-classifier MPNN can be verified formally when a limit on the degree of input graphs is given a priori. We discuss the implications of these results, for the purpose of obtaining a complete picture of the principle possibility to formally verify GNN, depending on the expressiveness of the involved GNN models and input-output specifications",
    "checked": true,
    "id": "e71a31fee2ba7db5232ef0bb00bb2afb1c8f870a",
    "semantic_title": "fundamental limits in formal verification of message-passing neural networks",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4DU_HCijfJp": {
    "title": "Short-Term Memory Convolutions",
    "volume": "poster",
    "abstract": "The real-time processing of time series signals is a critical issue for many real-life applications. The idea of real-time processing is especially important in audio domain as the human perception of sound is sensitive to any kind of disturbance in perceived signals, especially the lag between auditory and visual modalities. The rise of deep learning (DL) models complicated the landscape of signal processing. Although they often have superior quality compared to standard DSP methods, this advantage is diminished by higher latency. In this work we propose novel method for minimization of inference time latency and memory consumption, called Short-Term Memory Convolution (STMC) and its transposed counterpart. The main advantage of STMC is the low latency comparable to long short-term memory (LSTM) networks. Furthermore, the training of STMC-based models is faster and more stable as the method is based solely on convolutional neural networks (CNNs). In this study we demonstrate an application of this solution to a U-Net model for a speech separation task and GhostNet model in acoustic scene classification (ASC) task. In case of speech separation we achieved a 5-fold reduction in inference time and a 2-fold reduction in latency without affecting the output quality. The inference time for ASC task was up to 4 times faster while preserving the original accuracy",
    "checked": true,
    "id": "84f681bba83ba75cc7962ec37297415fe8cd7674",
    "semantic_title": "short-term memory convolutions",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=PfpEtB3-csK": {
    "title": "LexMAE: Lexicon-Bottlenecked Pretraining for Large-Scale Retrieval",
    "volume": "poster",
    "abstract": "In large-scale retrieval, the lexicon-weighting paradigm, learning weighted sparse representations in vocabulary space, has shown promising results with high quality and low latency. Despite it deeply exploiting the lexicon-representing capability of pre-trained language models, a crucial gap remains between language modeling and lexicon-weighting retrieval -- the former preferring certain or low-entropy words whereas the latter favoring pivot or high-entropy words -- becoming the main barrier to lexicon-weighting performance for large-scale retrieval. To bridge this gap, we propose a brand-new pre-training framework, lexicon-bottlenecked masked autoencoder (LexMAE), to learn importance-aware lexicon representations. Essentially, we present a lexicon-bottlenecked module between a normal language modeling encoder and a weakened decoder, where a continuous bag-of-words bottleneck is constructed to learn a lexicon-importance distribution in an unsupervised fashion. The pre-trained LexMAE is readily transferred to the lexicon-weighting retrieval via fine-tuning. On the ad-hoc retrieval benchmark, MS-Marco, it achieves 42.6% MRR@10 with 45.8 QPS for the passage dataset and 44.4% MRR@100 with 134.8 QPS for the document dataset, by a CPU machine. And LexMAE shows state-of-the-art zero-shot transfer capability on BEIR benchmark with 12 datasets",
    "checked": true,
    "id": "b2fbfaefd5d02244cc9a8dbfc6643869d754f022",
    "semantic_title": "lexmae: lexicon-bottlenecked pretraining for large-scale retrieval",
    "citation_count": 37,
    "authors": []
  },
  "https://openreview.net/forum?id=pHMpgT5xWaE": {
    "title": "A GNN-Guided Predict-and-Search Framework for Mixed-Integer Linear Programming",
    "volume": "poster",
    "abstract": "Mixed-integer linear programming (MILP) is widely employed for modeling combinatorial optimization problems. In practice, similar MILP instances with only coefficient variations are routinely solved, and machine learning (ML) algorithms are capable of capturing common patterns across these MILP instances. In this work, we combine ML with optimization and propose a novel predict-and-search framework for efficiently identifying high-quality feasible solutions. Specifically, we first utilize graph neural networks to predict the marginal probability of each variable, and then search for the best feasible solution within a properly defined ball around the predicted solution. We conduct extensive experiments on public datasets, and computational results demonstrate that our proposed framework achieves 51.1% and 9.9% performance improvements to MILP solvers SCIP and Gurobi on primal gaps, respectively",
    "checked": true,
    "id": "887d11c3fbc1a87a8bfe97bd53cfedab89496f99",
    "semantic_title": "a gnn-guided predict-and-search framework for mixed-integer linear programming",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=piIsx-G3Gux": {
    "title": "On Explaining Neural Network Robustness with Activation Path",
    "volume": "poster",
    "abstract": "Despite their verified performance, neural networks are prone to be misled by maliciously designed adversarial examples. This work investigates the robustness of neural networks from the activation pattern perspective. We find that despite the complex structure of the deep neural network, most of the neurons provide locally stable contributions to the output, while the minority, which we refer to as float neurons, can greatly affect the prediction. We decompose the computational graph of the neural network into the fixed path and float path and investigate their role in generating adversarial examples. Based on our analysis, we categorize the vulnerable examples into Lipschitz vulnerability and float neuron vulnerability. We show that the boost of robust accuracy from randomized smoothing is the result of correcting the latter. We then propose an SC-RFP (smoothed classifier with repressed float path) to further reduce the instability of the float neurons and show that our result can provide a higher certified radius as well as accuracy",
    "checked": true,
    "id": "9d242b5c0c00fc36002161deb6a124f2c624e982",
    "semantic_title": "on explaining neural network robustness with activation path",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=O_lFCPaF48t": {
    "title": "Structure by Architecture: Structured Representations without Regularization",
    "volume": "poster",
    "abstract": "We study the problem of self-supervised structured representation learning using autoencoders for downstream tasks such as generative modeling. Unlike most methods which rely on matching an arbitrary, relatively unstructured, prior distribution for sampling, we propose a sampling technique that relies solely on the independence of latent variables, thereby avoiding the trade-off between reconstruction quality and generative performance typically observed in VAEs. We design a novel autoencoder architecture capable of learning a structured representation without the need for aggressive regularization. Our structural decoders learn a hierarchy of latent variables, thereby ordering the information without any additional regularization or supervision. We demonstrate how these models learn a representation that improves results in a variety of downstream tasks including generation, disentanglement, and extrapolation using several challenging and natural image datasets",
    "checked": false,
    "id": "9e1f82ce5f6a31d7d0ca48d33235c7f5f6466bc0",
    "semantic_title": "structure by architecture: disentangled representations without regularization",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=1UCaQYUdE_o": {
    "title": "Understanding Neural Coding on Latent Manifolds by Sharing Features and Dividing Ensembles",
    "volume": "poster",
    "abstract": "Systems neuroscience relies on two complementary views of neural data, characterized by single neuron tuning curves and analysis of population activity. These two perspectives combine elegantly in neural latent variable models that constrain the relationship between latent variables and neural activity, modeled by simple tuning curve functions. This has recently been demonstrated using Gaussian processes, with applications to realistic and topologically relevant latent manifolds. Those and previous models, however, missed crucial shared coding properties of neural populations. We propose $\\textit{feature sharing}$ across neural tuning curves which significantly improves performance and helps optimization. We also propose a solution to the $\\textit{ensemble detection}$ problem, where different groups of neurons, i.e., ensembles, can be modulated by different latent manifolds. Achieved through a soft clustering of neurons during training, this allows for the separation of mixed neural populations in an unsupervised manner. These innovations lead to more interpretable models of neural population activity that train well and perform better even on mixtures of complex latent manifolds. Finally, we apply our method on a recently published grid cell dataset, and recover distinct ensembles, infer toroidal latents and predict neural tuning curves in a single integrated modeling framework",
    "checked": true,
    "id": "049ee767e458d83adb939be704c0d42474c742f9",
    "semantic_title": "understanding neural coding on latent manifolds by sharing features and dividing ensembles",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=q-PbpHD3EOk": {
    "title": "Learning Fast and Slow for Online Time Series Forecasting",
    "volume": "poster",
    "abstract": "Despite the recent success of deep learning for time series forecasting, these methods are not scalable for many real-world applications where data arrives sequentially. Training deep neural forecasters on the fly is notoriously challenging because of their limited ability to adapt to non-stationary environments and remember old knowledge. We argue that the fast adaptation capability of deep neural networks is critical and successful solutions require handling changes to both new and recurring patterns effectively. In this work, inspired by the Complementary Learning Systems (CLS) theory, we propose Fast and Slow learning Network (FSNet) as a novel framework to address the challenges of online forecasting. Particularly, FSNet improves the slowly-learned backbone by dynamically balancing fast adaptation to recent changes and retrieving similar old knowledge. FSNet achieves this mechanism via an interaction between two novel complementary components: (i) a per-layer adapter to support fast learning from individual layers, and (ii) an associative memory to support remembering, updating, and recalling repeating events. Extensive experiments on real and synthetic datasets validate FSNet's efficacy and robustness to both new and recurring patterns. Our code is publicly available at: \\url{https://github.com/salesforce/fsnet/}",
    "checked": true,
    "id": "4d755a5a66a8c46b722b44613788085191524e11",
    "semantic_title": "learning fast and slow for online time series forecasting",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=FtOxgKe_Zg2": {
    "title": "Guess the Instruction! Flipped Learning Makes Language Models Stronger Zero-Shot Learners",
    "volume": "poster",
    "abstract": "Meta-training, which fine-tunes the language model (LM) on various downstream tasks by maximizing the likelihood of the target label given the task instruction and input instance, has improved the zero-shot task generalization performance. However, meta-trained LMs still struggle to generalize to challenging tasks containing novel labels unseen during meta-training. In this paper, we propose Flipped Learning, an alternative method of meta-training which trains the LM to generate the task instruction given the input instance and label. During inference, the LM trained with Flipped Learning, referred to as FLIPPED, selects the label option that is most likely to generate the task instruction. On 14 tasks of the BIG-bench benchmark, the 11B-sized FLIPPED outperforms zero-shot T0-11B (Sanh et al, 2021) and even a 16 times larger 3-shot GPT-3 (175B) (Brown et al, 2020) on average by 8.4% and 9.7% points, respectively. FLIPPED gives particularly large improvements on tasks with unseen labels, outperforming T0-11B by up to +20% average F1 score. This indicates that the strong task generalization of FLIPPED comes from improved generalization to novel labels. We release our code at github.com/seonghyeonye/Flipped-Learning",
    "checked": true,
    "id": "b9ec37d028fae61752c33a55fb88bd27e6cb8c4d",
    "semantic_title": "guess the instruction! flipped learning makes language models stronger zero-shot learners",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=_BoPed4tYww": {
    "title": "Timing is Everything: Learning to Act Selectively with Costly Actions and Budgetary Constraints",
    "volume": "poster",
    "abstract": "Many real-world settings involve costs for performing actions; transaction costs in financial systems and fuel costs being common examples. In these settings, performing actions at each time step quickly accumulates costs leading to vastly suboptimal outcomes. Additionally, repeatedly acting produces wear and tear and ultimately, damage. Determining when to act is crucial for achieving successful outcomes and yet, the challenge of efficiently learning to behave optimally when actions incur minimally bounded costs remains unresolved. In this paper, we intro- duce a reinforcement learning (RL) framework named Learnable Impulse Control Reinforcement Algorithm (LICRA), for learning to optimally select both when to act and which actions to take when actions incur costs. At the core of LICRA is a nested structure that combines RL and a form of policy known as impulse control which learns to maximise objectives when actions incur costs. We prove that LICRA, which seamlessly adopts any RL method, converges to policies that optimally select when to perform actions and their optimal magnitudes. We then augment LICRA to handle problems in which the agent can perform at most k < ∞ actions and more generally, faces a budget constraint. We show LICRA learns the optimal value function and ensures budget constraints are satisfied almost surely. We demonstrate empirically LICRA's superior performance against benchmark RL methods in OpenAI gym's Lunar Lander and in Highway environments and a variant of the Merton portfolio problem within finance",
    "checked": true,
    "id": "2bb383228065a17cbb3de273d2572e2097115612",
    "semantic_title": "timing is everything: learning to act selectively with costly actions and budgetary constraints",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=Lt8bMlhiwx2": {
    "title": "DeCap: Decoding CLIP Latents for Zero-Shot Captioning via Text-Only Training",
    "volume": "poster",
    "abstract": "Large-scale pre-trained multi-modal models (e.g., CLIP) demonstrate strong zero-shot transfer capability in many discriminative tasks, e.g., image classification. Their adaptation to zero-shot image-conditioned text generation tasks has drawn increasing interest. Prior arts approach to zero-shot captioning by either utilizing the existing large language models (e.g., GPT-2) or pre-training the encoder-decoder network in an end-to-end manner. However, the large language models may not generate sensible descriptions due to the task discrepancy between captioning and language modeling, while the end-to-end pre-training requires paired data and extensive computational resources. In this work, we propose a simple framework, named DeCap, for zero-shot captioning. We introduce a lightweight visual-aware language decoder. This decoder is both data-efficient and computation-efficient: 1) it only requires the \\textit{text} data for training, easing the burden on the collection of paired data. 2) it does not require end-to-end training. When trained with text-only data, the decoder takes the text embedding extracted from the off-the-shelf CLIP encoder as a prefix embedding. The challenge is that the decoder is trained on the text corpus but at the inference stage, it needs to generate captions based on visual inputs. Though the CLIP text embedding and the visual embedding are correlated, the \\textit{modality gap} issue is widely observed in multi-modal contrastive models that prevents us from directly taking the visual embedding as the prefix embedding. We propose a training-free mechanism to reduce the modality gap. We project the visual embedding into the CLIP text embedding space, while the projected embedding retains the information of the visual input. Taking the projected embedding as the prefix embedding, the decoder generates high-quality descriptions that match the visual input. The experiments show that DeCap outperforms other zero-shot captioning methods and unpaired captioning methods by a large margin on the typical image captioning benchmarks, i.e., MSCOCO and NoCaps. We apply DeCap to video captioning and achieve state-of-the-art zero-shot performance on MSR-VTT and ActivityNet-Captions. The code is available at https://github.com/dhg-wei/DeCap",
    "checked": true,
    "id": "a20e7e5e4338644d24145e71a9b89100af87e5d0",
    "semantic_title": "decap: decoding clip latents for zero-shot captioning via text-only training",
    "citation_count": 97,
    "authors": []
  },
  "https://openreview.net/forum?id=wZ2SVhOTzBX": {
    "title": "That Label's got Style: Handling Label Style Bias for Uncertain Image Segmentation",
    "volume": "poster",
    "abstract": "Segmentation uncertainty models predict a distribution over plausible segmentations for a given input, which they learn from the annotator variation in the training set. However, in practice these annotations can differ systematically in the way they are generated, for example through the use of different labeling tools. This results in datasets that contain both data variability and differing label styles. In this paper, we demonstrate that applying state-of-the-art segmentation uncertainty models on such datasets can lead to model bias caused by the different label styles. We present an updated modelling objective conditioning on labeling style for aleatoric uncertainty estimation, and modify two state-of-the-art-architectures for segmentation uncertainty accordingly. We show with extensive experiments that this method reduces label style bias, while improving segmentation performance, increasing the applicability of segmentation uncertainty models in the wild. We curate two datasets, with annotations in different label styles, which we will make publicly available along with our code upon publication",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sAJDi9lD06L": {
    "title": "Holistic Adversarially Robust Pruning",
    "volume": "poster",
    "abstract": "Neural networks can be drastically shrunk in size by removing redundant parameters. While crucial for the deployment on resource-constraint hardware, oftentimes, compression comes with a severe drop in accuracy and lack of adversarial robustness. Despite recent advances, counteracting both aspects has only succeeded for moderate compression rates so far. We propose a novel method, HARP, that copes with aggressive pruning significantly better than prior work. For this, we consider the network holistically. We learn a global compression strategy that optimizes how many parameters (compression rate) and which parameters (scoring connections) to prune specific to each layer individually. Our method fine-tunes an existing model with dynamic regularization, that follows a step-wise incremental function balancing the different objectives. It starts by favoring robustness before shifting focus on reaching the target compression rate and only then handles the objectives equally. The learned compression strategies allow us to maintain the pre-trained model's natural accuracy and its adversarial robustness for a reduction by 99% of the network's original size. Moreover, we observe a crucial influence of non-uniform compression across layers. The implementation of HARP is publicly available at https://intellisec.de/research/harp",
    "checked": false,
    "id": "47cbf414220626972ce2b609bf13514f98beef43",
    "semantic_title": "non-uniform adversarially robust pruning",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=syfgJE6nFRW": {
    "title": "PASHA: Efficient HPO and NAS with Progressive Resource Allocation",
    "volume": "poster",
    "abstract": "Hyperparameter optimization (HPO) and neural architecture search (NAS) are methods of choice to obtain the best-in-class machine learning models, but in practice they can be costly to run. When models are trained on large datasets, tuning them with HPO or NAS rapidly becomes prohibitively expensive for practitioners, even when efficient multi-fidelity methods are employed. We propose an approach to tackle the challenge of tuning machine learning models trained on large datasets with limited computational resources. Our approach, named PASHA, extends ASHA and is able to dynamically allocate maximum resources for the tuning procedure depending on the need. The experimental comparison shows that PASHA identifies well-performing hyperparameter configurations and architectures while consuming significantly fewer computational resources than ASHA",
    "checked": true,
    "id": "bd4579d383337f2402dade25323c285f7d109ae8",
    "semantic_title": "pasha: efficient hpo and nas with progressive resource allocation",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=3VO1y5N7K1H": {
    "title": "StableDR: Stabilized Doubly Robust Learning for Recommendation on Data Missing Not at Random",
    "volume": "poster",
    "abstract": "In recommender systems, users always choose the favorite items to rate, which leads to data missing not at random and poses a great challenge for unbiased evaluation and learning of prediction models. Currently, the doubly robust (DR) methods have been widely studied and demonstrate superior performance. However, in this paper, we show that DR methods are unstable and have unbounded bias, variance, and generalization bounds to extremely small propensities. Moreover, the fact that DR relies more on extrapolation will lead to suboptimal performance. To address the above limitations while retaining double robustness, we propose a stabilized doubly robust (StableDR) learning approach with a weaker reliance on extrapolation. Theoretical analysis shows that StableDR has bounded bias, variance, and generalization error bound simultaneously under inaccurate imputed errors and arbitrarily small propensities. In addition, we propose a novel learning approach for StableDR that updates the imputation, propensity, and prediction models cyclically, achieving more stable and accurate predictions. Extensive experiments show that our approaches significantly outperform the existing methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aoDyX6vSqsd": {
    "title": "Sampling-based inference for large linear models, with application to linearised Laplace",
    "volume": "poster",
    "abstract": "Large-scale linear models are ubiquitous throughout machine learning, with contemporary application as surrogate models for neural network uncertainty quantification; that is, the linearised Laplace method. Alas, the computational cost associated with Bayesian linear models constrains this method's application to small networks, small output spaces and small datasets. We address this limitation by introducing a scalable sample-based Bayesian inference method for conjugate Gaussian multi-output linear models, together with a matching method for hyperparameter (regularisation) selection. Furthermore, we use a classic feature normalisation method (the g-prior) to resolve a previously highlighted pathology of the linearised Laplace method. Together, these contributions allow us to perform linearised neural network inference with ResNet-18 on CIFAR100 (11M parameters, 100 output dimensions × 50k datapoints) and with a U-Net on a high-resolution tomographic reconstruction task (2M parameters, 251k output dimensions)",
    "checked": true,
    "id": "2c1ac6f0ee28c8c86f46f90a3a29fd75cf9f4aaf",
    "semantic_title": "sampling-based inference for large linear models, with application to linearised laplace",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=5-Df3tljit7": {
    "title": "Defending against Adversarial Audio via Diffusion Model",
    "volume": "poster",
    "abstract": "Deep learning models have been widely used in commercial acoustic systems in recent years. However, adversarial audio examples can cause abnormal behaviors for those acoustic systems, while being hard for humans to perceive. Various methods, such as transformation-based defenses and adversarial training, have been proposed to protect acoustic systems from adversarial attacks, but they are less effective against adaptive attacks. Furthermore, directly applying the methods from the image domain can lead to suboptimal results because of the unique properties of audio data. In this paper, we propose an adversarial purification-based defense pipeline, AudioPure, for acoustic systems via off-the-shelf diffusion models. Taking advantage of the strong generation ability of diffusion models, AudioPure first adds a small amount of noise to the adversarial audio and then runs the reverse sampling step to purify the noisy audio and recover clean audio. AudioPure is a plug-and-play method that can be directly applied to any pretrained classifier without any fine-tuning or re-training. We conduct extensive experiments on the speech command recognition task to evaluate the robustness of AudioPure. Our method is effective against diverse adversarial attacks (e.g. L2 or L∞-norm). It outperforms the existing methods under both strong adaptive white-box and black-box attacks bounded by L2 or L∞-norm (up to +20% in robust accuracy). Besides, we also evaluate the certified robustness for perturbations bounded by L2-norm via randomized smoothing. Our pipeline achieves a higher certified accuracy than baselines",
    "checked": true,
    "id": "f4b0306d9ddf227f3fe5167981035c4b50310389",
    "semantic_title": "defending against adversarial audio via diffusion model",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=Jifob4dSh99": {
    "title": "Theoretical Characterization of the Generalization Performance of Overfitted Meta-Learning",
    "volume": "poster",
    "abstract": "Meta-learning has arisen as a successful method for improving training performance by training over many similar tasks, especially with deep neural networks (DNNs). However, the theoretical understanding of when and why overparameterized models such as DNNs can generalize well in meta-learning is still limited. As an initial step towards addressing this challenge, this paper studies the generalization performance of overfitted meta-learning under a linear regression model with Gaussian features. In contrast to a few recent studies along the same line, our framework allows the number of model parameters to be arbitrarily larger than the number of features in the ground truth signal, and hence naturally captures the overparameterized regime in practical deep meta-learning. We show that the overfitted min $\\ell_2$-norm solution of model-agnostic meta-learning (MAML) can be beneficial, which is similar to the recent remarkable findings on \"benign overfitting\" and \"double descent\" phenomenon in the classical (single-task) linear regression. However, due to the uniqueness of meta-learning such as task-specific gradient descent inner training and the diversity/fluctuation of the ground-truth signals among training tasks, we find new and interesting properties that do not exist in single-task linear regression. We first provide a high-probability upper bound (under reasonable tightness) on the generalization error, where certain terms decrease when the number of features increases. Our analysis suggests that benign overfitting is more significant and easier to observe when the noise and the diversity/fluctuation of the ground truth of each training task are large. Under this circumstance, we show that the overfitted min $\\ell_2$-norm solution can achieve an even lower generalization error than the underparameterized solution",
    "checked": true,
    "id": "346d43e185d6593b2d5d7c156fdaf26a335cf445",
    "semantic_title": "theoretical characterization of the generalization performance of overfitted meta-learning",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=_hHYaKu0jcj": {
    "title": "Robust Explanation Constraints for Neural Networks",
    "volume": "poster",
    "abstract": "Post-hoc explanation methods are used with the intent of providing insights about neural networks and are sometimes said to help engender trust in their outputs. However, popular explanations methods have been found to be fragile to minor perturbations of input features or model parameters. Relying on constraint relaxation techniques from non-convex optimization, we develop a method that upper-bounds the largest change an adversary can make to a gradient-based explanation via bounded manipulation of either the input features or model parameters. By propagating a compact input or parameter set as symbolic intervals through the forwards and backwards computations of the neural network we can formally certify the robustness of gradient-based explanations. Our bounds are differentiable, hence we can incorporate provable explanation robustness into neural network training. Empirically, our method surpasses the robustness provided by previous heuristic approaches. We find that our training method is the only method able to learn neural networks with certificates of explanation robustness across all six datasets tested",
    "checked": true,
    "id": "ac9c07f716e43db03b088d3e17bc9093c271662e",
    "semantic_title": "robust explanation constraints for neural networks",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=42zs3qa2kpy": {
    "title": "Offline Reinforcement Learning via High-Fidelity Generative Behavior Modeling",
    "volume": "poster",
    "abstract": "In offline reinforcement learning, weighted regression is a common method to ensure the learned policy stays close to the behavior policy and to prevent selecting out-of-sample actions. In this work, we show that due to the limited distributional expressivity of policy models, previous methods might still select unseen actions during training, which deviates from their initial motivation. To address this problem, we adopt a generative approach by decoupling the learned policy into two parts: an expressive generative behavior model and an action evaluation model. The key insight is that such decoupling avoids learning an explicitly parameterized policy model with a closed-form expression. Directly learning the behavior policy allows us to leverage existing advances in generative modeling, such as diffusion-based methods, to model diverse behaviors. As for action evaluation, we combine our method with an in-sample planning technique to further avoid selecting out-of-sample actions and increase computational efficiency. Experimental results on D4RL datasets show that our proposed method achieves competitive or superior performance compared with state-of-the-art offline RL methods, especially in complex tasks such as AntMaze. We also empirically demonstrate that our method can successfully learn from a heterogeneous dataset containing multiple distinctive but similarly successful strategies, whereas previous unimodal policies fail",
    "checked": true,
    "id": "0f1402c536cc3cbbcb73b06f96289e50a34ca3cf",
    "semantic_title": "offline reinforcement learning via high-fidelity generative behavior modeling",
    "citation_count": 132,
    "authors": []
  },
  "https://openreview.net/forum?id=rB6TpjAuSRy": {
    "title": "CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "707bd332d2c21dc5eb1f02a52d4a0506199aae76",
    "semantic_title": "cogvideo: large-scale pretraining for text-to-video generation via transformers",
    "citation_count": 706,
    "authors": []
  },
  "https://openreview.net/forum?id=tXc-riXhmx": {
    "title": "Revisit Finetuning strategy for Few-Shot Learning to Transfer the Emdeddings",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "827d1a5fc6d174c83aff9bcee61852e0afbc0cab",
    "semantic_title": "revisit finetuning strategy for few-shot learning to transfer the emdeddings",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=Vf6WcUDnY7c": {
    "title": "Optimizing Spca-based Continual Learning: A Theoretical Approach",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5e293220b40d27559dc9aaa34582a088f1081671",
    "semantic_title": "optimizing spca-based continual learning: a theoretical approach",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=UYcIheNY9Pf": {
    "title": "Value Memory Graph: A Graph-Structured World Model for Offline Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "67b950f8d31e1b5dfed49993ef4d809e3bf0cc2c",
    "semantic_title": "value memory graph: a graph-structured world model for offline reinforcement learning",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=Tuk3Pqaizx": {
    "title": "Sampling-free Inference for Ab-Initio Potential Energy Surface Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5cAI0qXxyv": {
    "title": "$\\mathscr{N}$-WL: A New Hierarchy of Expressivity for Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "1c465203bd9cf02eaaf64b3d9b3699c276a31ec3",
    "semantic_title": "𝒩-wl: a new hierarchy of expressivity for graph neural networks",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=47B_ctC4pJ": {
    "title": "Learning Input-agnostic Manipulation Directions in StyleGAN with Text Guidance",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2e5dc2bbfd04bd965635218d10716b4fb996ca7c",
    "semantic_title": "learning input-agnostic manipulation directions in stylegan with text guidance",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=CW6KmU5wPh": {
    "title": "DAVA: Disentangling Adversarial Variational Autoencoder",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9d3284c33da3c60ee6c12ec88495e70b7aff33dd",
    "semantic_title": "dava: disentangling adversarial variational autoencoder",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=EIgLnNx_lC": {
    "title": "TDR-CL: Targeted Doubly Robust Collaborative Learning for Debiased Recommendations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "84b9802042f29f4490eb36c2ac69a8ce9cdb2196",
    "semantic_title": "tdr-cl: targeted doubly robust collaborative learning for debiased recommendations",
    "citation_count": 45,
    "authors": []
  },
  "https://openreview.net/forum?id=7IG0wsTND7w": {
    "title": "Domain Generalisation via Domain Adaptation: An Adversarial Fourier Amplitude Approach",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8b2b1bd8601d61b5c5a1ea04f7f4ca674a4fe6b0",
    "semantic_title": "domain generalisation via domain adaptation: an adversarial fourier amplitude approach",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=J_Cja7cpgW": {
    "title": "Consolidator: Mergable Adapter with Group Connections for Visual Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "a17d4a542bcdd1d5ade9b64cef093fa76437f96a",
    "semantic_title": "consolidator: mergeable adapter with grouped connections for visual adaptation",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=hxUwnEGxW87": {
    "title": "Statistical Theory of Differentially Private Marginal-based Data Synthesis Algorithms",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f4ca8df6c831936d466a683fbc5a7ccd306341f8",
    "semantic_title": "statistical theory of differentially private marginal-based data synthesis algorithms",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=J3Y7cgZOOS": {
    "title": "Anti-Symmetric DGN: a stable architecture for Deep Graph Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e516225868c0247007f917213f5b23434299e5df",
    "semantic_title": "anti-symmetric dgn: a stable architecture for deep graph networks",
    "citation_count": 64,
    "authors": []
  },
  "https://openreview.net/forum?id=xPkJYRsQGM": {
    "title": "Contrastive Learning for Unsupervised Domain Adaptation of Time Series",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=47KG_AvNqeZ": {
    "title": "Online Low Rank Matrix Completion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5Egggz1q575": {
    "title": "Explaining RL Decisions with Trajectories",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c3a6432b582d2c3bbffa3c8f69276f9b42939829",
    "semantic_title": "explaining rl decisions with trajectories",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=rnRiiHw8Vy": {
    "title": "FastFill: Efficient Compatible Model Update",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "628bc8e9a8408a780b61295373568f68617851ec",
    "semantic_title": "fastfill: efficient compatible model update",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=WsUMeHPo-2": {
    "title": "Learnable Graph Convolutional Attention Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a1ec107ca5e5e3ec22949aa283462947180ffd66",
    "semantic_title": "learnable graph convolutional attention networks",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=N4K5ck-BTT": {
    "title": "Scaffolding a Student to Instill Knowledge",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a4COps0uokg": {
    "title": "User-Interactive Offline Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "41479416d858bacfaa01bfc068ceb86466d542e9",
    "semantic_title": "user-interactive offline reinforcement learning",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=EBS4C77p_5S": {
    "title": "SLTUNET: A Simple Unified Model for Sign Language Translation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3bbc8841012fdb5971d1c86dff528edd8590f1b8",
    "semantic_title": "sltunet: a simple unified model for sign language translation",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=iUYpN14qjTF": {
    "title": "Understanding the Generalization of Adam in Learning Neural Networks with Proper Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0_TxFpAsEI": {
    "title": "A law of adversarial risk, interpolation, and label noise",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c0c3da6e4511112f5c79c98966b7c6f2b0158c77",
    "semantic_title": "a law of adversarial risk, interpolation, and label noise",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=nchvKfvNeX0": {
    "title": "Learning ReLU networks to high uniform accuracy is intractable",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MnEjsw-vj-X": {
    "title": "Active Learning for Object Detection with Evidential Deep Learning and Hierarchical Uncertainty Aggregation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9ba1bc3f605e0d07a61bd44ed109096b448ac318",
    "semantic_title": "active learning for object detection with evidential deep learning and hierarchical uncertainty aggregation",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=5spDgWmpY6x": {
    "title": "How Sharpness-Aware Minimization Minimizes Sharpness?",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8d0f6d369b2a0adb77a127f4f3085facfc7acce5",
    "semantic_title": "how sharpness-aware minimization minimizes sharpness?",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=xtbog7cfsr": {
    "title": "The Implicit Bias of Minima Stability in Multivariate Shallow ReLU Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f9b464154bda6ffaafa462c4e8a8253907db95b9",
    "semantic_title": "the implicit bias of minima stability in multivariate shallow relu networks",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=5KUPKjHYD-l": {
    "title": "MAST: Masked Augmentation Subspace Training for Generalizable Self-Supervised Priors",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "05e73f7da4b0a52b409459b69fcaed950b0b09f3",
    "semantic_title": "mast: masked augmentation subspace training for generalizable self-supervised priors",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=yHIIM9BgOo": {
    "title": "Graph-based Deterministic Policy Gradient for Repetitive Combinatorial Optimization Problems",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "325acb7159234497a0938c019d21ed7bc7f9514f",
    "semantic_title": "graph-based deterministic policy gradient for repetitive combinatorial optimization problems",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=2mvALOAWaxY": {
    "title": "Lower Bounds on the Depth of Integral ReLU Neural Networks via Lattice Polytopes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JLLTtEdh1ZY": {
    "title": "Wasserstein Auto-encoded MDPs: Formal Verification of Efficiently Distilled RL Policies with Many-sided Guarantees",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "48a56cdff6979009ab5f38596c68a810605556ab",
    "semantic_title": "wasserstein auto-encoded mdps: formal verification of efficiently distilled rl policies with many-sided guarantees",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=OTbRTIY4YS": {
    "title": "Global Explainability of GNNs via Logic Combination of Learned Concepts",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5bce30f98caa5decac63ef6cf58f5580b4c17883",
    "semantic_title": "global explainability of gnns via logic combination of learned concepts",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=JpRExTbl1-": {
    "title": "Gradient Gating for Deep Multi-Rate Learning on Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sKWlRDzPfd7": {
    "title": "MAESTRO: Open-Ended Environment Design for Multi-Agent Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gu-SC0dpkvw": {
    "title": "Almost Linear Constant-Factor Sketching for $\\ell_1$ and Logistic Regression",
    "volume": "poster",
    "abstract": "We improve upon previous oblivious sketching and turnstile streaming results for $\\ell_1$ and logistic regression, giving a much smaller sketching dimension achieving $O(1)$-approximation and yielding an efficient optimization problem in the sketch space. Namely, we achieve for any constant $c>0$ a sketching dimension of $\\tilde{O}(d^{1+c})$ for $\\ell_1$ regression and $\\tilde{O}(\\mu d^{1+c})$ for logistic regression, where $\\mu$ is a standard measure that captures the complexity of compressing the data. For $\\ell_1$-regression our sketching dimension is near-linear and improves previous work which either required $\\Omega(\\log d)$-approximation with this sketching dimension, or required a larger $\\operatorname{poly}(d)$ number of rows. Similarly, for logistic regression previous work had worse $\\operatorname{poly}(\\mu d)$ factors in its sketching dimension. We also give a tradeoff that yields a $1+\\varepsilon$ approximation in input sparsity time by increasing the total size to $(d\\log(n)/\\varepsilon)^{O(1/\\varepsilon)}$ for $\\ell_1$ and to $(\\mu d\\log(n)/\\varepsilon)^{O(1/\\varepsilon)}$ for logistic regression. Finally, we show that our sketch can be extended to approximate a regularized version of logistic regression where the data-dependent regularizer corresponds to the variance of the individual logistic losses",
    "checked": false,
    "id": "fa8c80bb3c44d5f4e8f6b8936f7bf9dbcc635eb8",
    "semantic_title": "almost linear constant-factor sketching for 𝓁1 and logistic regression",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=7tJyBmu9iCj": {
    "title": "Neural-based classification rule learning for sequential data",
    "volume": "poster",
    "abstract": "Discovering interpretable patterns for classification of sequential data is of key importance for a variety of fields, ranging from genomics to fraud detection or more generally interpretable decision-making. In this paper, we propose a novel differentiable fully interpretable method to discover both local and global patterns (i.e. catching a relative or absolute temporal dependency) for rule-based binary classification. It consists of a convolutional binary neural network with an interpretable neural filter and a training strategy based on dynamically-enforced sparsity. We demonstrate the validity and usefulness of the approach on synthetic datasets and on an open-source peptides dataset. Key to this end-to-end differentiable method is that the expressive patterns used in the rules are learned alongside the rules themselves",
    "checked": true,
    "id": "95b0634c6ff3fc372c36e380232d881f6af757f1",
    "semantic_title": "neural-based classification rule learning for sequential data",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=ORp91sAbzI": {
    "title": "Leveraging Unlabeled Data to Track Memorization",
    "volume": "poster",
    "abstract": "Deep neural networks may easily memorize noisy labels present in real-world data, which degrades their ability to generalize. It is therefore important to track and evaluate the robustness of models against noisy label memorization. We propose a metric, called $\\textit{susceptibility}$, to gauge such memorization for neural networks. Susceptibility is simple and easy to compute during training. Moreover, it does not require access to ground-truth labels and it only uses unlabeled data. We empirically show the effectiveness of our metric in tracking memorization on various architectures and datasets and provide theoretical insights into the design of the susceptibility metric. Finally, we show through extensive experiments on datasets with synthetic and real-world label noise that one can utilize susceptibility and the overall training accuracy to distinguish models that maintain a low memorization on the training set and generalize well to unseen clean data",
    "checked": true,
    "id": "e8bcdd6bb6e3692e760ae73e0d44458dd549d5ed",
    "semantic_title": "leveraging unlabeled data to track memorization",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=SmufNDN90G": {
    "title": "Policy-Based Self-Competition for Planning Problems",
    "volume": "poster",
    "abstract": "AlphaZero-type algorithms may stop improving on single-player tasks in case the value network guiding the tree search is unable to approximate the outcome of an episode sufficiently well. One technique to address this problem is transforming the single-player task through self-competition. The main idea is to compute a scalar baseline from the agent's historical performances and to reshape an episode's reward into a binary output, indicating whether the baseline has been exceeded or not. However, this baseline only carries limited information for the agent about strategies how to improve. We leverage the idea of self-competition and directly incorporate a historical policy into the planning process instead of its scalar performance. Based on the recently introduced Gumbel AlphaZero (GAZ), we propose our algorithm GAZ ‘Play-to-Plan' (GAZ PTP), in which the agent learns to find strong trajectories by planning against possible strategies of its past self. We show the effectiveness of our approach in two well-known combinatorial optimization problems, the Traveling Salesman Problem and the Job-Shop Scheduling Problem. With only half of the simulation budget for search, GAZ PTP consistently outperforms all selected single-player variants of GAZ",
    "checked": true,
    "id": "0d8742c18f5c0a509a52a76228c8a0d86a472d7d",
    "semantic_title": "policy-based self-competition for planning problems",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=KkazG4lgKL": {
    "title": "Out-of-Distribution Detection based on In-Distribution Data Patterns Memorization with Modern Hopfield Energy",
    "volume": "poster",
    "abstract": "Out-of-Distribution (OOD) detection is essential for safety-critical applications of deep neural networks. OOD detection is challenging since DNN models may produce very high logits value even for OOD samples. Hence, it is of great difficulty to discriminate OOD data by directly adopting Softmax on output logits as the confidence score. Differently, we detect the OOD sample with Hopfield energy in a store-then-compare paradigm. In more detail, penultimate layer outputs on the training set are considered as the representations of in-distribution (ID) data. Thus they can be transformed into stored patterns that serve as anchors to measure the discrepancy of unseen data for OOD detection. Starting from the energy function defined in Modern Hopfield Network for the discrepancy score calculation, we derive a simplified version SHE with theoretical analysis. In SHE, we utilize only one stored pattern to present each class, and these patterns can be obtained by simply averaging the penultimate layer outputs of training samples within this class. SHE has the advantages of hyperparameterfree and high computational efficiency. The evaluations of nine widely-used OOD datasets show the promising performance of such a simple yet effective approach and its superiority over State-of-the-Art models. Code is available at https://github.com/zjs975584714/SHE ood detection",
    "checked": true,
    "id": "cfd5982c81538f128bd7aa755929c8c0f4ee2bbe",
    "semantic_title": "out-of-distribution detection based on in-distribution data patterns memorization with modern hopfield energy",
    "citation_count": 70,
    "authors": []
  },
  "https://openreview.net/forum?id=Ki4ocDm364": {
    "title": "Scaling Pareto-Efficient Decision Making via Offline Multi-Objective RL",
    "volume": "poster",
    "abstract": "The goal of multi-objective reinforcement learning (MORL) is to learn policies that simultaneously optimize multiple competing objectives. In practice, an agent's preferences over the objectives may not be known apriori, and hence, we require policies that can generalize to arbitrary preferences at test time. In this work, we propose a new data-driven setup for offline MORL, where we wish to learn a preference-agnostic policy agent using only a finite dataset of offline demonstrations of other agents and their preferences. The key contributions of this work are two-fold. First, we introduce D4MORL, (D)atasets for MORL that are specifically designed for offline settings. It contains 1.8 million annotated demonstrations obtained by rolling out reference policies that optimize for randomly sampled preferences on 6 MuJoCo environments with 2-3 objectives each. Second, we propose Pareto-Efficient Decision Agents (PEDA), a family of offline MORL algorithms that builds and extends Decision Transformers via a novel preference-and-return-conditioned policy. Empirically, we show that PEDA closely approximates the behavioral policy on the D4MORL benchmark and provides an excellent approximation of the Pareto-front with appropriate conditioning, as measured by the hypervolume and sparsity metrics",
    "checked": true,
    "id": "cecc6b1f862bccf2bcc8071ca838161cd26eafe2",
    "semantic_title": "scaling pareto-efficient decision making via offline multi-objective rl",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=8KYeilT3Ow": {
    "title": "NAGphormer: A Tokenized Graph Transformer for Node Classification in Large Graphs",
    "volume": "poster",
    "abstract": "The graph Transformer emerges as a new architecture and has shown superior performance on various graph mining tasks. In this work, we observe that existing graph Transformers treat nodes as independent tokens and construct a single long sequence composed of all node tokens so as to train the Transformer model, causing it hard to scale to large graphs due to the quadratic complexity on the number of nodes for the self-attention computation. To this end, we propose a Neighborhood Aggregation Graph Transformer (NAGphormer) that treats each node as a sequence containing a series of tokens constructed by our proposed Hop2Token module. For each node, Hop2Token aggregates the neighborhood features from different hops into different representations and thereby produces a sequence of token vectors as one input. In this way, NAGphormer could be trained in a mini-batch manner and thus could scale to large graphs. Moreover, we mathematically show that as compared to a category of advanced Graph Neural Networks (GNNs), the decoupled Graph Convolutional Network, NAGphormer could learn more informative node representations from the multi-hop neighborhoods. Extensive experiments on benchmark datasets from small to large are conducted to demonstrate that NAGphormer consistently outperforms existing graph Transformers and mainstream GNNs. Code is available at https://github.com/JHL-HUST/NAGphormer",
    "checked": true,
    "id": "295526bc7ee119423af707e649cb55ab98854a47",
    "semantic_title": "nagphormer: a tokenized graph transformer for node classification in large graphs",
    "citation_count": 128,
    "authors": []
  },
  "https://openreview.net/forum?id=iYC5hOMqUg": {
    "title": "Bayesian Oracle for bounding information gain in neural encoding models",
    "volume": "poster",
    "abstract": "In recent years, deep learning models have set new standards in predicting neural population responses. Most of these models currently focus on predicting the mean response of each neuron for a given input. However, neural variability around this mean is not just noise and plays a central role in several theories on neural computation. To capture this variability, we need models that predict full response distributions for a given stimulus. However, to measure the quality of such models, commonly used correlation-based metrics are not sufficient as they mainly care about the mean of the response distribution. An interpretable alternative evaluation metric for likelihood-based models is \\textit{Information Gain} (IG) which evaluates the likelihood of a model relative to a lower and upper bound. However, while a lower bound is usually easy to obtain, constructing an upper bound turns out to be challenging for neural recordings with relatively low numbers of repeated trials, high (shared) variability, and sparse responses. In this work, we generalize the jack-knife oracle estimator for the mean---commonly used for correlation metrics---to a flexible Bayesian oracle estimator for IG based on posterior predictive distributions. We describe and address the challenges that arise when estimating the lower and upper bounds from small datasets. We then show that our upper bound estimate is data-efficient and robust even in the case of sparse responses and low signal-to-noise ratio. We further provide the derivation of the upper bound estimator for a variety of common distributions including the state-of-the-art zero-inflated mixture models, and relate IG to common mean-based metrics. Finally, we use our approach to evaluate such a mixture model resulting in $90\\%$ IG performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oztkQizr3kk": {
    "title": "$\\Lambda$-DARTS: Mitigating Performance Collapse by Harmonizing Operation Selection among Cells",
    "volume": "poster",
    "abstract": "Differentiable neural architecture search (DARTS) is a popular method for neural architecture search (NAS), which performs cell-search and utilizes continuous relaxation to improve the search efficiency via gradient-based optimization. The main shortcoming of DARTS is performance collapse, where the discovered architecture suffers from a pattern of declining quality during search. Performance collapse has become an important topic of research, with many methods trying to solve the issue through either regularization or fundamental changes to DARTS. However, the weight-sharing framework used for cell-search in DARTS and the convergence of architecture parameters has not been analyzed yet. In this paper, we provide a thorough and novel theoretical and empirical analysis on DARTS and its point of convergence. We show that DARTS suffers from a specific structural flaw due to its weight-sharing framework that limits the convergence of DARTS to saturation points of the softmax function. This point of convergence gives an unfair advantage to layers closer to the output in choosing the optimal architecture, causing performance collapse. We then propose two new regularization terms that aim to prevent performance collapse by harmonizing operation selection via aligning gradients of layers. Experimental results on six different search spaces and three different datasets show that our method ($\\Lambda$-DARTS) does indeed prevent performance collapse, providing justification for our theoretical analysis and the proposed remedy",
    "checked": false,
    "id": "426d452eb45ac5eca6ccc58b62bc78db3ec09ae8",
    "semantic_title": "λ-darts: mitigating performance collapse by harmonizing operation selection among cells",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=nYWqxUwFc3x": {
    "title": "Learning Vortex Dynamics for Fluid Inference and Prediction",
    "volume": "poster",
    "abstract": "We propose a novel differentiable vortex particle (DVP) method to infer and predict fluid dynamics from a single video. Lying at its core is a particle-based latent space to encapsulate the hidden, Lagrangian vortical evolution underpinning the observable, Eulerian flow phenomena. Our differentiable vortex particles are coupled with a learnable, vortex-to-velocity dynamics mapping to effectively capture the complex flow features in a physically-constrained, low-dimensional space. This representation facilitates the learning of a fluid simulator tailored to the input video that can deliver robust, long-term future predictions. The value of our method is twofold: first, our learned simulator enables the inference of hidden physics quantities (e.g., velocity field) purely from visual observation; secondly, it also supports future prediction, constructing the input video's sequel along with its future dynamics evolution. We compare our method with a range of existing methods on both synthetic and real-world videos, demonstrating improved reconstruction quality, visual plausibility, and physical integrity",
    "checked": true,
    "id": "f7e7f83b0287ab3cf2f6151a8a37f903f3ca4e7f",
    "semantic_title": "learning vortex dynamics for fluid inference and prediction",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=53FyUAdP7d": {
    "title": "Discovering Generalizable Multi-agent Coordination Skills from Multi-task Offline Data",
    "volume": "poster",
    "abstract": "Cooperative multi-agent reinforcement learning (MARL) faces the challenge of adapting to multiple tasks with varying agents and targets. Previous multi-task MARL approaches require costly interactions to simultaneously learn or fine-tune policies in different tasks. However, the situation that an agent should generalize to multiple tasks with only offline data from limited tasks is more in line with the needs of real-world applications. Since offline multi-task data contains a variety of behaviors, an effective data-driven approach is to extract informative latent variables that can represent universal skills for realizing coordination across tasks. In this paper, we propose a novel Offline MARL algorithm to Discover coordInation Skills (ODIS) from multi-task data. ODIS first extracts task-invariant coordination skills from offline multi-task data and learns to delineate different agent behaviors with the discovered coordination skills. Then we train a coordination policy to choose optimal coordination skills with the centralized training and decentralized execution paradigm. We further demonstrate that the discovered coordination skills can assign effective coordinative behaviors, thus significantly enhancing generalization to unseen tasks. Empirical results in cooperative MARL benchmarks, including the StarCraft multi-agent challenge, show that ODIS obtains superior performance in a wide range of tasks only with offline data from limited sources",
    "checked": true,
    "id": "cc95b268a5f0ac69af25194dc0c7b052ec8d2303",
    "semantic_title": "discovering generalizable multi-agent coordination skills from multi-task offline data",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=bLmSMXbqXr": {
    "title": "Quality-Similar Diversity via Population Based Reinforcement Learning",
    "volume": "poster",
    "abstract": "Diversity is a growing research topic in Reinforcement Learning (RL). Previous research on diversity has mainly focused on promoting diversity to encourage exploration and thereby improve quality (the cumulative reward), maximizing diversity subject to quality constraints, or jointly maximizing quality and diversity, known as the quality-diversity problem. In this work, we present the quality-similar diversity problem that features diversity among policies of similar qualities. In contrast to task-agnostic diversity, we focus on task-specific diversity defined by a set of user-specified Behavior Descriptors (BDs). A BD is a scalar function of a trajectory (e.g., the fire action rate for an Atari game), which delivers the type of diversity the user prefers. To derive the gradient of the user-specified diversity with respect to a policy, which is not trivially available, we introduce a set of BD estimators and connect it with the classical policy gradient theorem. Based on the diversity gradient, we develop a population-based RL algorithm to adaptively and efficiently optimize the population diversity at multiple quality levels throughout training. Extensive results on MuJoCo and Atari demonstrate that our algorithm significantly outperforms previous methods in terms of generating user-specified diverse policies across different quality levels",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M0_sUuEyHs": {
    "title": "Better Teacher Better Student: Dynamic Prior Knowledge for Knowledge Distillation",
    "volume": "poster",
    "abstract": "Knowledge distillation (KD) has shown very promising capabilities in transferring learning representations from large models (teachers) to small models (students). However, as the capacity gap between students and teachers becomes larger, existing KD methods fail to achieve better results. Our work shows that the 'prior knowledge' is vital to KD, especially when applying large teachers. Particularly, we propose the dynamic prior knowledge (DPK), which integrates part of teacher's features as the prior knowledge before the feature distillation. This means that our method also takes the teacher's feature as `input', not just `target'. Besides, we dynamically adjust the ratio of the prior knowledge during the training phase according to the feature gap, thus guiding the student in an appropriate difficulty. To evaluate the proposed method, we conduct extensive experiments on two image classification benchmarks (i.e. CIFAR100 and ImageNet) and an object detection benchmark (\\i.e. MS COCO). The results demonstrate the superiority of our method in performance under varying settings. Besides, our DPK makes the performance of the student model positively correlated with that of the teacher model, which means that we can further boost the accuracy of students by applying larger teachers. More importantly, DPK provides a fast solution in teacher model selection for any given model. Our codes will be publicly available for reproducibility",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rOFKmzNTbC": {
    "title": "Tensor-Based Sketching Method for the Low-Rank Approximation of Data Streams",
    "volume": "poster",
    "abstract": "Low-rank approximation in data streams is a fundamental and significant task in computing science, machine learning and statistics. Multiple streaming algorithms have emerged over years and most of them are inspired by randomized algorithms, more specifically, sketching methods. However, many algorithms are not able to leverage information of data streams and consequently suffer from low accuracy. Existing data-driven methods improve accuracy but the training cost is expensive in practice. In this paper, from a subspace perspective, we propose a tensor-based sketching method for low-rank approximation of data streams. The proposed algorithm fully exploits the structure of data streams and obtains quasi-optimal sketching matrices by performing tensor decomposition on training data. A series of experiments are carried out and show that the proposed tensor-based method can be more accurate and much faster than the previous work",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cEygmQNOeI": {
    "title": "Language Models are Realistic Tabular Data Generators",
    "volume": "poster",
    "abstract": "Tabular data is among the oldest and most ubiquitous forms of data. However, the generation of synthetic samples with the original data's characteristics remains a significant challenge for tabular data. While many generative models from the computer vision domain, such as variational autoencoders or generative adversarial networks, have been adapted for tabular data generation, less research has been directed towards recent transformer-based large language models (LLMs), which are also generative in nature. To this end, we propose GReaT (Generation of Realistic Tabular data), which exploits an auto-regressive generative LLM to sample synthetic and yet highly realistic tabular data. Furthermore, GReaT can model tabular data distributions by conditioning on any subset of features; the remaining features are sampled without additional overhead. We demonstrate the effectiveness of the proposed approach in a series of experiments that quantify the validity and quality of the produced data samples from multiple angles. We find that GReaT maintains state-of-the-art performance across numerous real-world and synthetic data sets with heterogeneous feature types coming in various sizes",
    "checked": true,
    "id": "394bd431e522b86581086bcb5cd9be161cf1cdf4",
    "semantic_title": "language models are realistic tabular data generators",
    "citation_count": 275,
    "authors": []
  },
  "https://openreview.net/forum?id=y4uc4NtTWaq": {
    "title": "Data augmentation alone can improve adversarial training",
    "volume": "poster",
    "abstract": "Adversarial training suffers from the issue of robust overfitting, which seriously impairs its generalization performance. Data augmentation, which is effective at preventing overfitting in standard training, has been observed by many previous works to be ineffective in mitigating overfitting in adversarial training. This work proves that, contrary to previous findings, data augmentation alone can significantly boost accuracy and robustness in adversarial training. We find that the hardness and the diversity of data augmentation are important factors in combating robust overfitting. In general, diversity can improve both accuracy and robustness, while hardness can boost robustness at the cost of accuracy within a certain limit and degrade them both over that limit. To mitigate robust overfitting, we first propose a new crop transformation Cropshift with improved diversity compared to the conventional one (Padcrop). We then propose a new data augmentation scheme, based on Cropshift, with much improved diversity and well-balanced hardness. Empirically, our augmentation method achieves the state-of-the-art accuracy and robustness for data augmentations in adversarial training. Furthermore, it matches, or even exceeds when combined with weight averaging, the performance of the best contemporary regularization methods for alleviating robust overfitting",
    "checked": true,
    "id": "68be99a37c23486fcdd016fdf7833bb9092146ae",
    "semantic_title": "data augmentation alone can improve adversarial training",
    "citation_count": 55,
    "authors": []
  },
  "https://openreview.net/forum?id=UG8bQcD3Emv": {
    "title": "CUTS: Neural Causal Discovery from Irregular Time-Series Data",
    "volume": "poster",
    "abstract": "Causal discovery from time-series data has been a central task in machine learning. Recently, Granger causality inference is gaining momentum due to its good explainability and high compatibility with emerging deep neural networks. However, most existing methods assume structured input data and degenerate greatly when encountering data with randomly missing entries or non-uniform sampling frequencies, which hampers their applications in real scenarios. To address this issue, here we present CUTS, a neural Granger causal discovery algorithm to jointly impute unobserved data points and build causal graphs, via plugging in two mutually boosting modules in an iterative framework: (i) Latent data prediction stage: designs a Delayed Supervision Graph Neural Network (DSGNN) to hallucinate and register unstructured data which might be of high dimension and with complex distribution; (ii) Causal graph fitting stage: builds a causal adjacency matrix with imputed data under sparse penalty. Experiments show that CUTS effectively infers causal graphs from irregular time-series data, with significantly superior performance to existing methods. Our approach constitutes a promising step towards applying causal discovery to real applications with non-ideal observations",
    "checked": true,
    "id": "ce3a8101612d9adf68236a43e165ca08649c9611",
    "semantic_title": "cuts: neural causal discovery from irregular time-series data",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=OOWLRfAI_V_": {
    "title": "Quantized Compressed Sensing with Score-Based Generative Models",
    "volume": "poster",
    "abstract": "We consider the general problem of recovering a high-dimensional signal from noisy quantized measurements. Quantization, especially coarse quantization such as 1-bit sign measurements, leads to severe information loss and thus a good prior knowledge of the unknown signal is helpful for accurate recovery. Motivated by the power of score-based generative models (SGM, also known as diffusion models) in capturing the rich structure of natural signals beyond simple sparsity, we propose an unsupervised data-driven approach called quantized compressed sensing with SGM (QCS-SGM), where the prior distribution is modeled by a pre-trained SGM. To perform posterior sampling, an annealed pseudo-likelihood score called ${\\textit{noise perturbed pseudo-likelihood score}}$ is introduced and combined with the prior score of SGM. The proposed QCS-SGM applies to an arbitrary number of quantization bits. Experiments on a variety of baseline datasets demonstrate that the proposed QCS-SGM significantly outperforms existing state-of-the-art algorithms by a large margin for both in-distribution and out-of-distribution samples. Moreover, as a posterior sampling method, QCS-SGM can be easily used to obtain confidence intervals or uncertainty estimates of the reconstructed results. $\\textit{The code is available at}$ https://github.com/mengxiangming/QCS-SGM",
    "checked": true,
    "id": "8ead85160abac1795e6926abb1a3d4c40f143a14",
    "semantic_title": "quantized compressed sensing with score-based generative models",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=qihMOPw4Sf_": {
    "title": "Valid P-Value for Deep Learning-driven Salient Region",
    "volume": "poster",
    "abstract": "Various saliency map methods have been proposed to interpret and explain predictions of deep learning models. Saliency maps allow us to interpret which parts of the input signals have a strong influence on the prediction results. However, since a saliency map is obtained by complex computations in deep learning models, it is often difficult to know how reliable the saliency map itself is. In this study, we propose a method to quantify the reliability of a saliency region in the form of p-values. Our idea is to consider a saliency map as a selected hypothesis by the trained deep learning model and employ the selective inference framework. The proposed method provably provides a valid p-value for the detected salient region, i.e., we can provably control the false positive rate of the detected salient region. We demonstrate the validity of the proposed method through numerical examples in synthetic and real datasets. Furthermore, we develop a Keras-based framework for conducting the proposed selective inference for a wide class of CNNs without additional implementation cost",
    "checked": true,
    "id": "127db6eee606e5efc2dc26d76e952f610db0dfaf",
    "semantic_title": "valid p-value for deep learning-driven salient region",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=yf1icZHC-l9": {
    "title": "Complexity-Based Prompting for Multi-step Reasoning",
    "volume": "poster",
    "abstract": "We study the task of prompting large-scale language models to perform multi-step reasoning. Existing work shows that when prompted with a chain of thoughts (CoT), sequences of short sentences describing intermediate reasoning steps towards a final answer, large language models can generate new reasoning chains and predict answers for new inputs. A central question is which reasoning examples make the most effective prompts. In this work, we propose complexity-based prompting, a simple and effective example selection scheme for multi-step reasoning. We show that prompts with higher reasoning complexity, i.e., chains with more reasoning steps, achieve substantially better performance on math word reasoning tasks over strong baselines. We further extend our complexity-based criteria from prompting (selecting inputs) to decoding (selecting outputs), where we sample multiple reasoning chains from the model, then choose the majority of generated answers from complex reasoning chains (over simple chains). When used to prompt GPT-3, our approach substantially improves multi-step reasoning accuracy, with an 8.6% absolute improvement on GSM8K, and 6.4% on MathQA. Compared with existing example selection schemes like manual tuning or retrieval-based selection, selection based on reasoning complexity is intuitive, easy to implement, and annotation-efficient. Further results demonstrate the robustness of performance gains from complex prompts under format perturbation and distribution shift",
    "checked": true,
    "id": "c88cafa3e980765a64febe369ceb7c2aa7261d2a",
    "semantic_title": "complexity-based prompting for multi-step reasoning",
    "citation_count": 462,
    "authors": []
  },
  "https://openreview.net/forum?id=mXPoBtnpMnuy": {
    "title": "Unsupervised 3D Object Learning through Neuron Activity aware Plasticity",
    "volume": "poster",
    "abstract": "We present an unsupervised deep learning model for 3D object classification. Conventional Hebbian learning, a well-known unsupervised model, suffers from loss of local features leading to reduced performance for tasks with complex geometric objects. We present a deep network with a novel Neuron Activity Aware (NeAW) Hebbian learning rule that dynamically switches the neurons to be governed by Hebbian learning or anti-Hebbian learning, depending on its activity. We analytically show that NeAW Hebbian learning relieves the bias in neuron activity, allowing more neurons to attend to the representation of the 3D objects. Empirical results show that the NeAW Hebbian learning outperforms other variants of Hebbian learning and shows higher accuracy over fully supervised models when training data is limited",
    "checked": true,
    "id": "62c6c28691a93e1504279d752e3230538254917a",
    "semantic_title": "unsupervised 3d object learning through neuron activity aware plasticity",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=8IN-qLkl215": {
    "title": "Visually-Augmented Language Modeling",
    "volume": "poster",
    "abstract": "Human language is grounded on multimodal knowledge including visual knowledge like colors, sizes, and shapes. However, current large-scale pre-trained language models rely on the text-only self-supervised training with massive text data, which precludes them from utilizing relevant visual information when necessary. To address this, we propose a novel pre-training framework, named VaLM, to Visually-augment text tokens with retrieved relevant images for Language Modeling. Specifically, VaLM builds on a novel latent text-image alignment method via an image retrieval module to fetch corresponding images given a textual context. With the visually-augmented context, VaLM uses a visual knowledge fusion layer to enable multimodal grounded language modeling by attending on both text context and visual knowledge in images. We evaluate VaLM on various visual knowledge intensive commonsense reasoning tasks, which require visual information to excel. The experimental results illustrate that VaLM outperforms all strong language-only and vision-language baselines with substantial gains on reasoning object commonsense including color, size, and shape",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XrgjF5-M3xi": {
    "title": "Incremental Learning of Structured Memory via Closed-Loop Transcription",
    "volume": "poster",
    "abstract": "This work proposes a minimal computational model for learning structured memories of multiple object classes in an incremental setting. Our approach is based on establishing a {\\em closed-loop transcription} between the classes and a corresponding set of subspaces, known as a linear discriminative representation, in a low-dimensional feature space. Our method is simpler than existing approaches for incremental learning, and more efficient in terms of model size, storage, and computation: it requires only a single, fixed-capacity autoencoding network with a feature space that is used for both discriminative and generative purposes. Network parameters are optimized simultaneously without architectural manipulations, by solving a constrained minimax game between the encoding and decoding maps over a single rate reduction-based objective. Experimental results show that our method can effectively alleviate catastrophic forgetting, achieving significantly better performance than prior work of generative replay on MNIST, CIFAR-10, and ImageNet-50, despite requiring fewer resources",
    "checked": true,
    "id": "918ea9b6a17fc4e25087232b753e37a382c98445",
    "semantic_title": "incremental learning of structured memory via closed-loop transcription",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=lMO7TC7cuuh": {
    "title": "When Data Geometry Meets Deep Function: Generalizing Offline Reinforcement Learning",
    "volume": "poster",
    "abstract": "In offline reinforcement learning (RL), one detrimental issue to policy learning is the error accumulation of deep \\textit{Q} function in out-of-distribution (OOD) areas. Unfortunately, existing offline RL methods are often over-conservative, inevitably hurting generalization performance outside data distribution. In our study, one interesting observation is that deep \\textit{Q} functions approximate well inside the convex hull of training data. Inspired by this, we propose a new method, \\textit{DOGE (Distance-sensitive Offline RL with better GEneralization)}. DOGE marries dataset geometry with deep function approximators in offline RL, and enables exploitation in generalizable OOD areas rather than strictly constraining policy within data distribution. Specifically, DOGE trains a state-conditioned distance function that can be readily plugged into standard actor-critic methods as a policy constraint. Simple yet elegant, our algorithm enjoys better generalization compared to state-of-the-art methods on D4RL benchmarks. Theoretical analysis demonstrates the superiority of our approach to existing methods that are solely based on data distribution or support constraints. Code is available at https://github.com/Facebear-ljx/DOGE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sVzBN-DlJRi": {
    "title": "Budgeted Training for Vision Transformer",
    "volume": "poster",
    "abstract": "The superior performances of Vision Transformers often come with higher training costs. Compared to their CNN counterpart, Transformer models are hungry for large-scale data and their training schedules are usually prolonged. This sets great restrictions on training Transformers with limited resources, where a proper trade-off between training cost and model performance is longed. In this paper, we address the problem by proposing a framework that enables the training process under \\textit{any training budget} from the perspective of model structure, while achieving competitive model performances. Specifically, based on the observation that Transformer exhibits different levels of model redundancies at different training stages, we propose to dynamically control the activation rate of the model structure along the training process and meet the demand on the training budget by adjusting the duration on each level of model complexity. Extensive experiments demonstrate that our framework is applicable to various Vision Transformers, and achieves competitive performances on a wide range of training budgets",
    "checked": true,
    "id": "a40f5b9acd81fad048a97562336b46d04cde4023",
    "semantic_title": "budgeted training for vision transformer",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=4rXMRuoJlai": {
    "title": "Mind's Eye: Grounded Language Model Reasoning through Simulation",
    "volume": "poster",
    "abstract": "Successful and effective communication between humans and AI relies on a shared experience of the world. By training solely on written text, current language models (LMs) miss the grounded experience of humans in the real-world---their failure to relate language to the physical world causes knowledge to be misrepresented and obvious mistakes in their reasoning. We present Mind's Eye, a paradigm to ground language model reasoning in the physical world. Given a physical reasoning question, we use a computational physics engine (DeepMind's MuJoCo) to simulate the possible outcomes, and then use the simulation results as part of the input, which enables language models to perform reasoning. Experiments on 39 tasks in a physics alignment benchmark demonstrate that Mind's Eye can improve reasoning ability by a large margin (27.9% zero-shot, and 46.0% few-shot absolute accuracy improvement on average). Smaller language models armed with Mind's Eye can obtain similar performance to models that are 100x larger. Finally, we confirm the robustness of Mind's Eye through ablation studies",
    "checked": true,
    "id": "2cb97b2bb6ab2eb17add9ffe69d5cbeaca2b29c8",
    "semantic_title": "mind's eye: grounded language model reasoning through simulation",
    "citation_count": 84,
    "authors": []
  },
  "https://openreview.net/forum?id=azCKuYyS74": {
    "title": "What Do Self-Supervised Vision Transformers Learn?",
    "volume": "poster",
    "abstract": "We present a comparative study on how and why contrastive learning (CL) and masked image modeling (MIM) differ in their representations and in their performance of downstream tasks. In particular, we demonstrate that self-supervised Vision Transformers (ViTs) have the following properties: (1) CL trains self-attentions to capture longer-range global patterns than MIM, such as the shape of an object, especially in the later layers of the ViT architecture. This CL property helps ViTs linearly separate images in their representation spaces. However, it also makes the self-attentions collapse into homogeneity for all query tokens and heads. Such homogeneity of self-attention reduces the diversity of representations, worsening scalability and dense prediction performance. (2) CL utilizes the low-frequency signals of the representations, but MIM utilizes high-frequencies. Since low- and high-frequency information respectively represent shapes and textures, CL is more shape-oriented and MIM more texture-oriented. (3) CL plays a crucial role in the later layers, while MIM mainly focuses on the early layers. Upon these analyses, we find that CL and MIM can complement each other and observe that even the simplest harmonization can help leverage the advantages of both methods. The code is available at https://github.com/naver-ai/cl-vs-mim",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fB4V-2QvCEm": {
    "title": "Population-size-Aware Policy Optimization for Mean-Field Games",
    "volume": "poster",
    "abstract": "In this work, we attempt to bridge the two fields of finite-agent and infinite-agent games, by studying how the optimal policies of agents evolve with the number of agents (population size) in mean-field games, an agent-centric perspective in contrast to the existing works focusing typically on the convergence of the empirical distribution of the population. To this end, the premise is to obtain the optimal policies of a set of finite-agent games with different population sizes. However, either deriving the closed-form solution for each game is theoretically intractable, training a distinct policy for each game is computationally intensive, or directly applying the policy trained in a game to other games is sub-optimal. We address these challenges through the \\textbf{P}opulation-size-\\textbf{A}ware \\textbf{P}olicy \\textbf{O}ptimization (PAPO). Our contributions are three-fold. First, to efficiently generate efficient policies for games with different population sizes, we propose PAPO, which unifies two natural options (augmentation and hypernetwork) and achieves significantly better performance. PAPO consists of three components: i) the population-size encoding which transforms the original value of population size to an equivalent encoding to avoid training collapse, ii) a hypernetwork to generate a distinct policy for each game conditioned on the population size, and iii) the population size as an additional input to the generated policy. Next, we construct a multi-task-based training procedure to efficiently train the neural networks of PAPO by sampling data from multiple games with different population sizes. Finally, extensive experiments on multiple environments show the significant superiority of PAPO over baselines, and the analysis of the evolution of the generated policies further deepens our understanding of the two fields of finite-agent and infinite-agent games",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qs2YCziX2o-": {
    "title": "On The Relative Error of Random Fourier Features for Preserving Kernel Distance",
    "volume": "poster",
    "abstract": "The method of random Fourier features (RFF), proposed in a seminal paper by Rahimi and Recht (NIPS'07), is a powerful technique to find approximate low-dimensional representations of points in (high-dimensional) kernel space, for shift-invariant kernels. While RFF has been analyzed under various notions of error guarantee, the ability to preserve the kernel distance with \\emph{relative} error is less understood. We show that for a significant range of kernels, including the well-known Laplacian kernels, RFF cannot approximate the kernel distance with small relative error using low dimensions. We complement this by showing as long as the shift-invariant kernel is analytic, RFF with $\\mathrm{poly}(\\epsilon^{-1} \\log n)$ dimensions achieves $\\epsilon$-relative error for pairwise kernel distance of $n$ points, and the dimension bound is improved to $\\mathrm{poly}(\\epsilon^{-1}\\log k)$ for the specific application of kernel $k$-means. Finally, going beyond RFF, we make the first step towards data-oblivious dimension-reduction for general shift-invariant kernels, and we obtain a similar $\\mathrm{poly}(\\epsilon^{-1} \\log n)$ dimension bound for Laplacian kernels. We also validate the dimension-error tradeoff of our methods on simulated datasets, and they demonstrate superior performance compared with other popular methods including random-projection and Nystr\\\"{o}m methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sE7-XhLxHA": {
    "title": "DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing",
    "volume": "poster",
    "abstract": "This paper presents a new pre-trained language model, NewModel, which improves the original DeBERTa model by replacing mask language modeling (MLM) with replaced token detection (RTD), a more sample-efficient pre-training task. Our analysis shows that vanilla embedding sharing in ELECTRA hurts training efficiency and model performance. This is because the training losses of the discriminator and the generator pull token embeddings in different directions, creating the \"tug-of-war\" dynamics. We thus propose a new gradient-disentangled embedding sharing method that avoids the tug-of-war dynamics, improving both training efficiency and the quality of the pre-trained model. We have pre-trained NewModel using the same settings as DeBERTa to demonstrate its exceptional performance on a wide range of downstream natural language understanding (NLU) tasks. Taking the GLUE benchmark with eight tasks as an example, the NewModel Large model achieves a 91.37% average score, which is 1.37% over DeBERTa and 1.91% over ELECTRA, setting a new state-of-the-art (SOTA) among the models with a similar structure. Furthermore, we have pre-trained a multi-lingual model mNew-Model and observed a larger improvement over strong baselines compared to English models. For example, the mNewModel Base achieves a 79.8% zero-shot cross-lingual accuracy on XNLI and a 3.6% improvement over XLM-R Base, creating a new SOTA on this benchmark. We will make our model and code publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z_tmYu060Kr": {
    "title": "Squeeze Training for Adversarial Robustness",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7e9e38a9101fd6e9ee3cd44f72d9be6842b7a42d",
    "semantic_title": "squeeze training for adversarial robustness",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=MofT9KEF0kw": {
    "title": "Pushing the Accuracy-Group Robustness Frontier with Introspective Self-play",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b1ab0635586fc7677f9a54590e56dcb0717db664",
    "semantic_title": "pushing the accuracy-group robustness frontier with introspective self-play",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=n-hKHMzBgy": {
    "title": "Max-Margin Works while Large Margin Fails: Generalization without Uniform Convergence",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oGVu9spZaJJ": {
    "title": "Asymptotic Instance-Optimal Algorithms for Interactive Decision Making",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SNwH0dDGl7_": {
    "title": "Near-Optimal Deployment Efficiency in Reward-Free Reinforcement Learning with Linear Function Approximation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "88298748d970690e09fe6043d68125b93ccce89d",
    "semantic_title": "near-optimal deployment efficiency in reward-free reinforcement learning with linear function approximation",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=k5PEHHY4spM": {
    "title": "An Equal-Size Hard EM Algorithm for Diverse Dialogue Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=04K3PMtMckp": {
    "title": "The hidden uniform cluster prior in self-supervised learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8be831a07abe8c5475c2bd91cc41bf4c1c2be771",
    "semantic_title": "the hidden uniform cluster prior in self-supervised learning",
    "citation_count": 54,
    "authors": []
  },
  "https://openreview.net/forum?id=sXfWoK4KvSW": {
    "title": "Long-Tailed Partial Label Learning via Dynamic Rebalancing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QrnDe_9ZFd8": {
    "title": "Task Ambiguity in Humans and Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z92lBy1ehjI": {
    "title": "Winning Both the Accuracy of Floating Point Activation and the Simplicity of Integer Arithmetic",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Peot1SFDX0": {
    "title": "Preference Transformer: Modeling Human Preferences using Transformers for RL",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c6478decdfff11ccbd085967c2f83aea11927a46",
    "semantic_title": "preference transformer: modeling human preferences using transformers for rl",
    "citation_count": 79,
    "authors": []
  },
  "https://openreview.net/forum?id=znLlSgN-4S0": {
    "title": "More Centralized Training, Still Decentralized Execution: Multi-Agent Conditional Policy Factorization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bc789d637841e01efe283ab2a5d798b0141dec6e",
    "semantic_title": "more centralized training, still decentralized execution: multi-agent conditional policy factorization",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=2YQrqe4RNv": {
    "title": "Edgeformers: Graph-Empowered Transformers for Representation Learning on Textual-Edge Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lEkl0jdSb7B": {
    "title": "Any-scale Balanced Samplers for Discrete Space",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "09c2aa9f94e10f05551382347ea3e17950c8755e",
    "semantic_title": "any-scale balanced samplers for discrete space",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=4MbGnp4iPQ": {
    "title": "Equivariant Shape-Conditioned Generation of 3D Molecules for Ligand-Based Drug Design",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7a78ce6533d26135b087c7c85f79749f09c0d2d0",
    "semantic_title": "equivariant shape-conditioned generation of 3d molecules for ligand-based drug design",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=rVM8wD2G7Dy": {
    "title": "Imbalanced Semi-supervised Learning with Bias Adaptive Classifier",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "cad82941268526b364b8b16d374aacf4573bc4eb",
    "semantic_title": "imbalanced semi-supervised learning with bias adaptive classifier",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=rJcLocAJpA6": {
    "title": "On Compositional Uncertainty Quantification for Seq2seq Graph Parsing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GPTjnA57h_3": {
    "title": "Free Lunch for Domain Adversarial Training: Environment Label Smoothing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JxpBP1JM15-": {
    "title": "Scaling Forward Gradient With Local Losses",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7a31b37b4844653e084ff096f1ec5861b95389a9",
    "semantic_title": "scaling forward gradient with local losses",
    "citation_count": 56,
    "authors": []
  },
  "https://openreview.net/forum?id=ugA1HX69sf": {
    "title": "Understanding Embodied Reference with Touch-Line Transformer",
    "volume": "poster",
    "abstract": "We study embodied reference understanding, the task of locating referents using embodied gestural signals and language references. Human studies have revealed that, contrary to popular belief, objects referred to or pointed to do not lie on the elbow-wrist line, but rather on the so-called virtual touch line. Nevertheless, contemporary human pose representations lack the virtual touch line. To tackle this problem, we devise the touch-line Transformer: It takes as input tokenized visual and textual features and simultaneously predicts the referent's bounding box and a touch-line vector. Leveraging this touch-line prior, we further devise a geometric consistency loss that promotes co-linearity between referents and touch lines. Using the touch line as gestural information dramatically improves model performances: Experiments on the YouRefIt dataset demonstrate that our method yields a +25.0% accuracy improvement under the 0.75 IoU criterion, hence closing 63.6% of the performance difference between models and humans. Furthermore, we computationally validate prior human studies by demonstrating that computational models more accurately locate referents when employing the virtual touch line than when using the elbow-wrist line",
    "checked": true,
    "id": "43b529f03d32d0e4a2830ac6c3bc066cee733bf8",
    "semantic_title": "understanding embodied reference with touch-line transformer",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=wzlWiO_WY4": {
    "title": "Calibration Matters: Tackling Maximization Bias in Large-scale Advertising Recommendation Systems",
    "volume": "poster",
    "abstract": "Calibration is defined as the ratio of the average predicted click rate to the true click rate. The optimization of calibration is essential to many online advertising recommendation systems because it directly affects the downstream bids in ads auctions and the amount of money charged to advertisers. Despite its importance, calibration often suffers from a problem called \"maximization bias\". Maximization bias refers to the phenomenon that the maximum of predicted values overestimates the true maximum. The problem is introduced because the calibration is computed on the set selected by the prediction model itself. It persists even if unbiased predictions are achieved on every datapoint and worsens when covariate shifts exist between the training and test sets. To mitigate this problem, we quantify maximization bias and propose a variance-adjusting debiasing (VAD) meta-algorithm in this paper. The algorithm is efficient, robust, and practical as it is able to mitigate maximization bias problem under covariate shifts, without incurring additional online serving costs or compromising the ranking performance. We demonstrate the effectiveness of the proposed algorithm using a state-of-the-art recommendation neural network model on a large-scale real-world dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cJWxqmmDL2b": {
    "title": "Memorization-Dilation: Modeling Neural Collapse Under Noise",
    "volume": "poster",
    "abstract": "The notion of neural collapse refers to several emergent phenomena that have been empirically observed across various canonical classification problems. During the terminal phase of training a deep neural network, the feature embedding of all examples of the same class tend to collapse to a single representation, and the features of different classes tend to separate as much as possible. Neural collapse is often studied through a simplified model, called the layer-peeled model, in which the network is assumed to have ``infinite expressivity'' and can map each data point to any arbitrary representation. In this work we study a more realistic variant of the layer-peeled model, which takes the positivity of the features into account. Furthermore, we extend this model to also incorporate the limited expressivity of the network. Empirical evidence suggests that the memorization of noisy data points leads to a degradation (dilation) of the neural collapse. Using a model of the memorization-dilation (M-D) phenomenon, we show one mechanism by which different losses lead to different performances of the trained network on noisy data. Our proofs reveal why label smoothing, a modification of cross-entropy empirically observed to produce a regularization effect, leads to improved generalization in classification tasks",
    "checked": true,
    "id": "ef5dd59b4e1effa613e9c5b686f9e08970a0ba21",
    "semantic_title": "memorization-dilation: modeling neural collapse under noise",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=qV_M_rhYajc": {
    "title": "Spacetime Representation Learning",
    "volume": "poster",
    "abstract": "Much of the data we encounter in the real world can be represented as directed graphs. In this work, we introduce a general family of representations for directed graphs through connected time-oriented Lorentz manifolds, called \"spacetimes\" in general relativity. Spacetimes intrinsically contain a causal structure that indicates whether or not there exists a causal or even chronological order between points of the manifold, called events. This chronological order allows us to naturally represent directed edges via imposing the correct ordering when the nodes are embedded as events in the spacetime. Previous work in machine learning only considers embeddings lying on the simplest Lorentz manifold or does not exploit the connection between Lorentzian pre-length spaces and directed graphs. We introduce a well-defined approach to map data onto a general family of spacetimes. We empirically evaluate our framework in the tasks of hierarchy extraction of undirected graphs, directed link prediction and representation of directed graphs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lid14UkLPd4": {
    "title": "Learning to Extrapolate: A Transductive Approach",
    "volume": "poster",
    "abstract": "Machine learning systems, especially with overparameterized deep neural networks, can generalize to novel test instances drawn from the same distribution as the training data. However, they fare poorly when evaluated on out-of-support test points. In this work, we tackle the problem of developing machine learning systems that retain the power of overparameterized function approximators while enabling extrapolation to out-of-support test points when possible. This is accomplished by noting that under certain conditions, a \"transductive\" reparameterization can convert an out-of-support extrapolation problem into a problem of within-support combinatorial generalization. We propose a simple strategy based on bilinear embeddings to enable this type of combinatorial generalization, thereby addressing the out-of-support extrapolation problem under certain conditions. We instantiate a simple, practical algorithm applicable to various supervised learning and imitation learning tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FlCg47MNvBA": {
    "title": "Label-free Concept Bottleneck Models",
    "volume": "poster",
    "abstract": "Concept bottleneck models (CBM) are a popular way of creating more interpretable neural networks by having hidden layer neurons correspond to human-understandable concepts. However, existing CBMs and their variants have two crucial limitations: first, they need to collect labeled data for each of the predefined concepts, which is time consuming and labor intensive; second, the accuracy of a CBM is often significantly lower than that of a standard neural network, especially on more complex datasets. This poor performance creates a barrier for adopting CBMs in practical real world applications. Motivated by these challenges, we propose Label-free CBM which is a novel framework to transform any neural network into an interpretable CBM without labeled concept data, while retaining a high accuracy. Our Label-free CBM has many advantages, it is: scalable - we present the first CBM scaled to ImageNet, efficient - creating a CBM takes only a few hours even for very large datasets, and automated - training it for a new dataset requires minimal human effort. Our code is available at https://github.com/Trustworthy-ML-Lab/Label-free-CBM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XGagtiJ8XC": {
    "title": "Multi-level Protein Structure Pre-training via Prompt Learning",
    "volume": "poster",
    "abstract": "A protein can focus on different structure levels to implement its functions. Each structure has its own merit and driving forces in describing some specific characteristics, and they cannot replace each other. Most existing function prediction methods take the tertiary structure as input, unintentionally ignoring the other levels of protein structures. Considering protein sequences can determine multi-level structures, in this paper, we aim to realize the comprehensive potential of protein sequences for function prediction. Specifically, we propose a new prompt-guided multi-task pre-training and fine-tuning framework, and the resulting protein model is called PromptProtein. Through the prompt-guided multi-task pre-training, we learn multiple prompt signals to steer the model to focus on different structure levels. We also design a prompt fine-tuning module to provide downstream tasks the on-demand flexibility of utilizing respective levels of structure information. Extensive experiments on function prediction and protein engineering show that PromptProtein outperforms state-of-the-art methods by large margins",
    "checked": true,
    "id": "785628876bccf1b72d6e78c0972289f4026ccfda",
    "semantic_title": "multi-level protein structure pre-training via prompt learning",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=-Aw0rrrPUF": {
    "title": "GLM-130B: An Open Bilingual Pre-trained Model",
    "volume": "poster",
    "abstract": "We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It is an attempt to open-source a 100B-scale model as good as GPT-3 (davinci) and unveil how models of such a scale can be successfully pre-trained. Over the course of this effort, we face numerous unexpected technical and engineering challenges, particularly on loss spikes and divergence. In this paper, we introduce the pre-training process of GLM-130B including its design choices, training strategies for both efficiency and stability, and engineering efforts. The resultant GLM-130B model offers significant outperformance over GPT-3 175B on a wide range of popular English benchmarks while the performance advantage is not observed in OPT-175B and BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN 3.0 260B—the largest Chinese language model—across related benchmarks. Finally, we leverage a unique scaling property of GLM-130B to reach INT4 quantization with almost no performance loss, making it the first among 100B-scale models and more importantly, allowing its effective inference on 4×RTX 3090 (24G) or 8×RTX 2080 Ti (11G) GPUs, the most ever affordable GPUs required for using 100B-scale models. The GLM-130B model weights are publicly accessible and its code, training logs, related toolkit, and lessons learned are open-sourced at https://github.com/THUDM/GLM-130B/",
    "checked": true,
    "id": "1d26c947406173145a4665dd7ab255e03494ea28",
    "semantic_title": "glm-130b: an open bilingual pre-trained model",
    "citation_count": 1132,
    "authors": []
  },
  "https://openreview.net/forum?id=Ha2MnQM9Ph": {
    "title": "Causal Estimation for Text Data with (Apparent) Overlap Violations",
    "volume": "poster",
    "abstract": "Consider the problem of estimating the causal effect of some attribute of a text document; for example: what effect does writing a polite vs. rude email have on response time? To estimate a causal effect from observational data, we need to adjust for confounding aspects of the text that affect both the treatment and outcome---e.g., the topic or writing level of the text. These confounding aspects are unknown a priori, so it seems natural to adjust for the entirety of the text (e.g., using a transformer). However, causal identification and estimation procedures rely on the assumption of overlap: for all levels of the adjustment variables, there is randomness leftover so that every unit could have (not) received treatment. Since the treatment here is itself an attribute of the text, it is perfectly determined, and overlap is apparently violated. The purpose of this paper is to show how to handle causal identification and obtain robust causal estimation in the presence of apparent overlap violations. In brief, the idea is to use supervised representation learning to produce a data representation that preserves confounding information while eliminating information that is only predictive of the treatment. This representation then suffices for adjustment and satisfies overlap. Adapting results on non-parametric estimation, we show that this procedure shows robustness with respect to conditional outcome misestimation and yields a low-bias estimator that admits valid uncertainty quantification under weak conditions. Empirical results show reductions in bias and strong improvements in uncertainty quantification relative to the natural (transformer-based) baseline",
    "checked": true,
    "id": "5cd83a86584065c82837cd9e73fc844fb2b2e18e",
    "semantic_title": "causal estimation for text data with (apparent) overlap violations",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=JdTnc9gjVfJ": {
    "title": "MoDem: Accelerating Visual Model-Based Reinforcement Learning with Demonstrations",
    "volume": "poster",
    "abstract": "Poor sample efficiency continues to be the primary challenge for deployment of deep Reinforcement Learning (RL) algorithms for real-world applications, and in particular for visuo-motor control. Model-based RL has the potential to be highly sample efficient by concurrently learning a world model and using synthetic rollouts for planning and policy improvement. However, in practice, sample-efficient learning with model-based RL is bottlenecked by the exploration challenge. In this work, we find that leveraging just a handful of demonstrations can dramatically improve the sample-efficiency of model-based RL. Simply appending demonstrations to the interaction dataset, however, does not suffice. We identify key ingredients for leveraging demonstrations in model learning -- policy pretraining, targeted exploration, and oversampling of demonstration data -- which forms the three phases of our model-based RL framework. We empirically study three complex visuo-motor control domains and find that our method is 160%-250% more successful in completing sparse reward tasks compared to prior approaches in the low data regime (100k interaction steps, 5 demonstrations). Code and videos are available at https://nicklashansen.github.io/modemrl",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zS9sRyaPFlJ": {
    "title": "PD-MORL: Preference-Driven Multi-Objective Reinforcement Learning Algorithm",
    "volume": "poster",
    "abstract": "Multi-objective reinforcement learning (MORL) approaches have emerged to tackle many real-world problems with multiple conflicting objectives by maximizing a joint objective function weighted by a preference vector. These approaches find fixed customized policies corresponding to preference vectors specified during training. However, the design constraints and objectives typically change dynamically in real-life scenarios. Furthermore, storing a policy for each potential preference is not scalable. Hence, obtaining a set of Pareto front solutions for the entire preference space in a given domain with a single training is critical. To this end, we propose a novel MORL algorithm that trains a single universal network to cover the entire preference space scalable to continuous robotic tasks. The proposed approach, Preference-Driven MORL (PD-MORL), utilizes the preferences as guidance to update the network parameters. It also employs a novel parallelization approach to increase sample efficiency. We show that PD-MORL achieves up to 25% larger hypervolume for challenging continuous control tasks and uses an order of magnitude fewer trainable parameters compared to prior approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s130rTE3U_X": {
    "title": "Understanding the Role of Nonlinearity in Training Dynamics of Contrastive Learning",
    "volume": "poster",
    "abstract": "While the empirical success of self-supervised learning (SSL) heavily relies on the usage of deep nonlinear models, existing theoretical works on SSL understanding still focus on linear ones. In this paper, we study the role of nonlinearity in the training dynamics of contrastive learning (CL) on one and two-layer nonlinear networks with homogeneous activation $h(x) = h'(x)x$. We have two major theoretical discoveries. First, the presence of nonlinearity can lead to many local optima even in 1-layer setting, each corresponding to certain patterns from the data distribution, while with linear activation, only one major pattern can be learned. This suggests that models with lots of parameters can be regarded as a \\emph{brute-force} way to find these local optima induced by nonlinearity. Second, in the 2-layer case, linear activation is proven not capable of learning specialized weights into diverse patterns, demonstrating the importance of nonlinearity. In addition, for 2-layer setting, we also discover \\emph{global modulation}: those local patterns discriminative from the perspective of global-level patterns are prioritized to learn, further characterizing the learning process. Simulation verifies our theoretical findings",
    "checked": true,
    "id": "f5db3b0a99e9ab7777b2fecf8b5d237715a3464d",
    "semantic_title": "understanding the role of nonlinearity in training dynamics of contrastive learning",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=s7oOe6cNRT8": {
    "title": "M-L2O: Towards Generalizable Learning-to-Optimize by Test-Time Fast Self-Adaptation",
    "volume": "poster",
    "abstract": "Learning to Optimize (L2O) has drawn increasing attention as it often remarkably accelerates the optimization procedure of complex tasks by \"overfitting\" specific task type, leading to enhanced performance compared to analytical optimizers. Generally, L2O develops a parameterized optimization method (i.e., \"optimizer\") by learning from solving sample problems. This data-driven procedure yields L2O that can efficiently solve problems similar to those seen in training, that is, drawn from the same \"task distribution\". However, such learned optimizers often struggle when new test problems come with a substantially deviation from the training task distribution. This paper investigates a potential solution to this open challenge, by meta-training an L2O optimizer that can perform fast test-time self-adaptation to a out-of-distribution task, in only a few steps. We theoretically characterize the generalization of L2O, and further show that our proposed framework (termed as M-L2O) provably facilitates rapid task adaptation by locating well-adapted initial points for the optimizer weight. Empirical observations on several classic tasks like LASSO and Quadratic, demonstrate that M-L2O converges significantly faster than vanilla L2O with only $5$ steps of adaptation, echoing our theoretical results. Codes are available in https://github.com/VITA-Group/M-L2O",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wsZsjOSytRA": {
    "title": "3D UX-Net: A Large Kernel Volumetric ConvNet Modernizing Hierarchical Transformer for Medical Image Segmentation",
    "volume": "poster",
    "abstract": "The recent 3D medical ViTs (e.g., SwinUNETR) achieve the state-of-the-art performances on several 3D volumetric data benchmarks, including 3D medical image segmentation. Hierarchical transformers (e.g., Swin Transformers) reintroduced several ConvNet priors and further enhanced the practical viability of adapting volumetric segmentation in 3D medical datasets. The effectiveness of hybrid approaches is largely credited to the large receptive field for non-local self-attention and the large number of model parameters. We hypothesize that volumetric ConvNets can simulate the large receptive field behavior of these learning approaches with fewer model parameters using depth-wise convolution. In this work, we propose a lightweight volumetric ConvNet, termed 3D UX-Net, which adapts the hierarchical transformer using ConvNet modules for robust volumetric segmentation. Specifically, we revisit volumetric depth-wise convolutions with large kernel (LK) size (e.g. starting from $7\\times7\\times7$) to enable the larger global receptive fields, inspired by Swin Transformer. We further substitute the multi-layer perceptron (MLP) in Swin Transformer blocks with pointwise depth convolutions and enhance model performances with fewer normalization and activation layers, thus reducing the number of model parameters. 3D UX-Net competes favorably with current SOTA transformers (e.g. SwinUNETR) using three challenging public datasets on volumetric brain and abdominal imaging: 1) MICCAI Challenge 2021 FLARE, 2) MICCAI Challenge 2021 FeTA, and 3) MICCAI Challenge 2022 AMOS. 3D UX-Net consistently outperforms SwinUNETR with improvement from 0.929 to 0.938 Dice (FLARE2021) and 0.867 to 0.874 Dice (Feta2021). We further evaluate the transfer learning capability of 3D UX-Net with AMOS2022 and demonstrates another improvement of $2.27\\%$ Dice (from 0.880 to 0.900). The source code with our proposed model are available at https://github.com/MASILab/3DUX-Net",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NpsVSN6o4ul": {
    "title": "Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 Small",
    "volume": "poster",
    "abstract": "Research in mechanistic interpretability seeks to explain behaviors of ML models in terms of their internal components. However, most previous work either focuses on simple behaviors in small models, or describes complicated behaviors in larger models with broad strokes. In this work, we bridge this gap by presenting an explanation for how GPT-2 small performs a natural language task that requires logical reasoning: indirect object identification (IOI). Our explanation encompasses 28 attention heads grouped into 7 main classes, which we discovered using a combination of interpretability approaches including causal interventions and projections. To our knowledge, this investigation is the largest end-to-end attempt at reverse-engineering a natural behavior \"in the wild\" in a language model. We evaluate the reliability of our explanation using three quantitative criteria - faithfulness, completeness and minimality. Though these criteria support our explanation, they also point to remaining gaps in our understanding. Our work provides evidence that a mechanistic understanding of large ML models is feasible, opening opportunities to scale our understanding to both larger models and more complex tasks",
    "checked": true,
    "id": "6edd112383ad494f5f2eba72b6f4ffae122ce61f",
    "semantic_title": "interpretability in the wild: a circuit for indirect object identification in gpt-2 small",
    "citation_count": 617,
    "authors": []
  },
  "https://openreview.net/forum?id=dnjZSPGmY5O": {
    "title": "Equivariant Descriptor Fields: SE(3)-Equivariant Energy-Based Models for End-to-End Visual Robotic Manipulation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BR_ZhvcYbGJ": {
    "title": "Explaining Temporal Graph Models through an Explorer-Navigator Framework",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l9vM_PaUKz": {
    "title": "Soft Neighbors are Positive Supporters in Contrastive Visual Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aBH_DydEvoH": {
    "title": "Offline RL for Natural Language Generation with Implicit Language Q Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H-T3F0dMbyj": {
    "title": "CLIPSep: Learning Text-queried Sound Separation with Noisy Unlabeled Videos",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z57WK5lGeHd": {
    "title": "On the Soft-Subnetwork for Few-Shot Class Incremental Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6Wl7-M2BC-": {
    "title": "An Adaptive Policy to Employ Sharpness-Aware Minimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jBEXnEMdNOL": {
    "title": "Fairness and Accuracy under Domain Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SaRj2ka1XZ3": {
    "title": "Language Models Can Teach Themselves to Program Better",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yIxtevizEA": {
    "title": "Latent Bottlenecked Attentive Neural Processes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8oJHwb3Sgp": {
    "title": "Represent to Control Partially Observed Systems: Representation Learning with Provable Sample Efficiency",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5gDz_yTcst": {
    "title": "Towards Better Selective Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=-G1kjTFsSs": {
    "title": "Learning Kernelized Contextual Bandits in a Distributed and Asynchronous Environment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G_HSyfLk0m": {
    "title": "Graph Signal Sampling for Inductive One-Bit Matrix Completion: a Closed-form Solution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cHf1DcCwcH3": {
    "title": "LipsFormer: Introducing Lipschitz Continuity to Vision Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5NTt8GFjUHkr": {
    "title": "Automatic Chain of Thought Prompting in Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fzberKYWKsI": {
    "title": "An efficient encoder-decoder architecture with top-down attention for speech separation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VzwfoFyYDga": {
    "title": "Machine Unlearning of Federated Clusters",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7D5EECbOaf9": {
    "title": "Moderate Coreset: A Universal Method of Data Selection for Real-world Data-efficient Deep Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R1U5G2spbLd": {
    "title": "Federated Nearest Neighbor Machine Translation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mQpmZVzXK1h": {
    "title": "Latent Variable Representation for Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2r6YMqz4Mml": {
    "title": "ROCO: A General Framework for Evaluating Robustness of Combinatorial Optimization Solvers on Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O-G91-4cMdv": {
    "title": "Words are all you need? Language as an approximation for human similarity judgments",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PDrUPTXJI_A": {
    "title": "FreeMatch: Self-adaptive Thresholding for Semi-supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sOXU-PEJSgQ": {
    "title": "Confidence Estimation Using Unlabeled Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FBMLeaXpZN": {
    "title": "Spectral Decomposition Representation for Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fYzLpCsGZVf": {
    "title": "On Accelerated Perceptrons and Beyond",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ymt1zQXBDiF": {
    "title": "SoftMatch: Addressing the Quantity-Quality Tradeoff in Semi-supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dCOL0inGl3e": {
    "title": "Certifiably Robust Policy Learning against Adversarial Multi-Agent Communication",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LE5LxBgjB4V": {
    "title": "Disentangling the Mechanisms Behind Implicit Regularization in SGD",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TTLLGx3eet": {
    "title": "Sequential Attention for Feature Selection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jpsw-KuOi7r": {
    "title": "Improved Sample Complexity for Reward-free Reinforcement Learning under Low-rank MDPs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XSEBx0iSjFQ": {
    "title": "Re-Imagen: Retrieval-Augmented Text-to-Image Generator",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qd0p0bl-A9t": {
    "title": "Provably Efficient Lifelong Reinforcement Learning with Linear Representation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9Jaz4APHtWD": {
    "title": "Link Prediction with Non-Contrastive Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cw8FeirkIfU": {
    "title": "Distributed Differential Privacy in Multi-Armed Bandits",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jClGv3Qjhb": {
    "title": "A Theoretical Understanding of Shallow Vision Transformers: Learning, Generalization, and Sample Complexity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AjC0KBjiMu": {
    "title": "Contrastive Learning Can Find An Optimal Basis For Approximately View-Invariant Functions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DlpCotqdTy": {
    "title": "Provably Auditing Ordinary Least Squares in Low Dimensions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qamz7Q_Ta1k": {
    "title": "Direct Embedding of Temporal Network Edges via Time-Decayed Line Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WL8FlAugqQ": {
    "title": "Neural DAG Scheduling via One-Shot Priority Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QZfdDpTX1uM": {
    "title": "Meta Temporal Point Processes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=flap0Bo6TK_": {
    "title": "Graph Neural Network-Inspired Kernels for Gaussian Processes in Semi-Supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9IaN4FkVSR1": {
    "title": "Deconstructing Distributions: A Pointwise Framework of Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Idusfje4-Wq": {
    "title": "Diffusion Models for Causal Discovery via Topological Ordering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eb_cpjZZ3GH": {
    "title": "Scalable and Equivariant Spherical CNNs by Discrete-Continuous (DISCO) Convolutions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hu4r-dedqR0": {
    "title": "Weakly Supervised Explainable Phrasal Reasoning with Neural Fuzzy Logic",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=462z-gLgSht": {
    "title": "DCI-ES: An Extended Disentanglement Framework with Connections to Identifiability",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ElC6LYO4MfD": {
    "title": "Faster federated optimization under second-order similarity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EUrxG8IBCrC": {
    "title": "Mutual Partial Label Learning with Competitive Label Noise",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jpq0qHggw3t": {
    "title": "Partial Label Unsupervised Domain Adaptation with Class-Prototype Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9HiGqC9C-KA": {
    "title": "simpleKT: A Simple But Tough-to-Beat Baseline for Knowledge Tracing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CL-sVR9pvF": {
    "title": "Weighted Ensemble Self-Supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zzqBoIFOQ1": {
    "title": "Guiding Safe Exploration with Weakest Preconditions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cYijsVZhb5": {
    "title": "Is a Caption Worth a Thousand Images? A Study on Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XSRSWxyJIC": {
    "title": "Parameter-Efficient Fine-Tuning Design Spaces",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=_01dDd3f78": {
    "title": "Concept Gradient: Concept-based Interpretation Without Linear Assumption",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1w_Amtk67X": {
    "title": "Constraining Representations Yields Models That Know What They Don't Know",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n70oyIlS4g": {
    "title": "An Extensible Multi-modal Multi-task Object Dataset with Materials",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zWy7dqOcel": {
    "title": "Sampling with Mollified Interaction Energy Descent",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nhKHA59gXz": {
    "title": "Self-Stabilization: The Implicit Bias of Gradient Descent at the Edge of Stability",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=_X12NmQKvX": {
    "title": "TILP: Differentiable Learning of Temporal Logical Rules on Knowledge Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MIMwy4kh9lf": {
    "title": "Open-Vocabulary Object Detection upon Frozen Vision and Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=_wSHsgrVali": {
    "title": "Revisiting the Assumption of Latent Separability for Backdoor Defenses",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PINRbk7h01": {
    "title": "Restricted Strong Convexity of Deep Learning Models with Smooth Activations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kUmdmHxK5N": {
    "title": "Koopman Neural Operator Forecaster for Time-series with Temporal Distributional Shifts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C1ns08q9jZ": {
    "title": "MetaGL: Evaluation-Free Selection of Graph Learning Models via Meta-Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oX3tGygjW1q": {
    "title": "Minimum Description Length Control",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hxEIgUXLFF": {
    "title": "PerFedMask: Personalized Federated Learning with Optimized Masking Vectors",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3VFQfAG3vwi": {
    "title": "Variational Latent Branching Model for Off-Policy Evaluation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oLIZ2jGTiv": {
    "title": "Tuning Frequency Bias in Neural Network Training with Nonuniform Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6SRDbbvU8s": {
    "title": "Learning Multimodal Data Augmentation in Feature Space",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iTtGCMDEzS_": {
    "title": "BigVGAN: A Universal Neural Vocoder with Large-Scale Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zZhX4eYNeeh": {
    "title": "Achieving Sub-linear Regret in Infinite Horizon Average Reward Constrained MDP with Linear Function Approximation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B-z41MBL_tH": {
    "title": "Causal Imitation Learning via Inverse Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o58JtGDs6y": {
    "title": "The Surprising Computational Power of Nondeterministic Stack RNNs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EnrY5TOrbQ": {
    "title": "Agnostic Learning of General ReLU Activation Using Gradient Descent",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aCQt_BrkSjC": {
    "title": "Learning Hyper Label Model for Programmatic Weak Supervision",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U9yFP90jU0": {
    "title": "FedFA: Federated Feature Augmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PXVGer7hmJ": {
    "title": "Offline Congestion Games: How Feedback Type Affects Data Coverage Requirement",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2L9gzS80tA4": {
    "title": "Does Learning from Decentralized Non-IID Unlabeled Data Benefit from Self Supervision?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dQNL7Zsta3": {
    "title": "Malign Overfitting: Interpolation and Invariance are Fundamentally at Odds",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aRTKuscKByJ": {
    "title": "Exploring and Exploiting Decision Boundary Dynamics for Adversarial Robustness",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OIe3kpwl40D": {
    "title": "SMART: Sentences as Basic Units for Text Evaluation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SZdfz5k7cd1": {
    "title": "Tier Balancing: Towards Dynamic Fairness over Underlying Causal Factors",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xYWqSjBcGMl": {
    "title": "Anamnesic Neural Differential Equations with Orthogonal Polynomial Projections",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y81ppNf_vg": {
    "title": "AutoTransfer: AutoML with Knowledge Transfer - An Application to Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C0q9oBc3n4": {
    "title": "Temporal Dependencies in Feature Importance for Time Series Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Tvms8xrZHyR": {
    "title": "Characterizing the spectrum of the NTK via a power series expansion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tJbbQfw-5wv": {
    "title": "A critical look at the evaluation of GNNs under heterophily: Are we really making progress?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vw-5EgYbJZr": {
    "title": "A Non-monotonic Self-terminating Language Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qc_OopMEBnC": {
    "title": "Learning to Segment from Noisy Annotations: A Spatial Correction Approach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7bJizxLKrR": {
    "title": "Measuring Forgetting of Memorized Training Examples",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QAV2CcLEDh": {
    "title": "MaskViT: Masked Visual Pre-Training for Video Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HehQobsr0S": {
    "title": "Text Summarization with Oracle Expectation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=_4n3k3d1ob": {
    "title": "Continuous-time identification of dynamic state-space models by deep subspace encoding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=klK17OQ3KB": {
    "title": "How to Train your HIPPO: State Space Models with Generalized Orthogonal Basis Projections",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TkQ1sxd9P4": {
    "title": "Interpretable Debiasing of Vectorized Language Representations with Iterative Orthogonalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jwdqNwyREyh": {
    "title": "Layer Grafted Pre-training: Bridging Contrastive Learning And Masked Image Modeling For Label-Efficient Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ETKGuby0hcs": {
    "title": "Discovering Latent Knowledge in Language Models Without Supervision",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H0gdPxSwkPb": {
    "title": "Diffusion Adversarial Representation Learning for Self-supervised Vessel Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gmSZ-GPNY6": {
    "title": "Noise Injection Node Regularization for Robust Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jpR98ZdIm2q": {
    "title": "Efficient Edge Inference by Selective Query",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JtC6yOHRoJJ": {
    "title": "Human-level Atari 200x faster",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mkJm5Uy4HrQ": {
    "title": "Incompatibility Clustering as a Defense Against Backdoor Poisoning Attacks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DpE5UYUQzZH": {
    "title": "A Unified Approach to Reinforcement Learning, Quantal Response Equilibria, and Two-Player Zero-Sum Games",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ovZE0KsbM3S": {
    "title": "Pitfalls of Gaussians as a noise distribution in NCE",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZrEbzL9eQ3W": {
    "title": "Scaling Laws for a Multi-Agent Reinforcement Learning Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HQ67mj5rJdR": {
    "title": "Perfectly Secure Steganography Using Minimum Entropy Coupling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jPVAFXHlbL": {
    "title": "Calibrating Transformers via Sparse Gaussian Processes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z37tDDHHgi": {
    "title": "Red PANDA: Disambiguating Image Anomaly Detection by Removing Nuisance Factors",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xE-LtsE-xx": {
    "title": "Is Attention All That NeRF Needs?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oJZ8bPtCar": {
    "title": "Stochastic No-regret Learning for General Games with Variance Reduction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bsZULlDGXe": {
    "title": "The Dark Side of AutoML: Towards Architectural Backdoor Search",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9F_xlC7sk9": {
    "title": "Generalization and Estimation Error Bounds for Model-based Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E8mzu3JbdR": {
    "title": "ChordMixer: A Scalable Neural Attention Model for Sequences with Different Length",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SZynfVLGd5": {
    "title": "Boosting Adversarial Transferability using Dynamic Cues",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lLp-C5nTdJG": {
    "title": "Static Prediction of Runtime Errors by Learning to Execute Programs with External Resource Descriptions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q9VherQJd8_": {
    "title": "Matching receptor to odorant with protein language and graph neural networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6xXtM8bFFJ": {
    "title": "SGDA with shuffling: faster convergence for nonconvex-PŁ minimax optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H0HGljkxQFN": {
    "title": "MOAT: Alternating Mobile Convolution and Attention Brings Strong Vision Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bAMTaeqluh4": {
    "title": "Part-Based Models Improve Adversarial Robustness",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CgCmwcfgEdH": {
    "title": "PGrad: Learning Principal Gradients For Domain Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ndYXTEL6cZz": {
    "title": "Extremely Simple Activation Shaping for Out-of-Distribution Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kQxry8Z6Fd9": {
    "title": "Statistical Guarantees for Consensus Clustering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w2P7fMy_RH": {
    "title": "Expressive Monotonic Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K9RHxPpjn2": {
    "title": "Active Image Indexing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NEtep2C7yD": {
    "title": "Learning Simultaneous Navigation and Construction in Grid Worlds",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZcnzsHC10Y": {
    "title": "Learning to CROSS exchange to solve min-max vehicle routing problems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iUdSB2kK9GY": {
    "title": "PandA: Unsupervised Learning of Parts and Appearances in the Feature Maps of GANs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PEuxUXIMLlA": {
    "title": "Compositional Law Parsing with Latent Random Functions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NVZvalzCLg": {
    "title": "LilNetX: Lightweight Networks with EXtreme Model Compression and Structured Sparsification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7mgUec-7GMv": {
    "title": "Mitigating Dataset Bias by Using Per-Sample Gradient",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fhcu4FBLciL": {
    "title": "Efficient Model Updates for Approximate Unlearning of Graph-Structured Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CYK7RfcOzQ4": {
    "title": "AudioGen: Textually Guided Audio Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2WklawyeI08": {
    "title": "Hebbian and Gradient-based Plasticity Enables Robust Memory and Rapid Learning in RNNs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U9HW6vyNClg": {
    "title": "Towards Minimax Optimal Reward-free Reinforcement Learning in Linear MDPs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=-nm-rHXi5ga": {
    "title": "On the Data-Efficiency with Contrastive Image Transformation in Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zoz7Ze4STUL": {
    "title": "Energy-based Out-of-Distribution Detection for Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O8Vc52xFSUR": {
    "title": "Quasi-optimal Reinforcement Learning with Continuous Actions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=-EHqoysUYLx": {
    "title": "Generalization Bounds for Federated Learning: Fast Rates, Unparticipating Clients and Unbounded Losses",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bXNl-myZkJl": {
    "title": "More ConvNets in the 2020s: Scaling up Kernels Beyond 51x51 using Sparsity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wlMDF1jQF86": {
    "title": "Which Layer is Learning Faster? A Systematic Exploration of Layer-wise Convergence Rate for Deep Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CJd-BtnwtXq": {
    "title": "A Non-Asymptotic Analysis of Oversmoothing in Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sCrnllCtjoE": {
    "title": "Scaleformer: Iterative Multi-scale Refining Transformers for Time Series Forecasting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g4OTKRKfS7R": {
    "title": "Liquid Structural State-Space Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RiTjKoscnNd": {
    "title": "Equivariant Hypergraph Diffusion Neural Operators",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sPCKNl5qDps": {
    "title": "Ollivier-Ricci Curvature for Hypergraphs: A Unified Framework",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wq0luyH3m4": {
    "title": "Hard-Meta-Dataset++: Towards Understanding Few-Shot Performance on Difficult Tasks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gJW8hSGBys8": {
    "title": "Compositional Semantic Parsing with Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zClyiZ5V6sL": {
    "title": "TiAda: A Time-scale Adaptive Algorithm for Nonconvex Minimax Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=shzu8d6_YAR": {
    "title": "FaiREE: fair classification with finite-sample and distribution-free guarantee",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1_jtWjhSSkr": {
    "title": "Exponential Generalization Bounds with Near-Optimal Rates for $L_q$-Stable Algorithms",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EMvG1Jdhw_8": {
    "title": "Disentangling Learning Representations with Density Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GVSf7Z7DbYL": {
    "title": "Teacher Guided Training: An Efficient Framework for Knowledge Transfer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GULFHQfgw0g": {
    "title": "Neural Agents Struggle to Take Turns in Bidirectional Emergent Communication",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=98p5x51L5af": {
    "title": "Prompting GPT-3 To Be Reliable",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ReDQ1OUQR0X": {
    "title": "Human alignment of neural network representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j3cUWIMsFBN": {
    "title": "Unbiased Stochastic Proximal Solver for Graph Neural Networks with Equilibrium States",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UaAD-Nu86WX": {
    "title": "DiGress: Discrete Denoising diffusion for graph generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gVOXZproe-e": {
    "title": "How to prepare your task head for finetuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jQj-_rLVXsj": {
    "title": "DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=-Y34L45JR6z": {
    "title": "Policy Expansion for Bridging Offline-to-Online Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6qcYDVlVLnK": {
    "title": "Mitigating Memorization of Noisy Labels via Regularization between Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dqnNW2omZL6": {
    "title": "Graph Neural Networks are Inherently Good Generalizers: Insights by Bridging GNNs and MLPs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zob4P9bRNcK": {
    "title": "Learning Cut Selection for Mixed-Integer Linear Programming via Hierarchical Sequence Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZxdkjTgK_Dl": {
    "title": "BSTT: A Bayesian Spatial-Temporal Transformer for Sleep Staging",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6qZC7pfenQm": {
    "title": "Improving Deep Policy Gradients with Value Function Search",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=txlWziuCE5W": {
    "title": "MEDICAL IMAGE UNDERSTANDING WITH PRETRAINED VISION LANGUAGE MODELS: A COMPREHENSIVE STUDY",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=-t4D61w4zvQ": {
    "title": "Temporal Coherent Test Time Optimization for Robust Video Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rdfgqiwz7lZ": {
    "title": "A Learning Based Hypothesis Test for Harmful Covariate Shift",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NPrsUQgMjKK": {
    "title": "Deep Transformers without Shortcuts: Modifying Self-attention for Faithful Signal Propagation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZKDUlVMqG_O": {
    "title": "Self-Supervised Geometric Correspondence for Category-Level 6D Object Pose Estimation in the Wild",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JHklpEZqduQ": {
    "title": "Non-parametric Outlier Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r90KYcuB7JS": {
    "title": "Approximation and non-parametric estimation of functions over high-dimensional spheres via deep ReLU networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sVU54nyaA9K": {
    "title": "Learning Adversarial Linear Mixture Markov Decision Processes with Bandit Feedback and Unknown Transition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4yqxDCbzS98": {
    "title": "Weakly Supervised Knowledge Transfer with Probabilistic Logical Reasoning for Object Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rLguqxYvYHB": {
    "title": "A Neural Mean Embedding Approach for Back-door and Front-door Adjustment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UVAmFAtC5ye": {
    "title": "TranSpeech: Speech-to-Speech Translation With Bilateral Perturbation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aBIpZvMdS56": {
    "title": "Over-parameterized Model Optimization with Polyak-{\\L}ojasiewicz Condition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BPwIgvf5iQ": {
    "title": "Jointly Learning Visual and Auditory Speech Representations from Raw Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H4Ncs5jhTCu": {
    "title": "Diminishing Return of Value Expansion Methods in Model-Based Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GNjzMAgawq": {
    "title": "CLIP-ViP: Adapting Pre-trained Image-Text Model to Video-Language Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r0otLtOwYW": {
    "title": "Equivariant Energy-Guided SDE for Inverse Molecular Design",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KB1sc5pNKFv": {
    "title": "On the Feasibility of Cross-Task Transfer with Model-Based Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IVESH65r0Ar": {
    "title": "A Simple Yet Powerful Deep Active Learning With Snapshots Ensembles",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bcYZwYo-0t": {
    "title": "Decoupled Training for Long-Tailed Classification With Stochastic Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2XLRBjY46O6": {
    "title": "ViewCo: Discovering Text-Supervised Segmentation Masks via Multi-View Semantic Consistency",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vINj_Hv9szL": {
    "title": "Benchmarking Constraint Inference in Inverse Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jHc8dCx6DDr": {
    "title": "Memory Gym: Partially Observable Challenges to Memory-Based Agents",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kjkdzBW3b8p": {
    "title": "Discovering Policies with DOMiNO: Diversity Optimization Maintaining Near Optimality",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Mg5CLXZgvLJ": {
    "title": "SpeedyZero: Mastering Atari with Limited Data and Time",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p8coElqiSDw": {
    "title": "Neural Architecture Design and Robustness: A Dataset",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QB1dMPEXau5": {
    "title": "Does Deep Learning Learn to Abstract? A Systematic Probing Framework",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0f-0I6RFAch": {
    "title": "Improving Out-of-distribution Generalization with Indirection Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F0KTk2plQzO": {
    "title": "Accelerating Guided Diffusion Sampling with Splitting Numerical Methods",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Dk7QQp8jHEo": {
    "title": "Batch Multivalid Conformal Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sb-IkS8DQw2": {
    "title": "Accurate Bayesian Meta-Learning by Accurate Task Posterior Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wtcud6HroZr": {
    "title": "Learning to Decompose Visual Features with Latent Textual Prompts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XrMWUuEevr": {
    "title": "Context-enriched molecule representations improve few-shot drug discovery",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EzLtB4M1SbM": {
    "title": "Test-Time Adaptation via Self-Training with Nearest Neighbor Information",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yTbNYYcopd": {
    "title": "Accurate Neural Training with 4-bit Matrix Multiplications at Standard Formats",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lUpjsrKItz4": {
    "title": "Unsupervised Manifold Alignment with Joint Multidimensional Scaling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uu1GBD9SlLe": {
    "title": "Simple and Scalable Nearest Neighbor Machine Translation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v8JIQdiN9Sh": {
    "title": "On the Effectiveness of Out-of-Distribution Data in Self-Supervised Long-Tail Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZIkHSXzd9O7": {
    "title": "Dynamic Update-to-Data Ratio: Minimizing World Model Overfitting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6K2RM6wVqKu": {
    "title": "Uni-Mol: A Universal 3D Molecular Representation Learning Framework",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YgC62m4CY3r": {
    "title": "Learning with Auxiliary Activation for Memory-Efficient Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sIoED-yPK9l": {
    "title": "Massively Scaling Heteroscedastic Classifiers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2nocgE1m0A": {
    "title": "KnowDA: All-in-One Knowledge Mixture Model for Data Augmentation in Low-Resource NLP",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9ImtNIZ7bYx": {
    "title": "Finding the Global Semantic Representation in GAN through Fréchet Mean",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i2_TvOFmEml": {
    "title": "MultiViz: Towards Visualizing and Understanding Multimodal Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sKHqgFOaFXI": {
    "title": "How Informative is the Approximation Error from Tensor Decomposition for Neural Network Compression?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OjDkC57x5sz": {
    "title": "Blurring Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3Bh6sRPKS3J": {
    "title": "Hyperbolic Self-paced Learning for Self-supervised Skeleton-based Action Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Yt-yM-JbYFO": {
    "title": "Efficient Offline Policy Optimization with a Learned Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fxC7kJYwA_a": {
    "title": "New Insights for the Stability-Plasticity Dilemma in Online Continual Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dRjWsd3gwsm": {
    "title": "MixPro: Data Augmentation with MaskMix and Progressive Attention Labeling for Vision Transformer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ojpb1y8jflw": {
    "title": "StyleMorph: Disentangled 3D-Aware Image Synthesis with a 3D Morphable StyleGAN",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Dvs-a3aymPe": {
    "title": "Searching Lottery Tickets in Graph Neural Networks: A Dual Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KLrGlNoxzb4": {
    "title": "Video Scene Graph Generation from Single-Frame Weak Supervision",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nI2HmVA0hvt": {
    "title": "Unsupervised visualization of image datasets using contrastive learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s1KljJpAukm": {
    "title": "PowerQuant: Automorphism Search for Non-Uniform Quantization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Bd7GueaTxUz": {
    "title": "BAYES RISK CTC: CONTROLLABLE CTC ALIGNMENT IN SEQUENCE-TO-SEQUENCE TASKS",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0jxPyVWmiiF": {
    "title": "A Convergent Single-Loop Algorithm for Relaxation of Gromov-Wasserstein in Graph Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zOHQGKO3WGY": {
    "title": "Semi-supervised learning with a principled likelihood from a generative model of data curation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sO1QiAftQFv": {
    "title": "E3Bind: An End-to-End Equivariant Network for Protein-Ligand Docking",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q-WfHzmiG9m": {
    "title": "Re-weighting Based Group Fairness Regularization via Classwise Robust Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uagC-X9XMi8": {
    "title": "Are More Layers Beneficial to Graph Transformers?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=_QLsH8gatwx": {
    "title": "Simplicial Hopfield networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2nLeOOfAjK": {
    "title": "Versatile Neural Processes for Learning Implicit Neural Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ymFhZxw70uz": {
    "title": "Classically Approximating Variational Quantum Machine Learning with Random Fourier Features",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LGkmUauBUL": {
    "title": "Distributional Meta-Gradient Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lRdhvzMpVYV": {
    "title": "A Differential Geometric View and Explainability of GNN on Evolving Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7L2mgi0TNEP": {
    "title": "$\\rm A^2Q$: Aggregation-Aware Quantization for Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pfuqQQCB34": {
    "title": "Variance Reduction is an Antidote to Byzantines: Better Rates, Weaker Assumptions and Communication Compression as a Cherry on the Top",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Nk2pDtuhTq": {
    "title": "Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cRxYWKiTan": {
    "title": "Better Generative Replay for Continual Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4PJUBT9f2Ol": {
    "title": "Generative Modelling with Inverse Heat Dissipation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ubc74gTVo3": {
    "title": "Self-supervision through Random Segments with Autoregressive Coding (RandSAC)",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4oYUGeGBPm": {
    "title": "Transformer-Patcher: One Mistake Worth One Neuron",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8E5Yazboyh": {
    "title": "Sharper Bounds for Uniformly Stable Algorithms with Stationary Mixing Process",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UrEwJebCxk": {
    "title": "Benign Overfitting in Classification: Provably Counter Label Noise with Larger Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0uRm1YmFTu": {
    "title": "Predictive Inference with Feature Conformal Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ivwZO-HnzG_": {
    "title": "Recon: Reducing Conflicting Gradients From the Root For Multi-Task Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g2oB_k-18b": {
    "title": "Measure the Predictive Heterogeneity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o8xdgmwCP8l": {
    "title": "Time to augment self-supervised visual representation learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=-vKlt84fHs": {
    "title": "Towards Lightweight, Model-Agnostic and Diversity-Aware Active Anomaly Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AwWaBXLIJE": {
    "title": "Q-Pensieve: Boosting Sample Efficiency of Multi-Objective RL Through Memory Sharing of Q-Snapshots",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tkwP32nsEq": {
    "title": "Variance-Aware Sparse Linear Bandits",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zQWqV2tzDv": {
    "title": "CircNet: Meshing 3D Point Clouds with Circumcenter Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dfDv0WU853R": {
    "title": "In-sample Actor Critic for Offline Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CGBCTp2M6lA": {
    "title": "Leveraging Future Relationship Reasoning for Vehicle Trajectory Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=P44WPn1_aJV": {
    "title": "LMSeg: Language-guided Multi-dataset Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G1H4NSATlr": {
    "title": "RoPAWS: Robust Semi-supervised Representation Learning from Uncurated Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DWn1TEb2fK": {
    "title": "Treeformer: Dense Gradient Trees for Efficient Attention Computation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kJWcI39kXY": {
    "title": "ODAM: Gradient-based Instance-Specific Visual Explanations for Object Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xZD10GhCvM": {
    "title": "Toward Adversarial Training on Contextualized Language Representation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sbS10BCtc7": {
    "title": "Gromov-Wasserstein Autoencoders",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ltWade-cpK": {
    "title": "Optimal Activation Functions for the Random Features Regression Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=_-FN9mJsgg": {
    "title": "Improving Object-centric Learning with Query Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zH9GcZ3ZGXu": {
    "title": "Feature Reconstruction From Outputs Can Mitigate Simplicity Bias in Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xQAjSr64PTc": {
    "title": "EUCLID: Towards Efficient Unsupervised Reinforcement Learning with Multi-choice Dynamics Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lj1Eb1OPeNw": {
    "title": "Maximizing Spatio-Temporal Entropy of Deep 3D CNNs for Efficient Video Recognition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7d-g8KozkiE": {
    "title": "Cycle to Clique (Cy2C) Graph Neural Network: A Sight to See beyond Neighborhood Aggregation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b0UksKFcTOL": {
    "title": "Latent State Marginalization as a Low-cost Approach for Improving Exploration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=inU2quhGdNU": {
    "title": "Generalizing and Decoupling Neural Collapse via Hyperspherical Uniformity Gap",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QzbKH8nNq_V": {
    "title": "MaskFusion: Feature Augmentation for Click-Through Rate Prediction via Input-adaptive Mask Fusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8U4joMeLRF": {
    "title": "Rethinking Self-Supervised Visual Representation Learning in Pre-training for 3D Human Pose and Shape Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UiaUEICawgw": {
    "title": "Learned Index with Dynamic $\\epsilon$",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OxNQXyZK-K8": {
    "title": "Boosting Multiagent Reinforcement Learning via Permutation Invariant and Permutation Equivariant Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v8Mi8KU6056": {
    "title": "wav2tok: Deep Sequence Tokenizer for Audio Retrieval",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o3yygm3lnzS": {
    "title": "PV3D: A 3D Generative Model for Portrait Video Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=51GXyzOKOp": {
    "title": "Characterizing the Influence of Graph Elements",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jh1nCir1R3d": {
    "title": "SWIFT: Rapid Decentralized Federated Learning via Wait-Free Model Communication",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CUOaVn6mYEj": {
    "title": "Hierarchical Sliced Wasserstein Distance",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nUsP9lFADUF": {
    "title": "Prototypical Calibration for Few-shot Learning of Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NO0ThzteQdI": {
    "title": "NERDS: A General Framework to Train Camera Denoisers from Raw-RGB Noisy Image Pairs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9X-hgLDLYkQ": {
    "title": "Learning Hierarchical Protein Representations via Complete 3D Graph Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1UbNwQC89a": {
    "title": "RGI: robust GAN-inversion for mask-free image inpainting and unsupervised pixel-wise anomaly detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QwKvL6wC8Yi": {
    "title": "Coverage-centric Coreset Selection for High Pruning Rates",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OM7doLjQbOQ": {
    "title": "ILA-DA: Improving Transferability of Intermediate Level Attack with Data Augmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x0BPR9iXc1": {
    "title": "Contrastive Alignment of Vision to Language Through Parameter-Efficient Transfer Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iP77_axu0h3": {
    "title": "BEEF: Bi-Compatible Class-Incremental Learning via Energy-Based Expansion and Fusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gUZWOE42l6Q": {
    "title": "Out-of-distribution Representation Learning for Time Series Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VGI9dSmTgPF": {
    "title": "Schema Inference for Interpretable Image Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XFSCKELP3bp": {
    "title": "Your Contrastive Learning Is Secretly Doing Stochastic Neighbor Embedding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OhUAblg27z": {
    "title": "Harnessing Mixed Offline Reinforcement Learning Datasets via Trajectory Weighting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1PL1NIMMrw": {
    "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pgU3k7QXuz0": {
    "title": "Spiking Convolutional Neural Networks for Text Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E3ip6qBLF7": {
    "title": "Distributionally Robust Recourse Action",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HgQR0mXQ1_a": {
    "title": "Write and Paint: Generative Vision-Language Models are Unified Modal Learners",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zJXg_Wmob03": {
    "title": "Progressive Voronoi Diagram Subdivision Enables Accurate Data-free Class-Incremental Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XIzO8zr-WbM": {
    "title": "Data Valuation Without Training of a Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YDJRFWBMNby": {
    "title": "HotProtein: A Novel Framework for Protein Thermostability Prediction and Editing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HnSceSzlfrY": {
    "title": "RPM: Generalizable Multi-Agent Policies for Multi-Agent Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hQ4K9Bf4G2B": {
    "title": "Behavior Prior Representation learning for Offline Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o0LFPcoFKnr": {
    "title": "SCALE-UP: An Efficient Black-box Input-level Backdoor Detection via Analyzing Scaled Prediction Consistency",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tQG-o3SeipT": {
    "title": "On the Perils of Cascading Robust Classifiers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4D4TSJE6-K": {
    "title": "Learning Math Reasoning from Self-Sampled Correct and Partially-Correct Solutions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3yJ-hcJBqe": {
    "title": "Adaptive Robust Evidential Optimization For Open Set Detection from Imbalanced Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5HLoTvVGDe": {
    "title": "Dual Diffusion Implicit Bridges for Image-to-Image Translation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=boik01yhssB": {
    "title": "Average Sensitivity of Decision Tree Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WmIwYTd0YTF": {
    "title": "Stable Target Field for Reduced Variance Score Estimation in Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m3twGT2bAug": {
    "title": "Continuous pseudo-labeling from the start",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X9yCkmT5Qrl": {
    "title": "GNNDelete: A General Strategy for Unlearning in Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uHaWaNhCvZD": {
    "title": "Meta-Learning in Games",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kKF8_K-mBbS": {
    "title": "DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yLzLfM-Esnu": {
    "title": "Constructive TT-representation of the tensors given as index interaction functions with applications",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=78xgBm6ckZr": {
    "title": "Sparse tree-based Initialization for Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AdPJb9cud_Y": {
    "title": "VoGE: A Differentiable Volume Renderer using Gaussian Ellipsoids for Analysis-by-Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TJ2nxciYCk-": {
    "title": "The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3YjQfCLdrzz": {
    "title": "FoSR: First-order spectral rewiring for addressing oversquashing in GNNs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3OaBBATwsvP": {
    "title": "Generative Modeling Helps Weak Supervision (and Vice Versa)",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8JCg5xJCTPR": {
    "title": "Provable Memorization Capacity of Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A7v2DqLjZdq": {
    "title": "Bridge the Inference Gaps of Neural Processes via Expectation Maximization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZhuXksSJYWn": {
    "title": "Masked Vision and Language Modeling for Multi-modal Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8WTAh0tj2jC": {
    "title": "Agent-based Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6JMXLWX68Kj": {
    "title": "On the Performance of Temporal Difference Learning With Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b0JxQC7JLWh": {
    "title": "Certified Defences Against Adversarial Patch Attacks on Semantic Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=81VJDmOE2ol": {
    "title": "Markup-to-Image Diffusion Models with Scheduled Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Yo06F8kfMa1": {
    "title": "How Much Space Has Been Explored? Measuring the Chemical Space Covered by Databases and Machine-Generated Molecules",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DBMttEEoLbw": {
    "title": "Understanding new tasks through the lens of training data via exponential tilting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0qSOodKmJaN": {
    "title": "Calibrating Sequence likelihood Improves Conditional Language Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vdv6CmGksr0": {
    "title": "Learning differentiable solvers for systems with hard constraints",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6P9Y25Pljl6": {
    "title": "FedDAR: Federated Domain-Aware Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TFbwV6I0VLg": {
    "title": "SlotFormer: Unsupervised Visual Dynamics Simulation with Object-Centric Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MQcmfgRxf7a": {
    "title": "Simplifying Model-based RL: Learning Representations, Latent-space Models, and Policies with One Objective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o7koEEMA1bR": {
    "title": "Deep Generative Symbolic Regression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p66AzKi6Xim": {
    "title": "What Can we Learn From The Selective Prediction And Uncertainty Estimation Performance Of 523 Imagenet Classifiers?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2SV2dlfBuE3": {
    "title": "Predictor-corrector algorithms for stochastic optimization under gradual distribution shift",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CIoSZ_HKHS7": {
    "title": "AIM: Adapting Image Models for Efficient Video Action Recognition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sciA_xgYofB": {
    "title": "Impossibly Good Experts and How to Follow Them",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3KUfbI9_DQE": {
    "title": "Distributionally Robust Post-hoc Classifiers under Prior Shifts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HnlCZATopvr": {
    "title": "Transformer Meets Boundary Value Inverse Problems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3YFDsSRSxB-": {
    "title": "Unicom: Universal and Compact Representation Learning for Image Retrieval",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ik91mY-2GN": {
    "title": "Diffusion Probabilistic Fields",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6w1k-IixnL8": {
    "title": "Beyond calibration: estimating the grouping loss of modern neural networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yyBis80iUuU": {
    "title": "Hybrid RL: Using both offline and online data can make RL efficient",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p0yrSRbN5Bu": {
    "title": "Model ensemble instead of prompt fusion: a sample-specific knowledge transfer method for few-shot prompt tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RlPmWBiyp6w": {
    "title": "GAIN: On the Generalization of Instructional Action Understanding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lcSfirnflpW": {
    "title": "ManyDG: Many-domain Generalization for Healthcare Applications",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XHc5zRPxqV9": {
    "title": "DecAF: Joint Decoding of Answers and Logical Forms for Question Answering over Knowledge Bases",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=elDEe8LYW7-": {
    "title": "NANSY++: Unified Voice Synthesis with Neural Analysis and Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8XqDnrmZQNF": {
    "title": "Causality Compensated Attention for Contextual Biased Visual Recognition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TjEzIsyEsQ6": {
    "title": "Multi-Objective Reinforcement Learning: Convexity, Stationarity and Pareto Optimality",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J4mJjotSauh": {
    "title": "Fooling SHAP with Stealthily Biased Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vPXp7K_Yhre": {
    "title": "Asynchronous Gradient Play in Zero-Sum Multi-agent Games",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HtoA0oT30jC": {
    "title": "Novel View Synthesis with Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C_PRLz8bEJx": {
    "title": "DM-NeRF: 3D Scene Geometry Decomposition and Manipulation from 2D Images",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eWtMdr6yCmL": {
    "title": "Trading Information between Latents in Hierarchical Variational Autoencoders",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0paCJSFW7j": {
    "title": "ISAAC Newton: Input-based Approximate Curvature for Newton's Method",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r0xte-t40I": {
    "title": "Learning Human-Compatible Representations for Case-Based Decision Support",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S-h1oFv-mq": {
    "title": "Long-Tailed Learning Requires Feature Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aEFaE0W5pAd": {
    "title": "How to Exploit Hyperspherical Embeddings for Out-of-Distribution Detection?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yyLvxYBJV1B": {
    "title": "AnyDA: Anytime Domain Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=raU07GpP0P": {
    "title": "Improving Deep Regression with Ordinal Entropy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8JqINxA-2a": {
    "title": "Unified Discrete Diffusion for Simultaneous Vision-Language Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QCrw0u9LQ7": {
    "title": "Iterative Patch Selection for High-Resolution Image Recognition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LSz-gQyd0zE": {
    "title": "Fuzzy Alignments in Directed Acyclic Graph for Non-Autoregressive Machine Translation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uhLAcrAZ9cJ": {
    "title": "Efficient Federated Domain Translation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4dZeBJ83oxk": {
    "title": "3D Segmenter: 3D Transformer based Semantic Segmentation via 2D Panoramic Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=okwxL_c4x84": {
    "title": "Clifford Neural Layers for PDE Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W-nZDQyuy8D": {
    "title": "GOOD: Exploring geometric cues for detecting objects in an open world",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OgbtSLESnI": {
    "title": "TabCaps: A Capsule Neural Network for Tabular Data Classification with BoW Routing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=-CoNloheTs": {
    "title": "An Exact Poly-Time Membership-Queries Algorithm for Extracting a Three-Layer ReLU Network",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EXnIyMVTL8s": {
    "title": "Towards Understanding and Mitigating Dimensional Collapse in Heterogeneous Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xI1ZTtVOtlz": {
    "title": "Evidential Uncertainty and Diversity Guided Active Learning for Scene Graph Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=socffUzSIlx": {
    "title": "Anisotropic Message Passing: Graph Neural Networks with Directional and Long-Range Interactions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=_8mS2NE-HXN": {
    "title": "SYNC: SAFETY-AWARE NEURAL CONTROL FOR STABILIZING STOCHASTIC DELAY-DIFFERENTIAL EQUATIONS",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1J-ZTr7aypY": {
    "title": "Differentiable Mathematical Programming for Object-Centric Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p8hMBcPtvju": {
    "title": "Scalable Subset Sampling with Neural Conditional Poisson Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FRLswckPXQ5": {
    "title": "Improved Convergence of Differential Private SGD with Gradient Clipping",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2vmGv5wPDBZ": {
    "title": "Learning to Estimate Single-View Volumetric Flow Motions without 3D Supervision",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n0Pb9T5kmb": {
    "title": "ArCL: Enhancing Contrastive Learning with Augmentation-Robust Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ejHUr4nfHhD": {
    "title": "Temperature Schedules for self-supervised contrastive methods on long-tail data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OoOIW-3uadi": {
    "title": "Deep Learning on Implicit Neural Representations of Shapes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9MbhFHqrti9": {
    "title": "ImaginaryNet: Learning Object Detectors without Real Images and Annotations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UT-_SVOyD1H": {
    "title": "Contextual bandits with concave rewards, and an application to fair ranking",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3VKiaagxw1S": {
    "title": "Gradient Boosting Performs Gaussian Process Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TrwE8l9aJzs": {
    "title": "Learning Zero-Shot Cooperation with Humans, Assuming Humans Are Biased",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZW5aK4yCRqU": {
    "title": "Modelling Long Range Dependencies in $N$D: From Task-Specific to a General Purpose CNN",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pia70sP2Oi1": {
    "title": "Planckian Jitter: countering the color-crippling effects of color jitter on self-supervised training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iLMgk2IGNyv": {
    "title": "GAMR: A Guided Attention Model for (visual) Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IpGgfpMucHj": {
    "title": "Voint Cloud: Multi-View Point Cloud Representation for 3D Understanding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=-jP_rDkyfpI": {
    "title": "Approximate Nearest Neighbor Search through Modern Error-Correcting Codes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q8vgHfPdoQP": {
    "title": "When to Make and Break Commitments?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QUK1ExlbbA": {
    "title": "DENSE RGB SLAM WITH NEURAL IMPLICIT MAPS",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=-iADdfa4GKH": {
    "title": "Monocular Scene Reconstruction with 3D SDF Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qU6NIcpaSi-": {
    "title": "Learning Heterogeneous Interaction Strengths by Trajectory Prediction with Graph Neural Network",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B8a1FcY0vi": {
    "title": "From $t$-SNE to UMAP with contrastive learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5fvXH49wk2": {
    "title": "D4AM: A General Denoising Framework for Downstream Acoustic Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lq62uWRJjiY": {
    "title": "Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6ZajpxqTlQ": {
    "title": "Generalize Learned Heuristics to Solve Large-scale Vehicle Routing Problems in Real-time",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XDJwuEYHhme": {
    "title": "Towards the Generalization of Contrastive Self-Supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QUaDoIdgo0": {
    "title": "CO3: Cooperative Unsupervised 3D Representation Learning for Autonomous Driving",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SbR9mpTuBn": {
    "title": "Bag of Tricks for Unsupervised Text-to-Speech",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bZjxxYURKT": {
    "title": "FedSpeed: Larger Local Interval, Less Communication Round, and Higher Generalization Accuracy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w-x7U26GM7j": {
    "title": "Advancing Radiograph Representation Learning with Masked Record Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FIrQfNSOoTr": {
    "title": "Instance-wise Batch Label Restoration via Gradients in Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B92TMCG_7rp": {
    "title": "Re-parameterizing Your Optimizers rather than Architectures",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VbCMhg7MRmj": {
    "title": "Protein Representation Learning via Knowledge Enhanced Primary Structure Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MTTPLcwvqTt": {
    "title": "The Provable Benefit of Unsupervised Data Sharing for Offline Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=-bVsNeR56KS": {
    "title": "Modeling Sequential Sentence Relation to Improve Cross-lingual Dense Retrieval",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pf8RIZTMU58": {
    "title": "DepthFL : Depthwise Federated Learning for Heterogeneous Clients",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1fZd4owfJP6": {
    "title": "Masked Image Modeling with Denoising Contrast",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NnOZT_CR26Z": {
    "title": "GoBigger: A Scalable Platform for Cooperative-Competitive Multi-Agent Interactive Simulation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZAKkiVxiAM9": {
    "title": "Masked Unsupervised Self-training for Label-free Image Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YfwMIDhPccD": {
    "title": "GeneFace: Generalized and High-Fidelity Audio-Driven 3D Talking Face Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3mRwyG5one": {
    "title": "DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dSYoPjM5J_W": {
    "title": "Revisiting Graph Adversarial Attack and Defense From a Data Distribution Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S31oTB72m0G": {
    "title": "Provable Sim-to-real Transfer in Continuous Domain with Partial Observations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=_9k5kTgyHT": {
    "title": "Globally Optimal Training of Neural Networks with Threshold Activation Functions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Rq13idF0F73": {
    "title": "Molecule Generation For Target Protein Binding with Structural Motifs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7GEvPKxjtt": {
    "title": "Towards Robustness Certification Against Universal Perturbations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M9u_ctqFUlg": {
    "title": "Deep Generative Modeling on Limited Data with Regularization by Nontransferable Pre-trained Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h8T5dZWTZ-Z": {
    "title": "Basic Binary Convolution Unit for Binarized Image Restoration Network",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hnk1WRMAYqg": {
    "title": "Multimodal Federated Learning via Contrastive Representation Ensemble",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=_Mic8V96Voy": {
    "title": "Eva: Practical Second-order Optimization with Kronecker-vectorized Approximation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TKIFuQHHECj": {
    "title": "Can CNNs Be More Robust Than Transformers?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=-RwZOVybbj": {
    "title": "Risk-Aware Reinforcement Learning with Coherent Risk Measures and Non-linear Function Approximation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kkpL4zUXtiw": {
    "title": "Bi-level Physics-Informed Neural Networks for PDE Constrained Optimization using Broyden's Hypergradients",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tFvr-kYWs_Y": {
    "title": "On the Saturation Effect of Kernel Ridge Regression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=to3qCB3tOh9": {
    "title": "Protein Representation Learning by Geometric Structure Pretraining",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8wbnpOJY-f": {
    "title": "Trainable Weight Averaging: Efficient Training by Optimizing Historical Solutions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UClBPxIZqnY": {
    "title": "Deep Declarative Dynamic Time Warping for End-to-End Learning of Alignment Paths",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3itjR9QxFw": {
    "title": "Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p7EagBsMAEO": {
    "title": "Understanding Edge-of-Stability Training Dynamics with a Minimalist Example",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PzBGIu-llo7": {
    "title": "Learning Proximal Operators to Discover Multiple Optima",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gfWNItGOES6": {
    "title": "Guiding continuous operator learning through Physics-based boundary constraints",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mX56bKDybu5": {
    "title": "Neural Radiance Field Codebooks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qBvBycTqVJ": {
    "title": "Generalized Precision Matrix for Scalable Estimation of Nonparametric Markov Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9aokcgBVIj1": {
    "title": "FiT: Parameter Efficient Few-shot Transfer Learning for Personalized and Federated Image Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1-MBdJssZ-S": {
    "title": "Discrete Contrastive Diffusion for Cross-Modal Music and Image Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6TxBxqNME1Y": {
    "title": "Diffusion Probabilistic Modeling of Protein Backbones in 3D for the motif-scaffolding problem",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kfOtMqYJlUU": {
    "title": "NeRF-SOS: Any-View Self-supervised Object Segmentation on Complex Scenes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fjh7UGQgOB": {
    "title": "Rethinking Graph Lottery Tickets: Graph Sparsity Matters",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TVY6GoURrw": {
    "title": "Private Federated Learning Without a Trusted Server: Optimal Algorithms for Convex Losses",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cddbeL1HWaD": {
    "title": "Cheap Talk Discovery and Utilization in Multi-Agent Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Oc2vlWU0jFY": {
    "title": "Reversible Column Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KE_wJD2RK4": {
    "title": "Modeling Multimodal Aleatoric Uncertainty in Segmentation with Mixture of Stochastic Experts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jbIYfq4Tr-": {
    "title": "On the Robustness of Safe Reinforcement Learning under Observational Perturbations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TPiwkItUSu": {
    "title": "Behind the Scenes of Gradient Descent: A Trajectory Analysis via Basis Function Decomposition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MjsDeTcDEy": {
    "title": "What Is Missing in IRM Training and Evaluation? Challenges and Solutions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1tHAZRqftM": {
    "title": "Multi-task Self-supervised Graph Neural Networks Enable Stronger Task Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V_06QV-kZX": {
    "title": "Analyzing Tree Architectures in Ensembles via Neural Tangent Kernel",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7sn6Vxp92xV": {
    "title": "Exploring The Role of Mean Teachers in Self-supervised Masked Auto-Encoders",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BrJATVZDWEH": {
    "title": "Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yHLvIlE9RGN": {
    "title": "Evaluating Long-Term Memory in 3D Mazes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CPIy9TWFYBG": {
    "title": "Proactive Multi-Camera Collaboration for 3D Human Pose Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Sy-o2N0hF4f": {
    "title": "Become a Proficient Player with Limited Data through Watching Pure Videos",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lQVpasnQS62": {
    "title": "Human MotionFormer: Transferring Human Motions with Vision Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YPChvOgRXRA": {
    "title": "Backstepping Temporal Difference Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dYHYXZ3uGdQ": {
    "title": "A General Rank Preserving Framework for Asymmetric Image Retrieval",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qNLe3iq2El": {
    "title": "Mega: Moving Average Equipped Gated Attention",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6zrOr_Rdhjs": {
    "title": "Parallel Deep Neural Networks Have Zero Duality Gap",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c5tbxWXU9-y": {
    "title": "Information-Theoretic Analysis of Unsupervised Domain Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PbkBDQ5_UbV": {
    "title": "Pessimism in the Face of Confounders: Provably Efficient Offline Reinforcement Learning in Partially Observable Markov Decision Processes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=P4bXCawRi5J": {
    "title": "Understanding Zero-shot Adversarial Robustness for Large-Scale Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YV8tP7bW6Kt": {
    "title": "Can We Faithfully Represent Absence States to Compute Shapley Values on a DNN?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FCnohuR6AnM": {
    "title": "Dataless Knowledge Fusion by Merging Weights of Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PQOlkgsBsik": {
    "title": "Universal Vision-Language Dense Retrieval: Learning A Unified Representation Space for Multi-Modal Retrieval",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5O2uzDusEN5": {
    "title": "DFlow: Learning to Synthesize Better Optical Flow Datasets via a Differentiable Pipeline",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k1FHgri5y3-": {
    "title": "Sparse Random Networks for Communication-Efficient Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vVJZtlZB9D": {
    "title": "A General Framework For Proving The Equivariant Strong Lottery Ticket Hypothesis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4LMIZY7gt7h": {
    "title": "Robust Fair Clustering: A Novel Fairness Attack and Defense Framework",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UMERaIHMwB3": {
    "title": "Learning to Jointly Share and Prune Weights for Grounding Based Vision and Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3DIpIf3wQMC": {
    "title": "Spatial Attention Kinetic Networks with E(n)-Equivariance",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OysfLgrk8mk": {
    "title": "Graph Domain Adaptation via Theory-Grounded Spectral Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5aT4ganOd98": {
    "title": "CLARE: Conservative Model-Based Reward Learning for Offline Inverse Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=_hb4vM3jspB": {
    "title": "Data-Free One-Shot Federated Learning Under Very High Statistical Heterogeneity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8duT3mi_5n": {
    "title": "GReTo: Remedying dynamic graph topology-task discordance via target homophily",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0WVNuEnqVu": {
    "title": "Deep Reinforcement Learning for Cost-Effective Medical Diagnosis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=chDrutUTs0K": {
    "title": "POPGym: Benchmarking Partially Observable Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pOnhudsvzR": {
    "title": "Everybody Needs Good Neighbours: An Unsupervised Locality-based Method for Bias Mitigation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6OphWWAE3cS": {
    "title": "Particle-based Variational Inference with Preconditioned Functional Gradient Flow",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dPs6BGO2QT0": {
    "title": "Learning Locality and Isotropy in Dialogue Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eKllxpLOOm": {
    "title": "Combating Exacerbated Heterogeneity for Robust Models in Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vqSyt8D3ny": {
    "title": "Towards Robust Object Detection Invariant to Real-World Domain Shifts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yYEb8v65X8": {
    "title": "Light Sampling Field and BRDF Representation for Physically-based Neural Rendering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X5SUR7g2vVw": {
    "title": "Policy Pre-training for Autonomous Driving via Self-supervised Geometric Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ju_Uqw384Oq": {
    "title": "TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gfPUokHsW-": {
    "title": "Learning without Prejudices: Continual Unbiased Learning via Benign and Malignant Forgetting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tLScKVhcCR": {
    "title": "FINDE: Neural Differential Equations for Finding and Preserving Invariant Quantities",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3ZPESALKXO": {
    "title": "Approximate Vanishing Ideal Computations at Scale",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qY1hlv7gwg": {
    "title": "Selective Annotation Makes Language Models Better Few-Shot Learners",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PQ2zoIZqvm": {
    "title": "Switch-NeRF: Learning Scene Decomposition with Mixture of Experts for Large-scale Neural Radiance Fields",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CRNwGauQpb6": {
    "title": "NORM: Knowledge Distillation via N-to-One Representation Matching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ObtGcyKmwna": {
    "title": "Critic Sequential Monte Carlo",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8Oun8ZUVe8N": {
    "title": "Autoencoders as Cross-Modal Teachers: Can Pretrained 2D Image Transformers Help 3D Representation Learning?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0Q9H_Pgx132": {
    "title": "Deep Learning meets Nonparametric Regression: Are Weight-Decayed DNNs Locally Adaptive?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VV0hSE8AxCw": {
    "title": "Sparse Token Transformer with Attention Back Tracking",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ALDM5SN2r7M": {
    "title": "Robust Active Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zuc_MHtUma4": {
    "title": "Kernel Neural Optimal Transport",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=-qg8MQNrxZw": {
    "title": "SeaFormer: Squeeze-enhanced Axial Transformer for Mobile Semantic Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4UldFtZ_CVF": {
    "title": "Joint Edge-Model Sparse Learning is Provably Efficient for Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TXPN6MtdSE4": {
    "title": "Learning Sparse and Low-Rank Priors for Image Recovery via Iterative Reweighted Least Squares Minimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jXQ0ipgMdU": {
    "title": "Spherical Sliced-Wasserstein",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m6ahb1mpwwX": {
    "title": "InPL: Pseudo-labeling the Inliers First for Imbalanced Semi-supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=-CefY2EOupj": {
    "title": "Maximizing Communication Efficiency for Large-scale Training via 0/1 Adam",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WVRb98rwbv9": {
    "title": "Truthful Self-Play",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TuHkVOjSAR": {
    "title": "Strategic Classification with Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PolHquob8M7": {
    "title": "Continual Transformers: Redundancy-Free Attention for Online Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f2wN4v_2__W": {
    "title": "Learning Symbolic Models for Graph-structured Physical Mechanism",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0v4VkCSkHNm": {
    "title": "Priors, Hierarchy, and Information Asymmetry for Skill Transfer in Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kIAx30hYi_p": {
    "title": "Self-Supervised Set Representation Learning for Unsupervised Meta-Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=itZ6ggvMnzS": {
    "title": "Causal Representation Learning for Instantaneous and Temporal Effects in Interactive Systems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OnM3R47KIiU": {
    "title": "Visual Imitation Learning with Patch Rewards",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ktrw68Cmu9c": {
    "title": "CodeT: Code Generation with Generated Tests",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JHW30A4DXtO": {
    "title": "Learning to Generate Columns with Application to Vertex Coloring",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XUxad2Gj40n": {
    "title": "EVC: Towards Real-Time Neural Image Compression with Mask Decay",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ICYasJBlZNs": {
    "title": "Predicting Cellular Responses with Variational Causal Inference and Refined Relational Information",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HmPOzJQhbwg": {
    "title": "ResAct: Reinforcing Long-term Engagement in Sequential Recommendation with Residual Actor",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4wZiAXD29TQ": {
    "title": "Dataset Pruning: Reducing Training Data by Examining Generalization Influence",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HE_75XY5Ljh": {
    "title": "StrucTexTv2: Masked Visual-Textual Prediction for Document Image Pre-training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z289SIQOQna": {
    "title": "Plateau in Monotonic Linear Interpolation --- A \"Biased\" View of Loss Landscape for Deep Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qUKsCztWlKq": {
    "title": "The KFIoU Loss for Rotated Object Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xmcYx_reUn6": {
    "title": "BrainBERT: Self-supervised representation learning for intracranial recordings",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XWkWK2UagFR": {
    "title": "General Neural Gauge Fields",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fB0hRu9GZUS": {
    "title": "Generate rather than Retrieve: Large Language Models are Strong Context Generators",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vk-j5pQY3Gv": {
    "title": "Discovering Informative and Robust Positives for Video Domain Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ashPce_W8F-": {
    "title": "Understanding Why Generalized Reweighting Does Not Improve Over ERM",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hY6M0JHl3uL": {
    "title": "Linear Connectivity Reveals Generalization Strategies",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9DZKk85Z4zA": {
    "title": "Gradient-Guided Importance Sampling for Learning Binary Energy-Based Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gmwDKo-4cY": {
    "title": "Composing Ensembles of Pre-trained Models via Iterative Consensus",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vTb1JI0Gps_": {
    "title": "Automated Data Augmentations for Graph Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v3y68gz-WEz": {
    "title": "Riemannian Metric Learning via Optimal Transport",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8HRvyxc606": {
    "title": "Reliability of CKA as a Similarity Measure in Deep Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9vcXCMp9VEp": {
    "title": "Fair Attribute Completion on Graph with Missing Attributes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=_ruvo2KCL2x": {
    "title": "Deep Ranking Ensembles for Hyperparameter Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kUI41mY8bHl": {
    "title": "Robustness to corruption in pre-trained Bayesian neural networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=resApVNcqSB": {
    "title": "Weakly-supervised HOI Detection via Prior-guided Bi-level Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KXRSh0sdVTP": {
    "title": "Meta-learning Adaptive Deep Kernel Gaussian Processes for Molecular Property Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FYZCHEtt6H0": {
    "title": "ERL-Re$^2$: Efficient Evolutionary Reinforcement Learning with Shared State Representation and Individual Policy Representation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WZH7099tgfM": {
    "title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hZftxQGJ4Re": {
    "title": "Deep Ensembles for Graphs with Higher-order Dependencies",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PaEUQiY40Dk": {
    "title": "Towards Understanding Why Mask Reconstruction Pretraining Helps in Downstream Tasks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=20GtJ6hIaPA": {
    "title": "Self-Supervised Category-Level Articulated Object Pose Estimation with Part-Level SE(3) Equivariance",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6orC5MvgPBK": {
    "title": "Thalamus: a brain-inspired algorithm for biologically-plausible continual learning and disentangled representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8aeSJNbmbQq": {
    "title": "Deep Variational Implicit Processes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zDjtZZBZtqK": {
    "title": "Denoising Masked Autoencoders Help Robust Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ULsuEVQbV-9": {
    "title": "Estimating individual treatment effects under unobserved confounding using binary instruments",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a2-aoqmeYM4": {
    "title": "Approximate Bayesian Inference with Stein Functional Variational Gradient Descent",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s-c96mSU0u5": {
    "title": "SCoMoE: Efficient Mixtures of Experts with Structured Communication",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5OygDd-4Eeh": {
    "title": "An Additive Instance-Wise Approach to Multi-class Model Interpretation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ILQVw4cA5F9": {
    "title": "LDMIC: Learning-based Distributed Multi-view Image Coding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HaHCoGcpV9": {
    "title": "Sound Randomized Smoothing in Floating-Point Arithmetic",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hLbeJ6jObDD": {
    "title": "Collaborative Pure Exploration in Kernel Bandit",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Yn0xg-kHNW-": {
    "title": "Provably Efficient Risk-Sensitive Reinforcement Learning: Iterated CVaR and Worst Path",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3aBuJEza5sq": {
    "title": "Test-Time Robust Personalization for Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BGF9IeDfmlH": {
    "title": "Learning to Linearize Deep Neural Networks for Secure and Efficient Private Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TDf-XFAwc79": {
    "title": "Meta Knowledge Condensation for Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9-umxtNPx5E": {
    "title": "Masked Frequency Modeling for Self-Supervised Visual Pre-Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DHyHRBwJUTN": {
    "title": "Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mjHlitXvReu": {
    "title": "Learning Object-Language Alignments for Open-Vocabulary Object Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iN3Lh-Vy2TH": {
    "title": "Phase transition for detecting a small community in a large network",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b4t9_XASt6G": {
    "title": "On the Word Boundaries of Emergent Languages Based on Harris's Articulation Scheme",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CIFOsnhZvON": {
    "title": "TempCLR: Temporal Alignment Representation with Contrastive Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=My57qBufZWs": {
    "title": "Bort: Towards Explainable Neural Networks with Bounded Orthogonal Constraint",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bPiHuNUNv_R": {
    "title": "The Power of Regularization in Solving Extensive-Form Games",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=P8YIphWNEGO": {
    "title": "MLPInit: Embarrassingly Simple GNN Training Acceleration with MLP Initialization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8T4qmZbTkW7": {
    "title": "Progressively Compressed Auto-Encoder for Self-supervised Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gx2yJS-ENqI": {
    "title": "S-NeRF: Neural Radiance Fields for Street Views",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wC98X1qpDBA": {
    "title": "Cycle-consistent Masked AutoEncoder for Unsupervised Domain Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yAYHho4fATa": {
    "title": "CFlowNets: Continuous Control with Generative Flow Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OXP9Ns0gnIq": {
    "title": "Differentiable Gaussianization Layers for Inverse Problems Regularized by Deep Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZccFLU-Yk65": {
    "title": "DBQ-SSD: Dynamic Ball Query for Efficient 3D Object Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=01KmhBsEPFO": {
    "title": "Exploring Low-Rank Property in Multiple Instance Learning for Whole Slide Image Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F91SROvVJ_6": {
    "title": "Causal Balancing for Domain Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rzrqh85f4Sc": {
    "title": "Towards Addressing Label Skews in One-Shot Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=-jTaz3CMk72": {
    "title": "Breaking Correlation Shift via Conditional Invariant Regularizer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h21yJhdzbwz": {
    "title": "Towards One-shot Neural Combinatorial Solvers: Theoretical and Empirical Notes on the Cardinality-Constrained Case",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VWm4o4l3V9e": {
    "title": "Block and Subword-Scaling Floating-Point (BSFP) : An Efficient Non-Uniform Quantization For Low Precision Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0qmwFNJyxCL": {
    "title": "Rethinking the Effect of Data Augmentation in Adversarial Contrastive Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cxvEGLCHpgl": {
    "title": "Semi-supervised Community Detection via Structural Similarity Metrics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0vqjc50HfcC": {
    "title": "DDM$^2$: Self-Supervised Diffusion MRI Denoising with Generative Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rdjeCNUS6TG": {
    "title": "Multivariate Time-series Imputation with Disentangled Temporal Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FvevdI0aA_h": {
    "title": "Unified Detoxifying and Debiasing in Language Generation via Inference-time Adaptive Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KfptQCEKVW4": {
    "title": "Automating Nearest Neighbor Search Configuration with Constrained Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HDxgaKk956l": {
    "title": "Truncated Diffusion Probabilistic Models and Diffusion-based Adversarial Auto-Encoders",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=-5EWhW_4qWP": {
    "title": "NTK-SAP: Improving neural network pruning by aligning training dynamics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cbpRzMy-UZH": {
    "title": "Effective Self-supervised Pre-training on Low-compute Networks without Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L2MUOUp0beo": {
    "title": "CoRTX: Contrastive Framework for Real-time Explanation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7ynoX1ojPMt": {
    "title": "OTOv2: Automatic, Generic, User-Friendly",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fiB2RjmgwQ6": {
    "title": "Filter-Recovery Network for Multi-Speaker Audio-Visual Speech Separation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sbWVtxq8-zE": {
    "title": "Can discrete information extraction prompts generalize across language models?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bzaPGEllsjE": {
    "title": "A view of mini-batch SGD via generating functions: conditions of convergence, phase transitions, benefit from negative momenta",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PFbzoWZyZRX": {
    "title": "Bridging the Gap between ANNs and SNNs by Calibrating Offset Spikes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bHW9njOSON": {
    "title": "ESD: Expected Squared Difference as a Tuning-Free Trainable Calibration Measure",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AP0iZoaRaS": {
    "title": "Interactive Portrait Harmonization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kj6oK_Hj40": {
    "title": "Self-Distillation for Further Pre-training of Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PldynS56bN": {
    "title": "Contextual Convolutional Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KemSBwOYJC": {
    "title": "Statistical Inference for Fisher Market Equilibrium",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tPrRs6YB2P": {
    "title": "Scenario-based Question Answering with Interacting Contextual Properties",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rSUCajhLsQ": {
    "title": "Easy Differentially Private Linear Regression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8pOVAeo8ie": {
    "title": "LPT: Long-tailed Prompt Tuning for Image Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NkJOhtNKX91": {
    "title": "DamoFD: Digging into Backbone Design on Face Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W918Ora75q": {
    "title": "Towards Smooth Video Composition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=06mk-epSwZ": {
    "title": "DiffMimic: Efficient Motion Mimicking with Differentiable Physics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=li4GQCQWkv": {
    "title": "Towards Inferential Reproducibility of Machine Learning Research",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Fg3mYW8owg": {
    "title": "Knowledge Distillation based Degradation Estimation for Blind Super-Resolution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PLUXnnxUdr4": {
    "title": "Graph Contrastive Learning for Skeleton-based Action Recognition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s4WVupnJjmX": {
    "title": "Explicit Box Detection Unifies End-to-End Multi-Person Pose Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=frE4fUwz_h": {
    "title": "Spikformer: When Spiking Neural Network Meets Transformer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NRHajbzg8y0P": {
    "title": "Multimodal Analogical Reasoning over Knowledge Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N92hjSf5NNh": {
    "title": "MECTA: Memory-Economic Continual Test-Time Model Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R_OL5mLhsv": {
    "title": "Interpretability with full complexity by constraining feature information",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3zSn48RUO8M": {
    "title": "What shapes the loss landscape of self supervised learning?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=-z9hdsyUwVQ": {
    "title": "Linear Convergence of Natural Policy Gradient Methods with Log-Linear Policies",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UP_GHHPw7rP": {
    "title": "Nearly Minimax Optimal Offline Reinforcement Learning with Linear Function Approximation: Single-Agent MDP and Markov Game",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3KWnuT-R1bh": {
    "title": "Conditional Positional Encodings for Vision Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b_CQDy9vrD1": {
    "title": "ManiSkill2: A Unified Benchmark for Generalizable Manipulation Skills",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L8iZdgeKmI6": {
    "title": "Deja Vu: Continual Model Generalization for Unseen Domains",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=65XDF_nwI61": {
    "title": "A Graph Neural Network Approach to Automated Model Building in Cryo-EM Maps",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S3D9NLzjnQ5": {
    "title": "Distilling Cognitive Backdoor Patterns within an Image",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QjQibO3scV_": {
    "title": "Revocable Deep Reinforcement Learning with Affinity Regularization for Outlier-Robust Graph Matching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vZTp1oPV3PC": {
    "title": "One Transformer Can Understand Both 2D & 3D Molecular Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WumysvcMvV6": {
    "title": "Mind the Gap: Offline Policy Optimization for Imperfect Rewards",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S8-A2FXnIh": {
    "title": "Learning to Compose Soft Prompts for Compositional Zero-Shot Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IDJx97BC38": {
    "title": "SQA3D: Situated Question Answering in 3D Scenes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NJENsJ37sQ": {
    "title": "Empowering Networks With Scale and Rotation Equivariance Using A Similarity Convolution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wcNtbEtcGIC": {
    "title": "Robust and Controllable Object-Centric Learning through Energy-based Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ylMq8MBnAp": {
    "title": "Topology-aware Robust Optimization for Out-of-Distribution Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mfIX4QpsARJ": {
    "title": "EAGLE: Large-scale Learning of Turbulent Fluid Dynamics with Mesh Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XqcQhVUr2h0": {
    "title": "Limitless Stability for Graph Convolutional Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q_Jexl8-qDi": {
    "title": "De Novo Molecular Generation via Connection-aware Motif Mining",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SNgLnzFQeiD": {
    "title": "Revisiting the Entropy Semiring for Neural Speech Recognition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yQdBtFfleh6": {
    "title": "Rethinking skip connection model as a learnable Markov chain",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lZOUQQvwI3q": {
    "title": "Measuring axiomatic soundness of counterfactual image models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KKBMz-EL4tD": {
    "title": "Alternating Differentiation for Optimization Layers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hdghx6wbGuD": {
    "title": "Out-of-distribution Detection with Implicit Outlier Transformation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cMAjKYftNwx": {
    "title": "Extracting Robust Models with Uncertain Examples",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pza24zf9FpS": {
    "title": "Neural Groundplans: Persistent Neural Scene Representations from a Single Image",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g1GnnCI1OrC": {
    "title": "E-CRF: Embedded Conditional Random Field for Boundary-caused Class Weights Confusion in Semantic Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9x3CO0ZU9LR": {
    "title": "Sample Complexity of Nonparametric Off-Policy Evaluation on Low-Dimensional Manifolds using Deep Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3nM5uhPlfv6": {
    "title": "Stochastic Differentially Private and Fair Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MxvHVNukama": {
    "title": "On The Inadequacy of Optimizing Alignment and Uniformity in Contrastive Learning of Sentence Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EVrz7UM-ZDm": {
    "title": "Volumetric Optimal Transportation by Fast Fourier Transform",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uKiE0VIluA-": {
    "title": "GFlowNets and variational inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zlwBI2gQL3K": {
    "title": "Hierarchical Relational Learning for Few-Shot Knowledge Graph Completion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pgHNOcxEdRI": {
    "title": "Function-Consistent Feature Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xLr0I_xYGAs": {
    "title": "The Devil is in the Wrongly-classified Samples: Towards Unified Open-set Recognition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1FxRPKrH8bw": {
    "title": "MCAL: Minimum Cost Human-Machine Active Labeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hVVUY7p64WL": {
    "title": "Learnable Topological Features For Phylogenetic Inference via Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=woa783QMul": {
    "title": "Fairness-aware Contrastive Learning with Partially Annotated Sensitive Attributes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=_X9Yl1K2mD": {
    "title": "Rotamer Density Estimator is an Unsupervised Learner of the Effect of Mutations on Protein-Protein Interaction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q3-1vRh3HOA": {
    "title": "Dilated convolution with learnable spacings",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t9Zd7Oi5JPl": {
    "title": "PatchDCT: Patch Refinement for High Quality Instance Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1ROAstc9jv": {
    "title": "ChiroDiff: Modelling chirographic data with Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PmP_sf3JkrH": {
    "title": "Real-Time Image Demoir$\\acute{e}$ing on Mobile Devices",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Kn-HA8DFik": {
    "title": "Cross-Level Distillation and Feature Denoising for Cross-Domain Few-Shot Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eGm22rqG93": {
    "title": "DELTA: DEGRADATION-FREE FULLY TEST-TIME ADAPTATION",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YUDiZcZTI8": {
    "title": "Bit-Pruning: A Sparse Multiplication-Less Dot-Product",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x5mtJD2ovc": {
    "title": "kNN-Diffusion: Image Generation via Large-Scale Retrieval",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nQai_B1Zrt": {
    "title": "Decompose to Generalize: Species-Generalized Animal Pose Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ConT6H7MWL": {
    "title": "IDEAL: Query-Efficient Data-Free Learning from Black-Box Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AZFvpnnewr": {
    "title": "Trainability Preserving Neural Pruning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D-zfUK7BR6c": {
    "title": "Diagnosing and Rectifying Vision Models using Language",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=boNyg20-JDm": {
    "title": "Harnessing Out-Of-Distribution Examples via Augmenting Content and Style",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Kn6i2BZW69w": {
    "title": "DropIT: Dropping Intermediate Tensors for Memory-Efficient DNN Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cCFqcrq0d8": {
    "title": "A Unified Framework for Soft Threshold Pruning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=-CwPopPJda": {
    "title": "TaskPrompter: Spatial-Channel Multi-Task Prompting for Dense Scene Understanding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=-HHJZlRpGb": {
    "title": "Learning Domain-Agnostic Representation for Disease Diagnosis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JdgO-ht1uTN": {
    "title": "Logical Entity Representation in Knowledge-Graphs for Differentiable Rule Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=-2zfgNS917": {
    "title": "BEVDistill: Cross-Modal BEV Distillation for Multi-View 3D Object Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MLJ5TF5FtXH": {
    "title": "A Multi-Grained Self-Interpretable Symbolic-Neural Model For Single/Multi-Labeled Text Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CGuvK3U09LH": {
    "title": "Suppressing the Heterogeneity: A Strong Feature Extractor for Few-shot Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hfUJ4ShyDEU": {
    "title": "Achieve the Minimum Width of Neural Networks for Universal Approximation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NPfDKT9OUJ3": {
    "title": "H2RBox: Horizontal Box Annotation is All You Need for Oriented Object Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xzmqxHdZAwO": {
    "title": "Pushing the Limits of Fewshot Anomaly Detection in Industry Vision: Graphcore",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8FroynZv4C": {
    "title": "Representation Learning for Low-rank General-sum Markov Games",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=APuPRxjHvZ": {
    "title": "Surgical Fine-Tuning Improves Adaptation to Distribution Shifts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RVTOp3MwT3n": {
    "title": "Diversify and Disambiguate: Out-of-Distribution Robustness via Disagreement",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TQ5WUwS_4ai": {
    "title": "On amortizing convex conjugates for optimal transport",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I_YZANaz5X": {
    "title": "DualAfford: Learning Collaborative Visual Affordance for Dual-gripper Manipulation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CjTHVo1dvR": {
    "title": "Molecular Geometry Pretraining with SE(3)-Invariant Denoising Distance Matching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BqrPeZ_e5P": {
    "title": "SIMPLE: Specialized Model-Sample Matching for Domain Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6kxApT2r2i": {
    "title": "The Augmented Image Prior: Distilling 1000 Classes by Extrapolating from a Single Image",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=07tc5kKRIo": {
    "title": "Delving into Semantic Scale Imbalance",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jgmuRzM-sb6": {
    "title": "DAG Matters! GFlowNets Enhanced Explainer for Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A3sgyt4HWp": {
    "title": "Contextual Image Masking Modeling via Synergized Contrasting without View Augmentation for Faster and Better Visual Pretraining",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=10R_bcjFwJ": {
    "title": "Patch-Level Contrasting without Patch Correspondence for Accurate and Dense Contrastive Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=P5Z-Zl9XJ7": {
    "title": "Continuous-Discrete Convolution for Geometry-Sequence Modeling in Proteins",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xmY_plRB15j": {
    "title": "Statistical Property Testing for Generative Models",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5aO1lsEJGu": {
    "title": "Can Conformal Prediction Obtain Meaningful Safety Guarantees for ML Models?",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LcXWYmA8Ek": {
    "title": "A two-parameter learnable Logmoid Activation Unit",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rrZtzI7xj2b": {
    "title": "Cross Domain Vulnerability Detection using Graph Contrastive Learning",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BWWrDHaP29": {
    "title": "Tiny Attention: A Simple yet Effective Method for Learning Contextual Word Embeddings",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8WiNDyXgj6": {
    "title": "Training Data Eigenvector Dynamics in the EigenPro Implementation of the Neural Tangent Kernel and Recursive Feature Machines",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fVuTIiTBky": {
    "title": "Metric Transform: Exploring beyond Affine Transform for Neural Networks",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mIEMVZ47aNA": {
    "title": "MaskedFusion360: Reconstruct LiDAR Data by Querying Camera Features",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7QqlQW9hJ8J": {
    "title": "Meta-Learning for Subject Adaptation in Low-Data Environments for EEG-Based Motor Imagery Brain-Computer Interfaces",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AHnLJBD7xKx": {
    "title": "Fostering Effective Communication Between Humans and Machines",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7lyEQXHkGpl": {
    "title": "Transfer Learning on Kinyarwanda Tweets Sentiment Analysis",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RuCQRXk7a7G": {
    "title": "Symbolic Regression in Financial Economics",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=75mWq5j4iso": {
    "title": "FRESCO: Federated Reinforcement Energy System for Cooperative Optimization",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jbNqgEJf0EI": {
    "title": "Learning Rotation-Agnostic Representations via Group Equivariant VAEs",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KPHbtTtCDw": {
    "title": "Can Text Encoders be Deceived by Length Attack?",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kidk_E11QQ": {
    "title": "Whispering Across the Continent: Collecting and Analyzing African Culture Using Community Radios",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e2gSQqH3V10": {
    "title": "Handling unstructured data for operator learning using implicit neural representations",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=njpSzZ6mCU": {
    "title": "Model Extraction Attacks on DistilBERT",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qq-bA-VLUN": {
    "title": "One Important Thing To Do Before Federated Training",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=_cL7Uj4LXAJ": {
    "title": "Insights into the mechanism behind reusing Teacher's classifier in Knowledge Distillation",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iCqvQSvar5V": {
    "title": "When will federated learning transfer from generalization to personalization?",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S9NTReFikL2": {
    "title": "A Change of Heart: Improving Speech Emotion Recognition through Speech-to-Text Modality Conversion",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=if1Mmrxf-pq": {
    "title": "Fairness Under Partial Observability",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cFtt9fU7YB6": {
    "title": "Geodesic Mode Connectivity",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=_cZLvP7LAt7": {
    "title": "Pursuit Policies in Dynamic Environments",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OJILbuOodvm": {
    "title": "Resource-efficient image inpainting",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TS8l4VS7_BK": {
    "title": "Recursive Reasoning with Neural Networks",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wUEY2CdQGi1": {
    "title": "SUDANESE ARABIC DIALECT ENCODING USING XLM-RoBERTa LANGUAGE MODEL: Zol-ROBERTA",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PIfJnq9kpdw": {
    "title": "Reducing the Effect of Incomplete Annotations in Object Detection for Histopathology",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nSqrgBKBGkv": {
    "title": "Experimenting with Multimodal AutoML: Detection and Evaluation of Alzheimer's Disease",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2KxH_4US0ZH": {
    "title": "GFlowNets with Human Feedback",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R82eeIF4rP_": {
    "title": "Attention-likelihood relationship in Transformers",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cQ_eEMsc6p": {
    "title": "Truly Generative Data Augmentation for Image Segmentation - Case of Cloud Images",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ghfL1e2rOd": {
    "title": "Fast Fourier Convolutions in Self-Supervised Neural Networks for Image Denoising",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lpcO1957Tv": {
    "title": "Personalized Federated Learning for Medical Segmentation using Hypernetworks",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R5NNAThG0i": {
    "title": "Career Path Modeling and Recommendations with Linkedin Career Data and Predicted Salary Estimations",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q_lJooPbX_": {
    "title": "Sleep Deprivation in the Forward-Forward Algorithm",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nF70Sl-HUZ": {
    "title": "MetaXLR - Mixed Language Meta Representation Transformation for Low-resource Cross-lingual Learning based on Multi-Armed Bandit",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cKlgcx7nSZ": {
    "title": "Prune and Tune: Improving Efficient Pruning Techniques for Massive Language Models",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XptW6NuULJ": {
    "title": "Model Extraction Attacks on Arabic BERT-Based APIs",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=baF9FqIdTY": {
    "title": "IMITATION LEARNING USING THE FORWARD-FORWARD ALGORITHM",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dGuMR8tLDs": {
    "title": "The Geometry of Multilingual Language Models: An Equality Lens",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Muwb2KohnX": {
    "title": "Dynamic Human AI Collaboration",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jd7Hy1jRiv4": {
    "title": "The Responsibility Problem in Neural Networks with Unordered Targets",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=losgEaOWIL7": {
    "title": "Concept Understanding in Large Language Models: An Empirical Study",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rAT51tL04I2": {
    "title": "Adaptive Distance Message Passing From the Multi-Relational Edge View",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DLwlmWwmJBi": {
    "title": "Sustainable Resource Management",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n5aZMLXVndP": {
    "title": "Chain Of Thought Prompting Under Streaming Batch: A Case Study",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F7mdgA7c2zD": {
    "title": "Language Models can do Zero-Shot Visual Referring Expression Comprehension",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=isodM5jTA7h": {
    "title": "Simple Parameter-free Self-attention Approximation",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cYKtDg5JnxV": {
    "title": "Neuromodulation Gated Transformer",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Lm7z2vYergk": {
    "title": "Decomposing Causality and Fairness",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EARgl3EH-nq": {
    "title": "Self-Supervised Image Denoising with Swin Transformer",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1wtUadpmVzu": {
    "title": "SimbaML: Connecting Mechanistic Models and Machine Learning with Augmented Data",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zwywBS3GyFs": {
    "title": "GeValDi: Generative Validation of Discriminative Models",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kbhUUAMZmQT": {
    "title": "Regularized Offline GFlowNets",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xZGPLvRpf4N": {
    "title": "SECURE COMMUNICATION MODEL FOR QUANTUM FEDERATED LEARNING: A PROOF OF CONCEPT",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wecTsVkrjit": {
    "title": "Learn to Select: Efficient Cross-device Federated Learning via Reinforcement Learning",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hli7A0ioiS_": {
    "title": "A Dynamic Prompt-tuning Method for Data Augmentation with Associated Knowledge",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1LeLyB6T0JM": {
    "title": "MARKOVIAN EMBEDDINGS FOR COALITIONAL BARGAINING GAMES",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1_PEwKmTepo": {
    "title": "The Point to Which Soft Actor-Critic Converges",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qNJRvdKDGYg": {
    "title": "No Double Descent in Self-Supervised Learning",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kBkou5ucR_d": {
    "title": "Resampling Gradients Vanish in Differentiable Sequential Monte Carlo Samplers",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uNrSvEr9Lqc": {
    "title": "Generalised Lookahead Optimiser",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d0m0Rl15q3g": {
    "title": "Revisiting CounteRGAN for Counterfactual Explainability of Graphs",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GJhsHNKm7kj": {
    "title": "Language Models Inversely Scale on Piecewise Function Evaluation with Biased Examples",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iF2w_kqmYmw": {
    "title": "RETHINKING POSITIONAL EMBEDDING: A CASE STUDY IN TEMPORAL EVENT SEQUENCE MODELLING",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TFrzVBZk05g": {
    "title": "Exploring Efficient and Simple Initialization Strategies for Bayesian Optimization with SETUP-BO",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x4MuUFPKEIj": {
    "title": "Quota Constraints for Diversity Interventions in Subset Selection",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zYF6NLJl6LM": {
    "title": "Mitigating Metastable Failures in Distributed Systems with Offline Reinforcement Learning",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=-bMH1Sk8SSF": {
    "title": "Knowledge Distillation of BERT Language Model on the Arabic Language",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DoOiwBcRir3": {
    "title": "Offensiveness as an Opinion: Dissecting population-level Label Distributions",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z2Gr8YqsimF": {
    "title": "How do ConvNets Understand Image Intensity?",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x_adzmY6PQ5": {
    "title": "Learning Weight Sensitivity from Entropy",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BQpCuJoMykZ": {
    "title": "A Study on Sample Diversity in Generative Models: GANs vs. Diffusion Models",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XCbe51-arJt": {
    "title": "Artificial Intelligent Life: A New Perspective on Artificial General Intelligence",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cRZSr6Tpr1S": {
    "title": "Generative AI for Therapy? Opportunities and Barriers for ChatGPT in Speech-Language Therapy",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1yXbt6_o6av": {
    "title": "The Art of Embedding Fusion: Optimizing Hate Speech Detection",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=STpRX-XCO6t": {
    "title": "COLLABORATIVE CONCEPT DRIFT DETECTION",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EyliiBqhFz": {
    "title": "End-to-End Learnable Masks With Differentiable Indexing",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hx_iTXnCR5": {
    "title": "FigGen: Text to Scientific Figure Generation",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OMARmh02Ruk": {
    "title": "Evaluating Impact of Emoticons and Pre-processing on Sentiment Classification of Translated African Tweets",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OiSbJbVWBJT": {
    "title": "SoftEDA: Rethinking Rule-Based Data Augmentation with Soft Labels",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5CdkvFyatt2": {
    "title": "MatPropXtractor: Generate to Extract",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=-AIukSeLAz9": {
    "title": "Zero-Shot Classification Reveals Potential Positive Sentiment Bias in African Languages Translations",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c-YTzVUkfAW": {
    "title": "Lesion Search with Self-supervised Learning",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GPA-BPLwYHf": {
    "title": "Feature Importance Analysis for Mini Mental Status Score Prediction in Alzheimer's Disease",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1F8pPnUinbU": {
    "title": "On a Relation Between the Rate-Distortion Function and Optimal Transport",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YdGkE4Ugg2C": {
    "title": "Is CLIP Fooled by Optical Illusions?",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=_QreMdMNIz-": {
    "title": "Seeing in Words: Learning to Classify through Language Bottlenecks",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0sxmoci9Ma": {
    "title": "Towards Stochastic Gradient Variance Reduction by Solving a Filtering Problem",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aVepdnlRb5": {
    "title": "MACHINE TRANSLATION BASELINES FOR ARABIC - SWAHILI",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QimsmhYvsf": {
    "title": "Accuracy of white box and black box adversarial attacks on a sign activation 01 loss neural network ensemble",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AidIUjh__t": {
    "title": "TopEx: Topic-based Explanations for Model Comparison",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cxmjk8O3Yn": {
    "title": "GeneDAE: A Sparse Denoising Autoencoder for Deriving Interpretable Gene Embeddings",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XHfWgU2IiP": {
    "title": "Improving Hyperspectral Adversarial Robustness Under Multiple Attacks",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hBz5h3C9Sq": {
    "title": "Prompt Programming for the Visual Domain",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ufA2FuCGyz": {
    "title": "Mapping the Typographic Latent Space of Digits",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Mrz9PgP3sT": {
    "title": "Hyperbolic Deep Reinforcement Learning for Continuous Control",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XXsjViheWZ": {
    "title": "Incorporating Expert Prior Knowledge for Oral Lesion Recognition",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2M8dEAJcG5": {
    "title": "LEARNING LIGHTWEIGHT STRUCTURE-AWARE EMBEDDINGS FOR PROTEIN SEQUENCES",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lmcPpHDa0B": {
    "title": "Proactive policing as reinforcement learning",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G0heahVv5Y": {
    "title": "The Small Batch Size Anomaly in Multistep Deep Reinforcement Learning",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VBuUL2IWlq": {
    "title": "Bootstrapping Parallel Anchors for Relative Representations",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qqKO_rrg9y": {
    "title": "A Brief History of the Speculative Measures for Autonomy",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=_1z2Bqte5L": {
    "title": "Semantic feature verification in FLAN-T5",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XiZOalwf_U": {
    "title": "Augmenting Collective Intelligence through Belbin's Team Roles",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7Zyv70nGl_g": {
    "title": "Text2Face: 3D Morphable Faces From Text",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oKa5_mxHBV": {
    "title": "Towards Parametric Robust Activation Functions in Adversarial Machine Learning",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iWiwox99aJ": {
    "title": "Understanding Label Bias in Single Positive Multi-Label Learning",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5lZaexgIey": {
    "title": "Knowledge and Attitude of Medical Students and Doctors towards Artificial Intelligence: A study of University of Ilorin",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K-SVVOIcsP": {
    "title": "Human-machine cooperation for semantic feature listing",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vHOO1lxggJ": {
    "title": "Improving generalization by loss modification",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6ry6ibTKOx": {
    "title": "A Rate--Distortion View on Model Updates",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2g4m5S_knF": {
    "title": "Text-Based Games as a Challenging Benchmark for Large Language Models",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0Zhwu1VaOs": {
    "title": "Learned Learning Rate Schedules for Deep Neural Network Training Using Reinforcement Learning",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CTZigc9V69": {
    "title": "Speaker-Invariant Speech Recognition through Fine-Tuning on Individual-Specific Data with Voice Conversion",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SBqbPjVFfm": {
    "title": "Effect of training fragment length on Transformers in text complexity prediction",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lkWvTn2IzA": {
    "title": "Revisiting Bisimulation: A Sampling-Based State Similarity Pseudo-metric",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AAu_WuIiwi": {
    "title": "Efficient Learning rate schedules for Stochastic Non-negative Matrix Factorization via Reinforcement Learning",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H2mbtfasD4K": {
    "title": "Chaotic Transformers for Deep Reinforcement Learning in Algorithmic Trading",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G_MpqMHYo-": {
    "title": "Effects of Single-Attribute Control on the Music Generated by FIGARO",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FWohKbMhlo": {
    "title": "Iterative weakly supervised learning for novel class object detection",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LV33sOiYCEP": {
    "title": "BANDIT SAMPLING FOR FASTER NEURAL NETWORK TRAINING WITH SGD",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MjVdwBGkys": {
    "title": "Train Monolingual, Infer Bilingual",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TPTbHxeR6U": {
    "title": "A Simple, Fast Algorithm for Continual Learning from High-Dimensional Data",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H9BGkFz-Sm": {
    "title": "Discerning Self-Supervised Learning and Weakly Supervised Learning",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x2XNoPdXF8J": {
    "title": "DiffGANPaint: Fast Inpainting Using Denoising Diffusion GANs",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dJfdug9aGd8": {
    "title": "Effectiveness of Debiasing Techniques: An Indigenous Qualitative Analysis",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FPzByCI0yz1": {
    "title": "An Analysis of Transferability in Network Intrusion Detection using Distributed Deep Learning",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=99XvUeDFYTD": {
    "title": "MLP-Attention: Improving Transformer Architecture with MLP Attention Weights",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4isz71_aZN": {
    "title": "Averager Student: Distillation from Undistillable Teacher",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2rp3guEM3A": {
    "title": "Inducing Document Representations from Graphs: A Blueprint",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a9VgV-hywP": {
    "title": "Uncertainty-Aware Test-Time Augmented Ensemble of BERTs for Classification of Common Mental Illnesses on Social Media Posts",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uMlLT_xuiE": {
    "title": "Predicting Targets with Data from Non-Conforming Sources",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o7uGWBK6Uo": {
    "title": "One Explanation Does Not Fit XIL",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xKf-LSD2-Jg": {
    "title": "Fast Adversarial CNN-based Perturbation Attack on No-Reference Image- and Video-Quality Metrics",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CuE1F1M0_yR": {
    "title": "GraphEx: A User-Centric Model-Level Explainer for Graph Neural Networks",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vd16AYbem3Z": {
    "title": "Theta sequences as eligibility traces: A biological solution to credit assignment",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mvo72yTjhTl": {
    "title": "Federated Learning with Variational Autoencoders",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AXxBPw5zdl4": {
    "title": "TRACTABLE LARGE SCALE CALIBRATION WITH RL",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A2VfgYliIT": {
    "title": "A Variational Condition for Minimal-Residual Latent Representations",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j1gj0ndrk1": {
    "title": "Attention Based Variational Graph Auto-Encoder (AVGAE)",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=-IH_dcPGWM": {
    "title": "Generative STResnet for Crime Prediction",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KtVonyo4AS": {
    "title": "A Light Spectrometer Device for Crop Disease Monitoring",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jbBPBUGk-4": {
    "title": "Can Arterial Blood Pressure Predict Age? A ConvNet Classification Task",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IkHVGw_Ipu": {
    "title": "Pay Attention to Multi-Channel for Improving Graph Neural Networks",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8kX0btdpAU5": {
    "title": "pGS-CAM: Interpretable LiDAR Point Cloud Semantic Segmentation via Gradient Based Localization",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N0lQfjeNWOE": {
    "title": "Large Language Models Perform Diagnostic Reasoning",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WaAJ883AqiY": {
    "title": "A Simple Loss Function for Convergent Algorithm Synthesis using RNNs",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zEGstYVHBt": {
    "title": "The Obscure Limitation of Modular Multilingual Language Models",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zdhCATDn_Q": {
    "title": "Exploratory Analysis of Scholarly Publications on Artificial Intelligence (AI) in Colonoscopy using Litstudy",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RR_w2fbYmV": {
    "title": "Propagate Deeper and Adaptive Graph Convolutional Networks",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qW_GZYyn7C": {
    "title": "L2 Norm Guided Adaptive Computation",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nazr0QFvHR": {
    "title": "Efficient Temporal Denoising for Improved Depth Map Applications",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QRKKFN7FLm": {
    "title": "AI-based opportunistic analysis of the CT images during COVID (2021): Does living in a metropolitan area affect the vertebral body mineral density in older people?",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1PW_txDkX7": {
    "title": "One Student Knows All Experts Know: From Sparse to Dense",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WkDqZD3VRo": {
    "title": "AN ENSEMBLE LEARNING FRAMEWORK FOR VISIBILITY PREDICTION IN INDO-GANGETIC REGION",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fcQFbluDTX": {
    "title": "Evolutionary Federated Learning Using Particle Swarm Optimization",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0GpMf9UeI3G": {
    "title": "Understanding the Effectiveness of Cross-Domain Contrastive Unsupervised Domain Adaptation",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A-E41oZCfrf": {
    "title": "Performance Evaluation of Enhanced ConvNeXtTiny-based Fire Detection System in Real-world Scenarios",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zZjPRz0EX5T": {
    "title": "Robustness Evaluation of Multi-Agent Reinforcement Learning Algorithms using GNAs",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PjypHLTo29v": {
    "title": "State Advantage Weighting for Offline RL",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6I5i0Ytnlul": {
    "title": "Towards Robust Feature Learning with t-vFM Similarity for Continual Learning",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=28si4RXwDt1": {
    "title": "When Biology has Chemistry: Solubility And Drug Subcategory Prediction using SMILES Strings",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D2lo4toTUTo": {
    "title": "Answering Questions Over Knowledge Graphs Using Logic Programming Along with Language Models",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZTp85mW5nFy": {
    "title": "Contrastive Training with more data",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GSlYBJ3aOpC": {
    "title": "Statistical Methods for Auditing the Quality of Manual Content Reviews",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TqkzImZ92t8": {
    "title": "ARTIFICIAL PSYCHOLOGY",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=87oCobKKS6x": {
    "title": "Automated Mapping of Healthcare Concepts to a Standardized Healthcare Taxonomy",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=frB4MiYGoD_": {
    "title": "RepFair-GAN: Mitigating Representation Bias in GANs Using Gradient Clipping",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p7sHcNt_tqo": {
    "title": "Prior knowledge meets Neural ODEs: a two-stage training method for improved explainability",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=udl9OobOxZu": {
    "title": "Self-Supervised Continual Learning",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MuOFB0LQKcy": {
    "title": "When Spiking Neural Networks Meet Temporal Attention Image Decoding and Adaptive Spiking Neuron",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HM_jOWEYL7y": {
    "title": "Characters Are Like Faces",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uwbyW92Sonu": {
    "title": "A Scalable Self-supervised Learner for Hyperspectral Image Classification",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B5riBS9HZGn": {
    "title": "Optimizing MPJPE promotes miscalibration in multi-hypothesis human pose lifting",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Tbzv_BbjjO8": {
    "title": "Unsupervised Detection of Cell Assemblies with Graph Neural Networks",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wFxjFCHUkS": {
    "title": "Heat Up The Sentiment Learning With ICE",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IcDTYTI0Nx": {
    "title": "Almost Sure Last Iterate Convergence of Sharpness-Aware Minimization",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WKVH54a1W4": {
    "title": "On the application and impact of ε-DP and fairness in ambulance engagement time prediction",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5-ROmmBJKV": {
    "title": "Adversarial Policy Gradient for Learning Graph-Based Representation in Human Visual Processing",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qX8cGLnfAd": {
    "title": "Federated Learning for Local and Global Data Distribution",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rLqN6XLbON": {
    "title": "Is DFR for Soft Biometrics Prediction in Unconstrained Images Fair and Effective?",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1WEPXTIjAd": {
    "title": "INTEGRATING INFORMATION FROM NATURAL LANGUAGE PARSE TREE TO CODE GENERATION",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=-CH1C-aQ5pk": {
    "title": "Pseudo Labels for Single Positive Multi-Label Learning",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=osqgjMNm4_": {
    "title": "Drowning Detection based on YOLOv8 improved by GP-GAN Augmentation",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eaKoBpxCPe": {
    "title": "Beyond Negativity: Re-Analysis and Follow-Up Experiments on Hope Speech Detection",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=niyvAOOnwPM": {
    "title": "Group Equivariant Convolutional Networks",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2Z-dQTRezZ": {
    "title": "Exploring Semantic Variations in GAN Latent Spaces via Matrix Factorization",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iSkcAjBqUHU": {
    "title": "The Polarised Regime of identifiable Variational Autoencoders",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=91Bcj6sgcxt": {
    "title": "Uni-Match: A Semantic Unified Model for Query-Product Retrieval",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Peb3QdR8zzP": {
    "title": "Hierarchical Dialogue Understanding with Special Tokens and Turn-level Attention",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZCv4E1unfJP": {
    "title": "Large Sparse Kernels for Federated Learning",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WFatA9XIQ0m": {
    "title": "Using vision transformer-based GANs against Vision Transformers",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PaHmtktx86H": {
    "title": "Pivot Pre-finetuning for Low Resource MT: A Case Study in Kikamba",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WU3veNUvvU": {
    "title": "Unsupervised Learning for Anomaly Detection: A Comparison of Deep Generative Models",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U4o5iSWSaD": {
    "title": "Bayes classifier cannot be learned from noisy responses with unknown noise rates",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ChW0YYRIni": {
    "title": "Contrastive Learning with 3D Shapes",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ihzsru2bw2": {
    "title": "Adaptive-saturated RNN: Remember more with less instability",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pKd6q-FrprW": {
    "title": "Stratospheric Aerosols: Establishing a Novel Optical Thickness Benchmark for Effective Climate Change Mitigation",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cKLmwCTFiI": {
    "title": "CausalStructCodec: Causally-aware observational and interventional data generator",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bRI_3OFg4o": {
    "title": "Semantic Similarity Based Label Augmentation for Visual Classification",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3EfxJTp_-Cj": {
    "title": "Prompt Engineering and Calibration for Zero-Shot Commonsense Reasoning",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nbqO93YTz-": {
    "title": "Fidelity of Interpretability Methods and Perturbation Artifacts in Neural Networks",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EVz_vcZQvvg": {
    "title": "EDCDE - Extended Discovery of Closed-Form Differential Equations",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AFLNyWMg4D2": {
    "title": "Synthetic Controls as Balancing Scores",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qPwZouq5sY_": {
    "title": "Clustered Federated Learning with Slightly Skewed Labels",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Jx44OPxLZ2-": {
    "title": "Fusing 3D-CNN and lightweight Swin Transformer networks for HSI",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=_3_VZtMkvMB": {
    "title": "Compound Tokens: Channel Fusion for Vision-Language Representation Learning",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=99Go96dla5y": {
    "title": "Message-passing Selection: Towards Interpretable GNNs for Graph Classification",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OaZktJBVpUy": {
    "title": "MULTI-AGENT REINFORCEMENT LEARNING FOR COALITIONAL BARGAINING GAMES",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ChqP6ORFYK6": {
    "title": "Astroformer: More Data might not be all you need for Classification",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EVwbNcRa6Yf": {
    "title": "Error Analysis of Fitted Q-iteration with ReLU-activated Deep Neural Networks",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ddFJsnpZtTX": {
    "title": "General Purpose Artificial Intelligence Systems as Group Agents",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L38bbHmRKx": {
    "title": "An Empirical Study of the Effect of Background Data Size on the Stability of SHapley Additive exPlanations (SHAP) for Deep Learning Models",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RNlfw6KXJey": {
    "title": "Interpretable Machine Learning-Based Risk Scoring with Individual and Ensemble Model Selection for Clinical Decision Making",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g6ZFp73_T7": {
    "title": "Analytical solutions for a family of single layer neural network regression problems",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Idalad_7wG": {
    "title": "Pretrained Vision Models for Predicting High-Risk Breast Cancer Stage",
    "volume": "tiny",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  }
}